<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6642 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6642</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6642</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-5a20aa49b81b4e14fdb36814e557b3da60259ce9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5a20aa49b81b4e14fdb36814e557b3da60259ce9" target="_blank">Chain of Thought Empowers Transformers to Solve Inherently Serial Problems</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> A theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness is provided and an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision is shown.</p>
                <p><strong>Paper Abstract:</strong> Instructing the model to generate a sequence of intermediate steps, a.k.a., a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks. However, the mechanism behind CoT remains unclear. This work provides a theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness. Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. Given input length $n$, previous works have shown that constant-depth transformers with finite precision $\mathsf{poly}(n)$ embedding size can only solve problems in $\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in $\mathsf{AC}^0$, a proper subset of $ \mathsf{TC}^0$. However, with $T$ steps of CoT, constant-depth transformers using constant-bit precision and $O(\log n)$ embedding size can solve any problem solvable by boolean circuits of size $T$. Empirically, enabling CoT dramatically improves the accuracy for tasks that are hard for parallel computation, including the composition of permutation groups, iterated squaring, and circuit value problems, especially for low-depth transformers.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6642.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6642.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ModularAddition_Cp</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modular addition task (C_p)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Synthetic sequence task where tokens are integers in 0..p-1 and the label is the sum modulo p; evaluated with decoder-only transformers trained from scratch under three settings (base, hint, cot). CoT provides partial modular sums as intermediate tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>decoder-only transformer (trained from scratch, nanoGPT codebase)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not reported (embedding size 512, 8 attention heads, depth varied across experiments; total parameter count not reported)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Synthetic data: for each sequence length n, inputs x1..x_{n-1} sampled uniformly from {0..p-1}, xn set to '='; fresh sampled batches of size 64 each training step; training budget 1e6 steps; float16 arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Modular Addition (C_p)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>addition modulo p</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>symbolic token sequence; final token '=' marks output; in CoT setting the chain-of-thought is a sequence of running partial sums (tokens) appended after the input</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>easy / parallelizable (belongs to TC^0); sequence length varied but experiments in paper not extremely long)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>three supervised settings: base (answer-only), hint (per-position supervision using CoT as labels aligned to inputs), cot (CoT tokens appended to input as intermediate tokens; model trained to predict sequence autoregressively)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>training/validation accuracy (exact match on final label or probability of predicting all CoT tokens in cot setting)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>high training accuracy reported for modular addition; low-depth transformers with hint can solve well for reasonable input lengths; CoT further improves performance especially for long sequences (no single numeric aggregate provided)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper provides theoretical context: modular addition is in TC^0 and thus parallelizable; constant-depth transformers with sufficient precision/width can implement such parallel computations without CoT. Finite-precision rounding considerations discussed (iterated rounded addition non-associative), but for the modular addition experiments float16 behaved like log-precision for the sequence lengths tested.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not highlighted as a failure case for this task; main caveat is finite-precision/sequence length interplay—associativity loss of rounded addition can limit scaling if precision/exponent are too small.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Task is parallelizable; performance is robust for low-depth models when precision/width suffice. CoT still improves performance for longer sequences where serialization helps, but modular addition does not require CoT theoretically.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Thought Empowers Transformers to Solve Inherently Serial Problems', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6642.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6642.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PermutationComposition_S5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Permutation composition task over S_p (notably S_5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Task: compose a sequence of permutations (each represented as a block of tokens) and output the resulting permutation; designed instance uses p=5 where the problem family is conjectured to be outside AC^0/TC^0 and thus hard for shallow parallel models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>decoder-only transformer (trained from scratch, nanoGPT codebase)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not reported (embedding size 512, 8 attention heads, depth varied; parameter count not reported)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Synthetic data: sequences encoding m random permutations over [p] with parentheses and composition; final token '='; CoT is partial compositions (intermediate composed permutations) appended as tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Permutation composition (S_p), reported for p=5</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>permutation composition / group word problem</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>symbolic token sequence representing permutations (blocks) and parentheses; final '=' token; CoT provides partial compositions (strings) after the input</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>inherently serial / conjectured hard for constant-depth parallel models (S_5 composition is NC^1-complete and believed outside TC^0 for p >= 5)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>base, hint, and cot supervised settings; CoT setting supplies the intermediate partial compositions as appended tokens</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>training/validation accuracy (exact match on final permutation); for cot measured as probability of predicting all provided CoT tokens correctly (lower bound for final-answer accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>without CoT accuracy ~20% (reported as 'for most time the accuracy without CoT is ~20%', essentially random guessing over 5 outputs); with CoT accuracy is substantially higher (figures indicate large improvement though exact percentage numbers are not specified in text)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Theoretical results (Theorem 3.3 and Corollary 3.4) explain why CoT helps: CoT steps let a constant-depth transformer simulate serial gate computations and thus can recognize regular languages (including S_5 word problem) when CoT length is linear; conversely, constant-depth transformers without CoT (T[poly(n), log n]) are bounded by TC^0 and cannot compute S_5 composition under standard complexity assumptions. The paper links this to finite-precision issues and the need for O(log n) embedding to encode gate/position ids.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Shallow transformers without CoT fail catastrophically on this inherently serial task, often performing at chance (~20% for p=5); increasing depth (serial computation budget) can help but is inefficient.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Providing CoT (increasing number of autoregressive intermediate steps) dramatically improves performance; theoretical scaling: T CoT steps allow simulation of circuits of size T, polynomial T gives P/poly; embedding size needs to be O(log n) to store positional/gate ids.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Thought Empowers Transformers to Solve Inherently Serial Problems', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6642.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6642.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IteratedSquaring_IS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterated squaring task (modular exponentiation sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Task: given base r and modulus p, and a sequence of squaring operations (symbol '^2' repeated), compute r^{2^{m}} mod p; CoT/hints are the intermediate residues after each squaring step.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>decoder-only transformer (trained from scratch, nanoGPT codebase)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not reported (embedding size 512, 8 attention heads, depth varied; parameter count not reported)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Synthetic data: tokens include numeric symbols for residues (0..T-1), '^2' tokens repeated up to m times, final '=' token; uniform random generation over allowed ranges; training uses fresh sampled batches each step in online supervised fashion.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Iterated squaring (IS)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>iterated modular squaring (modular exponentiation by repeated squaring)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>symbolic token sequence: (p, r, ^2, ^2, ..., =) where ^2 tokens indicate repeated squaring steps; CoT provides the sequence of intermediate residues (f_{r,p}(i))</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>inherently serial / conjectured cryptographic hardness (no faster parallel algorithm than sequential squarings under technical assumptions)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>base, hint, and cot; in cot/hint the intermediate residues are provided as tokens/labels respectively; cot appends the chain of computed residues</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>training/validation accuracy (exact match on final residue or probability of predicting full CoT sequence in cot setting)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Without CoT the problem is difficult for transformers to learn; with CoT the authors state the problem 'can be perfectly expressed with CoT even with depth 1' — reported as near-perfect / perfect training accuracy in cot setting (no precise numeric reported).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper frames iterated squaring as inherently serial and gives complexity-based justification: CoT provides the serial steps required to compute sequential squaring; theoretical results show CoT steps can be used to simulate gate-by-gate computation, enabling representation of serial algorithms even in constant-depth models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Without CoT, transformers fail to learn the sequential structure and cannot perform the repeated squaring reliably; providing hints (per-step labels) helps but CoT (autoregressive intermediate tokens) gives the largest empirical improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Depth 1 with CoT suffices empirically; increasing CoT length (exposing intermediate computation steps) enables exact computation even with very low model depth. Conversely, without CoT increasing depth is required to approximate serial computation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Thought Empowers Transformers to Solve Inherently Serial Problems', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6642.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6642.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CircuitValue_CVP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Circuit Value Problem (CVP) task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Given an explicit Boolean circuit encoded as a sequence of tokens (each gate as four tokens: type, two input ids, current gate id), the task is to compute the value of the final gate; CoT/hints provide intermediate gate values for each gate in topological order.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>decoder-only transformer (trained from scratch, nanoGPT codebase)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not reported (embedding size 512, 8 attention heads, depth varied; parameter count not reported)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Synthetic data: randomly generated circuits with m gates sorted topologically; each gate is represented as four tokens (type, input ids, current id); CoT/hints include per-gate tokenizations of input values and gate value; trained online with fresh samples per step.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Circuit Value Problem (CVP)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>computing output of a Boolean circuit (P-complete problem)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>symbolic token sequence encoding circuit structure and gate identifiers; final token '='; CoT contains 4 tokens per gate with gate type, input gate values, and gate value.</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>inherently serial / P-complete (computing the value of an arbitrary circuit is P-complete under AC^0 reductions)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>base, hint, and cot supervised settings; cot supplies the sequence of gate evaluations as intermediate tokens</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>training/validation accuracy on final gate value (exact match) and probability of correctly predicting full CoT tokens in cot setting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Empirically CoT substantially improves accuracy on CVP for low-depth transformers; exact numeric accuracies not provided in the text, but figures indicate large gains and the paper emphasizes that without CoT shallow models cannot solve CVP reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Theoretical analysis: CVP is P-complete, and Theorem 3.3 shows that CoT steps allow a constant-depth transformer to simulate a circuit gate-by-gate (each CoT step simulates one gate), hence with polynomial CoT steps the transformer can represent P/poly computations. The paper also studies finite-precision effects and embedding-size requirements (O(log n) to encode gate ids).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Shallow transformers without CoT fail on CVP and cannot simulate the serial gate-evaluation order; simply providing labels (hint setting) helps but CoT (autoregressive intermediate tokens) yields the most dramatic improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Increasing number of CoT steps converts a constant-depth transformer into a serial procedural simulator—T CoT steps enable simulation of size-T circuits; polynomial CoT implies P/poly expressiveness. Depth alone bounds the serial computation the model can do without CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Thought Empowers Transformers to Solve Inherently Serial Problems', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Transformers learn shortcuts to automata <em>(Rating: 2)</em></li>
                <li>The parallelism tradeoff: Limitations of log-precision transformers <em>(Rating: 2)</em></li>
                <li>What every computer scientist should know about floating-point arithmetic <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6642",
    "paper_id": "paper-5a20aa49b81b4e14fdb36814e557b3da60259ce9",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "ModularAddition_Cp",
            "name_full": "Modular addition task (C_p)",
            "brief_description": "Synthetic sequence task where tokens are integers in 0..p-1 and the label is the sum modulo p; evaluated with decoder-only transformers trained from scratch under three settings (base, hint, cot). CoT provides partial modular sums as intermediate tokens.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "decoder-only transformer (trained from scratch, nanoGPT codebase)",
            "model_family": "decoder-only transformer",
            "model_size": "not reported (embedding size 512, 8 attention heads, depth varied across experiments; total parameter count not reported)",
            "training_data_description": "Synthetic data: for each sequence length n, inputs x1..x_{n-1} sampled uniformly from {0..p-1}, xn set to '='; fresh sampled batches of size 64 each training step; training budget 1e6 steps; float16 arithmetic.",
            "benchmark_name": "Modular Addition (C_p)",
            "task_type": "addition modulo p",
            "problem_format": "symbolic token sequence; final token '=' marks output; in CoT setting the chain-of-thought is a sequence of running partial sums (tokens) appended after the input",
            "difficulty_level": "easy / parallelizable (belongs to TC^0); sequence length varied but experiments in paper not extremely long)",
            "prompting_method": "three supervised settings: base (answer-only), hint (per-position supervision using CoT as labels aligned to inputs), cot (CoT tokens appended to input as intermediate tokens; model trained to predict sequence autoregressively)",
            "performance_metric": "training/validation accuracy (exact match on final label or probability of predicting all CoT tokens in cot setting)",
            "performance_value": "high training accuracy reported for modular addition; low-depth transformers with hint can solve well for reasonable input lengths; CoT further improves performance especially for long sequences (no single numeric aggregate provided)",
            "internal_analysis": "Paper provides theoretical context: modular addition is in TC^0 and thus parallelizable; constant-depth transformers with sufficient precision/width can implement such parallel computations without CoT. Finite-precision rounding considerations discussed (iterated rounded addition non-associative), but for the modular addition experiments float16 behaved like log-precision for the sequence lengths tested.",
            "failure_modes": "Not highlighted as a failure case for this task; main caveat is finite-precision/sequence length interplay—associativity loss of rounded addition can limit scaling if precision/exponent are too small.",
            "scaling_trend": "Task is parallelizable; performance is robust for low-depth models when precision/width suffice. CoT still improves performance for longer sequences where serialization helps, but modular addition does not require CoT theoretically.",
            "uuid": "e6642.0",
            "source_info": {
                "paper_title": "Chain of Thought Empowers Transformers to Solve Inherently Serial Problems",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "PermutationComposition_S5",
            "name_full": "Permutation composition task over S_p (notably S_5)",
            "brief_description": "Task: compose a sequence of permutations (each represented as a block of tokens) and output the resulting permutation; designed instance uses p=5 where the problem family is conjectured to be outside AC^0/TC^0 and thus hard for shallow parallel models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "decoder-only transformer (trained from scratch, nanoGPT codebase)",
            "model_family": "decoder-only transformer",
            "model_size": "not reported (embedding size 512, 8 attention heads, depth varied; parameter count not reported)",
            "training_data_description": "Synthetic data: sequences encoding m random permutations over [p] with parentheses and composition; final token '='; CoT is partial compositions (intermediate composed permutations) appended as tokens.",
            "benchmark_name": "Permutation composition (S_p), reported for p=5",
            "task_type": "permutation composition / group word problem",
            "problem_format": "symbolic token sequence representing permutations (blocks) and parentheses; final '=' token; CoT provides partial compositions (strings) after the input",
            "difficulty_level": "inherently serial / conjectured hard for constant-depth parallel models (S_5 composition is NC^1-complete and believed outside TC^0 for p &gt;= 5)",
            "prompting_method": "base, hint, and cot supervised settings; CoT setting supplies the intermediate partial compositions as appended tokens",
            "performance_metric": "training/validation accuracy (exact match on final permutation); for cot measured as probability of predicting all provided CoT tokens correctly (lower bound for final-answer accuracy)",
            "performance_value": "without CoT accuracy ~20% (reported as 'for most time the accuracy without CoT is ~20%', essentially random guessing over 5 outputs); with CoT accuracy is substantially higher (figures indicate large improvement though exact percentage numbers are not specified in text)",
            "internal_analysis": "Theoretical results (Theorem 3.3 and Corollary 3.4) explain why CoT helps: CoT steps let a constant-depth transformer simulate serial gate computations and thus can recognize regular languages (including S_5 word problem) when CoT length is linear; conversely, constant-depth transformers without CoT (T[poly(n), log n]) are bounded by TC^0 and cannot compute S_5 composition under standard complexity assumptions. The paper links this to finite-precision issues and the need for O(log n) embedding to encode gate/position ids.",
            "failure_modes": "Shallow transformers without CoT fail catastrophically on this inherently serial task, often performing at chance (~20% for p=5); increasing depth (serial computation budget) can help but is inefficient.",
            "scaling_trend": "Providing CoT (increasing number of autoregressive intermediate steps) dramatically improves performance; theoretical scaling: T CoT steps allow simulation of circuits of size T, polynomial T gives P/poly; embedding size needs to be O(log n) to store positional/gate ids.",
            "uuid": "e6642.1",
            "source_info": {
                "paper_title": "Chain of Thought Empowers Transformers to Solve Inherently Serial Problems",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "IteratedSquaring_IS",
            "name_full": "Iterated squaring task (modular exponentiation sequence)",
            "brief_description": "Task: given base r and modulus p, and a sequence of squaring operations (symbol '^2' repeated), compute r^{2^{m}} mod p; CoT/hints are the intermediate residues after each squaring step.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "decoder-only transformer (trained from scratch, nanoGPT codebase)",
            "model_family": "decoder-only transformer",
            "model_size": "not reported (embedding size 512, 8 attention heads, depth varied; parameter count not reported)",
            "training_data_description": "Synthetic data: tokens include numeric symbols for residues (0..T-1), '^2' tokens repeated up to m times, final '=' token; uniform random generation over allowed ranges; training uses fresh sampled batches each step in online supervised fashion.",
            "benchmark_name": "Iterated squaring (IS)",
            "task_type": "iterated modular squaring (modular exponentiation by repeated squaring)",
            "problem_format": "symbolic token sequence: (p, r, ^2, ^2, ..., =) where ^2 tokens indicate repeated squaring steps; CoT provides the sequence of intermediate residues (f_{r,p}(i))",
            "difficulty_level": "inherently serial / conjectured cryptographic hardness (no faster parallel algorithm than sequential squarings under technical assumptions)",
            "prompting_method": "base, hint, and cot; in cot/hint the intermediate residues are provided as tokens/labels respectively; cot appends the chain of computed residues",
            "performance_metric": "training/validation accuracy (exact match on final residue or probability of predicting full CoT sequence in cot setting)",
            "performance_value": "Without CoT the problem is difficult for transformers to learn; with CoT the authors state the problem 'can be perfectly expressed with CoT even with depth 1' — reported as near-perfect / perfect training accuracy in cot setting (no precise numeric reported).",
            "internal_analysis": "Paper frames iterated squaring as inherently serial and gives complexity-based justification: CoT provides the serial steps required to compute sequential squaring; theoretical results show CoT steps can be used to simulate gate-by-gate computation, enabling representation of serial algorithms even in constant-depth models.",
            "failure_modes": "Without CoT, transformers fail to learn the sequential structure and cannot perform the repeated squaring reliably; providing hints (per-step labels) helps but CoT (autoregressive intermediate tokens) gives the largest empirical improvement.",
            "scaling_trend": "Depth 1 with CoT suffices empirically; increasing CoT length (exposing intermediate computation steps) enables exact computation even with very low model depth. Conversely, without CoT increasing depth is required to approximate serial computation.",
            "uuid": "e6642.2",
            "source_info": {
                "paper_title": "Chain of Thought Empowers Transformers to Solve Inherently Serial Problems",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "CircuitValue_CVP",
            "name_full": "Circuit Value Problem (CVP) task",
            "brief_description": "Given an explicit Boolean circuit encoded as a sequence of tokens (each gate as four tokens: type, two input ids, current gate id), the task is to compute the value of the final gate; CoT/hints provide intermediate gate values for each gate in topological order.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "decoder-only transformer (trained from scratch, nanoGPT codebase)",
            "model_family": "decoder-only transformer",
            "model_size": "not reported (embedding size 512, 8 attention heads, depth varied; parameter count not reported)",
            "training_data_description": "Synthetic data: randomly generated circuits with m gates sorted topologically; each gate is represented as four tokens (type, input ids, current id); CoT/hints include per-gate tokenizations of input values and gate value; trained online with fresh samples per step.",
            "benchmark_name": "Circuit Value Problem (CVP)",
            "task_type": "computing output of a Boolean circuit (P-complete problem)",
            "problem_format": "symbolic token sequence encoding circuit structure and gate identifiers; final token '='; CoT contains 4 tokens per gate with gate type, input gate values, and gate value.",
            "difficulty_level": "inherently serial / P-complete (computing the value of an arbitrary circuit is P-complete under AC^0 reductions)",
            "prompting_method": "base, hint, and cot supervised settings; cot supplies the sequence of gate evaluations as intermediate tokens",
            "performance_metric": "training/validation accuracy on final gate value (exact match) and probability of correctly predicting full CoT tokens in cot setting",
            "performance_value": "Empirically CoT substantially improves accuracy on CVP for low-depth transformers; exact numeric accuracies not provided in the text, but figures indicate large gains and the paper emphasizes that without CoT shallow models cannot solve CVP reliably.",
            "internal_analysis": "Theoretical analysis: CVP is P-complete, and Theorem 3.3 shows that CoT steps allow a constant-depth transformer to simulate a circuit gate-by-gate (each CoT step simulates one gate), hence with polynomial CoT steps the transformer can represent P/poly computations. The paper also studies finite-precision effects and embedding-size requirements (O(log n) to encode gate ids).",
            "failure_modes": "Shallow transformers without CoT fail on CVP and cannot simulate the serial gate-evaluation order; simply providing labels (hint setting) helps but CoT (autoregressive intermediate tokens) yields the most dramatic improvement.",
            "scaling_trend": "Increasing number of CoT steps converts a constant-depth transformer into a serial procedural simulator—T CoT steps enable simulation of size-T circuits; polynomial CoT implies P/poly expressiveness. Depth alone bounds the serial computation the model can do without CoT.",
            "uuid": "e6642.3",
            "source_info": {
                "paper_title": "Chain of Thought Empowers Transformers to Solve Inherently Serial Problems",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "Transformers learn shortcuts to automata",
            "rating": 2
        },
        {
            "paper_title": "The parallelism tradeoff: Limitations of log-precision transformers",
            "rating": 2
        },
        {
            "paper_title": "What every computer scientist should know about floating-point arithmetic",
            "rating": 1
        }
    ],
    "cost": 0.014672,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Chain of Thought Empowers Transformers to Solve Inherently Serial Problems</h1>
<p>Zhiyuan $\mathrm{Li}^{1,2}$, Hong Liu ${ }^{1}$, Denny Zhou ${ }^{3}$, and Tengyu Ma ${ }^{1}$<br>${ }^{1}$ Stanford University, ${ }^{2}$ Toyota Technological Institute at Chicago, ${ }^{3}$ Google</p>
<h4>Abstract</h4>
<p>Instructing the model to generate a sequence of intermediate steps, a.k.a., a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks. However, the mechanism behind CoT remains unclear. This work provides a theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness. Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. Given input length $n$, previous works have shown that constantdepth transformers with finite precision poly $(n)$ embedding size can only solve problems in $\mathrm{TC}^{0}$ without CoT. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in $\mathrm{AC}^{0}$, a proper subset of $\mathrm{TC}^{0}$. However, with $T$ steps of CoT, constant-depth transformers using constant-bit precision and $O(\log n)$ embedding size can solve any problem solvable by boolean circuits of size $T$. Empirically, enabling CoT dramatically improves the accuracy for tasks that are hard for parallel computation, including the composition of permutation groups, iterated squaring, and circuit value problems, especially for low-depth transformers.</p>
<h2>1 Introduction</h2>
<p>Large Language Models (LLMs) exhibit exceptional capabilities in complex reasoning tasks such as mathematical problem-solving and code generation (Chowdhery et al., 2023; Anil et al., 2023; Achiam et al., 2023; Romera-Paredes et al., 2023; Trinh et al., 2024), far surpassing standard supervised machine learning techniques. The key to unlocking these advanced reasoning abilities lies in enabling LLMs to generate intermediate steps, or a chain of thought (CoT), before finalizing the final answer. This can be achieved through various methods, including training or instruction tuning a model with examples enriched with intermediate steps (Ling et al., 2017; Cobbe et al., 2021; Nye et al., 2021; Chung et al., 2022), or through few-shot CoT prompting (Reynolds \&amp; McDonell, 2021; Nye et al., 2021; Wei et al., 2022).</p>
<p>A natural explanation is that the intermediate steps provide extra information about the tasks and efficient approaches to solving, so that a model can imitate. However, intriguingly, the efficacy of generating thought steps extends to zero-shot CoT prompting (Kojima et al., 2022), where LLMs are only instructed with the prompt "let's think step by step", and to even using incorrect reasoning steps in the few-shot examples (Wang et al., 2022a; Madaan \&amp; Yazdanbakhsh, 2022). These observations suggest that the form of CoT prompting is as important as (if not more important than) its content, because merely instructing LLMs to generate the intermediate steps helps.</p>
<p>This paper aims to study why the form of CoT improves the reasoning capability of LLMs. Our hypothesis is that CoT allows for performing more serial computations that a vanilla transformer cannot do without CoT. We formulate and analyze this hypothesis through the lens of expressiveness with and without CoT. We adopt the language of circuit complexity to discuss the capability of transformers. Previous works (Liu et al., 2022b; Merrill \&amp; Sabharwal, 2023b) have shown standard decoder-only transformers (that output answers directly) are efficient parallel computers and can only express functions computable in an $O(1)$-parallel run-time with threshold circuits, $\mathrm{TC}^{0}$, a computational model that allows the AND, OR, NOT and MAJORITY function with multiple inputs to be computed efficiently in parallel. We first show a tighter upper bound (Theorem 3.1) for expressiveness of constant-precision transformer - it can only express a proper subset class of $\mathrm{TC}^{0}$, $\mathrm{AC}^{0}$, where MAJORITY gates are not allowed. Our upper bound is also more realistic because it handles the rounding issue or iterative addition of floating point numbers, while most previous results essentially only work for fixed-point number addition.</p>
<p>We then show that transformers equipped with CoT -allowing the transformer to auto-regressively generate a sequence of intermediate tokens before answering the questions-can solve complex problems that inherently require serial computations (assuming well-known conjectures in complexity theory). Intuitively, without CoT, the number of serial computations conducted by the transformer is bounded by the depth (which is considered as a fixed constant for this work), whereas with $T$ intermediate steps, the number of serial computations possible is boosted to $T$. Note that $T$ can easily increase as the sequence length increases where the depth is a fixed number that depends on the architecture.</p>
<p>Concretely, we prove that a constant-precision transformer with $T$ intermediate steps and embedding dimension logarithmic in the sequence length can express any functions computable by a circuit of size $T$ in Theorem 3.3. Taking $T$ to be polynomial in the sequence length, the result suggests that transformers with polynomially many intermediate steps are capable of computing all circuits in with polynomial size, $\mathrm{P} /$ poly, a superclass of P . Theorem 3.3 also implies that transformers with linearly many intermediate steps can compute all regular languages, including composition of non-solvable groups, like permutation group over five elements, $S_{5}$, which does not belong to $\mathrm{AC}^{0}$ and is also widely conjectured to be out of $\mathrm{TC}^{0}$. As such, polynomially many CoT steps makes transformers with bounded depth and precision strictly more powerful. We define the problem class that transformers can solve with a certain amount of CoT steps formally in Definition 3.4 and summarize our theoretical results in Figure 1. Interestingly, we also show that logarithmically many CoT steps do not allow the transformer to compute functions beyond $\mathrm{AC}^{0}$. (Theorem 3.1)</p>
<p>To corroborate our theoretical analysis, we empirically evaluate the capability of transformers in solving four core problems: modular addition, permutation composition, iterated squaring, and circuit value problem. We learn transformers to solve these tasks with a large amount of synthetic data, with and without CoT, or with additional hint but not CoT. The modular addition belongs to $\mathrm{TC}^{0}$, meaning it can be easily solved in parallel. Liu et al. (2022a) shows it is solvable by constantdepth transformers with log-precision and, indeed empirically depth 1 is sufficient for the parity problem (Modulo 2 addition). The other three tasks are all conjectured to require inherently serial computations. As expected, the vanilla transformer either requires a huge depth to solve these tasks (because the depth is the upper bound on the number of serial computation by transformers), or cannot solve the tasks at all. On the other hand, CoT can solve these tasks as long as the depth exceeds a small threshold. These experiments demonstrate CoT can provide more serial computations to solve complex reasoning tasks.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Relationship diagram between cotcomplexity class with different embedding sizes $d(n)$ and CoT lengths $T(n)$. We fix the precision to be constant (the above diagram holds with or without constantly many exponent bits) and omit them in the notation for simplicity. The diagram for log precision is similar (with $\mathrm{AC}^{0}$ replaced by $\mathrm{TC}^{0}$ ), and is thus deferred to the appendix, Figure 10.</p>
<h1>2 Notations and Preliminaries</h1>
<p>We use $\mathbb{N}$ and $\mathbb{R}$ to denote the set of natural numbers and real numbers respectively. For any $n \in \mathbb{N}^{+}$, we define $[n] \triangleq{1,2, \ldots, n}$. We define relu $(x) \triangleq \max (x, 0)$. For vector $x$, we use $x_{a: b}$ to denote the vector containing coordinates of $x$ from position $a$ to position $b$. For matrix $M$, we define $M_{a_{1}: b_{1}, a_{2}: b_{2}}$ to denote the submatrix by selecting rows from $a_{1}$ to $b_{1}$, columns from $a_{2}$ to $b_{2}$. We also use $a_{1}$ : to denote the subset of indices from $a_{1}$ to the end, : $b_{1}$ to denote the subset of indices from the beginning (1) to $b_{1}$ and : to denote all indices. Given two non-negative functions $f, g$, we say $f(n)=O(g(n))$ (resp. $f(n)=\Omega(g(n))$ ) iff there exists $C&gt;0$, such that for all $n \geq 0$, $f(n) \leq C g(n)$ (resp. $f(n) \geq C g(n)$ ). We use poly $(n) \triangleq\left{T: \mathbb{N} \rightarrow \mathbb{N} \mid \exists k&gt;0, T(n)=O\left(n^{k}\right)\right}$ to denote the set of functions with at most polynomial growth rate.</p>
<p>We use $\phi(x)=\sum_{i=1}^{|x|} 2^{|x|-i} x_{i}$ to denote the value of binary number represented by binary string $x$. We use $\operatorname{bin}<em k="k">{k}(x)$ to denote the usual binary encoding of natural number $x$ using $k$ binary bits in the sense that $\phi\left(\operatorname{bin}</em>}(x)\right)=x$ and $\operatorname{spin<em k="k">{k}(x)$ to denote the signed binary encoding, which is $2 \operatorname{bin}</em>(x))}(x)-(1, \ldots, 1)$. For any $n \in \mathbb{N}^{+}$, we define softmax : $\mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ as $(\operatorname{softmax<em i="i">{i}=$ $\exp \left(x</em>$ and $i \in[n]$. We use $\odot$ to denote the element-wise product of two vectors. We use $a | b$ or $(a, b)$ to denote the concatenation of two vectors $a$ and $b$.}\right) / \sum_{i=1}^{n} \exp \left(x_{i}\right)$ for any $x \in \mathbb{R}^{n</p>
<h3>2.1 Decoder-only Transformers</h3>
<p>Given a vocabulary $\mathcal{V}$, a decoder-only transformer with parameter $\theta$ and maximal input length $n_{\text {max }}$ maps a sequence of input tokens $\left(x_{1}, \ldots, x_{n}\right) \in \mathcal{V}^{n}$ to a probability distribution over $\mathcal{V}$ for all $n \leq n_{\max }$, denoted by $p_{\theta}\left(\cdot \mid x_{1}, \ldots, x_{n}\right)$. We also define function $\operatorname{TF}<em _theta="\theta">{\theta}(x)$ by the token in $\mathcal{V}$ that $\operatorname{maximizes} p</em>}\left(\cdot \mid x_{1}, \ldots, x_{n}\right)$, that is, $\operatorname{TF<em 1="1">{\theta}\left(x</em>\right) \triangleq \arg \max }, \ldots, x_{n<em _theta="\theta">{y \in \mathcal{V}} p</em>\right)$.}\left(y \mid x_{1}, \ldots, x_{n</p>
<p>Next-token Generator: Given a vocabulary $\mathcal{V}$, a next-token generator with parameter $\theta$ and maximal input length $n_{\max }$ is a mapping from $\cup_{n=1}^{n_{\max }} \mathcal{V}^{n}$ to $\mathcal{V}$. The main next-token generator we are interested in this work is decoder-only transformers, $\operatorname{TF}<em 1="1">{\theta}\left(x</em>}, \ldots, x_{n}\right)$ where $x_{i} \in \mathcal{V}$ for all $i \in$ $[n]$. We also recursively define $\operatorname{TF<em 1="1">{\theta}^{i}\left(x</em>}, \ldots, x_{n}\right) \triangleq \operatorname{TF<em 1="1">{\theta}^{i-1}\left(x</em>}, \ldots, x_{n}, \operatorname{TF<em 1="1">{\theta}\left(x</em>\right)\right)$, for every}, \ldots, x_{n</p>
<p>positive integer $i$ and $n$ satisfying that $i+n \leq n_{\max }-1$ with the base case that $\operatorname{TF}<em 1="1">{\theta}^{1}\left(x</em>}, \ldots, x_{n}\right) \triangleq$ $\mathrm{TF<em 1="1">{\theta}\left(x</em>}, \ldots, x_{n}\right)$. In other words, for all $0 \leq i \leq n_{\max }-n-1$, the output with $i$ steps of CoT is $x_{n+i+1}=\operatorname{TF<em 1="1">{\theta}^{i+1}\left(x</em>}, \ldots, x_{n}\right)=\operatorname{TF<em 1="1">{\theta}\left(x</em>\right)$.}, \ldots, x_{n}, x_{n+1}, \ldots, x_{n+i</p>
<p>Transformer Architecture Overview: The decoder-only transformer model we consider in this paper is very similar to GPT style architectures (Radford et al., 2019) and consists of four parts: a token embedding layer (TE), a position encoding layer (PE), an output linear layer (OUTPUT), and a stack of $L$ identical layers serving as the "decoder" where $L$ is also called the depth of the model. Each decoder layer has two sub-layers: a multi-head self-attention layer (ATTN) and a position-wise fullyconnected feed-forward network (FF). Each layer mentioned above has its own trainable parameters and is indexed by the layer name and the depth for attention and feedforward layers. ${ }^{1}$ That is we can split the model parameter $\theta$ in the following way: $\theta=\left(\theta_{\mathrm{PE}}, \theta_{\mathrm{TE}}, \theta_{\text {OUTPUT }},\left{\theta_{\mathrm{ATTN}}^{(l)}, \theta_{\mathrm{FF}}^{(l)}\right}_{l=0}^{L-1}\right)$, which are all trainable. (See formal definition in Algorithm 2). Throughout this paper, we use $d$ to denote the embedding size of a transformer.</p>
<p>Self-Attention Mechanism: Given attention parameter $\theta_{\mathrm{ATTN}}=\left(W_{Q}, W_{K}, W_{V}, W_{O}\right) \in$ $\mathbb{R}^{d \times d} \times \mathbb{R}^{d \times d} \times \mathbb{R}^{d \times d} \times \mathbb{R}^{d \times d}$, we define the Attention layer with mask for decoder-only transformer in Algorithm 3. Note allowing multi-head attention will not change the class of problems solvable by constant layer decoder-only transformers as we can simulate 1 multi-head attention layer with any constantly many heads with multiple single-head attention layers. Thus for simplicity of presentation, we do not include multi-head attention in the definition below.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">Causal</span><span class="w"> </span><span class="k">Self</span><span class="o">-</span><span class="nx">Attention</span><span class="p">,</span><span class="w"> </span><span class="nx">ATTN</span>
</code></pre></div>

<p>Input: Parameter $\theta_{\mathrm{ATTN}}=\left(W_{Q}, W_{K}, W_{V}, W_{O}\right)$, Input embedding $h=\left(h_{1}, \ldots, h_{n}\right) \in \mathbb{R}^{n d}$.
Output: Output embedding $h^{\prime}=\left(h_{1}^{\prime}, \ldots, h_{n}^{\prime}\right) \triangleq \mathrm{ATTN}<em _mathrm_ATTN="\mathrm{ATTN">{\theta</em>\right)$.
$\begin{array}{ll}\text { 1: } q_{i} \triangleq W_{Q} h_{i}, k_{i} \triangleq W_{K} h_{i}, v_{i} \triangleq W_{V} h_{i}, \forall i \in[n] \ \text { 2: } s_{i} \triangleq \operatorname{softmax}\left(\left\langle q_{i}, k_{1}\right\rangle, \ldots,\left\langle q_{i}, k_{i}\right\rangle\right) |(0, \ldots, 0) . \ \text { 3: } h_{i}^{\prime} \triangleq W_{O} \sum_{j=1}^{n}\left(s_{i}\right)}}}\left(h_{1}, \ldots, h_{n<em j="j">{j} v</em>$} .\end{array</p>
<p>Feed-Forward Network: Given the parameter of fully-connected feedforward network layer $\theta_{\mathrm{FF}}=\left(W_{1}, b_{1}, W_{2}, b_{2}\right) \in \mathbb{R}^{d \times d} \times \mathbb{R}^{d} \times \mathbb{R}^{d \times d} \times \mathbb{R}^{d}$, we define the fully-connected feedforward layer $\mathrm{FF}<em _mathrm_FF="\mathrm{FF">{\theta</em>}}}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$ as $\mathrm{FF<em _mathrm_FF="\mathrm{FF">{\theta</em>$.}}}(h) \triangleq W_{2} \operatorname{relu}\left(W_{1} h+b_{1}\right)+b_{2</p>
<p>Token Embedding: Given the parameter of token embedding layer $\theta_{\mathrm{TE}} \in \mathbb{R}^{d \times|\mathcal{V}|}$, we define the token embedding layer by viewing $\theta_{\mathrm{TE}}$ as a mapping from $\mathcal{V}$ to $\mathbb{R}^{d}$, that is, for all $x \in \mathcal{V}$, the token embedding is $\theta_{\mathrm{TE}}(x)$.</p>
<p>Position Encoding: Given the parameter of position encoding layer $\theta_{\mathrm{PE}} \in \mathbb{R}^{d \times n_{\max }}$, we define the token embedding layer by viewing $\theta_{\mathrm{PE}}$ as a mapping from $\left[n_{\max }\right]$ to $\mathbb{R}^{d}$ that is, for all $n \in\left[n_{\max }\right]$, the position embedding is as $\theta_{\mathrm{PE}}(n)$.</p>
<p>Output Layer: Given the parameter of output layer $\theta_{\text {OUTPUT }} \in \mathbb{R}^{|\mathcal{V}| \times d}$, we define the output layer $\operatorname{OUTPUT}<em _OUTPUT="{OUTPUT" _text="\text">{\theta</em>}}}: \mathbb{R}^{d} \rightarrow \mathcal{V}$ as $\operatorname{OUTPUT<em _OUTPUT="{OUTPUT" _text="\text">{\theta</em>$.}}}(h) \triangleq \operatorname{softmax}\left(\theta_{\text {OUTPUT }} h\right)$ for all $h \in \mathbb{R}^{d</p>
<h1>2.2 Circuit Complexity</h1>
<p>Problem. In this paper we consider the following notion of problems: given a sequence of input tokens, output a token as the answer. Mathematically, given a vocabulary $\mathcal{V}$, we call a mapping $\mathcal{L}: \cup_{k \in \mathbb{N}^{+}} \mathcal{V}^{k} \rightarrow \mathcal{V}$ a problem. If the correct answer is always 0 or 1 , we call $\mathcal{L}$ a decision problem. In circuit complexity, such $\mathcal{L}$ is also called a language.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="n">Decoder</span><span class="o">-</span><span class="k">only</span><span class="w"> </span><span class="n">Transformer</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">TF</span><span class="err">}</span><span class="n">_</span><span class="err">{\</span><span class="n">theta</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">p_</span><span class="err">{\</span><span class="n">theta</span><span class="err">}\</span><span class="p">)</span>
<span class="k">Input</span><span class="err">:</span><span class="w"> </span><span class="n">Transformer</span><span class="w"> </span><span class="k">parameter</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">theta</span><span class="o">=</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="err">\</span><span class="n">theta_</span><span class="err">{\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">PE</span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">theta_</span><span class="err">{\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">TE</span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">theta_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="k">OUTPUT</span><span class="w"> </span><span class="err">}}</span><span class="p">,</span><span class="err">\</span><span class="nf">left</span><span class="err">\{\</span><span class="n">theta_</span><span class="err">{\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">ATTN</span><span class="err">}}</span><span class="o">^</span><span class="err">{</span><span class="p">(</span><span class="n">l</span><span class="p">)</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">theta_</span><span class="err">{\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">FF</span><span class="err">}}</span><span class="o">^</span><span class="err">{</span><span class="p">(</span><span class="n">l</span><span class="p">)</span><span class="err">}\</span><span class="nf">right</span><span class="err">\}</span><span class="n">_</span><span class="err">{</span><span class="n">l</span><span class="o">=</span><span class="mi">0</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">L</span><span class="o">-</span><span class="mi">1</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="k">input</span><span class="w"> </span><span class="n">tokens</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="mi">1</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">x_</span><span class="err">{</span><span class="n">n</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="ow">in</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">V</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">n</span><span class="err">}\</span><span class="p">).</span>
<span class="k">Output</span><span class="err">:</span><span class="w"> </span><span class="k">Output</span><span class="w"> </span><span class="n">distribution</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">p_</span><span class="err">{\</span><span class="n">theta</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="err">\</span><span class="n">cdot</span><span class="w"> </span><span class="err">\</span><span class="n">mid</span><span class="w"> </span><span class="n">x_</span><span class="err">{</span><span class="mi">1</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">x_</span><span class="err">{</span><span class="n">i</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="ow">all</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="err">\</span><span class="ow">in</span><span class="o">[</span><span class="n">n</span><span class="o">]</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="k">output</span><span class="w"> </span><span class="n">token</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">TF</span><span class="err">}</span><span class="n">_</span><span class="err">{\</span><span class="n">theta</span><span class="err">}</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="err">:</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">h_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">theta_</span><span class="err">{\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">TE</span><span class="err">}}\</span><span class="nf">left</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="n">i</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="o">+</span><span class="err">\</span><span class="n">theta_</span><span class="err">{\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">PE</span><span class="err">}}</span><span class="p">(</span><span class="n">i</span><span class="p">),</span><span class="w"> </span><span class="err">\</span><span class="n">forall</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="err">\</span><span class="ow">in</span><span class="o">[</span><span class="n">n</span><span class="o">]</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">l</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">L</span><span class="o">-</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">h_</span><span class="err">{</span><span class="mi">1</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mf">0.5</span><span class="p">)</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">h_</span><span class="err">{</span><span class="n">n</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mf">0.5</span><span class="p">)</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">h_</span><span class="err">{</span><span class="mi">1</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="p">(</span><span class="n">l</span><span class="p">)</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">h_</span><span class="err">{</span><span class="n">n</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="p">(</span><span class="n">l</span><span class="p">)</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="o">+</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">ATTN</span><span class="err">}</span><span class="n">_</span><span class="err">{\</span><span class="n">theta_</span><span class="err">{\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">ATTN</span><span class="err">}}</span><span class="o">^</span><span class="err">{</span><span class="p">(</span><span class="n">l</span><span class="p">)</span><span class="err">}}\</span><span class="nf">left</span><span class="p">(</span><span class="n">h_</span><span class="err">{</span><span class="mi">1</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="p">(</span><span class="n">l</span><span class="p">)</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">h_</span><span class="err">{</span><span class="n">n</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="p">(</span><span class="n">l</span><span class="p">)</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">h_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">h_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mf">0.5</span><span class="p">)</span><span class="err">}</span><span class="o">+</span><span class="err">\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">FF</span><span class="err">}</span><span class="n">_</span><span class="err">{\</span><span class="n">theta_</span><span class="err">{\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">FF</span><span class="err">}}</span><span class="o">^</span><span class="err">{</span><span class="p">(</span><span class="n">l</span><span class="p">)</span><span class="err">}}\</span><span class="nf">left</span><span class="p">(</span><span class="n">h_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mf">0.5</span><span class="p">)</span><span class="err">}\</span><span class="nf">right</span><span class="p">),</span><span class="w"> </span><span class="err">\</span><span class="n">forall</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="err">\</span><span class="ow">in</span><span class="o">[</span><span class="n">n</span><span class="o">]</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="n">p_</span><span class="err">{\</span><span class="n">theta</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="err">\</span><span class="n">cdot</span><span class="w"> </span><span class="err">\</span><span class="n">mid</span><span class="w"> </span><span class="n">x_</span><span class="err">{</span><span class="mi">1</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">x_</span><span class="err">{</span><span class="n">i</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="k">OUTPUT</span><span class="err">}</span><span class="n">_</span><span class="err">{\</span><span class="n">theta_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="k">OUTPUT</span><span class="w"> </span><span class="err">}}}\</span><span class="nf">left</span><span class="p">(</span><span class="n">h_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="p">(</span><span class="n">L</span><span class="p">)</span><span class="err">}\</span><span class="nf">right</span><span class="p">),</span><span class="w"> </span><span class="err">\</span><span class="n">forall</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="err">\</span><span class="ow">in</span><span class="o">[</span><span class="n">n</span><span class="o">]</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">TF</span><span class="err">}</span><span class="n">_</span><span class="err">{\</span><span class="n">theta</span><span class="err">}</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">arg</span><span class="w"> </span><span class="err">\</span><span class="nf">max</span><span class="w"> </span><span class="n">_</span><span class="err">{</span><span class="n">y</span><span class="err">}</span><span class="w"> </span><span class="n">p_</span><span class="err">{\</span><span class="n">theta</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="err">\</span><span class="n">mid</span><span class="w"> </span><span class="n">x_</span><span class="err">{</span><span class="mi">1</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">x_</span><span class="err">{</span><span class="n">n</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">).</span>
</code></pre></div>

<p>Though the standard definition of circuit complexity only deals with binary strings, given any finite vocabulary $\mathcal{V}$, we can always replace each token in $\mathcal{V}$ by its binary representation, and the length of the input only blows up by a constant factor. Therefore we can extend existing complexity classes listed to arbitrary finite vocabulary naturally.
P. The class P contains all problems solvable by a deterministic Turing machine in polynomial time.</p>
<p>Boolean Circuit. A Boolean circuit over $n$ variables is a directed acyclic graph where nodes are AND, OR, or NOT gates. The gates with in-degree 0 are the inputs, which are assigned one of the $n$ boolean variables. Given the inputs, the circuit computes the value of each non-input gate based on the value of the incoming gates and outputs a number at the output gate.</p>
<p>SIZE $[T(n)]$. Given any function $T$, SIZE $[T(n)]$ denotes the class of problems that can be solved by boolean circuits with $O(T(n))$ gates when the input length is $n$. Formally, a problem $\mathcal{L}$ is in SIZE $[T(n)]$ if and only if there exists a sequence of circuits $\left{C_{n}\right}$ such that each circuit $C_{n}$ has $n$ inputs and 1 output, the size of each circuit $C_{n}$ is at most $O(T(n))$, and for all strings $x, x$ is in $L$ if and only if $C_{|x|}(x)=1$.
$\mathrm{P} /$ poly. We define the class $\mathrm{P} /$ poly as the set of problems that can be solved by a family of polynomial-size circuits, that is, $\mathrm{P} /$ poly $\triangleq \cup_{k \in \mathbb{N}^{+}}$SIZE $\left[n^{k}\right]$. Since any Turing Machine with time bound $T(n)$ can be simulated by a circuit of size $T(n) \log T(n)$ (Pippenger \&amp; Fischer, 1979), we know that $\mathrm{P} \subseteq \mathrm{P} /$ poly.
$\mathrm{NC}, \mathrm{AC}$, and TC. The class NC contains all problems that can be solved in a small parallel runtime-polylogarithmic in input length-and with a polynomial number of processors. Formally, for a positive integer $k$, a problem $\mathcal{L}$ is in $\mathrm{NC}^{k}$ if and only if there exists a polynomial $p(n)$ and a family of circuits $\left{C_{n}\right}$ such that each circuit $C_{n}$ has $n$ inputs and 1 output, the fan-in of the gates is at most 2 , the size of each circuit $C_{n}$ is at most $p(n)$, the depth of each circuit $C_{n}$ is $O\left((\log n)^{k}\right)$, and for all strings $x, x$ is in if and only if $C_{|x|}(x)=1$. Finally we define $\mathrm{NC}=\cup_{k \in \mathbb{N}} \mathrm{NC}^{k}$. The class $\mathrm{AC}^{k}$ is defined almost the same as $\mathrm{NC}^{k}$ for each $k \in \mathbb{N}^{+}$, except the AND and OR gates in $\mathrm{AC}^{k}$ allow unbounded fan-in. The class $\mathrm{TC}^{k}$ allows a more powerful type of gate, MAJORITY, compared to $\mathrm{AC}^{k}$. MAJORITY gate can have unbounded fan-in and is defined as MAJORITY $\left(x_{1}, \ldots, x_{n}\right)=\left\lfloor\frac{1}{2}+\frac{\left(\sum_{i=1}^{n} x_{i}\right)-1 / 2}{n}\right\rfloor$.</p>
<p>It holds that $\mathrm{NC}^{i} \subseteq \mathrm{AC}^{i} \subseteq \mathrm{TC}^{i} \subseteq \mathrm{NC}^{i+1}$ for all natural number $i$. Therefore $\mathrm{NC}=\mathrm{AC}=\mathrm{TC}$, which all stands for the problem class that can be solved in polylogarithmic time with polynomial parallel processors.</p>
<h1>3 Expressiveness Theory for Transformers with Chain of Thought(CoT)</h1>
<p>In this section, we study the expressiveness of transformers with CoT from a theoretical perspective.</p>
<h3>3.1 Finite Precision Modeling</h3>
<p>In practice, training and inference of transformers are typically done with 16- or 32-bit floating point numbers. Thus in this paper, we mainly focus on the computation model of constant-precision transformers, where the output of each arithmetic operation is rounded to the closest floating point number representable by a fixed number of digits following IEEE 754 standard (Definition 3.2), thus avoiding the unrealistic infinite precision assumption made by prior works (Pérez et al., 2019; Dehghani et al., 2018).</p>
<p>Below we give a formal definition of the floating-point number and rounding operation. Recall $\phi(a)=\sum_{i=1}^{k} 2^{k-i} a_{i}$ denote the value of binary number represented by $a \in{0,1}^{k}$ for any $k \in \mathbb{N}^{+}$.</p>
<p>Definition 3.1 (Floating-point Representation). Let $e$ be the number of bits for exponents and $s$ be the number of bits for significand. A $(e+2 s+1)$-bit binary string $a=\left(a_{1}, a_{2}, \ldots a_{e+2 s+1}\right) \in$ ${0,1}^{e+2 s+1}$ is a floating-point binary representation of number $\phi_{e, s}(a) \triangleq \operatorname{sign}(a) \cdot 2^{\operatorname{exponent}(a)}$. $\operatorname{significand}(a)$ with $e$-bit exponent and $2 s$-precision, where the sign is $\operatorname{sign}(a) \triangleq 2 a_{1}-1$, the significand is significand $(a) \triangleq 2^{-s} \phi\left(a_{2: 2 s+1}\right)$, and the exponent is $\operatorname{exponent}(a) \triangleq \phi\left(a_{2 s+2: 2 s+e+1}\right)-$ $2^{\max (0, e-1)}$. We further use $\mathbb{F}<em e_="e," s="s">{e, s}$ to denote all the floating numbers representable using $e$-bit exponent and $2 s$-bit precision (significand), that is, $\mathbb{F}</em>$.} \triangleq\left{S \cdot 2^{-s+E} \mid-2^{2 s}+1 \leq S \leq\right.$ $\left.2^{2 s}-1,-2^{\max (0, e-1)} \leq E \leq 2^{e}-1-2^{\max (0, e-1)}, E, S \in \mathbb{N}\right}$. We define $B_{e, s} \triangleq \max \mathbb{F}_{e, s</p>
<p>We also use $\psi_{e, s}: \mathbb{F}<em e_="e," s="s">{e, s} \rightarrow{0,1}^{e+2 s+1}$ to denote the inverse of $\phi</em>}$. We note that when the number of exponent bits is larger than 0 , there are multiple ways to represent a number in $\mathbb{F<em e_="e," s="s">{e, s}$ by a binary string and we assign $\psi</em>(0)\right)=1$.}(x)$ as the string $a \in{0,1}^{e+2 s+1}$ with the smallest $|\operatorname{exponent}(a)|$, which is unique for all non-zero numbers. For 0 we additionally set $\operatorname{sign}\left(\psi_{e, s</p>
<p>Definition 3.2 (Correct Rounding). For any $x \in \mathbb{R}$ and any closed subset of $\mathbb{R}$ containing $0, \mathbb{F}$, we define correct rounding $\operatorname{round}(x, \mathbb{F})$ as the closest number to $x$ in $\mathbb{F}$. We break the tie by picking the one with a smaller absolute value.</p>
<p>In particular, we denote the rounding operation with $e$-bit exponent, $2 s$-bit precision by $\operatorname{round}<em e_="e," s="s">{e, s}(\cdot) \triangleq$ $\operatorname{round}\left(\cdot, \mathbb{F}</em>\right)$, which is also denoted by $[\cdot]<em e_="e," s="s">{e, s}$ for convenience. We extend the definition of round and $\operatorname{round}</em>$ to vector inputs by rounding coordinate-wisely.</p>
<p>Our notion of floating-point number simplifies the IEEE 754 Standard for Floating-point Arithmetic (IEEE, 2008) by removing $\infty$ and $-\infty$. When overflow happens, we always round the output to the (negative) largest representable number in $\mathbb{F}_{e, s}$. For unary functions like $\exp (\cdot)$ and binary functions including addition, subtraction, multiplication, and division, we simply define their rounded version by rounding their outputs. Whenever division by 0 happens, we treat it as the model outputs the wrong result.</p>
<p>Next, we define finite-precision summation over more two numbers by decomposing it as a chain of rounded binary addition in a fixed order. ${ }^{2}$</p>
<p>Definition 3.3 (Summation with Iterative Rounding). For any $s, n \in \mathbb{N}^{+}$and vector $x \in \mathbb{R}^{n}$, we define summation with iterative rounding to e bit exponent and $2 s$-bit precision as sum ${ }_{e, s}$ :</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>$\cup_{n \in \mathbb{N}^{+}}\left(\mathbb{F}<em e_="e," s="s">{e, s}\right)^{n} \rightarrow \mathbb{F}</em>$,}$, where for any $n \in \mathbb{N}^{+}$and $x \in \mathbb{R}^{n</p>
<p>$$
\operatorname{sum}<em 1="1">{e, s}(x) \triangleq\left[\left[\left[\left[x</em>\right]}+x_{2<em 3="3">{e, s}+x</em>\right]<em n-1="n-1">{e, s}+\cdots x</em>\right]<em n="n">{e, s}+x</em>
$$}\right]_{e, s</p>
<p>We further define the following operations:</p>
<ul>
<li>Finite-precision inner product: $\langle x, y\rangle_{e, s} \triangleq \operatorname{sum}_{e, s}(x \odot y)$;</li>
<li>Finite-precision matrix product: $\left(A \times_{e, s} B\right)<em i_:="i,:">{i, j} \triangleq\left\langle\left(A</em>$;}\right)^{\top}, B_{:, j}\right\rangle_{e, s</li>
<li>Finite-precision softmax: $\operatorname{softmax}<em e_="e," s="s">{e, s}(x) \triangleq\left[[\exp (x)]</em>} / \operatorname{sum<em e_="e," s="s">{e, s}\left([\exp (x)]</em>$.}\right)\right]_{e, s</li>
</ul>
<p>Finally, a finite-precision transformer can be defined by replacing all the infinite-precision operations by their finite-precision counterparts listed above. (See details in Algorithm 4). We postpone the details of the finite-precision version of individual transformer layers into Appendix B.</p>
<h1>3.2 CoT: Complexity Class for Constant-depth Transformers with CoT</h1>
<p>In this subsection, we define the complexity class consisting of all the problems that can be solved by some decoder-only transformers with CoT with finite precision.</p>
<p>Definition 3.4 (CoT). Given a finite vocabulary $\mathcal{V}$ and four functions $T(n), d(n), s(n), e(n)$, informally, $\operatorname{CoT}[T(n), d(n), s(n), e(n)]$ is the family of problems solvable by a transformer with a constant depth, $s(n)$ bits of precision, $e(n)$ bits of exponent, embedding size $d(n)$ and $T(n)$ steps of CoT. Formally, we say a problem $\mathcal{L}: \cup_{n \in \mathbb{N}^{+}} \mathcal{V}^{n} \rightarrow{0,1}$ is in $\operatorname{CoT}[T(n), d(n), s(n), e(n)]$ iff there is an integer $L$ and three functions $T^{\prime}(n)=O(T(n)), d^{\prime}(n)=O(d(n)), s^{\prime}(n)=O(s(n))$, $e^{\prime}(n)=O(e(n))$, such that for every positive integer $n$, there is a $L$-layer decoder-only transformer, denoted by $\mathrm{TF}<em n="n">{\theta</em>(n)$ steps of chain of thought. Mathematically, it means}}$ with embedding size $d^{\prime}(n), 2 s^{\prime}(n)$ bits of precision, and $e^{\prime}(n)$ bits of exponent, that can output $\mathcal{L}(x)$ given any input $x$ in $\mathcal{V}^{n}$, using $T^{\prime</p>
<p>$$
\mathrm{TF}<em n="n">{\theta</em>
$$}}^{1+T^{\prime}(n)}(x)=\mathcal{L}(x), \quad \forall x \in \mathcal{V}^{n</p>
<p>We also extend the definition of CoT to a class of function instead of a single function. For example, $\operatorname{CoT}[T(n), \operatorname{poly}(n), s(n), e(n)] \triangleq \cup_{k \in \mathbb{N}^{+}} \operatorname{CoT}\left[T(n), n^{k}, s(n), e(n)\right]$.</p>
<p>Definition 3.5 (T). We define $\mathrm{T}[d(n), s(n), e(n)] \triangleq \operatorname{CoT}[0, d(n), s(n), e(n)]$ as the problems that a constant-depth, constant-precision decoder-only transformer can solve with $O(s(n))$ bits of precision, $O(e(n))$ bits of exponent, $O(d(n))$ embedding size and without CoT (or with only 0 step of CoT).</p>
<p>By definition, $\operatorname{CoT}[T(n), d(n), s(n), e(n)]$ is monotone in all $T(n), d(n), s(n), e(n)$, e.g., $\operatorname{CoT}\left[T^{\prime}(n), d(n), s(n), e(n)\right] \subseteq \operatorname{CoT}[T(n), d(n), s(n), e(n)]$ if $T^{\prime}(n) \leq T(n)$ for all $n \in \mathbb{N}$. In particular, we have $\mathrm{T}[d(n), s(n), e(n)] \triangleq \operatorname{CoT}[0, d(n), s(n), e(n)] \subseteq \operatorname{CoT}[T(n), d(n), s(n), e(n)]$.</p>
<p>Note the above-defined complexity class CoT is non-uniform, that is, it allows a different program for every input size. This is in contrast to previous works (Pérez et al., 2019, 2021; Yao et al., 2021; Weiss et al., 2021; Chiang et al., 2023; Hao et al., 2022; Merrill \&amp; Sabharwal, 2023a; Merrill et al., 2022) which focus on the uniform transformer classes. Please refer to Appendix G for a discussion.</p>
<h1>3.3 Tighter Upper Bounds on Transformer Expressiveness</h1>
<p>Existing works (Merrill \&amp; Sabharwal, 2023b; Liu et al., 2022a) have shown that constant depth, polynomial width, and log precision transformers can be simulated in a small parallel time, i.e., using $\mathrm{TC}^{0}$ circuits. These results are built on the fact that multiplication and division of $n$-bits binary numbers (Hesse, 2001), as well as the iterated addition over $n$ different $n$-bit binary integers are in $\mathrm{TC}^{0}$.</p>
<p>However, such $\mathrm{TC}^{0}$ expressiveness upper bounds may be unrealistic for transformers operating with floating point numbers. (Merrill \&amp; Sabharwal, 2023b; Liu et al., 2022a) implicitly assumes when adding more than one floating-point number, the algorithm first computes the exact answer without rounding using arbitrarily more precision and only performs rounding in the end. However, in practice rounding happens after each addition between two numbers and it is open if such $\mathrm{TC}^{0}$ upper bounds still holds. Immediate rounding makes iterated addition over floating point numbers no longer associative (Goldberg, 1991), for example, round $(a+\operatorname{round}(b+c)) \neq \operatorname{round}(\operatorname{round}(a+b)+c))$. The associativity of integer addition plays a crucial role in the fact that the iterated addition over $n$ different $n$-bit binary integers is in $\mathrm{TC}^{0}$.</p>
<p>In this section, we present two novel expressiveness upper bounds for transformers which round the immediate result after each step of the arithmetic operation. First, we show a strictly tighter upper bound than $\mathrm{TC}^{0}$, which is $\mathrm{AC}^{0}$, for constant-depth transformers with both constant bits of precision and exponents. (Theorem 3.1) This suggests when input length is sufficiently long, constant-precision transformers cannot count eventually, even in the sense of modular. For example, it is well known that no $\mathrm{AC}^{0}$ circuits can decide the parity of a binary string.</p>
<p>Theorem 3.1. $\mathrm{T}[\operatorname{poly}(n), 1,1] \subseteq \operatorname{CoT}[\log n, \operatorname{poly}(n), 1,1] \subseteq \mathrm{AC}^{0}$.
Our second result, Theorem 3.2, shows that when the number of bits for the exponent is 0 (i.e. fixed-point numbers), $\mathrm{TC}^{0}$ upper bounds for the expressiveness of constant-depth, log-precision transformers still holds, even with the correct rounding defined in Definition 3.2.</p>
<p>Theorem 3.2. $\mathrm{T}[\operatorname{poly}(n), \log (n), 0] \subseteq \operatorname{CoT}[\log n, \operatorname{poly}(n), \log (n), 0] \subseteq \mathrm{TC}^{0}$.
We note that the fact that a single forward pass of the transformer can be simulated by an $\mathrm{AC}^{0}$ circuit immediately implies that transformer output with $O(\log n)$ steps of CoT can also be simulated by $\mathrm{AC}^{0}$. This is because in general one can the transformer output with $T$ steps of CoT as an OR of $2^{T}$ disjoint subcircuits, where each of them enumerates all possible values of $T$ CoT tokens and output the value of the token in the branch where all the intermediate token values are consistent. This enumeration can be done in parallel and thus only takes constant depth. When $T=O(\log n)$, this only leads $\operatorname{poly}(n)$ factor of explosion in circuit size and thus still in $\mathrm{AC}^{0}$. The same argument holds for $\mathrm{TC}^{0}$ as well.</p>
<p>The main technical difficulties in above two results are showing $\operatorname{sum}<em e_="e," s="s">{e, s}:\left(\mathbb{F}</em>}\right)^{n} \rightarrow \mathbb{F<em e_="e," s="s">{e, s}$ has $\mathrm{AC}^{0}$ (resp. $\mathrm{TC}^{0}$ ) circuits when $e, s$ are both constants (resp. $e=0, s=O(\log (n))$ ). We view iterated addition with rounding over $\mathbb{F}</em>}$ as an automaton with both state space and vocabulary being $\mathbb{F<em e_="e," s="s">{e, s}$. The first result are due to a novel application of classical Krhon-Rhodes decomposition theorem for automata (Theorem C.2), where we use the property of rounded addition that for all $x, x^{\prime} \in \mathbb{F}</em>}, y \in \mathbb{F<em e_="e," s="s">{e, s}, x \geq x^{\prime} \Longrightarrow[x+y]</em>$ circuits (McNaughton \&amp; Papert, 1971).} \geq\left[x^{\prime}+y\right]_{e, s}$. We formalize this property in Definition D. 2 as ordered automata and show all ordered automata are counter-free Theorem D. 3 and thus can be simulated by $\mathrm{AC}^{0</p>
<p>The proof technique for Theorem 3.1 does not generalize to Theorem 3.2 because the depth of $\mathrm{AC}^{0}$ circuits constructed before depends on the number of the states of the automaton and thus is</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />
(c) Two ways to serialize circuits. The left (blue) one is the most natural one and the right (green) one is used to construct the position embedding so the transformer with CoT simulates the original circuit in Figure 2a.</p>
<p>Figure 2: Illustration of Theorem 3.3 on a 2-gate and 2-input circuit.
not constant. Our proof for Theorem 3.2 is motivated by Algorithm 1 in Liu et al. (2022a) for the automaton named 'GridWorld'.</p>
<p>However, it remains open whether constant-depth, log-precision transformers with log bits for exponents $\mathrm{T}[\operatorname{poly}(n), \log (n), \log (n)]$ or even constant bits for exponents $\mathrm{T}[\operatorname{poly}(n), \log (n), 1]$ have $\mathrm{TC}^{0}$ circuits.</p>
<h1>3.4 CoT Makes Transformers More Expressive</h1>
<p>Now we are ready to present our main theoretical results (Theorem 3.3) which characterize the expressiveness of constant-depth, constant-precision transformers with CoT and $O(\log (n))$ embedding size. $\log (n)$ embedding sizes are necessary to ensure that the position embeddings for $n$ inputs are different. All the lower bounds for transformer expressiveness (with or without CoT ) are proved for fixed-point numbers, i.e., without using any exponent bits. Allowing exponent bits will only make transformers more expressive. For convenience, we define $\operatorname{CoT}[T(n), d(n), s(n)] \triangleq \operatorname{CoT}[T(n), d(n), s(n), 0]$. The omitted proofs in this section can be found in Appendix E.</p>
<p>Theorem 3.3. For any polynomial function $T: \mathbb{N}^{+} \rightarrow \mathbb{N}^{+}$, SIZE $[T(n)] \subseteq \operatorname{CoT}[T(n), \log n, 1]$. In particular, $\mathrm{P} / \mathrm{poly}=\operatorname{CoT}[\operatorname{poly}(n), \log n, 1]$.</p>
<p>Compared to Theorems 3.1 and 3.2, Theorem 3.3 shows that allowing polynomial steps of CoT</p>
<p>strictly makes constant-depth, constant-precision, decoder-only transformer more expressive and logprecision transformers more expressive under a standard hardness assumption that $\mathrm{TC}^{0} \subsetneq \mathrm{P} /$ poly. ${ }^{3}$</p>
<p>Proof sketch of Theorem 3.3. The high-level proof idea is that we use each step in CoT to simulate one gate operation in the target circuit and write the gate output as next input. To do that, we use one position encoding to store the information for each gate, which contains four parts: the current gate id, the next gate type {AND, OR, NOT, TRUE, FALSE}, and the two input gates id of the next gate. Since there are total $\operatorname{poly}(n)$ gates, $d(n)=\Theta(\log n)$ embedding size suffices to store the above information. The CoT here is constructed to be the values of each gate in the increasing order of id. Therefore, in each step, we can use attention to pull the value (either computed already or it is input) of the two input gates and use a feedforward network to compute the value of the current gate. The proof idea is illustrated in Figure 2.</p>
<p>As we can see from the proof sketch, a crucial step for CoT to simulate any depth circuit is to write the output token back to the next input position. This action resets the "depth" of the intermediate output in the circuit to 0 . Our theory explains the ablation experiment in Wei et al. (2022) that when the model is prompted to output a only sequence of dots (. . .) equal to the number of tokens needed to solve the problem, the performance is no better than directly outputting the answer.</p>
<p>Because every regular language can be recognized by a finite state automaton (Definition C.1) and finite state automata can clearly be simulated by linear size circuits. The following holds as a direct corollary of Theorem 3.3</p>
<p>Corollary 3.4. Every regular language belongs to $\operatorname{CoT}[n, \log n, 1]$.
Below we give a concrete regular language that constant-depth, poly-embedding-size transformers can solve only with CoT, the wording problem of permutation group over 5 elements, $S_{5}$ in Theorem 3.5, under a standard hardness assumption that $\mathrm{TC}^{0} \subsetneq \mathrm{NC}^{1}$ (Yao, 1989).</p>
<p>Definition 3.6 (Wording problem of group $G$ ). Given $n$ elements from $G,\left(g_{1}, \ldots, g_{n}\right)$, we use $\mathcal{L}<em 1="1">{G}$ to denote the decision problem of whether $g</em>$ is equal to the identity of $G$.} \circ g_{2} \circ \cdots \circ g_{n</p>
<p>For convenience, in this paper, we extend the domain of $\mathcal{L}_{G}$ to the sequence of groups encoded by binary strings. The proof of Theorem 3.5 is a direct consequence of Theorems 3.2, 3.3 and 3.6.</p>
<p>Theorem 3.5. Assuming $\mathrm{TC}^{0} \subsetneq \mathrm{NC}^{1}$, the wording problem of $S_{5}, \mathcal{L}<em 5="5">{S</em>(n), \log n]$.}}$ is in $\operatorname{CoT}[n, \log n, 1]$ but not $\mathrm{T}[\operatorname{poly</p>
<p>Theorem 3.6 (Barrington (1986)). The wording problem of $S_{5}$ is $\mathrm{NC}^{1}$-complete under $\mathrm{AC}^{0}$ reductions. That is, for any decision problem $\mathcal{L}$ in $\mathrm{NC}^{1}$, there is a family of $\mathrm{AC}^{0}$ circuits $\left{C_{n}\right}<em S__5="S_{5">{n=1}^{\infty}$ (constant depth, poly $(n)$ fan-outs), such that for any $n \in \mathbb{N}^{+}$and $x \in{0,1}^{n}, \mathcal{L}(x)=\mathcal{L}</em>(x)\right)$.}}\left(C_{n</p>
<p>Proof of Theorem 3.5. First $\mathcal{L}<em 5="5">{S</em>}}$ is a regular language, thus belonging to $\operatorname{CoT}[n, \log n, 1]$ by Corollary 3.4. Since $\mathcal{L<em 5="5">{S</em>}}$ is $\mathrm{NC}^{1}$-complete by Theorem 3.6, assuming $\mathrm{TC}^{0} \subsetneq \mathrm{NC}^{1}, \mathcal{L<em 5="5">{S</em>$.}}$ does not belong to $\mathrm{TC}^{0}$. This proof is completed by applying Theorem 3.2, which says $\mathrm{T}[\operatorname{poly}(n), \log (n)] \subseteq \mathrm{TC}^{0</p>
<p>Results for $\operatorname{poly}(n)$ embedding size: So far we have been focusing on the expressiveness of transformer with $O(\log n)$ embedding size, so it is natural to ask whether transformers can also benefit from having a larger embedding size, say $\operatorname{poly}(n)$ ? Our Theorem 3.7 answers this question positively by showing that log-precision (resp. constant-precision) constant-depth poly-embeddingsize decoder-only transformers with $T(n)$ steps of CoT can simulate any $T(n)$-size circuit with some $\mathrm{TC}^{0}$ (resp. $\mathrm{AC}^{0}$ ) oracle gates with poly $(n)$ input.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Permutation Composition $\left(S_{5}\right)$. The label is the composition of all the permutations, where given two permutation $\sigma=\left(\sigma_{1}, \ldots, \sigma_{5}\right), \pi=\left(\pi_{1}, \ldots, \pi_{5}\right)$, we define $\sigma \circ \pi \triangleq\left(\sigma_{\pi_{1}}, \ldots, \sigma_{\pi_{5}}\right)$. The chain of thoughts and hints are the partial compositions (string A). Only CoT can solve this task well, as predicted by our Theorem 3.5. Note for the most time the accuracy without CoT is $\sim 20 \%$, which is no better than randomly guessing a number between 1 and 5 .</p>
<p>Formally, given a decision problem $\mathcal{L}: \cup_{n=1}^{\infty}{0,1}^{n} \rightarrow{0,1}$, we use $\mathcal{L}_{n}$ to denote the restriction of $\mathcal{L}$ on ${0,1}^{n}$, which can also be viewed as an single gate with $n$ fan-ins. We define problems that can be solved by circuits with certain sizes of gates (including oracle gates) by Definition 3.7. ${ }^{4}$</p>
<p>Definition 3.7 (SIZE ${ }^{\mathcal{L}}$ ). For any decision problem $\mathcal{L}$ and $T(n) \subseteq O(\operatorname{poly}(n))$, we define SIZE ${ }^{\mathcal{L}}(T(n))$ as the set of decision problems $\mathcal{L}^{\prime}$ such that there exists $p(n) \in \operatorname{poly}(n)$ and circuits $\left{C_{n}\right}<em n="n">{n=1}^{\infty}$ where $C</em>}$ contains at most $O(T(n))$ AND, OR, NOT, and $\mathcal{L<em _mathcal_L="\mathcal{L">{p(n)}$ gates. For a complexity class $\mathcal{C}$, we define $\operatorname{SIZE}^{\mathcal{C}}(T(n)) \triangleq \cup</em>(T(n))$.} \in \mathcal{C}} \operatorname{SIZE}^{\mathcal{L}</p>
<p>Theorem 3.7. For any $T(n) \in \operatorname{poly}(n)$, it holds that $\operatorname{SIZE}^{\mathrm{TC}^{0}}[1+T(n)]=\operatorname{CoT}[T(n), \operatorname{poly}(n), \log n]$. Specifically, for $T(n)=0$, we have $\mathrm{TC}^{0}=\operatorname{SIZE}^{\mathrm{TC}^{0}}[1]=\operatorname{CoT}[0, \operatorname{poly}(n), \log n]=\mathrm{T}[\operatorname{poly}(n), \log n]$.</p>
<p>Theorem 3.8. For any $T(n) \in \operatorname{poly}(n)$, it holds that $\operatorname{SIZE}^{\mathrm{AC}^{0}}[1+T(n)]=\operatorname{CoT}[T(n), \operatorname{poly}(n), 1]$. Specifically, for $T(n)=0$, we have $\mathrm{AC}^{0}=\operatorname{SIZE}^{\mathrm{AC}^{0}}[1]=\operatorname{CoT}[0, \operatorname{poly}(n), 1]=\mathrm{T}[\operatorname{poly}(n), 1]$.</p>
<p>Theorem 3.8 shows that for $T(n)=\operatorname{poly}(n)$ steps of CoT, using poly $(n)$ embedding size does not improve expressiveness over using $\log (n)$ embedding size (Theorem 3.3), because $\operatorname{SIZE}^{\mathrm{TC}^{0}}[\operatorname{poly}(n)]=$ $\operatorname{SIZE}^{\mathrm{AC}^{0}}[\operatorname{poly}(n)]=\operatorname{SIZE}[\operatorname{poly}(n)]$. However, Theorem 3.9 shows that for any specific polynomial $T(n)=n^{k}$ steps of CoT, increasing embedding width from $O(\log (n))$ to poly $(n)$ make transformers strictly more powerful.</p>
<p>Theorem 3.9. For any $s(n)=O(\log n), \mathrm{T}[\log n, s(n)] \subsetneq \mathrm{T}[\operatorname{poly}(n), s(n)]$ and for all $k \in \mathbb{N}$, $\operatorname{CoT}\left[n^{k}, \log n, s(n)\right] \subsetneq \operatorname{CoT}\left[n^{k}, \operatorname{poly}(n), s(n)\right]$.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Modular Addition $\left(C_{7}\right)$. The label is the sum of the inputs modulo a positive integer, which is 7 in this case. The chain of thoughts and hints are the partial modular sum. Low-depth transformers with hint can solve this task well for a reasonable input sequence length, but with cot the performance is much better, especially with a long input sequence, as predicted by our Theorem 3.3. See experiments for $C_{2}$ in Figure 7.</p>
<h1>4 CoT Empirically Improves Expressiveness of Low-Depth Transformers on Inherently Serial Problems</h1>
<p>This section is an empirical study of the expressiveness of decoder-only transformers with CoT on four different arithmetic problems: modular addition, permutation composition ( $S_{5}$ ), iterated squaring, and circuit value problem. The first problem is parallelizable and can be solved by constant-depth transformers with log-precision while the latter three are inherently serial under some standard hardness assumptions in computational complexity or cryptography. As a prediction of our theory, we expect to see a huge improvement in accuracy when CoT is turned on.</p>
<p>General Setup. To examine the expressiveness of decode-only transformers with and without CoT on these four types of problems, we train the transformer using Adam (Kingma \&amp; Ba, 2014) from random initialization in the online supervised setting for each problem and each different sequence length $n$. At each step, we sample a batch of training data from a distribution $p_{n}(x)$ where $x=\left(x_{1}, \ldots, x_{n}\right)$ is training data and $y=f^{*}\left(x_{1}, \ldots, x_{n}\right)$ is the label. We always set $x_{n}$ to be ' $=$ '. We consider three different settings, base, cot, and hint:</p>
<ul>
<li>base: The optimization objective is simply $\ell_{\text {base }}(\theta) \triangleq \mathbb{E}<em _theta="\theta">{x \sim p}-\log p</em>(x) \mid x\right)$.}\left(f^{*</li>
<li>cot: We manually design a chain of thought for each instance $x$, which is a string in $\mathcal{V}$ and we denote by $c(x)$. We ensure the last token of $c(x)$ is always equal to the answer, $f^{*}(x)$. With $\tilde{x} \triangleq(x, c(x))$, the concatenation of $x$ and $c(x)$, and $m$ be the length of $c(x)$, the optimization objective is $\ell_{\text {cot }}(\theta) \triangleq \frac{1}{m} \mathbb{E}<em i="n">{x \sim p} \sum</em>}^{n+m-1}-\ln p_{\theta}\left(\tilde{x<em 1="1">{i+1} \mid \tilde{x}</em>\right)$ ).}, \ldots, \tilde{x}_{i</li>
<li>hint: Even if the transformer has better performance in cot setting than base setting, one may argue that besides the difference expressiveness, cot setting also has a statistical advantage over base, as cot provides more labels and thus more information about the groundtruth $f^{*}$ to the model. This motivates us to design the following loss which provides the chain of thought $c(x)$ as the labels. Here for simplicity, we assume the length of $c(x)$ is equal to $n .{ }^{5}$ Formally we define $\ell_{\text {hint }}(\theta) \triangleq \frac{1}{n} \mathbb{E}<em i="1">{x \sim p} \sum</em>\right)$ ).}^{n}-\ln p_{\theta}\left(c_{i}(x) \mid x_{1}, \ldots, x_{i</li>
</ul>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Iterated Squaring(IS). The vocabulary $\mathcal{V} \triangleq{0,1, \ldots, T-1,=, \wedge 2}$ with $T=1000$. We randomly generate input of format $(p, r, \wedge 2, \ldots, \wedge 2,=)$ with $1 \leq r, p \leq T-1, p$ being a prime and random number of $\wedge 2$ tokens (at most $m$ ). The label is $f_{r, p}(n) \equiv\left(r^{2^{m}}\right) \bmod p$. CoT and hints are $\left(f_{r, p}(i)\right)_{i=1}^{n}$. Though our construction does not exactly satisfy the technical conditions of the hardness assumption, this problem is difficult for transformers without CoT to learn, but can be perfectly expressed with CoT even with depth 1.</p>
<p>Performance Evaluation. Since we train transformers using fresh sampled synthetic data each step, the training accuracy/loss is just the same as validation accuracy/loss. For base and hint setting, we evaluate the accuracy of the final answer directly. For cot setting, directly evaluating the final answer is too easy because it only measures the ability of the transformer to correctly compute the last step since CoT is given as inputs. Ideally, we should measure the answer output by transformers after auto-regressively generating $|c(x)|$ tokens. But for computational efficiency, we measure the probability that transformers can predict all tokens in the given CoT correctly. Note this probability is a lower bound of the ideal metric because there is a small possibility that transformers can answer correctly with a wrong CoT. Nevertheless, even with this slightly more difficult evaluation metric, transformers in cot setting still optimize much faster than without CoT.</p>
<p>Due to space limit, we defer the details of the training and each setting to Appendix A. Our experimental results are presented in Figures 3 to 6.</p>
<p>Our Findings: Unsurprisingly, the accuracy in hint setting is always higher than base setting. Due to the space limit, we postpone all results for base settings into Appendix A. For the problems hard for parallel computation, i.e., permutation composition, iterated squaring, and circuit value problem, we find that cot is always better than hint and base, and the improvement is huge especially when depth is small. Our experiments suggest that turning on CoT drastically improves the expressiveness of low-depth transformers on problems that are hard to parallel compute, i.e., those inherently serial problems.</p>
<h1>5 Related Works</h1>
<p>Despite the numerous empirical achievements, unanswered questions concerning the inner workings of neural networks capable of algorithmic reasoning. The ability of self-attention to create lowcomplexity circuits has been recognized (Edelman et al., 2022; Hahn, 2020; Merrill et al., 2021), as</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Circuit Value Problem(CVP). Given a randomly generated circuit with $m$ gates (sorted by topological order), the vocabulary $\mathcal{V}=[m] \cup{$ TRUE, FALSE, AND, OR, NOT, NA, $=}$. Each gate is represented by four consecutive tokens, which are gate type, two input gate ids, and the current gate id. The output is the value of the last gate $m$. CoT and hints also contain 4 tokens for each gate, which are gate type, two input gate values, and the current gate value.
well as its capacity to form declarative programs (Weiss et al., 2021), and Turing machines (Dehghani et al., 2018; Giannou et al., 2023; Pérez et al., 2021). Moreover, it has been demonstrated that interpretable symbolic computations can be drawn from trained models (Clark et al., 2019; Tenney et al., 2019; Vig, 2019; Wang et al., 2022b).</p>
<p>Liu et al. (2022a) is a closely related work to ours, which studies the expressiveness of low-depth transformers for semi-automata. Their setting corresponds to using only 1 step of CoT and our contribution is to show that allowing more steps of CoT enables the transformers to solve more difficult problems than semi-automata, especially those inherently serial problems, like the circuit value problem, which is P-complete.</p>
<p>Constant precision versus logarithmic precision: We note that most previous literature on the expressiveness of transformers focuses on the setting of logarithmic precision, including (Merrill \&amp; Sabharwal, 2023b; Merrill et al., 2022, 2021; Liu et al., 2022a), etc. One main reason as argued by Merrill \&amp; Sabharwal (2023a) is that log precision allows the transformer to use uniform attention over the rest tokens. However, recent advancements in LLMs showed that uniform attention might not be necessary towards good performance, at least for natural language tasks. For example, one of the most successful open-sourced LLM, LLAMA2 (Touvron et al., 2023) takes the input of a sequence of 4096 tokens and uses BF16 precision, which has 1 sign bit, 8 exponent bits and 7 mantissa bits (plus one extra leading bit). As a consequence, for example, BF16 cannot express any floating-point number between $2^{8}=256$ and $2^{8}+2=258$, which makes LLAMA2 impossible to compute uniform attention over 257 elements.</p>
<p>A concurrent work Feng et al. (2023) also studies the benefit of CoT via the perspective of</p>
<p>expressiveness, where they show with CoT , transformers can solve some specific P -complete problem. Our result is stronger in the sense that we give a simple and clean construction for each problem in $\mathrm{P} /$ poly. We also note the slight difference in the settings, while we mainly focus on constant-precision transformers with $O(\log n)$ embedding size, they focus on $O(\log (n))$ precision transformers with bounded embedding size.</p>
<h1>6 Conclusion</h1>
<p>We study the capability of CoT for decoder-only transformers through the lens of expressiveness. We adopt the language of circuit complexity and define a new complexity class $\operatorname{CoT}[T(n), d(n), s(n), e(n)]$ which corresponds to a problem class solvable by constant-depth, constant-precision decoder-only transformers with $O(T(n))$ steps of $\operatorname{CoT}, O(d(n))$ embedding size and floating-point numbers with $O(e(n))$ bits of exponents and $O(s(n))$ bits of significand. Our theory suggests that increasing the length of CoT can drastically make transformers more expressive. We also empirically verify our theory in four arithmetic problems. We find that for those three inherently serial problems, transformers can only express the groundtruth function by using CoT.</p>
<h2>Acknowledgement</h2>
<p>The authors would like to thank the support from NSF IIS 2045685. The authors also thank Wei Zhan and Lijie Chen for providing references on circuit complexity and various inspiring discussions, Cyril Zhang and Bingbin Liu for helpful discussions on Khron-Rhodes decomposition theorem, and Kaifeng Lyu for his helpful feedback.</p>
<h2>References</h2>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.</p>
<p>Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.</p>
<p>David A. Barrington. Bounded-width polynomial-size branching programs recognize exactly those languages in nc. pp. 1-5, 1986.</p>
<p>Ashok K Chandra, Steven Fortune, and Richard Lipton. Unbounded fan-in circuits and associative functions. In Proceedings of the fifteenth annual ACM symposium on Theory of computing, pp. $52-60,1983$.</p>
<p>David Chiang, Peter Cholak, and Anand Pillay. Tighter bounds on the expressivity of transformer encoders. arXiv preprint arXiv:2301.10743, 2023.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1-113, 2023.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.</p>
<p>Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What does bert look at? an analysis of bert's attention. arXiv preprint arXiv:1906.04341, 2019.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018.</p>
<p>Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. In International Conference on Machine Learning, pp. 5793-5831. PMLR, 2022.</p>
<p>Guhao Feng, Yuntian Gu, Bohang Zhang, Haotian Ye, Di He, and Liwei Wang. Towards revealing the mystery behind chain of thought: a theoretical perspective. arXiv preprint arXiv:2305.15408, 2023.</p>
<p>Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. arXiv preprint arXiv:2301.13196, 2023.</p>
<p>David Goldberg. What every computer scientist should know about floating-point arithmetic. ACM computing surveys (CSUR), 23(1):5-48, 1991.</p>
<p>Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156-171, 2020.</p>
<p>Yiding Hao, Dana Angluin, and Robert Frank. Formal language recognition by hard attention transformers: Perspectives from circuit complexity. Transactions of the Association for Computational Linguistics, 10:800-810, 2022.</p>
<p>William Hesse. Division is in uniform tc0. In International Colloquium on Automata, Languages, and Programming, pp. 104-114. Springer, 2001.</p>
<p>IEEE. Ieee standard for floating-point arithmetic. IEEE Std 754-2008, pp. 1-70, 2008. doi: 10.1109/IEEESTD.2008.4610935.</p>
<p>Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in Neural Information Processing Systems, 2022.</p>
<p>Kenneth Krohn and John Rhodes. Algebraic theory of machines. i. prime decomposition theorem for finite semigroups and machines. Transactions of the American Mathematical Society, 116: $450-464,1965$.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146, 2017.</p>
<p>Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata. arXiv preprint arXiv:2210.10749, 2022a.</p>
<p>Yong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, and Yang You. Towards efficient and scalable sharpness-aware minimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12360-12370, 2022b.</p>
<p>Alex Lombardi and Vinod Vaikuntanathan. Fiat-shamir for repeated squaring with applications to ppad-hardness and vdfs. In Advances in Cryptology-CRYPTO 2020: 40th Annual International Cryptology Conference, CRYPTO 2020, Santa Barbara, CA, USA, August 17-21, 2020, Proceedings, Part III, pp. 632-651. Springer, 2020.</p>
<p>Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022.</p>
<p>Oded Maler. On the krohn-rhodes cascaded decomposition theorem. In Time for Verification: Essays in Memory of Amir Pnueli, pp. 260-278. Springer, 2010.</p>
<p>Robert McNaughton and Seymour A Papert. Counter-Free Automata (MIT research monograph no. 65). The MIT Press, 1971.</p>
<p>William Merrill and Ashish Sabharwal. A logic for expressing log-precision transformers. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a.</p>
<p>William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision transformers. Transactions of the Association for Computational Linguistics, 11:531-545, 2023b.</p>
<p>William Merrill, Yoav Goldberg, and Noah A Smith. On the power of saturated transformers: A view from circuit complexity. arXiv preprint arXiv:2106.16213, 2021.</p>
<p>William Merrill, Ashish Sabharwal, and Noah A Smith. Saturated transformers are constant-depth threshold circuits. Transactions of the Association for Computational Linguistics, 10:843-856, 2022.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.</p>
<p>Jorge Pérez, Javier Marinković, and Pablo Barceló. On the turing completeness of modern neural network architectures. arXiv preprint arXiv:1901.03429, 2019.</p>
<p>Jorge Pérez, Pablo Barceló, and Javier Marinkovic. Attention is turing complete. The Journal of Machine Learning Research, 22(1):3463-3497, 2021.</p>
<p>Nicholas Pippenger and Michael J Fischer. Relations among complexity measures. Journal of the ACM (JACM), 26(2):361-381, 1979.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 2019.</p>
<p>Laria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, pp. 1-7, 2021.</p>
<p>Ronald L Rivest, Adi Shamir, and David A Wagner. Time-lock puzzles and timed-release crypto. 1996.</p>
<p>Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, Jordan Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli, and Alhussein Fawzi. Mathematical discoveries from program search with large language models. Nature, 2023. doi: 10.1038/s41586-023-06924-6.</p>
<p>Ian Tenney, Dipanjan Das, and Ellie Pavlick. Bert rediscovers the classical nlp pipeline. arXiv preprint arXiv:1905.05950, 2019.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476-482, 2024.</p>
<p>Jesse Vig. Visualizing attention in transformer-based language representation models. arXiv preprint arXiv:1904.02679, 2019.</p>
<p>Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. Towards understanding chain-of-thought prompting: An empirical study of what matters. arXiv preprint arXiv:2212.10001, 2022a.</p>
<p>Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: a circuit for indirect object identification in gpt-2 small. arXiv preprint arXiv:2211.00593, 2022b.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 2022.</p>
<p>Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers. In International Conference on Machine Learning, pp. 11080-11090. PMLR, 2021.</p>
<p>Christopher B Wilson. Relativized circuit complexity. Journal of Computer and System Sciences, 31(2):169-181, 1985.</p>
<p>Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pp. 10524-10533. PMLR, 2020.</p>
<p>Andrew Chi-Chih Yao. Circuits and local computation. pp. 186-196, 1989.
Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan. Self-attention networks can process bounded hierarchical languages. arXiv preprint arXiv:2105.11115, 2021.</p>
<h1>Contents</h1>
<p>1 Introduction ..... 1
2 Notations and Preliminaries ..... 3
2.1 Decoder-only Transformers ..... 3
2.2 Circuit Complexity ..... 4
3 Expressiveness Theory for Transformers with Chain of Thought(CoT) ..... 6
3.1 Finite Precision Modeling ..... 6
3.2 CoT: Complexity Class for Constant-depth Transformers with CoT ..... 7
3.3 Tighter Upper Bounds on Transformer Expressiveness ..... 8
3.4 CoT Makes Transformers More Expressive ..... 9
4 CoT Empirically Improves Expressiveness of Low-Depth Transformers on Inherently Serial Problems ..... 12
5 Related Works ..... 13
6 Conclusion ..... 15
A Additional Experimental Results ..... 21
B Details on Finite-Precision Layers ..... 23
C Preliminary of Automata and Krohn-Rhodes Decomposition Theorem ..... 24
C. 1 The Krohn-Rhodes Decomposition Theorem ..... 25
C. 2 Counter-free Automata ..... 25
D Proofs for Expressiveness Upper Bounds (Section 3.3) ..... 26
D. 1 Proofs for Theorem D. 1 ..... 26
D. 2 Proofs for Theorem D. 2 ..... 28
E Proofs for Expressiveness Lower Bounds (Section 3.4) ..... 29
E. 1 Proof of Theorem 3.3 ..... 29
E. 2 Proof of Theorems 3.7 and 3.8 ..... 32
E. 3 Proof of Theorem 3.9 ..... 35
E. 4 Auxiliary Lemmas ..... 35
F Discussion on Variants in Transformer Architecture ..... 37
F. 1 Extension to Transformers with LayerNorm ..... 37
F. 2 Extension to Transformers with Multihead Attention ..... 37
G Discussion on Non-uniformity ..... 37</p>
<h1>A Additional Experimental Results</h1>
<p>In this section present the experimental results for base setting which is omitted in the main paper and the details of training and each task. We use the nanogpt ${ }^{6}$ codebase for language modeling.</p>
<p>Training Details. For all settings we use Adam with $10^{-5}$ learning rate, 0 weight decay, $\beta_{1}=0.9, \beta_{2}=0.95$, and gradient clipping with threshold equal to 1.0. The total training budget is $10^{6}$ steps and we use a linear warmup in the first 2000 steps starting from $10^{-6}$. For each step, we use a fresh sampled batch of size 64 from population distribution. We turn off dropout and use float16. We vary the depth of the transformer for different settings while the embedding size and the number of attention heads are fixed to be 512 and 8 respectively.</p>
<p>Below we present the setting and the experimental results of each problem respectively.
Modular Addition ( $C_{p}$ ). Given any positive integer $p$, the vocabulary of modular addition problem is ${0,1, \ldots, p-1,=}$. We generate $x=\left(x_{1}, \ldots, x_{n}\right)$ in the following way: for each $i \in[n-1]$, we independently sample $x_{i}$ from ${0,1, \ldots, p-1}$ and set $x_{n}=$ ' $=$ '. The label is $f^{*}(x)=\sum_{i=1}^{n-1} x_{i} \bmod p$ and $\operatorname{CoT} c(x)$ is $\left(\sum_{i=1}^{k} x_{i} \bmod p\right)_{k=1}^{n-1}$. Unsurprisingly, this task is an easy task for transformers because attention can easily express the average function across different positions, and so is the sum function. Then the feedforward layers can compute the modulus of the sum and $m$. We note that the high training accuracy here is not contradictory with our Theorem 3.1, because our sequence length is not long enough and float16 is like log-precision. This intuitive argument is elegantly extended to all solvable groups by leveraging Khron-Rhodes decomposition theorem by Liu et al. (2022a).</p>
<p>Permutation Composition $\left(S_{p}\right)$. Given any $p \in \mathbb{N}^{+}$, the vocabulary of permutation composition problem is ${1, \ldots, p,(),,=}$. We pick $n=(p+2) m+1$ and generate $x=\left(x_{1}, \ldots, x_{n}\right)$ in the following way: for each $i \in[m]$, we set $x_{(p+2)(i-1)+1}$ as '(', $x_{(p+2) i}$ as ') and independently sample a random permutation over $[p], \sigma_{i}=\left(x_{(p+2)(i-1)+2}, \ldots, x_{(p+2)(i-1)+p+1}\right)$. We set $x_{n}$ to be ' $=$ '. Different from other settings which only have the label at one position, we have $p$ labels for this setting, which is the composition of $\sigma_{1} \circ \ldots \circ \sigma_{n}$. The $\operatorname{CoT} c(x)$ is the partial composition from $\sigma_{1}$ to $\sigma_{n}$.</p>
<p>As mentioned in Section 3, unless $\mathrm{TC}^{0}=\mathrm{NC}^{1}$, composition of $S_{p}$ cannot be computed by $\mathrm{TC}^{0}$ for any $p \geq 5$, since composition of $S_{p}$ implies the wording problem of $S_{p}$, which is $\mathrm{NC}^{1}$-complete under $\mathrm{AC}^{0}$ reductions. Since all constant-depth poly-embedding-size transformers can be simulated by $\mathrm{TC}^{0}$ circuits (Theorem 3.2), shallow transformers are not able to solve the composition problem of $S_{p}$ for $p \geq 5$. Our experimental results in Figure 3 matches this theoretic prediction very well.</p>
<p>Iterated Squaring (IS). Iterated squaring refers to the following problem: given integers $r, n, p$, we define the iterated squaring function $f_{r, p}(n) \triangleq r^{2^{n}} \bmod p$. It is often used as hardness assumptions in cryptography (Rivest et al., 1996; Lombardi \&amp; Vaikuntanathan, 2020) that iterated squaring cannot be solved in $n-o(n)$ time even with polynomial parallel processors under certain technical conditions (e.g., $p$ is the product of two primes of a certain magnitude and there is some requirement on the order of $r$ as an element of the multiplicative group of integers modulo $p$ ). In other words, people conjecture there is no faster parallel algorithm than doing squaring for $n$ times.</p>
<p>Circuit Value Problem (CVP). Circuit value problem is the computational problem of computing the output of a given Boolean circuit on a given input. It is complete for P under $\mathrm{AC}^{0}$-reductions. This means if one can solve CVP with constant-depth transformers (or any $\mathrm{TC}^{0}$ circuits), then any problem in P becomes solvable by $\mathrm{TC}^{0}$, which is widely believed to be impossible.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ https://github.com/karpathy/nanoGPT&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9028 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9028</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9028</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-221d43a7a43fc7687417ab3df528beccb1aed790</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/221d43a7a43fc7687417ab3df528beccb1aed790" target="_blank">Improving LLM Leaderboards with Psychometrical Methodology</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper demonstrates the advantages of applying contemporary psychometric methodologies - originally developed for human tests and surveys - to improve the ranking of large language models on leaderboards, using the Hugging Face Leaderboard as an example.</p>
                <p><strong>Paper Abstract:</strong> The rapid development of large language models (LLMs) has necessitated the creation of benchmarks to evaluate their performance. These benchmarks resemble human tests and surveys, as they consist of sets of questions designed to measure emergent properties in the cognitive behavior of these systems. However, unlike the well-defined traits and abilities studied in social sciences, the properties measured by these benchmarks are often vaguer and less rigorously defined. The most prominent benchmarks are often grouped into leaderboards for convenience, aggregating performance metrics and enabling comparisons between models. Unfortunately, these leaderboards typically rely on simplistic aggregation methods, such as taking the average score across benchmarks. In this paper, we demonstrate the advantages of applying contemporary psychometric methodologies - originally developed for human tests and surveys - to improve the ranking of large language models on leaderboards. Using data from the Hugging Face Leaderboard as an example, we compare the results of the conventional naive ranking approach with a psychometrically informed ranking. The findings highlight the benefits of adopting psychometric techniques for more robust and meaningful evaluation of LLM performance.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9028.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9028.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ARC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI2 Reasoning Challenge (ARC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark of grade-school science multiple-choice questions (~2,590 items) designed to test commonsense knowledge and deeper text comprehension; administered in a 25-shot evaluation on the Hugging Face Leaderboard v1.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>AI2 Reasoning Challenge (ARC)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Grade-school science MC questions probing commonsense and deeper comprehension (2,590 items); in this paper evaluated in a 25-shot setting and reported as a parcel (average accuracy) on the Hugging Face Leaderboard v1.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Parceled average accuracies reported on the Hugging Face Leaderboard for 3,792 LLMs (dataset used in Study 1); the paper does not report specific numeric accuracy values for ARC in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline comparison presented in this paper for ARC; analysis focuses on inter-benchmark factor structure across many LLMs rather than per-model vs human comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluation retrieved from Hugging Face Leaderboard v1 (data snapshot Nov 30, 2024); administered in a 25-shot few-shot format; results were parceled (averaged within benchmark) before psychometric analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No per-model numeric performances for ARC are reported here; parceling collapses many items to an average which loses item-level detail; no human normative data provided in this paper for direct comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving LLM Leaderboards with Psychometrical Methodology', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9028.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9028.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HellaSwag</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HellaSwag</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large commonsense natural language inference multiple-choice dataset (70,000 items) focusing on next-event/narrative commonsense reasoning; in the Hugging Face Leaderboard v1 it was evaluated in a 10-shot manner and used as a parcel in psychometric modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>HellaSwag</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Commonsense NLI multiple-choice questions assessing narrative/commonsense reasoning (70k items); administered in 10-shot setting on the leaderboard and summarized as a parcel (average accuracy) for factor analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Parceled average accuracies reported on the Hugging Face Leaderboard for 3,792 LLMs (Study 1); no per-benchmark numeric accuracies are provided in the paper text.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No direct human baseline reported in this paper; psychometric analysis shows HellaSwag loads strongly on the general factor but has a negative residual correlation with TruthfulQA.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot (10-shot) multiple-choice evaluation; data were taken from Hugging Face Leaderboard v1 and parceled (benchmarks averaged) prior to factor analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Paper notes a negative residual correlation between HellaSwag and TruthfulQA (suggesting trade-offs between commonsense-style performance and factual/truthful responses); no human baseline or detailed per-model scores provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving LLM Leaderboards with Psychometrical Methodology', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9028.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9028.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MMLU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Massive Multitask Language Understanding (MMLU)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A broad multiple-choice benchmark of 15,908 questions across 57 topics spanning elementary math to U.S. foreign policy used as a proxy for general knowledge; included as a parcel in the Hugging Face Leaderboard analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MMLU</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Large multi-domain MC benchmark (15,908 items, 57 subjects) assessing general knowledge across many topics; used as a parcel on Hugging Face leaderboards and included in both Study 1 and Study 2 analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Parceled average accuracies reported on the Hugging Face Leaderboard (present in both v1 and v2 datasets); specific numeric accuracies are not reported in the paper excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No direct human baseline or per-model human-vs-LLM comparisons are reported in this paper; MMLU shows high loadings on the single general factor in FA.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Multiple-choice format; used as one parcel among others for factor analysis of leaderboard-reported average accuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Paper treats MMLU as part of a parceled set rather than analyzing item-level behavior; some STEM content in MMLU shares variance with GSM8K (positive residual correlation), indicating overlapping constructs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving LLM Leaderboards with Psychometrical Methodology', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9028.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9028.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of 684 multiple-choice questions across 38 topics measuring whether LLMs answer in ways that reflect false human beliefs or misconceptions (i.e., robustness to misinformation); evaluated in a 6-shot setting on the leaderboard and treated as a parcel in psychometric analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Benchmark targeting truthfulness/robustness to common human misconceptions across health, law, finance, politics (684 MC items, 38 topics); aims to detect answers that mirror human false beliefs rather than factual accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Parceled average accuracies reported on Hugging Face Leaderboard v1; the present paper does not provide numeric accuracy values for TruthfulQA.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline provided; psychometric analysis indicates TruthfulQA contains substantial factor-irrelevant variance and may measure a distinct aspect (robustness to bias) compared to other 'cognitive' benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluated in 6-shot few-shot format on the Hugging Face Leaderboard; included as a parcel for FA/CRM approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Paper emphasizes that TruthfulQA measures a different latent aspect (bias/robustness) and shows negative residual correlation with HellaSwag; parceling may obscure item-level patterns important for truthfulness evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving LLM Leaderboards with Psychometrical Methodology', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9028.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9028.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WinoGrande</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WinoGrande</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Winograd Schema-style dataset of 1,767 multiple-choice questions designed to measure commonsense coreference resolution from in-sentence context; evaluated in a 5-shot setting and included as a parcel in the analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>WinoGrande</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Winograd Schema-derived MC benchmark (1,767 items) assessing in-sentence commonsense reasoning and pronoun/coreference resolution; evaluated 5-shot on the leaderboard and aggregated as a parcel for factor analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Parceled average accuracies reported on Hugging Face Leaderboard v1; no numeric values are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No direct human baseline comparisons in this paper; WinoGrande loads positively on the general factor in the unidimensional model.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluation in 5-shot few-shot format; results taken from Hugging Face Leaderboard v1 and used as one of six parcels in Study 1.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Parcel-level analysis prevents examination of fine-grained error patterns; paper notes potential residual dependencies with other benchmarks but does not report WinoGrande-specific residual correlations beyond model-fitting diagnostics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving LLM Leaderboards with Psychometrical Methodology', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9028.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9028.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset of ~8,500 grade-school math word problems used to probe informal mathematical reasoning and stepwise solutions; evaluated in 5-shot on Hugging Face Leaderboard v1 and included as a parcel in psychometric modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Grade-school math word-problem benchmark (~8,500 items) assessing informal multistep arithmetic reasoning; administered in 5-shot setting on the leaderboard and represented as a parcel for FA.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Parceled average accuracies reported on the Hugging Face Leaderboard (Study 1); paper does not provide the specific numeric accuracies in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline provided; GSM8K had high factor loading and showed a positive residual correlation with MMLU, suggesting shared STEM-related variance.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluated 5-shot; parcel-level averages from Hugging Face Leaderboard v1 were used in factor-analytic models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Parceling hides item-level solution steps and chain-of-thought behavior; psychometric fit required adding residual covariance between GSM8K and MMLU to improve model fit.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving LLM Leaderboards with Psychometrical Methodology', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9028.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9028.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IFEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction-Following Evaluation (IFEval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approximately 500-item benchmark assessing LLMs' ability to follow explicit, verifiable instructions (e.g., produce word N times, exact phrase endings, JSON formatting); included in Hugging Face Leaderboard v2 dataset and analyzed as a parcel (Study 2).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Instruction-Following Evaluation (IFEval)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Benchmark of ~500 items focused on adherence to explicit verifiable instructions rather than content correctness (instruction-following domain); used as a parcel in the Hugging Face Leaderboard v2 factor analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Parceled average accuracies reported on Hugging Face Leaderboard v2 for 1,543 LLMs (Study 2); the paper's excerpt does not list numeric accuracies for IFEval.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline provided; IFEval loads moderately on the unidimensional factor in Study 2 (standardized loading ~0.587 as reported), indicating it relates to the general factor but is not identical to other benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Task items check explicit instruction adherence (e.g., repeating words, exact output formatting); scores parceled and analyzed via factor analysis (Study 2 initial model used MLR estimator).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>IFEval measures a different competence (instruction-following) distinct from classical cognitive tests; parceling and averaging may mask nuanced instruction-following failures and does not provide human baseline comparisons here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving LLM Leaderboards with Psychometrical Methodology', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9028.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9028.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BBH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Big Bench Hard (BBH)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curated subset of 6,511 difficult Big-Bench items grouped into 23 tasks covering multistep arithmetic, algorithmic reasoning, language understanding, and world knowledge; included as a parcel in Hugging Face Leaderboard v2 (Study 2).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Big Bench Hard (BBH)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Set of 6,511 challenging items from BigBench across 23 tasks (algorithmic, arithmetic, language understanding, world knowledge); used in v2 dataset and represented as a parcel in psychometric analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Parceled average accuracies on Hugging Face Leaderboard v2 (1,543 LLMs); numeric performance values are not provided in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No direct comparison to human baselines in this paper; BBH had a very high standardized loading (~0.981 reported in Study 2 initial model), indicating strong relation to the general factor.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>BBH parcel taken from Hugging Face Leaderboard v2; initial Study 2 FA used MLR estimation and included BBH among six parcels.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Paper does not report per-task or per-model results; parcel-level treatment masks heterogeneity across the 23 tasks and omits explicit human performance baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving LLM Leaderboards with Psychometrical Methodology', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9028.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9028.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MATH lvl 5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MATH level 5 (subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A subset (level 5) of the MATH benchmark containing ~12,500 high-school competition math problems (5,000 test items); used in Hugging Face Leaderboard v2 to probe higher-difficulty mathematical problem solving and included as a parcel in Study 2.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MATH level 5</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>High-school competition-style mathematics items (level 5 subset of MATH, ~12,500 items total with a 5,000-item test set) assessing advanced math problem solving and reasoning; included as a parcel in v2 analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Parceled average accuracies reported on Hugging Face Leaderboard v2 (Study 2); specific numeric accuracy values not provided in the paper excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline presented here; MATH had a relatively high unstandardized factor loading in Study 2 (unstandardized~1.542) but a moderate standardized loading (~0.513) in the initial model reported.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Items formatted with LaTeX/Asymptote; leaderboard includes level 5 items only; scores were parcelled for FA in Study 2.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Paper analyzes parcel-level aggregates, which obscure step-by-step solution behavior and chain-of-thought; no human baseline comparisons or per-item statistics are reported in this excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving LLM Leaderboards with Psychometrical Methodology', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9028.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9028.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graduate-Level Google-Proof Q&A (GPQA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of 448 challenging knowledge questions created by PhD-level experts across domains (biology, physics, chemistry), designed to be hard for laypersons even with Google but easier for experts; included as a parcel in Hugging Face Leaderboard v2.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Graduate-Level Google-Proof Q&A (GPQA)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>448 expert-crafted knowledge questions aimed at graduate/expert-level difficulty across STEM domains; tests deep domain knowledge rather than general commonsense reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Parceled average accuracies on Hugging Face Leaderboard v2 (Study 2); the paper does not provide numeric GPQA accuracy values in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human-baseline comparisons included; GPQA shows a high standardized loading (~0.845 in the Study 2 initial model) indicating close relation to the general factor in the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Included as one of six parcels analyzed in Study 2; raw scores used in Study 2a (and corrected versions in Study 2b for MC guess correction where applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No direct per-model reporting or human expert baselines given in this paper; parceling reduces visibility into specific question types where LLMs may fail or succeed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving LLM Leaderboards with Psychometrical Methodology', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9028.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e9028.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuSR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multistep Soft Reasoning (MuSR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark of algorithmically generated complex narrative problems (~756 items across categories) requiring long-range context parsing and multistep reasoning (murder mysteries, object placement, team allocation); included in Hugging Face Leaderboard v2 and analyzed as a parcel.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Multistep Soft Reasoning (MuSR)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Narrative-based multistep reasoning tasks (~1,000-word contexts) across categories (murder mysteries, object placement, team allocation) requiring long-range context and multi-step inference; used as a parcel in Study 2.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Parceled average accuracies reported on Hugging Face Leaderboard v2 (Study 2) for 1,543 LLMs; no numeric accuracy values are given in the paper excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baselines provided here; MuSR had a moderate standardized loading (~0.604 in Study 2 initial model), indicating relation to general factor but with distinctive variance.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Algorithmically generated long narrative items; included in Study 2 datasets and parcelled before factor analysis; some v2 benchmarks underwent anti-guessing correction in Study 2b analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Long narratives and multistep structure may yield complex residual dependencies not visible at parcel level; no human normative data reported in this paper for MuSR comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving LLM Leaderboards with Psychometrical Methodology', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9028.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e9028.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MMLU-PRO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MMLU-PRO (Massive Multitask Language Understanding - Professional)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An expert-refined MMLU variant with 12,032 multiple-choice items across 14 professional domains (math, physics, law, etc.) and 10 response alternatives per item; included as a parcel in Hugging Face Leaderboard v2.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MMLU-PRO</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Refined 12,032-item MMLU variant targeting professional domains with 10-choice MC format across 14 areas (math, physics, chemistry, law, engineering, economics, health, psychology, business, biology, computer science, philosophy, miscellaneous); used as parcel in v2 analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Parceled average accuracies reported on Hugging Face Leaderboard v2 for 1,543 LLMs; specific numeric values are not listed in the paper excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline comparisons are reported in this paper; MMLU-PRO had a very high standardized loading (~0.955 in Study 2 initial model), indicating strong alignment with the general factor.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>10-response multiple-choice format for each item; included in Study 2 factor analysis and noted among benchmarks where an anti-guessing correction was applied in some analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Anti-guessing correction (to set baseline random probability 1/O_i) affects interpretation; parceling conceals per-item difficulty and potential domain-specific human baselines which are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving LLM Leaderboards with Psychometrical Methodology', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>AI2 Reasoning Challenge (ARC) <em>(Rating: 2)</em></li>
                <li>HellaSwag <em>(Rating: 2)</em></li>
                <li>Massive Multitask Language Understanding (MMLU) <em>(Rating: 2)</em></li>
                <li>TruthfulQA <em>(Rating: 2)</em></li>
                <li>WinoGrande <em>(Rating: 2)</em></li>
                <li>GSM8K <em>(Rating: 2)</em></li>
                <li>Instruction-Following Evaluation (IFEval) <em>(Rating: 2)</em></li>
                <li>Big Bench Hard (BBH) <em>(Rating: 2)</em></li>
                <li>MATH level 5 <em>(Rating: 2)</em></li>
                <li>Graduate-Level Google-Proof Q&A (GPQA) <em>(Rating: 2)</em></li>
                <li>Multistep Soft Reasoning (MuSR) <em>(Rating: 2)</em></li>
                <li>MMLU-PRO <em>(Rating: 2)</em></li>
                <li>open_llm_leaderboard (Hugging Face Leaderboard) <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9028",
    "paper_id": "paper-221d43a7a43fc7687417ab3df528beccb1aed790",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "ARC",
            "name_full": "AI2 Reasoning Challenge (ARC)",
            "brief_description": "A benchmark of grade-school science multiple-choice questions (~2,590 items) designed to test commonsense knowledge and deeper text comprehension; administered in a 25-shot evaluation on the Hugging Face Leaderboard v1.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "test_battery_name": "AI2 Reasoning Challenge (ARC)",
            "test_description": "Grade-school science MC questions probing commonsense and deeper comprehension (2,590 items); in this paper evaluated in a 25-shot setting and reported as a parcel (average accuracy) on the Hugging Face Leaderboard v1.",
            "llm_performance": "Parceled average accuracies reported on the Hugging Face Leaderboard for 3,792 LLMs (dataset used in Study 1); the paper does not report specific numeric accuracy values for ARC in the main text.",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline comparison presented in this paper for ARC; analysis focuses on inter-benchmark factor structure across many LLMs rather than per-model vs human comparisons.",
            "experimental_details": "Evaluation retrieved from Hugging Face Leaderboard v1 (data snapshot Nov 30, 2024); administered in a 25-shot few-shot format; results were parceled (averaged within benchmark) before psychometric analysis.",
            "limitations_or_caveats": "No per-model numeric performances for ARC are reported here; parceling collapses many items to an average which loses item-level detail; no human normative data provided in this paper for direct comparison.",
            "uuid": "e9028.0",
            "source_info": {
                "paper_title": "Improving LLM Leaderboards with Psychometrical Methodology",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "HellaSwag",
            "name_full": "HellaSwag",
            "brief_description": "A large commonsense natural language inference multiple-choice dataset (70,000 items) focusing on next-event/narrative commonsense reasoning; in the Hugging Face Leaderboard v1 it was evaluated in a 10-shot manner and used as a parcel in psychometric modeling.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "test_battery_name": "HellaSwag",
            "test_description": "Commonsense NLI multiple-choice questions assessing narrative/commonsense reasoning (70k items); administered in 10-shot setting on the leaderboard and summarized as a parcel (average accuracy) for factor analysis.",
            "llm_performance": "Parceled average accuracies reported on the Hugging Face Leaderboard for 3,792 LLMs (Study 1); no per-benchmark numeric accuracies are provided in the paper text.",
            "human_baseline_performance": null,
            "performance_comparison": "No direct human baseline reported in this paper; psychometric analysis shows HellaSwag loads strongly on the general factor but has a negative residual correlation with TruthfulQA.",
            "experimental_details": "Few-shot (10-shot) multiple-choice evaluation; data were taken from Hugging Face Leaderboard v1 and parceled (benchmarks averaged) prior to factor analysis.",
            "limitations_or_caveats": "Paper notes a negative residual correlation between HellaSwag and TruthfulQA (suggesting trade-offs between commonsense-style performance and factual/truthful responses); no human baseline or detailed per-model scores provided here.",
            "uuid": "e9028.1",
            "source_info": {
                "paper_title": "Improving LLM Leaderboards with Psychometrical Methodology",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "MMLU",
            "name_full": "Massive Multitask Language Understanding (MMLU)",
            "brief_description": "A broad multiple-choice benchmark of 15,908 questions across 57 topics spanning elementary math to U.S. foreign policy used as a proxy for general knowledge; included as a parcel in the Hugging Face Leaderboard analyses.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "test_battery_name": "MMLU",
            "test_description": "Large multi-domain MC benchmark (15,908 items, 57 subjects) assessing general knowledge across many topics; used as a parcel on Hugging Face leaderboards and included in both Study 1 and Study 2 analyses.",
            "llm_performance": "Parceled average accuracies reported on the Hugging Face Leaderboard (present in both v1 and v2 datasets); specific numeric accuracies are not reported in the paper excerpt.",
            "human_baseline_performance": null,
            "performance_comparison": "No direct human baseline or per-model human-vs-LLM comparisons are reported in this paper; MMLU shows high loadings on the single general factor in FA.",
            "experimental_details": "Multiple-choice format; used as one parcel among others for factor analysis of leaderboard-reported average accuracies.",
            "limitations_or_caveats": "Paper treats MMLU as part of a parceled set rather than analyzing item-level behavior; some STEM content in MMLU shares variance with GSM8K (positive residual correlation), indicating overlapping constructs.",
            "uuid": "e9028.2",
            "source_info": {
                "paper_title": "Improving LLM Leaderboards with Psychometrical Methodology",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "TruthfulQA",
            "name_full": "TruthfulQA",
            "brief_description": "A set of 684 multiple-choice questions across 38 topics measuring whether LLMs answer in ways that reflect false human beliefs or misconceptions (i.e., robustness to misinformation); evaluated in a 6-shot setting on the leaderboard and treated as a parcel in psychometric analyses.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "test_battery_name": "TruthfulQA",
            "test_description": "Benchmark targeting truthfulness/robustness to common human misconceptions across health, law, finance, politics (684 MC items, 38 topics); aims to detect answers that mirror human false beliefs rather than factual accuracy.",
            "llm_performance": "Parceled average accuracies reported on Hugging Face Leaderboard v1; the present paper does not provide numeric accuracy values for TruthfulQA.",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline provided; psychometric analysis indicates TruthfulQA contains substantial factor-irrelevant variance and may measure a distinct aspect (robustness to bias) compared to other 'cognitive' benchmarks.",
            "experimental_details": "Evaluated in 6-shot few-shot format on the Hugging Face Leaderboard; included as a parcel for FA/CRM approximation.",
            "limitations_or_caveats": "Paper emphasizes that TruthfulQA measures a different latent aspect (bias/robustness) and shows negative residual correlation with HellaSwag; parceling may obscure item-level patterns important for truthfulness evaluation.",
            "uuid": "e9028.3",
            "source_info": {
                "paper_title": "Improving LLM Leaderboards with Psychometrical Methodology",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "WinoGrande",
            "name_full": "WinoGrande",
            "brief_description": "A Winograd Schema-style dataset of 1,767 multiple-choice questions designed to measure commonsense coreference resolution from in-sentence context; evaluated in a 5-shot setting and included as a parcel in the analyses.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "test_battery_name": "WinoGrande",
            "test_description": "Winograd Schema-derived MC benchmark (1,767 items) assessing in-sentence commonsense reasoning and pronoun/coreference resolution; evaluated 5-shot on the leaderboard and aggregated as a parcel for factor analysis.",
            "llm_performance": "Parceled average accuracies reported on Hugging Face Leaderboard v1; no numeric values are reported in this paper.",
            "human_baseline_performance": null,
            "performance_comparison": "No direct human baseline comparisons in this paper; WinoGrande loads positively on the general factor in the unidimensional model.",
            "experimental_details": "Evaluation in 5-shot few-shot format; results taken from Hugging Face Leaderboard v1 and used as one of six parcels in Study 1.",
            "limitations_or_caveats": "Parcel-level analysis prevents examination of fine-grained error patterns; paper notes potential residual dependencies with other benchmarks but does not report WinoGrande-specific residual correlations beyond model-fitting diagnostics.",
            "uuid": "e9028.4",
            "source_info": {
                "paper_title": "Improving LLM Leaderboards with Psychometrical Methodology",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "GSM8K",
            "name_full": "GSM8K",
            "brief_description": "A dataset of ~8,500 grade-school math word problems used to probe informal mathematical reasoning and stepwise solutions; evaluated in 5-shot on Hugging Face Leaderboard v1 and included as a parcel in psychometric modeling.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "test_battery_name": "GSM8K",
            "test_description": "Grade-school math word-problem benchmark (~8,500 items) assessing informal multistep arithmetic reasoning; administered in 5-shot setting on the leaderboard and represented as a parcel for FA.",
            "llm_performance": "Parceled average accuracies reported on the Hugging Face Leaderboard (Study 1); paper does not provide the specific numeric accuracies in the excerpt.",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline provided; GSM8K had high factor loading and showed a positive residual correlation with MMLU, suggesting shared STEM-related variance.",
            "experimental_details": "Evaluated 5-shot; parcel-level averages from Hugging Face Leaderboard v1 were used in factor-analytic models.",
            "limitations_or_caveats": "Parceling hides item-level solution steps and chain-of-thought behavior; psychometric fit required adding residual covariance between GSM8K and MMLU to improve model fit.",
            "uuid": "e9028.5",
            "source_info": {
                "paper_title": "Improving LLM Leaderboards with Psychometrical Methodology",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "IFEval",
            "name_full": "Instruction-Following Evaluation (IFEval)",
            "brief_description": "An approximately 500-item benchmark assessing LLMs' ability to follow explicit, verifiable instructions (e.g., produce word N times, exact phrase endings, JSON formatting); included in Hugging Face Leaderboard v2 dataset and analyzed as a parcel (Study 2).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "test_battery_name": "Instruction-Following Evaluation (IFEval)",
            "test_description": "Benchmark of ~500 items focused on adherence to explicit verifiable instructions rather than content correctness (instruction-following domain); used as a parcel in the Hugging Face Leaderboard v2 factor analysis.",
            "llm_performance": "Parceled average accuracies reported on Hugging Face Leaderboard v2 for 1,543 LLMs (Study 2); the paper's excerpt does not list numeric accuracies for IFEval.",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline provided; IFEval loads moderately on the unidimensional factor in Study 2 (standardized loading ~0.587 as reported), indicating it relates to the general factor but is not identical to other benchmarks.",
            "experimental_details": "Task items check explicit instruction adherence (e.g., repeating words, exact output formatting); scores parceled and analyzed via factor analysis (Study 2 initial model used MLR estimator).",
            "limitations_or_caveats": "IFEval measures a different competence (instruction-following) distinct from classical cognitive tests; parceling and averaging may mask nuanced instruction-following failures and does not provide human baseline comparisons here.",
            "uuid": "e9028.6",
            "source_info": {
                "paper_title": "Improving LLM Leaderboards with Psychometrical Methodology",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "BBH",
            "name_full": "Big Bench Hard (BBH)",
            "brief_description": "A curated subset of 6,511 difficult Big-Bench items grouped into 23 tasks covering multistep arithmetic, algorithmic reasoning, language understanding, and world knowledge; included as a parcel in Hugging Face Leaderboard v2 (Study 2).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "test_battery_name": "Big Bench Hard (BBH)",
            "test_description": "Set of 6,511 challenging items from BigBench across 23 tasks (algorithmic, arithmetic, language understanding, world knowledge); used in v2 dataset and represented as a parcel in psychometric analysis.",
            "llm_performance": "Parceled average accuracies on Hugging Face Leaderboard v2 (1,543 LLMs); numeric performance values are not provided in the excerpt.",
            "human_baseline_performance": null,
            "performance_comparison": "No direct comparison to human baselines in this paper; BBH had a very high standardized loading (~0.981 reported in Study 2 initial model), indicating strong relation to the general factor.",
            "experimental_details": "BBH parcel taken from Hugging Face Leaderboard v2; initial Study 2 FA used MLR estimation and included BBH among six parcels.",
            "limitations_or_caveats": "Paper does not report per-task or per-model results; parcel-level treatment masks heterogeneity across the 23 tasks and omits explicit human performance baselines.",
            "uuid": "e9028.7",
            "source_info": {
                "paper_title": "Improving LLM Leaderboards with Psychometrical Methodology",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "MATH lvl 5",
            "name_full": "MATH level 5 (subset)",
            "brief_description": "A subset (level 5) of the MATH benchmark containing ~12,500 high-school competition math problems (5,000 test items); used in Hugging Face Leaderboard v2 to probe higher-difficulty mathematical problem solving and included as a parcel in Study 2.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "test_battery_name": "MATH level 5",
            "test_description": "High-school competition-style mathematics items (level 5 subset of MATH, ~12,500 items total with a 5,000-item test set) assessing advanced math problem solving and reasoning; included as a parcel in v2 analyses.",
            "llm_performance": "Parceled average accuracies reported on Hugging Face Leaderboard v2 (Study 2); specific numeric accuracy values not provided in the paper excerpt.",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline presented here; MATH had a relatively high unstandardized factor loading in Study 2 (unstandardized~1.542) but a moderate standardized loading (~0.513) in the initial model reported.",
            "experimental_details": "Items formatted with LaTeX/Asymptote; leaderboard includes level 5 items only; scores were parcelled for FA in Study 2.",
            "limitations_or_caveats": "Paper analyzes parcel-level aggregates, which obscure step-by-step solution behavior and chain-of-thought; no human baseline comparisons or per-item statistics are reported in this excerpt.",
            "uuid": "e9028.8",
            "source_info": {
                "paper_title": "Improving LLM Leaderboards with Psychometrical Methodology",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "GPQA",
            "name_full": "Graduate-Level Google-Proof Q&A (GPQA)",
            "brief_description": "A set of 448 challenging knowledge questions created by PhD-level experts across domains (biology, physics, chemistry), designed to be hard for laypersons even with Google but easier for experts; included as a parcel in Hugging Face Leaderboard v2.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "test_battery_name": "Graduate-Level Google-Proof Q&A (GPQA)",
            "test_description": "448 expert-crafted knowledge questions aimed at graduate/expert-level difficulty across STEM domains; tests deep domain knowledge rather than general commonsense reasoning.",
            "llm_performance": "Parceled average accuracies on Hugging Face Leaderboard v2 (Study 2); the paper does not provide numeric GPQA accuracy values in the excerpt.",
            "human_baseline_performance": null,
            "performance_comparison": "No human-baseline comparisons included; GPQA shows a high standardized loading (~0.845 in the Study 2 initial model) indicating close relation to the general factor in the dataset.",
            "experimental_details": "Included as one of six parcels analyzed in Study 2; raw scores used in Study 2a (and corrected versions in Study 2b for MC guess correction where applicable).",
            "limitations_or_caveats": "No direct per-model reporting or human expert baselines given in this paper; parceling reduces visibility into specific question types where LLMs may fail or succeed.",
            "uuid": "e9028.9",
            "source_info": {
                "paper_title": "Improving LLM Leaderboards with Psychometrical Methodology",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "MuSR",
            "name_full": "Multistep Soft Reasoning (MuSR)",
            "brief_description": "A benchmark of algorithmically generated complex narrative problems (~756 items across categories) requiring long-range context parsing and multistep reasoning (murder mysteries, object placement, team allocation); included in Hugging Face Leaderboard v2 and analyzed as a parcel.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "test_battery_name": "Multistep Soft Reasoning (MuSR)",
            "test_description": "Narrative-based multistep reasoning tasks (~1,000-word contexts) across categories (murder mysteries, object placement, team allocation) requiring long-range context and multi-step inference; used as a parcel in Study 2.",
            "llm_performance": "Parceled average accuracies reported on Hugging Face Leaderboard v2 (Study 2) for 1,543 LLMs; no numeric accuracy values are given in the paper excerpt.",
            "human_baseline_performance": null,
            "performance_comparison": "No human baselines provided here; MuSR had a moderate standardized loading (~0.604 in Study 2 initial model), indicating relation to general factor but with distinctive variance.",
            "experimental_details": "Algorithmically generated long narrative items; included in Study 2 datasets and parcelled before factor analysis; some v2 benchmarks underwent anti-guessing correction in Study 2b analysis.",
            "limitations_or_caveats": "Long narratives and multistep structure may yield complex residual dependencies not visible at parcel level; no human normative data reported in this paper for MuSR comparisons.",
            "uuid": "e9028.10",
            "source_info": {
                "paper_title": "Improving LLM Leaderboards with Psychometrical Methodology",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "MMLU-PRO",
            "name_full": "MMLU-PRO (Massive Multitask Language Understanding - Professional)",
            "brief_description": "An expert-refined MMLU variant with 12,032 multiple-choice items across 14 professional domains (math, physics, law, etc.) and 10 response alternatives per item; included as a parcel in Hugging Face Leaderboard v2.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "test_battery_name": "MMLU-PRO",
            "test_description": "Refined 12,032-item MMLU variant targeting professional domains with 10-choice MC format across 14 areas (math, physics, chemistry, law, engineering, economics, health, psychology, business, biology, computer science, philosophy, miscellaneous); used as parcel in v2 analyses.",
            "llm_performance": "Parceled average accuracies reported on Hugging Face Leaderboard v2 for 1,543 LLMs; specific numeric values are not listed in the paper excerpt.",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline comparisons are reported in this paper; MMLU-PRO had a very high standardized loading (~0.955 in Study 2 initial model), indicating strong alignment with the general factor.",
            "experimental_details": "10-response multiple-choice format for each item; included in Study 2 factor analysis and noted among benchmarks where an anti-guessing correction was applied in some analyses.",
            "limitations_or_caveats": "Anti-guessing correction (to set baseline random probability 1/O_i) affects interpretation; parceling conceals per-item difficulty and potential domain-specific human baselines which are not provided here.",
            "uuid": "e9028.11",
            "source_info": {
                "paper_title": "Improving LLM Leaderboards with Psychometrical Methodology",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "AI2 Reasoning Challenge (ARC)",
            "rating": 2,
            "sanitized_title": "ai2_reasoning_challenge_arc"
        },
        {
            "paper_title": "HellaSwag",
            "rating": 2
        },
        {
            "paper_title": "Massive Multitask Language Understanding (MMLU)",
            "rating": 2,
            "sanitized_title": "massive_multitask_language_understanding_mmlu"
        },
        {
            "paper_title": "TruthfulQA",
            "rating": 2,
            "sanitized_title": "truthfulqa"
        },
        {
            "paper_title": "WinoGrande",
            "rating": 2,
            "sanitized_title": "winogrande"
        },
        {
            "paper_title": "GSM8K",
            "rating": 2
        },
        {
            "paper_title": "Instruction-Following Evaluation (IFEval)",
            "rating": 2,
            "sanitized_title": "instructionfollowing_evaluation_ifeval"
        },
        {
            "paper_title": "Big Bench Hard (BBH)",
            "rating": 2,
            "sanitized_title": "big_bench_hard_bbh"
        },
        {
            "paper_title": "MATH level 5",
            "rating": 2,
            "sanitized_title": "math_level_5"
        },
        {
            "paper_title": "Graduate-Level Google-Proof Q&A (GPQA)",
            "rating": 2,
            "sanitized_title": "graduatelevel_googleproof_qa_gpqa"
        },
        {
            "paper_title": "Multistep Soft Reasoning (MuSR)",
            "rating": 2,
            "sanitized_title": "multistep_soft_reasoning_musr"
        },
        {
            "paper_title": "MMLU-PRO",
            "rating": 2
        },
        {
            "paper_title": "open_llm_leaderboard (Hugging Face Leaderboard)",
            "rating": 2,
            "sanitized_title": "openllmleaderboard_hugging_face_leaderboard"
        }
    ],
    "cost": 0.016573249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Improving LLM Leaderboards with Psychometrical Methodology</h1>
<p>Denis Federiakin, Ph.D.;
Department of Business and Economics Education, Johannes Gutenberg University Mainz, Germany, denis.federiakin@uni-mainz.de;
Institute of Psychology, Goethe University Frankfurt, Germany, federiakin@psych.unifrankfurt.de.</p>
<h4>Abstract</h4>
<p>The rapid development of large language models (LLMs) has necessitated the creation of benchmarks to evaluate their performance. These benchmarks resemble human tests and surveys, as they consist of sets of questions designed to measure emergent properties in the cognitive behavior of these systems. However, unlike the well-defined traits and abilities studied in social sciences, the properties measured by these benchmarks are often vaguer and less rigorously defined. The most prominent benchmarks are often grouped into leaderboards for convenience, aggregating performance metrics and enabling comparisons between models. Unfortunately, these leaderboards typically rely on simplistic aggregation methods, such as taking the average score across benchmarks.</p>
<p>In this paper, we demonstrate the advantages of applying contemporary psychometric methodologies - originally developed for human tests and surveys - to improve the ranking of large language models on leaderboards. Using data from the Hugging Face Leaderboard as an example, we compare the results of the conventional nave ranking approach with a psychometrically informed ranking. The findings highlight the benefits of adopting psychometric techniques for more robust and meaningful evaluation of LLM performance.</p>
<h1>1. Introduction</h1>
<p>Ever since the introduction of ChatGPT by OpenAI in the fall of 2022, Artificial Intelligence (AI)-enhanced chatbots based on Large Language Models (LLMs) have become game-changers in many areas of human activity, revolutionizing them across the board (e.g., Iu \&amp; Wong, 2023; Mehnen et al., 2023; Biswas, 2023). The potential of AI-assisted tools to facilitate and accelerate the execution of many professional and everyday-life functions is rooted in the cognitive capacities of LLMs to act as virtually universal assistants in information processing. This has also paved the way for the constant and rapid improvement of the cognitive performance of AI-assisted tools, supported by the steady development and release of different LLMs.</p>
<p>Correspondingly, the need for testing and comparing different LLMs has emerged. The necessity for evidence-based comparison of the capabilities of various LLMs has resulted in the rise of benchmarks - sets of tasks and questions provided to the models in natural (or visual) language, requiring LLMs to generate responses. These responses are then judged on their correctness, as the questions are presumed to have definitive answers (similar to human tests). As a result, thousands of specific benchmarks have been developed over the past few years (Guo et al., 2023; Chang et al., 2023). By now, benchmarks for LLMs exist in virtually every professional field, cognitive process, or aspect of ethics.</p>
<p>One such benchmark, MMLU (Hendrycks et al., 2020), has gained special popularity. MMLU contains 15,908 multiple-choice questions covering 57 different topics, ranging from high elementary mathematics to U.S. foreign policy. This breadth allows MMLU to serve as a proxy for assessing the general awareness of LLMs about the world - essentially a measure of their general knowledge. However, MMLU is far from the only popular benchmark.</p>
<p>With the diversification of various benchmarks, the problem of systematizing information on LLM performance has also arisen. With new benchmarks and LLMs appearing almost daily, the issue of comparing and integrating information from these benchmarks has become increasingly important. Correspondingly, multiple LLM leaderboards have emerged. These leaderboards openly publish information on how well various LLMs perform across a selected set of benchmarks. As a result, LLM leaderboards have become one of the most important and trustworthy sources of information on the relative capabilities of different LLMs.</p>
<p>One such leaderboard - the Hugging Face Leaderboard (Beeching et al., 2023; https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) - is particularly significant due to its community support and popularity. This leaderboard has now become one of the gold standards for LLM comparison.</p>
<p>Yet, despite the widespread popularity and attention surrounding LLMs in general and LLM benchmarking in particular, there is a surprising lack of literature that examines the quality of the benchmarks being used (Wang et al., 2023) or the ranks provided by the leaderboards. Such quality can be analyzed using elements of psychometric methodology, which aims to ensure high-quality information for decision-making (e.g., ranking individuals based on some ability or trait). This methodology is responsible for the development of key psychological concepts such as IQ and the Big Five personality theory, having been carefully refined over the past century.</p>
<p>In general, psychometricians enhance various tests and surveys by refining item formulations and statistically analyzing their performance to ensure that the claims based on test results are valid and reliable. However, to the best of our knowledge, there are almost no studies on the quality of the prominent leaderboards. This paper aims to address this gap.</p>
<p>The primary purpose of this paper is to investigate the psychometric quality of the Hugging Face Leaderboard. We begin by discussing the differences between test development and benchmark development practices, highlighting the advantages and shortcomings of each. Next, we provide a brief introduction to psychometric modeling, which is used to establish validity evidence for human tests and surveys. Following this, we describe the methodology of the study.</p>
<p>We then apply this modeling approach to the performance data of various LLMs on Hugging Face Leaderboard v. 1 (Study 1) and v. 2 (Study 2). Subsequently, we compare the reported average scores from the leaderboards to the estimated factor scores obtained in Studies 1 and 2, and we discuss their differences (Study 3). Finally, we present a discussion on the future of psychometric analysis in evaluating LLM benchmarking data.</p>
<h1>2. Test-development vs. benchmark-development</h1>
<p>The entirety of the test development practice can be roughly summed up as proving that a test measures what it is intended to measure. This is referred to as providing validity evidence for the specific types of claims made about respondents based on the test results (AERA, APA, NCME, 2014). The focus on the types of claims is crucial, as a test developed for, say, research purposes might be unsuitable for use in clinical practice (Truijens et al., 2019). Hence, the purpose of test application (i.e., the targeted claim) is critical for constructing validity arguments.</p>
<p>There are numerous approaches to systematizing this process, but Evidence-Centered Design (ECD) is one of the most prominent in education, social sciences, and cognitive sciences (Riconscente et al., 2015; Mislevy et al., 2003; Mislevy \&amp; Haertel, 2006). The aim of</p>
<p>ECD is to eliminate alternative explanations for the targeted claims. For instance, if testing results suggest that respondent A has lower science literacy than respondent B , test developers must demonstrate that:</p>
<ol>
<li>The test results genuinely reflect science literacy (as defined by the test developers) and not, for example, mathematical ability, general intelligence, or any other construct.</li>
<li>The test results differ due to a systematic difference in science literacy between respondents, rather than due to randomness in responses (i.e., differences attributable to the standard error of measurement) or construct-irrelevant factors (such as demographic differences).</li>
</ol>
<p>Each of these conclusions is based on numerous lower-level conclusions and studies, as addressing them requires unpacking a multitude of intertwined concepts and phenomena from social and cognitive science. As a result, ECD is a highly complex methodology that effectively integrates the test development process with the gathering of validity evidence. However, when properly implemented, it ensures that the test meets the Standards for Educational and Psychological Testing (AERA, APA, NCME, 2014). This means that it is clear what the test measures and how precise it is in accomplishing its intended purpose.</p>
<p>This approach essentially reflects the reasoning behind the causal theory of measurement (Markus \&amp; Borsboom, 2013; Mari et al., 2023). According to this perspective, when developing any measurement instrument, one must demonstrate that the results of the measurement are caused by the characteristic the instrument is intended to measure. Applied to test development using the ECD framework, this means that once psychometricians and test developers have ruled out competing explanations for the causes of the observed item responses, the only remaining explanation is the targeted characteristic.</p>
<p>This naturally brings up the issue of social constructivism (see Fried, 2017). Social constructivism essentially means that the studied phenomenon does not exist by itself, but the way it unfolds is defined by the approach taken to interact with it. In other words, the phenomenon is not "set in stone" but is rather built up (constructed) while it is studied. For example, researcher A can define mathematical ability as proficiency in addition and subtraction, while researcher B can define it as proficiency in division and multiplication. Then, the empirical consequences of this theoretical difference can arise to the point of their contradictions even though both constructs are called mathematical ability. This is a special type of problem in itself as it creates a number of theories that are seemingly similar, but describe different aspects of the same or maybe even different phenomena (Elson et al., 2023). However, this results in all tests with convincing validity evidence having a clear and detailed</p>
<p>definitions of what they intend to measure - it is a side-product of test-developers agreeing on what they mean when they name the construct.</p>
<p>The development of benchmarks, however, follows another path. While not said explicitly in any of the papers published alongside the benchmarks, the authors, apparently, ground their philosophy of measurement in representativism. In social science measurements, representativism assumes that the items exhaust all aspects (or at least are a representative sample from all possible aspects) of the underlying quality that the test targets. For example, according to this perspective, the length of the table is represented by the reading on the ruler. At the same time, within the causal approach to measurement this idea would sound as "the length of the table causes readings on the ruler". While this might appear as a semantic wordplay, this slight philosophical difference has large practical consequences for the measurement design and mathematical modeling used for it (Markus \&amp; Borsboom, 2013).</p>
<p>This naturally raises the issue of social constructivism (see Fried, 2017). Social constructivism essentially posits that the phenomenon under study does not exist independently but is shaped by the approach taken to interact with it. In other words, the phenomenon is not "set in stone" but is instead constructed during the process of studying it. For example, researcher A might define mathematical ability as proficiency in addition and subtraction, while researcher B might define it as proficiency in division and multiplication. The empirical consequences of these differing theoretical definitions can lead to contradictions, even though both constructs are referred to as mathematical ability.</p>
<p>This is a unique problem because it creates multiple theories that appear similar but actually describe different aspects of the same - or potentially even different - phenomena (Elson et al., 2023). However, this issue has a silver lining: all tests with convincing validity evidence are accompanied by clear and detailed definitions of what they intend to measure. This is a natural byproduct of test developers agreeing on what they mean when they define a construct.</p>
<p>The development of benchmarks, however, follows a different trajectory. While this is not explicitly stated in the papers accompanying benchmarks, it appears that their authors ground their philosophy of measurement in representativism. In social science measurement, representativism assumes that the items in a test exhaust all aspects - or at least constitute a representative sample of all possible aspects - of the underlying quality the test aims to measure. For example, from this perspective, the length of a table is represented by the reading on a ruler.</p>
<p>In contrast, the causal approach to measurement would frame this differently, stating that "the length of the table causes the readings on the ruler". While this may seem like semantic wordplay, this subtle philosophical distinction has significant practical consequences for measurement design and the mathematical modeling used to support it (Markus \&amp; Borsboom, 2013).</p>
<p>This is not to say that one approach is better than the other; rather, each is suited to different measurement contexts. While the "length of the table" example does not fully illustrate the differences between the approaches, the measurement of more complex properties does. For instance, to measure a complex LLM's "skills" from the representativist paradigm, one would need an enormous number of questions in a benchmark to capture as many aspects of the skill as possible. This requirement is evident from the general tradition of benchmark development - they typically consist of an extensive number of questions. While administering such a large number of test items to human respondents is impractical, the infinite stamina of LLMs allows benchmarks to adhere to the representativist measurement tradition.</p>
<p>However, this approach poses challenges - not because the number of measures needs to be vast, but because it must be sufficiently large to fully reflect all facets of the targeted characteristic. Herein lies the problem: elusive characteristics such as "mathematical reasoning," "higher-order reasoning," or "intelligence" have infinitely many aspects since the number of situations in which these characteristics manifest is infinite. Consequently, no set of items, no matter how large, can be definitively proven to represent the entirety of the target characteristic. This limitation means that even extensive question sets can systematically omit critical aspects of the phenomena under investigation.</p>
<p>Precisely because of this issue, assessment in education, social, and cognitive sciences has shifted toward the causal theory of measurement. According to this theory, it is not the size of the observation set that matters but the cause of the outcomes. A clear construct definition, coupled with a solid theoretical justification for item development, defines the essence of the construct to be measured. This approach liberates the test developer from the need to represent all possible aspects of the targeted trait; instead, only those aspects with well-defined parameters need to be measured.</p>
<p>These philosophical differences dictate many operational distinctions in benchmark and test development. Tests developed using the causal approach generally exhibit much clearer construct definitions and theoretical frameworks. Only a handful of benchmarks can match the level of clarity in answering the question "What is being measured?" that social science tests provide (Wang et al., 2023; e.g., Fei et al., 2023; Kardanova et al., 2024). However, this</p>
<p>approach to test development is significantly more labor-intensive than benchmark development. In benchmark development, one can simply clone an item numerous times - an operation that, in ECD terms, involves generating observable indicators of the measured characteristic across various item contexts.</p>
<p>Nonetheless, the radically representativist approach of benchmark development does offer certain advantages. For instance, the standard error of measurement becomes vanishingly small with sufficiently long tests (e.g., Linacre, 1995). Thus, reliability of measurement is generally not a concern in this domain.</p>
<p>However, another challenge in benchmark development is sample size. For example, if a custom LLM is tuned for a very specific task, its progress can be tracked against a custom benchmark, even if the sample size (i.e., the number of LLMs) is just one. In contrast, psychometric statistical modeling requires a larger sample size, typically involving multiple respondents. This fundamental requirement makes psychometric modeling generally infeasible in such cases. This is one of the reasons why psychometric modeling has not yet been applied to analyze the quality of benchmarks or leaderboards.</p>
<h1>3. A brief summary of psychometric modeling</h1>
<p>The essence of psychometric modeling, as applied in this paper, can be defined as the controllable dimensionality reduction of an item set to a comprehensible number of dimensions using statistical models whose assumptions and interpretations align with the theoretical framework of the characteristic being measured. This means that making a claim about respondents based on 60 items is much more complex than doing so based on a single variable (e.g., ability). To reduce a relatively large number of items to fewer dimensions, specialized statistical models are employed. A key feature of these models is that they are interpretable in the context of the theory underlying the construct. For example, if a construct is presumed to consist of several interconnected traits, the model should reduce the observed variables to exactly this number of traits. Furthermore, since items are often designed to measure only specific traits rather than all traits simultaneously, the dimension reduction process should account for this aspect. This reflects the confirmatory (or reflective) approach to psychometric modeling (Hanafiah, 2020).</p>
<p>It is important to note that psychometric modeling encompasses multiple paradigms. One paradigm that appears to dominate the field is parametric latent variable modeling, which assumes the existence of a latent variable underlying several observed variables (Cai, 2012). Psychometric models in this paradigm link the observed variables (item responses) to the latent variable (e.g., ability) and are used to derive individual estimates of this latent variable.</p>
<p>However, this is by no means the only paradigm employed by psychometricians. Depending on the type of data, the intended claims about respondents, the purpose of the modeling, and the resources available, other paradigms can be applied.</p>
<p>These include non-parametric modeling of latent variables (Sijtsma \&amp; Van Der Ark, 2022), Classical Test Theory (CTT, Crocker \&amp; Algina, 1986; which seems to dominate the LLM benchmarking field), Generalizability Theory (Jiang, 2018), Network Modeling (Marsman et al., 2018; Costantini et al., 2015), and others. Notably, while CTT uses observed scores (e.g., item sums or averages), it is still based on statistical modeling and is not free from statistical assumptions (Novick, 1966). In this regard, parametric modeling of latent variables is not inherently "better" than CTT; rather, it is suited to solving certain problems and addressing specific questions that CTT cannot (Hambleton \&amp; Jones, 1993).</p>
<p>Here, we will elaborate on just two of these advantages: advanced ability estimation and test quality analysis. Simple averaging of responses to different test items assumes that all items are interchangeable. In other words, it assumes that all items have the same level of difficulty - a typical assumption in many equations of Classical Test Theory (CTT). This assumption, however, is almost never true. Psychometric latent variable models account for this by recognizing that correct responses to more difficult items should "count more" than responses to easier items. Furthermore, these models typically assume that items differ in their sensitivity to the latent variable. In other words, different items vary in their usefulness for estimating the targeted ability - some provide a lot of (Fisher) information (Muraki, 1993) about the ability, while others provide less. This approach allows psychometric models to account for differences not only across respondents but also across items, by describing items in terms of several properties (parameters). This enables the identification and exclusion of items that are nearly useless or even detrimental to ability estimation, such as items where the probability of a correct response decreases as ability increases.</p>
<p>The first major benefit of the most popular psychometric models is that they enhance the precision of the scale used to reflect ability. Because items differ in difficulty, the average (or sum) score across items operates on an ordinal scale (Stevens, 1946), as it reflects the number of items solved correctly, rather than the underlying cause of the responses. By contrast, one of the most widely used psychometric models, Item Response Theory (IRT), assumes that responding to an item correctly is a random event with a probability that depends on the latent variable (ability). Consequently, the estimates of latent ability derived from IRT are placed on an interval (metric) scale, defined by a logit-transformed probability scale. This</p>
<p>provides the estimates with a unit of measurement, significantly enhancing the range of analyses that can be performed on this scale and improving the quality of respondent rankings.</p>
<p>This improvement, however, comes at the cost of several parametric assumptions required by these models. While the use of Stevens' (1946) scale classification is generally criticized by contemporary psychometricians (Zumbo \&amp; Kroc, 2019; Thomas, 2019), it remains helpful for understanding the comparative advantages and disadvantages of different approaches. Additionally, such advanced ability estimates filter out item-specific noise, allowing for the precise ranking of respondents based solely on variance common across items.</p>
<p>There are several families of models for parametric modeling of latent variables. For example, Item Response Theory (IRT; Van der Linden, 2018) maps discrete item responses to continuous latent traits, while Factor Analysis (FA; Brown, 2015) maps continuous responses to continuous latent traits. Psychometrics also employs specialized types of finite mixture modeling: Latent Class Analysis (Eshima, 2022) and Cognitive Diagnostic Modeling (von Davier \&amp; Lee, 2019) are used to map discrete responses to discrete latent characteristics, while models such as Latent Profile Analysis (Oberski, 2016) map continuous responses to discrete latent characteristics. Each of these families varies further in terms of their preferred estimation techniques.</p>
<p>Over the years, psychometricians have developed and routinely employed a variety of estimation methods, including Maximum Likelihood estimators (Chen \&amp; Zhang, 2021), Least Squares estimators (Savalei \&amp; Rosseel, 2022), Bayesian samplers (Levy \&amp; Mislevy, 2017; Wu et al., 2020), and regularization techniques (Robitzsch, 2023), as well as numerous modifications and combinations tailored to specific models. More recently, backpropagation has been proposed as a technique for estimating psychometric models (Urban \&amp; Bauer, 2021; Converse, 2021).</p>
<p>Particular attention, however, is given by psychometricians to global (model-level; Goretzko et al., 2024; Cai \&amp; Monroe, 2014) and local (item- and person-level; Khler et al., 2020; Chalmers \&amp; Ng, 2017; Mller, 2020) model fit analysis. These procedures are crucial for verifying whether the theoretical assumptions underlying model development hold true in the observed data. Each branch of psychometric modeling contains countless models that differ in their assumptions about the data. Psychometricians continually strive to strengthen the connection between theoretical assumptions derived from educational, cognitive, and social sciences and the assumptions made by mathematical models.</p>
<p>Further elaboration on the broader purposes of psychometrics is beyond the scope of this paper.</p>
<h1>4. Methods and Data</h1>
<h3>4.1 Analysis methodolody</h3>
<p>In the case of applying psychometric models to LLM benchmarking data, there is a significant challenge of having more parameters than observations. Psychometric models estimate at least one parameter per observed variable (benchmark question) in Rasch (1960) models, and typically two or more in most other models. However, given the enormous number of benchmarking questions, there are not enough LLMs in existence to ensure that the number of observations (LLMs) exceeds the number of model parameters.</p>
<p>A relatively simple approach to addressing this problem is parceling - collapsing groups of observed variables into a single variable (Matsunaga, 2008) through summation or averaging. While this practice has its limitations (Little et al., 2002), the parceled information is precisely what the Leaderboard provides: it reports the average accuracy of LLMs responding to relatively homogeneous groups of items (i.e., averages calculated within each benchmark). The relative homogeneity of these items allows for a better understanding of relationships between groups and resolves the issue of having more model parameters than observations.</p>
<p>Parceling, however, introduces a less obvious challenge for the application of psychometric modeling, as parcels (especially those derived through averaging) produce continuous variables. On the one hand, this makes the data suitable for Factor Analysis (FA), but unsuitable for Item Response Theory (IRT). However, FA assumes that the observed variables follow a normal distribution, implying that they are not only continuous but also unbounded. By contrast, parcels derived from benchmark data have fixed lower and upper bounds of 0 and 1 , respectively.</p>
<p>Fortunately, Samejima $(1973,1974)$ developed a unidimensional IRT model for such observed variables as a limiting case of her Graded Response Model for polytomous responses (Samejima, 1969), where the number of categories approaches infinity. Later, Wang and Zeng (1998) extended Samejima's model by introducing a parcel "difficulty" parameter. Subsequently, Ferrando (2002) formally explored the relationships between Wang and Zeng's modification of Samejima's model and linear FA. In this section, we will follow Ferrando's derivations.</p>
<p>Assume the observed data contains the performance of $M$ LLMs on $P$ observed variables (parcels). The observed performance $U_{m p}$ of model $m(m=1,2, \ldots, M)$ on parcel $p$ $(p=1,2, \ldots, P)$ is standardized such that $0&lt;U_{m p}&lt;1$. Then, the CRM assumes</p>
<p>$$
U_{m p}=\log i t^{-1}\left(V_{m p}\right)=\log i t^{-1}\left(\mu_{p}+\lambda_{p} \theta_{m}+\omega_{m p}\right)
$$</p>
<p>where $\mu_{p}$ is the easiness of parcel $p\left(\boldsymbol{\mu} \in \mathbb{R}^{p}\right)$,
$\lambda_{p}$ is the sensitivity of parcel $p$ to the changes in the target ability (the discrimination) $\left(\lambda \in \mathbb{R}^{p}\right.$, however, but typically $\lambda \in \mathbb{R}<em m="m">{+}^{p}$ for model identification),
$\theta</em>(0,1)$ for model identification), and
$\omega_{m p}$ is the residual interaction of model $m$ and parcel $p\left(\boldsymbol{\omega} \in \mathbb{R}^{p} ; \omega_{p} \sim \mathcal{N}\left(0, \sigma_{p}\right)\right)$.
Given that the distribution of $\boldsymbol{V}=\left(V_{1}, V_{2}, \ldots, V_{p}, \ldots, V_{p}\right)^{T}$ is assumed to be multivariate normal $(\boldsymbol{V} \sim \mathcal{N}(\boldsymbol{k}, \boldsymbol{S}))$, the conditional distribution of $U_{p}$ is assumed to be}$ is the value of latent variable denoting the target ability of model $m(\theta \in \mathbb{R}$ in the unidimensional case; $\theta \sim \mathcal{N</p>
<p>$$
f\left(U_{p} \mid \theta_{m}\right)=\frac{1}{\sigma_{p} \sqrt{2 \pi}} \frac{1}{U_{p}\left(1-U_{p}\right)} \exp \left{-\frac{1}{2}\left[\frac{\operatorname{logit}\left(U_{p}\right)-\left(\mu_{p}+\lambda_{p} \theta_{m}\right)}{\sigma_{p}}\right]^{2}\right}
$$</p>
<p>As noted by Ferrando (2002), Equation 2 is known as the $S_{B}$ distribution (Johnson, 1949) or the four-parameter log-normal distribution (Aitchison, \&amp; Brown, 1957). After the introduction of factor loading $\alpha_{i}=\frac{\lambda_{p}}{\sigma_{p}}$ and intercept $\tau_{p}=-\frac{\mu_{p}}{\lambda_{p}}$, Eq. (1) becomes</p>
<p>$$
f\left(U_{p} \mid \theta_{m}\right)=\frac{\alpha_{i}}{\lambda_{p} \sqrt{2 \pi}} \frac{1}{U_{p}\left(1-U_{p}\right)} \exp \left{-\frac{1}{2}\left[\alpha_{i}\left{\theta_{m}-\tau_{p}-\frac{\operatorname{logit}\left(U_{p}\right)}{\lambda_{p}}\right}\right]^{2}\right}
$$</p>
<p>Which is the form of CRM derived by Wang and Zeng (1998) - with the addition of the parcel intercept $\tau_{p}$. Thanks to the assumption of multivariate normality of $\boldsymbol{V}_{p}$, Equation 2 has the following conditional expectation:</p>
<p>$$
E\left(U_{p} \mid \theta\right)=\int_{-\infty}^{+\infty} \frac{1}{1+\exp \left[-\left(\mu_{p}+\lambda_{p} \theta+z \sigma_{p}\right)\right]} \varphi(z) d z
$$</p>
<p>where $\varphi(z)$ is the density function of the standard normal distribution. This conditional expectation serves as Item Characteristic Curve of $U_{p}$.</p>
<p>The equivalence of Equation 2 to $S_{B}$ distribution (Johnson, 1949) is useful, as it highlights several desirable properties of this conditional distribution. Among them is the introduction of the skewness of the distribution density in the direction of 0.5 and the reduction in its variance proportionally to the proximity of the boundary value. Additionally, if $\mu_{p}=\theta_{m}$, the conditional expectation of Equation 4 is 0.5 , further building analogies with more traditional IRT models, such as Rasch or 2PL models.</p>
<p>The key insight from these derivations is that the CRM assumes logistic link-function between $\boldsymbol{V}$ and $\boldsymbol{U}$, while the traditional linear FA assumes an identity link-function. This means</p>
<p>that CRM can be approximated by an FA model on $\boldsymbol{V}=\operatorname{logit}(\boldsymbol{U})$. However, in this approximation, the mean structure must be estimated alongside the covariance structure in the FA model. This process is referred to as the heuristic estimation procedure for CRM (Bejar, 1977).</p>
<p>One important detail is that the traditional CRM (Wang \&amp; Zeng, 1998) is estimated via Marginal Maximum Likelihood (Bock \&amp; Aitkin, 1981), where model-specific parameters $\left(\theta_{m}\right)$ are marginalized to the standard normal distribution. In the realm of FA, this approach is known as full-information FA (Bartholomew, 1981).</p>
<p>The reason for resorting to the FA approximation of CRM is that it offers a significant advantage over the classical CRM parameterization: it provides a highly flexible set of tools for exploring the dimensionality of the latent factor space and enables advanced model fit analyses in a convenient manner. For example, a researcher can easily test whether multiple latent factors, rather than a single factor, are sufficient to approximate $\boldsymbol{S}$. Additionally, a researcher can explore alternative item loadings on latent factors.</p>
<p>In general, an FA model with a mean structure assumes that</p>
<p>$$
\boldsymbol{V}=\boldsymbol{\mu}+\boldsymbol{\Lambda} \boldsymbol{\theta}+\boldsymbol{\varepsilon}
$$</p>
<p>where $\boldsymbol{\mu} \in \mathbb{R}^{P}$ is the vector of means of the observed variables (easiness from Eq. 1), $\boldsymbol{\Lambda} \in \mathbb{R}^{P \times F}$ is matrix of factor loadings on $F$ factors $(F&lt;P$; typically, for model identification $\left.\boldsymbol{\Lambda} \in \mathbb{R}_{\tau}^{P \times F}\right)$
$\boldsymbol{\theta} \in \mathbb{R}^{F}$ is the vector of $F$ factors scores $(\boldsymbol{\theta} \sim \mathcal{N}(\mathbf{0}, \boldsymbol{\Xi})$ with $\operatorname{diag}(\boldsymbol{\Xi})=\mathbf{1}$ for model identification),
$\boldsymbol{\varepsilon} \in \mathbb{R}^{P}$ is the observed residuals vector $(\boldsymbol{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \boldsymbol{\Psi}))$.
FA attempts to approximate the sample variance-covariance matrix $\boldsymbol{S}$ from $\boldsymbol{V} \sim \mathcal{N}(\boldsymbol{k}, \boldsymbol{S})$ with the model-implied variance-covariance matrix $\boldsymbol{\Sigma}$ :</p>
<p>$$
\boldsymbol{S} \approx \boldsymbol{\Sigma}=\boldsymbol{\Lambda} \boldsymbol{\Xi} \boldsymbol{\Lambda}^{T}+\boldsymbol{\Psi}
$$</p>
<p>The log-likelihood of the data is the given by</p>
<p>$$
\ell(\boldsymbol{\mu}, \boldsymbol{\Lambda}, \boldsymbol{\Xi}, \boldsymbol{\Psi})=-\frac{M}{2}\left(P \log (2 \pi)+\log |\boldsymbol{\Sigma}|+\operatorname{trace}\left(\boldsymbol{S} \boldsymbol{\Sigma}^{-1}\right)+(\boldsymbol{\mu}-\boldsymbol{k})^{T} \boldsymbol{\Sigma}^{-1}(\boldsymbol{\mu}-\boldsymbol{k})\right)
$$</p>
<p>where $|\boldsymbol{\Sigma}|$ is the determinant of $\boldsymbol{\Sigma}$. And the maximum likelihood estimates are given by</p>
<p>$$
(\overline{\boldsymbol{\mu}}, \overline{\boldsymbol{\Lambda}}, \overline{\boldsymbol{\Xi}}, \overline{\boldsymbol{\Psi}})=\underset{\boldsymbol{\mu}, \boldsymbol{\Lambda}, \boldsymbol{\Xi}, \boldsymbol{\Psi}}{\arg \max } \ell(\boldsymbol{\mu}, \boldsymbol{\Lambda}, \boldsymbol{\Xi}, \boldsymbol{\Psi})
$$</p>
<p>If specific constraints are imposed on $\boldsymbol{\Lambda}, \boldsymbol{\Xi}$, and $\boldsymbol{\Psi}$ (e.g., if a researcher assumes that certain items do not load on specific factors - then, corresponding elements of $\boldsymbol{\Lambda}$ being</p>
<p>constrained to 0 ; or if there is an assumption of a single factor underlying all observed variables), then the model is said to be of a Confirmatory FA (CFA) nature.</p>
<p>If there are no constraints on these matrices, then the model is said to be of Exploratory FA (EFA) nature, with $\boldsymbol{\Xi}=\boldsymbol{I}$ (identity matrix, for identification purposes). While EFA tends to yield a better representation of $\boldsymbol{S}$, CFA is generally preferred for higher-stakes decisionmaking, as it aligns with reflective (rather than formative) measurement principles (Hanafiah, 2020).</p>
<p>In CFA, the residual covariance matrix $\boldsymbol{\Psi}$ is typically diagonal initially (with offdiagonal elements constrained to 0 and the main diagonal values estimated). Later, some constraints on the absent residual covariances can be relaxed based on automatic diagnostic such as model modification indices (Whittaker, 2012). If some observed variables in $\boldsymbol{S}$ are more strongly correlated than $\boldsymbol{\Sigma}$ predicts from $\boldsymbol{\Lambda} \boldsymbol{\Xi} \boldsymbol{\Lambda}^{T}$, the model will be poor and adding estimated parameters in $\boldsymbol{\Psi}$ becomes warranted.</p>
<p>However, such model modifications are also interpretationally significant. They reveal correlations in the data that are not explained by the factor scores. In such cases, the correlated observed variables should not be treated as locally independent by the model (conditionally independent given person parameters), as they reflect a substantial amount of common variance beyond that attributable to the latent factors ("common cause" for the observed variables). Therefore, interpreting residual covariances - or increasing the number of latent factors requires careful content-based interpretation.</p>
<p>This modeling approach can be seeing as analogous to a variational autoencoder with the shallow decoder (one neuron in depth; Urban \&amp; Bauer, 2021). In this analogy, the neuron parameters from the (final) decoder layer correspond to item parameters, and the means of the observation-specific latent representation distributions serve as factor scores. However, while analyzing latent correlations between factors is not typically a focus in autoencoders, it is a central area of interest in FA. Additionally, autoencoders do not examine residual covariances. Furthermore, in CFA, the matrix of factor loadings $\boldsymbol{\Lambda}$ includes some entries constrained to 0 , whereas in autoencoders, all entries are estimated (making them more similar to EFA).</p>
<p>Alternatively, FA and IRT can be viewed as collaborative filtering engines (Bergner et al., 2022) designed to mathematically separate item (variable) parameters from person (observation) parameters.</p>
<p>In the field of CFA, several model fit indices are widely used:</p>
<ul>
<li>Root Mean Square Error of Approximation (RMSEA; Steiger, 1990; Steiger, 1998) as a measure of the average difference between the observed variance-covariance matrix $\boldsymbol{S}$ and the model-implied variance-covariance matrix $\boldsymbol{\Sigma}$ per degree of freedom,</li>
<li>Standardized Root Mean Square Residual (SRMS; Hu \&amp; Bentler, 1998) as a measure of the averaged squared differences between each bivariate empirical correlation (in $\boldsymbol{S}$ ) and its corresponding model-implied counterpart (in $\boldsymbol{\Sigma}$ ),</li>
<li>Comparative Fit Index (CFI; Bentler, 1990) as a measure of the relative improvement in model fit from the baseline model to the tested model,</li>
<li>Tucker-Lewis Index (TLI; Tucker \&amp; Lewis, 1973) as a measure of the relative reduction in misfit from the baseline model to the tested model per degree of freedom.</li>
</ul>
<p>For RMSEA and SRMS, the following cut-off criteria are applied: $0&lt;$ good fit $&lt;0.05$ $\leq$ acceptable fit $&lt;0.08 \leq$ poor fit. For CFI and TLI, the following cut-off criteria are applied: poor fit $\leq 0.9&lt;$ acceptable fit $\leq 0.95&lt;$ good fit $&lt;1$. Additionally, we report the $\chi^{2}$-statistic $(-2 \ell(\boldsymbol{\mu}, \boldsymbol{\Lambda}, \boldsymbol{\Xi}, \boldsymbol{\Psi}))$ which analyzes the statistical significance of the differences between $\boldsymbol{S}$ and $\boldsymbol{\Sigma}$ in the tested model. However, this statistic is not used for decision-making due to its generally high Type I error rates.</p>
<p>Nonetheless, this approach relies heavily on the assumption of multivariate normality of the observed data, which is rarely true. To address this, modifications such as the so-called Maximum Likelihood Robust (MLR) method have been developed. These modifications adjust the standard errors and the $\chi^{2}$-statistic value. Specifically, they scale the $\chi^{2}$-statistic by a factor determined by the multivariate skewness and kurtosis of the observed data (Yuan \&amp; Bentler, 2000) and estimate standard errors using a sandwich approach with the observed Fisher information matrix at the standard maximum likelihood estimates (Huber, 1967).</p>
<p>Once the model has been calibrated, the FA-reliability of measurement can be calculated. One common method is to compute the composite reliability coefficient (Bentler, 1968; often referred to as McDonald's $\omega$; McDonald, 1970) coefficient. In the case of a unidimensional model $(\theta \sim \mathcal{N}(0,1)$; regardless of presence or absence of residual covariances), it can be estimated as:</p>
<p>$$
\rho=\frac{\left(\sum_{p=1}^{p} \lambda_{p}\right)^{2}}{\mathbf{1}^{T} \boldsymbol{S} \mathbf{1}}
$$</p>
<p>where the column-vector $\mathbf{1}$ of length $P$ is used to sum the entries in $\boldsymbol{S}$.
While the exact definition and interpretation of reliability remain a topic of debate (Cho, 2021), it is generally understood operationally as a measure of the non-randomness in observed</p>
<p>variables, given the psychometric model being used. The closer $\rho$ is to $1(0 \leq \rho \leq 1)$, the more reliable the results are, and the less the Standard Error of Measurement (S.E.) is.</p>
<p>The latent ability estimates can then be obtained via the Maximum a Posteriori (MAP) method (also known as Modal a Posteriori, Empirical Bayes, Bayes Modal, or nave regression factor scores; Skrondal \&amp; Rabe-Hesketh, 2004). In this approach, the posterior distribution of the ability $\left(\boldsymbol{\theta}<em m="m">{m}\right)$ is defined as $P\left(\boldsymbol{\theta}</em>} \mid \boldsymbol{V<em m="m">{m}\right) \propto P\left(\boldsymbol{V}</em>} \mid \boldsymbol{\theta<em m="m">{m}\right) P\left(\boldsymbol{\theta}</em>}\right)$, i.e., as the product of the likelihood $\left(\boldsymbol{V<em m="m">{m} \mid \boldsymbol{\theta}</em>} \sim \mathcal{N}\left(\boldsymbol{\mu}+\boldsymbol{\Lambda} \boldsymbol{\theta<em m="m">{m}, \boldsymbol{\Psi}\right)\right)$ and the priors $\left(\boldsymbol{\theta}</em>)\right)$. The negative loglikelihood (up to constants) is:} \sim \mathcal{N}(\mathbf{0}, \boldsymbol{\Xi</p>
<p>$$
-\ell\left(\boldsymbol{V}<em m="m">{m} \mid \boldsymbol{\theta}</em>}\right)=\frac{1}{2}\left(\boldsymbol{V<em m="m">{m}-\boldsymbol{\mu}-\boldsymbol{\Lambda} \boldsymbol{\theta}</em>}\right)^{T} \boldsymbol{\Psi}^{-1}\left(\boldsymbol{V<em m="m">{m}-\boldsymbol{\mu}-\boldsymbol{\Lambda} \boldsymbol{\theta}</em>\right)
$$</p>
<p>and the negative log-priors (up to constants) is:</p>
<p>$$
-\log P\left(\boldsymbol{\theta}<em m="m">{m}\right)=\frac{1}{2} \boldsymbol{\theta}</em>
$$}^{T} \boldsymbol{\Xi}^{-1} \boldsymbol{\theta}_{m</p>
<p>The estimate is then obtained by minimizing the negative log-posterior:</p>
<p>$$
\overline{\boldsymbol{\theta}}<em _boldsymbol_theta="\boldsymbol{\theta">{m}=\arg \min </em><em m="m">{m}}\left{\frac{1}{2}\left[\left(\boldsymbol{V}</em>}-\boldsymbol{\mu}-\boldsymbol{\Lambda} \boldsymbol{\theta<em m="m">{m}\right)^{T} \boldsymbol{\Psi}^{-1}\left(\boldsymbol{V}</em>}-\boldsymbol{\mu}-\boldsymbol{\Lambda} \boldsymbol{\theta<em m="m">{m}\right)+\boldsymbol{\theta}</em>\right]\right}
$$}^{T} \boldsymbol{\Xi}^{-1} \boldsymbol{\theta}_{m</p>
<p>After taking the gradient of the minimized term in Equation 12 and setting it to 0 , a closed-form solution for the factor scores can then be derived as:</p>
<p>$$
\overline{\boldsymbol{\theta}}<em m="m">{m}=\left(\boldsymbol{\Lambda}^{T} \boldsymbol{\Psi}^{-1} \boldsymbol{\Lambda}+\boldsymbol{\Xi}^{-1}\right)^{-1} \boldsymbol{\Lambda}^{T} \boldsymbol{\Psi}^{-1}\left(\boldsymbol{V}</em>\right)
$$}-\boldsymbol{\mu}\right)=\boldsymbol{\Xi} \boldsymbol{\Lambda}^{T}\left(\boldsymbol{\Lambda} \boldsymbol{\Xi} \boldsymbol{\Lambda}^{T}+\boldsymbol{\Psi}\right)^{-1}\left(\boldsymbol{V}_{m}-\boldsymbol{\mu</p>
<p>Such factor scores provide more stable results, filtered out of noise, on the continuous unbounded scale, which offers a better metric than a simple average.</p>
<p>In addition to discussing and using the MAP point estimates of factor scores, their posterior variance is also critical. This variance serves as a measure of uncertainty (standard error) around the ability estimates and can be computed as:</p>
<p>$$
\text { S.E. }(\overline{\boldsymbol{\theta}})^{2}=\operatorname{var}\left(\overline{\boldsymbol{\theta}} \mid \boldsymbol{V}_{m}\right)=\left(\boldsymbol{\Xi}^{-1}-\boldsymbol{\Lambda}^{T} \boldsymbol{\Psi}^{-1} \boldsymbol{\Lambda}\right)^{-1}=\boldsymbol{\Xi}-\boldsymbol{\Xi} \boldsymbol{\Lambda}^{T}\left(\boldsymbol{\Lambda} \boldsymbol{\Xi} \boldsymbol{\Lambda}^{T}+\boldsymbol{\Psi}\right)^{-1} \boldsymbol{\Lambda} \boldsymbol{\Xi}
$$</p>
<p>Note, that in Equation 14, the posterior variance estimate is constant and does not depend on $\overline{\boldsymbol{\theta}}$. While this holds true in the case of CFA (since linear models provide a constant amount of Fisher information about the parameter), it is not true in the general case of IRT. IRT leverages non-linear models, and as a result, it explicitly accounts for the fact that different ability levels are associated with varying degrees of measurement precision.</p>
<p>The exact distribution of the Fisher information function depends on the properties of the item bank. However, it is generally the case that extreme (high and low) ability levels are measured with greater uncertainty, as item information is typically concentrated around medium levels of ability (the region of the highest density of the observations).</p>
<p>All analyses were conducted using the lavaan package (v. 0.6-17; Rosseel et al., 2023) for the statistical programming language R (v. 4.3.0). FA reliability was estimated using the semTools package (v. 0.5-6; Jorgensen et al., 2022).</p>
<h1>4.2 Data</h1>
<p>The data was retrieved from the Hugging Face Leaderboard (Beeching et al., 2023), where the performance of various models is published in open access. On this leaderboard, the evaluation results are presented in a few-shot manner (Brown et al., 2020). This means that the model is provided with a few examples of similar questions along with their correct answers before being evaluated on the target question. Within each task (type of questions within a benchmark), the number of shots is standardized. For some of the benchmarks, the performance is evaluated in a zero-shot manner, which is a special case of the few-shot approach where the number of preliminary examples administered is 0 . All questions used in the leaderboard evaluations are Multiple-Choice (MC) with several options to choose from.</p>
<p>Study 1 aimed to analyze the first version of the Hugging Face Leaderboard. Here we use the dataset retrieved from https://huggingface.co/spaces/open-llm-leaderboardold/open_llm leaderboard on November 30, 2024. This dataset contains the parceled performance of 3,792 LLMs on six benchmarks. These benchmarks include:</p>
<ul>
<li>AI2 Reasoning Challenge (ARC; Clark et al., 2018) - a set of 2,590 grade-school science questions designed to test commonsense knowledge and advanced methods for deeper text comprehension. These questions are administered in a 25 -shot manner.</li>
<li>HellaSwag (Zellers et al., 2019) - a set of 70,000 commonsense natural language inference MC questions. Evaluation questions are administered in a 10 -shot manner.</li>
<li>MMLU (see the introduction section above).</li>
<li>TruthfulQA (Lin et al., 2021) - a set of 684 MC questions from 38 topics, including health, law, finance, and politics. This benchmark measures how well LLMs answer questions that some humans would answer incorrectly due to false beliefs or misconceptions. These questions are administered in a 6 -shot manner.</li>
<li>
<p>WinoGrande (Sakaguchi et al., 2021) - a set of 1,767 MC questions based on the Winograd Schema Challenge (Levesque et al., 2012), designed to measure commonsense reasoning from in-sentence context. These questions are administered in a 5 -shot manner.</p>
</li>
<li>
<p>GSM8K (Cobbe et al., 2021) - a set of 8,500 grade-school math questions and natural language solutions, used to probe the informal reasoning abilities of large language models. These questions are administered in a 5-shot manner.
Study 2 aimed to analyze the second version of the Hugging Face Leaderboard. For this purpose, we used a dataset retrieved from https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard on November 30, 2024. This dataset contains the parceled performance of 1,543 LLMs on six benchmarks:</p>
</li>
<li>Instruction-Following Evaluation (IFEval; Zhou et al., 2024) - a set of approximately 500 items designed to evaluate LLMs' ability to follow 25 types of explicit, verifiable instructions. Examples include: "In your response, the word {word} should appear ${\mathrm{N}}$ times", "Finish your response with this exact phrase: {end phrase}. No other words should follow this phrase", or "The entire output should be wrapped in JSON format". This benchmark evaluates adherence to instructions rather than the content of the response.</li>
<li>Big Bench Hard (BBH; Suzgun et al., 2022) - a set of 6,511 items grouped into 23 tasks from the Big Bench benchmark, focusing on the most challenging problems for LLMs. These tasks include multistep arithmetic, algorithmic reasoning (e.g., Boolean expressions, SVG shapes), language understanding (e.g., sarcasm detection, name disambiguation), and world knowledge.</li>
<li>MATH lvl 5 (Hendrycks et al., 2021) - subset of the MATH benchmark consisting of 12,500 items ( 7,500 training and 5,000 test items) based on high-school-level competition problems gathered from various sources. Items are consistently formatted using LaTeX for equations and Asymptote for figures. The benchmark is categorized into five levels of difficulty and seven content areas (Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus). The leaderboard used here includes only items from difficulty level 5, giving this subset its name.</li>
<li>Graduate-Level Google-Proof Q\&amp;A Benchmark (GPQA; Rein et al., 2023) - a set of 448 highly challenging knowledge questions crafted by PhD-level domain experts in fields such as biology, physics, and chemistry. The questions are designed to be difficult for laypersons (even with access to Google) but relatively easy for experts.</li>
<li>Multistep Soft Reasoning (MuSR; Sprague et al., 2023) - a benchmark consisting of algorithmically generated complex problems, presented in narratives approximately</li>
</ul>
<p>1,000 words in length that leverage long-range context parsing. The problems are divided into three categories: murder mysteries (250), object placement questions (256), and team allocation optimizations (250).</p>
<ul>
<li>Massive Multitask Language Understanding - Professional (MMLU-PRO; Wang et al., 2024) - an expert-refined version of the MMLU benchmark, consisting of 12,032 multiple-choice items (with 10 response alternatives per item) across 14 areas: math, physics, chemistry, law, engineering, economics, health, psychology, business, biology, computer science, philosophy, and miscellaneous.</li>
</ul>
<p>Importantly, four of six benchmarks in Study 2 include an anti-guessing correction in the measure of LLM performance, as the majority of the questions are in an MC format. This correction is based on setting the baseline probability of a randomly selected answer as $P\left(U_{i}=1\right)=1 / O_{i}$, where $U_{i}$ is the score on item $i$, and $O_{i}$ is the number of response options for item $i$. This adjustment leads to two substudies in Study 2: Study 2a and Study 2b, which focus on investigating the structure of the second benchmark using unnormalized (raw) and normalized (corrected) scores for the four benchmarks, respectively.</p>
<h1>5. Results</h1>
<h3>5.1 Study 1 - Analysis of the Hugging Face Leaderboard v. 1</h3>
<p>The initial unidimensional model calibrated on the older leaderboard dataset, exhibited relatively poor model fit under Maximum Likelihood Robust estimator (Yuan \&amp; Bentler, 2000). Specifically, SRMR $=0.054$, robust RMSEA $=0.304$ ( $90 \%$ CI for RMSEA $=[0.294$, $0.314]$ ), CFI $=0.901$, TLI $=0.836$ (the baseline model scaled $\chi^{2}$-statistic $=10,613.699$, YuanBentler correction factor $=3.010$, degrees of freedom for $\chi^{2}=15$, p -value $&lt;0.001$; the tested model scaled $\chi^{2}$-statistic $=1,332.422$, Yuan-Bentler correction factor $=2.372$, degrees of freedom for $\chi^{2}=9$, p-value $&lt;0.001$ ). Despite the poor model fit, all standardized factor loadings were exceptionally high and statistically significant (the lowest z-value $=47.101$ ). The parameter estimates are presented in Table 1.</p>
<h2>Table 1</h2>
<p>Parameter estimates from the initial model</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Benchmark</th>
<th style="text-align: center;">Factor loading</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Intercept</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Residual variance</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">UnStd</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Std</td>
<td style="text-align: center;">UnStd</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Std</td>
<td style="text-align: center;">UnStd</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Std</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Est.</td>
<td style="text-align: center;">S.E.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Est.</td>
<td style="text-align: center;">S.E.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Est.</td>
<td style="text-align: center;">S.E.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ARC</td>
<td style="text-align: center;">13.928</td>
<td style="text-align: center;">0.146</td>
<td style="text-align: center;">0.997</td>
<td style="text-align: center;">54.003</td>
<td style="text-align: center;">0.227</td>
<td style="text-align: center;">3.867</td>
<td style="text-align: center;">1.017</td>
<td style="text-align: center;">0.735</td>
<td style="text-align: center;">0.005</td>
</tr>
<tr>
<td style="text-align: center;">HellaSwag</td>
<td style="text-align: center;">15.239</td>
<td style="text-align: center;">0.258</td>
<td style="text-align: center;">0.932</td>
<td style="text-align: center;">73.862</td>
<td style="text-align: center;">0.266</td>
<td style="text-align: center;">4.517</td>
<td style="text-align: center;">35.186</td>
<td style="text-align: center;">1.247</td>
<td style="text-align: center;">0.132</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">MMLU</th>
<th style="text-align: center;">14.89</th>
<th style="text-align: center;">0.139</th>
<th style="text-align: center;">0.918</th>
<th style="text-align: center;">52.381</th>
<th style="text-align: center;">0.263</th>
<th style="text-align: center;">3.231</th>
<th style="text-align: center;">41.094</th>
<th style="text-align: center;">1.31</th>
<th style="text-align: center;">0.156</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">TruthfulQA</td>
<td style="text-align: center;">6.451</td>
<td style="text-align: center;">0.137</td>
<td style="text-align: center;">0.652</td>
<td style="text-align: center;">49.589</td>
<td style="text-align: center;">0.161</td>
<td style="text-align: center;">5.011</td>
<td style="text-align: center;">56.320</td>
<td style="text-align: center;">1.326</td>
<td style="text-align: center;">0.575</td>
</tr>
<tr>
<td style="text-align: center;">Winograde</td>
<td style="text-align: center;">9.23</td>
<td style="text-align: center;">0.118</td>
<td style="text-align: center;">0.957</td>
<td style="text-align: center;">72.298</td>
<td style="text-align: center;">0.157</td>
<td style="text-align: center;">7.499</td>
<td style="text-align: center;">7.759</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">0.083</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">18.857</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.740</td>
<td style="text-align: center;">29.588</td>
<td style="text-align: center;">0.414</td>
<td style="text-align: center;">1.161</td>
<td style="text-align: center;">293.761</td>
<td style="text-align: center;">5.855</td>
<td style="text-align: center;">0.452</td>
</tr>
</tbody>
</table>
<p>TruthfulQA appears to exhibit a substantial amount of factor-irrelevant variance, suggesting that while most of the other benchmarks measure a similar property (possibly excluding GSM8K), TruthfulQA reflects a distinct aspect of LLM performance. This observation aligns with its purpose: while other benchmarks primarily capture the "cognitive" aspects of LLMs, TruthfulQA focuses on measuring LLM robustness against various biases.</p>
<p>The high factor loadings on the general factor are consistent with previous research attempting to apply an EFA model to similar data. For instance, Ili (2023) provided strong evidence supporting a single general factor of intelligence across various LLMs and leaderboards, using SRMR as the model fit statistic. Similarly, Perlitz et al. (2024) demonstrated a relatively high degree of agreement between different benchmarks, although some benchmarks were less correlated with the majority than others. However, our analysis indicates that despite strong and positive factor loadings and low SRMR, the unidimensional model generally exhibits poor fit. This finding suggests that the multivariate distribution of LLM performance data is more complex than initially expected.</p>
<p>To improve model fit, we employed a "greedy algorithm" strategy. This involved (1) analyzing model modification indices, (2) adding the residual covariance parameter that promised the greatest improvement in model fit, (3) recalibrating the model, and (4) repeating the process. On the third iteration, the model suggested adding a residual covariance between the WinoGrande and ARC benchmarks. However, this addition resulted in negative residual variance estimates for ARC, indicating poor model convergence. Therefore, Table 2 reports only the results from the second iteration.</p>
<p>The revised model showed improved fit across most indices ( $\operatorname{SRMR}=0.041$, robust RMSEA $=0.209$ ( $90 \%$ CI for RMSEA $=[0.197,0.221])$, CFI $=0.964$, TLI $=0.922$; the tested model scaled $\chi^{2}$-statistic $=554.775$, Yuan-Bentler correction factor $=2.106$, degrees of freedom for $\chi^{2}=7$, p-value $&lt;0.001$ ).</p>
<h1>Table 2</h1>
<p>Residual correlations in the revised model in the order of addition</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Benchmark</th>
<th style="text-align: left;">Factor loading</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Intercept</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Residual variance</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">UnStd</td>
<td style="text-align: left;">Std</td>
<td style="text-align: left;">UnStd</td>
<td style="text-align: left;">Std</td>
<td style="text-align: left;">UnStd</td>
<td style="text-align: left;">Std</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Est.</th>
<th style="text-align: center;">S.E.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Est.</th>
<th style="text-align: center;">S.E.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Est.</th>
<th style="text-align: center;">S.E.</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ARC</td>
<td style="text-align: center;">13.916</td>
<td style="text-align: center;">0.146</td>
<td style="text-align: center;">0.997</td>
<td style="text-align: center;">54.003</td>
<td style="text-align: center;">0.227</td>
<td style="text-align: center;">3.867</td>
<td style="text-align: center;">1.349</td>
<td style="text-align: center;">0.694</td>
<td style="text-align: center;">0.007</td>
</tr>
<tr>
<td style="text-align: center;">HellaSwag</td>
<td style="text-align: center;">15.277</td>
<td style="text-align: center;">0.257</td>
<td style="text-align: center;">0.934</td>
<td style="text-align: center;">73.862</td>
<td style="text-align: center;">0.266</td>
<td style="text-align: center;">4.517</td>
<td style="text-align: center;">34.033</td>
<td style="text-align: center;">1.371</td>
<td style="text-align: center;">0.127</td>
</tr>
<tr>
<td style="text-align: center;">MMLU</td>
<td style="text-align: center;">14.872</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.917</td>
<td style="text-align: center;">52.381</td>
<td style="text-align: center;">0.263</td>
<td style="text-align: center;">3.231</td>
<td style="text-align: center;">41.634</td>
<td style="text-align: center;">1.323</td>
<td style="text-align: center;">0.158</td>
</tr>
<tr>
<td style="text-align: center;">TruthfulQA</td>
<td style="text-align: center;">6.489</td>
<td style="text-align: center;">0.133</td>
<td style="text-align: center;">0.656</td>
<td style="text-align: center;">49.589</td>
<td style="text-align: center;">0.161</td>
<td style="text-align: center;">5.011</td>
<td style="text-align: center;">55.834</td>
<td style="text-align: center;">1.258</td>
<td style="text-align: center;">0.570</td>
</tr>
<tr>
<td style="text-align: center;">Winograde</td>
<td style="text-align: center;">9.239</td>
<td style="text-align: center;">0.117</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">72.298</td>
<td style="text-align: center;">0.157</td>
<td style="text-align: center;">7.499</td>
<td style="text-align: center;">7.582</td>
<td style="text-align: center;">0.665</td>
<td style="text-align: center;">0.082</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">18.786</td>
<td style="text-align: center;">0.232</td>
<td style="text-align: center;">0.737</td>
<td style="text-align: center;">29.588</td>
<td style="text-align: center;">0.414</td>
<td style="text-align: center;">1.161</td>
<td style="text-align: center;">296.438</td>
<td style="text-align: center;">5.752</td>
<td style="text-align: center;">0.457</td>
</tr>
<tr>
<td style="text-align: center;">Residual covariances</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Benchmarks</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">UnStd</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Std</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Est.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">S.E.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">HellaSwag TruthfulQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-24.601</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.799</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$-0.564$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">MMLU GSM8K</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">44.408</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.845</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Adding these residual correlations resulted in the improvement in AIC at each step (from 157258.794 to 155909.493 , to 155270.746 ), indicating a better model fit with each iteration. Lower AIC values signify improved relative model fit, even after accounting for the penalty for additional model parameters. The decision to use AIC instead of information criteria that incorporate sample size (e.g., Bayesian Information Criterion; Schwarz, 1978) stems from the tendency of such criteria to oversimplify the data-generating model (Evans, 2019), particularly in the context of IRT (Robitzsch, 2022). Importantly, none of the factor loadings on the general factor became insignificant or showed a substantial decrease in standardized estimates after the inclusion of these parameters.</p>
<p>Regarding the interpretation of the added residual covariances, all are meaningful. The negative correlation between HellaSwag and TruthfulQA suggests that vaguely defined common sense (HellaSwag) may conflict with factual accuracy (TruthfulQA). This is consistent with TruthfulQA's design, which aims to detect such contradictions. As a result, a model performing well on common sense questions might perform worse on TruthfulQA. Additionally, the positive correlation between MMLU and GSM8K can be attributed to the presence of STEM-related items in MMLU. These items share substantial common variance with the mathematics items in GSM8K, as both measure the same mathematical ability of LLMs.</p>
<p>Overall, the unidimensional model supports the presence of a single general factor across all benchmarks. This implies that the benchmarks measure, to some extent, the same</p>
<p>underlying ability. Specifying a multidimensional model to approximate the variancecovariance matrix with additional factors would be futile for two reasons.</p>
<p>First, the specification of latent factors should be grounded in a substantial theoretical hypothesis about the latent variables underlying the observed data. Without a robust theoretical framework, such specification is not feasible. Second, given the high factor loadings on the latent factor in the unidimensional model, the correlation between factors in a multidimensional model would likely approach unity, causing convergence issues. This finding is consistent with Ili (2023).</p>
<p>However, other studies suggest that LLM performance can be described using three factors: reasoning, comprehension, and core language modeling (Burnell et al., 2023), or by several principal components (Ruan et al., 2024). Our results may also partially support these findings. Specifically, our results imply that while a general factor is present in the data, residual dependencies related to task content also exist.</p>
<h1>5.2.1 Study 2a - Analysis of the Hugging Face Leaderboard v. 2 (Raw Data)</h1>
<p>The initial unidimensional model calibrated with Maximum Likelihood Robust estimator on Hugging Face Leaderboard dataset exhibited a somewhat better model fit than the model the Leaderboard v.1. Particularly, SRMR $=0.038$, robust RMSEA $=0.134$ ( $90 \%$ CI for RMSEA $=[0.118,0.151]), \mathrm{CFI}=0.965, \mathrm{TL1}=0.941$ (the baseline model scaled $\chi^{2}$-statistic $=$ 2599.826, Yuan-Bentler scaling factor $=2.397$, degrees of freedom for $\chi^{2}=15$, p-value $&lt;$ 0.001 ; the tested model scaled $\chi^{2}$-statistic $=174.291$, Yuan-Bentler scaling factor $=1.323$, degrees of freedom for $\chi^{2}=9$, p-value $&lt;0.001$ ). The parameter estimates are presented in Table 3.</p>
<p>Table 3
Parameter estimates from the initial model</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Benchmark</th>
<th style="text-align: center;">Factor loading</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Intercept</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Residual variance</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">UnStd</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Std</td>
<td style="text-align: center;">UnStd</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Std</td>
<td style="text-align: center;">UnStd</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Std</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Est.</td>
<td style="text-align: center;">S.E.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Est.</td>
<td style="text-align: center;">S.E.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Est.</td>
<td style="text-align: center;">S.E.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">IFEval</td>
<td style="text-align: center;">0.663</td>
<td style="text-align: center;">0.029</td>
<td style="text-align: center;">0.587</td>
<td style="text-align: center;">$-0.399$</td>
<td style="text-align: center;">0.031</td>
<td style="text-align: center;">$-0.353$</td>
<td style="text-align: center;">0.838</td>
<td style="text-align: center;">0.166</td>
<td style="text-align: center;">0.656</td>
</tr>
<tr>
<td style="text-align: center;">BBH</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.008</td>
<td style="text-align: center;">0.981</td>
<td style="text-align: center;">$-0.131$</td>
<td style="text-align: center;">0.012</td>
<td style="text-align: center;">$-0.297$</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.037</td>
</tr>
<tr>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">1.542</td>
<td style="text-align: center;">0.088</td>
<td style="text-align: center;">0.513</td>
<td style="text-align: center;">$-4.032$</td>
<td style="text-align: center;">0.082</td>
<td style="text-align: center;">$-1.340$</td>
<td style="text-align: center;">6.671</td>
<td style="text-align: center;">0.446</td>
<td style="text-align: center;">0.737</td>
</tr>
<tr>
<td style="text-align: center;">GPQA</td>
<td style="text-align: center;">0.141</td>
<td style="text-align: center;">0.004</td>
<td style="text-align: center;">0.845</td>
<td style="text-align: center;">$-0.894$</td>
<td style="text-align: center;">0.005</td>
<td style="text-align: center;">$-5.345$</td>
<td style="text-align: center;">0.008</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.287</td>
</tr>
<tr>
<td style="text-align: center;">MuSR</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.004</td>
<td style="text-align: center;">0.604</td>
<td style="text-align: center;">$-0.398$</td>
<td style="text-align: center;">0.005</td>
<td style="text-align: center;">$-2.329$</td>
<td style="text-align: center;">0.019</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.636</td>
</tr>
<tr>
<td style="text-align: center;">MMLU-PRO</td>
<td style="text-align: center;">0.595</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">0.955</td>
<td style="text-align: center;">$-0.892$</td>
<td style="text-align: center;">0.017</td>
<td style="text-align: center;">$-1.431$</td>
<td style="text-align: center;">0.034</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.088</td>
</tr>
</tbody>
</table>            </div>
        </div>

    </div>
</body>
</html>
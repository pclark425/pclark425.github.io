<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5695 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5695</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5695</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-266191741</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.07910v3.pdf" target="_blank">PromptBench: A Unified Library for Evaluation of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> The evaluation of large language models (LLMs) is crucial to assess their performance and mitigate potential security risks. In this paper, we introduce PromptBench, a unified library to evaluate LLMs. It consists of several key components that are easily used and extended by researchers: prompt construction, prompt engineering, dataset and model loading, adversarial prompt attack, dynamic evaluation protocols, and analysis tools. PromptBench is designed to be an open, general, and flexible codebase for research purposes that can facilitate original study in creating new benchmarks, deploying downstream applications, and designing new evaluation protocols. The code is available at: https://github.com/microsoft/promptbench and will be continuously supported.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5695.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5695.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity of LLMs to prompt wording and format</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An observed phenomenon that LLM performance can change substantially depending on prompt wording, structure, context, and formatting; motivates PromptBench and the inclusion of many prompt formats and robustness tests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs evaluated in PromptBench (open-source and proprietary; paper explicitly mentions ChatGPT and GPT-4 among others)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Various tasks across 22 public datasets (GLUE tasks, MMLU, SQuAD v2, Math, GSM8K, CommonsenseQA, VQA, MMMU, MathVista, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A broad set of NLP, reasoning, math, multimodal, and translation tasks used to measure how model outputs change across prompt formats and attacks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Varied: task-oriented prompts, role-oriented prompts, zero-shot, few-shot (3 examples), chain-of-thought styles, generated-knowledge prompting, expert/role priming, emotional priming, least-to-most decomposition, and adversarially transformed prompts (character/word/sentence/semantic).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: performance varies widely with prompt format; no single prompt format uniformly best across datasets (paper reports variable effects across tasks and methods).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>varies (can improve or reduce performance depending on format and task)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The paper states that prompt wording and format change the model's behavior by priming roles, providing exemplars or intermediate reasoning steps, or by introducing noise/ distractions; therefore prompt structure leverages or misleads the model's learned patterns, altering outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Paper notes that many prompt engineering methods are effective only on special fields and 'cannot surpass the baseline in every dataset' (i.e., no universal improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptBench: A Unified Library for Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5695.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5695.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt types: task-oriented vs role-oriented</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task-oriented and Role-oriented Prompt Formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two high-level prompt presentation styles: task-oriented prompts explicitly state the task, while role-oriented prompts position the model in a role (expert/advisor/etc.) to guide answer style.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs evaluated in PromptBench (including GPT series, LLaMA/Llama2, Mixtral, Mistral, Vicuna, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Various classification, NLI, grammar, and other tasks drawn from PromptBench datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard NLP tasks where the instruction/prompt framing can shape the expected label or output format.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Task-oriented: explicit instruction of the expected output (e.g., 'Determine if the given pair... respond with equivalent or not equivalent'). Role-oriented: instruct model to adopt a role (e.g., 'Functioning as a grammar evaluation tool...').</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Task-oriented prompts vs Role-oriented prompts (paper allows evaluating lists of prompts and compares performance across them).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: role-oriented prompts can subtly guide expected output format and behavior; PromptBench uses both types and reports that performance depends on dataset and prompt choice.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>context-dependent (can improve alignment to expected answer format and sometimes accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Role priming changes the model's behavioral prior, encouraging outputs in the desired style or level of expertise; task-oriented prompts more directly constrain output content.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptBench: A Unified Library for Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5695.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5695.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot vs Zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot and Zero-shot Prompting (including few-shot with 3 exemplars used by PromptBench)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PromptBench supports zero-shot and few-shot prompting; for few-shot evaluation they randomly choose 3 training set examples per task as exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (GPT series, Llama2, Mistral, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple tasks (classification, NLI, reasoning, math, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard tasks evaluated under zero-shot and few-shot conditions to measure how exemplars in the prompt affect output.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot: no exemplars; Few-shot: prompt includes 3 training examples (question/answer pairs) before test item.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot vs Few-shot (3-shot) comparisons supported</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: few-shot exemplars are used to improve performance on some tasks; PromptBench reports dataset-dependent gains but no universal numeric improvements in the paper text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>often improved on some tasks, but not universally</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Providing exemplars gives explicit in-context examples of the desired mapping, which can bias the model toward correct output formats and answers; effect depends on exemplar selection and task.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Paper explicitly notes that prompt engineering methods (including few-shot) do not beat baseline on every dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptBench: A Unified Library for Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5695.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5695.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt engineering technique that instructs models to produce intermediate reasoning steps before a final answer, intended to improve multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (paper indicates CoT implemented across models supported by PromptBench; GPT-4 mentioned in overall comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reasoning tasks (math, logical reasoning, algorithmic problems among those listed in DyVal and other benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring multi-step reasoning where intermediate chain-of-thought steps can help derive final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Chain-of-Thought: prompt includes instructions or exemplars that request intermediate reasoning steps (i.e., step-by-step chain-of-thought).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard direct-answer prompting vs Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: stated to improve reasoning ability and task performance on multi-step problems; PromptBench implements CoT as one of its six prompt engineering methods but does not report numeric values in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (on many reasoning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>By forcing intermediate reasoning outputs, CoT allows the model to realize and articulate multi-step solutions rather than relying on shallow pattern-matching to a final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Paper cautions that prompt engineering methods, including CoT, do not guarantee improvement across every dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptBench: A Unified Library for Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5695.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5695.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot CoT ("Let's think step by step")</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-Shot Chain-of-Thought prompting (append 'Let's think step by step')</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lightweight zero-shot prompting trick where appending a simple phrase ('Let's think step by step') elicits chain-of-thought style outputs and improves reasoning without few-shot exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are zero-shot reasoners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (used as a standard prompt engineering method in PromptBench evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reasoning and multi-step tasks (e.g., arithmetic, logical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring reasoning where zero-shot CoT may induce intermediate reasoning steps improving final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot CoT: append phrase 'Let's think step by step' to the question prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard zero-shot direct-question prompt vs zero-shot CoT appended prompt</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: reported as an effective method to elicit reasoning; PromptBench includes it among prompt engineering techniques but provides no numeric metrics in the paper text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (for reasoning tasks in many cases)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The phrase acts as an instruction that biases the model to produce intermediate reasoning, leveraging internal reasoning capabilities without exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptBench: A Unified Library for Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5695.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5695.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Least-to-Most</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Least-to-Most Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decomposition-based prompting method that breaks complex problems into simpler subproblems and solves them sequentially, using prior subproblem answers to solve later ones.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Least-to-most prompting enables complex reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (implemented as one of PromptBench's prompt-engineering methods)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Complex reasoning tasks that benefit from problem decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where solving simpler subproblems sequentially yields a solution to a harder overall problem.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Least-to-Most: prompts instruct the model to decompose a complex problem into subproblems and solve each step by step.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct prompting/CoT vs Least-to-Most decomposition prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: described as useful for problems harder than exemplars; PromptBench includes it but reports that no method universally beats baseline across all datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved for some complex tasks</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Decomposition reduces cognitive load per step and enables iterative use of intermediate answers to guide later steps.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptBench: A Unified Library for Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5695.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5695.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generated Knowledge Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generated Knowledge Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage prompting technique where the model first generates supporting knowledge and then uses that generated knowledge as input to answer the target question, intended to aid commonsense reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generated knowledge prompting for commonsense reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (implemented in PromptBench as one prompt-engineering method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Commonsense reasoning and related QA tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that require external or intermediate facts (commonsense) to solve questions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Generate supporting knowledge in an intermediate step, then feed the knowledge plus the question to produce final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct prompting vs generated-knowledge two-step prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: reported to help commonsense reasoning tasks; PromptBench implements it but presents no numeric comparisons in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved in some commonsense tasks</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Providing generated knowledge reduces reliance on hidden world-model retrieval in a single pass and grounds the model's answer on the explicitly generated facts.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptBench: A Unified Library for Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5695.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5695.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EmotionPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Emotional-priming Prompting (EmotionPrompt)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt engineering method that adds emotional stimuli or framing (drawn from psychology) to the prompt to influence the model's outputs and potentially improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models understand and can be enhanced by emotional stimuli</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (included among PromptBench's prompt engineering methods)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Tasks where emotional framing could affect responses or decision thresholds (as implemented for evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard NLP or reasoning tasks augmented by emotional phrases appended to prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Add emotional phrase (e.g., 'This is very important to my career.') to original prompt to bias the model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard prompt vs emotionally-primed prompt</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: paper lists EmotionPrompt as a method thought to improve model responses in some contexts; no numeric metrics provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>sometimes improved (context-dependent)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Emotional cues may change the model's internal weighting of outcomes or rhetorical style, improving alignment with human-stated priorities in answers.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptBench: A Unified Library for Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5695.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5695.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExpertPrompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expert Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique that generates an 'expert identity' via exemplars and conditions the answer on that generated expert identity to encourage higher-quality or domain-specific responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Expertprompting: Instructing large language models to be distinguished experts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (implemented in PromptBench)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Tasks where role-specific expertise could help (e.g., technical Q&A, domain reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks tested with role/expert-conditioned prompts versus neutral prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Generate expert identity via instruction-expert exemplars, then ask model to answer conditioned on that identity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard prompt vs expert-identity conditioned prompt</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: the method is described as useful to generate domain-conforming answers in some cases; PromptBench reports method inclusion but not numeric outcomes in the paper body.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved in some domain tasks</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Explicitly conditioning on an expert persona primes the model to use more domain-relevant reasoning patterns and vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptBench: A Unified Library for Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5695.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e5695.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adversarial prompt attacks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adversarial Prompt Attacks (character-, word-, sentence-, semantic-level)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Attacks that alter prompt presentation to degrade model performance: character-level typos, word-level synonym substitutions, sentence-level appended distractors, and semantic-level style alterations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards evaluating the robustness of large language models on adversarial prompts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs evaluated in PromptBench; paper explicitly notes ChatGPT and GPT-4 among tested models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>A range of tasks across PromptBench datasets (classification, QA, reasoning, etc.) used to evaluate robustness to prompt perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard tasks with prompts transformed by adversarial methods to simulate user errors or malicious inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Four attack levels: character-level (typos/TextBugger, DeepWord-Bug), word-level (synonym substitution/TextFooler, BertAttack), sentence-level (append irrelevant sentences/StressTest, CheckList), semantic-level (simulate linguistic styles/region-based).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Clean prompt vs attacked prompt (each attack type compared to original)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: 'All models exhibit vulnerability to adversarial prompts'; ChatGPT and GPT-4 demonstrate the strongest robustness among tested models. No numeric metrics included in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (adversarial formats generally reduce performance)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Attacks mimic realistic input noise and exploit models' sensitivities to surface-form changes, which can mislead token-level or context matching behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptBench: A Unified Library for Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5695.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e5695.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dynamic evaluation (DyVal)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DyVal: Graph-informed dynamic evaluation of LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dynamic evaluation protocol that synthesizes evaluation samples on-the-fly with complexity tailored to models rather than relying on static benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dyval: Graph-informed dynamic evaluation of large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs including GPT-4 (paper reports GPT-4 highest in dynamic eval results)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Synthesized reasoning tasks (seven reasoning types in DyVal: mathematics, logical reasoning, algorithmic analysis, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>On-the-fly generated problems with controllable complexity to probe model reasoning limits (mathematics, boolean/deductive/abductive logic, reachability, max-sum path, linear equations).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Dynamically generated test items tailored by complexity rather than fixed dataset prompts; PromptBench supports DyVal as a protocol for avoiding contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Static benchmark evaluation vs DyVal dynamic evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: 'GPT-4 outperforms its counterparts significantly' in dynamic evaluations; specific weaknesses remain on linear equation, abductive logic, and max sum path tasks. No numeric values reported in the paper body.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>dynamic evaluation revealed performance differences and weaknesses (i.e., dynamic format exposes limitations that static benchmarks might not)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Dynamic, complexity-tailored generation reduces contamination and allows probing of capabilities at controlled difficulty levels, revealing where formats and problem generation expose model weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptBench: A Unified Library for Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5695.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e5695.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Semantic evaluation (MSTemp)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MSTemp: Semantic evaluation protocol (Meta semantic template)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A protocol that generates out-of-distribution samples for evaluation by relying on evaluator LLMs and word replacement, testing sensitivity to semantic rephrasings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Meta semantic template for evaluation of large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (supported by PromptBench for semantic evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Semantic robustness tasks (out-of-distribution / rephrasing tests)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tests that evaluate how semantic-level changes (e.g., paraphrase, synonym replacement, style shifts) affects model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Semantic-level sample generation (word replacement and semantic rephrasing) to produce OOD test cases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>In-distribution prompts vs semantically altered OOD prompts</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: PromptBench supports MSTemp; semantic evaluation is used to generate OOD samples and measure sensitivity; paper does not report numeric values in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced or variable (semantic rephrasing can change model performance)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Semantic rephrasings create distributional shifts that can move examples out of the patterns models learned during training or instruction tuning, revealing robustness gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptBench: A Unified Library for Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5695.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e5695.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt engineering methods (overall)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt engineering techniques evaluated in PromptBench (6 methods)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PromptBench implements six prominent prompt engineering techniques (Chain-of-Thought, Zero-Shot CoT, EmotionPrompt, Expert Prompting, Generated Knowledge, Least-to-Most) and provides tooling to compare them across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (evaluated across the PromptBench suite)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Various tasks across PromptBench datasets (reasoning, commonsense, math, QA, NLU)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A heterogeneous set intended to reveal which prompt engineering techniques help on which tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Each method represents a different presentation/structure of problems (intermediate reasoning steps, exemplars, emotional framing, role priming, generated knowledge, decomposition).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Multiple methods compared against baseline direct prompting (paper references Figure 4 for comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: 'Most methods are effective for special fields, so these methods cannot surpass the baseline in every dataset'  i.e., effectiveness is task-dependent; no numeric table in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>mixed (some methods improve on some tasks, none universally superior)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Different prompt engineering strategies leverage different model capabilities (e.g., chain-of-thought leverages internal multi-step reasoning, generated-knowledge provides grounding), so their effectiveness depends on alignment between method and task structure.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>The paper explicitly states that prompt engineering methods do not universally improve performance across all datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptBench: A Unified Library for Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Generated knowledge prompting for commonsense reasoning <em>(Rating: 2)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Towards evaluating the robustness of large language models on adversarial prompts <em>(Rating: 2)</em></li>
                <li>Dyval: Graph-informed dynamic evaluation of large language models <em>(Rating: 2)</em></li>
                <li>Meta semantic template for evaluation of large language models <em>(Rating: 2)</em></li>
                <li>Large language models understand and can be enhanced by emotional stimuli <em>(Rating: 1)</em></li>
                <li>Expertprompting: Instructing large language models to be distinguished experts <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5695",
    "paper_id": "paper-266191741",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Prompt sensitivity",
            "name_full": "Sensitivity of LLMs to prompt wording and format",
            "brief_description": "An observed phenomenon that LLM performance can change substantially depending on prompt wording, structure, context, and formatting; motivates PromptBench and the inclusion of many prompt formats and robustness tests.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs evaluated in PromptBench (open-source and proprietary; paper explicitly mentions ChatGPT and GPT-4 among others)",
            "model_size": null,
            "task_name": "Various tasks across 22 public datasets (GLUE tasks, MMLU, SQuAD v2, Math, GSM8K, CommonsenseQA, VQA, MMMU, MathVista, etc.)",
            "task_description": "A broad set of NLP, reasoning, math, multimodal, and translation tasks used to measure how model outputs change across prompt formats and attacks.",
            "problem_format": "Varied: task-oriented prompts, role-oriented prompts, zero-shot, few-shot (3 examples), chain-of-thought styles, generated-knowledge prompting, expert/role priming, emotional priming, least-to-most decomposition, and adversarially transformed prompts (character/word/sentence/semantic).",
            "comparison_format": null,
            "performance": "Qualitative: performance varies widely with prompt format; no single prompt format uniformly best across datasets (paper reports variable effects across tasks and methods).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "varies (can improve or reduce performance depending on format and task)",
            "explanation_or_hypothesis": "The paper states that prompt wording and format change the model's behavior by priming roles, providing exemplars or intermediate reasoning steps, or by introducing noise/ distractions; therefore prompt structure leverages or misleads the model's learned patterns, altering outputs.",
            "counterexample_or_null_result": "Paper notes that many prompt engineering methods are effective only on special fields and 'cannot surpass the baseline in every dataset' (i.e., no universal improvement).",
            "uuid": "e5695.0",
            "source_info": {
                "paper_title": "PromptBench: A Unified Library for Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Prompt types: task-oriented vs role-oriented",
            "name_full": "Task-oriented and Role-oriented Prompt Formats",
            "brief_description": "Two high-level prompt presentation styles: task-oriented prompts explicitly state the task, while role-oriented prompts position the model in a role (expert/advisor/etc.) to guide answer style.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs evaluated in PromptBench (including GPT series, LLaMA/Llama2, Mixtral, Mistral, Vicuna, etc.)",
            "model_size": null,
            "task_name": "Various classification, NLI, grammar, and other tasks drawn from PromptBench datasets",
            "task_description": "Standard NLP tasks where the instruction/prompt framing can shape the expected label or output format.",
            "problem_format": "Task-oriented: explicit instruction of the expected output (e.g., 'Determine if the given pair... respond with equivalent or not equivalent'). Role-oriented: instruct model to adopt a role (e.g., 'Functioning as a grammar evaluation tool...').",
            "comparison_format": "Task-oriented prompts vs Role-oriented prompts (paper allows evaluating lists of prompts and compares performance across them).",
            "performance": "Qualitative: role-oriented prompts can subtly guide expected output format and behavior; PromptBench uses both types and reports that performance depends on dataset and prompt choice.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "context-dependent (can improve alignment to expected answer format and sometimes accuracy)",
            "explanation_or_hypothesis": "Role priming changes the model's behavioral prior, encouraging outputs in the desired style or level of expertise; task-oriented prompts more directly constrain output content.",
            "counterexample_or_null_result": null,
            "uuid": "e5695.1",
            "source_info": {
                "paper_title": "PromptBench: A Unified Library for Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Few-shot vs Zero-shot",
            "name_full": "Few-shot and Zero-shot Prompting (including few-shot with 3 exemplars used by PromptBench)",
            "brief_description": "PromptBench supports zero-shot and few-shot prompting; for few-shot evaluation they randomly choose 3 training set examples per task as exemplars.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (GPT series, Llama2, Mistral, etc.)",
            "model_size": null,
            "task_name": "Multiple tasks (classification, NLI, reasoning, math, etc.)",
            "task_description": "Standard tasks evaluated under zero-shot and few-shot conditions to measure how exemplars in the prompt affect output.",
            "problem_format": "Zero-shot: no exemplars; Few-shot: prompt includes 3 training examples (question/answer pairs) before test item.",
            "comparison_format": "Zero-shot vs Few-shot (3-shot) comparisons supported",
            "performance": "Qualitative: few-shot exemplars are used to improve performance on some tasks; PromptBench reports dataset-dependent gains but no universal numeric improvements in the paper text.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "often improved on some tasks, but not universally",
            "explanation_or_hypothesis": "Providing exemplars gives explicit in-context examples of the desired mapping, which can bias the model toward correct output formats and answers; effect depends on exemplar selection and task.",
            "counterexample_or_null_result": "Paper explicitly notes that prompt engineering methods (including few-shot) do not beat baseline on every dataset.",
            "uuid": "e5695.2",
            "source_info": {
                "paper_title": "PromptBench: A Unified Library for Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Chain-of-Thought",
            "name_full": "Chain-of-Thought Prompting",
            "brief_description": "A prompt engineering technique that instructs models to produce intermediate reasoning steps before a final answer, intended to improve multi-step reasoning.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (paper indicates CoT implemented across models supported by PromptBench; GPT-4 mentioned in overall comparisons)",
            "model_size": null,
            "task_name": "Reasoning tasks (math, logical reasoning, algorithmic problems among those listed in DyVal and other benchmarks)",
            "task_description": "Tasks requiring multi-step reasoning where intermediate chain-of-thought steps can help derive final answers.",
            "problem_format": "Chain-of-Thought: prompt includes instructions or exemplars that request intermediate reasoning steps (i.e., step-by-step chain-of-thought).",
            "comparison_format": "Standard direct-answer prompting vs Chain-of-Thought prompting",
            "performance": "Qualitative: stated to improve reasoning ability and task performance on multi-step problems; PromptBench implements CoT as one of its six prompt engineering methods but does not report numeric values in the text.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved (on many reasoning tasks)",
            "explanation_or_hypothesis": "By forcing intermediate reasoning outputs, CoT allows the model to realize and articulate multi-step solutions rather than relying on shallow pattern-matching to a final answer.",
            "counterexample_or_null_result": "Paper cautions that prompt engineering methods, including CoT, do not guarantee improvement across every dataset.",
            "uuid": "e5695.3",
            "source_info": {
                "paper_title": "PromptBench: A Unified Library for Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Zero-shot CoT (\"Let's think step by step\")",
            "name_full": "Zero-Shot Chain-of-Thought prompting (append 'Let's think step by step')",
            "brief_description": "A lightweight zero-shot prompting trick where appending a simple phrase ('Let's think step by step') elicits chain-of-thought style outputs and improves reasoning without few-shot exemplars.",
            "citation_title": "Large language models are zero-shot reasoners",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (used as a standard prompt engineering method in PromptBench evaluations)",
            "model_size": null,
            "task_name": "Reasoning and multi-step tasks (e.g., arithmetic, logical reasoning)",
            "task_description": "Tasks requiring reasoning where zero-shot CoT may induce intermediate reasoning steps improving final answers.",
            "problem_format": "Zero-shot CoT: append phrase 'Let's think step by step' to the question prompt.",
            "comparison_format": "Standard zero-shot direct-question prompt vs zero-shot CoT appended prompt",
            "performance": "Qualitative: reported as an effective method to elicit reasoning; PromptBench includes it among prompt engineering techniques but provides no numeric metrics in the paper text.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved (for reasoning tasks in many cases)",
            "explanation_or_hypothesis": "The phrase acts as an instruction that biases the model to produce intermediate reasoning, leveraging internal reasoning capabilities without exemplars.",
            "counterexample_or_null_result": null,
            "uuid": "e5695.4",
            "source_info": {
                "paper_title": "PromptBench: A Unified Library for Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Least-to-Most",
            "name_full": "Least-to-Most Prompting",
            "brief_description": "A decomposition-based prompting method that breaks complex problems into simpler subproblems and solves them sequentially, using prior subproblem answers to solve later ones.",
            "citation_title": "Least-to-most prompting enables complex reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (implemented as one of PromptBench's prompt-engineering methods)",
            "model_size": null,
            "task_name": "Complex reasoning tasks that benefit from problem decomposition",
            "task_description": "Tasks where solving simpler subproblems sequentially yields a solution to a harder overall problem.",
            "problem_format": "Least-to-Most: prompts instruct the model to decompose a complex problem into subproblems and solve each step by step.",
            "comparison_format": "Direct prompting/CoT vs Least-to-Most decomposition prompting",
            "performance": "Qualitative: described as useful for problems harder than exemplars; PromptBench includes it but reports that no method universally beats baseline across all datasets.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved for some complex tasks",
            "explanation_or_hypothesis": "Decomposition reduces cognitive load per step and enables iterative use of intermediate answers to guide later steps.",
            "counterexample_or_null_result": null,
            "uuid": "e5695.5",
            "source_info": {
                "paper_title": "PromptBench: A Unified Library for Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Generated Knowledge Prompting",
            "name_full": "Generated Knowledge Prompting",
            "brief_description": "A two-stage prompting technique where the model first generates supporting knowledge and then uses that generated knowledge as input to answer the target question, intended to aid commonsense reasoning.",
            "citation_title": "Generated knowledge prompting for commonsense reasoning",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (implemented in PromptBench as one prompt-engineering method)",
            "model_size": null,
            "task_name": "Commonsense reasoning and related QA tasks",
            "task_description": "Tasks that require external or intermediate facts (commonsense) to solve questions.",
            "problem_format": "Generate supporting knowledge in an intermediate step, then feed the knowledge plus the question to produce final answer.",
            "comparison_format": "Direct prompting vs generated-knowledge two-step prompting",
            "performance": "Qualitative: reported to help commonsense reasoning tasks; PromptBench implements it but presents no numeric comparisons in the main text.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved in some commonsense tasks",
            "explanation_or_hypothesis": "Providing generated knowledge reduces reliance on hidden world-model retrieval in a single pass and grounds the model's answer on the explicitly generated facts.",
            "counterexample_or_null_result": null,
            "uuid": "e5695.6",
            "source_info": {
                "paper_title": "PromptBench: A Unified Library for Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "EmotionPrompt",
            "name_full": "Emotional-priming Prompting (EmotionPrompt)",
            "brief_description": "A prompt engineering method that adds emotional stimuli or framing (drawn from psychology) to the prompt to influence the model's outputs and potentially improve performance.",
            "citation_title": "Large language models understand and can be enhanced by emotional stimuli",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (included among PromptBench's prompt engineering methods)",
            "model_size": null,
            "task_name": "Tasks where emotional framing could affect responses or decision thresholds (as implemented for evaluation)",
            "task_description": "Standard NLP or reasoning tasks augmented by emotional phrases appended to prompts.",
            "problem_format": "Add emotional phrase (e.g., 'This is very important to my career.') to original prompt to bias the model.",
            "comparison_format": "Standard prompt vs emotionally-primed prompt",
            "performance": "Qualitative: paper lists EmotionPrompt as a method thought to improve model responses in some contexts; no numeric metrics provided in text.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "sometimes improved (context-dependent)",
            "explanation_or_hypothesis": "Emotional cues may change the model's internal weighting of outcomes or rhetorical style, improving alignment with human-stated priorities in answers.",
            "counterexample_or_null_result": null,
            "uuid": "e5695.7",
            "source_info": {
                "paper_title": "PromptBench: A Unified Library for Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "ExpertPrompting",
            "name_full": "Expert Prompting",
            "brief_description": "A technique that generates an 'expert identity' via exemplars and conditions the answer on that generated expert identity to encourage higher-quality or domain-specific responses.",
            "citation_title": "Expertprompting: Instructing large language models to be distinguished experts",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (implemented in PromptBench)",
            "model_size": null,
            "task_name": "Tasks where role-specific expertise could help (e.g., technical Q&A, domain reasoning)",
            "task_description": "Tasks tested with role/expert-conditioned prompts versus neutral prompts.",
            "problem_format": "Generate expert identity via instruction-expert exemplars, then ask model to answer conditioned on that identity.",
            "comparison_format": "Standard prompt vs expert-identity conditioned prompt",
            "performance": "Qualitative: the method is described as useful to generate domain-conforming answers in some cases; PromptBench reports method inclusion but not numeric outcomes in the paper body.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved in some domain tasks",
            "explanation_or_hypothesis": "Explicitly conditioning on an expert persona primes the model to use more domain-relevant reasoning patterns and vocabulary.",
            "counterexample_or_null_result": null,
            "uuid": "e5695.8",
            "source_info": {
                "paper_title": "PromptBench: A Unified Library for Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Adversarial prompt attacks",
            "name_full": "Adversarial Prompt Attacks (character-, word-, sentence-, semantic-level)",
            "brief_description": "Attacks that alter prompt presentation to degrade model performance: character-level typos, word-level synonym substitutions, sentence-level appended distractors, and semantic-level style alterations.",
            "citation_title": "Towards evaluating the robustness of large language models on adversarial prompts",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs evaluated in PromptBench; paper explicitly notes ChatGPT and GPT-4 among tested models",
            "model_size": null,
            "task_name": "A range of tasks across PromptBench datasets (classification, QA, reasoning, etc.) used to evaluate robustness to prompt perturbations",
            "task_description": "Standard tasks with prompts transformed by adversarial methods to simulate user errors or malicious inputs.",
            "problem_format": "Four attack levels: character-level (typos/TextBugger, DeepWord-Bug), word-level (synonym substitution/TextFooler, BertAttack), sentence-level (append irrelevant sentences/StressTest, CheckList), semantic-level (simulate linguistic styles/region-based).",
            "comparison_format": "Clean prompt vs attacked prompt (each attack type compared to original)",
            "performance": "Qualitative: 'All models exhibit vulnerability to adversarial prompts'; ChatGPT and GPT-4 demonstrate the strongest robustness among tested models. No numeric metrics included in main text.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "reduced (adversarial formats generally reduce performance)",
            "explanation_or_hypothesis": "Attacks mimic realistic input noise and exploit models' sensitivities to surface-form changes, which can mislead token-level or context matching behavior.",
            "counterexample_or_null_result": null,
            "uuid": "e5695.9",
            "source_info": {
                "paper_title": "PromptBench: A Unified Library for Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Dynamic evaluation (DyVal)",
            "name_full": "DyVal: Graph-informed dynamic evaluation of LLMs",
            "brief_description": "A dynamic evaluation protocol that synthesizes evaluation samples on-the-fly with complexity tailored to models rather than relying on static benchmarks.",
            "citation_title": "Dyval: Graph-informed dynamic evaluation of large language models",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs including GPT-4 (paper reports GPT-4 highest in dynamic eval results)",
            "model_size": null,
            "task_name": "Synthesized reasoning tasks (seven reasoning types in DyVal: mathematics, logical reasoning, algorithmic analysis, etc.)",
            "task_description": "On-the-fly generated problems with controllable complexity to probe model reasoning limits (mathematics, boolean/deductive/abductive logic, reachability, max-sum path, linear equations).",
            "problem_format": "Dynamically generated test items tailored by complexity rather than fixed dataset prompts; PromptBench supports DyVal as a protocol for avoiding contamination.",
            "comparison_format": "Static benchmark evaluation vs DyVal dynamic evaluation",
            "performance": "Qualitative: 'GPT-4 outperforms its counterparts significantly' in dynamic evaluations; specific weaknesses remain on linear equation, abductive logic, and max sum path tasks. No numeric values reported in the paper body.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "dynamic evaluation revealed performance differences and weaknesses (i.e., dynamic format exposes limitations that static benchmarks might not)",
            "explanation_or_hypothesis": "Dynamic, complexity-tailored generation reduces contamination and allows probing of capabilities at controlled difficulty levels, revealing where formats and problem generation expose model weaknesses.",
            "counterexample_or_null_result": null,
            "uuid": "e5695.10",
            "source_info": {
                "paper_title": "PromptBench: A Unified Library for Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Semantic evaluation (MSTemp)",
            "name_full": "MSTemp: Semantic evaluation protocol (Meta semantic template)",
            "brief_description": "A protocol that generates out-of-distribution samples for evaluation by relying on evaluator LLMs and word replacement, testing sensitivity to semantic rephrasings.",
            "citation_title": "Meta semantic template for evaluation of large language models",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (supported by PromptBench for semantic evaluation)",
            "model_size": null,
            "task_name": "Semantic robustness tasks (out-of-distribution / rephrasing tests)",
            "task_description": "Tests that evaluate how semantic-level changes (e.g., paraphrase, synonym replacement, style shifts) affects model outputs.",
            "problem_format": "Semantic-level sample generation (word replacement and semantic rephrasing) to produce OOD test cases.",
            "comparison_format": "In-distribution prompts vs semantically altered OOD prompts",
            "performance": "Qualitative: PromptBench supports MSTemp; semantic evaluation is used to generate OOD samples and measure sensitivity; paper does not report numeric values in the main text.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "reduced or variable (semantic rephrasing can change model performance)",
            "explanation_or_hypothesis": "Semantic rephrasings create distributional shifts that can move examples out of the patterns models learned during training or instruction tuning, revealing robustness gaps.",
            "counterexample_or_null_result": null,
            "uuid": "e5695.11",
            "source_info": {
                "paper_title": "PromptBench: A Unified Library for Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Prompt engineering methods (overall)",
            "name_full": "Prompt engineering techniques evaluated in PromptBench (6 methods)",
            "brief_description": "PromptBench implements six prominent prompt engineering techniques (Chain-of-Thought, Zero-Shot CoT, EmotionPrompt, Expert Prompting, Generated Knowledge, Least-to-Most) and provides tooling to compare them across tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (evaluated across the PromptBench suite)",
            "model_size": null,
            "task_name": "Various tasks across PromptBench datasets (reasoning, commonsense, math, QA, NLU)",
            "task_description": "A heterogeneous set intended to reveal which prompt engineering techniques help on which tasks.",
            "problem_format": "Each method represents a different presentation/structure of problems (intermediate reasoning steps, exemplars, emotional framing, role priming, generated knowledge, decomposition).",
            "comparison_format": "Multiple methods compared against baseline direct prompting (paper references Figure 4 for comparisons).",
            "performance": "Qualitative: 'Most methods are effective for special fields, so these methods cannot surpass the baseline in every dataset'  i.e., effectiveness is task-dependent; no numeric table in main text.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "mixed (some methods improve on some tasks, none universally superior)",
            "explanation_or_hypothesis": "Different prompt engineering strategies leverage different model capabilities (e.g., chain-of-thought leverages internal multi-step reasoning, generated-knowledge provides grounding), so their effectiveness depends on alignment between method and task structure.",
            "counterexample_or_null_result": "The paper explicitly states that prompt engineering methods do not universally improve performance across all datasets.",
            "uuid": "e5695.12",
            "source_info": {
                "paper_title": "PromptBench: A Unified Library for Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Generated knowledge prompting for commonsense reasoning",
            "rating": 2,
            "sanitized_title": "generated_knowledge_prompting_for_commonsense_reasoning"
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models",
            "rating": 2,
            "sanitized_title": "leasttomost_prompting_enables_complex_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Towards evaluating the robustness of large language models on adversarial prompts",
            "rating": 2,
            "sanitized_title": "towards_evaluating_the_robustness_of_large_language_models_on_adversarial_prompts"
        },
        {
            "paper_title": "Dyval: Graph-informed dynamic evaluation of large language models",
            "rating": 2,
            "sanitized_title": "dyval_graphinformed_dynamic_evaluation_of_large_language_models"
        },
        {
            "paper_title": "Meta semantic template for evaluation of large language models",
            "rating": 2,
            "sanitized_title": "meta_semantic_template_for_evaluation_of_large_language_models"
        },
        {
            "paper_title": "Large language models understand and can be enhanced by emotional stimuli",
            "rating": 1,
            "sanitized_title": "large_language_models_understand_and_can_be_enhanced_by_emotional_stimuli"
        },
        {
            "paper_title": "Expertprompting: Instructing large language models to be distinguished experts",
            "rating": 1,
            "sanitized_title": "expertprompting_instructing_large_language_models_to_be_distinguished_experts"
        }
    ],
    "cost": 0.0187765,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PromptBench: A Unified Library for Evaluation of Large Language Models
20 Aug 2024</p>
<p>Kaijie Zhu 
Institute of Automation
Chinese Academy of Sciences</p>
<p>Qinlin Zhao 
University of Science and Technology of China</p>
<p>Hao Chen 
Carnegie Mellon University</p>
<p>Jindong Wang jindong.wang@microsoft.com 
Xing Xie 
Microsoft Research 
Zhu 
Zhao 
Chen 
Xie Wang 
Adversarial Promptbench 
Ood </p>
<p>Qinlin Zhao
Jindong WangHao Chen, Xing Xie</p>
<p>Foundation Scenario Protocol Benchmark Natural language Reasoning Agent Interdisciplinary Hallucination Bias Others Standard Dynamic Semantic PromptBench aka.ms/promptbench Principled</p>
<p>PromptBench: A Unified Library for Evaluation of Large Language Models
20 Aug 20240486FA193CFF1E0F0E0D97FE10F3BC7CarXiv:2312.07910v3[cs.AI]Submitted 1/24; Revised 6/24;Evaluationlarge language modelsframework
The evaluation of large language models (LLMs) is crucial to assess their performance and mitigate potential security risks.In this paper, we introduce PromptBench, a unified library to evaluate LLMs.It consists of several key components that can be easily used and extended by researchers: prompt construction, prompt engineering, dataset and model loading, adversarial prompt attack, dynamic evaluation protocols, and analysis tools.PromptBench is designed as an open, general, and flexible codebase for research purpose.It aims to facilitate original study in creating new benchmarks, deploying downstream applications, and designing new evaluation protocols.The code is available at: https://github.com/microsoft/promptbenchand will be continuously supported.</p>
<p>Introduction</p>
<p>Large language models (LLMs) are revolutionizing aspects of human life and society, such as medical diagnostics (McDuff et al., 2023;Thirunavukarasu et al., 2024), and educational tools (Ho et al., 2023).Evaluation is of paramount importance to understand the true capabilities of LLMs, mitigate potential risks, and eventually, benefit society further (Eisenstein, 2023;Chang et al., 2023).Recent efforts have evaluated LLMs from diverse aspects (Hendrycks et al., 2021;Liang et al., 2022;Zheng et al., 2023;Li et al., 2023c;Huang et al., 2023;HuggingFace, 2023).Among the findings, one of the most important is that current LLMs are sensitive to prompts (Wang et al., 2023b), vulnerable to adversarial prompt attacks (Zhu et al., 2023b), and exposed to testset data contamination (Willig et al., 2023;Zhou et al., 2023b;Zhu et al., 2023a), which pose severe security and privacy issues (Wang et al., 2023a;Simmons, 2022).On top of that, there have been various prompt learning algorithms developed based on different evaluation metrics, such as BDPL (Diao et al., 2022), GrIPS (Prasad et al., 2022) and Plum (Pan et al., 2023).Given the increasing popularity of LLMs, it is indispensable to develop a unified codebase to enable easy, fast, and flexible evaluation of large foundation models.</p>
<p>There are existing libraries such as LlamaIndex (Liu, 2022), semantic kernel (Microsoft, 2023), and LangChain (Chase, 2022).LlamaIndex and LangChain enhance LLM applications by incorporating databases and various data sources.Semantic Kernel aims to knowledge  merge AI services with programming languages for versatile AI app development.Evalharness (Gao et al., 2023) offers a comprehensive framework for evaluating generative language models.Zeno (Zeno, 2023) is an AI evaluation platform supporting interaction and visualization, but it is not easy to customize.LiteLLM (BerriAI, 2023) implements a unified API call for different LLM service prodiders.This paper introduces PromptBench, a unified Python library to evaluate LLMs from comprehensive dimensions.It is designed to fill the gaps current libraries have, offering comprehensive support not only for standard model evaluations but also for advanced scenarios including adversarial prompt attacks and dynamic evaluations.Its extensible architecture allows for the incorporation of new evaluation protocols, addressing the limitations found in other tools.The detailed comparisons are shown in Appendix A, highlighting how PromptBench provides a more complete toolkit for the nuanced evaluation of language models, especially in research contexts where adaptability and thoroughness are paramount.It consists of a wide range of LLMs and evaluation datasets, covering diverse tasks, evaluation protocols, adversarial prompt attacks, and prompt engineering techniques.As a holistic library, it also supports several analysis tools for interpreting the results.Our library is designed in a modular fashion, allowing researchers to easily build evaluation pipelines for their own projects.We open-source PromptBench with comprehensive documents and tutorials 1 to support easy, flexible, and collaborative evaluation.We believe PromptBench could enhance our understanding of LLMs and spur new research within the community.</p>
<p>PromptBench</p>
<p>PromptBench can be easily installed either via pip install promptbench or git clone.In this section, we briefly introduce the components of PromptBench and how to use it to build an evaluation pipeline for LLMs.An overview of PromptBench is shown in Figure 1.</p>
<p>Components</p>
<p>Models.</p>
<p>PromptBench supports both open-source and proprietary LLMs and VLMs and it is open to add more.Currently, it supports a diverse range of LLMs and VLMs, ranging from Llama2 series (Touvron et al., 2023b), Mixtral series (Jiang et al., 2024), LlaVa series (Liu et al., 2023a) to GPT series (OpenAI, 2023a,b).PromptBench provides unified LLMModel and VLMModel interfaces to allow easy construction and inference of a model with specified max generating tokens and generating temperature.The interfaces also support customized models, including those that have been fine-tuned for specific applications.More details of the supported models are shown in Appendix B.1.</p>
<p>Datasets and tasks.PromptBench comprises a wide array of tasks, currently supporting diverse challenges across 12 tasks and 22 public datasets, with the capacity for additional expansions.The supported tasks include fundamental NLP tasks such as sentiment analysis, grammar correctness, and duplicate sentence detection, as well as complex challenges involving natural language inference, multi-task knowledge, and reading comprehension.It also covers specialized areas like translation, mathematical problem-solving, and various forms of reasoning-logical, commonsense, symbolic, and algorithmic.For detailed descriptions of each dataset and specific task configurations (see Appendix B.2).The unified DatasetLoader interface facilitates easy and customizable loading and processing of these datasets, enhancing the usability and flexibility of PromptBench.</p>
<p>Prompts and prompt engineering.PromptBench offers a suite of 4 distinct prompt types, and additionally, users have the flexibility to craft custom prompts using the Prompt interface.Task-oriented prompts are structured to clearly delineate the specific task expected of the model, whereas role-oriented prompts position the model in a defined role, such as an expert, advisor, or translator.These prompt categories are adaptable for both zero-shot and few-shot learning contexts, offering diverse application possibilities.Moreover, PromptBench currently includes 6 prominent prompt engineering methods, details can be found in Appendix B.4.2.Our framework is not only equipped for the easy integration of these existing techniques through the prompt engineering module but is also actively evolving to encompass a broader spectrum of prompt engineering methods, enhancing its adaptability in varied evaluation scenarios.</p>
<p>Adversarial prompt attacks.To facilitate the investigation of LLMs' robustness on prompts, PromptBench integrates 4 types of attacks: (1) character-level attacks (Li et al., 2019;Gao et al., 2018), which manipulate texts by introducing typos or errors to words;</p>
<p>(2) word-level attacks (Jin et al., 2019;Li et al., 2020), which aim to replace words with synonyms or contextually similar words to deceive LLMs; (3) sentence-level attacks (Ribeiro et al., 2020;Naik et al., 2018), which append irrelevant or extraneous sentences to the end of prompts, intending to distract LLMs; (4) semantic-level attacks (Zhu et al., 2023b), which simulate the linguistic behavior of people from different countries.Details can be found in Appendix B.4.3These attacks can be easily called via the prompt attack interface.It also supports the usage of curated adversarial prompts to efficiently evaluate robustness.</p>
<p>Different evaluation protocols.By default, PromptBench supports the standard protocol, i.e., the direct inference.PromptBench further supports dynamic (Zhu et al., 2023a) and semantic (Liu et al., 2023b) evaluation protocols by dynamically generating testing data.It is open to integrate more new protocols to avoid data contamination.</p>
<p>Analysis tools.Finally, PromptBench offers a series of analysis tools to help researchers analyze their results.Particularly, it support sweep running to get the benchmark results.Then, attention visualization analysis can be done through the utils interface.</p>
<p>PromptBench also supports word frequency analysis to analyze the words used in attacks as well as defense analysis by integrating word correction tools.</p>
<p>Evaluation pipeline</p>
<p>PromptBench allows easy construction of an evaluation pipeline via four steps.Firstly, specify task and then load dataset via pb.DatasetLoader.PromptBench offers a streamlined one-line API for loading the desired dataset.Secondly, users can customize LLMs using the pb.LLMModel, which provides integrated inference pipelines compatible with most LLMs implemented in Huggingface.Thirdly, the prompt for the specified dataset and task is defined via pb.Prompt.Users have the option to input a list of prompts for evaluation and performance comparison.In cases where no prompts are supplied, our default prompts for the dataset are utilized.Finally, the pipeline requires the definition of input and output processing functions via class InputProcess and class OutputProcess defined in pb.utils.dataprocess,as well as the evaluation function via pb.metrics.The detailed introduction of the components are shown in Appendix B.</p>
<p>Supported research topics</p>
<p>Compared to current evaluation libraries, PromptBench is designed mainly for research purpose, thus it is easy to customize for different topics.As shown in Figure 1(b), it supports different evaluation topics from the research community including benchmarks, scenarios, and protocols.In benchmarks research, it supports standard natural language understanding, natural language generation, and reasoning tasks.It can also be extended to support research on AI agent and interdisciplinary study.In scenario research, it supports adversarial and out-of-distribution evaluation, and can also support other topics such as hallucination and bias by changing the metrics and DatasetLoader interface.In protocol, it naturally supports standard and dynamic evaluation, and can further be verified by including measurement theory.PromptBench offers three leaderboards to allow easy comparison: adversarial prompt attack, prompt engineering, and dynamic evaluation, as shown in Appendix C. Researchers are welcome to submit new results to our platform.Extensibility is shown in Appendix D that allows convenient extension of the framework.</p>
<p>Conclusion and Discussion</p>
<p>We presented PromptBench, a unified framework for LLMs evaluation, designed in a modular fashion to build evaluation pipelines with various models, tasks, and prompts.It supports research in prompt engineering, adversarial attacks, and dynamic evaluation.Prompt-Bench is the first step in assessing and exploring the capabilities of current LLMs.We believe our benchmark and analysis will inform the design of more robust, human-aligned models.In the future, new datasets, evaluation protocols, prompt variations, and analytical tools will be added into PromptBench.We also welcome any contributions for PromptBench.</p>
<p>Despite its versatility, PromptBench has limitations.It may not cover all evaluation scenarios, and some metrics might miss nuanced performance differences.The framework's effectiveness depends on the quality and diversity of datasets and prompts.Addressing these limitations is a priority for our ongoing and future work.</p>
<p> GPT-NEOX-20B (Black et al., 2022): This variant, part of the extensive GPT model series, features 20 billion parameters, exemplifying large-scale language model implementation.</p>
<p> Flan-UL2 (Brain, 2023): Flan-UL2, an encoder-decoder model, is grounded in the T5 architecture and enhanced with Flan prompt tuning and dataset techniques.</p>
<p> phi-1.5 and phi-2 (Li et al., 2023d): phi-1.5 is an LLM with 1.3 billion parameters, builds upon the dataset used for phi-1 with the addition of diverse NLP synthetic texts.</p>
<p> Mistral 7B (Jiang et al., 2023): Mistral 7B is trained by Mistral AI team.It excels in tasks like reasoning, mathematics, and code generation.It uses grouped-query attention for faster inference and sliding window attention for efficient handling of sequences.</p>
<p>There's also an instruction-following version, Mistral 7B-Instruct. Yi series (Yi, 2023): Developed by 01.AI, the Yi series are next-generation open-source large language models.Trained on a 3T multilingual corpus, they excel in language understanding, commonsense reasoning, and reading comprehension.</p>
<p> BLIP2 (Li et al., 2023b): This visual-language model is proposed by Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. BLIP-2 leverages frozen pre-trained image encoders and large language models (LLMs) by training a lightweight, 12-layer Transformer encoder in between them, achieving excellent performance in various vision-language tasks</p>
<p> LLaVA (Liu et al., 2024): LlaVA (Language-Image LLaMA) is a multimodal model combining language and image data.It extends the LLaMA architecture to handle both modalities, enabling tasks like image captioning, visual question answering, and imagebased text generation.</p>
<p> Qwen-VL series (Bai et al., 2023): Qwen-VL (Qwen Large Vision Language Model) is the multimodal version of the large model series, Qwen (abbr.Tongyi Qianwen), proposed by Alibaba Cloud.Qwen-VL accepts image, text, and bounding box as inputs, outputs text, and bounding box.</p>
<p> InternLM-XComposer2-VL (Dong et al., 2024): InternLM-XComposer2 is a cuttingedge vision-language model excelling in free-form text-image composition and comprehension, crafting content from diverse inputs like outlines, detailed specs, and reference images.Using a Partial LoRA (PLoRA) approach, it balances vision understanding and text composition.</p>
<p>Proprietary LLMs:</p>
<p> ChatGPT (OpenAI, 2023a) and GPT-4 (OpenAI, 2023b): OpenAI's ChatGPT and GPT-4 are advanced iterations of the GPT series.ChatGPT is tailored for interactive tasks, while GPT-4 is the most proficient in the series and supports image input.</p>
<p> PaLM 2 (Anil et al., 2023): PaLM 2 is an advanced language model that excels in multilingual and reasoning capabilities, offering greater computational efficiency than its predecessor, PaLM.This Transformer-based model enhances performance across various model sizes in English, multilingual tasks, and reasoning challenges.</p>
<p> Gemini (Google, 2023): The Gemini model is a multimodal language model developed by Google AI, capable of extracting insights from a diverse array of data formats, including images, and video.</p>
<p>B.2 Tasks and Datasets</p>
<p> GLUE (Wang et al., 2019): The GLUE benchmark (General Language Understanding Evaluation) offers a suite of tasks to evaluate the capability of NLP models in understanding language.For this research, we employed 8 specific tasks: Sentiment Analysis (SST-2 (Socher et al., 2013)), Grammar Correctness (CoLA (Warstadt et al., 2018)), Identifying Duplicate Sentences (QQP (Wang et al., 2017), MRPC (Dolan and Brockett, 2005)), and various Natural Language Inference tasks (MNLI (Williams et al., 2018), QNLI (Wang et al., 2019), RTE (Wang et al., 2019), WNLI (Levesque et al., 2012)).</p>
<p> MMLU (Hendrycks et al., 2021): The MMLU dataset tests the broad knowledge and problem-solving skills of large language models through 57 tasks with multiple-choice questions in fields like mathematics, history, and computer science.It is a comprehensive multitask benchmark.</p>
<p> SQuAD V2 (Rajpurkar et al., 2018): The SQuAD v2 dataset is pivotal in training and assessing NLP models for reading comprehension.It builds upon the original SQuAD by adding unanswerable questions, making it more challenging.Models must either identify the correct answer in the text or recognize questions as unanswerable.</p>
<p> UN Multi (Eisele and Chen, 2010): Comprising texts in the six official United Nations languages, the Multi UN dataset is a vast parallel corpus from UN documents.However, its focus on formal texts may restrict its use in informal or conversational language contexts.</p>
<p> IWSLT 2017 (Cettolo et al., 2017): Designed for spoken language translation system evaluation, the IWSLT 2017 dataset includes multilingual, multi-domain text data, primarily from the TED Talks Open Translation Project.It encompasses numerous language pairs, providing a rich resource for translation tasks.</p>
<p> Math (Saxton et al., 2019): The DeepMind Mathematics Dataset assesses AI models' mathematical reasoning by posing a wide array of math problems, from algebra to calculus.It tests the models' understanding and logical reasoning in mathematics.</p>
<p> BIG-Bench (bench authors, 2023): BIG-bench is a collaborative benchmark designed to evaluate the capabilities of large language models and predict their future potential.It consists of over 200 tasks, contributed by 444 authors from 132 institutions, covering a wide range of topics like linguistics, math, common-sense reasoning, and more.These tasks are intended to probe areas believed to be beyond the current capabilities of LMs.</p>
<p> GSM8K (Cobbe et al., 2021): The GSM8K dataset is a collection of 8.5K highquality, linguistically diverse grade school math word problems.It was created by human problem writers and is divided into 7.5K training problems and 1K test problems.These problems, which require 2 to 8 steps to solve, primarily involve basic arithmetic operations and are designed to be solvable by a bright middle school student.</p>
<p> CommonsenseQA (Talmor et al., 2019): The CommonsenseQA dataset is a challenging commonsense question-answering dataset.It comprises 12,247 questions with 5 multiple-choice answers each.</p>
<p> QASC (Khot et al., 2020): QASC (Question Answering via Sentence Composition) is a specialized collection designed for question-answering tasks with a focus on sentence composition.It comprises 9,980 eight-way multiple-choice questions about grade school science, divided into 8,134 for training, 926 for development, and 920 for testing .(Inour evaluation, we use development part.)The dataset is notable for its emphasis on multi-hop reasoning, requiring the retrieval and composition of facts from a broad corpus to answer each question.</p>
<p> NummerSense (Lin et al., 2020): NumerSense is a unique numerical commonsense reasoning probing task, featuring a diagnostic dataset with 3,145 masked-word-prediction probes.This dataset has applications in tasks such as knowledge base completion and open-domain question answering.</p>
<p> VQAv2 (Goyal et al., 2017): Visual Question Answering (VQA) v2.0 is a dataset containing open-ended questions about images.These questions require an understanding of vision, language and commonsense knowledge to answer.It is the second version of the VQA dataset.</p>
<p> NoCaps (Agrawal et al., 2019): NoCaps is a benchmark dataset for image captioning models that can describe images containing novel objects from object detection datasets.It consists of 166,100 human-generated captions describing 15,100 images from the Open Images validation and test sets.</p>
<p> MMMU (Yue et al., 2023): MMMU is a comprehensive benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning.MMMU includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks.These questions span 30 subjects and 183 subfields, comprising 32 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures.</p>
<p> MathVista (Lu et al., 2023): MathVista is a comprehensive benchmark for mathematical reasoning in visual contexts.It includes three new datasets: IQTest, FunctionQA, and PaperQA.These datasets cover various visual domains and are designed to evaluate logical reasoning on puzzle test figures, algebraic reasoning using functional plots, and scientific reasoning with academic paper figures, respectively.</p>
<p> AI2D (Kembhavi et al., 2016): AI2D is a dataset of illustrative diagrams for research on diagram understanding and associated question answering.It contains 5000 gradeschool science diagrams with over 150,000 rich annotations, their ground truth syntactic parses, and more than 15,000 corresponding multiple-choice questions.</p>
<p> ChartQA (Masry et al., 2022): ChartQA is a large-scale dataset of complex reasoning questions over charts that involve visual and logical operations, covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries.</p>
<p> ScienceQA (Lu et al., 2022): ScienceQA is a large-scale dataset for multimodal reasoning with diverse science topics and annotations of answers, lectures and explanations.It covers 26 topics, 127 categories and 379 skills from natural science, language science, and social science and so on.</p>
<p>B.3 Evaluation protocols</p>
<p>DyVal (Zhu et al., 2023a) is an approach for dynamic evaluation of LLMs by creating complexity-tailored evaluation samples on-the-fly, as opposed to relying on static benchmarks.DyVal synthesized seven distinct reasoning tasks, including: (1) Mathematics, focusing on arithmetic calculations and linear equation solving;</p>
<p>(2) Logical Reasoning, involving boolean, deductive, and abductive logic; and (3) Algorithmic Analysis, covering reachability and the maximum sum path problem.MSTemp (Liu et al., 2023b) stands for the semantic evalaution protocol which generate out-of-distribution samples by relying on evlauator LLMs and word replacement.</p>
<p>B.4 Prompts</p>
<p>B.4.1 Prompts</p>
<p>Our study examines four prompt categories, differentiated by their intended function and the required number of labeled samples.Task-oriented prompts are designed to clearly define the model's task, prompting it to generate outputs relevant to the task using its inherent pre-training knowledge.In contrast, role-oriented prompts position the model as a particular entity, like an expert, advisor, or translator, thereby subtly guiding the expected output format and behavior through the assumed role.Both categories can be adapted for zero-shot and few-shot learning contexts.We randomly choose three training set examples for each task to form the few shot examples.Examples of various prompt types are illustrated in Table 2.</p>
<p>B.4.2 Prompt Engineering</p>
<p>Prompt engineering is a process of structuring and optimizing prompts to efficiently use AI models.Methods in prompt engineering , such as chain-of-thought (Wei et al., 2023), generated knowledge prompting (Liu et al., 2022) and so on, help improve reasoning ability and task performance of AI models.We implement 6 prominent prompt engineering methods:</p>
<p> Chain-of-Thought (Wei et al., 2023): This method involves breaking down complex, multi-step problems into smaller, intermediate steps, enabling Models to tackle more intricate reasoning tasks.Chain-of-Thought differs from standard few-shot prompting by not just providing questions and answers but prompting the model to produce intermediate reasoning steps before arriving at the final answer.</p>
<p> Zero-Shot Chain-of-Thought (Kojima et al., 2022): Zero-Shot Chain of Thought improves Chain of Thought by simplifying the prompting process.The key innovation in Zero shot</p>
<p>Task oriented</p>
<p>Determine if the given pair of statements can be considered the same by responding with 'equivalent' or 'not equivalent'.</p>
<p>Role oriented</p>
<p>As an instrument for question comparison evaluation, consider the questions and determine if their meaning is the same, responding with 'equivalent' for similar questions or 'not equivalent' for different questions.</p>
<p>Few shot</p>
<p>Task oriented</p>
<p>Review the sentence below and identify whether its grammar is 'Acceptable' or 'Unacceptable': Here are three examples.Sentence: Our friends won't buy this analysis, let alone the next one we propose.Answer: acceptable.Sentence: One more pseudo generalization and I'm giving up.Answer: acceptable.Sentence: They drank the pub.Answer: unacceptable.</p>
<p>Role oriented</p>
<p>Functioning as a grammar evaluation tool, analyze the given sentence and decide if it is grammatically correct, responding with 'acceptable' or 'unacceptable': Here are three examples.Sentence: Our friends won't buy this analysis, let alone the next one we propose.Answer: acceptable.Sentence: One more pseudo generalization and I'm giving up.Answer: acceptable.Sentence: They drank the pub.Answer: unacceptable.</p>
<p>Zero-Shot Chain-of-Thought is appending the phrase "Let's think step by step" to the end of a question.</p>
<p> EmotionPrompt (Li et al., 2023a): Drawing inspiration from psychology and social science theories about human emotional intelligence, this method adds emotional stimuli to origin prompts.For example: "This is very important to my career."</p>
<p> Expert Prompting (Xu et al., 2023): The key idea is to let model be an expert in role playing.To generate the expert identity, we first provide several instruction-expert pair exemplars, then the model generates an expert identity of this question.Finally, we ask the model to answer the instruction conditioned on expert identity.</p>
<p> Generated Knowledge (Liu et al., 2022): Generated Knowledge Prompting is a method where a model first generates knowledge and then uses this generated information as additional input to answer questions.It enhances commonsense reasoning in AI without requiring task-specific knowledge integration or a structured knowledge base.</p>
<p> Least to Most (Zhou et al., 2023a): Least to Most breaks down a complex problem into a series of simpler subproblems and then solves them in sequence.The key idea is to solve each subproblem by using the answers to previously solved subproblems.This method is particularly useful for tasks that require solving problems harder than the exemplars shown in the prompts.</p>
<p>Note that there are plenty of prompt engineering techniques and we tried our best to include those general techniques instead of specific prompt engineering techniques such as Tree of Thoughts (Yao et al., 2023) that requires specific prompt design and decomposition of each problem.</p>
<p>B.4.3 Adversarial Prompt Attacks</p>
<p>Adversarial prompt attacks, as proposed by Zhu et al. (2023b), aims to simulate potential disturbances that could naturally arise in practical scenarios.The proposed prompt attacks are intended to resemble common user errors or expressions, as users often make various mistakes when inputting prompts, such as typos, diverse word choices, and different sentence constructions.The prompt attacks encompass four distinct levels:</p>
<p> Character-level: Techniques such as TextBugger (Li et al., 2019) and DeepWord-Bug (Gao et al., 2018) are employed.These methods introduce errors or typos into words by altering characters.</p>
<p> Word-level: Attacks like BertAttack (Li et al., 2020) and TextFooler (Jin et al., 2019) are utilized.They focus on substituting words with their synonyms or contextually similar alternatives.</p>
<p> Sentence-level: StressTest (Naik et al., 2018) and CheckList (Ribeiro et al., 2020) are applied.These attacks add irrelevant or redundant sentences to prompts.</p>
<p> Semantic-level: To simulate the linguistic styles of different global regions.</p>
<p>B.5 Pipeline</p>
<p>The full pipeline of using PromptBench for evaluation is shown in Figure 2.</p>
<p>Step 1. Load dataset</p>
<p>Step 2. Specify model</p>
<p>Step 3. Define prompts</p>
<p>Appendix C. Benchmark Results</p>
<p>C.1 Adversarial prompt robustness</p>
<p>The partial results of the robustness of different models on a range of tasks are presented in Figure 3.All models exhibit vulnerability to adversarial prompts, with ChatGPT and GPT-4 demonstrating the strongest robustness.implement two functions: init and query.For unified management, two points need be noticed: 1. all methods should inherits from Base class that has common code for prompt engineering methods.2. prompts used in methods should be stored in prompts/method oriented.py.</p>
<ol>
<li>Adding an Interface: After implementing a new methods, register it in the METHOD MAP that is used to map method names to their corresponding class.</li>
</ol>
<p>D.4 Add new metrics and input/output process functions</p>
<p>New evaluation metrics should be implemented as static functions in class Eval within the metrics module.Similarly, new input/output process functions should be implemented as static functions in class InputProcess and class OutputProcess in the utils module.</p>
<p>Figure 1 :
1
Figure 1: The components and supported research areas of PromptBench.</p>
<p>Step 4 .
4
Figure 2: A pipeline for evaluation of LLMs.</p>
<p>Figure 3 :
3
Figure 3: Adversarial prompt robustness results.</p>
<p>Figure 4 :Figure 5 :
45
Figure 4: Comparison among different prompt engineering techniques.</p>
<p></p>
<p>Mixtral8x7B (Mixtral, 2023): Engineering by Mistral AI team, this model is a highquality sparse mixture of experts model (SMoE) with open weights.There's also an instruction-following version, Mixtral 8x7B Instruct.</p>
<p>(Yang et al., 2023)Yang et al., 2023): Baichuan2 is developed by Baichuan Intelligent.Trained on 2.6 trillion high-quality tokens, it achieves the best results in its size class on multiple authoritative benchmarks in Chinese, English, and multilingual general and domain-specific tasks.</p>
<p>Table 2 :
2
Examples of 4 types of prompts.</p>
<p>Appendix A. Comparison with Related Code LibrariesAppendix B. Details of PromptBenchB.1 ModelsIn this section, we list the LLMs and VLMS implemented in PromptBench.Open-source LLMs: Flan-T5-large (Chung et al., 2022): Google's Flan-T5-large, a variation of the Textto-Text Transfer Transformer (T5). Dolly-6B(Databricks, 2023): The Dolly-6B model, developed by Databricks, is a 6-billion parameter causal language model.It is an extension of EleutherAI's GPT-J (Wang and Komatsuzaki, 2021), further refined with Stanford's Alpaca(Taori et al., 2023)dataset comprising 52K question/answer pairs. Vicuna series(Chiang et al., 2023): Developed from the LLaMA-13B base model, Vicuna-13B integrates over 70K user-shared conversations from ShareGPT.com,leveraging public APIs for data acquisition. Cerebras series(Dey et al., 2023): Modeled on the GPT-3 architecture, Cerebras-13B is part of the Cerebras-GPT series, trained according to Chinchilla scaling laws(Hoffmann et al., 2022)to optimize computational efficiency. Llama2 series(Touvron et al., 2023a): Engineered by Meta AI's FAIR team, the Llama2 model is an autoregressive language model adopting the transformer architecture.C.2 Prompt engineeringPrompt engineering results are shown in Figure4. Most methods are effective for special fields, so these methods can not surpass the baseline in every dataset.C.3 Dynamic evaluationFigure5illustrates the outcomes of dynamic evaluations across various models and tasks.GPT-4 outperforms its counterparts significantly, yet there remains potential for enhancement in the performance of linear equation, abductive logic, and max sum path task.Appendix D. ExtensibilityEach module in PromptBench can be easily extended.In the following, we provide basic guidelines for customizing your own datasets, models, prompt engineering methods, and evaluation metrics.The sample code for adding new datasets, models can be found in https://github.com/microsoft/promptbench/blob/main/examples/add_new_modules.mdD.1 Add new datasetsAdding new datasets involves two steps:1. Implementing a New Dataset Class: Datasets are supposed to be implemented in dataload/dataset.pyand inherit from the Dataset class.For your custom dataset, implement the init method to load your dataset.We recommend organizing your data samples as dictionaries to facilitate the input process.2. Adding an Interface: After customizing the dataset class, register it in the DataLoader class within dataload.py.D.2 Add new modelsSimilar to adding new datasets, the addition of new models also consists of two steps.
Nocaps: Novel object captioning at scale. Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, Peter Anderson, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2019</p>
<p>. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.104032023Palm 2 technical report. arXiv preprint</p>
<p>BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou, arXiv:2308.12966Qwen-vl: A frontier large vision-language model with versatile abilities. 2023. 2023arXiv preprint</p>
<p>BerriAI. 2023</p>
<p>Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle Mcdonell, Jason Phang, Michael Pieler, Shivanshu Usvsn Sai Prashanth, Laria Purohit, Jonathan Reynolds, Ben Tow, Samuel Wang, Weinbach, Gpt-neox-20b: An open-source autoregressive language model. 2022</p>
<p>A new open source flan 20b with ul2. Google Brain, 2023</p>
<p>Overview of the IWSLT 2017 evaluation campaign. Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian Stker, Katsuhito Sudoh, Koichiro Yoshino, Christian Federmann, Proceedings of the 14th International Conference on Spoken Language Translation. the 14th International Conference on Spoken Language TranslationTokyo, JapanDecember 14-15 2017International Workshop on Spoken Language Translation</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, arXiv:2307.031092023arXiv preprint</p>
<p>. Harrison Chase, Langchain, </p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing, March 2023</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Sharan Chowdhery, Gaurav Narang, Adams Mishra, Vincent Yu, Yanping Zhao, Andrew Huang, Hongkun Dai, Slav Yu, Ed H Petrov, Jeff Chi, Jacob Dean, Adam Devlin, Denny Roberts, Quoc V Zhou, Jason Le, Wei, 2022</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Contributors. promptfoo: test your llm app. 2023a</p>
<p>Opencompass: A universal evaluation platform for foundation models. Openai evals. 2023b. 2023c</p>
<p>Hello dolly: Democratizing the magic of chatgpt with open models. Databricks, 2023</p>
<p>Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster. Nolan Dey, Gurpreet Gosal, Zhiming, Hemant Chen, William Khachane, Ribhu Marshall, Marvin Pathria, Joel Tom, Hestness, 2023</p>
<p>Black-box prompt learning for pre-trained language models. Shizhe Diao, Zhichao Huang, Ruijia Xu, Xuechun Li, Yong Lin, Xiao Zhou, Tong Zhang, arXiv:2201.085312022arXiv preprint</p>
<p>Automatically constructing a corpus of sentential paraphrases. William B Dolan, Chris Brockett, Proceedings of the Third International Workshop on Paraphrasing (IWP2005). the Third International Workshop on Paraphrasing (IWP2005)2005</p>
<p>Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, Jiaqi Wang, arXiv:2401.16420Internlm-xcomposer2: Mastering free-form textimage composition and comprehension in vision-language large model. 2024arXiv preprint</p>
<p>MultiUN: A multilingual corpus from united nation documents. Andreas Eisele, Yu Chen, Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC'10). the Seventh International Conference on Language Resources and Evaluation (LREC'10)Valletta, MaltaEuropean Language Resources Association (ELRAMay 2010</p>
<p>A test of artificial intelligence. Michael Eisenstein, Nature Outlook: Robotics and artificial intelligence. 2023</p>
<p>Black-box generation of adversarial text sequences to evade deep learning classifiers. J Gao, J Lanchantin, M L Soffa, Y Qi, 10.1109/SPW.2018.000162018 IEEE Security and Privacy Workshops (SPW). May 2018</p>
<p>A framework for few-shot language model evaluation. Leo Gao, Jonathan Tow, Stella Baber Abbasi, Sid Biderman, Anthony Black, Charles Dipofi, Laurence Foster, Jeffrey Golding, Alain Hsu, Haonan Le Noac'h, Kyle Li, Niklas Mcdonell, Chris Muennighoff, Jason Ociepa, Laria Phang, Hailey Reynolds, Aviya Schoelkopf, Lintang Skowron, Eric Sutawika, Anish Tang, Ben Thite, Kevin Wang, Andy Wang, Zou, 12 2023</p>
<p>Google. 2023</p>
<p>Making the v in vqa matter: Elevating the role of image understanding in visual question answering. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, International Conference on Learning Representations. 2021</p>
<p>Large language models are reasoning teachers. Namgyu Ho, Laura Schmid, Se-Young Yun, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2023</p>
<p>C-eval: A multilevel multi-discipline chinese evaluation suite for foundation models. Jordan Hoffmann, arXiv:2305.083222022. 2023arXiv preprintTraining compute-optimal large language models</p>
<p>Open-source large language models leaderboard. Huggingface, 2023</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Mistral 7b. Renard Llio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothe Wang, William El Lacroix, Sayed, 2023</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Diego de las Casas. Florian Zhu, Zhao, Chen, Wang, XieEmma Bou Hanna</p>
<p>. Gianna Bressand, Guillaume Lengyel, Guillaume Bour, Lample, Renard Llio, Lucile Lavaud, Marie-Anne Saulnier, Pierre Lachaux, Sandeep Stock, Sophia Subramanian, Yang, Szymon Antoniak, Teven Le Scao, Thophile Gervet, Thibaut Lavril, Thomas Wang, Timothe Lacroix, and William El Sayed2024Mixtral of experts</p>
<p>Di Jin, Zhijing Jin, Joey Tianyi Zhou, Peter Szolovits, arXiv:1907.11932Is bert really robust? natural language attack on text classification and entailment. 2019arXiv preprint</p>
<p>A diagram is worth a dozen images. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Computer Vision-ECCV 2016: 14th European Conference. Amsterdam, The NetherlandsSpringerOctober 11-14, 2016. 2016Proceedings, Part IV 14</p>
<p>Qasc: A dataset for question answering via sentence composition. Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, Ashish Sabharwal, 2020</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022</p>
<p>The winograd schema challenge. Hector Levesque, Ernest Davis, Leora Morgenstern, Thirteenth international conference on the principles of knowledge representation and reasoning. 2012</p>
<p>Large language models understand and can be enhanced by emotional stimuli. Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu, Wenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang, Xing Xie, 2023a</p>
<p>TextBugger: Generating adversarial text against real-world applications. Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, Ting Wang, 10.14722/ndss.2019.23138Proceedings 2019 Network and Distributed System Security Symposium. 2019 Network and Distributed System Security SymposiumInternet Society2019</p>
<p>Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, International conference on machine learning. PMLR2023b</p>
<p>BERT-ATTACK: Adversarial attack against BERT using BERT. Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, Xipeng Qiu, 10.18653/v1/2020.emnlp-main.500Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsNovember 2020Online</p>
<p>Alpacaeval: An automatic evaluator of instruction-following models. Github repository. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023c</p>
<p>Yuanzhi Li, Sbastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, Yin Tat, Lee , arXiv:2309.05463Textbooks are all you need ii: phi-1.5 technical report. 2023darXiv preprint</p>
<p>Holistic evaluation of language models. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, arXiv:2211.091102022arXiv preprint</p>
<p>Birds have four legs?! numersense: Probing numerical commonsense knowledge of pre-trained language models. Seyeon Bill Yuchen Lin, Rahul Lee, Xiang Khanna, Ren, 2020</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Visual instruction tuning. 2023a</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in neural information processing systems. 202436</p>
<p>. Jerry Liu, Llamaindex, 11 2022</p>
<p>Generated knowledge prompting for commonsense reasoning. Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Le Ronan, Yejin Bras, Hannaneh Choi, Hajishirzi, 2022</p>
<p>Meta semantic template for evaluation of large language models. Yachuan Liu, Liang Chen, Jindong Wang, Qiaozhu Mei, Xing Xie, arXiv:2310.014482023barXiv preprint</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, The 36th Conference on Neural Information Processing Systems (NeurIPS). 2022</p>
<p>Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao, arXiv:2310.02255Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. 2023arXiv preprint</p>
<p>ChartQA: A benchmark for question answering about charts with visual and logical reasoning. Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, Enamul Hoque, 10.18653/v1/2022.findings-acl.177Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational LinguisticsMay 2022</p>
<p>. Daniel Mcduff, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal, Yash Sharma, Shekoofeh Azizi, Kavita Kulkarni, Le Hou, Yong Cheng, Yun Liu, Sara Mahdavi, Sushant Prakash, Anupam Pathak, Christopher Semturs, Shwetak Patel, Dale R Webster, Ewa Dominowska, Juraj Gottweis, Joelle Barral, Katherine Chou, Greg S Corrado, Yossi Matias, 2023Alan Karthikesalingam, and Vivek NatarajanJake SunshineTowards accurate differential diagnosis with large language models</p>
<p>Semantic kernel. Mixtral. Mixtral, 2023. Microsoft2023</p>
<p>Carolyn Rose, and Graham Neubig. Stress test evaluation for natural language inference. Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, ACL. Santa Fe, New Mexico, USAAssociation for Computational LinguisticsAugust 2018</p>
<p>OpenAI. Gpt-4 technical report. 2023a. 2023bOpenAI</p>
<p>Plum: Prompt learning using metaheuristic. Rui Pan, Shuo Xing, Shizhe Diao, Xiang Liu, Kashun Shum, Jipeng Zhang, Tong Zhang, arXiv:2311.083642023arXiv preprint</p>
<p>Grips: Gradient-free, edit-based instruction search for prompting large language models. Archiki Prasad, Peter Hase, Xiang Zhou, Mohit Bansal, arXiv:2203.072812022arXiv preprint</p>
<p>Know what you don't know: Unanswerable questions for SQuAD. Pranav Rajpurkar, Robin Jia, Percy Liang, 10.18653/v1/P18-2124ACL. Melbourne, AustraliaAssociation for Computational LinguisticsJuly 2018</p>
<p>Beyond accuracy: Behavioral testing of NLP models with CheckList. Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, Sameer Singh, 10.18653/v1/2020.acl-main.442ACL. Association for Computational LinguisticsJuly 2020</p>
<p>Analysing mathematical reasoning abilities of neural models. David Saxton, Edward Grefenstette, Felix Hill, Pushmeet Kohli, In ICLR. 2019</p>
<p>Moral mimicry: Large language models produce moral rationalizations tailored to political identity. Gabriel Simmons, arXiv:2209.121062022arXiv preprint</p>
<p>Recursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, Christopher Potts, EMNLP. Seattle, Washington, USAAssociation for Computational Linguistics2013</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 2019</p>
<p>Stanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Large language models approach expert-level clinical knowledge and reasoning in ophthalmology: A head-to-head cross-sectional study. Aj Thirunavukarasu, Mahmood, Malem, Foster, Sanghera, Hassan, 10.1371/journal.pdig.0000341PLOS Digital Health. 342024</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288arXiv:2307.09288Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, 2023a. 2023b2arXiv preprint</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, the Proceedings of ICLR. 2019</p>
<p>Ben Wang, Aran Komatsuzaki, Gpt-J-6b, A 6 Billion Parameter Autoregressive Language Model. May 2021</p>
<p>Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, arXiv:2306.11698A comprehensive assessment of trustworthiness in gpt models. 2023aarXiv preprint</p>
<p>On the robustness of chatgpt: An adversarial and out-of-distribution perspective. Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, International conference on learning representations (ICLR) workshop on Trustworthy and Reliable Large-Scale Machine Learning Models. 2023b</p>
<p>Bilateral multi-perspective matching for natural language sentences. Zhiguo Wang, Wael Hamza, Radu Florian, 2017</p>
<p>Alex Warstadt, Amanpreet Singh, Samuel R Bowman, arXiv:1805.12471Neural network acceptability judgments. 2018arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, Samuel Bowman, NAACL HLT. Association for Computational Linguistics2018</p>
<p>Causal parrots: Large language models may talk causality but are not causal. Moritz Willig, Matej Zecevic, Devendra Singh Dhami, Kristian Kersting, Transactions on machine learning research. 82023</p>
<p>Expertprompting: Instructing large language models to be distinguished experts. Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, Zhendong Mao, 2023</p>
<p>. Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chenxu Chao Yin, Lv, Dian Da Pan, Dong Wang, Fan Yan, Fei Yang, Feng Deng, Feng Wang, Guangwei Liu, Guosheng Ai, Haizhou Dong, Hang Zhao, Haoze Xu, Hongda Sun, Hui Zhang, Jiaming Liu, Jian Ji, Juntao Xie, Kun Dai, Lei Fang, Liang Su, Lifeng Song, Liyun Liu, Luyao Ru, Mang Ma, Mickel Wang, Mingan Liu, Nuolan Lin, Peidong Nie, Ruiyang Guo, Tao Sun, Tianpeng Zhang, Tianyu Li, Wei Li, Weipeng Cheng, Xiangrong Chen, Xiaochuan Zeng, Xiaoxi Wang, Xin Chen, Xin Men, Xuehai Yu, Yanjun Pan, Yiding Shen, Yiyu Wang, Youxin Li, Yuchen Jiang, Yupeng Gao, Zenan Zhang, Zhou, 2023and Zhiying Wu. Baichuan 2: Open large-scale language models</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>. Yi, Yi, 2023</p>
<p>Mmmu: A massive multidiscipline multimodal understanding and reasoning benchmark for expert agi. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, arXiv:2311.165022023arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, arXiv:2306.056852023arXiv preprint</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schrli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, Ed Chi, 2023a</p>
<p>Don't make your llm an evaluation benchmark cheater. Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, Jiawei Han, arXiv:2311.019642023barXiv preprint</p>
<p>Dyval: Graph-informed dynamic evaluation of large language models. Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, Xing Xie, arXiv:2309.171672023aarXiv preprint</p>
<p>Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, arXiv:2306.04528Towards evaluating the robustness of large language models on adversarial prompts. 2023barXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9000 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9000</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9000</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-276617925</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.07533v2.pdf" target="_blank">Large Language Models as Neurolinguistic Subjects: Discrepancy between Performance and Competence</a></p>
                <p><strong>Paper Abstract:</strong> This study investigates the linguistic understanding of Large Language Models (LLMs) regarding signifier (form) and signified (meaning) by distinguishing two LLM assessment paradigms: psycholinguistic and neurolinguis-tic. Traditional psycholinguistic evaluations often reflect statistical rules that may not accurately represent LLMs’ true linguistic competence. We introduce a neurolinguistic approach, utilizing a novel method that combines minimal pairs and diagnostic probing to analyze activation patterns across model layers. This method allows for a detailed examination of how LLMs represent form and meaning, and whether these representations are consistent across languages. We found: (1) Psycholinguistic and neurolin-guistic methods reveal that language performance and competence are distinct; (2) Direct probability measurement may not accurately assess linguistic competence; (3) Instruction tuning won’t change much competence but improve performance; (4) LLMs exhibit higher competence and performance in form compared to meaning. Additionally, we introduce new conceptual minimal pair datasets for Chinese (COMPS-ZH) and German (COMPS-DE), complementing existing English datasets. 1</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9000.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9000.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7-billion-parameter open foundation LLM (Llama2) used in the paper in both base and instruction-tuned (chat) variants; English-centric with some multilingual exposure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open foundation Llama2 model (English-centric, trained on large publicly available corpora with limited multilingual data; paper notes Llama family pretraining scale and multilingual fraction), evaluated in both base and instruction-tuned (chat) variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Minimal-pair linguistic batteries: BLiMP (English), CLiMP (Chinese), DistilLingEval (German) for form; COMPS (English), COMPS-ZH, COMPS-DE for conceptual knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Form tests: grammatical minimal-pair tasks probing syntax, morphology, and syntax-semantics interface (BLiMP, CLiMP, DistilLingEval). Meaning tests: conceptual minimal-pair tasks (COMPS family) probing everyday concept/property knowledge (taxonomy, property norms, co-occurrence, random). Evaluation paradigms: Psycholinguistic — Meta (metalinguistic prompting) and Direct (probability comparison); Neurolinguistic — Neuro (minimal-pair diagnostic probing of internal representations, layer-wise).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Pattern-level results reported: Neuro (probing) > Direct (probability) > Meta (prompting). Example: Neuro performance on Syntax tasks reaches ~97% (paper example averaged across models); Meta syntax reported ~56.1%. More generally, form-related tasks show substantially higher Neuro probing scores than meaning/concept tasks; exact per-task numeric tables are in paper figures.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No direct human baselines reported in this paper; the authors report that Llama2-7B (and other LLMs) show much higher competence when measured via neurolinguistic probing than when assessed by output-based psycholinguistic methods, and that form competence > meaning competence. Thus relative to human performance the paper does not provide numeric comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Three evaluation paradigms were applied: (1) Meta — metalinguistic prompting (multiple-choice style prompts); (2) Direct — direct probability measurement using model logits comparing probabilities of acceptable vs unacceptable sentences in minimal pairs; (3) Neuro — minimal-pair probing: extract last-token embedding from each layer, train logistic-regression probe (5-fold CV, F1 metric) to classify which member of a minimal pair is acceptable; correct probing score by a random-embedding baseline per Formula (1) in paper. Sentence embeddings used last-token pooling; feature learning saturation layer defined as first layer reaching 95% of peak; t-SNE/PCA used for unsupervised visualization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No human baseline performance is reported for the same tests. Experiments use relatively small-scale models (7B/8B) so results may not generalize to larger LLMs. German results were poor in part due to very long sentences in DistilLingEval. COMPS conceptual dataset limited to four conceptual relationship types. The paper emphasizes that Direct probability measurement (and prompting) can underestimate internal competence because final-layer outputs are optimized for next-token prediction; probing reveals additional competence in intermediate layers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Neurolinguistic Subjects: Discrepancy between Performance and Competence', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9000.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9000.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3 (8B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8-billion-parameter successor to Llama2 evaluated in base and chat forms; reported to show modest improvements in conceptual learning speed and encoding compared to Llama2 while grammatical capabilities change little.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama3 is an iteration of the Llama family (English-centric). In this study Llama3-8B was compared to Llama2-7B to assess layer-wise encoding of form and meaning; both base and instruction-tuned variants were evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Same minimal-pair batteries as above: BLiMP (English) for form, COMPS for conceptual; COMPS-ZH/COMPS-DE for multilingual conceptual checks</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Form (syntax/morphology/syntax-semantics interface) and meaning (conceptual COMPS) minimal-pair tasks using the three paradigms: Meta, Direct, Neuro (probing).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Overall pattern Neuro > Direct > Meta holds for Llama3 as well. Supplementary analyses report that Llama3 improves conceptual (meaning) encoding and requires fewer layers to encode conceptual features than Llama2 (faster/earlier saturation for meaning), while grammatical (form) capabilities remain similar between Llama2 and Llama3. Exact numeric values are shown in paper figures (layer-wise curves); no full human baselines provided.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>The paper reports Llama3 shows better conceptual encoding (faster and somewhat higher in early layers) relative to Llama2, but both show the same broad pattern of stronger form than meaning competence. No direct numeric human comparison is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Layer-wise neuro probing with last-token embeddings; logistic regression probes with 5-fold CV (F1). Comparative statistics: layer-wise t-tests per task and aggregation using Stouffer's Z; saturation layer comparisons reported with significance markers. Instruction tuning (chat) has little effect on neuro probing scores but improves Meta (prompted) performance for form tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Same limitations as for Llama2: small-scale models, absent human baselines, COMPS limited in conceptual granularity; observed improvements mostly in early-layer conceptual encoding rather than large gains in final output competence. Instruction tuning affects external performance more than internal competence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Neurolinguistic Subjects: Discrepancy between Performance and Competence', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9000.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9000.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7-billion-parameter multilingual LLM (Qwen-7B) evaluated for English and Chinese; trained on large multilingual corpora with emphasis on English and Chinese.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Qwen is a multilingual LLM developed by Alibaba, trained on 2–3 trillion tokens of multilingual pretraining data with focus on English and Chinese; evaluated in the paper for form and meaning across English/Chinese/German minimal-pair datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>BLiMP/CLiMP/DistilLingEval for form; COMPS/COMPS-ZH/COMPS-DE for meaning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same minimal-pair test families and evaluation paradigms (Meta, Direct, Neuro probing). Multilingual analysis explores whether meaning representations persist across different surface forms (languages).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Pattern Neuro > Direct > Meta also holds for Qwen. The paper reports Qwen learns Chinese syntax relatively quickly (steeper early rise in form curve) but encodes Chinese semantics (conceptual meaning) more slowly, leading to a larger early gap between form and meaning curves. No absolute human baseline numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Qwen shows better/faster encoding of Chinese form relative to other models but slower semantic encoding; across models and languages, form competence is higher than meaning competence. No direct human comparison presented.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Same experimental pipeline: direct probability measurement, metalinguistic prompting, and neurolinguistic minimal-pair probing using last-token pooling and logistic regression probes (F1 averaged across 5 folds). Multilingual COMPS datasets were produced via machine translation plus human post-editing; dataset sizes and correction counts are reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Paper notes language-resource differences: German results poor (likely smaller training data) and Chinese meaning competence low for Llama models despite adequate form competence; Qwen's relative strengths reflect training data composition. Limitations include small-scale models, dataset granularity (COMPS limited to 4 conceptual relation types), and lack of human baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Neurolinguistic Subjects: Discrepancy between Performance and Competence', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Decoding probing: Revealing internal linguistic structures in neural language models using minimal pairs <em>(Rating: 2)</em></li>
                <li>COMPS: Conceptual minimal pair sentences for testing robust property knowledge and its inheritance in pre-trained language models <em>(Rating: 2)</em></li>
                <li>BLiMP: The benchmark of linguistic minimal pairs for English <em>(Rating: 2)</em></li>
                <li>Prompting is not a substitute for probability measurements in large language models <em>(Rating: 2)</em></li>
                <li>Neural language models as psycholinguistic subjects: Representations of syntactic state <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9000",
    "paper_id": "paper-276617925",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "Llama2-7B",
            "name_full": "Llama 2 (7B)",
            "brief_description": "A 7-billion-parameter open foundation LLM (Llama2) used in the paper in both base and instruction-tuned (chat) variants; English-centric with some multilingual exposure.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama2-7B",
            "model_description": "Open foundation Llama2 model (English-centric, trained on large publicly available corpora with limited multilingual data; paper notes Llama family pretraining scale and multilingual fraction), evaluated in both base and instruction-tuned (chat) variants.",
            "model_size": "7B",
            "test_battery_name": "Minimal-pair linguistic batteries: BLiMP (English), CLiMP (Chinese), DistilLingEval (German) for form; COMPS (English), COMPS-ZH, COMPS-DE for conceptual knowledge",
            "test_description": "Form tests: grammatical minimal-pair tasks probing syntax, morphology, and syntax-semantics interface (BLiMP, CLiMP, DistilLingEval). Meaning tests: conceptual minimal-pair tasks (COMPS family) probing everyday concept/property knowledge (taxonomy, property norms, co-occurrence, random). Evaluation paradigms: Psycholinguistic — Meta (metalinguistic prompting) and Direct (probability comparison); Neurolinguistic — Neuro (minimal-pair diagnostic probing of internal representations, layer-wise).",
            "llm_performance": "Pattern-level results reported: Neuro (probing) &gt; Direct (probability) &gt; Meta (prompting). Example: Neuro performance on Syntax tasks reaches ~97% (paper example averaged across models); Meta syntax reported ~56.1%. More generally, form-related tasks show substantially higher Neuro probing scores than meaning/concept tasks; exact per-task numeric tables are in paper figures.",
            "human_baseline_performance": null,
            "performance_comparison": "No direct human baselines reported in this paper; the authors report that Llama2-7B (and other LLMs) show much higher competence when measured via neurolinguistic probing than when assessed by output-based psycholinguistic methods, and that form competence &gt; meaning competence. Thus relative to human performance the paper does not provide numeric comparisons.",
            "experimental_details": "Three evaluation paradigms were applied: (1) Meta — metalinguistic prompting (multiple-choice style prompts); (2) Direct — direct probability measurement using model logits comparing probabilities of acceptable vs unacceptable sentences in minimal pairs; (3) Neuro — minimal-pair probing: extract last-token embedding from each layer, train logistic-regression probe (5-fold CV, F1 metric) to classify which member of a minimal pair is acceptable; correct probing score by a random-embedding baseline per Formula (1) in paper. Sentence embeddings used last-token pooling; feature learning saturation layer defined as first layer reaching 95% of peak; t-SNE/PCA used for unsupervised visualization.",
            "limitations_or_caveats": "No human baseline performance is reported for the same tests. Experiments use relatively small-scale models (7B/8B) so results may not generalize to larger LLMs. German results were poor in part due to very long sentences in DistilLingEval. COMPS conceptual dataset limited to four conceptual relationship types. The paper emphasizes that Direct probability measurement (and prompting) can underestimate internal competence because final-layer outputs are optimized for next-token prediction; probing reveals additional competence in intermediate layers.",
            "uuid": "e9000.0",
            "source_info": {
                "paper_title": "Large Language Models as Neurolinguistic Subjects: Discrepancy between Performance and Competence",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Llama3-8B",
            "name_full": "Llama 3 (8B)",
            "brief_description": "An 8-billion-parameter successor to Llama2 evaluated in base and chat forms; reported to show modest improvements in conceptual learning speed and encoding compared to Llama2 while grammatical capabilities change little.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama3-8B",
            "model_description": "Llama3 is an iteration of the Llama family (English-centric). In this study Llama3-8B was compared to Llama2-7B to assess layer-wise encoding of form and meaning; both base and instruction-tuned variants were evaluated.",
            "model_size": "8B",
            "test_battery_name": "Same minimal-pair batteries as above: BLiMP (English) for form, COMPS for conceptual; COMPS-ZH/COMPS-DE for multilingual conceptual checks",
            "test_description": "Form (syntax/morphology/syntax-semantics interface) and meaning (conceptual COMPS) minimal-pair tasks using the three paradigms: Meta, Direct, Neuro (probing).",
            "llm_performance": "Overall pattern Neuro &gt; Direct &gt; Meta holds for Llama3 as well. Supplementary analyses report that Llama3 improves conceptual (meaning) encoding and requires fewer layers to encode conceptual features than Llama2 (faster/earlier saturation for meaning), while grammatical (form) capabilities remain similar between Llama2 and Llama3. Exact numeric values are shown in paper figures (layer-wise curves); no full human baselines provided.",
            "human_baseline_performance": null,
            "performance_comparison": "The paper reports Llama3 shows better conceptual encoding (faster and somewhat higher in early layers) relative to Llama2, but both show the same broad pattern of stronger form than meaning competence. No direct numeric human comparison is provided.",
            "experimental_details": "Layer-wise neuro probing with last-token embeddings; logistic regression probes with 5-fold CV (F1). Comparative statistics: layer-wise t-tests per task and aggregation using Stouffer's Z; saturation layer comparisons reported with significance markers. Instruction tuning (chat) has little effect on neuro probing scores but improves Meta (prompted) performance for form tasks.",
            "limitations_or_caveats": "Same limitations as for Llama2: small-scale models, absent human baselines, COMPS limited in conceptual granularity; observed improvements mostly in early-layer conceptual encoding rather than large gains in final output competence. Instruction tuning affects external performance more than internal competence.",
            "uuid": "e9000.1",
            "source_info": {
                "paper_title": "Large Language Models as Neurolinguistic Subjects: Discrepancy between Performance and Competence",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Qwen-7B",
            "name_full": "Qwen (7B)",
            "brief_description": "A 7-billion-parameter multilingual LLM (Qwen-7B) evaluated for English and Chinese; trained on large multilingual corpora with emphasis on English and Chinese.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen-7B",
            "model_description": "Qwen is a multilingual LLM developed by Alibaba, trained on 2–3 trillion tokens of multilingual pretraining data with focus on English and Chinese; evaluated in the paper for form and meaning across English/Chinese/German minimal-pair datasets.",
            "model_size": "7B",
            "test_battery_name": "BLiMP/CLiMP/DistilLingEval for form; COMPS/COMPS-ZH/COMPS-DE for meaning",
            "test_description": "Same minimal-pair test families and evaluation paradigms (Meta, Direct, Neuro probing). Multilingual analysis explores whether meaning representations persist across different surface forms (languages).",
            "llm_performance": "Pattern Neuro &gt; Direct &gt; Meta also holds for Qwen. The paper reports Qwen learns Chinese syntax relatively quickly (steeper early rise in form curve) but encodes Chinese semantics (conceptual meaning) more slowly, leading to a larger early gap between form and meaning curves. No absolute human baseline numbers provided.",
            "human_baseline_performance": null,
            "performance_comparison": "Qwen shows better/faster encoding of Chinese form relative to other models but slower semantic encoding; across models and languages, form competence is higher than meaning competence. No direct human comparison presented.",
            "experimental_details": "Same experimental pipeline: direct probability measurement, metalinguistic prompting, and neurolinguistic minimal-pair probing using last-token pooling and logistic regression probes (F1 averaged across 5 folds). Multilingual COMPS datasets were produced via machine translation plus human post-editing; dataset sizes and correction counts are reported in the paper.",
            "limitations_or_caveats": "Paper notes language-resource differences: German results poor (likely smaller training data) and Chinese meaning competence low for Llama models despite adequate form competence; Qwen's relative strengths reflect training data composition. Limitations include small-scale models, dataset granularity (COMPS limited to 4 conceptual relation types), and lack of human baseline comparisons.",
            "uuid": "e9000.2",
            "source_info": {
                "paper_title": "Large Language Models as Neurolinguistic Subjects: Discrepancy between Performance and Competence",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Decoding probing: Revealing internal linguistic structures in neural language models using minimal pairs",
            "rating": 2,
            "sanitized_title": "decoding_probing_revealing_internal_linguistic_structures_in_neural_language_models_using_minimal_pairs"
        },
        {
            "paper_title": "COMPS: Conceptual minimal pair sentences for testing robust property knowledge and its inheritance in pre-trained language models",
            "rating": 2,
            "sanitized_title": "comps_conceptual_minimal_pair_sentences_for_testing_robust_property_knowledge_and_its_inheritance_in_pretrained_language_models"
        },
        {
            "paper_title": "BLiMP: The benchmark of linguistic minimal pairs for English",
            "rating": 2,
            "sanitized_title": "blimp_the_benchmark_of_linguistic_minimal_pairs_for_english"
        },
        {
            "paper_title": "Prompting is not a substitute for probability measurements in large language models",
            "rating": 2,
            "sanitized_title": "prompting_is_not_a_substitute_for_probability_measurements_in_large_language_models"
        },
        {
            "paper_title": "Neural language models as psycholinguistic subjects: Representations of syntactic state",
            "rating": 1,
            "sanitized_title": "neural_language_models_as_psycholinguistic_subjects_representations_of_syntactic_state"
        }
    ],
    "cost": 0.00999275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models as Neurolinguistic Subjects: Discrepancy between Performance and Competence</p>
<p>Linyang He linyang.he@columbia.edu 
Columbia University</p>
<p>University of Michigan</p>
<p>Ercong Nie nie@cis.lmu.de 
Munich Center for Machine Learning
Germany</p>
<p>LMU Munich
Germany</p>
<p>Helmut Schmid schmid@cis.lmu.de 
LMU Munich
Germany</p>
<p>Hinrich Schütze hinrich@hotmail.com 
Munich Center for Machine Learning
Germany</p>
<p>LMU Munich
Germany</p>
<p>Nima Mesgarani 
Columbia University</p>
<p>Jonathan Brennan 
University of Michigan</p>
<p>Large Language Models as Neurolinguistic Subjects: Discrepancy between Performance and Competence
4471BC0D23DCD0DEFD8628F9DFA89FE9
This study investigates the linguistic understanding of Large Language Models (LLMs) regarding signifier (form) and signified (meaning) by distinguishing two LLM assessment paradigms: psycholinguistic and neurolinguistic.Traditional psycholinguistic evaluations often reflect statistical rules that may not accurately represent LLMs' true linguistic competence.We introduce a neurolinguistic approach, utilizing a novel method that combines minimal pairs and diagnostic probing to analyze activation patterns across model layers.This method allows for a detailed examination of how LLMs represent form and meaning, and whether these representations are consistent across languages.We found: (1) Psycholinguistic and neurolinguistic methods reveal that language performance and competence are distinct; (2) Direct probability measurement may not accurately assess linguistic competence; (3) Instruction tuning won't change much competence but improve performance; (4) LLMs exhibit higher competence and performance in form compared to meaning.Additionally, we introduce new conceptual minimal pair datasets for Chinese (COMPS-ZH) and German (COMPS-DE), complementing existing English datasets. 1</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have demonstrated remarkable reasoning, linguistic, arithmetic, and other cognitive abilities.The advent of LLMs has reignited cross-disciplinary discussions about what sorts of behavior are "intelligence", even if the intelligence exhibited by LLMs may differ from human intelligence (Sejnowski, 2023).LLMs have drawn the attention of researchers from various fields, including linguistics, cognitive science, computer science, and neuroscience, who investigate how LLMs develop and exhibit these capabilities.</p>
<p>There is currently a heated debate about whether LLMs understand human language or whether their 1 Code and data available here.performance is simply the product of complex statistical relationships (Mitchell and Krakauer, 2023).A central aspect of this debate concerns the nature of LLMs' linguistic representations.Using the semiotic framework of language proposed by De Saussure (1989), which distinguishes between the signifier (form) and the signified (meaning), we can inquire into the extent to which LLMs comprehend the form and meaning, and how form and meaning intertwist with each other.Is LLMs' understanding of language meaning merely a statistical outcome based on their grasp of language form?When different languages express a shared concept with distinct forms, do LLMs create similar representations for these variations?How can we better understand the representations of form and meaning in these systems that support the observed patterns of performance?</p>
<p>The underlying processes remain unclear due to the opaque nature of neural networks.Therefore, we need appropriate methods to assess their true linguistic understanding.</p>
<p>Drawing inspirations from the cognitive study on human language processing, we propose that the assessment of LLMs can be divided into two primary paradigms: psycholinguistic and neurolinguistic.As illustrated in Figure 2, the psycholin-arXiv:2411.07533v3 [cs.CL] 12 Jul 2025 Both direct probability measurement and metalinguistic prompting can be considered as psycholinguistic methods, while minimal pair probing (He et al., 2024) and other diagnostic probing are neurolinguistic.</p>
<p>guistic paradigm measures the model's output probabilities, directly reflecting the model's behavior and performance.The neurolinguistic paradigm delves into the internal representations of LLMs.</p>
<p>When treating LLMs as psycholinguistic subjects, their responses may leverage their grasp of form, relying on statistical correlations, to create an illusion of understanding meaning.This enables LLMs to produce structurally coherent but not necessarily semantically accurate responses, as their "understanding" is shaped by patterns rather than true conceptual processing (Harnad, 1990;Bender and Koller, 2020;Nie et al., 2024).Consequently, psycholinguistic evaluations tend to reflect performance rather than competence, as they assess external outputs that may not fully capture the underlying linguistic knowledge encoded within the model.This mismatch suggests that psycholinguistic evaluation results might not accurately represent the true linguistic competence of LLMs.</p>
<p>In contrast, examining LLMs as neurolinguistic subjects focuses on internal representations, providing a more direct assessment of competence by moving beyond surface-level biases (Firestone, 2020).To achieve this, we adapted the decoding probing method by He et al. (2024), referred to as "minimal pair probing", to analyze how LLMs encode form and meaning across layers.This approach allows for a finer distinction between performance and competence, revealing insights that psycholinguistic methods might overlook.</p>
<p>In order to address questions about whether LLMs maintain consistent underlying representa-tions of the same concept when the form changes across multiple languages, we also create a multilingual minimal pair dataset (COMPS-ZH for Chinese and COMPS-DE for German).</p>
<p>By evaluating LLMs in both psycholinguistic and neurolinguistic paradigms, we found: 1) Psycholinguistic and neurolinguistic results reveal very different patterns, suggesting both paradigms are necessary for a comprehensive understanding of LLMs.2) Though more intrinsic than metalinguistic prompting, direct probability measurement may still not accurately assess linguistic competence, as it remains influenced by statistical patterns.3) LLMs acquire competence in linguistic form more easily, earlier, and with greater accuracy than in meaning.4) As linguistic form varies across languages, LLMs' understanding of the same concept shifts accordingly, with meaning competence linearly correlated to form.This suggests that signifier and signified in LLMs may not be independent, and maintaining conceptual representations likely depends on statistical correlations with form.</p>
<p>Psycholinguistic vs. Neurolinguistic Paradigm</p>
<p>Cognitive Science Background</p>
<p>Psycholinguistics and neurolinguistics offer distinct yet complementary perspectives on human language processing.Psycholinguistics focuses on the psychological and cognitive processes that enable humans to understand and use language (Field, 2004;Traxler and Gernsbacher, 2011).In contrast, neurolinguistics explores the underlying neural mechanisms and brain structures involved in language processing (Friederici, 2011;Brennan, 2022;Kemmerer, 2022).Both paradigms offer a valuable model for probing the linguistic capacities and potential intelligence of LLMs.</p>
<p>In LLM Assessment Research</p>
<p>Psycholinguistic paradigm: direct probability measurement and metalinguistic prompting</p>
<p>Recent studies often use prompting to evaluate the linguistic capabilities of LLMs.These implicit tests were referred to as metalinguistic judgments by Hu and Levy (2023).However, it is important to note that the performance of LLMs in specific linguistic prompting tasks only indirectly reflects their internal linguistic representations due to the inherent limitations of such prompting tasks: an LLM chat system might give a "reasonable" response just because of the statistical relationships between prompt and reply (Hofstadter, 1995).Hu and Levy (2023) argue that it is uncertain whether the LLMs' responses to metalinguistic prompting align with the underlying internal representations.</p>
<p>Computing a model's probability of generating two minimally different sentences is one way to address these concerns (Hu and Levy, 2023).The minimal difference between the two sentences (e.g., replacement of a single word) makes one sentence acceptable while the other is not (Linzen et al., 2016).Here are two examples for testing grammatical and conceptual understanding, respectively:</p>
<p>(1) Simple agreement (Warstadt et al., 2020)</p>
<p>(unacceptable)</p>
<p>A language model is considered to perform correctly on this task if it assigns a higher probability to the acceptable sentence compared to the unacceptable one (Marvin and Linzen, 2018).Researchers have created syntactic, semantic/conceptual, and discourse inference tasks for the minimal pair method.They provide more precise insights into the abilities of LLMs compared to metalinguistic prompting (Futrell et al., 2019;Gauthier et al., 2020;Hu et al., 2020;Warstadt et al., 2020;Beyer et al., 2021;Misra et al., 2023;Kauf et al., 2023).</p>
<p>Through either metalinguistic judgement or direct probability measurement methods, these tasks essentially treat LLMs as psycholinguistic subjects (Futrell et al., 2019).This research paradigm resembles cognitive psychology by having LLMs perform tasks, such as cloze and question answering, and then evaluating their performance without examining the internal representations, in a manner similar to how subjects participate in psychological experiments.Information about the inner workings of a model is inferred either from its output or from the probabilities it assigns to different possible outputs.The internal states of the LLM (i.e. its intermediate layers) are not examined.</p>
<p>Neurolinguistic paradigm: diagnostic probing</p>
<p>Another line of research focuses on studying the internal representations, emphasizing a neurolinguistic approach to understanding LLMs.Essentially, diagnostic probing methods in evaluating language models can be considered as neurolinguistic paradigms as they examine the internal states of LMs (Belinkov and Glass, 2019;Belinkov, 2022), while the term 'neurolinguistic' hasn't been applied to the field before.Diagnostic probing involves training a classifier to predict linguistic properties from the hidden states of LMs.Following this paradigm, researchers decode syntactic, semantic, morphological, and other linguistic properties from the hidden states of LMs (Köhn, 2015;Gupta et al., 2015;Shi et al., 2016;Tenney et al., 2019;Hewitt and Manning, 2019;Manning et al., 2020).</p>
<p>3 Minimal Pair Probing = Minimal Pair + Diagnostic Probing</p>
<p>While prior neurolinguistic approaches have explored internal representations, they often employed coarse-grained datasets and primarily focused on decoding linguistic labels from embeddings, providing a general perspective on the linguistic features encoded in LMs.In contrast, the minimal pair probing method presented by He et al. (2024) integrates minimal pair design with diagnostic probing.This combination leverages the granularity of minimal pair design and the layer-wise insights of diagnostic probing, thereby enabling a more detailed analysis of internal patterns for form and meaning.We adopt minimal pair decoding as the neurolinguistic paradigm in our work.Specifically, given an LLM f : x 0,1,...,i → x i+1 trained on dataset D O , we can extract the hidden state representations f l (S) of the l-th layer of stimuli S. Given a minimal pair dataset D m = {(S i + , S  a label z, we have internal representation f l (S i + ) and f l (S i − ) for each sentence.A minimal probing classifier g : f l (S) → ẑ is trained and evaluated on D m , with grammatical/conceptual performance measure Perf(f, D O , g, D m ).</p>
<p>Note that our focus is on evaluating the linguistic competence of the LLM f itself, i.e., Perf(f, D O ), rather than the capacity of the probing classifier g.As suggested by Hewitt and Liang (2019), even untrained or random representations can yield surprisingly high probing accuracy, raising concerns that the classifier may exploit dataset artifacts rather than meaningful representations.To control for the potential bias introduced by g, we construct a random embedding baseline.Specifically, for each sentence in the dataset, we assign a fixed random vector r, sampled from a Gaussian distribution with the same mean and standard deviation as the real model embeddings f l (S).Importantly, each sentence is consistently assigned the same random vector across occurrences, preserving instance-level identity that the probing classifier might exploit.This allows us to assess the extent to which task performance can be driven by superficial sentence-level cues rather than meaningful representations.We then compute Perf(g, D m ) by training g on these random embeddings, which reflects the inherent predictability or "shortcut" potential of the probing task.Therefore, our performance score incorporates a correction factor based on this random baseline, defined as:
Perf(f, DO) ≜ Perf(f, DO, g, Dm)•(1+ 0.5 − Perf(g, Dm) 0.5 ) (1)
This formula applies a correction term, penalizing cases where the probing classifier performs well even on random embeddings.When Perf(g, D m ) = 0.5, the correction factor is 1; if the performance is higher, the factor shrinks toward 0, discouraging overfitting or trivial tasks; if it drops below 0.5, the factor exceeds 1, slightly amplifying the model's score.This ensures that only meaningful representations in f contribute to the final evaluation.</p>
<p>Experiment Setup</p>
<p>Datasets and Models</p>
<p>We use minimal pair probing for English, Chinese, and German to assess grammaticality (form) and conceptuality (meaning).Table 1 presents the overall dataset information used in our experiments.We use Llama2-7B, Llama3-8B, and Qwen-7B models in both base and chat versions.Further dataset and model descriptions are in Appendix B and C.</p>
<p>Setup for Psycholinguistic Analysis</p>
<p>Direct Direct probability measurement calculates the probability of a sentence based on model logits.Accuracy is determined by whether the model assigns a higher probability to the grammatically or conceptually correct sentence within the minimal pair.Meta Metalinguistic prompting involves explicitly asking a question or specifying a task that requires a judgement about a linguistic expression.Following Hu and Levy (2023), we use one prompt for a minimal pair to present both sentences at once.For form tasks, we assign an identifier (1 or 2) to each sentence in the pair, present a multiple-choice question comparing both sentences, and compare the probabilities assigned by the model to each answer option, "1" or "2".For meaning tasks, we reformulate the property into a question and compare the probabilities of acceptable and unacceptable concepts as sentence continuations.Table 2 presents the prompts used in the experiments.</p>
<p>Setup for Neurolinguistic Analysis</p>
<p>Sentence Embedding We extract the last token in each sentence from each layer to serve as the representation for the whole sentence.Last token pooling ensures the representation contains the infor-mation of all preceding tokens (Meng et al., 2024).</p>
<p>Probing Performance We use logistic regression as the probing classifier and F1 score as the evaluation metric.The score for Perf(f, D O , g, D m ) and Perf(g, D m ) is calculated as the average F1 score across 5 cross-validation folds.Final performance Perf(f, D O ) is given by Formula 1.</p>
<p>Saturation and Maximum Layer We define the feature learning Saturation Layer as the layer where performance first reaches 95% of the peak on the curve.This layer indicates the number of layers required for the model to adequately learn specific linguistic features, after which its ability to capture these features stabilizes.The Maximum Layer is the layer at which performance reaches its peak.</p>
<p>Unsupervised Analysis We use t-SNE to visualize the sentence embedding of Llama2-7B for English form tasks.We employ PCA to reduce the dimensionality of the sentence embedding to 50 before applying t-SNE.</p>
<p>Results</p>
<p>Psycholinguistic vs Neurolinguistic</p>
<p>Figure 3 shows the performance of LLMs across all linguistic tasks.Figure 4 demonstrates the averaged performance of LLMs across models and 4 levels (syntax, morphology, syntax-semantics interfaces, concept).Figure 5 presents the average performance of LLMs across form and meaning tasks for Direct, Meta, and Neuro2 methods.We use the last layer's performance in the Neuro method when comparing psycho-and neurolinguistic paradigms, as both direct probability measurement and metalinguistic prompting rely on the last layer of LLMs.</p>
<p>Language performance and competence are distinct (Competence &gt; Performance).Figure 4 and 5 shows distinct results between language performance and competence.Moving from Meta → Direct → Neuro, the evaluation focus gradually shifts from language performance (task execution ability) to language competence (the underlying linguistic ability).Within the same task category, Neuro methods consistently yield higher performance than Direct methods, which in turn outperform Meta methods.This indicates that when evaluating pure linguistic competence, LLMs perform well, but their performance drops when assessed in a task-based setting.Tasks that emphasize language performance become more difficult, even if their language competence is high.For example, in the Neuro setting, performance on Syntax tasks reaches 97%, while in the Meta setting, it drops to 56.1%, showing a significant gap.This suggests that even when an LLM has strong competence in a given task, its performance can significantly decline when assessed under a performance-oriented evaluation.Direct probability measurement might not be a true competence assessment.As the Neuro method measures the internal representations of LLMs directly, it could serve as a reliable ground truth for estimating linguistic competence.Direct probability measurement falls short of achieving this ground truth in form assessment (especially for syntax and syntax-semantics-interface as shown in Figure 5).LLMs exhibit stronger mastery of form than meaning, regardless of performance or com- petence.As shown in Figure 5, LLMs consistently perform better on form-related tasks than on meaning-related tasks.This trend holds regardless of whether the model is a base or chat version.Crucially, this pattern is evident across all evaluation methods.This indicates that LLMs have a stronger grasp of linguistic form than conceptual meaning, whether assessed through task execution or underlying capability.</p>
<p>Instruction tuning won't change much competence but improve performance.Neuro results between the base and chat versions of LLMs reveal that instruction fine-tuning does not significantly alter the language competence of the models (ttest between Neuro-Base vs. Neuro-Chat as shown in Figure 4).With instruction fine-tuning (chat versions of LLMs), Meta performance on form improves significantly while meaning understanding remains stable.Figure 6 illustrates that after instruction tuning, the competence-performance gap (Neuro-Meta) significantly decreases for formrelated tasks, while the change for meaning-related tasks remains relatively small.This indicates that fine-tuning with well-designed instructions helps LLMs improve their performance on form-related tasks, bringing them closer to their underlying competence.However, for meaning-related tasks, instruction tuning does not lead to a fundamental improvement in understanding.This indicates that more optimized information access strategies can enhance the external performance of language models, particularly for form-related tasks.</p>
<p>Neurolinguistic Analysis 3</p>
<p>Layer-wise unsupervised dynamics reveal gradual emergence of form features Figure 7 illustrates the layer-wise differences between embeddings for grammatically correct and incorrect sentences.In early layers, the embedding difference appears scattered and unstructured, but as depth increases, they form clearer clusters, indicating a progressively refined sensitivity to syntactic correctness.By Layer 16 and beyond, distinct clusters emerge corresponding to syntax, morphology, and syntax-semantics interface.The results demonstrate that LLMs encode grammaticality judgments dynamically across layers, progressively structuring linguistic representations.Moreover, the formation of distinct clusters for different linguistic phenomena in the unsupervised analysis provides supporting evidence for subsequent supervised classification.</p>
<p>Gradual decline in encoding performance from structure to meaning.The results in Figure 8-(c) show that the performance scores for conceptual understanding are significantly lower than those for grammatical understanding.This pattern is consistent across all six models, suggesting a universal characteristic of LLMs.Moreover, as illustrated in Figure 8-(a),(b), the encoding performance progressively declines from more structural tasks to more semantic tasks, spanning syntax, morphology, the syntax-semantic interface, and finally concep-   tual understanding.This highlights that LLMs encode features less effectively as the tasks shift from structure-focused to meaning-focused.</p>
<p>LLMs encode form earlier than meaning.We compute the feature learning saturation and maximum layers for all 12 grammatical tasks and 4 conceptual tasks, averaging them to represent form and meaning, respectively.As shown in Figure 9, the saturation and maximum layers for meaning are generally higher than those for form across all six models.This suggests that LLMs stabilize their encoding of grammatical features before conceptual features.</p>
<p>Instruction tuning has minimal impact on the internal linguistic representations.As Figure 10 shows, performance differences for form and meaning remain near zero across all layers, indicating that instruction tuning minimally impacts internal linguistic representations, consistent with our psycholinguistic vs. neurolinguistic analysis.How does LLMs' understanding of meaning change when the form (language) varies?Since our COMPS-ZH and COMPS-DE datasets align with the concepts in the English COMPS dataset, we can explore whether LLMs' grasp of different linguistic forms for the same concept correlates with their understanding of meaning across languages.</p>
<p>Multilingual analysis</p>
<p>Our previous results suggest that instruction tuning has little influence on the internal representations.Therefore, we focus on the base LLMs here.</p>
<p>From Figure 11, for all models and languages, form consistently achieves higher performance than meaning, indicating it's easier for LLMs to make a stronger grasp of structural elements compared to conceptual comprehension.Extended multilingual analysis can be found in Appendix E.</p>
<p>Discussion</p>
<p>Language performance vs. competence: probing reveals deeper linguistic understanding than direct probability.Our results demonstrate that neurolinguistic probing uncovers linguistic competencies in LLMs that are not captured by psycholinguistic methods.While Meta performs the worst and Direct performs better, Neuro consistently outperforms both, revealing a systematic underestimation of competence when relying on output-based evaluations.</p>
<p>Hu and Levy (2023) argued that Direct probability measurement, being more intrinsic than metalinguistic prompting, better reflects competence.However, our findings show that even Direct falls short of revealing the full extent of LLMs' linguistic capabilities.Direct relies on the final output layer, which is highly optimized for next-word prediction and thus entangled with task-specific objectives.Prior studies (Hewitt and Manning, 2019;Liu et al., 2019) have shown that syntactic and general linguistic information is often better represented in intermediate layers than in the final layer.Waldis et al. (2024) also emphasized that output correctness is an insufficient indicator of linguistic understanding, advocating for probing internal representations.</p>
<p>Our t-SNE visualizations corroborate this: clear linguistic clusters emerge in intermediate layers but dissolve in the final layer, reinforcing the view that the last layer is not optimal for assessing competence.These findings suggest that Direct, while more grounded than prompting, is still a limited proxy for internal knowledge.</p>
<p>In contrast, neurolinguistic probing inspects internal activation patterns across layers and tasks, uncovering the underlying representational structure of form and meaning, and further validates the discrepancy between performance and competence.</p>
<p>On the other hand, while Meta results underperform, this does not necessarily indicate that the LLMs lack the underlying linguistic competence.Instead, it may reflect limitations in information access, as suboptimal prompts can prevent models from exhibiting their full capabilities.Specifically, prompting failures do not always equate to a lack of encoded knowledge.This aligns with prior work (Firestone, 2020;Lampinen, 2024) emphasizing the need to distinguish performance conditions from underlying ability.Thus, we argue that probing, particularly when applied layer-wise, provides a more accurate and comprehensive assessment of linguistic competence than Direct probability alone.</p>
<p>Form and meaning: observations from Saussure's semiotics Our results reveal that LLMs consistently learn linguistic form before they grasp meaning.This may suggest a developmental trajectory where statistical patterns in syntax and grammar are more readily captured by the model than conceptual understanding.Second, the models' formal competence is generally superior to their semantic competence.This is evident in their ability to decode grammaticality structures accurately but with less reliable conceptual accuracy.</p>
<p>We further observe a linear correlation between form and meaning competence, particularly when linguistic forms vary across languages while meaning remains constant.This suggests that LLMs' understanding of meaning might rely heavily on form, with conceptual representation anchored to formal structures rather than independent meaning comprehension.These results offer a semiotic and neurolinguistic explanation for LLMs' longstanding issue of generating "confidently incorrect" responses, i.e., hallucinations (Ji et al., 2023).</p>
<p>Conclusion</p>
<p>This study adopts both psycho-and neuro-linguistic approaches to evaluating LLMs, revealing a distinction between linguistic performance and competence.Our results highlight the limitations of LLMs' semantic understanding and the need for future research to move beyond statistical correlations toward more grounded language representations.By introducing a cognitive neuroscience perspective, along with semiotics, we hope will inspire further research to deepen our understanding of the language capabilities of LLMs.</p>
<p>Limitations</p>
<p>This study has several limitations that may impact the generalizability and comprehensiveness of our findings.First, we did not include experiments covering a wider range of languages, which restricts the cross-linguistic applicability of our results.Especially for the analysis and discussion in Section 5.3 on multilingual content, which highlights the necessity of constructing multilingual conceptual datasets.</p>
<p>Second, the evaluation results for German are notably poor, potentially due to the presence of very long sentences in the DistilLingEval dataset, which might have introduced challenges for the models.This underscores the need for constructing syntactic minimal pair datasets for German.</p>
<p>Additionally, our experiments were conducted using small-scale LLMs due to computational resource constraints.This may have introduced a bias in our findings, as larger-scale models could exhibit different behaviors.Future studies should explore larger models to validate and extend the generalizability of these results.</p>
<p>Lastly, the COMPS dataset used for assessing conceptual understanding is not sufficiently finegrained, as it is limited to only four types of conceptual relationships.A more granular dataset could provide deeper insights into the nuances of how LLMs encode and process meaning.Future work should address this limitation by incorporating more diverse and detailed datasets.</p>
<p>Ethics Statement</p>
<p>Our project has the potential to raise greater awareness within the computational linguistics community about the challenges faced by low-resource languages.By highlighting the unique linguistic features and limited computational tools available for these languages, we aim to inspire further research and the development of more inclusive language technologies that can better serve underrepresented linguistic communities.</p>
<p>A Neuro Probing's Cognitive Science Background</p>
<p>Psycholinguistics often involves examining realtime language processing, linguistic knowledge storage, and language acquisition, using behavioral experimental methods such as reading times and eye-tracking.Neurolinguistics, on the other hand, focuses on the neural basis of language, employing techniques such as functional magnetic resonance imaging (fMRI), electroencephalography (EEG), and magnetoencephalography (MEG) to map linguistic functions to specific brain regions and to investigate how neural activity correlates with linguistic tasks.</p>
<p>While psycholinguistics aims to reveal mental processes underlying language use, neurolinguistics seeks to uncover the neural pathways that implement these processes.</p>
<p>Our minimal pair probing is inspired by cognitive neuroscience.In the field of neurolinguistics, decoding analysis has become a fundamental technique in cognitive neuroscience.It tries to extract information encoded in neural patterns (Kriegeskorte et al., 2006).It trains a classifier to predict the properties of the stimulus (e.g. a particular image or word input), from the neural responses.If the accuracy of the trained classifier is significantly better than chance, we conclude that the neural data encodes information about the predicted stimulus properties (Norman et al., 2006;Haynes and Rees, 2006).</p>
<p>Mesgarani and Chang ( 2012) is a representative work employing decoding analysis in the realm of language and speech.They use electrocorticography (ECoG) to record neural responses from subjects who are listening to speech.Leveraging decoding analysis, they were able to differentiate between neural patterns induced by attended speech and those elicited by background speech (to be ignored by the test persons), thereby highlighting the perceptual differences between the two speech stimulus conditions.</p>
<p>While psycholinguistic approaches provide valuable insights into LLMs' functional capabilities, they often fall short in revealing the underlying mechanisms of language processing.The opaque nature of neural network structures means that performance on external tasks does not necessarily reflect the internal cognitive processes at play.This gap necessitates a neurolinguistic approach to gain a deeper understanding of how LLMs encode and process language.</p>
<p>B Dataset Details</p>
<p>For each language, we use one dataset for grammatical minimal pairs and one dataset for conceptual minimal pairs.</p>
<p>B.1 Form: BLiMP, CLiMP, and DistilLingEval</p>
<p>BLiMP BLiMP (Warstadt et al., 2020) is a comprehensive English dataset of grammatical minimal pairs.It consists of minimal pairs for 13 higherlevel linguistic phenomena in the English language, further divided into 67 distinct realizations, called paradigms.Each paradigm comprises 1,000 individual minimal pairs, resulting in a total corpus size of 67,000 data points.</p>
<p>CLiMP CLiMP (Xiang et al., 2021) is a corpus of Chinese grammatical minimal pairs consisting of 16 datasets, each containing 1,000 sentence pairs.CLiMP covers 9 major Chinese language phenomena in total, fewer than the BLiMP dataset due to the less inflectional nature of Mandarin Chinese.</p>
<p>The vocabulary of the CLiMP dataset is based on the translation of the BLiMP dataset, with words and features specific to Chinese added.</p>
<p>DistilLingEval DistilLingEval (Vamvas and Sennrich, 2021) (Misra et al., 2023).In the realm of multilingual NLP research, it is a common practice to extend English datasets to other languages using human translation, machine translation, or translation assisted by LLMs (Nie et al., 2023;Wang et al., 2023;Beniwal et al., 2024).In this study, to create COMPS-DE and COMPS-ZH from the original English COMPS, we employed a hybrid approach that integrated process machine translation with meticulous human verification.</p>
<p>Specifically, we translated the concepts and properties of the English COMPS individually, subsequently merging them to form complete sentences and compose conceptual minimal pairs.The translation process began with the use of the Google Translate API4 , which provided initial translations of concepts and properties into German and Chinese.</p>
<p>Following this, native speakers of Chinese and German manually checked and refined these translations to ensure accuracy and quality.The manual review emphasized two main areas: accuracy of concepts and grammatical consistency of properties.For concepts, the focus was on correcting ambiguities that might arise from machine translation.For properties, attention was given to maintaining grammatical consistency with the original English text, such as ensuring subject-verb agreement, which is particularly challenging in German translations.</p>
<p>In summary, out of 521 concepts, manual corrections were made to 57 entries in the Chinese dataset and 49 in the German dataset.Similarly, out of 3,592 properties, 713 required manual corrections in the Chinese dataset, and 512 in the German dataset.This rigorous process was essential for preserving the integrity and reliability of the datasets.</p>
<p>C Model Details</p>
<p>In our experiments, we used three open-source LLMs, two English-centric LLMs (Llama2 and Llama3), and one multilingual LLM (Qwen) with a focus on English and Chinese.These models were trained on different amounts of English, Chinese, and German data (see Table 3).</p>
<p>Resource  Llama2 and Llama3 Llama2 (Touvron et al., 2023b) and Llama3 (AI, 2024) are two Englishcentric LLMs which represent an advanced iteration of the Llama foundation models developed by Meta AI (Touvron et al., 2023a).The Llama madels were trained on publicly available corpora predominantly in English.Despite this focus, Llama models are also exposed to a limited amount of multilingual data.Llama 1, for example, is pretrained on an extensive scale of corpora comprising over 1.4 trillion tokens, of which less than 4.5% constitute multilingual data from 20 different languages.Llama 2 expands this linguistic diversity, featuring 27 languages, each representing more than 0.005% of the pertaining data.Therefore, English-centric models harness multilingual abilities (Lai et al., 2023).In this work, we use Llama2-7B and Llama3-8B for our experiments.</p>
<p>QWen QWen is a series of LLMs developed by Alibaba Inc. (Bai et al., 2023).Qwen was trained on 2-3 trillion tokens of multilingual pre-training data.It is essentially a multilingual LLM with a focus on English and Chinese.We use the Qwen-7B model in our experiments.</p>
<p>D Supplemented Results for English</p>
<p>Some noteworthy points from Figure 8-(a) include:</p>
<p>1) For the ellipsis task, especially, local features in layer 0 (the embedding layer before the Transformer structure) already provide sufficient linguistic information to accomplish the task without any contextual information.</p>
<p>2) Among the conceptual tasks, the random relationship shows significantly higher accuracy compared to the other three conceptual relationships, suggesting that LLMs find it challenging to distinguish between similar concepts.</p>
<p>Compared to Llama2, Llama3 won't improve internal grammatical capabilities much, but will learn concepts better and faster.</p>
<p>Figure 12 shows the layer-wise performance differences between Llama3 and Llama2, as well as between their chat versions.The red curves (meaning) exhibit a notable positive difference in the early layers, indicating that Llama3 has better conceptual learning capabilities compared to Llama2.The blue curves (form) remain close to zero across all layers, suggesting that there is no significant improvement in grammatical capabilities in Llama3 compared to Llama2.The t-test statistics in Figure 12 further support these results.</p>
<p>Figure 13 compares the feature learning saturation layers between Llama2 and Llama3.The results for form learning (blue bars) do not differ significantly between Llama2 and Llama3, and weakly significantly between Llama2_chat and Llama3_chat.However, the results for meaning learning (blue bars) are both highly significant, indi-  cating that Llama3 requires fewer layers to encode conceptual features than Llama2.This suggests that Llama3 comprehends sentences faster.</p>
<p>E Supplemented Results for Multilingual Analysis</p>
<p>Meaning competence is correlated to form competence. Figure 14 illustrates the relationship between Form competence (x-axis) and Meaning competence (y-axis) across English, German, and Chinese in the neuro assessment for LLMs.The positive correlation (R² = 0.48) suggests that higher meaning competence generally corresponds to higher form competence. Llama's performance on Chinese.Despite Chinese not being the primary training language of the Llama2 models, they still perform well in encoding Chinese form/grammar.However, both Qwen, which is primarily trained on Chinese, and the Llama models show relatively poor performance in understanding Chinese meaning/concepts.</p>
<p>Improvement in Llama3 for Chinese Semantics.</p>
<p>Llama3 shows some improvement over Llama2 in understanding Chinese semantics, as indicated by the slightly higher red curve in the middle row.The improvement in syntactic understanding is minimal.</p>
<p>Qwen's Faster Syntax Learning but Slower Semantic Encoding for Chinese.Compared to the Llama models, Qwen learns Chinese grammar faster, as indicated by the sharper rise of the blue curve.However, it encodes Chinese semantics more slowly, evidenced by the larger gap between the form and meaning curves in the early layers.</p>
<p>Poor Performance for German.For German, a low-resource language, all three models perform poorly.Despite Chinese not being a primary training language for the Llama models, their performance is relatively decent, suggesting that the actual proportion of German training data might be much smaller.This highlights differences in the resource allocation for the three languages.</p>
<p>Form needs less data to capture compared to meaning.From Table 3, Chinese is classified as a mid-resourced language for Llama, yet it achieves high form competence (but low meaning competence), suggesting that capturing form requires less data than meaning.</p>
<p>F Supplemented Discussion</p>
<p>Developmental difference between human and machine intelligence.From a perspective of developmental psychology, human kids typically acquire conceptual understanding before mastering grammar (Bloom, 2002;Tomasello, 2005).Pinker (2009)'s semantic bootstrapping hypothesis posits that children initially learn vocabulary through semantic information and then use this semantic knowledge to infer syntactic structures.In contrast, our results indicate that LLMs learn grammar before meaning.Human intelligence is a combination of statistical inference and causal reasoning, whereas LLMs' intelligence is more likely a result of statistical inference (Tenenbaum et al., 2011;Gopnik and Wellman, 2012;Lake et al., 2017).Given this nature, the fact that LLMs learn form first might be because grammatical patterns are easier to statistically capture compared to meaning.</p>
<p>Symbol grounding problem and the quest for human-like intelligence In human language, the relationship between the signifier and the signified is often flexible and context-dependent, allowing for a more independent connection between syntax and semantics (Harnad, 1990).Human cognitive development typically involves acquiring conceptual understanding first, followed by the learning of rules and syntax.In contrast, our study shows that LLMs grasp syntax before meaning, relying on statistical correlations within formal structures to infer semantic content.This difference highlights a fundamental divergence between human and machine intelligence, as LLMs do not possess an inherent understanding of meaning detached from the formal structures they analyze.These observations suggest that, for LLMs to develop human-like intelligence, they must transcend mere statistical pattern recognition.This will likely require the integration of world knowledge and grounded experiences that go beyond linguistic inputs.To achieve a more robust form of artificial intelligence that mirrors human cognition, models must be able to ground symbols in realworld contexts, establishing a basis for genuine understanding (Tenenbaum et al., 2011;Lake et al., 2017) As it stands, the symbol grounding problem remains a significant barrier, and addressing it may be essential for constructing systems capable of true human-like reasoning and understanding.</p>
<p>Figure 1 :
1
Figure 1: Illustration of LLMs processing the same signified (meaning) across different signifiers (forms).</p>
<dl>
<dt>Figure 2 :</dt>
<dt>2</dt>
<dt>Figure 2: Psycholinguistic vs. Neurolinguistic Paradigm.Both direct probability measurement and metalinguistic prompting can be considered as psycholinguistic methods, while minimal pair probing (He et al., 2024) and other diagnostic probing are neurolinguistic.</dt>
<dd>
<p>a.The cats annoy Tim.(acceptable) b. <em>The cats annoys Tim.(unacceptable) (2) Concept understanding (Misra et al., 2023): a.A whisk adds air to a mixture.(acceptable) b. </em>A cup adds air to a mixture.</p>
</dd>
</dl>
<p>Figure 3 :
3
Figure 3: Psycholinguistic (meta and direct) and neurolinguistic performance across models and linguistic tasks.The x-axis represents different models and conditions (base and chat), while the y-axis categorizes linguistic tasks based on structural (syntax, morphology, syntax-semantics interface) and conceptual (meaning) levels.</p>
<p>Figure 4 :
4
Figure 4: Averaged psycholinguistic (meta and direct) and neurolinguistic results across models and tasks.ttests were conducted on the original (pre-averaging) results between base and chat models, with p-values annotated.</p>
<p>Figure 5 :
5
Figure 5: Psycholinguistic and neurolinguistic performance for form (morphology, syntax-semantics interface, and syntax) and meaning (concept).</p>
<p>Figure 6 :
6
Figure 6: Competence and performance gap drops after instruction tuning.</p>
<p>Figure 7
7
Figure7: t-SNE visualization of embedding differences between acceptable and unacceptable sentences, with red for syntax, purple for morphology, and yellow for the syntax-semantics interface.</p>
<p>Figure 8 :
8
Figure 8: (a) Neurolinguistic probing performance for 16 tasks in Llama-2, including 4 syntax tasks, 4 morphology tasks, 4 syntax-semantics interface tasks, and 4 conceptual tasks.(b) Average probing performance across the four linguistic categories in Llama-2.(c) Mean performance comparison between form-related tasks (syntax, morphology, syntax-semantics interface) and meaning-related tasks (concept), aggregated across all six models.</p>
<p>Figure 9 :
9
Figure 9: Feature learning saturation layer (defined as the first layer reaching 95% of peak performance) and the layer of maximum performance.</p>
<p>Figure 10 :
10
Figure 10: Difference in probing performance between base and instruction-tuned models across all layers.</p>
<p>Figure 11 :
11
Figure 11: Neuro probing results for English, Chinese, and German.</p>
<p>Figure 12 :
12
Figure 12: Top: Performance difference between Llama3 and Llama2.Bottom: Layer-wise t-test results.T-tests were first performed separately on each linguistic task, and then Stouffer's Z-score method (Stouffer et al., 1949) was employed to aggregate the final p-value at the condition level.</p>
<p>Figure 13 :
13
Figure 13: T-test results between Llama2 and Llama3 feature learning saturation layer.The symbols '**<em>', and '</em>' denote t-test p-values less than 0.001 and 0.05, respectively.</p>
<p>Figure 14 :
14
Figure 14: Correlation between meaning competence and form competence.</p>
<p>Figure 15 :
15
Figure 15: Detailed English decoding results on 6 models.</p>
<p>Figure 16 :
16
Figure16: Detailed Chinese decoding results on 6 models.Notice that the pink, orange, and blue curves don't denote morphology or semantics as those in English do.They are made just to make it easier to distinguish in the figure.All non-red curves represent grammatical tasks and red curves represent conceptual tasks.</p>
<p>Figure 17 :
17
Figure 17: Detailed German decoding results on 6 models.All non-red curves are grammatical tasks, and red curves are conceptual tasks.</p>
<p>Table 1 :
1
Overview of datasets in our study.
i − ), (z i + , z i − )} with each sentence S has</p>
<p>Table 2 :
2
Prompt examples for baseline methods.The region where we measure probability is marked in color.Correct sentences and answers are in blue; incorrect in red.</p>
<p>is a dataset of German grammatical minimal pairs.It consists of minimal pairs for eight German linguistic phenomena.This dataset contains 82,711 data samples in total.
B.2 Meaning: COMPS, COMPS-ZH, andCOMPS-DECOMPS COMPS (Misra et al., 2023) is an En-glish dataset of conceptual minimal pairs for testingan LLM's knowledge of everyday concepts (e.g.,a beaver/*gorilla has a flat tail). This dataset con-tains 49,340 sentence pairs, constructed using 521concepts and 3,592 properties. Concepts in thepairs constitute 4 types of knowledge relationships:taxonomy, property norms, co-occurrence, and ran-dom.COMPS-ZH and COMPS-DE COMPS-DEand COMPS-ZH are newly developed datasetsfeaturing conceptual minimal pairs in Chineseand German, derived from the English COMPSdataset</p>
<p>Table 3 :
3
Resource level for different languages across three LLMs.Note the resource levels are qualitative assessments based on available information, as specific quantitative data is not provided by the developers.</p>
<p>We refer to minimal pair probing as Neuro for simplicity.
Raw results for English, Chinese, and German can be found in Figure15, 16 and 17 in the Appendix.
https://cloud.google.com/translate
AcknowledgementsWe thank the anonymous reviewers for their valuable advice and feedback.This research was supported by DFG (German Research Foundation) grant SCHU 2246/14-1 and Munich Center for Machine Learning (MCML).
Introducing meta llama 3: The most capable openly available llm to date. A I Meta, 2024</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.16609Qwen technical report. 2023arXiv preprint</p>
<p>Probing classifiers: Promises, shortcomings, and advances. Yonatan Belinkov, Computational Linguistics. 4812022</p>
<p>Analysis methods in neural language processing: A survey. Yonatan Belinkov, James Glass, Transactions of the Association for Computational Linguistics. 72019</p>
<p>Climbing towards nlu: On meaning, form, and understanding in the age of data. M Emily, Alexander Bender, Koller, Proceedings of the 58th annual meeting of the association for computational linguistics. the 58th annual meeting of the association for computational linguistics2020</p>
<p>Cross-lingual editing in multilingual language models. Himanshu Beniwal, D Kowsik, Mayank Singh, Findings of the Association for Computational Linguistics: EACL 2024. St. Julian's, MaltaAssociation for Computational Linguistics2024</p>
<p>Is incoherence surprising? targeted evaluation of coherence prediction from language models. Anne Beyer, Sharid Loáiciga, David Schlangen, arXiv:2105.034952021arXiv preprint</p>
<p>How children learn the meanings of words. Paul Bloom, 2002MIT press</p>
<p>Language and the brain: a slim guide to neurolinguistics. Jonathan R Brennan, 2022Oxford University Press</p>
<p>Cours de linguistique générale. Ferdinand De, Saussure , 1989Otto Harrassowitz Verlag1</p>
<p>Psycholinguistics: The key concepts. John Field, 2004Psychology Press</p>
<p>Performance vs. competence in human-machine comparisons. Proceedings of the National Academy of Sciences. 117432020Chaz Firestone</p>
<p>The brain basis of language processing: from structure to function. Angela D Friederici, Physiological reviews. 9142011</p>
<p>Richard Futrell, Ethan Wilcox, Takashi Morita, Peng Qian, Miguel Ballesteros, Roger Levy, arXiv:1903.03260Neural language models as psycholinguistic subjects: Representations of syntactic state. 2019arXiv preprint</p>
<p>Syntaxgym: An online platform for targeted evaluation of language models. Jon Gauthier, Jennifer Hu, Ethan Wilcox, Peng Qian, Roger Levy, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations2020</p>
<p>Reconstructing constructivism: causal models, bayesian learning mechanisms, and the theory theory. Alison Gopnik, Henry M Wellman, Psychological bulletin. 138610852012</p>
<p>Distributional vectors encode referential attributes. Abhijeet Gupta, Gemma Boleda, Marco Baroni, Sebastian Padó, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language Processing2015</p>
<p>The symbol grounding problem. Stevan Harnad, Physica D: Nonlinear Phenomena. 421-31990</p>
<p>Decoding mental states from brain activity in humans. John-Dylan Haynes, Geraint Rees, Nature reviews neuroscience. 772006</p>
<p>Decoding probing: Revealing internal linguistic structures in neural language models using minimal pairs. Linyang He, Peili Chen, Ercong Nie, Yuanning Li, Jonathan R Brennan, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)2024</p>
<p>Designing and interpreting probes with control tasks. John Hewitt, Percy Liang, arXiv:1909.033682019arXiv preprint</p>
<p>A structural probe for finding syntax in word representations. John Hewitt, Christopher D Manning, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies20191</p>
<p>Fluid concepts and creative analogies: Computer models of the fundamental mechanisms of thought. Douglas R Hofstadter, 1995Basic books</p>
<p>Jennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox, Roger P Levy, arXiv:2005.03692A systematic assessment of syntactic generalization in neural language models. 2020arXiv preprint</p>
<p>Prompting is not a substitute for probability measurements in large language models. Jennifer Hu, Roger Levy, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, ACM Computing Surveys. 55122023</p>
<p>Event knowledge in large language models: the gap between the impossible and the unlikely. Carina Kauf, Anna A Ivanova, Giulia Rambelli, Emmanuele Chersoni, Selena Jingyuan, Zawad She, Evelina Chowdhury, Alessandro Fedorenko, Lenci, Cognitive Science. 4711e133862023</p>
<p>Cognitive neuroscience of language. David Kemmerer, 2022Routledge</p>
<p>What's in an embedding? analyzing word embeddings through multilingual evaluation. Arne Köhn, 2015</p>
<p>Information-based functional brain mapping. Nikolaus Kriegeskorte, Rainer Goebel, Peter Bandettini, 2006103Proceedings of the National Academy of Sciences of the United States of America</p>
<p>ChatGPT beyond English: Towards a comprehensive evaluation of large language models in multilingual learning. Viet Lai, Nghia Ngo, Amir Pouran, Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, Thien Nguyen, 10.18653/v1/2023.findings-emnlp.878Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Building machines that learn and think like people. Brenden M Lake, Joshua B Tomer D Ullman, Samuel J Tenenbaum, Gershman, Behavioral and brain sciences. 40e2532017</p>
<p>Can language models handle recursively nested grammatical structures? a case study on comparing models and humans. Andrew Lampinen, Computational Linguistics. 2024</p>
<p>Assessing the ability of lstms to learn syntaxsensitive dependencies. Tal Linzen, Emmanuel Dupoux, Yoav Goldberg, Transactions of the Association for Computational Linguistics. 20164</p>
<p>Matt Nelson F Liu, Yonatan Gardner, Matthew E Belinkov, Noah A Peters, Smith, arXiv:1903.08855Linguistic knowledge and transferability of contextual representations. 2019arXiv preprint</p>
<p>Emergent linguistic structure in artificial neural networks trained by self-supervision. Kevin Christopher D Manning, John Clark, Urvashi Hewitt, Omer Khandelwal, Levy, Proceedings of the National Academy of Sciences. the National Academy of Sciences2020117</p>
<p>Targeted syntactic evaluation of language models. Rebecca Marvin, Tal Linzen, 10.18653/v1/D18-1151Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Sfrembedding-mistral:enhance text retrieval with transfer learning. Rui Meng, Ye Liu, Rayhan Shafiq, Caiming Joty, Yingbo Xiong, Semih Zhou, Yavuz, 2024Salesforce AI Research Blog</p>
<p>Selective cortical representation of attended speaker in multitalker speech perception. Nima Mesgarani, Edward F Chang, Nature. 48573972012</p>
<p>Comps: Conceptual minimal pair sentences for testing robust property knowledge and its inheritance in pre-trained language models. Kanishka Misra, Julia Rayz, Allyson Ettinger, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. the 17th Conference of the European Chapter of the Association for Computational Linguistics2023</p>
<p>The debate over understanding in ai's large language models. Melanie Mitchell, David C Krakauer, Proceedings of the National Academy of Sciences. 12013e22159071202023</p>
<p>Cross-lingual retrieval augmented prompt for low-resource languages. Ercong Nie, Sheng Liang, Helmut Schmid, Hinrich Schütze, Findings of the Association for Computational Linguistics: ACL 2023. 2023</p>
<p>Decomposed prompting: Unveiling multilingual linguistic structure knowledge in englishcentric large language models. Ercong Nie, Shuzhou Yuan, Bolei Ma, Helmut Schmid, Michael Färber, Frauke Kreuter, Hinrich Schütze, arXiv:2402.183972024arXiv preprint</p>
<p>Beyond mind-reading: multivoxel pattern analysis of fmri data. Sean M Kenneth A Norman, Greg J Polyn, James V Detre, Haxby, Trends in cognitive sciences. 1092006</p>
<p>Language learnability and language development: with new commentary by the author. Steven Pinker, 2009Harvard University Press7</p>
<p>Large language models and the reverse turing test. Terrence J Sejnowski, Neural computation. 3532023</p>
<p>Does string-based neural mt learn source syntax?. Xing Shi, Inkit Padhi, Kevin Knight, Proceedings of the 2016 conference on empirical methods in natural language processing. the 2016 conference on empirical methods in natural language processing2016</p>
<p>Edward A Samuel A Stouffer, Leland C Suchman, Shirley A Devinney, Robin M Star, WilliamsJr, The american soldier: Adjustment during army life.(studies in social psychology in world war ii). 1949</p>
<p>Joshua B Tenenbaum, Charles Kemp, Thomas L Griffiths, Noah D Goodman, How to grow a mind: Statistics, structure, and abstraction. science. 2011331</p>
<p>What do you learn from context? probing for sentence structure in contextualized word representations. Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, Thomas Mccoy, Najoung Kim, Benjamin Van Durme, Dipanjan Samuel R Bowman, Das, arXiv:1905.063162019. 2005Harvard university pressarXiv preprintConstructing a language: A usage-based theory of language acquisition</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023barXiv preprint</p>
<p>Matthew Traxler, Morton Ann, Gernsbacher , Handbook of psycholinguistics. Elsevier2011</p>
<p>On the limits of minimal pairs in contrastive evaluation. Jannis Vamvas, Rico Sennrich, arXiv:2109.074652021arXiv preprint</p>
<p>Holmes a benchmark to assess the linguistic competence of language models. Andreas Waldis, Yotam Perlitz, Leshem Choshen, Transactions of the Association for Computational Linguistics. 122024Yufang Hou, and Iryna Gurevych</p>
<p>Weixuan Wang, Barry Haddow, Alexandra Birch, arXiv:2312.13040Retrieval-augmented multilingual knowledge editing. 2023arXiv preprint</p>
<p>Blimp: The benchmark of linguistic minimal pairs for english. Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, Samuel R Bowman, Transactions of the Association for Computational Linguistics. 82020</p>
<p>Beilei Xiang, Changbing Yang, Yu Li, Alex Warstadt, Katharina Kann, arXiv:2101.11131Climp: A benchmark for chinese language model evaluation. 2021arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
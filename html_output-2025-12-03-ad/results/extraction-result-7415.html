<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7415 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7415</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7415</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-272832326</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.15657v2.pdf" target="_blank">M^2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning</a></p>
                <p><strong>Paper Abstract:</strong> Multimodal Large Language Models (MLLMs) demonstrate remarkable performance across a wide range of domains, with increasing emphasis on enhancing their zero-shot generalization capabilities for unseen tasks across various modalities. Instruction tuning has emerged as an effective strategy for achieving zero-shot generalization by finetuning pretrained models on diverse multimodal tasks. As the scale of MLLMs continues to grow, parameter-efficient finetuning becomes increasingly critical. However, most existing parameter-efficient approaches focus only on single modalities and often overlook the multimodal characteristics during finetuning. In this work, we introduce a novel Multimodal Prompt Tuning (M^2PT) approach for efficient instruction tuning of MLLMs. M^2PT effectively integrates visual and textual prompts into the vision encoder and language processor respectively during finetuning, facilitating the extraction and alignment of features across modalities. Empirical results on various multimodal evaluation datasets demonstrate the superior performance of our approach compared to several state-of-the-art baselines. A comprehensive set of ablation studies validates the effectiveness of our prompt design and the efficiency of our approach.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7415.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7415.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>M2PT (multimodal prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multimodal Prompt Tuning (M2PT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parameter-efficient finetuning method that injects learned soft prompts into both the vision encoder (visual prompts) and the language model (textual prompts) and adds a cross-modality interaction layer to align visual embeddings to the LLM embedding space; evaluated for zero-shot multimodal instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA (stage-one) with M2PT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaVA multimodal model using Vicuna-based LLM (Vicuna-7B-v1.3 as base) and CLIP-L vision encoder; M2PT tunes only small sets of soft prompts + an interaction projection while keeping backbone frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (Vicuna-7B base LLM used in LLaVA); evaluation used Vicuna-13B-v1.5 as automated judge (separately)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MME (zero-shot multimodal evaluation) and other multimodal tasks (CIFAR-10/100, MNIST, Text-VQA, VSR, SNLI-VE, POPE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot instruction-following across perception and cognition multimodal tasks (classification, VQA, OCR, reasoning, entailment, hallucination detection).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot instruction prompts with learned continuous soft prompts injected into vision tokens and/or textual input; classification tasks formatted with an instruction template; predictions judged by Vicuna-13B-v1.5.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / input modality (multimodal prompt injection)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Learnable visual prompts prepended to visual token sequences (leftmost), textual prompts prepended to LLM input tokens; prompt lengths tuned from {0,5,10,20,40}; evaluated zero-shot (no in-context examples) using standardized instruction templates; cross-modality linear projection aligns visual outputs into LLM token space.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MME aggregate score and per-dataset accuracy (accuracy % for individual datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>M2PT 10/20: MME = 1503.98 (aggregate); CIFAR-10 = 89.29% accuracy; POPE = 81.26% accuracy; MNIST = 95.54% accuracy (see Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Best prompt baselines: APrompt MME = 1406.63; VPT MME = 1398.74; Full finetune LLaVA FT MME = 1587.26</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Compared to APrompt: +97.35 absolute MME (+6.9% relative); compared to VPT: +105.24 absolute MME (+7.5% relative); M2PT reaches 94.75% of full finetuning (1503.98 / 1587.26 = 94.75%).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot evaluation on MME and seven multimodal datasets; M2PT variants reported: 10/10 and 10/20 (textual/visual prompt lengths); training used Vision-Flan (~191K instances, up to 1k per task), grid search over learning rates and prompt lengths; evaluated with Vicuna-13B-v1.5 judge for per-dataset accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'M^2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7415.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7415.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Visual prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Visual soft prompts (layerwise)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Learnable continuous vectors inserted at the leftmost positions of vision encoder token sequences for specified layers; used to steer the visual encoder's representations during prompt tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA + M2PT (CLIP-L vision encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision encoder (CLIP-L) with learnable visual prompt tokens inserted at chosen encoder layers while keeping encoder weights frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>vision encoder size not explicitly given (CLIP-L), overall LLaVA uses Vicuna-7B for LLM</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MME and individual multimodal tasks (e.g., CIFAR-10, POPE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks require visual perception/classification or visual-linguistic reasoning; visual prompts aim to improve representation for zero-shot instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Soft visual prompt tokens prepended to vision token sequence (leftmost) at specific encoder layers (configurable: first layer, odd layers, top half, latter half, all layers).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>input modality (vision prompt insertion)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Visual prompt lengths searched over {0,5,10,20,40}; inserted at configurable layer sets (first, odd, top half, latter half, all). Visual prompts' activation maps show heightened activation in the visual encoder; ablation shows removing visual prompts reduces performance and their importance varies by task (most impact on MME).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MME aggregate score and task accuracies</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example: M2PT 10/20 (with visual prompts) MME = 1503.98; ablation (removing visual prompts) caused a measurable drop (figure reported but exact ablation numbers not tabulated in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>VPT (visual-only prompt tuning) MME = 1398.74; M2PT (multimodal visual+text prompts) improves over VPT by +105.24 MME points</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Adding visual prompts as part of M2PT (vs visual-only VPT) leads to +105.24 absolute MME improvement (7.5% relative), though this includes benefits from textual prompts and interaction layer as well.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Visual prompt length grid search; prompt location experiments (see next entry). Attention activation maps analyzed to show visual prompt influence.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'M^2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7415.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7415.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Textual prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Textual soft prompts (LLM injection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Learnable continuous prompt vectors prepended into the language model input at chosen layers to alter LLM internal representations and guide instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA + M2PT (Vicuna-7B LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vicuna-7B-based LLM kept frozen except for inserted textual prompt embeddings at selected LLM layers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (Vicuna-7B base LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MME and individual multimodal tasks (notably CIFAR-10 and POPE where textual prompts matter most)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot instruction-following where textual context and instruction formatting influence model output; textual prompts aim to shape LLM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Soft textual prompt tokens injected into LLM input at specified layer positions; textual prompt lengths tuned (0,5,10,20,40).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style (LLM prompt injection)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Textual prompt placement configurable per-layer; grid search over lengths; attention activation maps show textual prompt tokens exhibit elevated activations in LLM attention; ablation shows textual prompts critically important for CIFAR-10 and POPE tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MME and per-dataset accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>M2PT 10/20 vs ablations: full M2PT MME = 1503.98; removing textual prompts reduces accuracy (specific absolute drop reported in Figure 4 but not tabulated numerically in main table).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>APrompt (textual/attention prompts only) MME = 1406.63</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>M2PT (multimodal textual+visual) vs APrompt (textual-only): +97.35 absolute MME improvement; for certain tasks (CIFAR-10, POPE) authors report textual prompts are the most important component.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Textual prompt length grid search; textual prompt inject positions varied; zero-shot instruction templates used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'M^2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7415.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7415.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt length (visual/text)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt length hyperparameter (visual and textual prompt lengths)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The number of soft prompt tokens inserted per modality strongly affects performance; authors grid-searched prompt lengths and observed gains until saturation and possible overparameterization beyond ~40 tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA + M2PT (Vicuna-7B + CLIP-L)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same MLLM; prompt length controls number of learned embeddings per insertion location in vision encoder and LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B base LLM; prompt lengths themselves are small parameter additions (0.08–0.09% of total params for tested configs).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Vision-Flan finetuning and MME evaluation (representative tasks: MME, POPE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot multimodal tasks where differing prompt lengths change model capacity to capture modal patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt length combinations (textual length × visual length); authors evaluated grid: {0,5,10,20,40} for each modality and reported heatmap of performance.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt hyperparameter (prompt size)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Observed saturation around length 40; significant gains when visual length increased from 5→20 and textual from 5→10; optimal combos vary per task (MME best at visual=20,textual=10; POPE best at visual=10,textual=40); too-long prompts may slightly decrease performance (overparameterization).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MME aggregate and per-task accuracies</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example comparison: M2PT 10/10 MME = 1490.17 vs M2PT 10/20 MME = 1503.98, an absolute +13.81 MME (+0.92% relative) improvement by increasing visual prompt length from 10→20 while keeping textual=10.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Authors report variable absolute improvements depending on tasks; specific observed patterns: gains when increasing visual from 5 to 20 and textual from 5 to 10; saturation beyond 40 with slight decreases on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Grid search over prompt lengths {0,5,10,20,40}; trained with Vision-Flan finetuning (up to 191k instances) and evaluated zero-shot on MME and other datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'M^2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7415.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7415.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt location (layer depth)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt insertion location (which layers prompts are injected into)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Where prompts are injected across model layers changes performance: earlier-layer insertion and deeper coverage (all layers) generally yield better results than inserting only in later layers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA + M2PT (Vicuna-7B + CLIP-L)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompts can be inserted at various subsets of layers in the vision encoder and LLM; five settings tested (first layer, odd layers, top half, latter half, all layers).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B base LLM</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MME, CIFAR-10, POPE (examples reported in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot multimodal tasks; effect of prompt insertion depth on overall MME and specific dataset accuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt injection location variants: (a) first layer only, (b) every odd-numbered layer, (c) top half of layers, (d) latter half of layers, (e) all layers.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / architectural placement</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Authors report best performance when prompts are injected into all layers (setting e); injecting only into latter layers yields worse results; early-layer prompts matter more than late-layer-only insertion.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MME aggregate score and per-dataset accuracies</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table 2: (a) First Layer: MME=1320.99, CIFAR-10=82.96%, POPE=79.48%; (b) Odd Layer: MME=1396.87, CIFAR-10=87.65%, POPE=75.79%; (c) Top Half: MME=1473.42, CIFAR-10=85.82%, POPE=80.31%; (d) Latter Half: MME=1249.39, CIFAR-10=83.72%, POPE=79.34%; (e) All Layers: MME=1503.98, CIFAR-10=89.29%, POPE=81.26%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>First layer only (a) and latter half only (d) serve as weaker baselines compared to all layers (e).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>All-layers (e) vs First-layer (a): +182.99 absolute MME (+13.9% relative); All-layers (e) vs Latter-half (d): +254.59 absolute MME (+20.4% relative).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>M2PT 10/20 configuration used for prompt location ablation; best prompt length per setting selected with MME evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'M^2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7415.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7415.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cross-modality interaction layer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linear cross-modality interaction/projection layer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tunable linear projection layer that maps the final vision encoder outputs into the textual embedding space so visual tokens can interact with LLM tokens; included as a learned component in M2PT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA + M2PT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A small learned projection f_in aligns O_N^v (vision encoder outputs) to O'_v in the LLM token space enabling cross-modal integration while keeping main backbone frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>small projection layer; overall model is Vicuna-7B based LLaVA</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MME and other multimodal tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring integration of visual and textual information (VQA, visual reasoning, classification with image context); projection layer enables multimodal alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Architectural addition (linear projection) rather than a change in natural-language prompt template; affects how visual tokens are presented to the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>input modality / architectural interface</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Projection maps visual encoder outputs into embedding space compatible with LLM input tokens; authors ablate removal of this interaction layer and report drops in performance, indicating its importance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MME aggregate and per-dataset accuracy; ablation yields lower scores (Figure 4) though exact numeric ablation values not fully tabulated in main table.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Full M2PT (with interaction) MME = 1503.98; removing the interaction layer causes measurable performance decrease (qualitative description in text and Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>M2PT without interaction layer (ablation) yields lower performance (exact numbers not provided in main text table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Authors state performance decreases when the interaction layer is removed; no explicit absolute/percent numbers provided in main text for the ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Ablation experiments done on M2PT 10/20 across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'M^2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7415.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7415.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt initialization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt initialization method (Xavier vs Random)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The initialization scheme for soft prompts affects final performance; authors compare Xavier initialization to random initialization and find Xavier yields more stable and better scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA + M2PT (10/20)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>M2PT with different initialization strategies for prompt embedding vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>prompts are small additional parameters; underlying LLaVA uses Vicuna-7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MME aggregate evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot multimodal benchmark aggregate score used to compare initialization strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Same prompt injection format; only initialization of learned prompt embeddings differs (Xavier vs random).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt hyperparameter (initialization)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Table S3 compares two initializations: Random and Xavier (Glorot & Bengio).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MME aggregate score</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>M2PT 10/20 with Xavier initialization: MME = 1503.98; with Random initialization: MME = 1405.67.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Random initialization baseline: MME = 1405.67</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Xavier vs Random: +98.31 absolute MME improvement (+7.0% relative).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>M2PT 10/20 configuration; all other training settings identical.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'M^2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Visual prompt tuning. <em>(Rating: 2)</em></li>
                <li>The power of scale for parameter-efficient prompt tuning. <em>(Rating: 2)</em></li>
                <li>APrompt: Attention prompt tuning for efficient adaptation of pre-trained language models. <em>(Rating: 2)</em></li>
                <li>VPT: Visual Prompt Tuning (E2VPT / VPT references in text). <em>(Rating: 2)</em></li>
                <li>PromptFuse: Modular and parameter-efficient multimodal fusion with prompting. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7415",
    "paper_id": "paper-272832326",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "M2PT (multimodal prompts)",
            "name_full": "Multimodal Prompt Tuning (M2PT)",
            "brief_description": "A parameter-efficient finetuning method that injects learned soft prompts into both the vision encoder (visual prompts) and the language model (textual prompts) and adds a cross-modality interaction layer to align visual embeddings to the LLM embedding space; evaluated for zero-shot multimodal instruction following.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaVA (stage-one) with M2PT",
            "model_description": "LLaVA multimodal model using Vicuna-based LLM (Vicuna-7B-v1.3 as base) and CLIP-L vision encoder; M2PT tunes only small sets of soft prompts + an interaction projection while keeping backbone frozen.",
            "model_size": "7B (Vicuna-7B base LLM used in LLaVA); evaluation used Vicuna-13B-v1.5 as automated judge (separately)",
            "task_name": "MME (zero-shot multimodal evaluation) and other multimodal tasks (CIFAR-10/100, MNIST, Text-VQA, VSR, SNLI-VE, POPE)",
            "task_description": "Zero-shot instruction-following across perception and cognition multimodal tasks (classification, VQA, OCR, reasoning, entailment, hallucination detection).",
            "problem_format": "Zero-shot instruction prompts with learned continuous soft prompts injected into vision tokens and/or textual input; classification tasks formatted with an instruction template; predictions judged by Vicuna-13B-v1.5.",
            "format_category": "prompt style / input modality (multimodal prompt injection)",
            "format_details": "Learnable visual prompts prepended to visual token sequences (leftmost), textual prompts prepended to LLM input tokens; prompt lengths tuned from {0,5,10,20,40}; evaluated zero-shot (no in-context examples) using standardized instruction templates; cross-modality linear projection aligns visual outputs into LLM token space.",
            "performance_metric": "MME aggregate score and per-dataset accuracy (accuracy % for individual datasets)",
            "performance_value": "M2PT 10/20: MME = 1503.98 (aggregate); CIFAR-10 = 89.29% accuracy; POPE = 81.26% accuracy; MNIST = 95.54% accuracy (see Table 1)",
            "baseline_performance": "Best prompt baselines: APrompt MME = 1406.63; VPT MME = 1398.74; Full finetune LLaVA FT MME = 1587.26",
            "performance_change": "Compared to APrompt: +97.35 absolute MME (+6.9% relative); compared to VPT: +105.24 absolute MME (+7.5% relative); M2PT reaches 94.75% of full finetuning (1503.98 / 1587.26 = 94.75%).",
            "experimental_setting": "Zero-shot evaluation on MME and seven multimodal datasets; M2PT variants reported: 10/10 and 10/20 (textual/visual prompt lengths); training used Vision-Flan (~191K instances, up to 1k per task), grid search over learning rates and prompt lengths; evaluated with Vicuna-13B-v1.5 judge for per-dataset accuracy.",
            "statistical_significance": null,
            "uuid": "e7415.0",
            "source_info": {
                "paper_title": "M^2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Visual prompts",
            "name_full": "Visual soft prompts (layerwise)",
            "brief_description": "Learnable continuous vectors inserted at the leftmost positions of vision encoder token sequences for specified layers; used to steer the visual encoder's representations during prompt tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaVA + M2PT (CLIP-L vision encoder)",
            "model_description": "Vision encoder (CLIP-L) with learnable visual prompt tokens inserted at chosen encoder layers while keeping encoder weights frozen.",
            "model_size": "vision encoder size not explicitly given (CLIP-L), overall LLaVA uses Vicuna-7B for LLM",
            "task_name": "MME and individual multimodal tasks (e.g., CIFAR-10, POPE)",
            "task_description": "Tasks require visual perception/classification or visual-linguistic reasoning; visual prompts aim to improve representation for zero-shot instruction following.",
            "problem_format": "Soft visual prompt tokens prepended to vision token sequence (leftmost) at specific encoder layers (configurable: first layer, odd layers, top half, latter half, all layers).",
            "format_category": "input modality (vision prompt insertion)",
            "format_details": "Visual prompt lengths searched over {0,5,10,20,40}; inserted at configurable layer sets (first, odd, top half, latter half, all). Visual prompts' activation maps show heightened activation in the visual encoder; ablation shows removing visual prompts reduces performance and their importance varies by task (most impact on MME).",
            "performance_metric": "MME aggregate score and task accuracies",
            "performance_value": "Example: M2PT 10/20 (with visual prompts) MME = 1503.98; ablation (removing visual prompts) caused a measurable drop (figure reported but exact ablation numbers not tabulated in main text).",
            "baseline_performance": "VPT (visual-only prompt tuning) MME = 1398.74; M2PT (multimodal visual+text prompts) improves over VPT by +105.24 MME points",
            "performance_change": "Adding visual prompts as part of M2PT (vs visual-only VPT) leads to +105.24 absolute MME improvement (7.5% relative), though this includes benefits from textual prompts and interaction layer as well.",
            "experimental_setting": "Visual prompt length grid search; prompt location experiments (see next entry). Attention activation maps analyzed to show visual prompt influence.",
            "statistical_significance": null,
            "uuid": "e7415.1",
            "source_info": {
                "paper_title": "M^2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Textual prompts",
            "name_full": "Textual soft prompts (LLM injection)",
            "brief_description": "Learnable continuous prompt vectors prepended into the language model input at chosen layers to alter LLM internal representations and guide instruction following.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaVA + M2PT (Vicuna-7B LLM)",
            "model_description": "Vicuna-7B-based LLM kept frozen except for inserted textual prompt embeddings at selected LLM layers.",
            "model_size": "7B (Vicuna-7B base LLM)",
            "task_name": "MME and individual multimodal tasks (notably CIFAR-10 and POPE where textual prompts matter most)",
            "task_description": "Zero-shot instruction-following where textual context and instruction formatting influence model output; textual prompts aim to shape LLM reasoning.",
            "problem_format": "Soft textual prompt tokens injected into LLM input at specified layer positions; textual prompt lengths tuned (0,5,10,20,40).",
            "format_category": "prompt style (LLM prompt injection)",
            "format_details": "Textual prompt placement configurable per-layer; grid search over lengths; attention activation maps show textual prompt tokens exhibit elevated activations in LLM attention; ablation shows textual prompts critically important for CIFAR-10 and POPE tasks.",
            "performance_metric": "MME and per-dataset accuracy",
            "performance_value": "M2PT 10/20 vs ablations: full M2PT MME = 1503.98; removing textual prompts reduces accuracy (specific absolute drop reported in Figure 4 but not tabulated numerically in main table).",
            "baseline_performance": "APrompt (textual/attention prompts only) MME = 1406.63",
            "performance_change": "M2PT (multimodal textual+visual) vs APrompt (textual-only): +97.35 absolute MME improvement; for certain tasks (CIFAR-10, POPE) authors report textual prompts are the most important component.",
            "experimental_setting": "Textual prompt length grid search; textual prompt inject positions varied; zero-shot instruction templates used for evaluation.",
            "statistical_significance": null,
            "uuid": "e7415.2",
            "source_info": {
                "paper_title": "M^2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Prompt length (visual/text)",
            "name_full": "Prompt length hyperparameter (visual and textual prompt lengths)",
            "brief_description": "The number of soft prompt tokens inserted per modality strongly affects performance; authors grid-searched prompt lengths and observed gains until saturation and possible overparameterization beyond ~40 tokens.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaVA + M2PT (Vicuna-7B + CLIP-L)",
            "model_description": "Same MLLM; prompt length controls number of learned embeddings per insertion location in vision encoder and LLM.",
            "model_size": "7B base LLM; prompt lengths themselves are small parameter additions (0.08–0.09% of total params for tested configs).",
            "task_name": "Vision-Flan finetuning and MME evaluation (representative tasks: MME, POPE)",
            "task_description": "Zero-shot multimodal tasks where differing prompt lengths change model capacity to capture modal patterns.",
            "problem_format": "Prompt length combinations (textual length × visual length); authors evaluated grid: {0,5,10,20,40} for each modality and reported heatmap of performance.",
            "format_category": "prompt hyperparameter (prompt size)",
            "format_details": "Observed saturation around length 40; significant gains when visual length increased from 5→20 and textual from 5→10; optimal combos vary per task (MME best at visual=20,textual=10; POPE best at visual=10,textual=40); too-long prompts may slightly decrease performance (overparameterization).",
            "performance_metric": "MME aggregate and per-task accuracies",
            "performance_value": "Example comparison: M2PT 10/10 MME = 1490.17 vs M2PT 10/20 MME = 1503.98, an absolute +13.81 MME (+0.92% relative) improvement by increasing visual prompt length from 10→20 while keeping textual=10.",
            "baseline_performance": null,
            "performance_change": "Authors report variable absolute improvements depending on tasks; specific observed patterns: gains when increasing visual from 5 to 20 and textual from 5 to 10; saturation beyond 40 with slight decreases on some tasks.",
            "experimental_setting": "Grid search over prompt lengths {0,5,10,20,40}; trained with Vision-Flan finetuning (up to 191k instances) and evaluated zero-shot on MME and other datasets.",
            "statistical_significance": null,
            "uuid": "e7415.3",
            "source_info": {
                "paper_title": "M^2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Prompt location (layer depth)",
            "name_full": "Prompt insertion location (which layers prompts are injected into)",
            "brief_description": "Where prompts are injected across model layers changes performance: earlier-layer insertion and deeper coverage (all layers) generally yield better results than inserting only in later layers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaVA + M2PT (Vicuna-7B + CLIP-L)",
            "model_description": "Prompts can be inserted at various subsets of layers in the vision encoder and LLM; five settings tested (first layer, odd layers, top half, latter half, all layers).",
            "model_size": "7B base LLM",
            "task_name": "MME, CIFAR-10, POPE (examples reported in Table 2)",
            "task_description": "Zero-shot multimodal tasks; effect of prompt insertion depth on overall MME and specific dataset accuracies.",
            "problem_format": "Prompt injection location variants: (a) first layer only, (b) every odd-numbered layer, (c) top half of layers, (d) latter half of layers, (e) all layers.",
            "format_category": "prompt style / architectural placement",
            "format_details": "Authors report best performance when prompts are injected into all layers (setting e); injecting only into latter layers yields worse results; early-layer prompts matter more than late-layer-only insertion.",
            "performance_metric": "MME aggregate score and per-dataset accuracies",
            "performance_value": "Table 2: (a) First Layer: MME=1320.99, CIFAR-10=82.96%, POPE=79.48%; (b) Odd Layer: MME=1396.87, CIFAR-10=87.65%, POPE=75.79%; (c) Top Half: MME=1473.42, CIFAR-10=85.82%, POPE=80.31%; (d) Latter Half: MME=1249.39, CIFAR-10=83.72%, POPE=79.34%; (e) All Layers: MME=1503.98, CIFAR-10=89.29%, POPE=81.26%.",
            "baseline_performance": "First layer only (a) and latter half only (d) serve as weaker baselines compared to all layers (e).",
            "performance_change": "All-layers (e) vs First-layer (a): +182.99 absolute MME (+13.9% relative); All-layers (e) vs Latter-half (d): +254.59 absolute MME (+20.4% relative).",
            "experimental_setting": "M2PT 10/20 configuration used for prompt location ablation; best prompt length per setting selected with MME evaluation.",
            "statistical_significance": null,
            "uuid": "e7415.4",
            "source_info": {
                "paper_title": "M^2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Cross-modality interaction layer",
            "name_full": "Linear cross-modality interaction/projection layer",
            "brief_description": "A tunable linear projection layer that maps the final vision encoder outputs into the textual embedding space so visual tokens can interact with LLM tokens; included as a learned component in M2PT.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaVA + M2PT",
            "model_description": "A small learned projection f_in aligns O_N^v (vision encoder outputs) to O'_v in the LLM token space enabling cross-modal integration while keeping main backbone frozen.",
            "model_size": "small projection layer; overall model is Vicuna-7B based LLaVA",
            "task_name": "MME and other multimodal tasks",
            "task_description": "Tasks requiring integration of visual and textual information (VQA, visual reasoning, classification with image context); projection layer enables multimodal alignment.",
            "problem_format": "Architectural addition (linear projection) rather than a change in natural-language prompt template; affects how visual tokens are presented to the LLM.",
            "format_category": "input modality / architectural interface",
            "format_details": "Projection maps visual encoder outputs into embedding space compatible with LLM input tokens; authors ablate removal of this interaction layer and report drops in performance, indicating its importance.",
            "performance_metric": "MME aggregate and per-dataset accuracy; ablation yields lower scores (Figure 4) though exact numeric ablation values not fully tabulated in main table.",
            "performance_value": "Full M2PT (with interaction) MME = 1503.98; removing the interaction layer causes measurable performance decrease (qualitative description in text and Figure 4).",
            "baseline_performance": "M2PT without interaction layer (ablation) yields lower performance (exact numbers not provided in main text table).",
            "performance_change": "Authors state performance decreases when the interaction layer is removed; no explicit absolute/percent numbers provided in main text for the ablation.",
            "experimental_setting": "Ablation experiments done on M2PT 10/20 across datasets.",
            "statistical_significance": null,
            "uuid": "e7415.5",
            "source_info": {
                "paper_title": "M^2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Prompt initialization",
            "name_full": "Prompt initialization method (Xavier vs Random)",
            "brief_description": "The initialization scheme for soft prompts affects final performance; authors compare Xavier initialization to random initialization and find Xavier yields more stable and better scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaVA + M2PT (10/20)",
            "model_description": "M2PT with different initialization strategies for prompt embedding vectors.",
            "model_size": "prompts are small additional parameters; underlying LLaVA uses Vicuna-7B",
            "task_name": "MME aggregate evaluation",
            "task_description": "Zero-shot multimodal benchmark aggregate score used to compare initialization strategies.",
            "problem_format": "Same prompt injection format; only initialization of learned prompt embeddings differs (Xavier vs random).",
            "format_category": "prompt hyperparameter (initialization)",
            "format_details": "Table S3 compares two initializations: Random and Xavier (Glorot & Bengio).",
            "performance_metric": "MME aggregate score",
            "performance_value": "M2PT 10/20 with Xavier initialization: MME = 1503.98; with Random initialization: MME = 1405.67.",
            "baseline_performance": "Random initialization baseline: MME = 1405.67",
            "performance_change": "Xavier vs Random: +98.31 absolute MME improvement (+7.0% relative).",
            "experimental_setting": "M2PT 10/20 configuration; all other training settings identical.",
            "statistical_significance": null,
            "uuid": "e7415.6",
            "source_info": {
                "paper_title": "M^2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Visual prompt tuning.",
            "rating": 2,
            "sanitized_title": "visual_prompt_tuning"
        },
        {
            "paper_title": "The power of scale for parameter-efficient prompt tuning.",
            "rating": 2,
            "sanitized_title": "the_power_of_scale_for_parameterefficient_prompt_tuning"
        },
        {
            "paper_title": "APrompt: Attention prompt tuning for efficient adaptation of pre-trained language models.",
            "rating": 2,
            "sanitized_title": "aprompt_attention_prompt_tuning_for_efficient_adaptation_of_pretrained_language_models"
        },
        {
            "paper_title": "VPT: Visual Prompt Tuning (E2VPT / VPT references in text).",
            "rating": 2,
            "sanitized_title": "vpt_visual_prompt_tuning_e2vpt_vpt_references_in_text"
        },
        {
            "paper_title": "PromptFuse: Modular and parameter-efficient multimodal fusion with prompting.",
            "rating": 1,
            "sanitized_title": "promptfuse_modular_and_parameterefficient_multimodal_fusion_with_prompting"
        }
    ],
    "cost": 0.01677425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>M 2 PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning
30 Oct 2024</p>
<p>Taowen Wang 
Rochester Institute of Technology</p>
<p>Yiyang Liu 
Rochester Institute of Technology</p>
<p>James Chenhao Liang 
Rochester Institute of Technology</p>
<p>Junhan Zhao 
Harvard Medical School
3 ByteDance, 4 Meta AI, 5 Meituan</p>
<p>Yiming Cui 
Yuning Mao 
Shaoliang Nie 
Jiahao Liu 
Fuli Feng 
University of Science and Technology of China</p>
<p>Zenglin Xu 
Shanghai Academy of AI for Science</p>
<p>Fudan University</p>
<p>Cheng Han 
University of Missouri -Kansas City</p>
<p>Lifu Huang 
Naval Research Laboratory
University of California -Davis 11 U.S</p>
<p>Qifan Wang 
Dongfang Liu dongfang.liu@rit.edu 
Rochester Institute of Technology</p>
<p>M 2 PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning
30 Oct 2024E44E51FEC395154BF252194EA0E57998arXiv:2409.15657v4[cs.AI]
Multimodal Large Language Models (MLLMs) demonstrate remarkable performance across a wide range of domains, with increasing emphasis on enhancing their zero-shot generalization capabilities for unseen tasks across various modalities.Instruction tuning has emerged as an effective strategy for achieving zeroshot generalization by finetuning pretrained models on diverse multimodal tasks.As the scale of MLLMs continues to grow, parameterefficient finetuning becomes increasingly critical.However, most existing parameter-efficient approaches focus only on single modalities and often overlook the multimodal characteristics during finetuning.In this work, we introduce a novel Multimodal Prompt Tuning (M 2 PT) approach for efficient instruction tuning of MLLMs.M 2 PT effectively integrates visual and textual prompts into the vision encoder and language processor respectively during finetuning, facilitating the extraction and alignment of features across modalities.Empirical results on various multimodal evaluation datasets demonstrate the superior performance of our approach compared to several state-of-the-art baselines.A comprehensive set of ablation studies validates the effectiveness of our prompt design and the efficiency of our approach.</p>
<p>Introduction</p>
<p>Human cognition intricately integrates various sensory modalities to perceive, interpret, and engage with the environment, fostering a comprehensive understanding of the surrounding world (Liu et al., 2023c;Dumas et al., 2009;Chen et al., 2024;Jin et al., 2024a,c).The development of Multimodal Large Language Models (MLLMs) (Alayrac et al., 2022;Yin et al., 2023;Han et al., 2024a,c;Jin et al., 2024b,d) (Hu et al., 2022), PTUM (Yang et al., 2023a) and VPT (Han et al., 2024b), on multimodal tasks.Our approach exhibits superior performance across a range of benchmarks.</p>
<p>bridging the gap between human and machine intelligence.A key focus in advancing MLLMs is enhancing their zero-shot generalization to new multimodal tasks.In this pursuit, multimodal instruction tuning (Liu et al., 2023c,b;Xu et al., 2023), which finetunes pretrained models using diverse and instruction-based multimodal tasks, has proven effective in improving zero-shot generalization to unseen multimodal domains.</p>
<p>Despite considerable advancements, finetuning MLLMs for specific domain knowledge poses significant challenges.As the scale and complexity of these models increase, the training overhead for downstream tasks grows exponentially (Wu et al., 2024a;Xu et al., 2024a;Zhang et al., 2024).These escalating demands render the current tuning schemes for MLLMs obsolete and unsustainable, impeding their widespread utility.A promising solution is the utilization of parameter-efficient finetuning (PEFT) approaches (Lester et al., 2021;Hu et al., 2022;Chowdhury et al., 2023;Dong et al., 2023b;Wang et al., 2023a), which have been widely applied and achieved notable success in natural language processing (Liu et al., 2021;Wang et al., 2022) and computer vision tasks (Jia et al., 2022;Han et al., 2023;Ju et al., 2022a).However, most existing PEFT approaches only focus on single modality tuning while overlooking multimodal instructing learning.A primary challenge is the intricate process of finetuning data of multiple modalities within a cohesive model, extracting and aligning feature representations across modalities.Additionally, there is a pressing need to enhance the zero-shot generalization capabilities for unseen multimodal tasks while minimizing training costs.</p>
<p>In light of this, we present a novel Multimodal Prompt Tuning (M 2 PT) approach with efficient and effective MLLM adaptation for zero-shot instruction learning.M 2 PT demonstrates competitive performance across a wide spectrum of tasks (see Fig. 1) while tuning 0.09% of overall parameters.Specifically, we introduce two sets of soft prompts: visual prompts and textual prompts, which are prepended to the visual and instruction inputs, respectively.The learned embeddings of the visual prompts are projected into the embedding space of the textual prompts.The cross-modality interactions between the two sets of prompts are enforced during instruction tuning, facilitating the alignment and learning of the feature representation between the two modalities.In this way, M 2 PT provides explicit guidance and clear directives through instruction tuning, enabling models to understand context and reduce ambiguity in zero-shot settings.</p>
<p>To effectively assess our method, we conduct comprehensive experiments to evaluate its performance.In §4.2, we demonstrate that M 2 PT surpasses several state-of-the-art PEFT techniques while tuning only 0.09% of the trainable parameters.We further conduct activation analysis to show the effectiveness of the learned prompts during the attention computation.In §5, we provide various ablation analysis on the impact of model components, prompt length, prompt location, and data volumes in detail.Furthermore, we conduct case studies to better understand the advantage of M 2 PT and its limitations.We hope this work offers valuable insights into related fields.We summarize the main contributions as follows:</p>
<p>• We present multimodal prompt tuning by introducing both visual and textual prompts into vision encoder and language processor respec-tively.These modality specific prompts play a crucial role in effectively guiding the model's fine-tuning process, enabling fast and accurate multimodal model adaptation.</p>
<p>• We design the cross-modality interaction between the prompts from different modalities during the instruction tuning.By doing so, M 2 PT leverages the strengths of each modality, resulting in comprehensive and coherent learning results.This synergy empowers the model to perform complex tasks that require the integration from multimodal perspectives.</p>
<p>• We conduct comprehensive experiments on various multimodal tasks in the zero-shot setting, demonstrating the effectiveness of the proposed approach over several state-of-theart parameter efficient finetuning methods.</p>
<p>Related Work</p>
<p>Multimodal Large Language Models.MLLMs (Dai et al., 2023;Driess et al., 2023;Liu et al., 2023c;Yao et al., 2023;Sun et al., 2024a) integrate multimodal information (e.g., audio, image, video), extending beyond the textual semantic information processed by conventional Large Language Models (LLMs).A general structure of MLLMs includes three main components (Li et al., 2024a): a pretrained modality encoder to encode multimodality data, a pre-trained LLM to reason encoded multimodal data and perform generation tasks, and a connection layer to project modality information into tokens.During the standard full finetuning process (Liu et al., 2023c), a substantial amount of weights within all intermediate layers and the pretrained LLM are continuously updated.Conceptually different, our approach can elegantly fine-tune the model with adjusting a minimum of weights.</p>
<p>Instruction Tuning.To enhance the zero-shot and in-context learning (Brown et al., 2020;Li et al., 2024b) capabilities of large language models (LLMs) (Zhao et al., 2023), researchers have explored instruction tuning (Ouyang et al., 2022;Zhang et al., 2023), a technique that enables pre-trained LLMs to be more adaptable for intricate multimodal tasks.Specifically, instructiontuning is a process that refines LLMs by finetuning them on meticulously curated instruction-following datasets encapsulating user intent and desired outputs (Ouyang et al., 2022).With the rapid advancement of multimodal models, instruction tuning has emerged not only as a state-of-the-art approach for natural language processing (NLP) tasks but also naturally extend to vision-related tasks such as image captioning (He et al., 2020;Cornia et al., 2020), image classification (Radford et al., 2021), visual question answering (VQA) (Antol et al., 2015).</p>
<p>Parameter-Efficient Finetuning.With the drastic growth in the scale of AI models, especially MLLMs and LLMs, Parameter-efficient Finetuning (PEFT) (Hu et al., 2022;He et al., 2023;Jie et al., 2023;Yan et al., 2023) have shown its capability to efficiently adapt pre-trained models to diverse downstream tasks without updating significant amount of model parameters.Generally, current PEFT strategies can be categorized into partial tuning (Chen et al., 2021;Jia et al., 2021), extra module such as Low-Rank Adaptation (LoRA) (Jie et al., 2023) and prompt tuning (Jia et al., 2022;Ju et al., 2022b;Dong et al., 2023a).However, partial tuning faces limitations, including unsatisfactory performance relative to full finetuning (Jia et al., 2022;Chen et al., 2021;Jia et al., 2021).Extra module exhibits limited adaptability when considering various model backbones.In contrast, prompt tuning (Lester et al., 2021;Ma et al., 2022;He et al., 2022a;Liu et al., 2023d;Qiu et al., 2020) provides a general and straightforward solution with powerful performance gains.It introduces a set of learnable parameters to the input sequence of backbone models, updating only these parameters during finetuning.Despite its simplicity, the applicability of prompt tuning within the multimodal paradigm remains largely unexplored.Unlike approaches such as MaPLe (Khattak et al., 2023), CoOp (Zhou et al., 2022b), CoCoOp (Zhou et al., 2022a) and MoPE (Wu et al., 2024b), which focus on crafting CLIP-based prompts for classification tasks, our work targets enhancing the capabilities of MLLMs in zero-shot instruction-following scenarios, resulting in a fundamentally distinct method design.Furthermore, PromptFuse (Liang et al., 2022) only introduces learnable prompts into textual modality, neglecting the synergy of multimodality.Our approach offers flexibility in prompt design, allowing prompts to be independently tailored for each modality and inserted at various layers.This flexibility significantly enhances the MLLMs' adaptability while reducing the number of parameters, improving performance across various datasets.</p>
<p>Methodology</p>
<p>Preliminaries</p>
<p>Multimodal Large Language Models integrate visual and language processing capabilities, leveraging the power of LLMs to enhance the comprehension of multimodal information.A prevalent workflow of MLLMs begins with the utilization of pre-trained vision encoders f v (e.g., LLaVA (Liu et al., 2023c) and its variants (Li et al., 2022;Sun et al., 2023)), encoding visual input X im and extracting output
O v = f v (X im )y = f llm (O v , O t ). (1)
Prompt Tuning is a form of PEFT approach, demonstrating exceptional efficacy within singlemodality settings under both visual (Han et al., 2023) and textual (Wang et al., 2023a) domains.It entails learnable continuous soft prompts into the input space while concurrently preserving the majority of parameters within the backbone frozen.Specifically, given a K layer transformer-based model f , soft prompts P k combined with the input of k-th layer to obtain the output O k as:
O 1 = f 1 (P 1 , E) O k = f k (P k , O k−1 ),(2)
where k ∈ {2, 3, . . ., K}. • and • indicate frozen and tunable parameter during finetuning, respectively.E is the embedded vector of initial inputs.</p>
<p>Multimodal Prompt Tuning</p>
<p>In this section, we formally introduce M 2 PT, a novel multimodal prompt tuning approach for the effective and efficient finetuning of MLLMs.The overall model architecture is depicted in Figure 2. Fundamentally, our model necessitates the training of only three targeted components, while keeping the backbone parameters from both visual encoder and LLM frozen.Specifically, these components include ① Visual Prompt (P v ), which is the learnable parameter (i.e., soft prompt) integrated into the visual encoder; ② Textual Prompt (P t ) is incorporated into the LLM in order to capture the nuanced semantic relationships across modalities; ③ Cross-modality Interaction, where parameters are learned to enhance the alignment between features extracted by the vision encoder and textual representations.In summary, prompts from these distinct modalities (i.e., ①-②) facilitate the model's acquisition of knowledge from multimodal finetuning datasets, capturing the distinctive characteristics inherent in each modality and fostering cross-modal interaction.Visual Prompt.We denote the set of visual prompts as
P v = {P 1 v , P i v , • • • , P N v }, where P i v
indicates the set of visual prompts in the i-th layer of the visual encoder, consistent with previous practice for prompt tuning (Jia et al., 2022;Han et al., 2023).Each prompt is a d v -dimensional vector with the same dimensions as the original vision tokens.Prompts in each layer are placed at the leftmost positions within the sequence to interact with other vision tokens.Formally, we have:
O 1 v = f 1 v (P 1 v , X im ) O i v = f i v (P i v , O i−1 ),(3)
where i ∈ {2, 3, . . ., N }, and O i v is the i-th layer vision embedding.Textual Prompt.Visual prompts serve as an effective tool for capturing semantic content within the visual inputs, as well as gaining the ability to interact with the text modality through the mapping from visual domain.Nevertheless, the optimization of visual elements does not directly affect the inherent representation of LLMs in text modality.Naturally, to further enhance the text modality's processing ability, we introduce the textual prompts to capture text patterns and influence the inner representation within the LLM.Specifically, textual prompts are denoted as
P t = {P 1 t , P j t , • • • , P M t }
, where j indicates the prompt inject position of the j-th layer in a total M layers LLM.Each prompt is a d t -dimensional vector with the same dimensionality as the original text tokens.Formally, we incorporate textual prompts into the LLM as:
O 1 m = f 1 llm (P 1 t , O ′ v , O t ) O j m = f j llm (P j t , O j−1 m ) y = f head (O j m ),(4)
where j ∈ {2, 3, . . ., M }, y is the textual output of the MLLM, O t is the textual embedding, and f head is the task-specific head in order to decode the embeddings into texts.</p>
<p>Cross-modality Interaction.To achieve alignment between visual and textual modality, we introduce a tunable interaction layer f in , which is specifically designed to align the output O N v produced by the visual encoder and the textual embedding, through a linear projection:
O ′ v = f in (O N v ).(5)
Here O ′ v represents the aligned vision embedding.This transformation ensures that the visual encoder's output is effectively mapped onto a common textual representation space.The projected visual tokens then interact with the textual tokens through all LLM layers, facilitating the integration of visual and textual information.Our elegant design of M 2 PT enjoys a few appealing characteristics:</p>
<p>• Cross-modal Integration.Our M 2 PT model employs a unified prompt tuning paradigm.This approach not only captures the distinctive attributes of each modality but also facilitates the fluid exchange of cross-modal information, enhancing the model's capability to comprehend and generate multimodal data effectively.</p>
<p>• Optimized Parameter Utilization.M 2 PT demonstrates superior parameter efficiency by focusing only on the training of a minimal set of parameters while keeping the majority of the model's parameters frozen, allowing a significant reduction in the number of parameters required (0.09%).Despite this reduction, M 2 PT maintains superior performance on multimodal tasks (see Table 1) with a balance between computational efficiency and overall effectiveness in zero-shot setting.</p>
<p>Implementation Details</p>
<p>We employ LLaVA (Liu et al., 2023c) (Shen et al., 2024) and employ a scaled-down version containing up to 1, 000 instances per task, resulting in a total of 191, 105 instances.For zeroshot evaluation, we examine our approach on the comprehensive multimodal evaluation benchmark MME (Fu et al., 2023), measuring both perception and cognition abilities across 14 subtasks.</p>
<p>We further evaluate the model's capabilities using 7 multimodal datasets.Specifically, for Optical Character Recognition, we utilize the Text-VQA (Singh et al., 2019), and for reasoning, we employ the Visual Spatial Reasoning (VSR) (Liu et al., 2023a).Following (Zhai et al., 2023;Shen et al., 2024), the perception capability is tested on CIFAR-10/100 (Krizhevsky et al., 2009) and MNIST (Deng, 2012).SNLI-VE (Xie et al., 2019) evaluates Visual Entailment capabilities, while the POPE (Li et al., 2023) dataset examines the tendency towards object hallucination.More details are provided in the Appendix S1.</p>
<p>Training Details.Following previous works (Han et al., 2023;Shen et al., 2024;Jia et al., 2022), we conduct grid search to match the best tuning hyperparameters, learning rate (i.e., [1e −3 , 9e −4 , 7e −4 , 4e −4 , 1e −4 , 5e −5 ]), textual prompt length (i.e., [0,5,10,20,40]) and visual prompt length (i.e., [0,5,10,20,40]).For all models, the learning rate is scheduled following a cosine decay policy, the warm up ratio is set at 0.03 and trained for 3 epochs except in training epoch experiment.We follow the same batch size setting in (Shen et al., 2024) as 128 for instruction tun- ing.Further details are provided in the Appendix S2.M 2 PT is implemented in Pytorch (Paszke et al., 2019).Experiments are conducted on 8 NVIDIA A100 GPUs.Our code is available at https://github.com/William-wAng618/M2PT. Evaluation Metrics.The MME incorporates both Perception and Cognition metrics1 .For other multimodal datasets, we use Vicuna-13B-v1.5 (Zheng et al., 2023) to assess the accuracy of each prediction compared to the groundtruth.Further details are provided in the Appendix S3.</p>
<p>Main Result</p>
<p>In Table 1, our main result exhibits a comprehensive zero-shot evaluation of M 2 PT with several baselines on eight multimodal datasets.Specifically, we consider four state-of-the-art PEFT approaches, including LoRA (Hu et al., 2022), APrompt (Wang et al., 2023a), PTUM (Yang et al., 2023a) and VPT (Han et al., 2024b).Full finetuned LLaVA (i.e., LLaVA F T (Liu et al., 2023c)) serves as an upper-bound of multimodal evaluation.</p>
<p>We report the performance of M 2 PT under two different settings, with M 2 PT 10/10 and M 2 PT 10/20 .There are several key observations from these results.First, M 2 PT achieves the best performance among all PEFT approaches in most cases, demonstrating the effective design of visual and textual prompts.For example, on MME task, M 2 PT demonstrates a significant improvement of 6.90% and 7.51% compared to two strong prompt tuning methods, Aprompt and VPT, respectively.This highlights the limitation of existing prompt tuning approaches that primarily focus on single modality, failing to capture the cross-modality interactions.</p>
<p>In contrast, the interaction layer together with the multimodal LLM employed by our approach successfully bridges this gap, resulting in enhanced performance.Second, the performance of M 2 PT reaches 94.75% of the full finetuning performance while considering only 0.09% of the total model parameters, demonstrating the parameter efficiency of M 2 PT.Moreover, we observe that M 2 PT outperforms the full finetuning LLaVA on VSR, MNIST and POPE tasks, showing its strong capability in zero/few shot learning.This is consistent with the observations in several previous works (Han et al., 2023;Yang et al., 2023b).Third, it can be seen that M 2 PT does not perform very well on the visual entailment task, SNLI-VE.Our hypothesis is that logical relationships or causal scenario understanding is critical in this task, which might not be fully captured by prompt tuning based approaches.</p>
<p>Analysis and Discussion</p>
<p>Attention Activation Pattern Analysis.Following common practice (Sun et al., 2024b), we extract and discuss the activation maps from the attention block of MLLMs, and investigate the influence of visual and textual prompts in Fig. 3.We randomly select two samples from the MME dataset and visualize their corresponding activation maps of the attention block in the last layers of both visual encoder (Fig. 3(a)) and LLM (Fig. 3(b)).To analyze the impact of textual and visual prompts to the frozen components, we categorize them, according to LLaVa's model structure, into textual prompts, system text tokens, visual prompts, image tokens and instructions.Several findings can be observed.First, in the LLM attention activation map, we observe that the token regions corresponding to textual prompts exhibit elevated activation levels, indicating their significant role in shaping the model's responses.The activation levels of visual prompts within the LLM, while comparatively lower, remain notable relative to most other regions.This suggests a secondary yet substantial role in multimodal inference.Second, in the activation maps from Visual Encoder, the activation levels associated with visual prompts are noticeably higher than those of most other tokens, underscoring the critical role of visual prompts in processing visual information during tuning.These observations support that visual prompts effectively interact with the textual prompts, enhancing the alignment between modalities and thereby improving the model's performance on the zero-shot instruction learning.Our component analysis study below further strengthens this claim quantitatively.</p>
<p>Impact of Different Components.To investigate the impact of different components in M 2 PT across various datasets (i.e., visual prompts, textual prompts, and interaction layer), we conducted component ablation experiments in Fig. 4 by removing each component at a time from M 2 PT 10/20 .The results demonstrate that the model performance drops when any of the trainable prompts is removed, which aligns with our expectations.Moreover, the model performance also decreases without the trainable interaction layer, indicating the importance of this layer.Furthermore, we observe that the importance of different components varies across the tasks.For example, textual prompts play the most important role in CIFAR-10 and POPE, while visual prompts lift the performance most on MME.Once again, it is worth noting that combining the multimodal prompts with the interaction layer in M 2 PT leads to the best performance.</p>
<p>Impact of Prompt Length.In M 2 PT, prompt length is the only hyperparameter that needs to be tuned.To further analyze the impact of different prompt lengths on model performance, we conduct a comprehensive study on the lengths of visual and textual prompts on the Vision-Flan dataset to better understand their properties.Following common practice (Han et al., 2023;Jia et al., 2022;Han et al., 2024b), we use the grid search to span   a range from 5 to 40 on both visual and textual prompt lengths, reported in Fig. 5.We stop at a prompt length of 40 because performance saturation is observed around this point.Thus, extending the prompt length further would result in increased parameter usage without significant performance gains (i.e., M 2 PT shows a slight decrease in performance on MME.We argue that this may be due to overparameterization (Han et al., 2023;Jia et al., 2022)).When visual prompt length extends from 5 to 20, and textual prompt length extends from 5 to 10, noticeable performance gains can be observed.It can be seen that there is no universal optimal prompt length that consistently achieves the best performance across different tasks.For instance, the optimal performance of the model on MME is achieved with a configuration of 20 visual prompts and 10 textual prompts, while 10 visual prompts and 40 textual prompts achieve the highest performance on POPE.We hypothesize that different tasks exhibit distinct data distributions, with 'difficult' tasks potentially requiring longer prompts to effectively capture the underlying patterns.Nonetheless, we observed that the performance of M 2 PT remains relatively stable within a certain range of prompt lengths.</p>
<p>Impact of Prompt Location.Following (Jia et al., 2022), Table 2 studies the insertion of prompts at different layers.We design five distinct settings in which prompts are integrated into both the Vision Encoder and LLM but at different locations.Specifically, we introduce prompts into: Each variant reports the best prompt length combination selected with MME evaluation.Generally, M 2 PT's performance is positively correlated with prompt depth.Yet the accuracy drops when inserting prompts from top to bottom, suggesting that prompts at earlier layers matter more than those at latter layers, which is consistent with the observations in (Jia et al., 2022;Wang et al., 2023a).Effect of Data Volume and Training Epoch.In Fig. 6, we randomly sample data from Vision-Flan at different scales (i.e., 25%, 50%, 75%, 100%) to evaluate the performance of the model with limited data.The results demonstrate that M 2 PT maintains excellent performance despite significant data reduction, highlighting efficiency and robustness regarding data quantity.This property indicates that M 2 PT exhibits substantial tolerance to data scale, suggesting a promising future for real-world applications with constrained data.</p>
<p>In Fig. 7, we analyze the M 2 PT's performance on different training epochs.This experiment is Case Study.In Fig. 8, we present cases where M 2 PT demonstrates success on the MME and CIFAR-10, while other approaches fail (i.e., VPT, PTUM).For example, on MME, while M 2 PT correctly identifies "a chair" in the image, both VPT and PTUM fail to capture this concept and respond with the wrong answer.On CIFAR-10, VPT and PTUM both misidentify "cat" as a different category.We posit that the insufficiency of VPT may stem from its inadequate understanding of logical relationships or causal scenarios.PTUM employs single-modality tuning exclusively, rendering it inadequate for managing complex multi-modal behavior and capturing interactions between modalities.In sharp contrast, M 2 PT takes an aspect of multimodal, enhancing both visual modality comprehension and textual modality causal inference.</p>
<p>We further conduct failure case studies for M 2 PT on CIFAR-10 dataset.The results in Fig. 9 indicate that the predominant misclassification within the CIFAR-10 (i.e., 44% or 443 examples of all such misclassification) is the misidentification of "ships" as "boats".This high error rate could potentially be attributed to the limited resolution of the images within the CIFAR-10, coupled with the inherent similarity in the semantic characteristics between "ship" and "boat".Additional case studies and discussions are provided in Appendix S4.</p>
<p>Conclusion</p>
<p>We introduce M 2 PT, a novel framework in multimodal prompt tuning for zero-shot instruction learning.Our framework offers several advantages: i) it introduces visual and textual prompts elegantly into vision encoder and LLM, respectively, enabling fast and accurate multimodal adaptation; ii) it synergizes modalities by cross-modality interaction, enjoying coherent integration from multimodal perspectives; and iii) it significantly reduces the number of trainable parameters compared to conventional finetuning methods, while maintaining robust performance across zero-shot tasks.As a whole, we conclude that the outcomes elucidated in this paper impart essential understandings and necessitate further exploration within this realm.</p>
<p>Limitations</p>
<p>For potential limitations, M 2 PT requires two hyperparameters on prompt length searching (i.e., Visual Prompt, Textual Prompt).Though in practice, we find both lengths vary into a relatively narrow range (see §5), and are sufficient enough to outperform all current methods, there is still possible integration (He et al., 2022b) of a local searching network to generate optimal combinations of lengths.Another potential limitation is that M 2 PT, akin to other PEFT approaches (Han et al., 2023;Jia et al., 2022), lacks ad-hoc explainability (Biehl et al., 2016;Wang et al., 2023b).While in §4, we demonstrates that activation maps from MLLMs are significantly influenced by visual and textual prompts, further research is necessary to elucidate the underlying nature of these prompts.</p>
<p>SUMMARY OF THE APPENDIX</p>
<p>This appendix contains additional experimental results and discussions of our work, organized as:</p>
<p>• §S1 includes additional introduction on datasets applied in our paper.</p>
<p>• §S2 provides more training details on our proposed M 2 PT.</p>
<p>• §S3 presents more details on the evaluation metrics applied in §4.</p>
<p>• §S4 further includes more case studies of M 2 PT on its perceptual proficiency.</p>
<p>• §S5 discuss the relations of M 2 PT with previous works and their connections.</p>
<p>• §S6 provides discussions and additional results on the impact of prompt initialization.</p>
<p>• §S7 provides discussions on licenses, reproducibility, social impact, and directions of our future work.</p>
<p>S1 Data Statistics</p>
<p>Table S1 shows details of 9 multimodal datasets for our finetuning and evaluation.Vision-Flan (Xu et al., 2024b) includes 191 different multimodal tasks which is ideal for our finetuning process.</p>
<p>Each multimodal tasks contains up to 1,000 instances, resulting in a total of 191,105 instances.MME (Yin et al., 2023) serves as our comprehensive multimodal evaluation benchmark, measuring both perception and cognition capabilities across 14 subtasks.We further leverage 7 multimodal datasets for our evaluation.Specifically, for Optical Character Recognition, we utilize the Text-VQA (Singh et al., 2019), and for reasoning, we employ the Visual Spatial Reasoning (VSR) (Liu et al., 2023a).Following (Zhai et al., 2023;Shen et al., 2024), the perception capability is tested on CIFAR-10/100 (Krizhevsky et al., 2009) and MNIST (Deng, 2012).SNLI-VE (Xie et al., 2019) evaluates Visual Entailment capabilities, while the POPE (Li et al., 2023) dataset examines the tendency towards object hallucination.The MME metric is the sum of accuracy values across all subtasks, while for the other 7 multimodal evaluation datasets, the metric used is just accuracy.</p>
<p>S2 Implementation Details</p>
<p>Stage-one LLaVA (Liu et al., 2023c) is utilized as our pre-trained multimodal model.Specifically, we  S2.For LoRA, we directly import the best results from (Shen et al., 2024).For the prompt tuning baselines, APrompt (Wang et al., 2023a) and PTUM (Yang et al., 2023a) add textual/attention prompts into the LLM, while VPT (Han et al., 2024b) only appends visual prompts to the vision encoder.We use the optimal settings in the original papers to train their models, with grid search on the best learning rate.For the other configuration, we adopt LLaVA's (Liu et al., 2023c) default settings as provided in its codebase.</p>
<p>S3 Evaluation Metrics</p>
<p>For evluation, we utilize MME (Yin et al., 2023) and the other 7 multimodal datasets (see §S1).For MME, we employ the official evaluation tool (Yin et al., 2023) of MME, including the Perception and Cognition metrics.For the other 7 multimodal datasets, following (Shen et al., 2024), we employ the same prompt template to guide Vicuna-13B-v1.5 (Zheng et al., 2023) in evaluating the accuracy of each prediction, considering the specified task instructions and the groundtruth target output.All tasks are classification tasks and we calculated the final score of each multimodal dataset based on the percentage of vicuna 13b answering "Yes."</p>
<p>S4 More Case Study</p>
<p>To further investigate the model's performance and delineate instances of its suboptimal functioning, we conduct an in-depth visual assessment of a sample cohort drawn from eight distinct Zero-Shot datasets, as illustrated in Fig. S1.This visualization facilitates a comprehensive understanding of the model's efficacy across a diverse array of tasks and data, while concurrently revealing potential constraints inherent to the model and the underlying causes of its occasional shortcomings.From the listed failure cases, we summarize 2 failure patterns, which are: (a) Small objects perception failure in Text-VQA, CIFAR-10, MNIST.Small targets in images pose a challenge to perception, impacting the quality of VQA.Further research is essential to enhance accuracy in diverse contexts.(b) Semantic similar failure in CIFAR-10, MNIST, POPE, the inability to distinguish between semantically similar objects results in MLLMs generating wrong answers.Developing methods that can effectively differentiate between similar objects is essential for real-world applications.This involves enhancing the model's capacity to learn fine-grained features and contextual information, thereby improving its overall accuracy and robustness.</p>
<p>S5 Discussion with Previous Works</p>
<p>This section provides discussion that connects M 2 PT with previous methods.If we remove the textual prompts and the interaction layer, our model architecture degenerates to the visual prompt tuning approaches (Jia et al., 2022;Han et al., 2024bHan et al., , 2023)).If we completely freeze the vision encoder by only introducing the textual prompts, our model is similar to those traditional prompt tuning methods (Lester et al., 2021;Ma et al., 2022;Yang et al., 2023a) in LLM.Moreover, if we further incorporate the attention prompts in the LLM, our model is close to the APrompt approach (Wang et al., 2023a;Han et al., 2023).Nevertheless, very limited work focuses on the efficient tuning of multimodal large language models.</p>
<p>S6 Prompt Initialization</p>
<p>Table S3 reports the performance of M 2 PT with respect to x widely adopted initialization methods: Xavier (Glorot and Bengio, 2010) and random on MME.The results show that Xavier generally provides more stable and preferable performances.In conclusion, M 2 PT shows robustness on different initialization methods and is able to achieve comparable performance with full finetuning.</p>
<p>S7 Discussion</p>
<p>S7.1 Asset License and Consent</p>
<p>The majority of VPT (Jia et al., 2022), Text-VQA, is licensed under CC-BY-NC 4.0.Portions of (Jia et al., 2022) are available under separate license terms: google-research/task_adaptation, huggingface/transformers, LLaVA, Vicuna, VSR is licensed under Apache-2.0.ViT-pytorch (Dosovitskiy et al., 2021) and POPE is licensed under MIT; SNLI-VE are under BSD 3-Clause</p>
<p>S7.2 Reproducibility</p>
<p>M 2 PT is implemented in Pytorch (Paszke et al., 2019).Experiments are conducted on NVIDIA A100 GPUs.To guarantee reproducibility, our full implementation shall be publicly released upon paper acceptance.</p>
<p>S7.3 Social Impact</p>
<p>This work introduces M 2 PT possessing strong performance gains over state-of-the-art baselines in §4, with considerably low parameter usage for MLLMs.Our approach advances model accuracy, and is valuable in parameter-sensitive training applications, e.g., MLLMs on devices and fast adaptation with limited resources.</p>
<p>S7.4 Potential Risks</p>
<p>Consider the tuning process of LLM, which has potential risks for energy usage.Finetuning requires significant computational power, leading to high energy use and increased environmental impact.</p>
<p>S7.5 Future Work</p>
<p>Despite M 2 PT's systemic effectiveness and efficacy during instruction tuning, it also comes with new challenges and unveils some intriguing questions.For instance, incorporating an advanced network into M 2 PT to search the optimal combinations of prompt lengths (i.e., Visual Prompt, Tex-tual Prompt) might significantly reduce the search space of lengths and lead to further performance gains.Another essential future direction is the design and analysis of network interpretability (Arrieta et al., 2020;Laugel et al., 2019;Rudin, 2019) and ad-hoc explainability (Biehl et al., 2016;Wang et al., 2023b), which limits current adoption of M 2 PT in decision-critical, real-world applications.Overall, we believe the results presented in this paper warrant further exploration.</p>
<p>Figure 1 :
1
Figure1: Comparison of M 2 PT and several PEFT methods, including LoRA(Hu et al., 2022), PTUM(Yang et al., 2023a) and VPT(Han et al., 2024b), on multimodal tasks.Our approach exhibits superior performance across a range of benchmarks.</p>
<p>Figure 2 :
2
Figure2: Overview of our M 2 PT approach.Here, visual prompts are embedded into each layer of the Visual Encoder, and textual prompts are embedded into each layer of the LLM.These prompts facilitate the extraction and alignment of features across modalities (e.g., vision, language).The cross-modality interaction between visual and textual features is enhanced through layered integration, ultimately improving the model's capability in zero-shot instruction learning tasks (see §4).</p>
<p>Figure 3 :
3
Figure 3: Comprehensive visualization of attention activation maps.This figure presents a detailed examination of the activation patterns within the last layer of LLM and Visual Encoder, respectively.As seen, the vision prompts and textual prompts have noticeably high activation levels during inference (i.e., • and • represent textual prompts' activation signal and visual prompts' activation signal, respectively).</p>
<p>Figure 4 :
4
Figure 4: Impact of Different Components.</p>
<p>Figure 5 :
5
Figure 5: Performance of Different Prompt Length.Each cell in the map corresponds to the score of a model with a textual prompt length (row) and a visual prompt length.A darker hue indicates a higher score, whereas a lighter hue signifies a lower score.Table 2: Prompt Location Experiment (M 2 PT 10/20 ).</p>
<p>(a) the first layer; (b) every odd-number layer (i.e., [1, 3, . . ., 23] ∈ N , [1, 3, . . ., 31] ∈ M ); (c) the first half of the layers (i.e., [1-12] ∈ N , [1-16] ∈ M ); (d) the latter half of the layers (i.e., [12-24] ∈ N , [16-32] ∈ M ); and (e) all layers.</p>
<p>Figure 6 :
6
Figure 6: Data Volume Experiment (M 2 PT 10/20 ).</p>
<p>Figure 7 :
7
Figure 7: Training Epoch Experiment (M 2 PT 10/20 ).</p>
<p>Figure 8 :Figure 9 :
89
Figure 8: Winning cases on MME and CIFAR-10.</p>
<p>Figure S1 :
S1
Figure S1: More Case study on 8 Zero-Shot datasets (M 2 PT 10/20 ).</p>
<p>marks a significant advancement in emulating this capability, playing a pivotal role in
1503.9881.2679.60 76.161398.74 1393.67 1354.6235.64 34.28 33.6839.2072.3395.54 94.73 94.2983.4252.9554.66 53.93 53.7576.4945.9052.31 57.6382.8859.1444.5689.2990.10</p>
<p>Table 1 :
1
Zero-shot Multimodal Evaluation on all multimodal datasets.The MMAvg represents the average score on the right seven tasks.LLaVA Align is the stage-one LLaVA without end-to-end finetuning, and LLaVA F T indicates the fully fine-tuned LLaVA.All the fine-tuned processes are using the same Vision-Flan dataset.M 2 PT a/b means textual and visual prompt lengths 'a' and 'b', respectively.The best performance is in bold.
Method# paraMMEText-VQA VSR SNLI-VE CIFAR-10 CIFAR-100 MNIST POPE MMAvgLLaVA Align (Liu et al., 2023c)-1110.8232.6250.1634.5180.0058.0452.7959.1052.46LLaVA F T (Liu et al., 2023c)100%1587.2637.2653.7643.3592.9763.7394.2780.8266.59LoRA (Hu et al., 2022)0.63% 1393.6739.2052.9544.5690.1045.9083.4272.3361.21APrompt (Wang et al., 2023a) 0.23% 1406.6335.2653.1245.5885.7450.2784.6376.1661.52PTUM (Yang et al., 2023a)0.12% 1354.6234.2853.7530.8682.8857.6394.2980.3162.00VPT (Han et al., 2024b)0.06% 1398.7433.6853.9332.6276.4952.3194.7379.6060.48M 2 PT 10/10 (Ours)0.08% 1490.1735.6454.6632.5387.9257.8094.5381.2963.48M 2 PT 10/20 (Ours)0.09% 1503.9834.4853.1932.8989.2959.1495.5481.2663.68</p>
<p>Table 2 :
2
Prompt Location Experiment (M 2 PT 10/20 ).
Prompt LocationMMECIFAT-10 POPE(a) First Layer1320.9982.9679.48(b) Odd Layer1396.8787.6575.79(c) Top Half1473.4285.8280.31(d) Latter Half1249.3983.7279.34(e) All1503.9889.2981.26</p>
<p>Table S1 :
S1
Multimodal Dataset Details
DatasetExamplesTask CategoriesVision-Flan191KDiverseMME2374DiverseText-VQA5000OCRVSR1222ReasoningSNLI-VE17901EntailmentCIFAR-1010000PerceptionCIFAR-10010000PerceptionMNIST10000PerceptionPOPE9000Object Hallucinationemploy LLaVA with Vicuna-7B-v1.3 as the baseLLM and CLIP-L as the vision encoder for all vari-ants. The finetuning process for M 2 PT 10/20 takesapproximately 9 hours on 4 A100 GPUs (40G),with a batch size of 8 per GPU and a gradient ac-cumulation step of 4 (128 in total), with the samedata preprocess and normalize method as LLaVA.Additional configurations of M 2 PT are shown inTable</p>
<p>Table S2 :
S2
Hyperparameters and configurations.
Learning Rate7e −4Batch Size128Lr SchedulercosineWarmup Ratio0.03Activation Typebfloat16Weight Decay0Model Max Length1024</p>
<p>Table S3 :
S3
Initialization Comparison.We compare the performance of M 2 PT under different initialization methods in the setting of best prompt combination(i.e.,  10 and 20)
MethodInitializationMMEM 2 PT 10/20Random1405.67M 2 PT 10/20Xavier1503.98
https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation
AcknowledgementsThis research was supported by the National Science Foundation under Grant No. 2242243.The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of U.S. Naval Research Laboratory (NRL) or the U.S. Government.The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.
Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karén Simonyan, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2022. 2022. 2022. November 28 -December 9, 2022</p>
<p>VQA: visual question answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, Devi Parikh, 10.1109/ICCV.2015.2792015 IEEE International Conference on Computer Vision, ICCV 2015. Santiago, ChileIEEE Computer Society2015. December 7-13, 2015</p>
<p>Explainable artificial intelligence (XAI): concepts, taxonomies, opportunities and challenges toward responsible AI. Alejandro Barredo Arrieta, Natalia Díaz Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador García, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, Francisco Herrera, 10.1016/J.INFFUS.2019.12.012Inf. Fusion. 582020</p>
<p>Prototype-based models in machine learning. Michael Biehl, Barbara Hammer, Thomas Villmann, 10.1002/wcs.1378Wiley Interdisciplinary Reviews: Cognitive Science. 722016</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish; NeurIPS2020. 2020. 2020. December 6-12, 2020Language models are few-shot learners</p>
<p>Inertial confinement fusion forecasting via llms. Mingkai Chen, Taowen Wang, James Chenhao Liang, Chuan Liu, Chunshu Wu, Qifan Wang, Ying Nian Wu, Michael Huang, Chuang Ren, Ang Li, Tong Geng, Dongfang Liu, 10.48550/ARXIV.2407.11098CoRR, abs/2407.110982024</p>
<p>An empirical study of training self-supervised vision transformers. Xinlei Chen, Saining Xie, Kaiming He, 10.1109/ICCV48922.2021.009502021 IEEE/CVF International Conference on Computer Vision, ICCV 2021. Montreal, QC, CanadaIEEE2021. October 10-17, 2021</p>
<p>Apollo : Unified adapter and prompt learning for vision language models. Sanjoy Chowdhury, Sayan Nag, Dinesh Manocha, 10.18653/V1/2023.EMNLP-MAIN.629Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Meshed-memory transformer for image captioning. Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, Rita Cucchiara, 10.1109/CVPR42600.2020.010592020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020. Seattle, WA, USA2020. June 13-19, 2020Computer Vision Foundation / IEEE</p>
<p>Instructblip: Towards general-purpose visionlanguage models with instruction tuning. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven C H Hoi, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2023. 2023. 2023. December 10 -16, 2023</p>
<p>The MNIST database of handwritten digit images for machine learning research. Li Deng, 10.1109/MSP.2012.22114772012best of the web</p>
<p>. IEEE Signal Process. Mag. 296</p>
<p>Efficient multimodal semantic segmentation via dual-prompt learning. Shaohua Dong, Yunhe Feng, Qing Yang, Yan Huang, Dongfang Liu, Heng Fan, 10.48550/ARXIV.2312.00360CoRR, abs/2312.003602023a</p>
<p>Efficient adaptation of large vision transformer via adapter re-composing. Wei Dong, Dawei Yan, Zhijun Lin, Peng Wang, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023. NeurIPS; New Orleans, LA, USA2023b. 2023. December 10 -16, 2023</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. Austria2021. May 3-7, 2021OpenReview.net</p>
<p>Palm-e: An embodied multimodal language model. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Wenlong Yu, Yevgen Huang, Pierre Chebotar, Daniel Sermanet, Sergey Duckworth, Vincent Levine, Karol Vanhoucke, Marc Hausman, Klaus Toussaint, Andy Greff, Igor Zeng, Pete Mordatch, Florence, International Conference on Machine Learning, ICML 2023. Honolulu, Hawaii, USAPMLR2023. July 2023202</p>
<p>Multimodal interfaces: A survey of principles, models and frameworks. Bruno Dumas, Denis Lalanne, Sharon L Oviatt, 10.1007/978-3-642-00437-7_1Human Machine Interaction, Research Results of the MMI Program. Lecture Notes in Computer Science. Denis Lalanne, Jürg Kohlas, Springer20095440</p>
<p>MME: A comprehensive evaluation benchmark for multimodal large language models. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Rongrong Ji, 10.48550/ARXIV.2306.13394CoRR, abs/2306.133942023</p>
<p>Understanding the difficulty of training deep feedforward neural networks. Xavier Glorot, Yoshua Bengio, Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort. the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna ResortSardinia, Italy2010. May 13-15, 20109</p>
<p>Image translation as diffusion visual programmers. Cheng Han, James Chenhao Liang, Qifan Wang, Majid Rabbani, A Sohail, Raghuveer Dianat, Ying Nian Rao, Dongfang Wu, Liu, ICLR 2024The Twelfth International Conference on Learning Representations. Vienna, Austria2024a. May 7-11, 2024OpenReview.net</p>
<p>E 2 vpt: An effective and efficient approach for visual prompt tuning. Cheng Han, Qifan Wang, Yiming Cui, Zhiwen Cao, Wenguan Wang, Siyuan Qi, Dongfang Liu, 10.1109/ICCV51070.2023.01604IEEE/CVF International Conference on Computer Vision, ICCV 2023. Paris, FranceIEEE2023. October 1-6, 2023</p>
<p>Facing the elephant in the room: Visual prompt tuning or full finetuning?. Cheng Han, Qifan Wang, Yiming Cui, Wenguan Wang, Lifu Huang, Siyuan Qi, Dongfang Liu, 10.48550/ARXIV.2401.12902CoRR, abs/2401.129022024b</p>
<p>AMD: automatic multi-step distillation of large-scale vision models. Cheng Han, Qifan Wang, Sohail A Dianat, Majid Rabbani, M Raghuveer, Yi Rao, Qiang Fang, Lifu Guan, Dongfang Huang, Liu, 10.48550/ARXIV.2407.04208CoRR, abs/2407.042082024c</p>
<p>Image captioning through image transformer. Sen He, Wentong Liao, Hamed R Tavakoli, Ying Michael, Bodo Yang, Nicolas Rosenhahn, Pugeault, 10.1007/978-3-030-69538-5_10Computer Vision -ACCV 2020 -15th. 2020</p>
<p>Asian Conference on Computer Vision. Lecture Notes in Computer Science. Kyoto, JapanSpringerNovember 30 -December 4, 202012625Revised Selected Papers, Part IV</p>
<p>Parameter-efficient model adaptation for vision transformers. Xuehai He, Chunyuan Li, Pengchuan Zhang, Jianwei Yang, Xin Eric, Wang , 10.1609/AAAI.V37I1.25160Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence. Washington, DC, USAAAAI Press2023. February 7-14, 20232023Thirteenth Symposium on Educational Advances in Artificial Intelligence</p>
<p>Hyperprompt: Prompt-based task-conditioning of transformers. Yun He, Steven Huaixiu, Yi Zheng, Jai Prakash Tay, Yu Gupta, Vamsi Du, Zhe Aribandi, Yaguang Zhao, Zhao Li, Donald Chen, Heng-Tze Metzler, Ed H Cheng, Chi, International Conference on Machine Learning, ICML 2022. Baltimore, Maryland, USAPMLR2022a. 17-23 July 2022162of Proceedings of Machine Learning Research</p>
<p>Hyperprompt: Prompt-based task-conditioning of transformers. Yun He, Steven Huaixiu, Yi Zheng, Jai Prakash Tay, Yu Gupta, Vamsi Du, Zhe Aribandi, Yaguang Zhao, Zhao Li, Donald Chen, Heng-Tze Metzler, Ed H Cheng, Chi, International Conference on Machine Learning, ICML 2022. Baltimore, Maryland, USAPMLR2022b. 17-23 July 2022162of Proceedings of Machine Learning Research</p>
<p>Lora: Low-rank adaptation of large language models. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. 2022. April 25-29, 2022OpenReview.net</p>
<p>Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge J Belongie, Bharath Hariharan, Ser-Nam Lim, 10.48550/ARXIV.2203.12119CoRR, abs/2203.12119Visual prompt tuning. 2022</p>
<p>Exploring visual engagement signals for representation learning. Menglin Jia, Zuxuan Wu, Austin Reiter, Claire Cardie, Serge J Belongie, Ser-Nam Lim, 10.1109/ICCV48922.2021.004172021 IEEE/CVF International Conference on Computer Vision, ICCV 2021. Montreal, QC, CanadaIEEE2021. October 10-17, 2021</p>
<p>Revisiting the parameter efficiency of adapters from the perspective of precision redundancy. Shibo Jie, Haoqing Wang, Zhi-Hong Deng, 10.1109/ICCV51070.2023.01579IEEE/CVF International Conference on Computer Vision, ICCV 2023. Paris, FranceIEEE2023. October 1-6, 2023</p>
<p>ProLLM: Protein chainof-thoughts enhanced LLM for protein-protein interaction prediction. Mingyu Jin, Haochen Xue, Zhenting Wang, Boming Kang, Ruosong Ye, Kaixiong Zhou, Mengnan Du, Yongfeng Zhang ; Mingyu Jin, Qinkai Yu, Jingyuan Huang, Qingcheng Zeng, Zhenting Wang, Wenyue Hua, Haiyan Zhao, Kai Mei, 10.48550/ARXIV.2404.07066CoRR, abs/2404.07066Yanda Meng, Kaize Ding, Fan Yang, Mengnan Du, and Yongfeng Zhang. 2024b. Exploring concept depth: How large language models acquire. 2024aFirst Conference on Language Modeling</p>
<p>Health-llm: Personalized retrieval-augmented disease prediction system. Mingyu Jin, Qinkai Yu, Dong Shu, Chong Zhang, Lizhou Fan, Wenyue Hua, Suiyuan Zhu, Yanda Meng, Zhenting Wang, Mengnan Du, Yongfeng Zhang, Yanda Meng, 10.48550/ARXIV.2402.00746CoRR, abs/2402.007462024c</p>
<p>The impact of reasoning step length on large language models. Mingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, Mengnan Du, Findings of the Association for Computational Linguistics, ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024d. 2024and virtual meeting, August 11-16</p>
<p>Prompting visual-language models for efficient video understanding. Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, Weidi Xie, 10.1007/978-3-031-19833-5_7Computer Vision -ECCV 2022 -17th European Conference. Lecture Notes in Computer Science. Tel Aviv, IsraelSpringer2022a. October 23-27, 202213695Proceedings, Part XXXV</p>
<p>Prompting visual-language models for efficient video understanding. Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, Weidi Xie, 10.1007/978-3-031-19833-5_7Computer Vision -ECCV 2022 -17th European Conference. Lecture Notes in Computer Science. Tel Aviv, IsraelSpringer2022b. October 23-27, 202213695Proceedings, Part XXXV</p>
<p>Maple: Multi-modal prompt learning. Muhammad Uzair Khattak, Hanoona Abdul Rasheed, Muhammad Maaz, Salman H Khan, Fahad Shahbaz Khan, 10.1109/CVPR52729.2023.01832IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023. Vancouver, BC, CanadaIEEE2023. June 17-24, 2023</p>
<p>Learning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, 2009</p>
<p>The dangers of post-hoc interpretability: Unjustified counterfactual explanations. Thibault Laugel, Marie-Jeanne Lesot, Christophe Marsala, Xavier Renard, Marcin Detyniecki, 10.24963/IJCAI.2019/388Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019. the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019Macao, China2019. August 10-16, 2019</p>
<p>The power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, 10.18653/V1/2021.EMNLP-MAIN.243Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana. the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta CanaAssociation for Computational Linguistics2021. 7-11 November, 2021</p>
<p>Multimodal foundation models: From specialists to general-purpose assistants. Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao, 10.1561/0600000110Found. Trends Comput. Graph. Vis. 161-22024a</p>
<p>Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Wei Ji, Wenqiao Zhang, Tat-Seng Chua, arXiv:2308.04152Siliang Tang, Hanwang Zhang, and Yueting Zhuang. 2024b. Finetuning multimodal llms to follow zero-shot demonstrative instructions. Preprint</p>
<p>BLIP: bootstrapping language-image pretraining for unified vision-language understanding and generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven C H Hoi, International Conference on Machine Learning, ICML 2022. Baltimore, Maryland, USAPMLR2022. July 2022162of Proceedings of Machine Learning Research</p>
<p>Evaluating object hallucination in large vision-language models. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, Ji-Rong Wen, 10.18653/V1/2023.EMNLP-MAIN.20Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023Singapore2023. December 6-10, 2023Association for Computational Linguistics</p>
<p>Modular and parameter-efficient multimodal fusion with prompting. Sheng Liang, Mengjie Zhao, Hinrich Schütze, 10.18653/V1/2022.FINDINGS-ACL.234Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational Linguistics2022. May 22-27, 2022</p>
<p>Visual spatial reasoning. Fangyu Liu, Guy Emerson, Nigel Collier, Transactions of the Association for Computational Linguistics. 112023a</p>
<p>Improved baselines with visual instruction tuning. Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee, 10.48550/ARXIV.2310.03744CoRR, abs/2310.037442023b</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023. NeurIPS; New Orleans, LA, USA2023c. 2023. December 10 -16, 2023</p>
<p>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, 10.1145/3560815ACM Comput. Surv. 559352023d</p>
<p>P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, Jie Tang, ; Ma, Chen Zhang, Lei Ren, Jingang Wang, Qifan Wang, Wei Wu, Xiaojun Quan, Dawei Song, 10.18653/V1/2022.EMNLP-MAIN.758CoRR, abs/2110.07602Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2021. 2022. December 7-11, 20222022Xprompt: Exploring the extreme of prompt tuning</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, Ryan Lowe, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2022. 2022. 2022. November 28 -December 9, 2022</p>
<p>Pytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Z Yang, Zachary Devito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. NeurIPS; Vancouver, BC, Canada2019. 2019. 2019. December 8-14, 2019</p>
<p>Pre-trained models for natural language processing: A survey. Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, Xuanjing Huang, CoRR, abs/2003.082712020</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. the 38th International Conference on Machine Learning, ICML 2021PMLR2021. 18-24 July 2021139of Proceedings of Machine Learning Research</p>
<p>Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Cynthia Rudin, 10.1038/S42256-019-0048-XNat. Mach. Intell. 152019</p>
<p>Multimodal instruction tuning with conditional mixture of lora. Ying Shen, Zhiyang Xu, Qifan Wang, Yu Cheng, Wenpeng Yin, Lifu Huang, 10.48550/ARXIV.2402.15896CoRR, abs/2402.158962024</p>
<p>Towards VQA models that can read. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, Marcus Rohrbach, 10.1109/CVPR.2019.00851IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019. Long Beach, CA, USAComputer Vision Foundation / IEEE2019. June 16-20, 2019</p>
<p>Visual agents as fast and slow thinkers. Guangyan Sun, Mingyu Jin, Zhenting Wang, Cheng-Long Wang, Siqi Ma, Qifan Wang, Ying Nian Wu, Yongfeng Zhang, Dongfang Liu, CoRR, abs/2408.088622024a</p>
<p>Massive activations in large language models. Mingjie Sun, Xinlei Chen, J Zico Kolter, Zhuang Liu, 10.48550/ARXIV.2402.17762CoRR, abs/2402.177622024b</p>
<p>EVA-CLIP: improved training techniques for CLIP at scale. Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, Yue Cao, 10.48550/ARXIV.2303.15389CoRR, abs/2303.153892023</p>
<p>Aprompt: Attention prompt tuning for efficient adaptation of pre-trained language models. Qifan Wang, Yuning Mao, Jingang Wang, Hanchao Yu, Shaoliang Nie, Sinong Wang, Fuli Feng, Lifu Huang, Xiaojun Quan, Zenglin Xu, Dongfang Liu, 10.18653/V1/2023.EMNLP-MAIN.567Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023a. December 6-10, 2023</p>
<p>Visual recognition with deep nearest centroids. Wenguan Wang, Cheng Han, Tianfei Zhou, Dongfang Liu, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023b. May 1-5, 2023OpenReview.net</p>
<p>Dualprompt: Complementary prompting for rehearsal-free continual learning. Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer G Dy, Tomas Pfister, 10.1007/978-3-031-19809-0_36Computer Vision -ECCV 2022 -17th European Conference. Lecture Notes in Computer Science. Tel Aviv, IsraelSpringer2022. October 23-27, 202213686Proceedings, Part XXVI</p>
<p>Not all attention is needed: Parameter and computation efficient transfer learning for multi-modal large language models. Qiong Wu, Weihao Ye, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji, 10.48550/ARXIV.2403.15226CoRR, abs/2403.152262024a</p>
<p>Mixture-of-prompt-experts for multimodal semantic understanding. Zichen Wu, Hsiu-Yuan Huang, Fanyi Qu, Yunfang Wu, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024. the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024Torino, ItalyELRA and ICCL2024b. 20-25 May, 2024</p>
<p>Visual entailment: A novel task for fine-grained image understanding. Ning Xie, Farley Lai, Derek Doran, Asim Kadav, CoRR, abs/1901.067062019</p>
<p>Pllava : Parameter-free llava extension from images to videos for video dense captioning. Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See-Kiong Ng, Jiashi Feng, 10.48550/ARXIV.2404.16994CoRR, abs/2404.169942024a</p>
<p>Vision-flan: Scaling humanlabeled tasks in visual instruction tuning. Zhiyang Xu, Chao Feng, Rulin Shao, Trevor Ashby, Ying Shen, Di Jin, Yu Cheng, Qifan Wang, Lifu Huang, 10.48550/ARXIV.2402.11690CoRR, abs/2402.116902024b</p>
<p>Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning. Zhiyang Xu, Ying Shen, Lifu Huang, 10.18653/V1/2023.ACL-LONG.641Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 20231ACL 2023</p>
<p>Prompt learns prompt: Exploring knowledge-aware generative prompt collaboration for video captioning. Liqi Yan, Cheng Han, Zenglin Xu, Dongfang Liu, Qifan Wang, 10.24963/IJCAI.2023/180Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI 2023. the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI 2023Macao, SAR, China2023. 19th-25th August 2023</p>
<p>Prompt tuning for unified multimodal pretrained models. Hao Yang, Junyang Lin, An Yang, Peng Wang, Chang Zhou, 10.18653/V1/2023.FINDINGS-ACL.27Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023a. July 9-14, 2023</p>
<p>Mixpave: Mix-prompt tuning for few-shot product attribute value extraction. Li Yang, Qifan Wang, Jingang Wang, Xiaojun Quan, Fuli Feng, Yu Chen, Madian Khabsa, Sinong Wang, Zenglin Xu, Dongfang Liu, 10.18653/V1/2023.FINDINGS-ACL.633Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023b. July 9-14, 2023</p>
<p>Visual-language prompt tuning with knowledgeguided context optimization. Hantao Yao, Rui Zhang, Changsheng Xu, 10.1109/CVPR52729.2023.00653IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023. Vancouver, BC, CanadaIEEE2023. June 17-24, 2023</p>
<p>A survey on multimodal large language models. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, Enhong Chen, 10.48550/ARXIV.2306.13549CoRR, abs/2306.135492023</p>
<p>Investigating the catastrophic forgetting in multimodal large language models. Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, Yi Ma, 10.48550/ARXIV.2309.10313CoRR, abs/2309.103132023</p>
<p>When AI meets finance (stockagent): Large language modelbased stock trading in simulated real-world environments. Chong Zhang, Xinyi Liu, Mingyu Jin, Zhongmou Zhang, Lingyao Li, Zhenting Wang, Wenyue Hua, Dong Shu, Suiyuan Zhu, Xiaobo Jin, Sujian Li, Mengnan Du, Yongfeng Zhang, 10.48550/ARXIV.2407.18957CoRR, abs/2407.189572024</p>
<p>Instruction tuning for large language models: A survey. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang, 10.48550/ARXIV.2308.10792CoRR, abs/2308.107922023</p>
<p>A survey of large language models. Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Zhipeng Chen, Jinhao Chen, Ruiyang Jiang, Yifan Ren, Xinyu Li, Zikang Tang, Peiyu Liu, Jian-Yun Liu, Ji-Rong Nie, Wen, 10.48550/ARXIV.2303.18223CoRR, abs/2303.182232023</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2023. 2023. 2023. December 10 -16, 2023</p>
<p>Conditional prompt learning for vision-language models. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2022a</p>
<p>Learning to prompt for visionlanguage models. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, International Journal of Computer Vision. 2022bIJCV</p>
<p>Options: (a) yes (b) no Answer: (a) yes Groundtruth: (a) yes Question: What brand of soda is this? Answer: Pepsi Groundtruth: Dr pepper Text-VQA: VSR： Question:The text is about the spatial relationship between two objects in the image. TextThe car is in front of the cat. Options: (a) yes (b) Answer: (a) yes Groundtruth: (a) no Question: What brand of soda is this? Answer: Pepsi Groundtruth: Dr pepper CIFAR-100： CIFAR-10： Question: What is the object in the image? Please only answer a single object in apple, baby,... , willow_tree, wolf, woman, worm. Answer: mushroom Groundtruth: mushroom CIFAR-100： CIFAR-10： Question: What is the object in the image? Please only answer a single object in airplane. - Text, Vqa, MME： Question: Is a c++ code shown in the picture? Please answer yes or no. Answer: Yes Groundtruth: Yes Question: What brand of soda is this? Answer: rolex Groundtruth: rolex Text-VQA: MME： Question:Is this an image of Beijing Guozijian? Please answer yes or no. Answer: No Groundtruth: Yes Question: What is the number above the windshield on the yellow tractor on the left? Answer: 6 Groundtruth: a401 rsl Text. Answer: 2 Groundtruth: 7 Question: Is there a truck in the image? Answer: Yes Groundtruth: No SNLI-VE： Question: Folk dancers are entertaining the streets during a parade. (a) contradiction (b) neutral (c) entailment. Which relationship is correct? Answer: (b) neutral Groundtruth: (b) neutral SNLI-VE： Question:The soccer playing was on the sideline. (a) contradiction (b) neutral (c) entailment. Which relationship is correct? Answer: (b) neutral Groundtruth: (c) entailment Question: What is the object in the image? Please only answer a single object in airplane, bird, cat, deer, ..., horse, ship, truck. Answer: airplane Groundtruth: airplane</p>            </div>
        </div>

    </div>
</body>
</html>
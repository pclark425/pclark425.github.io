<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7346 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7346</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7346</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-272969036</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.18412v1.pdf" target="_blank">SciDFM: A Large Language Model with Mixture-of-Experts for Science</a></p>
                <p><strong>Paper Abstract:</strong> Recently, there has been a significant upsurge of interest in leveraging large language models (LLMs) to assist scientific discovery. However, most LLMs only focus on general science, while they lack domain-specific knowledge, such as chemical molecules and amino acid sequences. To bridge these gaps, we introduce SciDFM, a mixture-of-experts LLM, which is trained from scratch and is able to conduct college-level scientific reasoning and understand molecules and amino acid sequences. We collect a large-scale training corpus containing numerous scientific papers and books from different disciplines as well as data from domain-specific databases. We further fine-tune the pre-trained model on lots of instruction data to improve performances on downstream benchmarks. From experiment results, we show that SciDFM achieves strong performance on general scientific benchmarks such as SciEval and SciQ, and it reaches a SOTA performance on domain-specific benchmarks among models of similar size. We further analyze the expert layers and show that the results of expert selection vary with data from different disciplines. To benefit the broader research community, we open-source SciDFM at https://huggingface.co/OpenDFM/SciDFM-MoE-A5.6B-v1.0.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7346.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7346.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciDFM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciDFM: A Large Language Model with Mixture-of-Experts for Science</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based mixture-of-experts (MoE) large language model trained from scratch on a large scientific + general corpus (≈1.1T tokens total) and instruction-tuned to handle domain-specific scientific text including chemical molecules and amino acid sequences; evaluated on general scientific benchmarks and domain-specific molecular/protein tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SciDFM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>18.2B total parameters, 5.6B activated (MoE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>pretrained-from-scratch MoE transformer, instruction-tuned on domain-specific and general instructions</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry and Biology (molecules, proteins); also evaluated on general science, math, biology)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based simulation-style tasks including: (1) molecular generation (guided molecule design), (2) reagent prediction, (3) retrosynthesis generation, (4) molecular property prediction (MoleculeNet tasks), and (5) protein understanding tasks (functional description, catalytic activity, domain/motif prediction) — all framed as instruction-following/generation problems over SMILES and amino-acid sequences or natural language descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot instruction-following for all experiments (the paper states all experiments were conducted in zero-shot settings since SciDFM is instruction-following by default). Molecule/protein tokens were specially tokenized (each atom and amino-acid character as single tokens, wrapped with special identifiers).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Varies by task: Exact match, Levenshtein distance (lower better), fingerprint similarity metrics (RDK fingerprint Tanimoto, MACCS FTS, Morgan FTS), Validity (fraction valid molecules), AUC-ROC (for MoleculeNet molecular property prediction), ROUGE-L (for protein description tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Mol-Instructions molecular generation (Guided Molecule Design) — SciDFM row: Exact=0.084, Levenshtein=43.414, RDK FTS=0.586, MACCS FTS=0.704, Morgan FTS=0.443, Validity=0.994; Reagent Prediction — SciDFM: Exact=0.192, Levenshtein=17.527, RDK FTS=0.476, MACCS FTS=0.576, Morgan FTS=0.452, Validity=0.999; Retrosynthesis — SciDFM: Exact=0.665, Levenshtein=6.45, RDK FTS=0.916, MACCS FTS=0.937, Morgan FTS=0.888, Validity=0.998. Mol-Instructions protein understanding (ROUGE-L): Protein Function=0.60, Functional Description=0.72, Catalytic Activity=0.76, Domain/Motif=0.55. MoleculeNet (AUC-ROC) row reported for SciDFM (per table): "76.4 64.898.571.575.677.36" (columns in table: bace, bbbp, ClinTox, HIV, Tox21, Avg).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Baselines reported in same tables include ChatGLM3-6B, Galactica-6.7B, Galactica-30B, LLaMa variants, GPT4o, ChemDFM-13B, Text+Chem T5, Mol-Instructions (baseline rows). Example comparators: ChemDFM-13B (MoleculeNet row: "78.4 66.789.973.679.877.68" per paper table), Galactica-30B (MoleculeNet row: "72.7 59.682.275.968.571.78"), GPT4o entries shown in molecular/protein tables. (Paper reports comparative numbers in tables; see main text and Tables 7–9.)</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Mixture-of-Experts (MoE) architecture: expert selection behavior varies by discipline and is reported to influence specialization/accuracy across domains', 'Tokenization choices: treating each chemical atom and amino-acid character as single tokens (with special identifiers) to better encode molecules and sequences', 'Pretraining corpus composition and scale (≈1.1T tokens total; ~300B science-domain tokens) and domain-specific database inclusion', 'Instruction tuning on ≈9.3M instruction samples (including molecular/protein instructions) improved downstream performance', 'Model size and training data: SciDFM competitive with models of similar compute but weaker than larger models trained with more data', 'MoE hyperparameters (expert capacity factor, auxiliary loss factor) and training regimen (two epochs, learning rate schedules) — these training choices reported as part of experimental conditions that can affect results']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Pretraining: two epochs over ~1.1T tokens (≈570B tokens explicitly described in Table 2 for first pass), AdamW optimizer, sequence length 8192, macro batch size 4M tokens; MoE settings: num_experts=8, topk_experts=2, expert capacity factor=1.0, auxiliary loss factor=0.02. Instruction tuning: 5 epochs, lr=2e-5, sequence length=2048, macro batch size=32, ~9.3M instruction samples. All downstream evaluations run in zero-shot instruction-following mode (no few-shot examples reported). Hardware: trained on cluster of 16 nodes × 8 A800 GPUs per node.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Authors report SciDFM is weaker on general science tasks compared to the top-performing Llama3-8B-Instruct and is weaker than models that are larger or trained on more data; authors note SciDFM outperforms many comparable-size models on domain-specific molecular/protein tasks but still lags behind certain domain-specialized models (e.g., ChemDFM-13B in some metrics). Visualization analysis showed molecules and amino-acid sequences did not cluster similarly to research papers (no internal clustering among those modalities), indicating distinct vocabulary/representation patterns that may limit cross-domain transfer. The paper does not report advanced prompting strategies (e.g., chain-of-thought or few-shot) — all evaluations are zero-shot, which may limit peak performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciDFM: A Large Language Model with Mixture-of-Experts for Science', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7346.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7346.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemDFM-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemDFM (13B) — Dialogue Foundation Model for Chemistry (as referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chemistry-specialized LLM trained on chemical literature, textbooks, instructions and general domain data; presented by the authors as a state-of-the-art domain model for chemical tasks and used as a comparison baseline in SciDFM experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChemDFM (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemDFM-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>domain-specific trained/fine-tuned for chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (molecule understanding and molecular property prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Used as baseline for text-based simulation tasks involving molecular property prediction (MoleculeNet) and molecular generation / understanding tasks (Mol-Instructions).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot instruction-following (same experimental setup as SciDFM evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>AUC-ROC for MoleculeNet; Exact/Levenshtein/fingerprint similarities/validity for Mol-Instructions molecular tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Reported in paper tables (examples): MoleculeNet row for ChemDFM-13B: "78.4 66.789.973.679.877.68" (columns: bace, bbbp, ClinTox, HIV, Tox21, Avg). On Mol-Instructions molecular tasks ChemDFM-13B values appear in comparison rows in the tables (see Tables 7–8 in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared against SciDFM and other general/sci LLMs such as Galactica and GPT4o in the presented tables.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Domain-specific pretraining and instruction data quantity and curation (ChemDFM described as trained on 34B tokens from chemical literature in related-work summary)', 'Specialization yields improved chemical task performance relative to general-purpose LLMs']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Evaluations performed in zero-shot; exact training/hyperparameters for ChemDFM are described in its own referenced work (paper cites ChemDFM), not fully detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Not discussed in detail in this paper beyond comparative table entries; authors note SciDFM outperforms most LLMs except ChemDFM-13B on certain chemical tasks, implying ChemDFM retains advantages from large domain-specific pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciDFM: A Large Language Model with Mixture-of-Experts for Science', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7346.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7346.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (as listed in the paper comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-capability instruction-following model from OpenAI family used by the authors as a comparison point on molecular/protein tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-following, general-purpose foundation model</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Used as a comparator on chemistry and protein text-based tasks (Mol-Instructions, MoleculeNet comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Evaluated as a baseline for molecular generation and protein description tasks (as reported in Mol-Instructions/MoleculeNet tables).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot instruction-following (same overall experimental protocol in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Same metrics as other entries: Exact/Levenshtein/fingerprint metrics and ROUGE-L for protein tasks; AUC-ROC for MoleculeNet where reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Reported in tables: e.g., Mol-Instructions protein tasks row for GPT4o: values shown as "0.06 0.05 0.07 0.06" (Protein Function, Functional Description, Catalytic Activity, Domain/Motif respectively). MoleculeNet row for GPT4o appears in table as "62.5 61.55 1.665 5.955 5.259 9.34" per the table formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Serves as a strong general-purpose baseline compared to SciDFM and domain-specialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>["General-purpose capabilities vs domain specialization: GPT4o's performance is used to show trade-offs between general LLMs and domain-specialized models"]</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Evaluated zero-shot in same setting as other models; no special prompt engineering or few-shot examples reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>As a general model, GPT4o is outperformed by domain-specialized models (e.g., SciDFM on some Mol-Instructions protein tasks and ChemDFM on some chemical tasks) per the reported tables.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciDFM: A Large Language Model with Mixture-of-Experts for Science', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7346.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7346.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mol-Instructions (baseline model rows)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mol-Instructions (dataset and baseline model entries appearing in evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mol-Instructions is a large-scale biomolecular instruction dataset used both for instruction data and as an evaluation benchmark; the paper reports baseline model rows (e.g., 'Mol-Instructions' as a model/benchmark entry) for molecular and protein generation/understanding tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mol-instructions: A large-scale biomolecular instruction dataset for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mol-Instructions (benchmark / baseline rows)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A (dataset / baseline results presented)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>benchmark/dataset baseline (models trained or evaluated on Mol-Instructions in referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry and Biology — biomolecular text, protein sequences, molecules</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Benchmark tasks: molecular generation (guided design), reagent prediction, retrosynthesis; protein function and description generation (evaluated as generative text-simulation tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot evaluation in this paper's experiments (benchmarks reported under zero-shot condition).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Exact match, Levenshtein, RDK/MACCS/Morgan fingerprint similarity metrics, Validity, ROUGE-L for proteins.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Baseline rows labeled 'Mol-Instructions' in tables: molecular generation/reagent/retrosynthesis baseline values are provided (examples in table rows). For protein understanding, Mol-Instructions baseline row shows: Protein Function=0.43, Functional Description=0.44, Catalytic Activity=0.52, Domain/Motif=0.46 (ROUGE-L).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Used as a benchmark reference row in the tables; SciDFM and other models compared against these baseline values.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Dataset curation and instruction types included in Mol-Instructions influence model performance on molecular/protein instruction tasks']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Evaluations performed under the paper's zero-shot evaluation regime; dataset-specific baseline training details belong to the Mol-Instructions reference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>As a dataset baseline, performance depends on how a model was trained or fine-tuned on Mol-Instructions; the paper uses these baseline rows primarily for comparative purposes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciDFM: A Large Language Model with Mixture-of-Experts for Science', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mol-instructions: A large-scale biomolecular instruction dataset for large language models <em>(Rating: 2)</em></li>
                <li>Dialogue foundation model for chemistry <em>(Rating: 2)</em></li>
                <li>Moleculenet: a benchmark for molecular machine learning <em>(Rating: 2)</em></li>
                <li>Galactica: A large language model for science <em>(Rating: 1)</em></li>
                <li>ProGen2: exploring the boundaries of protein language models <em>(Rating: 1)</em></li>
                <li>BioGPT: generative pre-trained transformer for biomedical text generation and mining <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7346",
    "paper_id": "paper-272969036",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "SciDFM",
            "name_full": "SciDFM: A Large Language Model with Mixture-of-Experts for Science",
            "brief_description": "A transformer-based mixture-of-experts (MoE) large language model trained from scratch on a large scientific + general corpus (≈1.1T tokens total) and instruction-tuned to handle domain-specific scientific text including chemical molecules and amino acid sequences; evaluated on general scientific benchmarks and domain-specific molecular/protein tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SciDFM",
            "model_size": "18.2B total parameters, 5.6B activated (MoE)",
            "model_type": "pretrained-from-scratch MoE transformer, instruction-tuned on domain-specific and general instructions",
            "scientific_domain": "Chemistry and Biology (molecules, proteins); also evaluated on general science, math, biology)",
            "simulation_task_description": "Text-based simulation-style tasks including: (1) molecular generation (guided molecule design), (2) reagent prediction, (3) retrosynthesis generation, (4) molecular property prediction (MoleculeNet tasks), and (5) protein understanding tasks (functional description, catalytic activity, domain/motif prediction) — all framed as instruction-following/generation problems over SMILES and amino-acid sequences or natural language descriptions.",
            "prompting_strategy": "Zero-shot instruction-following for all experiments (the paper states all experiments were conducted in zero-shot settings since SciDFM is instruction-following by default). Molecule/protein tokens were specially tokenized (each atom and amino-acid character as single tokens, wrapped with special identifiers).",
            "evaluation_metric": "Varies by task: Exact match, Levenshtein distance (lower better), fingerprint similarity metrics (RDK fingerprint Tanimoto, MACCS FTS, Morgan FTS), Validity (fraction valid molecules), AUC-ROC (for MoleculeNet molecular property prediction), ROUGE-L (for protein description tasks).",
            "reported_accuracy": "Mol-Instructions molecular generation (Guided Molecule Design) — SciDFM row: Exact=0.084, Levenshtein=43.414, RDK FTS=0.586, MACCS FTS=0.704, Morgan FTS=0.443, Validity=0.994; Reagent Prediction — SciDFM: Exact=0.192, Levenshtein=17.527, RDK FTS=0.476, MACCS FTS=0.576, Morgan FTS=0.452, Validity=0.999; Retrosynthesis — SciDFM: Exact=0.665, Levenshtein=6.45, RDK FTS=0.916, MACCS FTS=0.937, Morgan FTS=0.888, Validity=0.998. Mol-Instructions protein understanding (ROUGE-L): Protein Function=0.60, Functional Description=0.72, Catalytic Activity=0.76, Domain/Motif=0.55. MoleculeNet (AUC-ROC) row reported for SciDFM (per table): \"76.4 64.898.571.575.677.36\" (columns in table: bace, bbbp, ClinTox, HIV, Tox21, Avg).",
            "baseline_accuracy": "Baselines reported in same tables include ChatGLM3-6B, Galactica-6.7B, Galactica-30B, LLaMa variants, GPT4o, ChemDFM-13B, Text+Chem T5, Mol-Instructions (baseline rows). Example comparators: ChemDFM-13B (MoleculeNet row: \"78.4 66.789.973.679.877.68\" per paper table), Galactica-30B (MoleculeNet row: \"72.7 59.682.275.968.571.78\"), GPT4o entries shown in molecular/protein tables. (Paper reports comparative numbers in tables; see main text and Tables 7–9.)",
            "factors_reported": [
                "Mixture-of-Experts (MoE) architecture: expert selection behavior varies by discipline and is reported to influence specialization/accuracy across domains",
                "Tokenization choices: treating each chemical atom and amino-acid character as single tokens (with special identifiers) to better encode molecules and sequences",
                "Pretraining corpus composition and scale (≈1.1T tokens total; ~300B science-domain tokens) and domain-specific database inclusion",
                "Instruction tuning on ≈9.3M instruction samples (including molecular/protein instructions) improved downstream performance",
                "Model size and training data: SciDFM competitive with models of similar compute but weaker than larger models trained with more data",
                "MoE hyperparameters (expert capacity factor, auxiliary loss factor) and training regimen (two epochs, learning rate schedules) — these training choices reported as part of experimental conditions that can affect results"
            ],
            "experimental_conditions": "Pretraining: two epochs over ~1.1T tokens (≈570B tokens explicitly described in Table 2 for first pass), AdamW optimizer, sequence length 8192, macro batch size 4M tokens; MoE settings: num_experts=8, topk_experts=2, expert capacity factor=1.0, auxiliary loss factor=0.02. Instruction tuning: 5 epochs, lr=2e-5, sequence length=2048, macro batch size=32, ~9.3M instruction samples. All downstream evaluations run in zero-shot instruction-following mode (no few-shot examples reported). Hardware: trained on cluster of 16 nodes × 8 A800 GPUs per node.",
            "limitations_or_failure_modes": "Authors report SciDFM is weaker on general science tasks compared to the top-performing Llama3-8B-Instruct and is weaker than models that are larger or trained on more data; authors note SciDFM outperforms many comparable-size models on domain-specific molecular/protein tasks but still lags behind certain domain-specialized models (e.g., ChemDFM-13B in some metrics). Visualization analysis showed molecules and amino-acid sequences did not cluster similarly to research papers (no internal clustering among those modalities), indicating distinct vocabulary/representation patterns that may limit cross-domain transfer. The paper does not report advanced prompting strategies (e.g., chain-of-thought or few-shot) — all evaluations are zero-shot, which may limit peak performance.",
            "uuid": "e7346.0",
            "source_info": {
                "paper_title": "SciDFM: A Large Language Model with Mixture-of-Experts for Science",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "ChemDFM-13B",
            "name_full": "ChemDFM (13B) — Dialogue Foundation Model for Chemistry (as referenced)",
            "brief_description": "A chemistry-specialized LLM trained on chemical literature, textbooks, instructions and general domain data; presented by the authors as a state-of-the-art domain model for chemical tasks and used as a comparison baseline in SciDFM experiments.",
            "citation_title": "ChemDFM (referenced)",
            "mention_or_use": "use",
            "model_name": "ChemDFM-13B",
            "model_size": "13B",
            "model_type": "domain-specific trained/fine-tuned for chemistry",
            "scientific_domain": "Chemistry (molecule understanding and molecular property prediction)",
            "simulation_task_description": "Used as baseline for text-based simulation tasks involving molecular property prediction (MoleculeNet) and molecular generation / understanding tasks (Mol-Instructions).",
            "prompting_strategy": "Zero-shot instruction-following (same experimental setup as SciDFM evaluations).",
            "evaluation_metric": "AUC-ROC for MoleculeNet; Exact/Levenshtein/fingerprint similarities/validity for Mol-Instructions molecular tasks.",
            "reported_accuracy": "Reported in paper tables (examples): MoleculeNet row for ChemDFM-13B: \"78.4 66.789.973.679.877.68\" (columns: bace, bbbp, ClinTox, HIV, Tox21, Avg). On Mol-Instructions molecular tasks ChemDFM-13B values appear in comparison rows in the tables (see Tables 7–8 in paper).",
            "baseline_accuracy": "Compared against SciDFM and other general/sci LLMs such as Galactica and GPT4o in the presented tables.",
            "factors_reported": [
                "Domain-specific pretraining and instruction data quantity and curation (ChemDFM described as trained on 34B tokens from chemical literature in related-work summary)",
                "Specialization yields improved chemical task performance relative to general-purpose LLMs"
            ],
            "experimental_conditions": "Evaluations performed in zero-shot; exact training/hyperparameters for ChemDFM are described in its own referenced work (paper cites ChemDFM), not fully detailed here.",
            "limitations_or_failure_modes": "Not discussed in detail in this paper beyond comparative table entries; authors note SciDFM outperforms most LLMs except ChemDFM-13B on certain chemical tasks, implying ChemDFM retains advantages from large domain-specific pretraining.",
            "uuid": "e7346.1",
            "source_info": {
                "paper_title": "SciDFM: A Large Language Model with Mixture-of-Experts for Science",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GPT4o",
            "name_full": "GPT-4o (as listed in the paper comparisons)",
            "brief_description": "A high-capability instruction-following model from OpenAI family used by the authors as a comparison point on molecular/protein tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT4o",
            "model_size": "not specified in paper",
            "model_type": "instruction-following, general-purpose foundation model",
            "scientific_domain": "Used as a comparator on chemistry and protein text-based tasks (Mol-Instructions, MoleculeNet comparisons)",
            "simulation_task_description": "Evaluated as a baseline for molecular generation and protein description tasks (as reported in Mol-Instructions/MoleculeNet tables).",
            "prompting_strategy": "Zero-shot instruction-following (same overall experimental protocol in the paper).",
            "evaluation_metric": "Same metrics as other entries: Exact/Levenshtein/fingerprint metrics and ROUGE-L for protein tasks; AUC-ROC for MoleculeNet where reported.",
            "reported_accuracy": "Reported in tables: e.g., Mol-Instructions protein tasks row for GPT4o: values shown as \"0.06 0.05 0.07 0.06\" (Protein Function, Functional Description, Catalytic Activity, Domain/Motif respectively). MoleculeNet row for GPT4o appears in table as \"62.5 61.55 1.665 5.955 5.259 9.34\" per the table formatting.",
            "baseline_accuracy": "Serves as a strong general-purpose baseline compared to SciDFM and domain-specialized models.",
            "factors_reported": [
                "General-purpose capabilities vs domain specialization: GPT4o's performance is used to show trade-offs between general LLMs and domain-specialized models"
            ],
            "experimental_conditions": "Evaluated zero-shot in same setting as other models; no special prompt engineering or few-shot examples reported.",
            "limitations_or_failure_modes": "As a general model, GPT4o is outperformed by domain-specialized models (e.g., SciDFM on some Mol-Instructions protein tasks and ChemDFM on some chemical tasks) per the reported tables.",
            "uuid": "e7346.2",
            "source_info": {
                "paper_title": "SciDFM: A Large Language Model with Mixture-of-Experts for Science",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Mol-Instructions (baseline model rows)",
            "name_full": "Mol-Instructions (dataset and baseline model entries appearing in evaluations)",
            "brief_description": "Mol-Instructions is a large-scale biomolecular instruction dataset used both for instruction data and as an evaluation benchmark; the paper reports baseline model rows (e.g., 'Mol-Instructions' as a model/benchmark entry) for molecular and protein generation/understanding tasks.",
            "citation_title": "Mol-instructions: A large-scale biomolecular instruction dataset for large language models",
            "mention_or_use": "use",
            "model_name": "Mol-Instructions (benchmark / baseline rows)",
            "model_size": "N/A (dataset / baseline results presented)",
            "model_type": "benchmark/dataset baseline (models trained or evaluated on Mol-Instructions in referenced work)",
            "scientific_domain": "Chemistry and Biology — biomolecular text, protein sequences, molecules",
            "simulation_task_description": "Benchmark tasks: molecular generation (guided design), reagent prediction, retrosynthesis; protein function and description generation (evaluated as generative text-simulation tasks).",
            "prompting_strategy": "Zero-shot evaluation in this paper's experiments (benchmarks reported under zero-shot condition).",
            "evaluation_metric": "Exact match, Levenshtein, RDK/MACCS/Morgan fingerprint similarity metrics, Validity, ROUGE-L for proteins.",
            "reported_accuracy": "Baseline rows labeled 'Mol-Instructions' in tables: molecular generation/reagent/retrosynthesis baseline values are provided (examples in table rows). For protein understanding, Mol-Instructions baseline row shows: Protein Function=0.43, Functional Description=0.44, Catalytic Activity=0.52, Domain/Motif=0.46 (ROUGE-L).",
            "baseline_accuracy": "Used as a benchmark reference row in the tables; SciDFM and other models compared against these baseline values.",
            "factors_reported": [
                "Dataset curation and instruction types included in Mol-Instructions influence model performance on molecular/protein instruction tasks"
            ],
            "experimental_conditions": "Evaluations performed under the paper's zero-shot evaluation regime; dataset-specific baseline training details belong to the Mol-Instructions reference.",
            "limitations_or_failure_modes": "As a dataset baseline, performance depends on how a model was trained or fine-tuned on Mol-Instructions; the paper uses these baseline rows primarily for comparative purposes.",
            "uuid": "e7346.3",
            "source_info": {
                "paper_title": "SciDFM: A Large Language Model with Mixture-of-Experts for Science",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mol-instructions: A large-scale biomolecular instruction dataset for large language models",
            "rating": 2,
            "sanitized_title": "molinstructions_a_largescale_biomolecular_instruction_dataset_for_large_language_models"
        },
        {
            "paper_title": "Dialogue foundation model for chemistry",
            "rating": 2,
            "sanitized_title": "dialogue_foundation_model_for_chemistry"
        },
        {
            "paper_title": "Moleculenet: a benchmark for molecular machine learning",
            "rating": 2,
            "sanitized_title": "moleculenet_a_benchmark_for_molecular_machine_learning"
        },
        {
            "paper_title": "Galactica: A large language model for science",
            "rating": 1,
            "sanitized_title": "galactica_a_large_language_model_for_science"
        },
        {
            "paper_title": "ProGen2: exploring the boundaries of protein language models",
            "rating": 1,
            "sanitized_title": "progen2_exploring_the_boundaries_of_protein_language_models"
        },
        {
            "paper_title": "BioGPT: generative pre-trained transformer for biomedical text generation and mining",
            "rating": 1,
            "sanitized_title": "biogpt_generative_pretrained_transformer_for_biomedical_text_generation_and_mining"
        }
    ],
    "cost": 0.0143795,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SciDFM: A Large Language Model with Mixture-of-Experts for Science
27 Sep 2024</p>
<p>Liangtai Sun 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
X-LANCE Lab
SJTU AI Institute Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Danyu Luo 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
X-LANCE Lab
SJTU AI Institute Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Da Ma 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
X-LANCE Lab
SJTU AI Institute Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Zihan Zhao 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
X-LANCE Lab
SJTU AI Institute Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Baocai Chen 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
X-LANCE Lab
SJTU AI Institute Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Zhennan Shen 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
X-LANCE Lab
SJTU AI Institute Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Su Zhu 
AI Speech Co, .Ltd
SuzhouChina</p>
<p>Lu Chen chenlusz@sjtu.edu.cn 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
X-LANCE Lab
SJTU AI Institute Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Suzhou Laboratory
SuzhouChina</p>
<p>Xin Chen 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
X-LANCE Lab
SJTU AI Institute Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Suzhou Laboratory
SuzhouChina</p>
<p>Kai Yu kai.yu@sjtu.edu.cn 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
X-LANCE Lab
SJTU AI Institute Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Suzhou Laboratory
SuzhouChina</p>
<p>SciDFM: A Large Language Model with Mixture-of-Experts for Science
27 Sep 2024D16631AF3FB298CF5D3A547572164DCEarXiv:2409.18412v1[cs.CL]
Recently, there has been a significant upsurge of interest in leveraging large language models (LLMs) to assist scientific discovery.However, most LLMs only focus on general science, while they lack domain-specific knowledge, such as chemical molecules and amino acid sequences.To bridge these gaps, we introduce SciDFM, a mixture-of-experts LLM, which is trained from scratch and is able to conduct college-level scientific reasoning and understand molecules and amino acid sequences.We collect a large-scale training corpus containing numerous scientific papers and books from different disciplines as well as data from domain-specific databases.We further fine-tune the pre-trained model on lots of instruction data to improve performances on downstream benchmarks.From experiment results, we show that SciDFM achieves strong performance on general scientific benchmarks such as SciEval and SciQ, and it reaches a SOTA performance on domain-specific benchmarks among models of similar size.We further analyze the expert layers and show that the results of expert selection vary with data from different disciplines.To benefit the broader research community, we open-source SciDFM at https://huggingface.co/OpenDFM/SciDFM-MoE-A5.6B-v1.0.Preprint.Under review.</p>
<p>Introduction</p>
<p>The advent of Large Language Models (LLMs) [1,2,3] has ignited a revolution in the realm of artificial intelligence and has pushed the field of AI for Science (AI4S) to an unprecedented new height.LLMs have demonstrated promising performances in assisting and accelerating scientific discovery [4,5,6], such as protein design [7], weather forecasting [8], and geoscience [9].Despite remarkable achievements in science, LLMs primarily focus on general scientific knowledge represented in text form, ignoring domain-specific contents such as molecules in chemistry and proteins in biology, which are fundamental to advances in these fields.</p>
<p>To overcome this limitation and fully exploit the potential of LLMs for scientific discovery, we introduce SciDFM, a mixture-of-experts Large Language Model trained from scratch with 18.2 billion parameters in total and 5.6 billion parameters activated.SciDFM integrates a mixture-ofexperts (MoE) [10,11,12] architecture into a transformer-based [13] framework, aiming at enhancing its sophisticated scientific reasoning and understanding capabilities and better modeling similarities and differences across different disciplines and modalities, i.e. text, molecule, protein.In this paper, we detail the pretraining and instruction tuning process of SciDFM, including training corpus and settings.SciDFM leverages a carefully curated corpus of scientific literature and domain-specific databases for pretraining to capture vast scientific knowledge, and is also trained on a large general corpus to retain general knowledge, consuming about 1.1T tokens in total.We meticulously finetune SciDFM using a set of instruction-following data containing about 9.3M samples, including interpreting molecular structures and amino acid sequences, thereby improving the performance on downstream benchmarks and bridging the gap in domain-specific knowledge.</p>
<p>To illustrate the prowess of SciDFM, we conduct extensive experiments on several scientific benchmarks.Empirical evaluations affirm the efficacy of SciDFM, demonstrating its superiority on both general scientific benchmarks like SciEval [14] and SciQ [15], as well as achieving stateof-the-art (SOTA) performance in domain-specific tasks among models of similar size, such as Mol-Instructions [16].Our analysis further delves into the results of the expert layer selection, revealing their adaptability to different scientific disciplines, and demonstrating the effectiveness of the MoE model in multidisciplinary scenarios.To benefit the broader research community, we will make SciDFM openly accessible.</p>
<p>SciDFM</p>
<p>In this section, we introduce the pretraining and instruction tuning details of SciDFM, including the training data construction, model architecture and infrastructure.SciDFM is based on a transformer architecture [13], and follows modifications of Llama [2], i.e.RMSNorm, RoPE and SwiGLU.SciDFM uses the same hyper-parameters of OpenLLaMa-3B [17], the details are shown in Table 1.And in order to better model knowledge of different disciplines, we replace the feed-forward block with Mixture-of-Expert (MoE) layers [10,11].</p>
<p>We also use the tokenizer of OpenLLaMa-3B, which is trained from scratch using the Bype-Pair Encoding (BPE) method.To better encode the molecules and amino acid sequences and distinguish them from normal characters for better modeling, we treat each chemical atom and amino acid character as a single token and add them into the vocabulary, with special identifiers wrapped [4].For example, molecules C(C(=O)O)N will be encoded as C,(,C,(,=,O,),O,),N, and amino acid sequences MIRL-GAPQTL will be encoded as M,I,R,L,G,A,P,Q,T,L, where the special identifiers are omitted.</p>
<p>Data Construction</p>
<p>To enhance the understanding and reasoning abilities of SciDFM on science domain, we collect a large-scale training corpus containing a large number of open-access scientific papers and books of different disciplines.And to acquire domain-specific knowledge, we also include data from some databases.Furthermore, in order to maintain the generic capabilities of SciDFM, we use data from some open-source generic corpora.The details of our pretraining corpus are shown in Table 2. Our pretraining corpus contains about 300B science-domain tokens and 270B general-domain tokens, with 570B tokens in total.We train SciDFM for two epochs, and for the second epoch, we re-sample data of C4, CC, and Github subsets of SlimPajama [23,24].</p>
<p>Training Details</p>
<p>Following Llama [2], SciDFM is trained from scratch using the AdamW optimizer [43] with β 1 = 0.9, β 2 = 0.95.We use a cosine learning rate schedule, such that the final learning rate is equal to 10% of the initial learning rate.We use a weight decay of 0.1 and gradient clipping of 1.0, and we set the macro batch size to 4M tokens and sequence length to 8192 tokens.For MoE layers, we set the auxiliary loss factor to 0.02 and set the expert capacity factor to 1.0.In total, we train SciDFM for two epochs, resulting in about 1.1T tokens fed.We use a learning rate of 3e − 4 for the first epoch and 3e − 5 for the second epoch, while the other settings remain the same as the above.We train SciDFM on a cluster of 16 nodes, with 8 A800 GPUs on each node for about two months.</p>
<p>Instruction Tuning</p>
<p>To improve the performance of SciDFM on downstream benchmarks, we collect a number of instruction-tuning data from open-source datasets.The details are shown in Table 3.We fine-tune the pre-trained SciDFM for 5 epochs in total.During fine-tuning, we use a learning rate of 2e-5, and we set the sequence length to 2048 and macro batch size to 32.The other settings are the same as the pretraining stage.</p>
<p>Evaluation</p>
<p>In this section, we show the performance of SciDFM on some general and domain-specific benchmarks, and analyze the results of expert selection in different domains.</p>
<p>Data</p>
<p>Domain # Samples</p>
<p>Arxivphy Physics 30231 MATH [18] Math 7500 MetaMathQA [25] Math 395000 MathInstruct [26] Math 262040 OrcaMath [27] Math 200035 DeepLoc2 [28] Biology 22642 BioASQ [29] Biology 8021 MedMCQA [30] Biology 182822 MedQA [31] Biology 10178 PubMedQA [32] Biology 500 Mol-Instructions [16] Chemistry &amp; Biology 1863630 ChemDFM-sft [5] Chemistry 1818154 SciQ [15] General Science 11679 Camel [33] General Science 110000 SciInstruct [6] General Science 82640 WebInstructSub [34] General Science 2327291 MMLU [35] General 99842 LIMA [36] General 1000 ROPES [37] General 10924 QASC [38] General 8134 OpenBookQA [39] General 4957 Dolly [40] General 15011 SlimOrca-Dedup [41] General 363491 GPT4All [42] General 806199 Other General 994896</p>
<p>Total(after dedup) -9325310 • SciQ [15] comprises high-quality, domain-targeted multiple-choice science exam questions, created through a novel method that combines crowd-sourcing with AI-assisted document and answer option selection.• ARC [44] presents a sophisticated question set, designed to advance AI research in complex question answering within the context of grade-school science, composed of an easy subset and a challenging subset.• GSM8K [45] is composed of diverse and linguistically rich grade school math word problems, designed to benchmark and improve the multi-step mathematical reasoning abilities of LLMs, revealing their limitations in handling complex reasoning tasks.• MATH [18] contains challenging competition mathematics problems, each with detailed solutions, designed to assess and enhance mathematical problem-solving and reasoning capabilities of LLMs, accompanied by an auxiliary pretraining dataset to bolster fundamental math understanding.• MedQA [31] is a pioneering multiple-choice dataset for open-domain question answering in the medical field, encompassing a number of questions sourced from professional medical board exams.• MedMCQA [30] is a large-scale medical multiple-choice question-answering dataset, spanning 2,400 healthcare topics and 21 subjects, designed to challenge models with diverse reasoning skills across various medical domains.• PubMedQA [32] is a biomedical question-answering dataset based on PubMed abstracts, requiring quantitative reasoning over research texts.• Mol-Instructions [16] is a specialized, meticulously curated dataset containing diverse biomolecular instructions, designed to enhance LLMs' understanding and prediction capabilities within the realms of molecular, protein, and broader biomolecular texts.</p>
<p>• MoleculeNet [46] is a comprehensive benchmark dataset for molecular machine learning, featuring curated public datasets, standardized evaluation metrics, and open-source implementation of various molecular featurization and learning methods.</p>
<p>In our experiments, we take Mol-Instructions and MoleculeNet as domain-specific tasks, and take the remaining benchmarks as general scientific language understanding and reasoning tasks.Evaluation Methods Since SciDFM is an instruction-following model by default, we conduct all experiments using zero-shot settings.And most of the models we select for comparison are able to follow instructions:</p>
<p>Model</p>
<p>• Galactica [4] is a large language model specifically designed to store, combine, and reason about vast amounts of scientific knowledge, outperforming existing models on various scientific tasks and aiming to serve as a new, advanced interface for scientific research.We select Galactica-30B and Galactica-6.7Bfor comparison.• Llama [2] is a series of open-source powerful language models, ranging from 7 billion to 70 billion parameters, trained on massive public datasets, and outperforms many of the available open-source models on common benchmarks.We select Llama3-8B, Llama3-8B-Instruct, Llama2-7B and Llama2-13B for comparison.• ChatGLM [47,48] is a series of advanced language models, excel in various metrics and tasks, rivaling or surpassing counterparts like GPT-4, thanks to their extensive training on multilingual data, specialized alignment techniques, and the ability to integrate diverse tools dynamically.We select ChatGLM2-6B, ChatGLM3-6B and ChatGLM3-6B-base for comparison.• SciGLM [6] is a suite of scientific language models that enhance college-level scientific reasoning through a self-reflective instruction annotation framework, addressing data scarcity in the science domain, and improving upon ChatGLM in handling complex scientific and mathematical problems without compromising language understanding.We select SciGLM-6B for comparison.• ChemDFM [5] is specifically trained for Chemistry, combining knowledge from chemical literature and general domains to excel in understanding, reasoning, and applying chemical information, outperforming generic LLMs and even GPT-4 on chemical tasks.We select ChemDFM-13B for comparison.</p>
<p>Main Results</p>
<p>General Scientific Benchmark Table 4 presents the evaluation results on eight general scientific language understanding and reasoning tasks.The results show that SciDFM reaches a better performance on average than Galactica-series models, Llama-series models except Llama3-8B-Instruct and ChatGLM-series models except ChatGLM3-6B-base.In Table 6, we also present the average performance of general science, math and biology tasks on the above eight benchmarks, in which SciEval, SciQ and ARC belong to general science task, GSM8K and MATH belong to math task, MedQA, MedMCQA and PubMedQA belong to biology task.We find that SciDFM outperforms all models except Llama3-8B-Instruct on math and biology domain, while it is weak in general science tasks.In conclusion, SciDFM can reach a similar performance to top-tier models of similar amount of compute, while it is weaker compared to models that are larger and trained using more data.outperforms most LLMs except ChemDFM-13B.To further evaluate model's ability on domain-specific benchmarks, we present the performance of molecule and protein understanding tasks on Mol-Instructions in Table 8 and Table 9.We find that SciDFM can reach a SOTA performance on molecule and protein understanding tasks.</p>
<p>Domain-specific Scientific Benchmark</p>
<p>Expert Choices Analysis</p>
<p>In this subsection, we conduct analysis on expert choice results on data from different domains.Formally, we denote the output of the ith attention layer as h i ∈ R l×d , where l is the sequence length and d is the hidden dimension, and we denote the weight of the ith gate network in the corresponding MoE layer as W g ∈ R d×e , where e represents the number of experts.Then, we have
g i = h i • W g ∈ R l×e
, representing the probability of each token being assigned to each expert.Suppose the number of hidden layers is N , for a given text T , we define the expert choice results as:
e i = Softmax( l j=1 g i [j, :]) ∈ R e ,(1)E T = Concat([e 1 , e 2 , . . . , e N ]) ∈ R N e .(2)
We randomly select 100 research papers each in the fields of math, chemistry, biology, and physics, and also select 100 chemical molecules and 100 amino acid sequences as analysis data.For each text T , we calculate E T using the above formulas.Then, we use the t-SNE [49] algorithm to reduce them to three dimensions and visualize them.</p>
<p>The visualization result is shown in Figure 1.It can be found that research papers in mathematics, chemistry, physics, and biology demonstrate a clear clustering pattern, indicative of discipline-specific language characteristics, while chemical molecules and amino acid sequences do not exhibit such a clustering phenomenon.In addition, the separation of molecular and protein data from other categories is stark, likely due to the unique vocabularies inherent to these domains, which are not shared with the remainder of the dataset.</p>
<p>Furthermore, the spatial proximity observed in the visualization adds another layer of insight: the clusters of mathematics and physics are in close proximity, as are those of chemistry and biology, with chemistry also showing affinity towards physics.This aligns well with the interrelationships and overlapping nature of knowledge between these scientific disciplines, as reflected in their linguistic characteristics.</p>
<p>Related Works</p>
<p>The success of pretraining language models like BERT [50] and GPT [51] makes researchers wonder whether the language model can bring about improved performance in the field of Science.</p>
<p>Domain-Specific Language Model for Science BioGPT [52] is a domain-specific generative language model pre-trained on large-scale biomedical literature, which outperforms previous models on six biomedical-related tasks.Based on case studies, the researchers further demonstrated the advantages of BioGPT in generating fluent descriptions for biomedical terms in biomedical literature.ProGen2 [53] is a protein language model pre-trained on a corpus of more than one billion protein sequences including genome, metagenome, and immune library databases.ProGen2 shows optimal performance in capturing observed evolutionary sequence distributions, generating new protein sequences, and predicting protein fitness without additional fine-tuning.Med-PaLM [54] is a large language model (LLM) designed to provide high-quality answers to medical questions, which is an instruction prompt-tuned version of Flan-PaLM [55] specialized for the medical domain.They reveal limitations of Flan-PaLM in scientific grounding, harm, and bias through evaluation, while Med-PaLM significantly reduces the gap (or even compares favorably) to clinicians on several of these axes, according to both clinicians and lay users.MTL-BERT [56] proposes to use large-scale pre-training, multi-task learning, and SMILES enumeration to alleviate the data sparsity problem.It mines the rich contextual information in SMILES strings through self-supervised pre-training, and then fine-tunes the pre-trained model simultaneously using multiple downstream tasks.At the same time, it combines SMILES enumeration as a data augmentation strategy to increase data diversity.Experimental results show that MTL-BERT can achieve optimal performance on molecular datasets.ChemDFM [5] is the first LLM towards Chemical General Intelligence (CGI), which is trained on 34B tokens from chemical literature, textbooks, and instructions as well as various data from the general domain.It can store, understand, and reason over chemical knowledge and languages while still possessing advanced free-form language comprehension capabilities.Extensive quantitative evaluation shows that ChemDFM can significantly outperform the representative open-sourced LLMs.</p>
<p>General-domain Language Model for Science SciBERT [57] is a pre-trained language model based on the BERT model architecture, which aims to address the lack of high-quality, largescale labeled scientific data.SciBERT uses a large multi-domain scientific publication corpus for pre-training to improve the performance of downstream scientific benchmarks and has achieved state-of-the-art performance on multiple tasks.Galactica [4] is a large language model that can store, combine and reason about scientific knowledge, which is trained on a large scientific corpus of papers, reference material, knowledge bases and many other sources.Galactica outperforms previous models on a range of scientific tasks and sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA.SciGLM [6] is a suite of scientific language models designed to enhance college-level scientific reasoning.It utilizes a self-reflective instruction annotation framework to address data scarcity in the science domain.SciGLM significantly improves upon ChatGLM by effectively handling complex scientific and mathematical problems, all while maintaining strong language understanding capabilities.</p>
<p>Compared to prior works, SciDFM either can achieve a better performance, or is more generalized.</p>
<p>With the utilization of Mixture-of-Experts architecture, SciDFM can better model similarities and differences across different disciplines and modalities and have stronger sophisticated scientific reasoning and understanding capabilities.</p>
<p>Conclusion</p>
<p>In this paper, we introduce SciDFM, a mixture-of-experts LLM able to conduct college-level scientific reasoning and understand molecules and amino acid sequences.We show the pretraining and instruction-tuning process of SciDFM in detail, including data, architecture and hyper-parameters.We conduct evaluation on eight general scientific language understanding and reasoning tasks and two domain-specific tasks.From the results, we show that SciDFM achieves strong performance on general scientific benchmarks such as SciEval and SciQ, and it reaches a SOTA performance on domain-specific benchmarks among models of similar size.We further analyze the expert choices of MoE layers and show that the results of expert selection vary with data from different disciplines and exhibit clustering phenomena related to their relationships.</p>
<p>Figure 1 :
1
Figure 1: Visualization of t-SNE results using data from different domains.</p>
<p>Table 1 :
1
Hyper-parameters of SciDFM
2.1 Pretraining2.1.1 Model ArchitectureParameterValuedim3200n_layers26head_dim100ffn_hidden_dim8640n_heads32n_kv_heads32context_len8192vocab_size32192num_experts8topk_experts2</p>
<p>Table 2 :
2
The training corpus details of SciDFM.
And for the-stack dataset, we only</p>
<p>Table 3 :
3
Details of instruction tuning dataset.
Model①②③④⑤⑥⑦⑧Avg.Llama2-7B27.06 57.00 36.43/46.59 3.94 3.96 26.32 29.84 66.80 32.95Galactica-6.7B46.28 74.20 44.28/61.83 2.80 6.32 30.48 36.46 48.80 38.91Llama2-13B33.88 78.10 56.66/72.35 22.82 3.90 32.68 34.28 77.80 45.45ChatGLM2-6B 54.25 75.80 57.08/73.57 25.09 7.18 27.42 34.21 60.40 45.94Galactica-30B54.24 83.10 57.85/75.04 13.65 8.66 37.71 48.43 58.80 48.35Llama3-8B59.70 90.00 71.16/84.05 5.91 7.00 48.78 52.74 26.60 49.59ChatGLM3-6B 51.13 77.60 60.84/75.97 60.27 23.52 24.59 31.39 51.80 50.53SciGLM-6B61.22 88.70 77.47/86.57 42.23 16.40 42.81 44.94 73.60 59.12ChatGLM3-6Bb 60.34 89.00 78.58/87.37 59.82 22.64 42.73 45.14 74.40 61.96Llama3-8B-it64.91 91.60 76.45/87.33 76.57 26.26 56.48 59.31 72.00 67.44SciDFM(ours)62.48 88.00 64.76/81.48 59.14 27.28 44.54 53.10 78.00 61.56</p>
<p>Table 4 :
4
Main Results on 8 general scientific language understanding and reasoning tasks.ChatGLM3-6Bb stands for ChatGLM3-6B-base, Llama3-8B-it stands for Llama3-8B-Instruct.The columns from ① to ⑧ stand for SciEval, SciQ, ARC, GSM8K, MATH, MedQA, MedMCQA.PubMedQA, respectively, where the results of ARC are shown in the form of ARC_C/ARC_E. .
3.1 Evaluation Setup
[14]uation Tasks Summarization of evaluation tasks are shown in Table5.These evaluation datasets cover a wide range of subjects, including math, chemistry, physics, biology, protein and molecule.•SciEval[14]isa comprehensive and multidisciplinary benchmark designed to assess the scientific research capabilities of Large Language Models.</p>
<p>Table 5 :
5
Overview of evaluation benchmarks.(♢: Chemistry, △: Biology, □: Math, ♡: Physics)
ModelAvg.G Avg.M Avg.BLLaMa2-7B41.773.9540.99Galactica-6.7B56.654.5638.58LLaMa2-13B60.2513.3648.25ChatGLM2-6B65.1816.1440.68Galactica-30B67.5611.1648.31LLaMa3-8B76.236.4642.71ChatGLM3-6B66.3941.9035.93SciGLM-6B78.4929.3253.78ChatGLM3-6B-base 78.8241.2354.09Llama3-8B-Instruct80.0751.4262.60SciDFM(ours)74.1843.2158.55</p>
<p>Table 6 :
6
Average Results of General Science, Math and Biology tasks on scientific language understanding and reasoning tasks.Avg.G, Avg.M and Avg.B stand for average performance on general science, math and biology tasks respectively.</p>
<p>Table 7 :
7
The Results of molecular property prediction tasks on MoleculeNet in AUC-ROC scores.
bace bbbp ClinTox HIV Tox21AvgLLaMa2-13B-chat 26.0 60.345.729.051.742.54GPT4o(0513)62.5 61.551.665.955.259.34Galactica-30B72.7 59.682.275.968.571.78ChemDFM-13B78.4 66.789.973.679.877.68SciDFM(ours)76.4 64.898.571.575.677.36</p>
<p>Table 7
7ModelExact↑ Levenshtein↓ RDK FTS↑ MACCS FTS↑ Morgan FTS↑ Validity↑Description Guided Molecule DesignChatGLM3-6B0.001199.470.1030.1740.0590.236Galactica-6.7B0.00044.1520.1340.2480.0880.992Text+Chem T50.09741.8190.3520.4740.3530.721Mol-Instructions 0.00241.3670.2310.4120.1471.000SciDFM(ours)0.08443.4140.5860.7040.4430.994Reagent PredictionChatGLM3-6B0.000154.730.0440.1440.0460.275Galactica-6.7B0.00035.0210.1560.2570.0970.946Text+Chem T50.23920.4130.7050.7890.6520.762Mol-Instructions 0.04527.2620.3130.5090.2621.000SciDFM(ours)0.19217.5270.4760.5760.4520.999RetrosynthesisChatGLM3-6B0.00059.0620.6360.6950.5700.492Galactica-6.7B0.00030.7600.0360.1270.0510.995Text+Chem T50.00049.3230.0390.1860.0520.313Mol-Instructions 0.04423.1670.2370.3640.2131.000SciDFM(ours)0.6656.450.9160.9370.8880.998
presents the performance of molecular property prediction tasks on MoleculeNet.From the results shown in AUC-ROC scores, we find that SciDFM</p>
<p>Table 8 :
8
The Results of molecular generation tasks on Mol-Instructions.Exact stands for exact matches, and validity stands for valid molecules.RDK, MACCS and Morgan are three kinds of molecular fingerprints.
ModelProtein Function Functional Description Catalytic Activity Domain/MotifGPT4o(0513)0.060.050.070.06ChatGLM0.150.140.130.10Mol-Instruction0.430.440.520.46SciDFM(ours)0.600.720.760.55</p>
<p>Table 9 :
9
The Results of protein understanding tasks on Mol-Instructions.All tasks are evaluated in ROUGE-L score.</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jeanbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, arXiv:2403.055302024arXiv preprint</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Hongshen Xu, Zichen Zhu, Su Zhu, Shuai Fan, Guodong Shen, arXiv:2401.14818Dialogue foundation model for chemistry. 2024arXiv preprint</p>
<p>Sciglm: Training scientific language models with self-reflective instruction annotation and tuning. Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, Jie Tang, 2024</p>
<p>Leveraging biomolecule and natural language through multi-modal learning: A survey. Qizhi Pei, Lijun Wu, Kaiyuan Gao, Jinhua Zhu, Yue Wang, Zun Wang, Tao Qin, Rui Yan, arXiv:2403.015282024arXiv preprint</p>
<p>Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, Qi Tian, arXiv:2211.02556Pangu-weather: A 3d high-resolution model for fast and accurate global weather forecast. 2022arXiv preprint</p>
<p>Zhouhan Lin, Cheng Deng, Le Zhou, Tianhang Zhang, Yi Xu, Yutong Xu, Zhongmou He, Yuanyuan Shi, Beiya Dai, Yunchong Song, arXiv:2401.00434A scientific large language model in geoscience. 2023arXiv preprint</p>
<p>Gshard: Scaling giant models with conditional computation and automatic sharding. Dmitry Lepikhin, Hyoukjoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen, arXiv:2006.166682020arXiv preprint</p>
<p>. Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.040882024Mixtral of experts. arXiv preprint</p>
<p>Deepspeed-moe: Advancing mixture-ofexperts inference and training to power next-generation ai scale. Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He, International conference on machine learning. PMLR2022</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Scieval: A multi-level large language model evaluation benchmark for scientific research. Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, Kai Yu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Crowdsourcing multiple choice science questions. Johannes Welbl, Nelson F Liu, Matt Gardner, arXiv:1707.062092017arXiv preprint</p>
<p>Mol-instructions: A large-scale biomolecular instruction dataset for large language models. Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, Huajun Chen, ICLR. OpenReview.net2024</p>
<p>Openllama: An open reproduction of llama. Xinyang Geng, Hao Liu, May 2023</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Mcaleer, Albert Q Jiang, Jia Deng, Stella Biderman, Sean Welleck, Llemma: An open language model for mathematics. 2023</p>
<p>Zhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang, Zehai He, Yuyi Guo, Jinfeng Bai, Jie Tang, arXiv:2309.03241Gpt can solve mathematical problems without a calculator. 2023arXiv preprint</p>
<p>Zengzhi Wang, Rui Xia, Pengfei Liu, arXiv:2312.17120Generative ai for math: Part i -mathpile: A billion-token-scale pretraining corpus for math. 2023arXiv preprint</p>
<p>Leandro von Werra, and Harm de Vries. The stack: 3 tb of permissively licensed source code. Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, 2022Preprint</p>
<p>Slimpajama: A 627b token cleaned and deduplicated version of redpajama. Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, Nolan Dey, 2023</p>
<p>Redpajama: An open source recipe to reproduce llama training dataset. 2023Together Computer</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, Metamath, arXiv:2309.12284Bootstrap your own mathematical questions for large language models. 2023arXiv preprint</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, arXiv:2309.05653Mammoth: Building math generalist models through hybrid instruction tuning. 2023arXiv preprint</p>
<p>Orca-math: Unlocking the potential of slms in grade school math. Arindam Mitra, Hamed Khanpour, Corby Rosset, Ahmed Awadallah, 2024</p>
<p>Deeploc 2.0: multi-label subcellular localization prediction using protein language models. Vineet Thumuluri, José Juan Almagro Armenteros, Alexander Rosenberg Johansen, Henrik Nielsen, Ole Winther, Nucleic Acids Research. 50W12022</p>
<p>An overview of the bioasq large-scale biomedical semantic indexing and question answering competition. George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Dirk Michael R Alvers, Anastasia Weissenborn, Sergios Krithara, Dimitris Petridis, Polychronopoulos, BMC bioinformatics. 1611382015</p>
<p>Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering. Ankit Pal, Logesh Kumar Umapathi, Malaikannan Sankarasubbu, Proceedings of the Conference on Health, Inference, and Learning. Gerardo Flores, George H Chen, Tom Pollard, Joyce C Ho, Tristan Naumann, the Conference on Health, Inference, and LearningPMLRApr 2022174</p>
<p>What disease does this patient have? a large-scale open domain question answering dataset from medical exams. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, Peter Szolovits, Applied Sciences. 111464212021</p>
<p>Pubmedqa: A dataset for biomedical research question answering. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, Xinghua Lu, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Camel: Communicative agents for "mind" exploration of large scale language model society. Guohao Li, Hasan Abed, Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, 2023</p>
<p>Xiang Yue, Tuney Zheng, Ge Zhang, Wenhu Chen, arXiv:2405.03548Mammoth2: Scaling instructions from the web. 2024arXiv preprint</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>Less is more for alignment. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Advances in Neural Information Processing Systems. 202436</p>
<p>Reasoning over paragraph effects in situations. Kevin Lin, Oyvind Tafjord, Peter Clark, Matt Gardner, MRQA@EMNLP. 2019</p>
<p>Qasc: A dataset for question answering via sentence composition. Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, Ashish Sabharwal, arXiv:1910.11473v22020</p>
<p>Can a suit of armor conduct electricity? a new dataset for open book question answering. Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal, EMNLP. 2018</p>
<p>Free dolly: Introducing the world's first truly open instruction-tuned llm. Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, Reynold Xin, Company Blog of Databricks. 2023</p>
<p>Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, Slimorca dedup: A deduplicated subset of slimorca. 2023and Nathan Hoos</p>
<p>Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo. Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, Andriy Mulyar, 2023GitHub</p>
<p>Ilya Loshchilov, Frank Hutter, arXiv:1711.05101Decoupled weight decay regularization. 2017arXiv preprint</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, arXiv:1803.054572018arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Moleculenet: a benchmark for molecular machine learning. Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, S Aneesh, Karl Pappu, Vijay Leswing, Pande, Chemical science. 922018</p>
<p>Glm-130b: An open bilingual pre-trained model. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, arXiv:2210.024142022arXiv preprint</p>
<p>Glm: General language model pretraining with autoregressive blank infilling. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Visualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 9112008</p>
<p>Jacob Devlin, Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. 2018arXiv preprint</p>
<p>Improving language understanding by generative pre-training. Alec Radford, 2018</p>
<p>Biogpt: generative pre-trained transformer for biomedical text generation and mining. Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, Tie-Yan Liu, Briefings in bioinformatics. 2364092022</p>
<p>Progen2: exploring the boundaries of protein language models. Erik Nijkamp, Jeffrey A Ruffolo, Eli N Weinstein, Nikhil Naik, Ali Madani, Cell systems. 14112023</p>
<p>Large language models encode clinical knowledge. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Nature. 62079722023</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Journal of Machine Learning Research. 242402023</p>
<p>Pushing the boundaries of molecular property prediction for drug discovery with multitask learning bert enhanced by smiles enumeration. Xiao-Chen Zhang, Cheng-Kun Wu, Jia-Cai Yi, Xiang-Xiang Zeng, Can-Qun Yang, Ai-Ping Lu, Ting-Jun Hou, Dong-Sheng Cao, Research. 42022. 2022</p>
<p>Scibert: A pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, arXiv:1903.106762019arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
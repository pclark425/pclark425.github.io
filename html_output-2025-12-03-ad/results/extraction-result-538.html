<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-538 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-538</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-538</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-e3cd9f01f87a601b274b4ef6513a84c8cde03214</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e3cd9f01f87a601b274b4ef6513a84c8cde03214" target="_blank">Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work provides two new data resources on multiple spatial language processing tasks and shows pretraining with automatically generated data significantly improves the SOTA results on several SQA and SPRL benchmarks, particularly when the training data in the target domain is small.</p>
                <p><strong>Paper Abstract:</strong> Recent research shows synthetic data as a source of supervision helps pretrained language models (PLM) transfer learning to new target tasks/domains. However, this idea is less explored for spatial language. We provide two new data resources on multiple spatial language processing tasks. The first dataset is synthesized for transfer learning on spatial question answering (SQA) and spatial role labeling (SpRL). Compared to previous SQA datasets, we include a larger variety of spatial relation types and spatial expressions. Our data generation process is easily extendable with new spatial expression lexicons. The second one is a real-world SQA dataset with human-generated questions built on an existing corpus with SPRL annotations. This dataset can be used to evaluate spatial language processing models in realistic situations. We show pretraining with automatically generated data significantly improves the SOTA results on several SQA and SPRL benchmarks, particularly when the training data in the target domain is small.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e538.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e538.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT (SQA experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT: Pretrained Transformer used for Spatial Question Answering experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained bidirectional transformer (BERT) fine-tuned for Spatial Question Answering (SQA) in this paper; further pretrained on synthetic corpora (SPARTUN, SPARTUN-S, SPARTQA-AUTO, StepGame) to probe how text-only PLMs encode and use spatial and object-relational knowledge for multi-hop spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BERT: Pre-training of deep bidirectional transformers for language understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard BERT-base (used via HuggingFace) with classification heads: binary classifiers for multi-label FR tasks and single multi-class classifier for single-answer tasks; trained by (1) further pretraining on synthetic spatial corpora (SPARTUN variants, StepGame, SPARTQA-AUTO) and (2) fine-tuning on target SQA datasets. Uses [CLS] token output to drive classification; focal loss for SQA.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Spatial Question Answering (SQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a textual scene description (story) the model must answer Yes/No (YN) or Find-Relations (FR) questions about spatial relationships between entities; many questions require multi-hop (indirect) spatial inference over chains of relations encoded in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning / reasoning (textual spatial QA)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on text corpora (general BERT pretraining) + further supervised pretraining on synthetic spatial corpora (SPARTUN, SPARTUN-S, SPARTQA-AUTO, StepGame) and fine-tuning on target SQA datasets</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>further pretraining (transfer learning) on synthetic corpora, then supervised fine-tuning; evaluation on held-out SQA benchmarks; post-processing constraints applied to multi-label outputs</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>implicit in model weights (distributed representations learned by BERT) induced from textual supervision; training targets are natural-language stories paired with labelled relation sets (triplet-based supporting facts); no explicit learned spatial maps or sensory embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (percentage correct) on YN and FR question sets; also per-hop accuracy on StepGame (k-step reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Notable results from the paper: fine-tuning BERT with SPARTUN improved SQA transfer: on RESQ, BERT (no synsup) = 57.37% accuracy; BERT + SPARTUN = 63.60% (absolute +6.23%). On StepGame multi-hop benchmark, BERT+SPARTUN has the best performance for k>3 (examples: for k=10 BERT baseline 33.62% vs BERT+SPARTUN 45.68% per Table 6); on bAbI small-sample experiments SPARTUN pretraining improved some settings (see Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>BERT learned to use multi-hop textual cues when pretrained on broad-coverage synthetic corpora: it improved inference over indirect relations (k>3) and transferred better to real-world textual SQA (RESQ, SPARTQA-HUMAN) when trained on SPARTUN; the model uses entity descriptions and relation expressions present in text to answer relational queries without any perceptual input.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Performance degrades with increasing number of reasoning hops (accuracy drops steadily as k increases); models still fall far short of human accuracy (human ~90% on several benchmarks); YN questions in small human datasets remain particularly challenging (models sometimes cannot beat simple majority baselines); models lack commonsense/world knowledge grounding for some RESQ questions that require implicit physical/common-sense inference.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Baselines include Majority Baseline (MB) and BERT without synthetic pretraining. Example: RESQ MB=50.21%, BERT (no synsup)=57.37%, BERT+SPARTUN=63.60%. Other synthetic pretraining comparators: SPARTQA-AUTO and StepGame; SPARTUN generally outperforms them for transfer to realistic SQA.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Indirect ablation via comparing different synthetic pretraining corpora: SPARTUN vs SPARTUN-S (less expression coverage) vs SPARTQA-AUTO vs StepGame. Removing broad lexical/relation coverage (SPARTUN-S or SPARTQA-AUTO) reduces transfer performance, especially on realistic/resampled benchmarks and on multi-hop (k>3) questions. No explicit architectural ablation of BERT layers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Text-only PLMs (BERT) can internalize substantial spatial and object-relational knowledge via supervised pretraining on large, synthetically generated spatial text; broader coverage of relation types and lexical variants (as in SPARTUN) yields better transfer to realistic textual spatial QA and improves multi-hop inference, but such models still degrade on many-step reasoning and lack grounding/commonsense that vision or embodied sensors would provide.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e538.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e538.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT (SPRL experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT-based pipeline for Spatial Role Labeling (SRol and SRel)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline using BERT for two SPRL sub-tasks: (1) spatial role extraction (BIO-tagging for Trajector/Landmark/Indicator) and (2) spatial relation extraction/classification (triplet classification + spatial-type classifier), with further pretraining on synthetic corpora (SPARTUN) to probe object-relational encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-module pipeline: Module SRol performs BIO token tagging (O, B-entity, I-entity, B-indicator, I-indicator) using BERT token representations + softmax; Module SRel constructs candidate triplets (tr, sp, lm), marks role positions via segment embeddings, passes concatenated input to BERT and uses [CLS] through MLP(s) to (a) predict whether the triplet is a relation and (b) predict spatial relation type (auxiliary multi-class task). Joint loss over both components used for training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Spatial Role Labeling (SPRL) and Spatial Relation Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given sentences describing spatial configurations, identify and classify spatial roles (Trajector, Landmark, Indicator) and the spatial relation type for each triplet (e.g., LEFT, NTPP, EC), using only textual input; evaluated on MSPRL and SPARTQA-HUMAN-derived SPRL annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object-relational extraction / relation classification</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational + spatial</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>fine-tuning on annotated corpora (MSPRL) and further pretraining on synthetic SPARTUN and other synthetic corpora for transfer</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>supervised fine-tuning; further pretraining on synthetic data for transfer; BIO tagging and triplet classification elicit role and relation knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>implicit in weights (token and segment embedding encodings), plus explicit training targets in the form of annotated triplets and spatial-type labels derived from synthetic scene graphs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Macro F1 (mean of class F1s) for SRol; F1 for spatial relation extraction for SRel</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>SRol: on MSPRL baseline SRol (no synsup) = 88.59 (Macro F1); SRol + SPARTUN = 88.03 (no big gain for MSPRL). On SPARTQA-HUMAN SRol: baseline 55.8 -> +SPARTUN = 72.43 (substantial gain). SRel: on MSPRL baseline 69.12 -> +SPARTUN = 71.23 (≈+2.1 F1); on SPARTQA-HUMAN stories (S) 48.58 -> 61.53 and questions (Q) 49.46 -> 63.22 with SPARTUN pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Pretraining on wide-coverage synthetic data (SPARTUN) substantially improved object-relational extraction on realistic/human datasets with diverse lexicalizations (SPARTQA-HUMAN), especially for relation typing and triplet prediction; BERT encodes role-bound positional cues (via segment embeddings) and lexical relation variants into weights and can generalize to unseen realistic textual descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Limited/no improvement on SRol for MSPRL suggests ceiling effects or domain mismatch; relation-extraction performance still below human-level and sensitive to limited lexical coverage in synthetic corpora (SPARTQA-AUTO performed worse than no-synsup in some cases). Models still falter when question/story annotations are incomplete (questions have missing roles) or when commonsense grounding is required.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to prior SOTA R-Inf (which used multimodal resources and global inference) R-Inf on MSPRL = 80.92 (SRol reference) and 68.78 (SRel). BERT-based SRol and SRel outperform R-Inf on MSPRL when using PLMs; SPARTUN further improves SRel over baselines (e.g., 69.12 -> 71.23).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Comparing different synthetic pretraining corpora acts as an ablation: SPARTUN (broad coverage) yields larger gains on SPARTQA-HUMAN than SPARTQA-AUTO or SPARTUN-S; removing broad expression coverage reduces transferability. No per-component removals of the pipeline are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Language models can learn object-relational role and relation-type distinctions from text-only supervision when supplied with synthetic corpora that cover diverse relation formalisms and lexicalizations; this enables better transfer to human-generated SPRL corpora, but improvements depend strongly on the diversity of relation expressions used in pretraining and remain limited without multimodal grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e538.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e538.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPARTUN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SPARTUN: Synthetic corpus for Spatial Reasoning and Role Labeling for Text Understanding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic dataset introduced in this paper that generates textual scene descriptions, SPRL annotations, and multi-hop SQA questions from NLVR scene graphs augmented with extra relation assignments and a Prolog-based spatial reasoner; designed to provide broad coverage of spatial relation types and lexical expressions for transfer learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SPARTUN (dataset / synthetic supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Automatically generated dataset pipeline: compute scene graphs from NLVR images (coordinates), augment relations (directional, RCC8, proper-parts), validate and infer indirect relations via a Prolog Horn-clause reasoner, select multi-hop paths as question triplets, generate stories and questions with a CFG and lexicalized relation vocabulary, and emit SPRL annotations (trajector/landmark/indicator/triplets) and supporting facts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Synthetic pretraining corpus for Spatial QA and SPRL</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provides labeled training data for SQA (YN & FR) and SPRL tasks: stories (avg. 8 sentences) describing scenes, questions that require multi-hop reasoning (paths length>1), annotated supporting facts and number of reasoning steps, and SPRL triplets for role and relation extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>data-generation / supervised pretraining resource for textual spatial reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (explicit symbolic relations converted to textual expressions)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>explicit symbolic scene graphs (from NLVR) + logic-based reasoner (Prolog) used to compute implicit relations and generate labeled text; lexical resources (spatial lexicons) for varied expressions</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>dataset generation used to supervise PLMs via further pretraining (transfer learning) and to provide gold labels / supporting facts for multi-hop questions and SPRL annotations</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit symbolic scene graphs / directed edges (triplets) and Horn-clause rules; paired natural-language stories and question-answer labels; SPRL triplet annotations stored as structured triplets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>dataset utility is measured by downstream improvement in PLM performance (accuracy, F1) on target SQA and SPRL tasks after pretraining on SPARTUN</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Using SPARTUN for further pretraining improved downstream results: e.g., on RESQ accuracy improved from 57.37% (BERT baseline) to 63.60% (BERT + SPARTUN); on SPARTQA-HUMAN FR accuracy and YN accuracy improved compared to SPARTQA-AUTO and other synthetic corpora (paper reports ~+2.6% FR and +9% YN vs SPARTQA-AUTO). On StepGame, SPARTUN yielded best results for k>3 reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>SPARTUN's broad coverage of relation types and lexical expressions helps PLMs generalize to realistic textual spatial tasks and boosts multi-hop relational inference; providing supporting facts and step counts helps train models for multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>SPARTUN still omits many linguistic phenomena (metaphors, implicit meanings) and is limited to explicit spatial semantics and verbs/prepositions; models pretrained on SPARTUN still lack commonsense/world grounding needed for some real-world RESQ items and fail as reasoning depth increases.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to pretraining on SPARTQA-AUTO, SPARTUN gives better transfer; also performs better than pretraining on StepGame for many realistic tasks (see tables). Compared to no synthetic supervision, SPARTUN yields consistent improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Comparisons across dataset variants (SPARTUN vs SPARTUN-S (single expression per relation) vs SPARTUN-Clock vs SPARTQA-AUTO vs StepGame) act as ablations: reducing lexical/regional coverage (SPARTUN-S) decreases performance on realistic transfer, especially for YN tasks and deeper reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A synthetic corpus that encodes explicit spatial logic, diverse lexicalization, and annotated multi-hop reasoning paths can teach PLMs implicit object-relational and spatial patterns sufficient to improve text-only spatial inference; breadth of relation types and expression variability are critical for transfer—but symbolic grounding or multimodal input would still be needed for embodied tasks requiring perception or commonsense physical knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e538.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e538.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logic-based spatial reasoner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prolog Horn-clause spatial reasoner (RCC8 + Directional + Proper-part combinations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Prolog-implemented logical reasoner encoding Horn-clause rules (inverse, symmetry, transitivity, combination) across multiple spatial formalisms (directional, distance, RCC8 topological, proper-part relations) used to validate paths, infer indirect relations, and compute supporting facts during dataset generation and answer computation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Prolog spatial reasoner</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An explicit symbolic reasoner implementing spatial axioms and combination rules as Horn clauses (including Dir relations like LEFT/RIGHT, RCC8 topological relations DC/EC/PO/EQ/TPP/NTPP/TPPI/NTPPI, and combination/transitivity rules) that computes implied relations across scene-graph edges and validates candidate multi-hop inference paths.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Symbolic inference for dataset generation and answer computation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given scene-graph triplets (facts) the reasoner queries implied relations between entity pairs, verifies validity of multi-edge paths, returns inferred relations to be used as question answers, and marks supporting facts and reasoning-step counts for each question.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>symbolic spatial reasoning / inference (offline dataset generation & oracle verification)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial (formal symbolic relations) + object-relational (triplet structure)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>hand-coded logical axioms and rules derived from spatial calculi (RCC, directional rulesets) and mapped lexical forms</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>deterministic symbolic inference (Prolog Horn clauses) executed over explicit scene graph triplets to produce labels and to validate indirect relations</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit symbolic predicates and Horn clauses representing spatial relations and meta-rules (inverse, transitivity, combination); operates over graph-structured triplets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>not reported as predictive model evaluation; used as oracle during dataset generation (correctness implicit in rule set and constraints)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Used as an oracle to generate and validate dataset items and answers; no numeric evaluation metrics reported for reasoner itself in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Correctly computes indirect relations via chained Horn-clause rules (e.g., NTPP(a,b), FRONT(b,c), TPPI(c,d) => FRONT(a,d)), enforces spatial consistency (inverse/symmetry constraints), and supplies supporting facts and reasoning-step counts; enables creation of multi-hop reasoning questions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Reasoner limited to explicit spatial calculi and codified rules; cannot model metaphoric/implicit uses or commonsense physical knowledge (authors note they exclude metaphoric/implicit meanings), and depends on correctness/coverage of the hand-authored rule set.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not compared to other reasoners in quantitative terms here; authors claim novelty in combining multiple formalisms for textual SQA dataset generation relative to prior spatial tools.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No explicit ablation of the reasoner itself reported; paper shows that using the reasoner to generate richer multi-hop questions and supporting facts (vs. simpler synthetic pipelines) yields better downstream PLM transfer when paired with broad lexical coverage in SPARTUN.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit symbolic spatial reasoning can generate high-quality supervision (supporting facts, step counts, indirect relations) that, when converted to text and used for PLM pretraining, helps PLMs internalize multi-hop spatial inferences in a text-only setting; however, symbolic rules remain necessary to produce reliable labels and cannot substitute for perceptual grounding required for embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SPARTQA: A textual question answering benchmark for spatial reasoning <em>(Rating: 2)</em></li>
                <li>Touchdown: Natural language navigation and spatial reasoning in visual street environments <em>(Rating: 2)</em></li>
                <li>Spatial representation and reasoning for human-robot collaboration <em>(Rating: 1)</em></li>
                <li>Spatial reasoning from natural language instructions for robot manipulation <em>(Rating: 2)</em></li>
                <li>A spatial logic based on regions and connection <em>(Rating: 2)</em></li>
                <li>Explicit object relation alignment for vision and language navigation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-538",
    "paper_id": "paper-e3cd9f01f87a601b274b4ef6513a84c8cde03214",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "BERT (SQA experiments)",
            "name_full": "BERT: Pretrained Transformer used for Spatial Question Answering experiments",
            "brief_description": "A pretrained bidirectional transformer (BERT) fine-tuned for Spatial Question Answering (SQA) in this paper; further pretrained on synthetic corpora (SPARTUN, SPARTUN-S, SPARTQA-AUTO, StepGame) to probe how text-only PLMs encode and use spatial and object-relational knowledge for multi-hop spatial reasoning.",
            "citation_title": "BERT: Pre-training of deep bidirectional transformers for language understanding.",
            "mention_or_use": "use",
            "model_name": "BERT",
            "model_size": null,
            "model_description": "Standard BERT-base (used via HuggingFace) with classification heads: binary classifiers for multi-label FR tasks and single multi-class classifier for single-answer tasks; trained by (1) further pretraining on synthetic spatial corpora (SPARTUN variants, StepGame, SPARTQA-AUTO) and (2) fine-tuning on target SQA datasets. Uses [CLS] token output to drive classification; focal loss for SQA.",
            "task_name": "Spatial Question Answering (SQA)",
            "task_description": "Given a textual scene description (story) the model must answer Yes/No (YN) or Find-Relations (FR) questions about spatial relationships between entities; many questions require multi-hop (indirect) spatial inference over chains of relations encoded in the text.",
            "task_type": "multi-step planning / reasoning (textual spatial QA)",
            "knowledge_type": "spatial + object-relational",
            "knowledge_source": "pre-training on text corpora (general BERT pretraining) + further supervised pretraining on synthetic spatial corpora (SPARTUN, SPARTUN-S, SPARTQA-AUTO, StepGame) and fine-tuning on target SQA datasets",
            "has_direct_sensory_input": false,
            "elicitation_method": "further pretraining (transfer learning) on synthetic corpora, then supervised fine-tuning; evaluation on held-out SQA benchmarks; post-processing constraints applied to multi-label outputs",
            "knowledge_representation": "implicit in model weights (distributed representations learned by BERT) induced from textual supervision; training targets are natural-language stories paired with labelled relation sets (triplet-based supporting facts); no explicit learned spatial maps or sensory embeddings",
            "performance_metric": "accuracy (percentage correct) on YN and FR question sets; also per-hop accuracy on StepGame (k-step reasoning)",
            "performance_result": "Notable results from the paper: fine-tuning BERT with SPARTUN improved SQA transfer: on RESQ, BERT (no synsup) = 57.37% accuracy; BERT + SPARTUN = 63.60% (absolute +6.23%). On StepGame multi-hop benchmark, BERT+SPARTUN has the best performance for k&gt;3 (examples: for k=10 BERT baseline 33.62% vs BERT+SPARTUN 45.68% per Table 6); on bAbI small-sample experiments SPARTUN pretraining improved some settings (see Table 4).",
            "success_patterns": "BERT learned to use multi-hop textual cues when pretrained on broad-coverage synthetic corpora: it improved inference over indirect relations (k&gt;3) and transferred better to real-world textual SQA (RESQ, SPARTQA-HUMAN) when trained on SPARTUN; the model uses entity descriptions and relation expressions present in text to answer relational queries without any perceptual input.",
            "failure_patterns": "Performance degrades with increasing number of reasoning hops (accuracy drops steadily as k increases); models still fall far short of human accuracy (human ~90% on several benchmarks); YN questions in small human datasets remain particularly challenging (models sometimes cannot beat simple majority baselines); models lack commonsense/world knowledge grounding for some RESQ questions that require implicit physical/common-sense inference.",
            "baseline_comparison": "Baselines include Majority Baseline (MB) and BERT without synthetic pretraining. Example: RESQ MB=50.21%, BERT (no synsup)=57.37%, BERT+SPARTUN=63.60%. Other synthetic pretraining comparators: SPARTQA-AUTO and StepGame; SPARTUN generally outperforms them for transfer to realistic SQA.",
            "ablation_results": "Indirect ablation via comparing different synthetic pretraining corpora: SPARTUN vs SPARTUN-S (less expression coverage) vs SPARTQA-AUTO vs StepGame. Removing broad lexical/relation coverage (SPARTUN-S or SPARTQA-AUTO) reduces transfer performance, especially on realistic/resampled benchmarks and on multi-hop (k&gt;3) questions. No explicit architectural ablation of BERT layers reported.",
            "key_findings": "Text-only PLMs (BERT) can internalize substantial spatial and object-relational knowledge via supervised pretraining on large, synthetically generated spatial text; broader coverage of relation types and lexical variants (as in SPARTUN) yields better transfer to realistic textual spatial QA and improves multi-hop inference, but such models still degrade on many-step reasoning and lack grounding/commonsense that vision or embodied sensors would provide.",
            "uuid": "e538.0",
            "source_info": {
                "paper_title": "Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "BERT (SPRL experiments)",
            "name_full": "BERT-based pipeline for Spatial Role Labeling (SRol and SRel)",
            "brief_description": "A pipeline using BERT for two SPRL sub-tasks: (1) spatial role extraction (BIO-tagging for Trajector/Landmark/Indicator) and (2) spatial relation extraction/classification (triplet classification + spatial-type classifier), with further pretraining on synthetic corpora (SPARTUN) to probe object-relational encoding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT",
            "model_size": null,
            "model_description": "Two-module pipeline: Module SRol performs BIO token tagging (O, B-entity, I-entity, B-indicator, I-indicator) using BERT token representations + softmax; Module SRel constructs candidate triplets (tr, sp, lm), marks role positions via segment embeddings, passes concatenated input to BERT and uses [CLS] through MLP(s) to (a) predict whether the triplet is a relation and (b) predict spatial relation type (auxiliary multi-class task). Joint loss over both components used for training.",
            "task_name": "Spatial Role Labeling (SPRL) and Spatial Relation Extraction",
            "task_description": "Given sentences describing spatial configurations, identify and classify spatial roles (Trajector, Landmark, Indicator) and the spatial relation type for each triplet (e.g., LEFT, NTPP, EC), using only textual input; evaluated on MSPRL and SPARTQA-HUMAN-derived SPRL annotations.",
            "task_type": "object-relational extraction / relation classification",
            "knowledge_type": "object-relational + spatial",
            "knowledge_source": "fine-tuning on annotated corpora (MSPRL) and further pretraining on synthetic SPARTUN and other synthetic corpora for transfer",
            "has_direct_sensory_input": false,
            "elicitation_method": "supervised fine-tuning; further pretraining on synthetic data for transfer; BIO tagging and triplet classification elicit role and relation knowledge",
            "knowledge_representation": "implicit in weights (token and segment embedding encodings), plus explicit training targets in the form of annotated triplets and spatial-type labels derived from synthetic scene graphs",
            "performance_metric": "Macro F1 (mean of class F1s) for SRol; F1 for spatial relation extraction for SRel",
            "performance_result": "SRol: on MSPRL baseline SRol (no synsup) = 88.59 (Macro F1); SRol + SPARTUN = 88.03 (no big gain for MSPRL). On SPARTQA-HUMAN SRol: baseline 55.8 -&gt; +SPARTUN = 72.43 (substantial gain). SRel: on MSPRL baseline 69.12 -&gt; +SPARTUN = 71.23 (≈+2.1 F1); on SPARTQA-HUMAN stories (S) 48.58 -&gt; 61.53 and questions (Q) 49.46 -&gt; 63.22 with SPARTUN pretraining.",
            "success_patterns": "Pretraining on wide-coverage synthetic data (SPARTUN) substantially improved object-relational extraction on realistic/human datasets with diverse lexicalizations (SPARTQA-HUMAN), especially for relation typing and triplet prediction; BERT encodes role-bound positional cues (via segment embeddings) and lexical relation variants into weights and can generalize to unseen realistic textual descriptions.",
            "failure_patterns": "Limited/no improvement on SRol for MSPRL suggests ceiling effects or domain mismatch; relation-extraction performance still below human-level and sensitive to limited lexical coverage in synthetic corpora (SPARTQA-AUTO performed worse than no-synsup in some cases). Models still falter when question/story annotations are incomplete (questions have missing roles) or when commonsense grounding is required.",
            "baseline_comparison": "Compared to prior SOTA R-Inf (which used multimodal resources and global inference) R-Inf on MSPRL = 80.92 (SRol reference) and 68.78 (SRel). BERT-based SRol and SRel outperform R-Inf on MSPRL when using PLMs; SPARTUN further improves SRel over baselines (e.g., 69.12 -&gt; 71.23).",
            "ablation_results": "Comparing different synthetic pretraining corpora acts as an ablation: SPARTUN (broad coverage) yields larger gains on SPARTQA-HUMAN than SPARTQA-AUTO or SPARTUN-S; removing broad expression coverage reduces transferability. No per-component removals of the pipeline are reported.",
            "key_findings": "Language models can learn object-relational role and relation-type distinctions from text-only supervision when supplied with synthetic corpora that cover diverse relation formalisms and lexicalizations; this enables better transfer to human-generated SPRL corpora, but improvements depend strongly on the diversity of relation expressions used in pretraining and remain limited without multimodal grounding.",
            "uuid": "e538.1",
            "source_info": {
                "paper_title": "Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "SPARTUN",
            "name_full": "SPARTUN: Synthetic corpus for Spatial Reasoning and Role Labeling for Text Understanding",
            "brief_description": "A synthetic dataset introduced in this paper that generates textual scene descriptions, SPRL annotations, and multi-hop SQA questions from NLVR scene graphs augmented with extra relation assignments and a Prolog-based spatial reasoner; designed to provide broad coverage of spatial relation types and lexical expressions for transfer learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SPARTUN (dataset / synthetic supervision)",
            "model_size": null,
            "model_description": "Automatically generated dataset pipeline: compute scene graphs from NLVR images (coordinates), augment relations (directional, RCC8, proper-parts), validate and infer indirect relations via a Prolog Horn-clause reasoner, select multi-hop paths as question triplets, generate stories and questions with a CFG and lexicalized relation vocabulary, and emit SPRL annotations (trajector/landmark/indicator/triplets) and supporting facts.",
            "task_name": "Synthetic pretraining corpus for Spatial QA and SPRL",
            "task_description": "Provides labeled training data for SQA (YN & FR) and SPRL tasks: stories (avg. 8 sentences) describing scenes, questions that require multi-hop reasoning (paths length&gt;1), annotated supporting facts and number of reasoning steps, and SPRL triplets for role and relation extraction.",
            "task_type": "data-generation / supervised pretraining resource for textual spatial reasoning",
            "knowledge_type": "spatial + object-relational (explicit symbolic relations converted to textual expressions)",
            "knowledge_source": "explicit symbolic scene graphs (from NLVR) + logic-based reasoner (Prolog) used to compute implicit relations and generate labeled text; lexical resources (spatial lexicons) for varied expressions",
            "has_direct_sensory_input": false,
            "elicitation_method": "dataset generation used to supervise PLMs via further pretraining (transfer learning) and to provide gold labels / supporting facts for multi-hop questions and SPRL annotations",
            "knowledge_representation": "explicit symbolic scene graphs / directed edges (triplets) and Horn-clause rules; paired natural-language stories and question-answer labels; SPRL triplet annotations stored as structured triplets",
            "performance_metric": "dataset utility is measured by downstream improvement in PLM performance (accuracy, F1) on target SQA and SPRL tasks after pretraining on SPARTUN",
            "performance_result": "Using SPARTUN for further pretraining improved downstream results: e.g., on RESQ accuracy improved from 57.37% (BERT baseline) to 63.60% (BERT + SPARTUN); on SPARTQA-HUMAN FR accuracy and YN accuracy improved compared to SPARTQA-AUTO and other synthetic corpora (paper reports ~+2.6% FR and +9% YN vs SPARTQA-AUTO). On StepGame, SPARTUN yielded best results for k&gt;3 reasoning steps.",
            "success_patterns": "SPARTUN's broad coverage of relation types and lexical expressions helps PLMs generalize to realistic textual spatial tasks and boosts multi-hop relational inference; providing supporting facts and step counts helps train models for multi-step reasoning.",
            "failure_patterns": "SPARTUN still omits many linguistic phenomena (metaphors, implicit meanings) and is limited to explicit spatial semantics and verbs/prepositions; models pretrained on SPARTUN still lack commonsense/world grounding needed for some real-world RESQ items and fail as reasoning depth increases.",
            "baseline_comparison": "Compared to pretraining on SPARTQA-AUTO, SPARTUN gives better transfer; also performs better than pretraining on StepGame for many realistic tasks (see tables). Compared to no synthetic supervision, SPARTUN yields consistent improvements.",
            "ablation_results": "Comparisons across dataset variants (SPARTUN vs SPARTUN-S (single expression per relation) vs SPARTUN-Clock vs SPARTQA-AUTO vs StepGame) act as ablations: reducing lexical/regional coverage (SPARTUN-S) decreases performance on realistic transfer, especially for YN tasks and deeper reasoning.",
            "key_findings": "A synthetic corpus that encodes explicit spatial logic, diverse lexicalization, and annotated multi-hop reasoning paths can teach PLMs implicit object-relational and spatial patterns sufficient to improve text-only spatial inference; breadth of relation types and expression variability are critical for transfer—but symbolic grounding or multimodal input would still be needed for embodied tasks requiring perception or commonsense physical knowledge.",
            "uuid": "e538.2",
            "source_info": {
                "paper_title": "Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Logic-based spatial reasoner",
            "name_full": "Prolog Horn-clause spatial reasoner (RCC8 + Directional + Proper-part combinations)",
            "brief_description": "A Prolog-implemented logical reasoner encoding Horn-clause rules (inverse, symmetry, transitivity, combination) across multiple spatial formalisms (directional, distance, RCC8 topological, proper-part relations) used to validate paths, infer indirect relations, and compute supporting facts during dataset generation and answer computation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Prolog spatial reasoner",
            "model_size": null,
            "model_description": "An explicit symbolic reasoner implementing spatial axioms and combination rules as Horn clauses (including Dir relations like LEFT/RIGHT, RCC8 topological relations DC/EC/PO/EQ/TPP/NTPP/TPPI/NTPPI, and combination/transitivity rules) that computes implied relations across scene-graph edges and validates candidate multi-hop inference paths.",
            "task_name": "Symbolic inference for dataset generation and answer computation",
            "task_description": "Given scene-graph triplets (facts) the reasoner queries implied relations between entity pairs, verifies validity of multi-edge paths, returns inferred relations to be used as question answers, and marks supporting facts and reasoning-step counts for each question.",
            "task_type": "symbolic spatial reasoning / inference (offline dataset generation & oracle verification)",
            "knowledge_type": "spatial (formal symbolic relations) + object-relational (triplet structure)",
            "knowledge_source": "hand-coded logical axioms and rules derived from spatial calculi (RCC, directional rulesets) and mapped lexical forms",
            "has_direct_sensory_input": false,
            "elicitation_method": "deterministic symbolic inference (Prolog Horn clauses) executed over explicit scene graph triplets to produce labels and to validate indirect relations",
            "knowledge_representation": "explicit symbolic predicates and Horn clauses representing spatial relations and meta-rules (inverse, transitivity, combination); operates over graph-structured triplets",
            "performance_metric": "not reported as predictive model evaluation; used as oracle during dataset generation (correctness implicit in rule set and constraints)",
            "performance_result": "Used as an oracle to generate and validate dataset items and answers; no numeric evaluation metrics reported for reasoner itself in the paper.",
            "success_patterns": "Correctly computes indirect relations via chained Horn-clause rules (e.g., NTPP(a,b), FRONT(b,c), TPPI(c,d) =&gt; FRONT(a,d)), enforces spatial consistency (inverse/symmetry constraints), and supplies supporting facts and reasoning-step counts; enables creation of multi-hop reasoning questions.",
            "failure_patterns": "Reasoner limited to explicit spatial calculi and codified rules; cannot model metaphoric/implicit uses or commonsense physical knowledge (authors note they exclude metaphoric/implicit meanings), and depends on correctness/coverage of the hand-authored rule set.",
            "baseline_comparison": "Not compared to other reasoners in quantitative terms here; authors claim novelty in combining multiple formalisms for textual SQA dataset generation relative to prior spatial tools.",
            "ablation_results": "No explicit ablation of the reasoner itself reported; paper shows that using the reasoner to generate richer multi-hop questions and supporting facts (vs. simpler synthetic pipelines) yields better downstream PLM transfer when paired with broad lexical coverage in SPARTUN.",
            "key_findings": "Explicit symbolic spatial reasoning can generate high-quality supervision (supporting facts, step counts, indirect relations) that, when converted to text and used for PLM pretraining, helps PLMs internalize multi-hop spatial inferences in a text-only setting; however, symbolic rules remain necessary to produce reliable labels and cannot substitute for perceptual grounding required for embodied tasks.",
            "uuid": "e538.3",
            "source_info": {
                "paper_title": "Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SPARTQA: A textual question answering benchmark for spatial reasoning",
            "rating": 2
        },
        {
            "paper_title": "Touchdown: Natural language navigation and spatial reasoning in visual street environments",
            "rating": 2
        },
        {
            "paper_title": "Spatial representation and reasoning for human-robot collaboration",
            "rating": 1
        },
        {
            "paper_title": "Spatial reasoning from natural language instructions for robot manipulation",
            "rating": 2
        },
        {
            "paper_title": "A spatial logic based on regions and connection",
            "rating": 2
        },
        {
            "paper_title": "Explicit object relation alignment for vision and language navigation",
            "rating": 1
        }
    ],
    "cost": 0.01882975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning</h1>
<p>Roshanak Mirzaee<br>Michigan State University<br>mirzaee@msu.edu</p>
<h2>Parisa Kordjamshidi</h2>
<p>Michigan State University
kordjams@msu.edu</p>
<h4>Abstract</h4>
<p>Recent research shows synthetic data as a source of supervision helps pretrained language models (PLM) transfer learning to new target tasks/domains. However, this idea is less explored for spatial language. We provide two new data resources on multiple spatial language processing tasks. The first dataset is synthesized for transfer learning on spatial question answering (SQA) and spatial role labeling (SpRL). Compared to previous SQA datasets, we include a larger variety of spatial relation types and spatial expressions. Our data generation process is easily extendable with new spatial expression lexicons. The second one is a real-world SQA dataset with humangenerated questions built on an existing corpus with SPRL annotations. This dataset can be used to evaluate spatial language processing models in realistic situations. We show pretraining with automatically generated data significantly improves the SOTA results on several SQA and SPRL benchmarks, particularly when the training data in the target domain is small.</p>
<h2>1 Introduction</h2>
<p>Understanding spatial language is important in many applications such as navigation (Zhang and Kordjamshidi, 2022; Zhang et al., 2021; Chen et al., 2019), medical domain (Datta et al., 2020; Kamel Boulos et al., 2019; Massa et al., 2015), and robotics (Venkatesh et al., 2021; Kennedy et al., 2007). However, few benchmarks have directly focused on comprehending the spatial semantics of the text. Moreover, the existing datasets are either synthetic (Mirzaee et al., 2021; Weston et al., 2015; Shi et al., 2022) or at small scale (Mirzaee et al., 2021; Kordjamshidi et al., 2017).</p>
<p>The synthetic datasets often focus on specific types of relations with a small coverage of spatial semantics needed for spatial language understanding in various domains. Figure 2 indicates the coverage of sixteen spatial relation types (in</p>
<p>Three boxes called one, two and three exist in an image. Box one contains a big yellow melon and a small orange watermelon. Box two has a small yellow apple. A small orange apple is inside and touching this box. Box one is in box three. Box two is to the south of, far from and to the west of box three. A small yellow watermelon is inside box three.</p>
<p>Q: Is the yellow apple to the west of the yellow watermelon? Yes
Q: Where is box two relative to the yellow watermelon? Left, Below, Far
(a) SPARTUN - A synthetic large dataset provided as a source of supervision.</p>
<p>A grey car is parking in front of a grey house with brown window frames and plants on the balcony.</p>
<p>Q: Are the plants in front of the car? No
Q: Are the plants in the house? Yes
(b) RESQ - A human-generated dataset for probing the models on realistic SQA</p>
<p>Figure 1: Two new datasets on SQA</p>
<p>Table 1) collected from existing resources (Randell et al., 1992; Wolter, 2009; Renz and Nebel, 2007). The human-generated datasets, despite helping study the problem as evaluation benchmarks, are less helpful for training models that can reliably understand spatial language due to their small size (Mirzaee et al., 2021).</p>
<p>In this work, we build a new synthetic dataset on SQA, called SPARTUN ${ }^{1}$ (Fig 1a) to provide a source of supervision with broad coverage of spatial relation types and expressions ${ }^{2}$.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Formalism <br> (General Type)</th>
<th style="text-align: left;">Specific value</th>
<th style="text-align: left;">Spatial type/Spatial value)</th>
<th style="text-align: left;">Expressions (e.g.)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">DC (disconnected) <br> EC (Externally Connected) <br> PO (Partially Overlapped) <br> EQ (Equal) <br> TPP (Tangential Proper Part) <br> NTPP (Non-Tangential Proper Part) <br> TPPI (Tangential Proper Part inverse) <br> NTPPI (Non-Tangential Proper Part inverse)</td>
<td style="text-align: left;">disjoint <br> touching <br> overlapped <br> equal <br> covered by <br> in, inside <br> covers <br> has</td>
</tr>
<tr>
<td style="text-align: left;">Topological</td>
<td style="text-align: left;">Relative</td>
<td style="text-align: left;">LEFT, RIGHT <br> BELOW, ABOVE <br> BEHIND, FRONT</td>
<td style="text-align: left;">left of, right of <br> under, over <br> behind, in front</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Qualitative</td>
<td style="text-align: left;">Far, Near</td>
<td style="text-align: left;">far, close</td>
</tr>
</tbody>
</table>
<p>Table 1: Spatial relation types and examples of spatial language expressions.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: The comparative coverage of relation types based on Table 1 for SQA datasets.</p>
<p>To generate SPARTUN, we follow the idea of SPARTQA (Mirzaee et al., 2021) benchmark and generate scene graphs from a set of images. The edges in this graph yield a set of triplets such as $\mathrm{ABOVE}($ blue circle, red triangle), which are used to generate a scene description (i.e., a story).</p>
<p>In SPARTUN, we map the spatial relation types in triplets (e.g., ABOVE) to a variety of spatial language expressions (e.g., over, north, above) to enable the transfer learning for various data domains ${ }^{3}$. We also build a logical spatial reasoner to compute all possible direct and indirect spatial relations between graph nodes. Then, the questions of this dataset are selected from the indirect relations.</p>
<p>To evaluate the effectiveness of SPARTUN in transfer learning, we created another dataset named RESQ $^{4}$ (Fig 1b). This dataset is built on MSPRL (Kordjamshidi et al., 2017) corpus while we added human-generated spatial questions and</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>answers to its real image descriptions. This dataset comparatively reflects more realistic challenges and complexities of the SQA problem.</p>
<p>We analyze the impact of SPARTUN as source of extra supervision on several SQA and SPRL benchmarks. To the best of our knowledge, we are the first to use synthetic supervision for the SPRL task. Our results show that the auto-generated data successfully improves the SOTA results on MSPRL and SPARTQA-HUMAN, which are annotated for SPRL task. Moreover, further pretraining models with SPARTUN for SQA task improves the result of previous models on RESQ, StepGame, and SPARTQA-HUMAN benchmarks. Furthermore, studying the broad coverage of spatial relation expressions of SPARTUN in realistic domains demonstrates that this feature is a key factor for transfer learning.</p>
<p>The contributions of this paper can be summarized as: (1) We build a new synthetic dataset to serve as a source of supervision and transfer learning for spatial language understanding tasks with broad coverage of spatial relation types and expressions (which is easily extendable); (2) We provide a human-generated dataset to evaluate the performance of transfer learning on real-world spatial question answering; (3) We evaluate the transferability of the models pretrained with SPARTUN on multiple SQA and SPRL benchmarks and show significant improvements in SOTA results.</p>
<h2>2 Related Research</h2>
<p>Requiring large amounts of annotated data is a wellknown issue in training complex deep neural mod-</p>
<p>els (Zhu et al., 2016) that is extended to spatial language processing tasks. In our study, we noticed that all available large datasets on SQA task including bAbI (Weston et al., 2015), SPARTQAAuto (Mirzaee et al., 2021), and StepGame (Shi et al., 2022) are, all, synthetic.
bAbI is a simple dataset that covers a limited set of relation types, spatial rules, and vocabulary. StepGame focuses on a few relation types but with more relation expressions for each and considers multiple reasoning steps. SPARTQA-AUTO, comparatively, contains more relation types and needs complex multi-hop spatial reasoning. However, it contains a single linguistic spatial expression for each relation type. All of these datasets are created based on controlled toy settings and are not comparable with real-world spatial problems in the sense of realistic language complexity and coverage of all possible relation types. SPARTQA-HUMAN (Mirzaee et al., 2021) is a human-generated version of SPARTQA-AUTO with more spatial expressions. However, this dataset is provided for probing purposes and has a small training set that is not sufficient for effectively training deep models.</p>
<p>For the SpRL task, mSPRL and SpaceEval (SemEval-2015 task 8) (Pustejovsky et al., 2015) are two available datasets with spatial roles and relation annotations. These are small-scale datasets for studying the SPRL problem. From the previous works which tried transfer learning on SPRL task, (Moussa et al., 2021) only used it on word embedding of their SPRL model, and (Shin et al., 2020) used PLM without any specifically designed dataset for further pretraining. These issues motivated us to create SPARTUN for further pretraining and transfer learning for SQA and SPRL.</p>
<p>Transfer learning has been used effectively in different NLP tasks to further fine-tune the PLMs (Razeghi et al., 2022; Alrashdi and O'Keefe, 2020; Magge et al., 2018). Besides transfer learning, several other approaches are used to tackle the lack of training data in various NLP areas, such as providing techniques to label the unlabeled data (Enayati et al., 2021), using semi-supervised models (Van Krieken et al., 2019; Li et al., 2021) or data augmentation with synthetic data (Li et al., 2019; Min et al., 2020). However, transfer learning is a simple way of using synthetic data as an extra source of supervision at no annotation cost. Compared to the augmentation methods, the data
in the transfer learning only needs to be close to the target task/domain (Ma et al., 2021) and not necessarily the same. Mirzaee et al. is the first work that considers transfer learning for SQA. It shows that training models on synthetic data and finetuning with small human-generated data results in a better performance of PLMs. However, their coverage of spatial relations and expressions is insufficient for effective transfer learning to realistic domains.</p>
<p>Using logical reasoning for building datasets that need complex reasoning for question answering is used before in building QA datasets (Clark et al., 2020; Saeed et al., 2021). More recent efforts even use the path of reasoning and train models to follow that (Tafjord et al., 2021). However, there are no previous works to model spatial reasoning as we do here with the broad coverage of spatial logic.</p>
<h2>3 Transfer Learning for Spatial Language Understanding</h2>
<p>To evaluate transfer learning on spatial language understanding, we select two main tasks, spatial question answering (SQA) and spatial role labeling (SPRL). Given the popularity of PLMs in transfer learning (Khashabi et al., 2020; Ma et al., 2021; Clark et al., 2020), we design PLM-based models for this evaluation. In the rest of this section, we describe each task and model in detail.</p>
<h3>3.1 Spatial Question Answering</h3>
<p>In spatial question answering, given a scene description, the task is to answer questions about the spatial relations between entities (e.g., Figure 1). Here, we focus on challenging questions that need multi-hop spatial reasoning over explicit relations. We consider two question types, YN (Yes/No) and FR(Find relations). The answer to YN is chosen from "Yes" or "No," and the answer to FR is chosen from a set of relation types.</p>
<p>We use a PLM with classification layers as a baseline for the SQA task. We use a binary classification layer for each label for questions with more than one valid answer and a multi-class classification layer for questions with a single valid answer. To predict the answer, we pass the concatenation of the question and story to the PLM (more detail in (Devlin et al., 2019).) The final output of $[C L S]$ token is passed to the classification layer and depending on the question type, a label or multiple labels with the highest probability are chosen as the final answer.</p>
<p>We train the models based on the summation of the cross-entropy losses of all binary classifiers in multi-label classification or the single crossentropy for a single classifier in multi-classification. In the multi-label setting, we remove inconsistent answers by post-processing during the inference phase. For instance, LEFT and RIGHT relations cannot be valid answers simultaneously.</p>
<h3>3.2 Spatial Role Labeling</h3>
<p>Spatial role labeling (Kordjamshidi et al., 2010, 2011) is the task of identifying and classifying the spatial roles (Trajector, Landmark, and spatial indicator) and their relations. A relation is selected from the relation types in Table 1 and assigned to each triplet of (Trajector, Spatial indicator, Landmark) extracted from the sentence. We call the former spatial role extraction and the latter spatial relation ${ }^{5}$ extraction (Figure 3).</p>
<p>Several neural models have been proposed to solve spatial role (Mazalov et al., 2015; Ludwig et al., 2016; Datta and Roberts, 2020). We take a similar approach to prior research (Shin et al., 2020) for the extraction of spatial roles (entities (Trajector/Landmark) and spatial indicators).</p>
<p>First, we separately tokenize each sentence in the context and use a PLM (which is BERT here) to compute the tokens representation. Next, we apply a BIO tagging layer on tokens representations using (O, B-entity, I-entity, B-indicator, I-indicator) tags. A Softmax layer on BIO tagger output is used to select the spatial entities and spatial indicators with the highest probability. For training, we use CrossEntropy loss given the spatial annotation.</p>
<p>For the spatial relation extraction model, similar to (Yao et al., 2019; Shin et al., 2020), we use BERT and a classification layer to extract correct triplets. Given the output of the spatial role extraction model, for each combination of (spatial entity(tr), spatial_indicator $(s p)$, spatial entity $(l m))$ in each sentence we create an input ${ }^{6}$ and pass it to the BERT model. To indicate the position of each spatial role in the sentence, we use segment embeddings and add 1 if it is a role position and 0 otherwise.</p>
<p>The $[C L S]$ output of BERT will be passed to a one-layer MLP that provides the probability for the triplet (see Fig 3). Compared to the prior research,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Spatial role labeling model includes two separately trained modules. E: entity, SP: spatial_indicators. As an example, triplet (a grey house, front, A grey car) is correct and the "spatial_type $=$ FRONT", and (A grey car, front, a grey house) is incorrect, and the "spatial_type $=\mathrm{NaN}$ ".
we predict the spatial type for each triplet as an auxiliary task for spatial relation extraction. To this aim, we apply another multi-class classification layer ${ }^{7}$ on the same $[C L S]$ token. To train the model, we use a joint loss function for both relation and type modules (more detail in Appendix B).</p>
<h2>4 SPARTUN: Dataset Construction</h2>
<p>To provide a source of supervision for spatial language understanding tasks, we generate a synthetic dataset with SQA format that contains SPRL annotation of sentences. We build this dataset by expanding SPARTQA in multiple aspects. The following additional features are considered in creating SPARTUN:
F1) A broad coverage of various types of spatial relations and including rules of reasoning over their combinations (e.g. $\operatorname{NTPP}(a, b), \operatorname{LEFT}(b, c) \rightarrow$ $\operatorname{LEFT}(a, c)$ ) in various domains.
F2) A broad coverage of spatial language expressions and utterances used in various domains.
F3) Including extra annotations such as the supporting facts and number of reasoning steps for SQA to be used in complex modeling.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: The data construction flow of SPARTUN. First, we generate scene graphs from NLVR images. Then a spatial reasoner validates each path between each pair of entities in this graph. All facts $(F)$ in the selected path and some extra facts $(E)$ from the scene graph are selected as story triplets, and the start and end nodes of the path are selected as question triplets. Finally, we pass all triplets to a text generation module and compute the final answer. We ignore paths with length one (e.g., $A(A B O V E) C$ ) and only keep questions that need multi-hop reasoning.</p>
<p>In the rest of this section, we describe the details of creating SPARTUN and the way we support the above mentioned features. Figure 4 depicts SPARTUN data construction flow.</p>
<p>Spatial Relation Computation. Following SPARTQA-AUTO, we use the NLVR scene graphs (Suhr et al., 2017) and compute relations between objects in each block based on their given coordinates. NLVR is limited to 2D relation types ${ }^{8}$, therefore to add more dimensions (FRONT and BEHIND), we randomly change the LEFT and RIGHT to BEHIND and FRONT in a subset of examples. Moreover, there are no relations between blocks in NLVR descriptions.</p>
<p>To expand the types of relations, we extend this limitation and randomly assign relations ${ }^{9}$ to the blocks while ensuring the spatial constraints are not violated. Then, we create a new scene graph with computed spatial relations. The nodes in this graph represent the entities (objects or blocks), and the directed edges are the spatial relations.</p>
<p>Question Selection. There are several paths between each pair of entities in the generated scene graph. We call a path valid if at least one relation can be inferred between its start and end nodes can be inferred. For example, in Figure 4, $\operatorname{NTPP}(A, X), \operatorname{FRONT}(X, Y), \operatorname{TPPI}(Y, B)$ is valid since it results in $\operatorname{FRONT}(A, B)$ while $\operatorname{NTPP}(A, X), \operatorname{NTPPI}(X, C)$ is not a valid path -there is no rules of reasoning that can be applied to infer new relations.</p>
<p>To verify the validity of each path, we pass its edges, represented as triplets in the predicatearguments form to a logical spatial reasoner (imple-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>mented in Prolog) and query all possible relations between the pair. The number of triplets in each path represents the number of reasoning steps for inferring the relation.</p>
<p>We generate the question triplets from the paths with the most steps of reasoning (edges). This question will ask about the spatial relationship between the head and tail entity of the selected path. The triplets in this path are used to generate the story and are annotated as supporting facts. Additionally, the story will include additional information (extra triplets) unnecessary for answering the question to increase the complexity of the task.</p>
<p>Spatial Reasoner. We implement several rules (in the form of Horn clauses shown in Table 2) in Prolog, which express the logic between the relation types (described in Table 1) in various formalisms and model the logical spatial reasoning computation (see Appendix B.1). Compared to previous tools (Wolter, 2009), we are the first to include the spatial, logical computation between multiple formalisms. This reasoner validates the question/queries based on the given facts. For instance, by using the Combination rule in Table 2 over the set of facts ${\operatorname{NTPP}(A, X), \operatorname{FRONT}(X, Y), \operatorname{TPPI}(Y, B)}$, the reasoner returns True for the query $\operatorname{FRONT}(A, B)$ and False for $\operatorname{FRONT}(B, A)$ or $\operatorname{BEHIND}(A, B)$.</p>
<p>Text generation. The scene description is generated from the selected story triplets in question selection phase and using a publicly available contextfree grammar (CFG) provided in SPARTQA-AUTO. However, we increase the variety of spatial expressions by using a vocabulary of various entity properties and relation expressions (e.g., above, over, or north for ABOVE relation type) taken from exist-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Not</th>
<th style="text-align: left;">$\forall(X, Y) \in$ Entities</th>
<th style="text-align: left;">$R \in{D i r \vee P P}$</th>
<th style="text-align: left;">IF $R(X, Y)$</th>
<th style="text-align: left;">$\Rightarrow \operatorname{NOT}(R _$reverse $(X, Y))$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Inverse</td>
<td style="text-align: left;">$\forall(X, Y) \in$ Entities</td>
<td style="text-align: left;">$R \in{D i r \vee P P}$</td>
<td style="text-align: left;">IF $R(Y, X)$</td>
<td style="text-align: left;">$\Rightarrow R _$reverse $(X, Y)$</td>
</tr>
<tr>
<td style="text-align: left;">Symmetry</td>
<td style="text-align: left;">$\forall(X, Y) \in$ Entities</td>
<td style="text-align: left;">$R \in{D i s \vee(R C C-P P)}$</td>
<td style="text-align: left;">IF $R(Y, X)$</td>
<td style="text-align: left;">$\Rightarrow R(X, Y)$</td>
</tr>
<tr>
<td style="text-align: left;">Transitivity</td>
<td style="text-align: left;">$\forall(X, Y, Z) \in$ Entities</td>
<td style="text-align: left;">$R \in{D i r \vee P P}$</td>
<td style="text-align: left;">IF $R(X, Z), R(Z, Y)$</td>
<td style="text-align: left;">$\Rightarrow R(X, Y)$</td>
</tr>
<tr>
<td style="text-align: left;">Combination</td>
<td style="text-align: left;">$\forall(X, Y, Z, H) \in$ Entities</td>
<td style="text-align: left;">$R \in D i r, * P P \in P P$</td>
<td style="text-align: left;">IF $* P P(X, Z), R(Z, H), * P P i(Z, Y)$</td>
<td style="text-align: left;">$\Rightarrow R(X, Y)$</td>
</tr>
</tbody>
</table>
<p>Table 2: Designed spatial rules. Dir: Directional relations (e.g., LEFT), Dis: Distance relations (e.g., FAR), $P P$ : all Proper parts relations (NTPP, NTPPI, TPPI, TPP), $R C C-P P$ : All RCC8 relation except proper parts relations. <em>PP: one of TPP or NTPP. </em>PPi: one of NTPPi or TPPi.
ing resources (Freeman, 1975; Mark et al., 1989; Lockwood et al., 2006; Stock et al., 2022; Herskovits, 1986) We map the relation types and the entity properties to the lexical forms in our collected vocabulary.</p>
<p>For the question text, we generate the entity description and relation expression for each question triplet. The entity description is generated based on a subset of its properties in the story. For instance, an expression such as "a black object" can be generated to refer to both "a big black circle" and "a black rectangle". We generate two question types, YN (Yes/No) questions that ask whether a specific relation exists between two entities, and FR (Find Relations) questions that ask about all possible relations between them. To make YN questions more complex, we add quantifiers ("all" and "any") to the entities' descriptions.</p>
<p>Our text generation method can flexibly use an extended vocabulary to provide a richer corpus to supervise new target tasks when required.</p>
<p>Finding Answers. We search all entities in the story based on the entity descriptions (e.g., all circles, a black object) in each question and use the spatial reasoner to find the final answer.</p>
<p>SpRL Annotations. Along with generating the sentences for the story and questions, we automatically annotate the described spatial configurations with spatial roles and relations (trajector, landmark, spatial indicator, spatial type, triplet, entity ids). These annotations are based on a previously proposed annotation scheme of SPRL and provide free annotations for the SPRL task.</p>
<p>To generate SPARTUN, we use 6.6 k NLVR scene graphs for training and 1 k for each dev and test set. We collect 20k training, 3 k dev, and 3 k test examples for each FR and YN question (see Table 3) ${ }^{10}$. On average, each story of SPARTUN contains eight sentences and 91 tokens that describe</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>on average 10 relations between different mentions of entities. More details about the dataset statistics can be seen in Appendix A.1.</p>
<h2>5 Experimental Results</h2>
<p>The focus of this paper is to provide a generic source of supervision for spatial language understanding tasks rather than proposing new techniques or architectures. Therefore, in the experiments, we analyze the impact of SPARTUN on SQA and SPRL using the PLM-based models described in Section 3.</p>
<p>In all experiments, we compare the performance of models fine-tuned with the target datasets with and without further pretraining on synthetic supervision (SynSup). All codes are publicly available ${ }^{11}$. The details of experimental settings and hyperparameters of datasets are provided in the Appendix.</p>
<h3>5.1 Spatial Question Answering</h3>
<p>Here, we evaluate the impact of SPARTUN and compare it with the supervision received from other existing synthetic datasets. Since the datasets that we use contain different question types, we supervise the models based on the same question type as the target task ${ }^{12}$.</p>
<p>The baselines for all experiments include a majority baseline (MB) which predicts the most repeated label as the answer to all questions, and a pretrained language model, that is, BERT here. We also report the human accuracy in answering the questions for the human-generated datasets ${ }^{13}$. For all experiments, to evaluate the models, we measure the accuracy which is the percentage of correct predictions in the test sets.</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<h1>5.1.1 SQA Evaluation Datasets</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">bAbI</td>
<td style="text-align: center;">8992</td>
<td style="text-align: center;">992</td>
<td style="text-align: center;">992</td>
</tr>
<tr>
<td style="text-align: left;">SPARTQA-AUTO (YN)</td>
<td style="text-align: center;">26152</td>
<td style="text-align: center;">3860</td>
<td style="text-align: center;">3896</td>
</tr>
<tr>
<td style="text-align: left;">SPARTQA-AUTO (FR)</td>
<td style="text-align: center;">25744</td>
<td style="text-align: center;">3780</td>
<td style="text-align: center;">3797</td>
</tr>
<tr>
<td style="text-align: left;">SPARTQA-HUMAN (YN)</td>
<td style="text-align: center;">162</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">143</td>
</tr>
<tr>
<td style="text-align: left;">SPARTQA-HUMAN (FR)</td>
<td style="text-align: center;">149</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">77</td>
</tr>
<tr>
<td style="text-align: left;">RESQ</td>
<td style="text-align: center;">1008</td>
<td style="text-align: center;">333</td>
<td style="text-align: center;">610</td>
</tr>
<tr>
<td style="text-align: left;">StepGame</td>
<td style="text-align: center;">50000</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">10000</td>
</tr>
<tr>
<td style="text-align: left;">SPARTUN (YN)</td>
<td style="text-align: center;">20334</td>
<td style="text-align: center;">3152</td>
<td style="text-align: center;">3193</td>
</tr>
<tr>
<td style="text-align: left;">SPARTUN (FR)</td>
<td style="text-align: center;">18400</td>
<td style="text-align: center;">2818</td>
<td style="text-align: center;">2830</td>
</tr>
</tbody>
</table>
<p>Table 3: Size of SQA benchmarks.
bAbI We use tasks 17 and 19 of bAbI. Task 17 is on spatial reasoning and contains binary Yes/No questions. Task 19 is on path finding and contains FR questions with answers in {LEFT, RIGHT, ABOVE, BELOW} set. The original dataset contains west, east, north, and south, which we mapped to their corresponding relative relation type.</p>
<p>SPARTQA-HUMAN is a small human-generated dataset containing YN and FR questions that need multi-hop spatial reasoning. The answer of YN questions is in ${$ Yes, No,DK $}$ where DK denotes Do not Know is used when the answer cannot be inferred from the context. The answer to FR questions is in ${$ left, right, above, below, near to, far from, touching, DK $}^{14}$.</p>
<p>StepGame is a synthetic SQA dataset containing FR questions which need $k$ reasoning steps to be answered $(k=1$ to 10$)$. The answer to each question is one relation in ${$ left, right, below, above, lower-left, upper-right, lower-right, upper-left $}$ set.</p>
<p>RESQ We created this dataset to reflect the natural complexity of real-world spatial descriptions and questions. We asked three volunteers (Englishspeaking undergrad students) to generate Yes/No questions for MSPRL dataset that contains complex human-generated sentences. The questions require at least one step of reasoning. The advantage of RESQ is that the human-generated spatial descriptions and their spatial annotations already exist in the original dataset. The statistics of this dataset are provided in Appendix A.2.</p>
<p>One of the challenges of the RESQ, which is not addressed here, is that the questions require spatial commonsense knowledge in addition to capturing</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 4: Impact of using synthetic supervision on the bAbI tasks. All the models are further fine-tuned on the training set of task 17 (size $=1 \mathrm{k}$ ) and 19 (size $=500$ ), and test on bAbI test sets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">SynSup</th>
<th style="text-align: center;">YN</th>
<th style="text-align: center;">FR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MB</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">53.60</td>
<td style="text-align: center;">24.52</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">$\mathbf{4 9 . 6 5}$</td>
<td style="text-align: center;">18.18</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">SPARTQA-A</td>
<td style="text-align: center;">39.86</td>
<td style="text-align: center;">48.05</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">StepGame</td>
<td style="text-align: center;">44.05</td>
<td style="text-align: center;">11.68</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">SPARTUN-S</td>
<td style="text-align: center;">44.75</td>
<td style="text-align: center;">37.66</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">SPARTUN</td>
<td style="text-align: center;">48.25</td>
<td style="text-align: center;">$\mathbf{5 0 . 6 4}$</td>
</tr>
<tr>
<td style="text-align: left;">Human</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">90.69</td>
<td style="text-align: center;">95.23</td>
</tr>
</tbody>
</table>
<p>Table 5: Transfer learning on SPARTQA-HUMAN. SPARTQA-A stands for SPARTQA-AUTO.
the spatial semantics. For example, by using commonsense knowledge from the sentence, "a lamp hanging on the ceiling", we can infer that the lamp is above all the objects in the room. To compute the human accuracy, we asked two volunteers to answer 100 questions from the test set of RESQ and compute the accuracy.</p>
<h3>5.1.2 Transfer Learning in SQA</h3>
<p>The following experiments demonstrate the impact of transfer learning for SQA benchmarks considering different supervisions.</p>
<p>Due to the simplicity of bAbI dataset, PLM can solve this benchmark with $100 \%$ accuracy (Mirzaee et al., 2021). Hence we run our experiment on only 1 k and 500 training examples of task 17 and task 19, respectively. Table 4 demonstrates the impact of synthetic supervision on both tasks of bAbI. The results with various synthetic data are fairly similar for these two tasks. However, pretraining the model with the simple version of SPARTUN, named SPARTUN-S, performs better than other synthetic datasets on task 17. This can be due to the fewer relation expressions in SPARTUN-S, which follows the same structure as task 17.</p>
<p>In the next experiment, we investigate the impact of SPARTUN on SPARTQA-HUMAN result. Comparing the results in Table 5, we find that even though the classification layer for SPARTQA-</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: center;">k steps of reasoning</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: left;">SynSup</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: left;">TP-MANN</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">85.77</td>
<td style="text-align: center;">60.31</td>
<td style="text-align: center;">50.18</td>
<td style="text-align: center;">37.45</td>
<td style="text-align: center;">31.25</td>
<td style="text-align: center;">28.53</td>
<td style="text-align: center;">26.45</td>
<td style="text-align: center;">23.67</td>
<td style="text-align: center;">22.52</td>
<td style="text-align: center;">21.46</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">98.44</td>
<td style="text-align: center;">94.77</td>
<td style="text-align: center;">91.78</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">57.56</td>
<td style="text-align: center;">50.34</td>
<td style="text-align: center;">45.17</td>
<td style="text-align: center;">39.69</td>
<td style="text-align: center;">35.41</td>
<td style="text-align: center;">33.62</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">SPARTQA-A</td>
<td style="text-align: center;">98.63</td>
<td style="text-align: center;">94.95</td>
<td style="text-align: center;">91.94</td>
<td style="text-align: center;">77.74</td>
<td style="text-align: center;">68.37</td>
<td style="text-align: center;">61.67</td>
<td style="text-align: center;">57.95</td>
<td style="text-align: center;">50.82</td>
<td style="text-align: center;">46.86</td>
<td style="text-align: center;">44.03</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">SPARTUN-S</td>
<td style="text-align: center;">$\mathbf{9 8 . 7 0}$</td>
<td style="text-align: center;">$\mathbf{9 5 . 2 1}$</td>
<td style="text-align: center;">$\mathbf{9 2 . 4 6}$</td>
<td style="text-align: center;">77.93</td>
<td style="text-align: center;">69.53</td>
<td style="text-align: center;">62.14</td>
<td style="text-align: center;">57.37</td>
<td style="text-align: center;">48.79</td>
<td style="text-align: center;">44.67</td>
<td style="text-align: center;">42.72</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">SPARTUN</td>
<td style="text-align: center;">98.55</td>
<td style="text-align: center;">95.02</td>
<td style="text-align: center;">92.04</td>
<td style="text-align: center;">$\mathbf{7 9 . 1}$</td>
<td style="text-align: center;">$\mathbf{7 0 . 3 4}$</td>
<td style="text-align: center;">$\mathbf{6 3 . 3 9}$</td>
<td style="text-align: center;">$\mathbf{5 8 . 7 4}$</td>
<td style="text-align: center;">$\mathbf{5 2 . 0 9}$</td>
<td style="text-align: center;">$\mathbf{4 8 . 3 6}$</td>
<td style="text-align: center;">$\mathbf{4 5 . 6 8}$</td>
</tr>
</tbody>
</table>
<p>Table 6: Result of models with and without extra synthetic supervision on StepGame.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">SynSup</th>
<th style="text-align: center;">Accu</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MB</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">50.21</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">57.37</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">SPARTQA-AUTO</td>
<td style="text-align: center;">55.08</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">StepGame</td>
<td style="text-align: center;">60.14</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">SPARTUN-S</td>
<td style="text-align: center;">58.03</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">SPARTUN</td>
<td style="text-align: center;">$\mathbf{6 3 . 6 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Human</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">90.38</td>
</tr>
</tbody>
</table>
<p>Table 7: Results with and without extra supervision on ReSQ. The Human accuracy is the performance of human on answering a subset of test set.</p>
<p>Auto and SPARTQA-Human are the same, the model trained on SPARTUN has a better transferability. It achieves $2.6 \%$ better accuracy on FR and $9 \%$ better accuracy on YN questions compared to SPARTQA-AUTO. YN is, yet, the most challenging question type in SPARTQA-HUMAN and none of the PLM-based models can reach even the simple majority baseline.</p>
<p>Table 6 demonstrates our experiments on StepGame. BERT without any extra supervision, significantly, outperforms the best reported model in Shi et al., TP-MANN, which is based on a neural memory network. As expected, all the PLM-based models almost solve the questions with one step of reasoning (i.e. where the answer directly exists in the text). However, with increasing the steps of reasoning, the performance of the models decreases. Comparing the impact of different synthetic supervision, SPARTUN achieves the best result on $k&gt;3$. For questions with $k&lt;=3$, SPARTUNS achieves competitive similar results compared to SPARTUN. Overall, the performance gap in SPARTUN-S, SPARTQA-AUTO and SPARTUN shows that more coverage of relation expressions in SPARTUN is effective.</p>
<p>In the next experiment, we show the influence of SPARTUN on real-world examples, which contain more types of spatial relations and need more rules of reasoning to be solved. Table 7 shows the
result of transfer learning on ReSQ. This result shows that the limited coverage of spatial relations and expression in SPARTQA-AUTO impacts the performance of BERT negatively. However, further pretraining BERT on SPARTUN-S improves the result on RESQ. This can be due to the higher coverage of relation types in SPARTUN-S than SPARTQA-AUTO. Using SPARTUN for further pretraining BERT has the best performance and improves the result by $5.5 \%$, indicating its advantage for transferring knowledge to solve real-world spatial challenges.</p>
<h3>5.2 Spatial Role Labeling</h3>
<p>Here, we analyze the influence of the extra synthetic supervision on SPRL task when evaluated on human-generated datasets. Table 8 shows the number of sentences in each SPRL benchmarks.</p>
<p>The pipeline model provided in Section 3, contains two main parts, a model for spatial role extraction (SRol) and a model for spatial relation extraction (SRel), which we analyze separately.</p>
<p>We further pretrain the BERT module in these models and then fine-tune it on the target domain. We use Macro F1-score (mean of F1 for all classes) to evaluate the performance of the SRol and SRel models.</p>
<h3>5.2.1 SPRL Evaluation Datasets</h3>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SPARTQA-AUTO (story)</td>
<td style="text-align: center;">25755</td>
<td style="text-align: center;">16214</td>
<td style="text-align: center;">16336</td>
</tr>
<tr>
<td style="text-align: left;">SPARTQA-AUTO (question)</td>
<td style="text-align: center;">23584</td>
<td style="text-align: center;">15092</td>
<td style="text-align: center;">15216</td>
</tr>
<tr>
<td style="text-align: left;">SPARTQA-HUMAN (story)</td>
<td style="text-align: center;">176</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">272</td>
</tr>
<tr>
<td style="text-align: left;">SPARTQA-HUMAN (question)</td>
<td style="text-align: center;">155</td>
<td style="text-align: center;">127</td>
<td style="text-align: center;">367</td>
</tr>
<tr>
<td style="text-align: left;">SPARTUN (story)</td>
<td style="text-align: center;">48368</td>
<td style="text-align: center;">7031</td>
<td style="text-align: center;">7191</td>
</tr>
<tr>
<td style="text-align: left;">SPARTUN (question)</td>
<td style="text-align: center;">38734</td>
<td style="text-align: center;">5970</td>
<td style="text-align: center;">6023</td>
</tr>
<tr>
<td style="text-align: left;">MSPRL</td>
<td style="text-align: center;">481</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">461</td>
</tr>
</tbody>
</table>
<p>Table 8: Number of sentences of SPRL benchmarks. To train the SPARTQA-AUTO, we only use the 3 k training examples ( 23 - 25 k sentences).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">SynSup</th>
<th style="text-align: center;">MSPRL</th>
<th style="text-align: center;">SPARTQA-H</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">R-Inf</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">80.92</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">SRol</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">$\mathbf{8 8 . 5 9}$</td>
<td style="text-align: center;">55.8</td>
</tr>
<tr>
<td style="text-align: left;">SRol</td>
<td style="text-align: left;">SPARTQA-A</td>
<td style="text-align: center;">88.41</td>
<td style="text-align: center;">57.28</td>
</tr>
<tr>
<td style="text-align: left;">SRol</td>
<td style="text-align: left;">SPARTUN</td>
<td style="text-align: center;">88.03</td>
<td style="text-align: center;">$\mathbf{7 2 . 4 3}$</td>
</tr>
</tbody>
</table>
<p>Table 9: Evaluating spatial role extraction (SRol) on two MSPRL and SPARTQA-HUMAN(SPARTQA-H) datasets with and without synthetic supervision.</p>
<p>MSPRL is a human-curated dataset provided on SPRL task. This dataset contains spatial description of real-world images and corresponding SPRL annotations (see Appendix A.6).</p>
<p>SPARTQA-HUMAN did not contain SPRL annotations. Hence, we asked two expert volunteers to annotate the story/questions of this dataset. Then another expert annotator checked the annotation and discarded the erroneous ones. As a result, half of this training data is annotated with SPRL tags.</p>
<h3>5.2.2 Transfer learning in SPRL</h3>
<p>Table 9 demonstrates the influence of synthetic supervision in spatial role extraction evaluated on MSPRL and SPARTQA-HUMAN.</p>
<p>We compare the result of SRol model with the previous SOTA, "R-Inf" (Manzoor and Kordjamshidi, 2018), on MSPRL dataset. R-Inf uses external multi-modal resources and global inference. All of the BERT-based SRol models outperform the R-Inf, which shows the power of PLMs for this task. However, since the accuracy of the SRol is already very high, using synthetic supervision shows no improvements compared to the model that only trained with MSPRL training set for the SRol. In contrast, on SPARTQA-HUMAN, using synthetic supervision helps the model perform better. Especially, using SPARTUN increases the performance of the SRol model dramatically, by $15 \%$.</p>
<p>In table 10, we show the result of SRel model (containing spatial relation extraction and spatial relation type classification) for spatial relation extraction, with and without extra supervision from synthetic data. Same as SRol model, extra supervision from SPARTUN achieves the best result when tested on SPARTQA-HUMAN.</p>
<p>For MSPRL, we compared the SRel model with R-Inf on spatial relation extraction. As table 10 demonstrates we improve the SOTA by $2.6 \%$ on F1 measure using SPARTUN as synthetic supervision. Also, model further pretrained on SPARTQAAuto gets lower result than model with no extra</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">SynSup</th>
<th style="text-align: center;">MSPRL</th>
<th style="text-align: center;">SPARTQA-H</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">R-Inf</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">68.78</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRel</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">69.12</td>
<td style="text-align: center;">S: 48.58</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q: 49.46</td>
</tr>
<tr>
<td style="text-align: center;">SRel</td>
<td style="text-align: center;">SPARTQA-A</td>
<td style="text-align: center;">68.84</td>
<td style="text-align: center;">S: 58.32</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q: 55.17</td>
</tr>
<tr>
<td style="text-align: center;">SRel</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">71.23</td>
<td style="text-align: center;">S: 61:53</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q: 63.22</td>
</tr>
</tbody>
</table>
<p>Table 10: Spatial relation extraction (SRel) on MSPRL and SPARTQA-HUMAN(SPARTQA-H) with and without synthetic supervision. Since the questions(Q) and stories(S) in SPARTQA-HUMAN have different annotations (questions have missing roles), we separately train and test this model on each.
supervision due to the limited relation expressions used in this data.</p>
<p>In conclusion, our experiments show the efficiency of SPARTUN in improving the performance of models on different benchmarks due to the flexible coverage of relation types and expressions.</p>
<h2>6 Conclusion and Future Work</h2>
<p>We created a new synthetic dataset as a source of supervision for transfer learning for spatial question answering (SQA) and spatial role labeling (SPRL) tasks. We show that expanding the coverage of relation types and combinations and spatial language expressions can provide a more robust source of supervision for pretraining and transfer learning. As a result, this data improves the models' performance in many experimental scenarios on both tasks when tested on various evaluation benchmarks. This data includes rules of spatial reasoning and the chain of logical reasoning for answering the questions that can be used for further research in the future.</p>
<p>Moreover, we provide a human-generated dataset on a realistic SQA task that can be used to evaluate the models and methods for spatial language understanding related tasks in real-world problems. This data is an extension of a previous benchmark on SPRL task with spatial semantic annotations. As a result, this dataset contains annotations for both SPRL and SQA tasks.</p>
<p>In future work, we plan to investigate explicit spatial reasoning over text by neuro-symbolic models. Moreover, using our methodology to generate synthetic spatial corpus in other languages or for other types of reasoning, such as temporal reasoning, is an exciting direction for future research.</p>
<h2>Limitations</h2>
<p>Though we aim for a broad coverage of relation types and relations, we collected this from our available resources and spatial lexicons but this is not by any means complete. There can be relations and expressions that are not covered. In particular, the relation expressions are limited to verbs and prepositions. The performance and reasoning ability of our models is improved with transfer learning but this is, certainly, far from the natural language understanding desiderata. Our models are based on large language models and need GPU resources to execute.</p>
<h2>Acknowledgements</h2>
<p>This project is supported by National Science Foundation (NSF) CAREER award 2028626 and partially supported by the Office of Naval Research (ONR) grant N00014-20-1-2005. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation nor the Office of Naval Research. We thank all reviewers for their helpful comments and suggestions. We also thank Sania Sinha and Timothy Moran for their help in the human data generation and annotations.</p>
<h2>References</h2>
<p>Reem Alrashdi and Simon O’Keefe. 2020. Automatic Labeling of Tweets for Crisis Response Using Distant Supervision, page 418-425. Association for Computing Machinery, New York, NY, USA.</p>
<p>Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and Yoav Artzi. 2019. Touchdown: Natural language navigation and spatial reasoning in visual street environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12538-12547.</p>
<p>Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over language. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pages 3882-3890. International Joint Conferences on Artificial Intelligence Organization. Main track.</p>
<p>Surabhi Datta and Kirk Roberts. 2020. A hybrid deep learning approach for spatial trigger extraction from radiology reports. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing, volume 2020, page 50. NIH Public Access.</p>
<p>Surabhi Datta, Yuqi Si, Laritza Rodriguez, Sonya E Shooshan, Dina Demner-Fushman, and Kirk Roberts. 2020. Understanding spatial language in radiology: Representation framework, annotation, and spatial relation extraction from chest x-ray reports using deep learning. Journal of biomedical informatics, 108:103473.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages $4171-4186$.</p>
<p>Saman Enayati, Ziyu Yang, Benjamin Lu, and Slobodan Vucetic. 2021. A visualization approach for rapid labeling of clinical notes for smoking status extraction. In Proceedings of the Second Workshop on Data Science with Human in the Loop: Language Advances, pages 24-30, Online. Association for Computational Linguistics.</p>
<p>John Freeman. 1975. The modelling of spatial relations. Computer graphics and image processing, 4(2):156171.
A. Herskovits. 1986. Language and Spatial Cognition: An Interdisciplinary Study of the Prepositions in English. Cambridge University Press.</p>
<p>Maged N Kamel Boulos, Guochao Peng, and Trang VoPham. 2019. An overview of geoai applications in health and healthcare. International journal of health geographics, 18(1):1-9.</p>
<p>William G Kennedy, Magdalena D Bugajska, Matthew Marge, William Adams, Benjamin R Fransen, Dennis Perzanowski, Alan C Schultz, and J Gregory Trafton. 2007. Spatial representation and reasoning for human-robot collaboration. In AAAI, volume 7, pages $1554-1559$.</p>
<p>Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. UNIFIEDQA: Crossing format boundaries with a single QA system. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1896-1907, Online. Association for Computational Linguistics.</p>
<p>Parisa Kordjamshidi, Marie-Francine Moens, and Martijn van Otterlo. 2010. Spatial Role Labeling: Task definition and annotation scheme. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC'10), pages 413-420. European Language Resources Association (ELRA).</p>
<p>Parisa Kordjamshidi, Taher Rahgooy, Marie-Francine Moens, James Pustejovsky, Umar Manzoor, and Kirk Roberts. 2017. Clef 2017: Multimodal spatial role</p>
<p>labeling (msprl) task overview. In International Conference of the Cross-Language Evaluation Forum for European Languages, pages 367-376. Springer.</p>
<p>Parisa Kordjamshidi, Martijn Van Otterlo, and MarieFrancine Moens. 2011. Spatial role labeling: Towards extraction of spatial relations from natural language. ACM Transactions on Speech and Language Processing (TSLP), 8(3):1-36.</p>
<p>Dongdong Li, Zhaochun Ren, Pengjie Ren, Zhumin Chen, Miao Fan, Jun Ma, and Maarten de Rijke. 2021. Semi-supervised variational reasoning for medical dialogue generation. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 544554 .</p>
<p>Tao Li, Vivek Gupta, Maitrey Mehta, and Vivek Srikumar. 2019. A logic-driven framework for consistency of neural models. arXiv preprint arXiv:1909.00126.</p>
<p>Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. 2017. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980-2988.</p>
<p>Kate Lockwood, Ken Forbus, D Halstead, and Jeffrey Usher. 2006. Automatic categorization of spatial prepositions. In Proceedings of the 28th annual conference of the cognitive science society, pages 17051710 .</p>
<p>Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101.</p>
<p>Oswaldo Ludwig, Xiao Liu, Parisa Kordjamshidi, and Marie-Francine Moens. 2016. Deep embedding for spatial role labeling. arXiv preprint arXiv:1603.08474.</p>
<p>Kaixin Ma, Filip Ilievski, Jonathan Francis, Yonatan Bisk, Eric Nyberg, and Alessandro Oltramari. 2021. Knowledge-driven data construction for zero-shot evaluation in commonsense question answering. In 35th AAAI Conference on Artificial Intelligence.</p>
<p>Arjun Magge, Davy Weissenbacher, Abeed Sarker, Matthew Scotch, and Graciela Gonzalez-Hernandez. 2018. Deep neural networks and distant supervision for geographic location mention extraction. Bioinformatics, 34(13):i565-i573.</p>
<p>Umar Manzoor and Parisa Kordjamshidi. 2018. Anaphora resolution for improving spatial relation extraction from text. In Proceedings of the First International Workshop on Spatial Language Understanding, pages 53-62.</p>
<p>David M Mark et al. 1989. Languages of spatial relations: Researchable questions \&amp; NCGIA research agenda. National Center for Geographic Information and Analysis Santa Barbara ....</p>
<p>Wouter Massa, Parisa Kordjamshidi, Thomas Provoost, and Marie-Francine Moens. 2015. Machine reading of biological texts: bacteria-biotope extraction. In Proceedings of the 6th international conference on bioinformatics models, methods and algorithms, pages 55-64. SCITEPRESS.</p>
<p>Alexey Mazalov, Bruno Martins, and David Matos. 2015. Spatial role labeling with convolutional neural networks. In Proceedings of the 9th Workshop on Geographic Information Retrieval, pages 1-7.</p>
<p>Junghyun Min, R. Thomas McCoy, Dipanjan Das, Emily Pitler, and Tal Linzen. 2020. Syntactic data augmentation increases robustness to inference heuristics. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2339-2352, Online. Association for Computational Linguistics.</p>
<p>Roshanak Mirzaee, Hossein Rajaby Faghihi, Qiang Ning, and Parisa Kordjamshidi. 2021. SPARTQA: A textual question answering benchmark for spatial reasoning. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4582-4598, Online. Association for Computational Linguistics.</p>
<p>Alaeddine Moussa, Sébastien Fournier, Khaoula Mahmoudi, Bernard Espinasse, and Sami Faiz. 2021. Spatial role labeling based on improved pre-trained word embeddings and transfer learning. Procedia Computer Science, 192:1218-1226.</p>
<p>James Pustejovsky, Parisa Kordjamshidi, MarieFrancine Moens, Aaron Levine, Seth Dworman, and Zachary Yocum. 2015. Semeval-2015 task 8: Spaceeval. In Proceedings of the 9th International Workshop on Semantic Evaluation (semeval 2015), pages 884-894. ACL.</p>
<p>David A Randell, Zhan Cui, and Anthony G Cohn. 1992. A spatial logic based on regions and connection. $K R$, $92: 165-176$.</p>
<p>Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. 2022. Impact of pretraining term frequencies on few-shot reasoning. arXiv preprint arXiv:2202.07206.</p>
<p>Jochen Renz and Bernhard Nebel. 2007. Qualitative spatial reasoning using constraint calculi. In Handbook of spatial logics, pages 161-215. Springer.</p>
<p>Mohammed Saeed, Naser Ahmadi, Preslav Nakov, and Paolo Papotti. 2021. RuleBERT: Teaching soft rules to pre-trained language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1460-1476, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Zhengxiang Shi, Qiang Zhang, and Aldo Lipani. 2022. Stepgame: A new benchmark for robust multi-hop</p>
<p>spatial reasoning in texts. In Proceedings of the Association for the Advancement of Artificial Intelligence, AAAI '22.</p>
<p>Hyeong Jin Shin, Jeong Yeon Park, Dae Bum Yuk, and Jae Sung Lee. 2020. Bert-based spatial information extraction. In Proceedings of the Third International Workshop on Spatial Language Understanding, pages $10-17$.</p>
<p>Kristin Stock, Christopher B Jones, Shaun Russell, Mansi Radke, Prarthana Das, and Niloofar Aflaki. 2022. Detecting geospatial location descriptions in natural language text. International Journal of Geographical Information Science, 36(3):547-584.</p>
<p>Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. 2017. A corpus of natural language for visual reasoning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 217-223, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021. ProofWriter: Generating implications, proofs, and abductive statements over natural language. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3621-3634, Online. Association for Computational Linguistics.</p>
<p>Emile Van Krieken, Erman Acar, and Frank Van Harmelen. 2019. Semi-supervised learning using differentiable reasoning. arXiv preprint arXiv:1908.04700.</p>
<p>Sagar Gubbi Venkatesh, Anirban Biswas, Raviteja Upadrashta, Vikram Srinivasan, Partha Talukdar, and Bharadwaj Amrutur. 2021. Spatial reasoning from natural language instructions for robot manipulation. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 11196-11202. IEEE.</p>
<p>Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merriënboer, Armand Joulin, and Tomas Mikolov. 2015. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698.</p>
<p>Diedrich Wolter. 2009. Sparq-a spatial reasoning toolbox. In AAAI Spring Symposium: Benchmarking of Qualitative Spatial and Temporal Reasoning Systems, page 53 .</p>
<p>Liang Yao, Chengsheng Mao, and Yuan Luo. 2019. Kgbert: Bert for knowledge graph completion. arXiv preprint arXiv:1909.03193.</p>
<p>Yue Zhang, Quan Guo, and Parisa Kordjamshidi. 2021. Towards navigation by reasoning over spatial configurations. In Proceedings of Second International Combined Workshop on Spatial Language Understanding and Grounded Communication for Robotics, pages 42-52, Online. Association for Computational Linguistics.</p>
<p>Yue Zhang and Parisa Kordjamshidi. 2022. Explicit object relation alignment for vision and language navigation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 322-331, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Xiangxin Zhu, Carl Vondrick, Charless C Fowlkes, and Deva Ramanan. 2016. Do we need more training data? International Journal of Computer Vision, 119(1):76-92.</p>
<p>Jordan Zlatev. 2008. Holistic spatial semantics of thai. In Cognitive linguistics and non-Indo-European languages, pages 305-336. De Gruyter Mouton.</p>
<h2>A Datasets</h2>
<h2>A. 1 SPARTUN</h2>
<p>As we described in Section 4 to cover more spatial expressions and spatial relation types, we provide an extendable vocabulary of these spatial phenomena. The entire vocabulary of supported relation expressions and entity properties are provided in Figure 10.</p>
<p>Statistic information: Each example in SPARTUN contains a story that describe the spatial relation between entities and some questions which ask about indirect relations between entities. On average, each story contains eight sentences and 91 tokens, which describe ten relations on average.</p>
<p>We follow SPARTQA for dataset split. The number of questions in each train, dev, and test sets is provided in Table 3. YN questions can have two answers "Yes," which is the answer to $54 \%$ of questions, and "No," which is the answer to $46 \%$ of questions.</p>
<p>FR is a question type with multiple answers. In below, you can see the percentage of existence of each relation in the whole data: { left : $10 \%$, right: $10 \%$, above: $27 \%$, below: $26 \%$, behind: $19 \%$, front: $10 \%$, near: $2 \%$, far: $15 \%$, dc: $26 \%$, ec: $7 \%$, po: $0.2 \%$, tpp: $2 \%$, ntpp: $10 \%$, tppi: $3 \%$, and ntppi: $8 \%$ }</p>
<h2>A. 2 RESQ</h2>
<p>The RESQ dataset generated over the context of MSPRL dataset. For each group of sentences ( describing an image), we ask three volunteers (English-speaking undergraduate students) to generate at least four Yes/No questions. On average, they spent 20 minutes generating questions for each group of sentences which, in total, they spent 210 hours generating the whole data. After gathering the data, another undergrad student check the questions and remove the incorrect ones and keep the rest. The train set is provided on the train set of MSPRL, and since it does not have a dev set, we split the $32 \%$ of test data (equal to $20 \%$ of the training set) and keep it as the dev set. $50 \%$ of questions in this data are "Yes" and 50\% are "No". The static information of this dataset comes in Table 3.</p>
<p>To compute the human accuracy we ask two undergraduate students, one from those who create the questions and one new volunteer to answer 100 questions from the test set of RESQ. In the end a third students grade their answers.</p>
<h2>A. 3 bAbI</h2>
<p>This dataset is automatically generated data including samples with two sentences describing relationships between three objects and Yes/No questions asking about the existence of a relation between two objects (Fig 5) focuses on multi-hop spatial reasoning question answering.</p>
<h2>"The pink rectangle is below the red square. The red square is below the blue square," <br> 1. Is the red square below the pink rectangle? No <br> 2. Is the pink rectangle below the blue square? Yes</h2>
<p>Figure 5: An example of bAbI
bAbI task 19, contain questions asking about the directed path from one room to another. More statistic information of this dataset comes in table 3.</p>
<h2>A. 4 SPARTQA</h2>
<p>SPARTQA-AUTO contains more complex textual context (story) and questions requiring complex multi-hop spatial reasoning (e.g. Fig 6). This datasets contains one large synthesized (SPARTQAAuto) and a small human-generated (SPARTQAHUMAN) subsets.</p>
<p>One of the advantages of SPARTQA is the SPRL annotation of whole data (Contexts and Questions) provided with the main dataset. In this work, we also recruited two experts annotator which spent 270 hours annotating 2 k sentences in SPARTQAHUMAN using WebAnno framework ${ }^{15}$. Then another expert annotator checks their annotation and discards the wrong ones. The statistic information of SPARTQA comes in Table 3.</p>
<h2>A. 5 StepGame</h2>
<p>StepGame is another synthesized datasets described in this paper. You can check a sample of this dataset in Figure 7.</p>
<h2>A. 6 MSPRL</h2>
<p>SPRL is the task of identifying and classifying the spatial arguments of the spatial expressions mentioned in a sentence (Kordjamshidi et al., 2010). The MSPRL(Kordjamshidi et al., 2017) is a dataset provided on SPRL task.. The statistic data of this dataset comes in Table 11. A SPRL can have following spatial semantic component (Zlatev, 2008) on the static environment, trajector (the main</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>STORY:</h1>
<p>We have three blocks, A, B and C. Block B is to the right of block C and it is below block A. Block A has two black medium squares. Medium black square number one is below medium black square number two and a medium blue square. It is touching the bottom edge of this block. The medium blue square is below medium black square number two. Block B contains one medium black square. Block C contains one medium blue square and one medium black square. The medium blue square is below the medium black square.</p>
<h2>QUESTIONS:</h2>
<p>FB: Which block(s) has a medium thing that is below a black square? A, B, C
FB: Which block(s) doesn't have any blue square that is to the left of a medium square? A, B
FR: What is the relation between the medium black square which is in block $C$ and the medium square that is below a medium black square that is touching the bottom edge of a block? Left
CO: Which object is above a medium black square? the medium black square which is in block C or medium black square number two? medium black square number two
YN: Is there a square that is below medium square number two above all medium black squares that are touching the bottom edge of a block? Yes</p>
<p>Figure 6: An example of SPARTQA-AUTO</p>
<div class="codehilite"><pre><span></span><code><span class="n">Story</span><span class="o">:</span>
<span class="w">    </span><span class="mi">0</span><span class="o">:</span><span class="s2">&quot;B is south east of J.&quot;</span>
<span class="w">    </span><span class="mi">1</span><span class="o">:</span><span class="s2">&quot;X is under E.&quot;</span>
<span class="w">    </span><span class="mi">2</span><span class="o">:</span><span class="s2">&quot;H is to the left of Z and is on the same horizontal plane.&quot;</span>
<span class="w">    </span><span class="mi">3</span><span class="o">:</span><span class="s2">&quot;If L is the center of a clock face, E is located between 10 and 11.&quot;</span>
<span class="w">    </span><span class="mi">4</span><span class="o">:</span><span class="s2">&quot;G is positioned above O.&quot;</span>
<span class="w">    </span><span class="mi">5</span><span class="o">:</span><span class="s2">&quot;Q is diagonally to the bottom right of L.&quot;</span>
<span class="w">    </span><span class="mi">6</span><span class="o">:</span><span class="s2">&quot;C and S are horizontal and C is to the left of 0.&quot;</span>
<span class="w">    </span><span class="mi">7</span><span class="o">:</span><span class="s2">&quot;I is above 0 with a small gap between them.&quot;</span>
<span class="w">    </span><span class="mi">8</span><span class="o">:</span><span class="s2">&quot;E is above N and to the left of N.&quot;</span>
<span class="w">    </span><span class="mi">9</span><span class="o">:</span><span class="s2">&quot;Q is below and to the right of 0.&quot;</span>
<span class="w">    </span><span class="mi">10</span><span class="o">:</span><span class="s2">&quot;X is to the left of C with a small gap between them.&quot;</span>
</code></pre></div>

<p>question:"What is the relation of the agent $L$ to the agent $J$ ? "lower-right"</p>
<p>Figure 7: StepGame. An example of questions which need 10 steps of reasoning.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Test</th>
<th style="text-align: center;">All</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Sentences</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">613</td>
<td style="text-align: center;">1213</td>
</tr>
<tr>
<td style="text-align: left;">Trajectors</td>
<td style="text-align: center;">716</td>
<td style="text-align: center;">874</td>
<td style="text-align: center;">1590</td>
</tr>
<tr>
<td style="text-align: left;">Landmarks</td>
<td style="text-align: center;">612</td>
<td style="text-align: center;">573</td>
<td style="text-align: center;">1185</td>
</tr>
<tr>
<td style="text-align: left;">Spatial Indicators</td>
<td style="text-align: center;">666</td>
<td style="text-align: center;">795</td>
<td style="text-align: center;">1461</td>
</tr>
<tr>
<td style="text-align: left;">Spatial Triplets</td>
<td style="text-align: center;">761</td>
<td style="text-align: center;">939</td>
<td style="text-align: center;">1700</td>
</tr>
</tbody>
</table>
<p>Table 11: MSPRL statistics.
entity), landmark(the reference entity), and spatial_indicator (the spatial term describing the relationship between trajector and landmark.). The dynamic environment can also have path, region, direction, and motion. To understand MSPRL better you can take a look at Figure 8. In this figure the spatial value assigned to each spatial triplet can be chosen from Table 1.</p>
<p>The white car in the street, is in front of the blue building.</p>
<p>Trajector $I=$ The white car
Landmark $I=$ the street
Spatial_indicator $=i n$
Genaral type $=$ Topological (RCC8 (TPP))
TPP(the white car, the street)</p>
<p>Trajector $I=$ The white car
Landmark $I=$ the blue building
Spatial_indicator $=$ in front
Genaral type $=$ Directional (FRONT)
FRONT(the white car, the blue building)</p>
<p>Figure 8: Spatia Role Labeling (SpRL).</p>
<h2>B Models and modules</h2>
<p>We use the huggingFace ${ }^{16}$ implementation of pretrained BERT base which has 768 hidden dimensions. All models are trained on the training set, evaluated on the dev set, and reported the result on the test set. For training, we train the model until no changes happen on the dev set and then store and use the best model on the dev set. We use AdamW ((Loshchilov and Hutter, 2017)) optimizer on all models and modules.</p>
<p>For SQA tasks we use Focal Loss (Lin et al., 2017) with $\gamma=2$. For spatial argument extraction,</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>we use cross-entropy loss for BIO-tagging, and for spatial relation extraction, we use the summation of loss for each spatial relation and relation type classification part.</p>
<p>$$
\begin{aligned}
\text { Loss }= &amp; \sum \text { CrossEntropyLoss }\left(\mathrm{p}^{\prime}, \mathrm{y}^{\prime}\right) \
&amp; +\operatorname{BCELoss}(\mathrm{p}, \mathrm{y})
\end{aligned}
$$</p>
<p>The rest of experimental setting such as number of epochs, batch size, and learning rate are provided in Table 13. This settings are chosen after trial and test on the dev set of the target task.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">YN</th>
<th style="text-align: center;">FR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SPARTUN</td>
<td style="text-align: center;">92.83</td>
<td style="text-align: center;">93.66</td>
</tr>
<tr>
<td style="text-align: left;">SPARTUN-Simple</td>
<td style="text-align: center;">90.30</td>
<td style="text-align: center;">93.66</td>
</tr>
<tr>
<td style="text-align: left;">SPARTUN-Clock</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">87.13</td>
</tr>
<tr>
<td style="text-align: left;">SPARTQA</td>
<td style="text-align: center;">82.05</td>
<td style="text-align: center;">94.17</td>
</tr>
</tbody>
</table>
<p>Table 12: Result of BERT (SQA) model trained and test on two synthetic supervision data.</p>
<p>Besides, The result of BERT model trained on SPARTUN and SPARTUN and tested on the same dataset are provided in Table 12. SPARTUNSimple only contains one spatial expresion for each relation types, and SPARTUN-Clock contains all relation expression plus clock expressions (Column 5 in Table 10a) for relation types.</p>
<h1>B. 1 Logic-based spatial reasoner</h1>
<p>We consider the logic rules mentioned in Figure 2 and in the form of the Horn clauses. we collect the different combinations of spatial relations mentioned in Table 1 and implement the logic-based spatial reasoner. Figure 9a shows an example of some parts of our code on LEFT relation. In Figure 9 b , on the left, some facts are given, and the query " $n t p p i($ room, $X)$ " ask about all objects that existed in the room. Below each query, there are all possible predictions for them.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Experiment</th>
<th style="text-align: center;">DS</th>
<th style="text-align: center;">epochs</th>
<th style="text-align: center;">batch size</th>
<th style="text-align: center;">learning rate</th>
<th style="text-align: center;">classifier type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">PLM</td>
<td style="text-align: center;">SPARTQA YN</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">$8 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">PLM</td>
<td style="text-align: center;">SPARTQA FR</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">$8 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">PLM</td>
<td style="text-align: center;">StepGame</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">multi-class</td>
</tr>
<tr>
<td style="text-align: center;">PLM</td>
<td style="text-align: center;">SPARTUN YN</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">$8 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">PLM</td>
<td style="text-align: center;">SPARTUN FR</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">$8 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">SQA experiments</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">bAbI task 17</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">bAbI task 17</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">bAbI task 17</td>
<td style="text-align: center;">SPARTQA</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">bAbI task 17</td>
<td style="text-align: center;">StepGame</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">bAbI task 19</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">bAbI task 19</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">bAbI task 19</td>
<td style="text-align: center;">SPARTQA</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">bAbI task 19</td>
<td style="text-align: center;">StepGame</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">SPARTQA-HUMAN YN</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">SPARTQA-HUMAN YN</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">SPARTQA-HUMAN YN</td>
<td style="text-align: center;">SPARTQA</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">SPARTQA-HUMAN YN</td>
<td style="text-align: center;">StepGame</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">SPARTQA-HUMAN FR</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">SPARTQA-HUMAN FR</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">SPARTQA-HUMAN FR</td>
<td style="text-align: center;">SPARTQA</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">SPARTQA-HUMAN FR</td>
<td style="text-align: center;">StepGame</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">StepGame</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-0$</td>
<td style="text-align: center;">multi-class</td>
</tr>
<tr>
<td style="text-align: center;">StepGame</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">multi-class</td>
</tr>
<tr>
<td style="text-align: center;">StepGame</td>
<td style="text-align: center;">SPARTQA</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">multi-class</td>
</tr>
<tr>
<td style="text-align: center;">RESQ</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$2 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">RESQ</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">RESQ</td>
<td style="text-align: center;">SPARTQA</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">RESQ</td>
<td style="text-align: center;">StepGame</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">boolean cls</td>
</tr>
<tr>
<td style="text-align: center;">SPRL experiments</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SRol</td>
<td style="text-align: center;">SPARTQA</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-06$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRel</td>
<td style="text-align: center;">SPARTQA</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-07$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRol</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$8 \mathrm{e}-06$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRel</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-07$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRol - SPARTQA-HUMAN</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-05$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRol - SPARTQA-HUMAN</td>
<td style="text-align: center;">SPARTQA</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-06$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRol - SPARTQA-HUMAN</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$4 \mathrm{e}-07$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRol - MSPRL</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-06$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRol - MSPRL</td>
<td style="text-align: center;">SPARTQA</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-06$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRol - MSPRL</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-06$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRel - SPARTQA-HUMAN</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-06$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRel - SPARTQA-HUMAN</td>
<td style="text-align: center;">SPARTQA</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-06$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRel - SPARTQA-HUMAN</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$8 \mathrm{e}-07$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRel - MSPRL</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 \mathrm{e}-05$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRel - MSPRL</td>
<td style="text-align: center;">SPARTQA</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$4 \mathrm{e}-06$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SRel - MSPRL</td>
<td style="text-align: center;">SPARTUN</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$6 \mathrm{e}-06$</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 13: The hyperparameters and setups information for each experiment. The first three rows are related to further pretraining model on the synthetic data. These models are used in the other experiments as the further pretrained models.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" />
(a) Examaple of implemented rule clauses in Prolog.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" />
(b) Example of Facts, Query, and answer of implemented model</p>
<p>Figure 9: Logic-bases spatial reasoner</p>
<table>
<thead>
<tr>
<th style="text-align: center;">formalism</th>
<th style="text-align: center;">Type</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Cardinals</th>
<th style="text-align: center;">Clocks</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Directional</td>
<td style="text-align: center;">left</td>
<td style="text-align: center;">"to the left of", "on the left side of", "to the left-hand side of"</td>
<td style="text-align: center;">"west of", "to the west of"</td>
<td style="text-align: center;">"at 9:00 position relative to", "at 9: <br> 00 position regarding to", "at 9 <br> o'clock position regarding to"</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">right</td>
<td style="text-align: center;">"to the right of", "on the right side of", "to the right-hand side of"</td>
<td style="text-align: center;">"east of", "to the east of"</td>
<td style="text-align: center;">"at 3:00 position relative to", "at 3: <br> 00 position regarding to", "at 3 <br> o'clock position regarding to"</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">below</td>
<td style="text-align: center;">"above", "over"</td>
<td style="text-align: center;">"north of", "to the north of"</td>
<td style="text-align: center;">"at 12:00 position relative to", "at 12:00 position regarding to", "at 12 o'clock position regarding to"</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">above</td>
<td style="text-align: center;">"below", "under"</td>
<td style="text-align: center;">"south of", "to the south of"</td>
<td style="text-align: center;">"at 6:00 position relative to", "at 6: <br> 00 position regarding to", "at 6 <br> o'clock position regarding to"</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">behind</td>
<td style="text-align: center;">"behind"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">front</td>
<td style="text-align: center;">"in front of"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Distances</td>
<td style="text-align: center;">far</td>
<td style="text-align: center;">"far from", "farther from", "away from"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">near</td>
<td style="text-align: center;">"near to", "close to"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Topological</td>
<td style="text-align: center;">DC</td>
<td style="text-align: center;">disconnected from</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EC</td>
<td style="text-align: center;">"touch[es]"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PO</td>
<td style="text-align: center;">"overlap[s]"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EQ</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TPP</td>
<td style="text-align: center;">"covered by", "inside and touching"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TPPI</td>
<td style="text-align: center;">"cover[s]"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NTPP</td>
<td style="text-align: center;">"in", "inside", "within"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NTPPI</td>
<td style="text-align: center;">"ha[s/ve]","contain[s]"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>(a) List of relation expression supported in SPARTUN.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">properties</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">block</td>
<td style="text-align: center;">Block, box</td>
</tr>
<tr>
<td style="text-align: center;">blocks</td>
<td style="text-align: center;">Blocks, boxes</td>
</tr>
<tr>
<td style="text-align: center;">object_general_name</td>
<td style="text-align: center;">thing, object, shape, fruit</td>
</tr>
<tr>
<td style="text-align: center;">objects_general_name</td>
<td style="text-align: center;">things, objects, shapes, fruits</td>
</tr>
<tr>
<td style="text-align: center;">block_name</td>
<td style="text-align: center;">AAA, BBB, CCC, DDD, EEE, JJJ, HHH, JJJ, LLL, KKK, one, two, three.</td>
</tr>
<tr>
<td style="text-align: center;">color</td>
<td style="text-align: center;">yellow, black, blue, green, red, orange, grey, white, purple</td>
</tr>
<tr>
<td style="text-align: center;">size</td>
<td style="text-align: center;">small, big, medium, midsize, large, tiny, little</td>
</tr>
<tr>
<td style="text-align: center;">type</td>
<td style="text-align: center;">circle, oval, square, rectangle, dimond, star, triangle, hexagon, pentagon, watermelon, apple, melon,</td>
</tr>
<tr>
<td style="text-align: center;">types</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>(b) List of entities properties supported in SPARTUN</p>
<p>Figure 10: The supported relation expression and entities properties in SPARTUN, which can easily extended based on the target task.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{16}$ https://huggingface.co/transformers/v2.9.1/ model_doc/bert.html&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{11}$ https://github.com/HLR/Spatial-QA-tasks
${ }^{12}$ StepGame only has FR question types. Hence, we use the model trained on FR questions for both FR and YN target tasks.
${ }^{13}$ All human results gathered by scoring the human answers over a subset of the test set.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6078 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6078</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6078</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-120.html">extraction-schema-120</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-76427fe94e4564fd5df2177bb259d93527fddca5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/76427fe94e4564fd5df2177bb259d93527fddca5" target="_blank">InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work introduces InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training and achieves new state-of-the-art results on the BEIR benchmark.</p>
                <p><strong>Paper Abstract:</strong> Recently, InPars introduced a method to efficiently use large language models (LLMs) in information retrieval tasks: via few-shot examples, an LLM is induced to generate relevant queries for documents. These synthetic query-document pairs can then be used to train a retriever. However, InPars and, more recently, Promptagator, rely on proprietary LLMs such as GPT-3 and FLAN to generate such datasets. In this work we introduce InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training. A simple BM25 retrieval pipeline followed by a monoT5 reranker finetuned on InPars-v2 data achieves new state-of-the-art results on the BEIR benchmark. To allow researchers to further improve our method, we open source the code, synthetic data, and finetuned models: https://github.com/zetaalphavector/inPars/tree/master/tpu</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6078.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6078.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InPars-v2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InPars-v2 dataset generator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source method that uses an open LLM to generate synthetic query-document pairs from corpora and a pretrained reranker to filter and select high-quality pairs for training rerankers for information retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>InPars-v2</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Generates synthetic queries for documents via few-shot prompting of an open LLM (GPT-J-6B), filters the generated query-document pairs with a monoT5-3B reranker, constructs positive/negative training pairs, and fine-tunes rerankers for retrieval tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-J-6B (generator); monoT5-3B (reranker used both for filtering and as the finetuned reranker)</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>For each BEIR dataset, up to 100k documents were sampled from the target corpus; one synthetic query per sampled document was generated using 3-shot prompts drawn from MS MARCO examples (gbq prompt template). When corpora were smaller than 100k (e.g., ArguAna), all documents were used.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Synthetic-data distillation: few-shot LLM-based query generation from documents, followed by reranker-based scoring to filter generated pairs; construct positives (top 10k scored pairs) and negatives (random documents from BM25 top-1000 for each synthetic query), then finetune a reranker on this synthetic dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>A synthetic labeled dataset of query-document pairs (10k positives + 10k negatives per dataset) and finetuned monoT5-3B reranker models (one per BEIR dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Zero-shot retrieval evaluation on BEIR: BM25 retrieves top-1000 documents per query (Pyserini), then the finetuned monoT5-3B reranker re-ranks; effectiveness measured with nDCG@10 per-dataset and averaged across BEIR.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Achieved new state-of-the-art average nDCG@10 on BEIR reported in the paper: average 0.545 vs BM25 0.424 and monoT5-3B (MS MARCO) 0.533. Specific improvements over InPars-v1 on datasets such as TREC-News, Climate-FEVER, Robust and Touche; per-dataset nDCG@10 listed in Table 1 (e.g., Robust 0.632, Natural Questions 0.638, FEVER 0.872).</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>BEIR benchmark (18 target corpora); MS MARCO used for 3-shot examples and pretraining/finetuning monoT5-3B; individual BEIR corpora such as ArguAna, TREC-News, Climate-FEVER, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Quality depends on synthetic query generation and filtering; compute cost (approx. 30 hours on A100 to generate 100k queries; ~1.5 hours on TPU v3-8 to score 100k pairs); some BEIR datasets (argument retrieval sets like ArguAna and Touche) benefit from dataset-specific prompting and alternative methods; only one synthetic query per document in these experiments; filtering reduces to top 10k pairs which may discard useful diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared directly to InPars-v1 (original proprietary-LM-based method), Promptagator, and RankT5. InPars-v2 uses open-source GPT-J instead of proprietary Curie/FLAN and adds reranker-based filtering, yielding improved average BEIR performance. Promptagator and RankT5 sometimes perform better on argument-focused datasets due to dataset-specific prompts or architecture choices; RankT5 and Promptagator use different model sizes and pipelines (Promptagator uses FLAN and smaller T5 models without MS MARCO supervision).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6078.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6078.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InPars-v1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InPars (original)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The original InPars method that uses a (proprietary) LLM to generate synthetic queries from documents for data augmentation in information retrieval tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Inpars: Data augmentation for information retrieval using large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>InPars (v1)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses few-shot prompting of a proprietary LLM (OpenAI Curie in the original) to generate synthetic queries from documents; selects top synthetic pairs by generation log-probability and trains rerankers on the resulting synthetic dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Proprietary OpenAI 'curie' model (as used in the original InPars work referenced by this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Original InPars sampled documents from target corpora and generated synthetic queries using few-shot examples (details in the original paper); specific sampling in this paper references InPars-v1 as prior work but exact sizes are in the original work.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Few-shot LLM generation of synthetic query-document pairs; filtering by model generation log-probability to select top pairs for training rerankers.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Synthetic query-document training pairs used to fine-tune rerankers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>InPars-v1 evaluated on BEIR in original work (comparison referenced here).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>InPars-v1 produced improvements over baseline retrieval models on BEIR as reported by Bonifacio et al.; InPars-v2 builds upon and improves this pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>BEIR; MS MARCO used for example prompts in few-shot generation (as referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Relied on a proprietary LLM (limiting reproducibility); used log-prob filtering which InPars-v2 replaces with reranker-based filtering for improved selection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared to InPars-v2 (this paper) which shows better performance when using open LLMs and reranker filtering; contrasted with Promptagator which uses dataset-specific prompts and a different retrieval pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6078.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6078.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Promptagator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Promptagator: Few-shot dense retrieval from 8 examples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses few-shot prompting of a (proprietary) LLM to generate alternative queries for documents and builds a fully trainable retrieval pipeline with smaller models, emphasizing dataset-specific prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Promptagator: Few-shot dense retrieval from 8 examples</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Promptagator</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Feeds dataset-specific prompts to an LLM (FLAN) to generate synthetic queries and trains a dense retrieval pipeline (with smaller T5 models) in an unsupervised/few-shot manner designed to work with few examples.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>FLAN (proprietary finetuned family referenced in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Described as using few-shot prompts (the paper cites using 8 examples) and dataset-specific prompts to generate alternative queries for each document; specific corpus sizes are not detailed in this InPars-v2 paper (see Promptagator original).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Few-shot LLM-based synthetic query generation using dataset-specific prompt engineering; then train smaller retrieval and reranking models (dense retrieval focus).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Synthetic query sets and a fully trainable retrieval pipeline (dense retrievers and rerankers) tuned per dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Promptagator evaluated on BEIR in its original work; compared in this paper via reported nDCG@10 on BEIR datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Promptagator reports competitive results on BEIR and particularly strong gains on argument-retrieval datasets (e.g., ArguAna and Touche) due to dataset-specific prompt engineering; in the InPars-v2 comparison Promptagator outperforms InPars-v2 on some argument datasets but InPars-v2 achieves a higher overall BEIR average.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>BEIR (as reported/computed in comparisons); Promptagator's own experiments described in its paper.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Relies on dataset-specific prompts and a proprietary FLAN model; may perform better on certain dataset types (argument retrieval) but not uniformly across heterogeneous benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared in this paper against InPars-v2 and RankT5; noted to perform well on datasets where custom prompts help (ArguAna, Touche), and uses smaller T5-based retrieval/reranking models without MS MARCO supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6078.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6078.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-J-6B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-J-6B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 6-billion-parameter autoregressive language model used here as a few-shot query generator to create synthetic queries from documents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-J-6B (used as synthetic query generator)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An autoregressive LLM used with 3-shot MS MARCO prompts and greedy decoding to generate one synthetic query per document from sampled corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-J-6B (open-source, 6B parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Inputs were individual documents from BEIR corpora (sampling up to 100k documents per dataset) plus a 3-shot prompt template (MS MARCO examples, 'gbq' prompt). Greedy decoding was used.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Generative few-shot synthesis of queries from document text (synthetic query generation as data augmentation).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Synthetic natural-language queries paired with the original document (query-document pairs for training).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Quality of generated-and-filtered data assessed indirectly via downstream retrieval effectiveness (nDCG@10) when training/finetuning rerankers and evaluating on BEIR.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Used to generate the synthetic datasets for InPars-v2; with reranker-based filtering and subsequent finetuning, led to improved BEIR performance (InPars-v2 average nDCG@10 0.545). Generation of 100k queries takes ~30 hours on an A100 GPU as reported.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>BEIR corpora were the source of documents used for generation; MS MARCO examples used as few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Generation quality depends on prompt design and model capabilities; greedy decoding used (no sampling) may limit diversity; computational cost non-trivial (30 hours/A100 for 100k queries).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Replaces proprietary Curie (used in InPars-v1) and FLAN (used by Promptagator) in this work; shown to be effective when combined with reranker filtering, enabling an open-source reproducible pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6078.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6078.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FLAN (Finetuned Language Net)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of finetuned instruction-following language models (FLAN) referenced as the proprietary LLM used by Promptagator for query generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Finetuned language models are zero-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>FLAN (as used by Promptagator)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A finetuned instruction-following LM family used in prior work (Promptagator) to generate synthetic queries, particularly in a few-shot/dataset-specific prompting setup.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>FLAN (FLAN family referenced; specific checkpoint used by Promptagator referenced in that paper).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Promptagator used few-shot dataset-specific prompts (e.g., 8 examples) to generate queries for documents; exact corpus sizes are in the Promptagator paper rather than this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Few-shot prompting and instruction-tuning to produce synthetic queries for data augmentation of retrieval models.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Synthetic queries and datasets enabling training of retrieval/reranking models.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Promptagator evaluated via retrieval benchmarks (BEIR) in its original work; referenced in this paper comparatively.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Promptagator using FLAN shows strong performance on some BEIR datasets, notably argument retrieval datasets where dataset-specific prompts help; exact numbers are reported in Promptagator's paper and summarized in this paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>BEIR (comparative evaluations reported); Promptagator's own experimental corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Proprietary model use limits reproducibility; benefits depend on prompt engineering and model capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared in this paper to InPars-v2; Promptagator may outperform InPars-v2 on argument-centric datasets because of dataset-specific prompts and different retrieval pipeline design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6078.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6078.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>monoT5-3B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>monoT5-3B reranker</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3-billion-parameter T5-based reranker used both to filter synthetic query-document pairs and as the finetuned reranker for final evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Document ranking with a pretrained sequence-to-sequence model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>monoT5-3B reranker</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A sequence-to-sequence model (T5 family) adapted to reranking by producing relevance scores; in InPars-v2 it is pretrained/finetuned on MS MARCO and used to score candidate synthetic query-document pairs, then further finetuned on the filtered synthetic dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>monoT5-3B (T5-based reranker, 3B parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Takes query-document pairs as input; in filtering, monoT5-3B scored 100k synthetic query-document pairs per dataset (scoring took ~1.5 hours on TPU v3-8). For finetuning: positive and negative pairs batched (64 pos + 64 neg per batch) for one epoch on MS MARCO and one epoch on synthetic InPars-v2 data.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Used for filtering (select top 10k scored synthetic pairs) and then trained on the synthetic labeled pairs to transfer the synthetic data into reranker parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Relevancy scores for filtering; finetuned reranker models used for downstream document re-ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Downstream retrieval quality measured on BEIR via pipeline BM25 -> monoT5 rerank; nDCG@10 reported.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>monoT5-3B finetuned on MS MARCO then InPars-v2 data produced improved nDCG@10 across BEIR relative to baselines; example: avg nDCG@10 improved to 0.545 when using InPars-v2 synthetic data for finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>MS MARCO (used for initial finetuning), BEIR synthetic datasets created by InPars-v2, BEIR benchmark for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Large model (3B) compute requirements; scoring and finetuning require TPU/GPU resources; selection of top-k pairs may discard useful examples and is sensitive to the reranker's calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>monoT5-3B used here as both a stronger filter and final reranker outperforms the log-prob filtering used in InPars-v1; compared to RankT5 (a modified monoT5) and to smaller T5 models used by Promptagator.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6078.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6078.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RankT5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RankT5: Fine-tuning T5 for text ranking with ranking losses</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modified variant of T5 adapted for ranking tasks referenced as a comparison point; reported to have unpublished checkpoint and code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rankt5: Fine-tuning t5 for text ranking with ranking losses</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RankT5</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A T5-based ranking model fine-tuned with ranking losses tailored for retrieval tasks; used as a comparative baseline in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Modified T5 variants (exact checkpoint and code not published according to this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Standard query-document pairs for ranking; details are in the RankT5 paper rather than this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Fine-tuning T5 for ranking (not a distillation-from-many-papers method); used for direct ranking rather than dataset synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>A reranking model producing document relevance scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Evaluated on BEIR in comparisons; reported nDCG@10 values used in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Reported by authors and compared in this paper; RankT5 shows competitive retrieval performance on BEIR and sometimes strong results on datasets where its design suits the task.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>BEIR (comparison in this paper); original RankT5 paper contains details.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Checkpoint and code not publicly available per this paper, limiting reproducibility and direct comparison; architecture differences and training losses may make direct apples-to-apples comparison difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared in Table 1 to monoT5 variants and InPars-v2; RankT5 and Promptagator vary in strengths across datasets, with RankT5 showing competitive numbers in the table.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Inpars: Data augmentation for information retrieval using large language models <em>(Rating: 2)</em></li>
                <li>Promptagator: Few-shot dense retrieval from 8 examples <em>(Rating: 2)</em></li>
                <li>Rankt5: Fine-tuning t5 for text ranking with ranking losses <em>(Rating: 2)</em></li>
                <li>Finetuned language models are zero-shot learners <em>(Rating: 1)</em></li>
                <li>Document ranking with a pretrained sequence-to-sequence model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6078",
    "paper_id": "paper-76427fe94e4564fd5df2177bb259d93527fddca5",
    "extraction_schema_id": "extraction-schema-120",
    "extracted_data": [
        {
            "name_short": "InPars-v2",
            "name_full": "InPars-v2 dataset generator",
            "brief_description": "An open-source method that uses an open LLM to generate synthetic query-document pairs from corpora and a pretrained reranker to filter and select high-quality pairs for training rerankers for information retrieval.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "InPars-v2",
            "system_description": "Generates synthetic queries for documents via few-shot prompting of an open LLM (GPT-J-6B), filters the generated query-document pairs with a monoT5-3B reranker, constructs positive/negative training pairs, and fine-tunes rerankers for retrieval tasks.",
            "llm_model_used": "GPT-J-6B (generator); monoT5-3B (reranker used both for filtering and as the finetuned reranker)",
            "input_type_and_size": "For each BEIR dataset, up to 100k documents were sampled from the target corpus; one synthetic query per sampled document was generated using 3-shot prompts drawn from MS MARCO examples (gbq prompt template). When corpora were smaller than 100k (e.g., ArguAna), all documents were used.",
            "distillation_approach": "Synthetic-data distillation: few-shot LLM-based query generation from documents, followed by reranker-based scoring to filter generated pairs; construct positives (top 10k scored pairs) and negatives (random documents from BM25 top-1000 for each synthetic query), then finetune a reranker on this synthetic dataset.",
            "output_type": "A synthetic labeled dataset of query-document pairs (10k positives + 10k negatives per dataset) and finetuned monoT5-3B reranker models (one per BEIR dataset).",
            "evaluation_methods": "Zero-shot retrieval evaluation on BEIR: BM25 retrieves top-1000 documents per query (Pyserini), then the finetuned monoT5-3B reranker re-ranks; effectiveness measured with nDCG@10 per-dataset and averaged across BEIR.",
            "results": "Achieved new state-of-the-art average nDCG@10 on BEIR reported in the paper: average 0.545 vs BM25 0.424 and monoT5-3B (MS MARCO) 0.533. Specific improvements over InPars-v1 on datasets such as TREC-News, Climate-FEVER, Robust and Touche; per-dataset nDCG@10 listed in Table 1 (e.g., Robust 0.632, Natural Questions 0.638, FEVER 0.872).",
            "datasets_or_benchmarks": "BEIR benchmark (18 target corpora); MS MARCO used for 3-shot examples and pretraining/finetuning monoT5-3B; individual BEIR corpora such as ArguAna, TREC-News, Climate-FEVER, etc.",
            "challenges_or_limitations": "Quality depends on synthetic query generation and filtering; compute cost (approx. 30 hours on A100 to generate 100k queries; ~1.5 hours on TPU v3-8 to score 100k pairs); some BEIR datasets (argument retrieval sets like ArguAna and Touche) benefit from dataset-specific prompting and alternative methods; only one synthetic query per document in these experiments; filtering reduces to top 10k pairs which may discard useful diversity.",
            "comparisons_to_other_methods": "Compared directly to InPars-v1 (original proprietary-LM-based method), Promptagator, and RankT5. InPars-v2 uses open-source GPT-J instead of proprietary Curie/FLAN and adds reranker-based filtering, yielding improved average BEIR performance. Promptagator and RankT5 sometimes perform better on argument-focused datasets due to dataset-specific prompts or architecture choices; RankT5 and Promptagator use different model sizes and pipelines (Promptagator uses FLAN and smaller T5 models without MS MARCO supervision).",
            "uuid": "e6078.0",
            "source_info": {
                "paper_title": "InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "InPars-v1",
            "name_full": "InPars (original)",
            "brief_description": "The original InPars method that uses a (proprietary) LLM to generate synthetic queries from documents for data augmentation in information retrieval tasks.",
            "citation_title": "Inpars: Data augmentation for information retrieval using large language models",
            "mention_or_use": "mention",
            "system_name": "InPars (v1)",
            "system_description": "Uses few-shot prompting of a proprietary LLM (OpenAI Curie in the original) to generate synthetic queries from documents; selects top synthetic pairs by generation log-probability and trains rerankers on the resulting synthetic dataset.",
            "llm_model_used": "Proprietary OpenAI 'curie' model (as used in the original InPars work referenced by this paper).",
            "input_type_and_size": "Original InPars sampled documents from target corpora and generated synthetic queries using few-shot examples (details in the original paper); specific sampling in this paper references InPars-v1 as prior work but exact sizes are in the original work.",
            "distillation_approach": "Few-shot LLM generation of synthetic query-document pairs; filtering by model generation log-probability to select top pairs for training rerankers.",
            "output_type": "Synthetic query-document training pairs used to fine-tune rerankers.",
            "evaluation_methods": "InPars-v1 evaluated on BEIR in original work (comparison referenced here).",
            "results": "InPars-v1 produced improvements over baseline retrieval models on BEIR as reported by Bonifacio et al.; InPars-v2 builds upon and improves this pipeline.",
            "datasets_or_benchmarks": "BEIR; MS MARCO used for example prompts in few-shot generation (as referenced).",
            "challenges_or_limitations": "Relied on a proprietary LLM (limiting reproducibility); used log-prob filtering which InPars-v2 replaces with reranker-based filtering for improved selection.",
            "comparisons_to_other_methods": "Compared to InPars-v2 (this paper) which shows better performance when using open LLMs and reranker filtering; contrasted with Promptagator which uses dataset-specific prompts and a different retrieval pipeline.",
            "uuid": "e6078.1",
            "source_info": {
                "paper_title": "InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Promptagator",
            "name_full": "Promptagator: Few-shot dense retrieval from 8 examples",
            "brief_description": "A method that uses few-shot prompting of a (proprietary) LLM to generate alternative queries for documents and builds a fully trainable retrieval pipeline with smaller models, emphasizing dataset-specific prompts.",
            "citation_title": "Promptagator: Few-shot dense retrieval from 8 examples",
            "mention_or_use": "mention",
            "system_name": "Promptagator",
            "system_description": "Feeds dataset-specific prompts to an LLM (FLAN) to generate synthetic queries and trains a dense retrieval pipeline (with smaller T5 models) in an unsupervised/few-shot manner designed to work with few examples.",
            "llm_model_used": "FLAN (proprietary finetuned family referenced in the paper).",
            "input_type_and_size": "Described as using few-shot prompts (the paper cites using 8 examples) and dataset-specific prompts to generate alternative queries for each document; specific corpus sizes are not detailed in this InPars-v2 paper (see Promptagator original).",
            "distillation_approach": "Few-shot LLM-based synthetic query generation using dataset-specific prompt engineering; then train smaller retrieval and reranking models (dense retrieval focus).",
            "output_type": "Synthetic query sets and a fully trainable retrieval pipeline (dense retrievers and rerankers) tuned per dataset.",
            "evaluation_methods": "Promptagator evaluated on BEIR in its original work; compared in this paper via reported nDCG@10 on BEIR datasets.",
            "results": "Promptagator reports competitive results on BEIR and particularly strong gains on argument-retrieval datasets (e.g., ArguAna and Touche) due to dataset-specific prompt engineering; in the InPars-v2 comparison Promptagator outperforms InPars-v2 on some argument datasets but InPars-v2 achieves a higher overall BEIR average.",
            "datasets_or_benchmarks": "BEIR (as reported/computed in comparisons); Promptagator's own experiments described in its paper.",
            "challenges_or_limitations": "Relies on dataset-specific prompts and a proprietary FLAN model; may perform better on certain dataset types (argument retrieval) but not uniformly across heterogeneous benchmarks.",
            "comparisons_to_other_methods": "Compared in this paper against InPars-v2 and RankT5; noted to perform well on datasets where custom prompts help (ArguAna, Touche), and uses smaller T5-based retrieval/reranking models without MS MARCO supervision.",
            "uuid": "e6078.2",
            "source_info": {
                "paper_title": "InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "GPT-J-6B",
            "name_full": "GPT-J-6B",
            "brief_description": "An open-source 6-billion-parameter autoregressive language model used here as a few-shot query generator to create synthetic queries from documents.",
            "citation_title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model",
            "mention_or_use": "use",
            "system_name": "GPT-J-6B (used as synthetic query generator)",
            "system_description": "An autoregressive LLM used with 3-shot MS MARCO prompts and greedy decoding to generate one synthetic query per document from sampled corpora.",
            "llm_model_used": "GPT-J-6B (open-source, 6B parameters).",
            "input_type_and_size": "Inputs were individual documents from BEIR corpora (sampling up to 100k documents per dataset) plus a 3-shot prompt template (MS MARCO examples, 'gbq' prompt). Greedy decoding was used.",
            "distillation_approach": "Generative few-shot synthesis of queries from document text (synthetic query generation as data augmentation).",
            "output_type": "Synthetic natural-language queries paired with the original document (query-document pairs for training).",
            "evaluation_methods": "Quality of generated-and-filtered data assessed indirectly via downstream retrieval effectiveness (nDCG@10) when training/finetuning rerankers and evaluating on BEIR.",
            "results": "Used to generate the synthetic datasets for InPars-v2; with reranker-based filtering and subsequent finetuning, led to improved BEIR performance (InPars-v2 average nDCG@10 0.545). Generation of 100k queries takes ~30 hours on an A100 GPU as reported.",
            "datasets_or_benchmarks": "BEIR corpora were the source of documents used for generation; MS MARCO examples used as few-shot prompts.",
            "challenges_or_limitations": "Generation quality depends on prompt design and model capabilities; greedy decoding used (no sampling) may limit diversity; computational cost non-trivial (30 hours/A100 for 100k queries).",
            "comparisons_to_other_methods": "Replaces proprietary Curie (used in InPars-v1) and FLAN (used by Promptagator) in this work; shown to be effective when combined with reranker filtering, enabling an open-source reproducible pipeline.",
            "uuid": "e6078.3",
            "source_info": {
                "paper_title": "InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "FLAN",
            "name_full": "FLAN (Finetuned Language Net)",
            "brief_description": "A family of finetuned instruction-following language models (FLAN) referenced as the proprietary LLM used by Promptagator for query generation.",
            "citation_title": "Finetuned language models are zero-shot learners",
            "mention_or_use": "mention",
            "system_name": "FLAN (as used by Promptagator)",
            "system_description": "A finetuned instruction-following LM family used in prior work (Promptagator) to generate synthetic queries, particularly in a few-shot/dataset-specific prompting setup.",
            "llm_model_used": "FLAN (FLAN family referenced; specific checkpoint used by Promptagator referenced in that paper).",
            "input_type_and_size": "Promptagator used few-shot dataset-specific prompts (e.g., 8 examples) to generate queries for documents; exact corpus sizes are in the Promptagator paper rather than this paper.",
            "distillation_approach": "Few-shot prompting and instruction-tuning to produce synthetic queries for data augmentation of retrieval models.",
            "output_type": "Synthetic queries and datasets enabling training of retrieval/reranking models.",
            "evaluation_methods": "Promptagator evaluated via retrieval benchmarks (BEIR) in its original work; referenced in this paper comparatively.",
            "results": "Promptagator using FLAN shows strong performance on some BEIR datasets, notably argument retrieval datasets where dataset-specific prompts help; exact numbers are reported in Promptagator's paper and summarized in this paper's comparisons.",
            "datasets_or_benchmarks": "BEIR (comparative evaluations reported); Promptagator's own experimental corpora.",
            "challenges_or_limitations": "Proprietary model use limits reproducibility; benefits depend on prompt engineering and model capacity.",
            "comparisons_to_other_methods": "Compared in this paper to InPars-v2; Promptagator may outperform InPars-v2 on argument-centric datasets because of dataset-specific prompts and different retrieval pipeline design.",
            "uuid": "e6078.4",
            "source_info": {
                "paper_title": "InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "monoT5-3B",
            "name_full": "monoT5-3B reranker",
            "brief_description": "A 3-billion-parameter T5-based reranker used both to filter synthetic query-document pairs and as the finetuned reranker for final evaluation.",
            "citation_title": "Document ranking with a pretrained sequence-to-sequence model",
            "mention_or_use": "use",
            "system_name": "monoT5-3B reranker",
            "system_description": "A sequence-to-sequence model (T5 family) adapted to reranking by producing relevance scores; in InPars-v2 it is pretrained/finetuned on MS MARCO and used to score candidate synthetic query-document pairs, then further finetuned on the filtered synthetic dataset.",
            "llm_model_used": "monoT5-3B (T5-based reranker, 3B parameters).",
            "input_type_and_size": "Takes query-document pairs as input; in filtering, monoT5-3B scored 100k synthetic query-document pairs per dataset (scoring took ~1.5 hours on TPU v3-8). For finetuning: positive and negative pairs batched (64 pos + 64 neg per batch) for one epoch on MS MARCO and one epoch on synthetic InPars-v2 data.",
            "distillation_approach": "Used for filtering (select top 10k scored synthetic pairs) and then trained on the synthetic labeled pairs to transfer the synthetic data into reranker parameters.",
            "output_type": "Relevancy scores for filtering; finetuned reranker models used for downstream document re-ranking.",
            "evaluation_methods": "Downstream retrieval quality measured on BEIR via pipeline BM25 -&gt; monoT5 rerank; nDCG@10 reported.",
            "results": "monoT5-3B finetuned on MS MARCO then InPars-v2 data produced improved nDCG@10 across BEIR relative to baselines; example: avg nDCG@10 improved to 0.545 when using InPars-v2 synthetic data for finetuning.",
            "datasets_or_benchmarks": "MS MARCO (used for initial finetuning), BEIR synthetic datasets created by InPars-v2, BEIR benchmark for evaluation.",
            "challenges_or_limitations": "Large model (3B) compute requirements; scoring and finetuning require TPU/GPU resources; selection of top-k pairs may discard useful examples and is sensitive to the reranker's calibration.",
            "comparisons_to_other_methods": "monoT5-3B used here as both a stronger filter and final reranker outperforms the log-prob filtering used in InPars-v1; compared to RankT5 (a modified monoT5) and to smaller T5 models used by Promptagator.",
            "uuid": "e6078.5",
            "source_info": {
                "paper_title": "InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "RankT5",
            "name_full": "RankT5: Fine-tuning T5 for text ranking with ranking losses",
            "brief_description": "A modified variant of T5 adapted for ranking tasks referenced as a comparison point; reported to have unpublished checkpoint and code.",
            "citation_title": "Rankt5: Fine-tuning t5 for text ranking with ranking losses",
            "mention_or_use": "mention",
            "system_name": "RankT5",
            "system_description": "A T5-based ranking model fine-tuned with ranking losses tailored for retrieval tasks; used as a comparative baseline in the paper.",
            "llm_model_used": "Modified T5 variants (exact checkpoint and code not published according to this paper).",
            "input_type_and_size": "Standard query-document pairs for ranking; details are in the RankT5 paper rather than this paper.",
            "distillation_approach": "Fine-tuning T5 for ranking (not a distillation-from-many-papers method); used for direct ranking rather than dataset synthesis.",
            "output_type": "A reranking model producing document relevance scores.",
            "evaluation_methods": "Evaluated on BEIR in comparisons; reported nDCG@10 values used in Table 1.",
            "results": "Reported by authors and compared in this paper; RankT5 shows competitive retrieval performance on BEIR and sometimes strong results on datasets where its design suits the task.",
            "datasets_or_benchmarks": "BEIR (comparison in this paper); original RankT5 paper contains details.",
            "challenges_or_limitations": "Checkpoint and code not publicly available per this paper, limiting reproducibility and direct comparison; architecture differences and training losses may make direct apples-to-apples comparison difficult.",
            "comparisons_to_other_methods": "Compared in Table 1 to monoT5 variants and InPars-v2; RankT5 and Promptagator vary in strengths across datasets, with RankT5 showing competitive numbers in the table.",
            "uuid": "e6078.6",
            "source_info": {
                "paper_title": "InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval",
                "publication_date_yy_mm": "2023-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Inpars: Data augmentation for information retrieval using large language models",
            "rating": 2
        },
        {
            "paper_title": "Promptagator: Few-shot dense retrieval from 8 examples",
            "rating": 2
        },
        {
            "paper_title": "Rankt5: Fine-tuning t5 for text ranking with ranking losses",
            "rating": 2
        },
        {
            "paper_title": "Finetuned language models are zero-shot learners",
            "rating": 1
        },
        {
            "paper_title": "Document ranking with a pretrained sequence-to-sequence model",
            "rating": 1
        }
    ],
    "cost": 0.013307,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval</h1>
<p>Vitor Jeronymo<br>NeuralMind, Brazil<br>FEEC-UNICAMP, Brazil<br>Marzieh Fadaee<br>Zeta Alpha, Netherlands</p>
<p>Luiz Bonifacio<br>NeuralMind, Brazil<br>FEEC-UNICAMP, Brazil<br>Roberto Lotufo<br>NeuralMind, Brazil<br>FEEC-UNICAMP, Brazil<br>Rodrigo Nogueira<br>NeuralMind, Brazil<br>FEEC-UNICAMP, Brazil<br>Zeta Alpha, Netherlands</p>
<p>Hugo Abonizio<br>NeuralMind, Brazil<br>FEEC-UNICAMP, Brazil<br>Jakub Zavrel<br>Zeta Alpha, Netherlands</p>
<h4>Abstract</h4>
<p>Recently, InPars introduced a method to efficiently use large language models (LLMs) in information retrieval tasks: via few-shot examples, an LLM is induced to generate relevant queries for documents. These synthetic query-document pairs can then be used to train a retriever. However, InPars and, more recently, Promptagator, rely on proprietary LLMs such as GPT-3 and FLAN to generate such datasets. In this work we introduce InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training. A simple BM25 retrieval pipeline followed by a monoT5 reranker finetuned on InPars-v2 data achieves new state-of-the-art results on the BEIR benchmark. To allow researchers to further improve our method, we open source the code, synthetic data, and finetuned models: https://github.com/zetaalphavector/inPars/tree/master/legacy/inpars-v2</p>
<h2>1 Introduction and Background</h2>
<p>Data augmentation has been a reliable tool to improve the effectiveness of AI models in the face of the scarcity of high-quality in-domain training data, which is a common problem in practical applications. Previous work by Bonifacio et al. [1] and Dai et al. [2] successfully leveraged the few-shot capabilities of LLMs to generate reliable synthetic training data for information retrieval models. These training data helped their models achieve state-of-the-art (SOTA) results on the BEIR benchmark [6].
Bonifacio et al. [1] propose InPars where they generate queries from documents in the corpus using LLMs. Similarly to Bonifacio et al. [1], the recently published Promptagator [2] model also feeds prompts to LLMs in order to generate alternative queries for a given document in an unsupervised manner. It differs primarily from InPars in that it uses dataset-specific prompts, a larger LLM to generate queries, and a fully trainable retrieval pipeline with smaller models.
This work extends the method of Bonifacio et al. [1] by using a reranker as a filtering mechanism to select the best synthetically generated examples and further improving retrieval effectiveness</p>
<p>on BEIR. We also use an open-source query generator as opposed to the proprietary one used by Bonifacio et al. and provide the source code and data to reproduce our results on TPUs. We refer to Bonifacio et al. [1] model as Inpars-v1 and the model presented in this paper as Inpars-v2.</p>
<h1>2 Methodology</h1>
<p>In this section, we explain the experiments we performed and how they differ from InPars-v1 [1].
To generate synthetic queries, we use the open-source GPT-J [8] with 6B parameters to replace OpenAI's curie model used in InPars-v1. For each dataset in the BEIR benchmark, we sample 100k documents from its corpus and generate one synthetic query per document using GPT-J prompted with 3 examples from MS MARCO. We use greedy decoding and the "gbq" prompt template from InPars-v1. Some corpora in BEIR such as ArguAna [7] have less than 100k documents. In these cases, we generate as many synthetic queries as there are documents in the corpus. It takes on average 30 hours on an A100 GPU to generate 100k queries.
Once the synthetic queries are generated, we apply a filtering step to select query-document pairs that are more likely to be relevant to each other. In InPars-v1, this filtering step consisted of selecting the top 10 k query-document pairs with the highest log probabilities of generating a query given the 3 -shot examples and the document as input. In InPars-v2, we use monoT5-3B [4] already finetuned on MS MARCO for one epoch ${ }^{1}$ to estimate a relevancy score for each of the 100k query-document pairs. Then, we keep only the top 10 k pairs with the highest scores as our positive query-document pairs for training. It takes approximately 1.5 hours to score 100 k query-document pairs on a TPU v3-8. It should take twice as much on a A100.
To obtain negatives (i.e., non-relevant) query-document pairs, we randomly sample one document from the top 1000 retrieved by BM25 when issued the synthetic query. Thus, our training set consists of 10 k positive query-document pairs and 10 k negative query-document pairs.
The rerankers are finetuned in the same manner as in InPars-v1: monoT5-3B is finetuned on MS MARCO for one epoch and then further finetuned for one epoch on the synthetic data. We use the Adafactor optimizer [5] with a constant learning rate of 1e-3. Each batch has 64 positive and 64 negative query-document pairs randomly sampled from the training dataset. We finetune one model on each synthetic dataset from BEIR, that is, we end up with 18 different rerankers, one per dataset, which are then evaluated on the corresponding test sets. Finetuning on each synthetic dataset takes less than 10 minutes on a TPU v3-8.
Evaluation is performed using the following pipeline: first we use Pyserini's [3] flat indexes ${ }^{2}$ to retrieve a thousand documents for each query using BM25 with default parameters ( $\mathrm{k} 1=0.9, \mathrm{~b}=0.4$ ), for each dataset. Then we use the finetuned monoT5-3B models to rerank these documents.</p>
<h2>3 Results</h2>
<p>Table 1 presents results for BM25 (2nd column), monoT5-3B finetuned on MS MARCO (3rd column), monoT5-3b finetuned on MS MARCO and further finetuned on InPars-v1 (4th column), and monoT5-3B finetuned on MS MARCO and then finetuned on InPars-v2 data (5th column). Compared to InPars-v1, our approach is substantially better on TREC-News, Climate-FEVER, Robust and Touche. Additionally, we compare our method with Promptagator [2] and RankT5 [10]. Taking into account the average of all BEIR datasets, these results represent a new state of the art on BEIR.
Promptagator and RankT5 strive on datasets that monoT5 and InPars-v2 cannot even surpass BM25, such as Touche and ArguAna. Note that these datasets focus on argument retrieval, which is slightly different from other datasets in the BEIR benchmark. As a result, they benefit from using custom prompts. ${ }^{3}$ Promptagator does this without using supervised data from MS MARCO and using smaller T5 models with 110M parameters for the retrieval and reranking steps.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">BM25</th>
<th style="text-align: center;">monoT5-3B <br> MARCO</th>
<th style="text-align: center;">+InPars-v1</th>
<th style="text-align: center;">+InPars-v2</th>
<th style="text-align: center;">PrGator</th>
<th style="text-align: center;">RankT5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">TREC-Covid</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.801</td>
<td style="text-align: center;">0.846</td>
<td style="text-align: center;">0.846</td>
<td style="text-align: center;">0.762</td>
<td style="text-align: center;">0.823</td>
</tr>
<tr>
<td style="text-align: left;">Robust</td>
<td style="text-align: center;">0.407</td>
<td style="text-align: center;">0.615</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.632</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">FiQA</td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">0.509</td>
<td style="text-align: center;">0.492</td>
<td style="text-align: center;">0.509</td>
<td style="text-align: center;">0.494</td>
<td style="text-align: center;">0.493</td>
</tr>
<tr>
<td style="text-align: left;">DBPedia</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">0.494</td>
<td style="text-align: center;">0.498</td>
<td style="text-align: center;">0.434</td>
<td style="text-align: center;">0.459</td>
</tr>
<tr>
<td style="text-align: left;">SciDocs</td>
<td style="text-align: center;">0.149</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.208</td>
<td style="text-align: center;">0.201</td>
<td style="text-align: center;">0.191</td>
</tr>
<tr>
<td style="text-align: left;">SciFact</td>
<td style="text-align: center;">0.678</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.731</td>
<td style="text-align: center;">0.760</td>
</tr>
<tr>
<td style="text-align: left;">NFCorpus</td>
<td style="text-align: center;">0.321</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.370</td>
<td style="text-align: center;">0.399</td>
</tr>
<tr>
<td style="text-align: left;">BioASQ</td>
<td style="text-align: center;">0.522</td>
<td style="text-align: center;">0.566</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.595</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.579</td>
</tr>
<tr>
<td style="text-align: left;">Natural Questions</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.638</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.647</td>
</tr>
<tr>
<td style="text-align: left;">HotpotQA</td>
<td style="text-align: center;">0.633</td>
<td style="text-align: center;">0.760</td>
<td style="text-align: center;">0.790</td>
<td style="text-align: center;">0.791</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.753</td>
</tr>
<tr>
<td style="text-align: left;">TREC-News</td>
<td style="text-align: center;">0.395</td>
<td style="text-align: center;">0.477</td>
<td style="text-align: center;">0.458</td>
<td style="text-align: center;">0.490</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Quora</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">0.874</td>
<td style="text-align: center;">0.845</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.819</td>
</tr>
<tr>
<td style="text-align: left;">FEVER</td>
<td style="text-align: center;">0.651</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.852</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.866</td>
<td style="text-align: center;">0.848</td>
</tr>
<tr>
<td style="text-align: left;">Climate-FEVER</td>
<td style="text-align: center;">0.165</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.287</td>
<td style="text-align: center;">0.323</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.275</td>
</tr>
<tr>
<td style="text-align: left;">Signal</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.302</td>
<td style="text-align: center;">0.319</td>
<td style="text-align: center;">0.308</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.319</td>
</tr>
<tr>
<td style="text-align: left;">ArguAna</td>
<td style="text-align: center;">0.397</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.371</td>
<td style="text-align: center;">0.369</td>
<td style="text-align: center;">0.630</td>
<td style="text-align: center;">0.406</td>
</tr>
<tr>
<td style="text-align: left;">Touche</td>
<td style="text-align: center;">0.442</td>
<td style="text-align: center;">0.309</td>
<td style="text-align: center;">0.260</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.381</td>
<td style="text-align: center;">0.486</td>
</tr>
<tr>
<td style="text-align: left;">CQADupstack</td>
<td style="text-align: center;">0.302</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Avg</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.533</td>
<td style="text-align: center;">0.539</td>
<td style="text-align: center;">0.545</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Avg PrGator</td>
<td style="text-align: center;">0.417</td>
<td style="text-align: center;">0.520</td>
<td style="text-align: center;">0.523</td>
<td style="text-align: center;">0.533</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.536</td>
</tr>
</tbody>
</table>
<p>Table 1: nDCG@10 on BEIR. "Avg PrGator" is the average of datasets reported by Promptagator.</p>
<p>Promptagator uses a proprietary model, FLAN [9], to generate synthetic queries. The RankT5 model is a modified version of the monoT5 reranker, but its checkpoint and code are not published. In this work, we make the code, models, and data open-source and publicly available.</p>
<h1>4 Conclusion</h1>
<p>In this work, we presented InPars-v2, an improved version of InPars [1] that uses a publicly available language model to generate queries and a better query-document pair selection process. Our results show that we achieve effectiveness on par with the state of the art on BEIR. The synthetic data and finetuned models were publicly released.</p>
<h2>Acknowledgments</h2>
<p>This research was partially supported by Fundao de Amparo  Pesquisa do Estado de So Paulo (FAPESP) (project id 2022/01640-2). We also thank Centro Nacional de Processamento de Alto Desempenho (CENAPAD-SP) and Google Cloud for computing credits.</p>
<h2>References</h2>
<p>[1] L. Bonifacio, H. Abonizio, M. Fadaee, and R. Nogueira. Inpars: Data augmentation for information retrieval using large language models. arXiv preprint arXiv:2202.05144, 2022.
[2] Z. Dai, V. Y. Zhao, J. Ma, Y. Luan, J. Ni, J. Lu, A. Bakalov, K. Guu, K. B. Hall, and M.-W. Chang. Promptagator: Few-shot dense retrieval from 8 examples. arXiv preprint arXiv:2209.11755, 2022.
[3] J. Lin, X. Ma, S.-C. Lin, J.-H. Yang, R. Pradeep, and R. Nogueira. Pyserini: An easy-to-use python toolkit to support replicable ir research with sparse and dense representations. arXiv preprint arXiv:2102.10073, 2021.
[4] R. Nogueira, Z. Jiang, R. Pradeep, and J. Lin. Document ranking with a pretrained sequence-to-sequence model. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 708-718, 2020.</p>
<p>[5] N. Shazeer and M. Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596-4604. PMLR, 2018.
[6] N. Thakur, N. Reimers, A. Rckl, A. Srivastava, and I. Gurevych. Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663, 2021.
[7] H. Wachsmuth, S. Syed, and B. Stein. Retrieval of the best counterargument without prior topic knowledge. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 241-251, Melbourne, Australia, July 2018. Association for Computational Linguistics.
[8] B. Wang and A. Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.
[9] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.
[10] H. Zhuang, Z. Qin, R. Jagerman, K. Hui, J. Ma, J. Lu, J. Ni, X. Wang, and M. Bendersky. Rankt5: Fine-tuning t5 for text ranking with ranking losses. arXiv preprint arXiv:2210.10634, 2022.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://huggingface.co/castorini/monot5-3b-msmarco-10k
${ }^{2}$ As opposed to the multifield index.
${ }^{3}$ In preliminary experiments, we also observed an improvement of more than 10 nDCG@10 points on ArguAna by using a dataset-specific prompt to generate synthetic queries. More details and results on the full BEIR benchmark will appear in an upcoming paper.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9853 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9853</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9853</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-e33a538e80d1877782df26e1493f5adc661ceec4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e33a538e80d1877782df26e1493f5adc661ceec4" target="_blank">AstroLLaMA: Towards Specialized Foundation Models in Astronomy</a></p>
                <p><strong>Paper Venue:</strong> WIESP</p>
                <p><strong>Paper TL;DR:</strong> AstroLLaMA is introduced, a 7-billion-parameter model fine-tuned from LLaMA-2 using over 300,000 astronomy abstracts from arXiv to spur astronomy-focused research, including automatic paper summarization and conversational agent development.</p>
                <p><strong>Paper Abstract:</strong> Large language models excel in many human-language tasks but often falter in highly specialized domains like scholarly astronomy. To bridge this gap, we introduce AstroLLaMA, a 7-billion-parameter model fine-tuned from LLaMA-2 using over 300,000 astronomy abstracts from arXiv. Optimized for traditional causal language modeling, AstroLLaMA achieves a 30% lower perplexity than Llama-2, showing marked domain adaptation. Our model generates more insightful and scientifically relevant text completions and embedding extraction than state-of-the-arts foundation models despite having significantly fewer parameters. AstroLLaMA serves as a robust, domain-specific model with broad fine-tuning potential. Its public release aims to spur astronomy-focused research, including automatic paper summarization and conversational agent development.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9853.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9853.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AstroLLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AstroLLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7-billion-parameter generative language model fine-tuned from LLaMA-2 on ~326k arXiv astrophysics abstracts to improve domain-specific text generation and embeddings for astronomy tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AstroLLaMA (fine-tuned from LLaMA-2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tuned causal-transformer model derived from LLaMA-2 using LoRA low-rank adaptation and 4-bit quantization; uses byte-pair encoding tokenizer with 32k vocab and 4096 token context window inherited from base.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Curated subset of arXiv 'astro-ph' abstracts from Kaggle spanning April 1992–July 2023; abstracts only (median length 291 tokens), concatenated and chunked to 512-token sequences for causal LM fine-tuning; 20% held out for testing.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>326238</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>No single targeted topic query; model was domain-adapted to the astronomy literature as a whole and evaluated on tasks including abstract completion (prompt: initial sentences of abstracts) and embedding-based semantic similarity over the corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Domain adaptation via supervised fine-tuning on next-token prediction (causal language modeling) over ~3 epochs (~230M processed tokens) with AdamW optimizer, cosine LR schedule (warmup to 3e-4), LoRA adaptation (alpha=32, dropout=0.05), 4-bit quantization, sequences prepended/appended with [BOS]/[EOS] and chunked to 512 tokens. Embeddings obtained by averaging final hidden states (excluding [BOS]).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated text completions (abstract completions) and dense text embeddings for document similarity/retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Completion of a Magellanic Stream abstract: a domain-aware multi-step search approach (wide-field imaging → Gaia astrometry refinement → spectroscopy), discussion of bifurcation and metallicity differences, and identification of candidate stellar stream features (example shown in Fig.2 of the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Perplexity on held-out abstracts (language-model metric); qualitative human judgements by two authors for abstract completion quality; embedding quality assessment via pairwise cosine similarity distributions over 10,000 randomly selected abstracts compared to OpenAI GPT-3 (ada-002) embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Perplexity decreased from ~10 (LLaMA-2 baseline on dataset) to 6.55 after fine-tuning (32.5% reduction). Qualitatively judged to produce more relevant and domain-specific completions than LLaMA-2 and GPT-4 in presented examples. Embeddings showed greater variance and better semantic discrimination versus GPT-3 (ada-002), which produced overly-generic similarity scores clustered around 0.7–0.9.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Strong domain adaptation despite modest parameter count (7B); substantially lower perplexity on astronomy abstracts; more context-aware, relevant abstract completions; embeddings exhibit higher discriminative power for domain semantics; efficient fine-tuning via LoRA and 4-bit quantization enabling resource-efficient adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Training focused on abstracts only (limited token budget compared to full papers); knowledge gaps in certain astronomy subareas; tendency to hallucinate or produce fictitious numerical data; fidelity constrained by causal next-token objective (no explicit factual grounding/alignment).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Example in Fig.2: inaccurate estimation of star-candidate counts from Gaia-ESO and generation of incorrect numerical claims; general tendency to hallucinate numeric values and to exhibit incomplete factuality for some domain-specific queries.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AstroLLaMA: Towards Specialized Foundation Models in Astronomy', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9853.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9853.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2 (base model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Meta's open foundation language model used as the base for AstroLLaMA; a general-purpose pre-trained causal transformer whose weights were adapted via domain fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained foundation causal language model with a 4096-token context window and byte-pair encoding tokenizer; pre-trained on ~2 trillion tokens as reported by Meta.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6.7B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Generic mixture corpus used in Meta's pretraining (≈2 trillion tokens); not astronomy-focused (paper estimates ~2.5% arXiv content and <5% of that astronomy).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Not applicable — used as a pre-trained baseline and starting point for fine-tuning rather than for topic-guided distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>In this paper LLaMA-2 is used as the base pre-trained model; no additional distillation beyond being fine-tuned to create AstroLLaMA.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Baseline generated completions and embeddings for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Abstract completions that often diverge off-topic quickly (qualitative examples shown in Fig.2).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Perplexity measured on the astronomy abstract dataset; qualitative comparisons of abstract completions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Average perplexity close to 10 on the astronomy abstract dataset pre-fine-tuning; poorer domain-specific completions compared to AstroLLaMA.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Large pre-training corpus and general capabilities; useful as a foundation for downstream domain adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Eclectic pretraining dilutes specialized domain knowledge, leading to poorer performance and off-topic generations on astronomy-specific tasks without further fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Tends to generate off-topic continuations when prompted with astronomy abstracts (early divergence from intended context in completions).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AstroLLaMA: Towards Specialized Foundation Models in Astronomy', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9853.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9853.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art general-purpose large language model by OpenAI, used both as a comparative baseline for text completions and mentioned as the primary model in prior works that generate substantive hypotheses via in-context prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT-4 Technical Report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large multimodal/generative transformer model (OpenAI) known for strong general reasoning and generation abilities; used here via ChatGPT to produce single-paragraph completions for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in this paper (general web and curated corpora as per OpenAI's reporting).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Used with prompts consisting of the first few sentences of abstracts, instructed to produce a single-paragraph completion; also cited in prior work as used for hypothesis generation via in-context prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not used to distill theories in this paper; used for in-context prompting/instruction-based completions as a baseline comparison. Prior referenced works used in‑context prompting and instruction learning primarily involving GPT-4 to generate hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Text completions and hypothesis generations (in cited prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>In Fig.2 GPT-4 produced a coherent but over-generic paragraph that failed to capture nuanced domain-specific details compared to AstroLLaMA.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Qualitative human judgment (two authors) for completion relevance; compared to AstroLLaMA and LLaMA-2.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-4's completions were coherent but judged to be too generic and less domain-specific than AstroLLaMA in the presented examples.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>High general coherence and fluency; strong in-context reasoning and instruction-following capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Tends to produce over-generic responses that may lack the domain specificity required to synthesize detailed theories from specialized scholarly corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Produced plausible but off-target/generic completions for astronomy abstracts; in referenced literature, may require tailored prompting or retrieval augmentation to produce domain-grounded hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AstroLLaMA: Towards Specialized Foundation Models in Astronomy', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9853.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9853.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (ada-002)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (ada-002 embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's embedding model (ada-002) used as a baseline for evaluating embedding quality on astronomy abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (ada-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Embedding-capable variant of OpenAI's GPT-3 family invoked via OpenAI API for text embedding extraction; treated as a general-purpose embedding baseline in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Used to embed 10,000 randomly selected astronomy abstracts from the AstroLLaMA corpus for pairwise similarity comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>10000</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>No topic-specific query — embeddings were computed for random abstracts to assess semantic discrimination across astronomy texts.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not used for distillation; used as an embedding baseline. Embeddings obtained via OpenAI text embedding API and compared to averaged final hidden states from AstroLLaMA.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Dense vector embeddings for documents used to compute pairwise cosine similarities.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Pairwise cosine similarities clustered around 0.7–0.9 (indicating overly generic embeddings).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Distributional analysis of pairwise cosine similarities across 10,000 abstracts and illustrative example comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-3 (ada-002) embeddings showed high similarity clustering (0.7–0.9), indicating less discriminative power compared to AstroLLaMA embeddings which showed higher variance and better semantic discrimination.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Robust, off-the-shelf embeddings for general-purpose tasks and easy API access.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Appeared overly generic for specialized astronomy literature, failing to discriminate nuanced domain differences in the tested examples.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>High similarity scores between semantically distinct papers when one or two overlapping keywords ('magnetized') dominated the embedding similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AstroLLaMA: Towards Specialized Foundation Models in Astronomy', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9853.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9853.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>astroBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>astroBERT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 110-million-parameter BERT-style model fine-tuned on ~400k ADS papers for astronomy tasks; non-generative and primarily intended for discriminative tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Building astroBERT, a language model for Astronomy & Astrophysics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>astroBERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Domain-specific BERT-family model fine-tuned on astronomical bibliographic data (ADS); smaller (≈110M parameters) and non-generative (discriminative tasks focus).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>110M</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Fine-tuned on nearly 400,000 ADS papers (as reported by Grezes et al., 2021).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>400000</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Not applicable in this paper — cited as an example of a prior domain-specific model in astronomy primarily used for discriminative tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Fine-tuning of a BERT-style masked-language/discriminative model (not a generative distillation pipeline); exact fine-tuning methods described in the astroBERT paper (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Discriminative outputs for downstream tasks (e.g., classification, retrieval); not a generative theory-synthesis system.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not detailed in this paper beyond citation; astroBERT is described as a non-generative specialized model.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Mentioned as state-of-the-art specialized model for discriminative tasks but limited in generative capability relative to AstroLLaMA.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Domain-specific pretraining/fine-tuning for astronomy; smaller, efficient model for discriminative tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Non-generative architecture limits applicability to generative tasks such as hypothesis generation or narrative synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not discussed in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AstroLLaMA: Towards Specialized Foundation Models in Astronomy', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9853.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9853.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactic ChitChat (Ciucă & Ting 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactic ChitChat: Using Large Language Models to Converse with Astronomy Literature</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited work that demonstrates use of LLMs to interact with and extract insights from the astronomy literature (cited here as evidence that LLMs can assist hypothesis generation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Galactic ChitChat: Using Large Language Models to Converse with Astronomy Literature</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>primarily GPT-4 (in-context prompting) as described by citing paper</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in this paper as using in-context prompting/instruction learning approaches — primarily involving GPT-4 — to converse with academic astronomy literature and to support hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in detail in this AstroLLaMA paper; cited as prior art demonstrating LLMs operating over astronomy literature.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Used to generate substantive hypotheses from the literature via in-context prompting and instruction learning (no single topic stated here).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>In-context prompting and instruction learning (as summarized by the AstroLLaMA paper); specifics are in the referenced work rather than this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Hypothesis generation and conversational interactions with literature.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not specified in this paper; cited claim that such approaches demonstrated significant potential for generating substantive hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Described qualitatively as having significant potential; no quantitative details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Ability to generate substantive hypotheses from scholarly texts via prompting without full model fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not detailed in this paper; details deferred to the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AstroLLaMA: Towards Specialized Foundation Models in Astronomy', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9853.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9853.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adversarial Prompting (Ciucă et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Harnessing the Power of Adversarial Prompting and Large Language Models for Robust Hypothesis Generation in Astronomy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited prior work using adversarial prompting techniques with LLMs (primarily GPT-4) to improve robustness of hypothesis generation from astronomical literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Harnessing the Power of Adversarial Prompting and Large Language Models for Robust Hypothesis Generation in Astronomy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (in-context prompting/adversarial prompting approach as described in citation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced as employing adversarial prompting strategies and instruction-style prompting to elicit robust hypotheses from LLMs; exact architectural/training details are in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified within this AstroLLaMA paper; cited as prior demonstration of LLM-driven hypothesis generation in astronomy.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Adversarial prompting targeted at generating robust scientific hypotheses from astronomy literature; specifics in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Adversarial prompting / in-context prompting / instruction learning (described at high level in this paper; implementation specifics are in the cited publication).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Hypothesis generation and robust idea synthesis from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not specified here; the AstroLLaMA paper refers to these works for demonstrations of promise in hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported in this paper as evidence that prompting/instruction approaches produce substantive hypotheses; no quantitative metrics provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Demonstrated potential to generate substantive hypotheses using prompting strategies without heavy model fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Details/limitations not enumerated in this paper; likely depend on prompting design and model grounding as discussed generally.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AstroLLaMA: Towards Specialized Foundation Models in Astronomy', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Galactic ChitChat: Using Large Language Models to Converse with Astronomy Literature <em>(Rating: 2)</em></li>
                <li>Harnessing the Power of Adversarial Prompting and Large Language Models for Robust Hypothesis Generation in Astronomy <em>(Rating: 2)</em></li>
                <li>Building astroBERT, a language model for Astronomy & Astrophysics <em>(Rating: 2)</em></li>
                <li>LLaMA: Open and Efficient Foundation Language Models <em>(Rating: 1)</em></li>
                <li>GPT-4 Technical Report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9853",
    "paper_id": "paper-e33a538e80d1877782df26e1493f5adc661ceec4",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "AstroLLaMA",
            "name_full": "AstroLLaMA",
            "brief_description": "A 7-billion-parameter generative language model fine-tuned from LLaMA-2 on ~326k arXiv astrophysics abstracts to improve domain-specific text generation and embeddings for astronomy tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "AstroLLaMA (fine-tuned from LLaMA-2)",
            "model_description": "Fine-tuned causal-transformer model derived from LLaMA-2 using LoRA low-rank adaptation and 4-bit quantization; uses byte-pair encoding tokenizer with 32k vocab and 4096 token context window inherited from base.",
            "model_size": "7B",
            "input_corpus_description": "Curated subset of arXiv 'astro-ph' abstracts from Kaggle spanning April 1992–July 2023; abstracts only (median length 291 tokens), concatenated and chunked to 512-token sequences for causal LM fine-tuning; 20% held out for testing.",
            "input_corpus_size": 326238,
            "topic_query_description": "No single targeted topic query; model was domain-adapted to the astronomy literature as a whole and evaluated on tasks including abstract completion (prompt: initial sentences of abstracts) and embedding-based semantic similarity over the corpus.",
            "distillation_method": "Domain adaptation via supervised fine-tuning on next-token prediction (causal language modeling) over ~3 epochs (~230M processed tokens) with AdamW optimizer, cosine LR schedule (warmup to 3e-4), LoRA adaptation (alpha=32, dropout=0.05), 4-bit quantization, sequences prepended/appended with [BOS]/[EOS] and chunked to 512 tokens. Embeddings obtained by averaging final hidden states (excluding [BOS]).",
            "output_type": "Generated text completions (abstract completions) and dense text embeddings for document similarity/retrieval.",
            "output_example": "Completion of a Magellanic Stream abstract: a domain-aware multi-step search approach (wide-field imaging → Gaia astrometry refinement → spectroscopy), discussion of bifurcation and metallicity differences, and identification of candidate stellar stream features (example shown in Fig.2 of the paper).",
            "evaluation_method": "Perplexity on held-out abstracts (language-model metric); qualitative human judgements by two authors for abstract completion quality; embedding quality assessment via pairwise cosine similarity distributions over 10,000 randomly selected abstracts compared to OpenAI GPT-3 (ada-002) embeddings.",
            "evaluation_results": "Perplexity decreased from ~10 (LLaMA-2 baseline on dataset) to 6.55 after fine-tuning (32.5% reduction). Qualitatively judged to produce more relevant and domain-specific completions than LLaMA-2 and GPT-4 in presented examples. Embeddings showed greater variance and better semantic discrimination versus GPT-3 (ada-002), which produced overly-generic similarity scores clustered around 0.7–0.9.",
            "strengths": "Strong domain adaptation despite modest parameter count (7B); substantially lower perplexity on astronomy abstracts; more context-aware, relevant abstract completions; embeddings exhibit higher discriminative power for domain semantics; efficient fine-tuning via LoRA and 4-bit quantization enabling resource-efficient adaptation.",
            "limitations": "Training focused on abstracts only (limited token budget compared to full papers); knowledge gaps in certain astronomy subareas; tendency to hallucinate or produce fictitious numerical data; fidelity constrained by causal next-token objective (no explicit factual grounding/alignment).",
            "failure_cases": "Example in Fig.2: inaccurate estimation of star-candidate counts from Gaia-ESO and generation of incorrect numerical claims; general tendency to hallucinate numeric values and to exhibit incomplete factuality for some domain-specific queries.",
            "uuid": "e9853.0",
            "source_info": {
                "paper_title": "AstroLLaMA: Towards Specialized Foundation Models in Astronomy",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "LLaMA-2",
            "name_full": "LLaMA-2 (base model)",
            "brief_description": "Meta's open foundation language model used as the base for AstroLLaMA; a general-purpose pre-trained causal transformer whose weights were adapted via domain fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-2",
            "model_description": "Pre-trained foundation causal language model with a 4096-token context window and byte-pair encoding tokenizer; pre-trained on ~2 trillion tokens as reported by Meta.",
            "model_size": "6.7B",
            "input_corpus_description": "Generic mixture corpus used in Meta's pretraining (≈2 trillion tokens); not astronomy-focused (paper estimates ~2.5% arXiv content and &lt;5% of that astronomy).",
            "input_corpus_size": null,
            "topic_query_description": "Not applicable — used as a pre-trained baseline and starting point for fine-tuning rather than for topic-guided distillation.",
            "distillation_method": "In this paper LLaMA-2 is used as the base pre-trained model; no additional distillation beyond being fine-tuned to create AstroLLaMA.",
            "output_type": "Baseline generated completions and embeddings for comparison.",
            "output_example": "Abstract completions that often diverge off-topic quickly (qualitative examples shown in Fig.2).",
            "evaluation_method": "Perplexity measured on the astronomy abstract dataset; qualitative comparisons of abstract completions.",
            "evaluation_results": "Average perplexity close to 10 on the astronomy abstract dataset pre-fine-tuning; poorer domain-specific completions compared to AstroLLaMA.",
            "strengths": "Large pre-training corpus and general capabilities; useful as a foundation for downstream domain adaptation.",
            "limitations": "Eclectic pretraining dilutes specialized domain knowledge, leading to poorer performance and off-topic generations on astronomy-specific tasks without further fine-tuning.",
            "failure_cases": "Tends to generate off-topic continuations when prompted with astronomy abstracts (early divergence from intended context in completions).",
            "uuid": "e9853.1",
            "source_info": {
                "paper_title": "AstroLLaMA: Towards Specialized Foundation Models in Astronomy",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A state-of-the-art general-purpose large language model by OpenAI, used both as a comparative baseline for text completions and mentioned as the primary model in prior works that generate substantive hypotheses via in-context prompting.",
            "citation_title": "GPT-4 Technical Report",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large multimodal/generative transformer model (OpenAI) known for strong general reasoning and generation abilities; used here via ChatGPT to produce single-paragraph completions for comparison.",
            "model_size": null,
            "input_corpus_description": "Not specified in this paper (general web and curated corpora as per OpenAI's reporting).",
            "input_corpus_size": null,
            "topic_query_description": "Used with prompts consisting of the first few sentences of abstracts, instructed to produce a single-paragraph completion; also cited in prior work as used for hypothesis generation via in-context prompting.",
            "distillation_method": "Not used to distill theories in this paper; used for in-context prompting/instruction-based completions as a baseline comparison. Prior referenced works used in‑context prompting and instruction learning primarily involving GPT-4 to generate hypotheses.",
            "output_type": "Text completions and hypothesis generations (in cited prior work).",
            "output_example": "In Fig.2 GPT-4 produced a coherent but over-generic paragraph that failed to capture nuanced domain-specific details compared to AstroLLaMA.",
            "evaluation_method": "Qualitative human judgment (two authors) for completion relevance; compared to AstroLLaMA and LLaMA-2.",
            "evaluation_results": "GPT-4's completions were coherent but judged to be too generic and less domain-specific than AstroLLaMA in the presented examples.",
            "strengths": "High general coherence and fluency; strong in-context reasoning and instruction-following capabilities.",
            "limitations": "Tends to produce over-generic responses that may lack the domain specificity required to synthesize detailed theories from specialized scholarly corpora.",
            "failure_cases": "Produced plausible but off-target/generic completions for astronomy abstracts; in referenced literature, may require tailored prompting or retrieval augmentation to produce domain-grounded hypotheses.",
            "uuid": "e9853.2",
            "source_info": {
                "paper_title": "AstroLLaMA: Towards Specialized Foundation Models in Astronomy",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GPT-3 (ada-002)",
            "name_full": "GPT-3 (ada-002 embeddings)",
            "brief_description": "OpenAI's embedding model (ada-002) used as a baseline for evaluating embedding quality on astronomy abstracts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 (ada-002)",
            "model_description": "Embedding-capable variant of OpenAI's GPT-3 family invoked via OpenAI API for text embedding extraction; treated as a general-purpose embedding baseline in the paper.",
            "model_size": null,
            "input_corpus_description": "Used to embed 10,000 randomly selected astronomy abstracts from the AstroLLaMA corpus for pairwise similarity comparison.",
            "input_corpus_size": 10000,
            "topic_query_description": "No topic-specific query — embeddings were computed for random abstracts to assess semantic discrimination across astronomy texts.",
            "distillation_method": "Not used for distillation; used as an embedding baseline. Embeddings obtained via OpenAI text embedding API and compared to averaged final hidden states from AstroLLaMA.",
            "output_type": "Dense vector embeddings for documents used to compute pairwise cosine similarities.",
            "output_example": "Pairwise cosine similarities clustered around 0.7–0.9 (indicating overly generic embeddings).",
            "evaluation_method": "Distributional analysis of pairwise cosine similarities across 10,000 abstracts and illustrative example comparisons.",
            "evaluation_results": "GPT-3 (ada-002) embeddings showed high similarity clustering (0.7–0.9), indicating less discriminative power compared to AstroLLaMA embeddings which showed higher variance and better semantic discrimination.",
            "strengths": "Robust, off-the-shelf embeddings for general-purpose tasks and easy API access.",
            "limitations": "Appeared overly generic for specialized astronomy literature, failing to discriminate nuanced domain differences in the tested examples.",
            "failure_cases": "High similarity scores between semantically distinct papers when one or two overlapping keywords ('magnetized') dominated the embedding similarity.",
            "uuid": "e9853.3",
            "source_info": {
                "paper_title": "AstroLLaMA: Towards Specialized Foundation Models in Astronomy",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "astroBERT",
            "name_full": "astroBERT",
            "brief_description": "A 110-million-parameter BERT-style model fine-tuned on ~400k ADS papers for astronomy tasks; non-generative and primarily intended for discriminative tasks.",
            "citation_title": "Building astroBERT, a language model for Astronomy & Astrophysics",
            "mention_or_use": "mention",
            "model_name": "astroBERT",
            "model_description": "Domain-specific BERT-family model fine-tuned on astronomical bibliographic data (ADS); smaller (≈110M parameters) and non-generative (discriminative tasks focus).",
            "model_size": "110M",
            "input_corpus_description": "Fine-tuned on nearly 400,000 ADS papers (as reported by Grezes et al., 2021).",
            "input_corpus_size": 400000,
            "topic_query_description": "Not applicable in this paper — cited as an example of a prior domain-specific model in astronomy primarily used for discriminative tasks.",
            "distillation_method": "Fine-tuning of a BERT-style masked-language/discriminative model (not a generative distillation pipeline); exact fine-tuning methods described in the astroBERT paper (not detailed here).",
            "output_type": "Discriminative outputs for downstream tasks (e.g., classification, retrieval); not a generative theory-synthesis system.",
            "output_example": null,
            "evaluation_method": "Not detailed in this paper beyond citation; astroBERT is described as a non-generative specialized model.",
            "evaluation_results": "Mentioned as state-of-the-art specialized model for discriminative tasks but limited in generative capability relative to AstroLLaMA.",
            "strengths": "Domain-specific pretraining/fine-tuning for astronomy; smaller, efficient model for discriminative tasks.",
            "limitations": "Non-generative architecture limits applicability to generative tasks such as hypothesis generation or narrative synthesis.",
            "failure_cases": "Not discussed in detail in this paper.",
            "uuid": "e9853.4",
            "source_info": {
                "paper_title": "AstroLLaMA: Towards Specialized Foundation Models in Astronomy",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Galactic ChitChat (Ciucă & Ting 2023)",
            "name_full": "Galactic ChitChat: Using Large Language Models to Converse with Astronomy Literature",
            "brief_description": "A cited work that demonstrates use of LLMs to interact with and extract insights from the astronomy literature (cited here as evidence that LLMs can assist hypothesis generation).",
            "citation_title": "Galactic ChitChat: Using Large Language Models to Converse with Astronomy Literature",
            "mention_or_use": "mention",
            "model_name": "primarily GPT-4 (in-context prompting) as described by citing paper",
            "model_description": "Described in this paper as using in-context prompting/instruction learning approaches — primarily involving GPT-4 — to converse with academic astronomy literature and to support hypothesis generation.",
            "model_size": null,
            "input_corpus_description": "Not specified in detail in this AstroLLaMA paper; cited as prior art demonstrating LLMs operating over astronomy literature.",
            "input_corpus_size": null,
            "topic_query_description": "Used to generate substantive hypotheses from the literature via in-context prompting and instruction learning (no single topic stated here).",
            "distillation_method": "In-context prompting and instruction learning (as summarized by the AstroLLaMA paper); specifics are in the referenced work rather than this paper.",
            "output_type": "Hypothesis generation and conversational interactions with literature.",
            "output_example": null,
            "evaluation_method": "Not specified in this paper; cited claim that such approaches demonstrated significant potential for generating substantive hypotheses.",
            "evaluation_results": "Described qualitatively as having significant potential; no quantitative details provided in this paper.",
            "strengths": "Ability to generate substantive hypotheses from scholarly texts via prompting without full model fine-tuning.",
            "limitations": "Not detailed in this paper; details deferred to the cited work.",
            "failure_cases": "Not detailed in this paper.",
            "uuid": "e9853.5",
            "source_info": {
                "paper_title": "AstroLLaMA: Towards Specialized Foundation Models in Astronomy",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Adversarial Prompting (Ciucă et al. 2023)",
            "name_full": "Harnessing the Power of Adversarial Prompting and Large Language Models for Robust Hypothesis Generation in Astronomy",
            "brief_description": "Cited prior work using adversarial prompting techniques with LLMs (primarily GPT-4) to improve robustness of hypothesis generation from astronomical literature.",
            "citation_title": "Harnessing the Power of Adversarial Prompting and Large Language Models for Robust Hypothesis Generation in Astronomy",
            "mention_or_use": "mention",
            "model_name": "GPT-4 (in-context prompting/adversarial prompting approach as described in citation)",
            "model_description": "Referenced as employing adversarial prompting strategies and instruction-style prompting to elicit robust hypotheses from LLMs; exact architectural/training details are in the cited work.",
            "model_size": null,
            "input_corpus_description": "Not specified within this AstroLLaMA paper; cited as prior demonstration of LLM-driven hypothesis generation in astronomy.",
            "input_corpus_size": null,
            "topic_query_description": "Adversarial prompting targeted at generating robust scientific hypotheses from astronomy literature; specifics in cited work.",
            "distillation_method": "Adversarial prompting / in-context prompting / instruction learning (described at high level in this paper; implementation specifics are in the cited publication).",
            "output_type": "Hypothesis generation and robust idea synthesis from literature.",
            "output_example": null,
            "evaluation_method": "Not specified here; the AstroLLaMA paper refers to these works for demonstrations of promise in hypothesis generation.",
            "evaluation_results": "Reported in this paper as evidence that prompting/instruction approaches produce substantive hypotheses; no quantitative metrics provided here.",
            "strengths": "Demonstrated potential to generate substantive hypotheses using prompting strategies without heavy model fine-tuning.",
            "limitations": "Details/limitations not enumerated in this paper; likely depend on prompting design and model grounding as discussed generally.",
            "failure_cases": "Not detailed in this paper.",
            "uuid": "e9853.6",
            "source_info": {
                "paper_title": "AstroLLaMA: Towards Specialized Foundation Models in Astronomy",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Galactic ChitChat: Using Large Language Models to Converse with Astronomy Literature",
            "rating": 2
        },
        {
            "paper_title": "Harnessing the Power of Adversarial Prompting and Large Language Models for Robust Hypothesis Generation in Astronomy",
            "rating": 2
        },
        {
            "paper_title": "Building astroBERT, a language model for Astronomy & Astrophysics",
            "rating": 2
        },
        {
            "paper_title": "LLaMA: Open and Efficient Foundation Language Models",
            "rating": 1
        },
        {
            "paper_title": "GPT-4 Technical Report",
            "rating": 1
        }
    ],
    "cost": 0.015835,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>AstroLLaMA</h1>
<p>Tuan Dung Nguyen ${ }^{1,2^{<em>}}$, Yuan-Sen Ting ${ }^{2,3^{</em>}}$, Ioana Ciucă ${ }^{2^{*}}$, Charles O'Neill ${ }^{2^{1}}$, Ze-Chang Sun ${ }^{4^{1}}$, Maja Jabłońska ${ }^{2^{1}}$, Sandor Kruk ${ }^{5^{1}}$, Ernest Perkowski ${ }^{5}$, Jack Miller ${ }^{2}$, Jason Jingshi Li ${ }^{6}$, Josh Peek ${ }^{7}$, Kartheik Iyer ${ }^{8}$, Tomasz Różański ${ }^{2,9}$, Pranav Khetarpal ${ }^{10}$, Sharaf Zaman ${ }^{2}$, David Brodrick ${ }^{2}$, Sergio J. Rodríguez Méndez ${ }^{2}$, Thang Bui ${ }^{2}$, Alyssa Goodman ${ }^{11}$, Alberto Accomazzi ${ }^{12}$, Jill Naiman ${ }^{13}$, Jesse Cranney ${ }^{2}$, Kevin Schawinski ${ }^{14}$, Roberta Rǎileanu ${ }^{15}$, UniverseTBD<br>${ }^{1}$ University of Pennsylvania, United States ${ }^{2}$ Australian National University, Australia<br>${ }^{3}$ Ohio State University, United States ${ }^{4}$ Tsinghua University, China<br>${ }^{5}$ European Space Agency, ESAC, Spain ${ }^{6}$ Learning Machines, Australia<br>${ }^{7}$ Space Telescope Science Institute, United States<br>${ }^{8}$ Columbia University, United States ${ }^{9}$ Wrocław University, Poland<br>${ }^{10}$ Indian Institute of Technology Delhi, India ${ }^{11}$ Harvard University, United States<br>${ }^{12}$ NASA Astrophysics Data System, Center for Astrophysics, United States<br>${ }^{13}$ University of Illinois at Urbana-Champaign ${ }^{14}$ Modulos AG, Switzerland<br>${ }^{15}$ University College London, United Kingdom</p>
<h4>Abstract</h4>
<p>Large language models often excel in many human-language tasks but tend to falter in highly specialized domains like scholarly astronomy. To bridge this gap, we introduce AstroLLaMA, a 7-billion-parameter model finetuned from LLaMA-2 using over 300,000 astronomy abstracts from arXiv. Optimized for traditional causal language modeling, AstroLLaMA shows marked domain adaptation by achieving a $30 \%$ lower perplexity than LLaMA2. Compared to state-of-the-art foundation models, AstroLLaMA generates more insightful and scientifically relevant text completions and embedding extraction despite having significantly fewer parameters. AstroLLaMA serves as a highly domain-specific model with broad fine-tuning potential: Its public release aims to spur astronomy-focused research, including automatic paper summarization, conversational agent development and hypothesis generation.</p>
<h2>1 Introduction</h2>
<p>The advent of Large Language Models (LLMs) has sparked interdisciplinary interest thanks to a confluence of factors: accumulation of massive datasets, leaps in computational power, and breakthroughs in neural architectures. Flagship models like GPT4 (OpenAI, 2023), PaLM (Chowdhery et al., 2022; Goo) and LLaMA (Touvron et al., 2023; Meta, 2023) have exhibited exceptional versatility in a variety of tasks from logical reasoning and comprehension to creative writing, often accomplished via</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>methods like prompting, fine-tuning, and human-in-the-loop reinforcement learning.</p>
<p>The astronomy discipline presents both a unique challenge and a fertile ground for the application of LLMs. The corpus of scholarly texts in astronomy likely constitutes but a minuscule portion of the data on which generic LLMs are trained, resulting in limitations like hallucinations in favor of more "generic" responses. Only about 2.5\% of LLaMA-2's training set, for example, likely comes from arXiv, of which less than $5 \%$ belongs to the astronomy literature. The nature of astronomical research, on the other hand, often involves cross-disciplinary insights due to universally applicable physical processes. When well-curated, LLMs could meaningfully assist with this effort, such as through hypothesis generation.</p>
<p>Existing scales based on in-context prompting and instruction learning, primarily involving GPT4, have already demonstrated significant potential for generating substantive hypotheses (Ciucă and Ting, 2023; Ciucă et al., 2023). Further, the astronomy community's "open sky" policy, which grants public access to the majority of its datasets either immediately or after a brief proprietary period (Almeida et al., 2023; Fabricius et al., 2021), pairs well with the wealth of resources available in archives like NASA's Astrophysics Data System (Accomazzi et al., 2015; Borgman and Wofford, 2021). Such an open-access policy can facilitate deep engagement with the astronomical literature.</p>
<p>Despite their general capabilities, LLMs frequently lag behind specialized, smaller models in</p>
<p>domain-specific applications. This disparity stems from two primary factors: (i) the eclectic nature of the pre-training datasets, which dilutes the focus on specialized subjects in favor of general predictive performance, and (ii) the design ethos of LLMs as "foundation models" aimed at subsequent finetuning tailored to specific tasks. The existing landscape for LLMs in astronomy remains limited, however. To our knowledge, the only specialized model is astroBERT (Grezes et al., 2021), which has 110 million parameters, fine-tuned on nearly 400,000 ADS papers. As an non-generative model, however, astroBERT's utility remains primarily limited to discriminative tasks.</p>
<p>Motivated by these gaps, we present AstroLLaMA, a state-of-the-art generative language model fine-tuned from LLaMA-2. Our model leverages a corpus of 300,000 astronomy abstracts from arXiv and boasts an architecture approximately 67 times larger than that of astroBERT. AstroLLaMA aspires to build upon astroBERT's foundation by offering more improved performance in generating specialized information and broader fine-tuning opportunities for astronomical research. We describe our methodology in Sec. 2, provide some evaluation results in Sec. 3, and finally concluding with some remarks in Sec. 4.</p>
<h2>2 AstroLLaMA</h2>
<p>In this section, we discuss AstroLLaMA's implementation, focusing on the curation of its dataset, base model architecture, and fine-tuning settings.</p>
<h3>2.1 Dataset</h3>
<p>We derive our dataset from the arXiv repository, available on Kaggle. ${ }^{a}$ Our curated subset focuses on papers classified under the astrophysics category (astro-ph), resulting in a collection of 326,238 articles spanning from April 1992 to July 2023. We extract these papers' abstracts to form a corpus consisting of approximately 95 million tokens. The median length of these abstracts is 291 tokens. To enable effective model evaluation, we randomly designate $20 \%$ of this curated dataset for testing.</p>
<h3>2.2 Base model</h3>
<p>Our base model is LLaMA-2, a 6.7 billionparameter model developed by Meta (Meta, 2023). Originally pre-trained on a corpus containing 2 trillion tokens, LLaMA-2 features a context window</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Learning curve of AstroLLaMA during its fine-tuning on the arXiv astrophysics dataset. The figure tracks the evolution of perplexity, a measure of the model's next-token prediction performance. The light blue curve shows the training perplexity after each parameter update step, while the dark black curve provides a smoothed average of the same metric taken over every 10 -step interval.
of 4,096 tokens. For tokenization, the model employs a bytepair encoding strategy (Sennrich et al., 2016; Kudo and Richardson, 2018), with a vocabulary of 32,000 unique tokens.</p>
<h3>2.3 Fine-tuning settings</h3>
<p>We rely on our curated training set, which includes 77 million tokens. The setting of the fine-tuning phase largely follows from Meta (2023). First, special [BOS] (Beginning Of Sequence) and [EOS] (End Of Sequence) tokens are prepended and appended to each training sequence. These sequences are then concatenated and divided into fixed-length chunks, each comprising 512 tokens.</p>
<p>We follow the causal language modeling objective employed during the model's pre-training phase, where the the next token is to be predicted using its preceding context. We use the AdamW optimizer (Loshchilov and Hutter, 2018) with hyperparameters $\beta_{1}=0.9, \beta_{2}=0.95, \epsilon=10^{-5}$ and a batch size of 32 . The learning rate follows a cosine schedule with a linear warmup to a peak value of $3 \times 10^{-4}$ in the first $10 \%$ of the optimization steps and a final learning rate of $10 \%$ of its peak. Additional settings include weight decay and gradient clipping values of 0.1 and 1.0 , respectively. Note that these hyperparameters are set according to LLaMA-2's pre-training phase.</p>
<p>We fine-tune LLaMA over nearly three epochs, corresponding to about 230 million processed tokens, using four NVIDIA A100 GPUs each equipped with 40 GB of VRAM. To achieve re-</p>
<h2>Original abstract</h2>
<p>The Magellanic Stream (MS) - an enormous ribbon of gas spanning $140 \times$ of the southern sky trailing the Magellanic Clouds - has been exquisitely mapped in the five decades since its discovery. However, despite concerted efforts, no stellar counterpart to the MS has been conclusively identified. This stellar stream would reveal the distance and 6 D kinematics of the MS, constraining its formation and the past orbital history of the Clouds. We have been conducting a spectroscopic survey of the most distant and luminous red giant stars in the Galactic outskirts. From this dataset, we have discovered a prominent population of 13 stars matching the extreme angular momentum of the Clouds, spanning up to $100 \times$ along the MS at distances of $60-120 \mathrm{kps}$. Furthermore, these kinematically-selected stars lie along a [ $\alpha / \mathrm{Fa}$ ]-deficient track in chemical space from $-2.5 \cdot[\mathrm{Fe} / \mathrm{H}] \sim 0.5$, consistent with their formation in the Clouds themselves. We identify these stars as high-confidence members of the Magellanic Stellar Stream. Half of these stars are metal-rich and closely follow the gaseous MS, whereas the other half are more scattered and metalpoor. We argue that the metal-rich stream is the recently-formed tidal counterpart to the MS, and speculate that the metal-poor population was thrown out of the SMC outskirts during an earlier interaction between the Clouds. The Magellanic Stellar Stream provides a strong set of constraints: distances, 6 D kinematics, and birth locations - that will guide future simulations towards unveiling the detailed history of the Clouds.</p>
<h2>Completed by LLaMA-2</h2>
<p>The Magellanic Stream (MS) - an enormous ribbon of gas spanning $140 \times$ of the southern sky trailing the Magellanic Clouds - has been exquisitely mapped in the five decades since its discovery. However, despite concerted efforts, no stellar counterpart to the MS has been conclusively identified. This stellar stream would reveal the distance and 6 D kinematics of the MS, constraining its formation and the past orbital history of the Clouds. We propose a novel approach to search for the elusive stellar counterpart of the Magellanic Stream. By employing advanced photometric and spectroscopic techniques, combined with state-of-the-art data mining and machine learning algorithms, we aim to systematically analyze and cross-reference multi-wavelength survey data to identify potential candidate stars belonging to the hyperbursand stellar stream. Our methodology involves leveraging the distinctive chemical signatures and proper motions associated with stars originating from the Magellanic Clouds, allowing us to differentiate them from the foreground and background contaminants. If successful, this discovery would not only provide essential insights into the origin and evolution of the Magellanic Stream, but also enhance our understanding of the interaction between the Magellanic Clouds and the Milky Way. Furthermore, the proposed approach establishes a powerful framework for investigating other unresolved galactic structures and tracing their underlying dynamical histories.</p>
<h2>Completed by AstroLLaMA</h2>
<p>The Magellanic Stream (MS) - an enormous ribbon of gas spanning $140 \times$ of the southern sky trailing the Magellanic Clouds - has been exquisitely mapped in the five decades since its discovery. However, despite concerted efforts, no stellar counterpart to the MS has been conclusively identified. This stellar stream would reveal the distance and 6 D kinematics of the MS, constraining its formation and the past orbital history of the Clouds. We propose a novel approach to search for the elusive stellar counterpart of the Magellanic Stream. By employing advanced photometric and spectroscopic techniques, combined with state-of-the-art data mining and machine learning algorithms, we aim to systematically analyze and cross-reference multi-wavelength survey data to identify potential candidate stars belonging to the hyperbursand stellar stream. Our methodology involves leveraging the distinctive chemical signatures and proper motions associated with stars originating from the Magellanic Clouds, allowing us to differentiate them from the foreground and background contaminants. If successful, this discovery would not only provide essential insights into the origin and evolution of the Magellanic Stream, but also enhance our understanding of the interaction between the Magellanic Clouds and the Milky Way. Furthermore, the proposed approach establishes a powerful framework for investigating other unresolved galactic structures and tracing their underlying dynamical histories.</p>
<h2>Completed by AstroLLaMA</h2>
<p>The Magellanic Stream (MS) - an enormous ribbon of gas spanning $140 \times$ of the southern sky trailing the Magellanic Clouds - has been exquisitely mapped in the five decades since its discovery. However, despite concerted efforts, no stellar counterpart to the MS has been conclusively identified. This stellar stream would reveal the distance and 6 D kinematics of the MS, constraining its formation and the past orbital history of the Clouds. We(1) present a new method to select MS candidate stars from wide field optical imaging, 2) apply this method to the MS, 3) compare the results to existing catalogs, and 4) identify the most promising MS candidate stars. We apply our method to 2547 stars in the Gaia EDR3 catalog that have Gaia proper motions consistent with the MS and use these as a pilot sample to optimize our method. We then apply our method to the entire Gaia EDR3 catalog to identify 46,000 stars that are candidates for the MS, and we use the Gaia-G93 Survey to identify 13,000 of these stars that are likely to be MS candidates. We identify 266 stars with (G-K) colours and [Fe/H] that are consistent with previous estimates of the MS. These stars have proper motions and radial velocities that are consistent with the MS, and we place them on a 3D map of the MS. We find a previously unknown stellar stream, which we call the Southwestern Stream (SES), in the direction of the Clouds and MS. The SES has an M31 distance of $32.2 \times 10.5 \mathrm{kps}$ and a 6 D velocity of $136 \times 110 \mathrm{~km} / \mathrm{s}$. The SES is about 20 times brighter in stars than the MS and about 40 times brighter in stars than the SMC. The SES is also 2.7 times more metal-rich than the MS, which suggests that it is a separate population.</p>
<p>Figure 2: Completion of an astronomy abstract from the arXiv database (ID: 2306.15719) using three different models: GPT-4, LLaMA-2, and AstroLLaMA. Each model is prompted with the same short text snippet, highlighted in their respective boxes, and then produces the rest of the abstract. Two authors of this paper subsequently judge the quality of each completed abstract. GPT-4 tends to produce over-generic statements, while LLaMA-2 often gives off-topic generations. AstroLLaMA demonstrates the most robust completion, offering more relevant concepts and deeper insights specific to the field of astronomy, thus significantly outperforming LLaMA-2 and GPT-4.
source efficiency, we employ 4-bit quantization of the model's parameters and utilize LoRA, a finetuning technique based on low-rank matrix decomposition (Hu et al., 2021). Specifically, we set LoRA's hyperparameters $\alpha$ and dropout rate to 32 and 0.05 , respectively. This process is implemented using Hugging Face's library in Python.</p>
<h3>2.4 Fine-tuning evaluation</h3>
<p>Fig. 1 depicts the performance of AstroLLaMA during its fine-tuning phase. Here, we present perplexity, a commonly used metric for evaluating causal language models. Perplexity is defined as the exponentiation of the training loss, with lower values indicating a better fit.</p>
<p>Our initial observations reveal that LLaMA-2 performs suboptimally on our dataset, with an average perplexity close to 10 . By the conclusion of three epochs, AstroLLaMA achieves an average perplexity of 6.55 . This represents a $32.5 \%$ reduction in perplexity compared to the base LLaMA-2 model, signifying a substantial improvement in the model's new-token prediction accuracy. Considering LLaMA-2 as a strong pre-trained baseline for language modeling, we believe this performance improvement is substantial in this application.</p>
<h2>3 Results</h2>
<p>As illustrated in the previous section, AstroLLaMA outperforms its pre-trained counterpart, LLaMA-2, in terms of context-awareness during token prediction within astronomy abstracts. To delve deeper into the advantages of fine-tuning, we assess AstroLLaMA's general abilities in two key aspects: text generation and embedding space quality. We compare its performance against multiple models, including LLaMA-2, GPT-4 and GPT-3 (ada-002) to provide a comprehensive evaluation.</p>
<h3>3.1 Text generation</h3>
<p>We task AstroLLaMA, LLaMA-2 and GPT-4 with completing a number of astronomy abstracts, allowing us to gauge their ability to comprehend the context and generate a meaningful continuation. Fig. 2 presents an example. In particular, we give each model the first few sentences of an abstract as a prompt and use that model to generate the rest of the abstract. For GPT-4, we utilize ChatGPT and instruct it to limit the completion to a single paragraph. AstroLLaMA and LLaMA-2 are deployed using standard sampling methods, with the temperature set to 0.3 and a maximum new tokens limit of 1,024. We find that altering the temperature setting</p>
<p>does not substantively improve LLaMA-2's results.
Our observations on all generated abstracts largely echo the patterns depicted in Fig. 2. LLaMA-2 frequently deviates from the intended context after generating only a short and often off-topic continuation, resulting in inferior completions. While GPT-4 produces more coherent text, its responses are too generic to capture the nuanced understanding required in the astronomy domain. Even when explicitly prompted to focus on astronomy-related topics, GPT-4's generated text remains largely off-target or generically applicable rather than domain-specific.</p>
<p>In stark contrast, AstroLLaMA exhibits remarkable context-awareness in its completions by showing a deep understanding of astronomical concepts. In Fig. 2, for example, AstroLLaMA comprehends that an effective search for stars in the Magellanic Stream involves a three-step process: initial widefield imaging, followed by refinement using astrometric data from Gaia, and then further curation with spectroscopic data. The model also understands Gaia-ESO is surveying the southern sky and hence can observe (part of) the Magellanic Stream. It also demonstrates nuanced knowledge of the Magellanic Stream, understanding the importance of bifurcation within the stream. As a result, it appropriately completes the text by discussing the southeast stream and exploring metallicity differences to ascertain their origins.</p>
<h3>3.2 Embedding space quality</h3>
<p>We assess models' ability to reflect semantic similarities among astronomy texts. We randomly choose 10,000 abstracts from our dataset and embed them using AstroLLaMA and GPT-3. Specifically, we use OpenAI's API to invoke the text embedding function for GPT-3 (ada-002). To get text embeddings from AstroLLaMA, we pass an input through the model and extract its final hidden states, which contain embeddings for all tokens in the input. Then, we omit the [BOS] token and take the average of all other tokens' embeddings to get the final result. For each pair of abstracts we calculate their cosine similarity (the normalized dot product) between on their vector embeddings.</p>
<p>The top panel of Fig. 3 presents the distribution of these pairwise similarities for the two embedding methods. We find that the embeddings by GPT-3 are overly generic with similarities clustering around relatively high values of $0.7-0.9$, sug-
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Top: Distribution of pairwise cosine similarities among 10,000 randomly selected abstracts from our corpus, divided into 10 equal bins based on similarity levels from GPT-3. Bottom: Two representative examples illustrating divergent cosine similarity values when comparing AstroLLaMA and GPT-3 embeddings.
gesting a lack of discriminative power (most papers are embedded very similarly). AstroLLaMA's embeddings, on the other hand, exhibit much higher variance within each bin. This suggests that our fine-tuned model is more adept at representing the specialized semantic variance inherent to the field of astronomy, which may enable a more granular representation of astronomical content and can facilitate higher-quality document retrieval and semantic analysis.</p>
<p>The bottom panel of Fig. 3 provides two representative examples where AstroLLaMA and GPT-3 classifications diverge. In the first example, GPT-3 fixates on the keyword "magnetized," resulting in an inflated similarity score despite the contents being markedly different. AstroLLaMA, on the other hand, successfully distinguishes between these disparate contexts. In the second example, AstroLLaMA accurately identifies that the study of Spitzer is closely related to star formation. GPT-3, however, fails to make this connection due to the absence of matching keywords.</p>
<h2>4 Conclusion</h2>
<p>In this work, we introduce AstroLLaMA, a 7-billion-parameter language model fine-tuned on a dataset encompassing over 300,000 abstracts from astronomical research papers. Compared to its base model, LLaMA-2, and even GPT-4, a cur-</p>
<p>rent state-of-the-art general LLM, AstroLLaMA exhibits marked improvements in generating highquality abstracts and a competent grasp of relevant information in this specialized literature.</p>
<p>The efficacy of AstroLLaMA demonstrated in this paper suggests a multitude of avenues worthy of exploration for subsequent work. With well-curated instruction datasets, researchers can fine-tune our model to perform tasks such as question answering, scientific paper summarization and academic writing assistance. Combining AstroLLaMA with other information retrieval models can lead to promising systems for hypothesis generation. Finally, AstroLLaMA is a potential candidate to be incorporated into specialized multi-modal models (Liu et al., 2023), going beyond the limits of text in astronomical research.</p>
<p>AstroLLaMA, nevertheless, is not without limitations. During its evaluation, the most salient drawback we find is the model's knowledge gaps in certain areas of astronomy. In Fig. 2, for example, AstroLLaMA's estimation of potential star candidates from Gaia-ESO data is notably inaccurate. Another concern lies in the model's tendency to generate hallucinated or fictitious numerical data, an issue most likely attributed to our simple focus on next-token prediction-a pure NLP objectiverather than explicitly steering the model toward factual accuracy. Achieving a desirable balance of "faithfulness" (respecting scientific evidence and accuracy) and "creativity" (being able to come up with interesting hypotheses) remains an open challenge in research at the intersection of generative models and other scientific disciplines.</p>
<p>There are a number of on-going efforts to address the limitations of AstroLLaMA as well as explore its broad capabilities in this sphere. We are in the process of enriching AstroLLaMA's training data by including each paper's full LaTeX sources, going beyond its abstracts and thereby increasing the token count by approximately two orders of magnitude. Although this requires a nontrivial data quality control procedure, it will almost certainly improve our model's predictive performance substantially, making it even more adapted to this literature and less prone to hallucination. A more systematic evaluation of AstroLLaMAincluding a larger set of candidate abstracts for completion, a more well-defined evaluation scheme and a larger, more diverse set of judging expertswill lead to more grounded comparison with state-
of-the-art models. Finally, the potential of AstroLLaMA to generate high-quality and creative hypotheses through novel prompting and fine-tuning techniques is being extensively studied.</p>
<p>AstroLLaMA stands as a compelling prototype for specialized LLMs in astronomy, showing superior context-aware capabilities compared to GPT4 despite having much fewer parameters. Our methodology is simple and general enough for researcher to explore even more specific areas of astrophysics or even to be adapted to other areas of scientific research.</p>
<p>We have made AstroLLaMA's weights, training data and code for reproducibility publicly available to researchers who are aiming to leverage LLMs for astronomy-centric applications. Along with this, we are establishing various "playgrounds" on Hugging Face to invite interested readers to explore AstroLLaMA and further refine this robust starting point for a variety of relevant downstream applications. ${ }^{\text {b }}$</p>
<h2>Acknowledgments</h2>
<p>We thank the Microsoft Accelerate Foundation Models Academic Research Initiative. Access to advanced AI capabilities from Microsoft Research has greatly accelerated our work in applying language models to automate the analysis of the astronomical literature. We also thank the anonymous reviewers who gave useful insights and suggestions, especially on the potential applications of AstroLLaMA within and beyond astronomy.</p>
<h2>Ethics Statement</h2>
<p>We obtain the pre-trained weights for LLaMA-2 from Meta, which offers these models for download on Hugging Face. The arXiv dataset used in this paper is publicly available on Kaggle. While we have demonstrated that AstroLLaMA is capable of generating high-quality, relevant abstracts for astronomical research papers, we have noted that it has the potential to generate inaccurate data and measurements. This should serve as a caution for researchers aiming to use this model for downstream tasks, and we invite the adoption of alignment strategies in future work to ameliorate this issue.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>References</h2>
<p>Google AI PaLM 2. https://ai.google/discover/palm2/.
A. Accomazzi, M. J. Kurtz, E. A. Henneken, R. Chyla, J. Luker, C. S. Grant, D. M. Thompson, A. Holachek, R. Dave, and S. S. Murray. 2015. ADS: The Next Generation Search Platform. In Open Science at the Frontiers of Librarianship, volume 492 of Astronomical Society of the Pacific Conference Series, page 189 .</p>
<p>Andrés Almeida, Scott F. Anderson, Maria ArgudoFernández, Carles Badenes, Kat Barger, Jorge K. Barrera-Ballesteros, Chad F. Bender, Erika Benitez, Felipe Besser, Dmitry Bizyaev, Michael R. Blanton, John Bochanski, Jo Bovy, William Nielsen Brandt, Joel R. Brownstein, Johannes Buchner, Esra Bulbul, Joseph N. Burchett, Mariana Cano Díaz, Joleen K. Carlberg, Andrew R. Casey, Vedant Chandra, Brian Cherinka, Cristina Chiappini, Abigail A. Coker, Johan Comparat, Charlie Conroy, Gabriella Contardo, Arlin Cortes, Kevin Covey, Jeffrey D. Crane, Katia Cunha, Collin Dabbieri, James W. Davidson Jr. au2, Megan C. Davis, Nathan De Lee, José Eduardo Méndez Delgado, Sebastian Demasi, Francesco Di Mille, John Donor, Peter Dow, Tom Dwelly, Mike Eracleous, Jamey Eriksen, Xiaohui Fan, Emily Farr, Sara Frederick, Logan Fries, Peter Frinchaboy, Boris T. Gaensicke, Junqiang Ge, Consuelo González Ávila, Katie Grabowski, Catherine Grier, Guillaume Guiglion, Pramod Gupta, Patrick Hall, Keith Hawkins, Christian R. Hayes, J. J. Hermes, Lorena Hernández-García, David W. Hogg, Jon A. Holtzman, Hector Javier IbarraMedel, Alexander Ji, Paula Jofre, Jennifer A. Johnson, Amy M. Jones, Karen Kinemuchi, Matthias Kluge, Anton Koekemoer, Juna A. Kollmeier, Marina Kounkel, Dhanesh Krishnarao, Mirko Krumpe, Ivan Lacerna, Paulo Jakson Assuncao Lago, Chervin Laporte, Ang Liu, Chao Liu, Xin Liu, Alexandre Roman Lopes, Matin Macktoobian, Viktor Malanushenko, Dan Maoz, Thomas Masseron, Karen L. Masters, Gal Matijevic, Aidan McBride, Ilija Medan, Andrea Merloni, Sean Morrison, Natalie Myers, Szabolcs Mészáros, C. Alenka Negrete, David L. Nidever, Christian Nitschelm, Audrey Oravetz, Daniel Oravetz, Kaike Pan, Yingjie Peng, Marc H. Pinsonneault, Rick Pogge, Dan Qiu, Anna Barbara de Andrade Queiroz, Solange V. Ramirez, HansWalter Rix, Daniela Fernández Rosso, Jessie Runnoe, Mara Salvato, Sebastian F. Sanchez, Felipe A. Santana, Andrew Saydjari, Conor Sayres, Kevin C. Schlaufman, Donald P. Schneider, Axel Schwope, Javier Serna, Yue Shen, Jennifer Sobeck, Ying-Yi Song, Diogo Souto, Taylor Spoo, Keivan G. Stassun, Matthias Steinmetz, Ilya Straumit, Guy Stringfellow, José Sánchez-Gallego, Manuchehr Taghizadeh-Popp, Jamie Tayar, Ani Thakar, Patricia B. Tissera, Andrew Tkachenko, Hector Hernandez Toledo, Benny Trakhtenbrot, Jose G. Fernandez Trincado, Nicholas Troup, Jonathan R. Trump, Sarah Tuttle, Natalie Ulloa, Jose Antonio Vazquez-Mata, Pablo Vera Alfaro, Sandro Villanova, Stefanie Wachter, Anne-Marie</p>
<p>Weijmans, Adam Wheeler, John Wilson, Leigh Wojno, Julien Wolf, Xiang-Xiang Xue, Jason E. Ybarra, Eleonora Zari, and Gail Zasowski. 2023. The eighteenth data release of the sloan digital sky surveys: Targeting and first spectra from sdss-v.</p>
<p>Christine L. Borgman and Morgan F. Wofford. 2021. From Data Processes to Data Products: Knowledge Infrastructures in Astronomy. arXiv e-prints.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling with Pathways.</p>
<p>Ioana Ciucă and Yuan-Sen Ting. 2023. Galactic ChitChat: Using Large Language Models to Converse with Astronomy Literature. arXiv e-prints.</p>
<p>Ioana Ciucă, Yuan-Sen Ting, Sandor Kruk, and Kartheik Iyer. 2023. Harnessing the Power of Adversarial Prompting and Large Language Models for Robust Hypothesis Generation in Astronomy. arXiv e-prints.
C. Fabricius, X. Luri, F. Arenou, C. Babusiaux, A. Helmi, T. Muraveva, C. Reylé , F. Spoto, A. Vallenari, T. Antoja, E. Balbinot, C. Barache, N. Bauchet, A. Bragaglia, D. Busonero, T. CantatGaudin, J. M. Carrasco, S. Diakité, M. Fabrizio, F. Figueras, A. Garcia-Gutierrez, A. Garofalo, C. Jordi, P. Kervella, S. Khanna, N. Leclerc, E. Licata, S. Lambert, P. M. Marrese, A. Masip, P. Ramos, N. Robichon, A. C. Robin, M. Romero-Gómez, S. Rubele, and M. Weiler. 2021. igaia/iearly data release 3. Astronomy \&amp; Astrophysics, 649:A5.</p>
<p>Felix Grezes, Sergi Blanco-Cuaresma, Alberto Accomazzi, Michael J. Kurtz, Golnaz Shapurian, Edwin Henneken, Carolyn S. Grant, Donna M. Thompson, Roman Chyla, Stephen McDonald, Timothy W. Hostetler, Matthew R. Templeton, Kelly E. Lockhart, Nemanja Martinovic, Shinyi Chen, Chris Tanner, and Pavlos Protopapas. 2021. Building astroBERT, a language model for Astronomy \&amp; Astrophysics.</p>
<p>Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. arXiv e-prints.</p>
<p>Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66-71.</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning.</p>
<p>Ilya Loshchilov and Frank Hutter. 2018. Decoupled Weight Decay Regularization. In International Conference on Learning Representations.</p>
<p>Meta. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/.</p>
<p>OpenAI. 2023. GPT-4 Technical Report.
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715-1725.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{\text {b }}$ All details can be found at https://huggingface.co/ universeTBD/astrollama.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9126 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9126</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9126</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-277103645</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.13503v3.pdf" target="_blank">SciHorizon: Benchmarking AI-for-Science Readiness from Scientific Data to Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> In recent years, the rapid advancement of Artificial Intelligence (AI) technologies, particularly Large Language Models (LLMs), has revolutionized the paradigm of scientific discovery, establishing AI-for-Science (AI4Science) as a dynamic and evolving field. However, there is still a lack of an effective framework for the overall assessment of AI4Science, particularly from a holistic perspective on data quality and model capability. Therefore, in this study, we propose SciHorizon, a comprehensive assessment framework designed to benchmark the readiness of AI4Science from both scientific data and LLM perspectives. First, we introduce a generalizable framework for assessing AI-ready scientific data, encompassing four key dimensions: Quality, FAIRness, Explainability, and Compliance-which are subdivided into 15 sub-dimensions. Drawing on data resource papers published between 2018 and 2023 in peer-reviewed journals, we present recommendation lists of AI-ready datasets for Earth, Life, and Materials Sciences, making a novel and original contribution to the field. Concurrently, to assess the capabilities of LLMs across multiple scientific disciplines, we establish 16 assessment dimensions based on five core indicators Knowledge, Understanding, Reasoning, Multimodality, and Values spanning Mathematics, Physics, Chemistry, Life Sciences, and Earth and Space Sciences. Using the developed benchmark datasets, we have conducted a comprehensive evaluation of over 50 representative open-source and closed source LLMs. All the results are publicly available and can be accessed online at www.scihorizon.cn/en.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Hypothesis generation with large language models <em>(Rating: 2)</em></li>
                <li>Can chatgpt be used to generate scientific hypotheses <em>(Rating: 2)</em></li>
                <li>What can large language models do in chemistry? a comprehensive benchmark on eight tasks <em>(Rating: 2)</em></li>
                <li>Text2Mol: Cross-modal molecule retrieval with natural language queries <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9126",
    "paper_id": "paper-277103645",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Hypothesis generation with large language models",
            "rating": 2,
            "sanitized_title": "hypothesis_generation_with_large_language_models"
        },
        {
            "paper_title": "Can chatgpt be used to generate scientific hypotheses",
            "rating": 2,
            "sanitized_title": "can_chatgpt_be_used_to_generate_scientific_hypotheses"
        },
        {
            "paper_title": "What can large language models do in chemistry? a comprehensive benchmark on eight tasks",
            "rating": 2,
            "sanitized_title": "what_can_large_language_models_do_in_chemistry_a_comprehensive_benchmark_on_eight_tasks"
        },
        {
            "paper_title": "Text2Mol: Cross-modal molecule retrieval with natural language queries",
            "rating": 1,
            "sanitized_title": "text2mol_crossmodal_molecule_retrieval_with_natural_language_queries"
        }
    ],
    "cost": 0.00867,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SciHorizon: Benchmarking AI-for-Science Readiness from Scientific Data to Large Language Models
29 May 2025</p>
<p>Chuan Qin 
Computer Network Information Center
Chinese Academy of Sciences</p>
<p>University of Chinese Academy of Sciences</p>
<p>Xin Chen 
Computer Network Information Center
Chinese Academy of Sciences</p>
<p>University of Chinese Academy of Sciences</p>
<p>University of Chinese Academy of Sciences</p>
<p>University of Science and Technology of China</p>
<p>Chengrui Wang 
Computer Network Information Center
Chinese Academy of Sciences</p>
<p>Pengmin Wu 
Computer Network Information Center
Chinese Academy of Sciences</p>
<p>Yihang Cheng 
Jingyi Zhao 
Computer Network Information Center
Chinese Academy of Sciences</p>
<p>Meng Xiao 
Computer Network Information Center
Chinese Academy of Sciences</p>
<p>Xiangchao Dong 
Computer Network Information Center
Chinese Academy of Sciences</p>
<p>Qingqing Long 
Computer Network Information Center
Chinese Academy of Sciences</p>
<p>Boya Pan 
Computer Network Information Center
Chinese Academy of Sciences</p>
<p>Han Wu 
Hefei University of Technology</p>
<p>Chengzan Li 
Computer Network Information Center
Chinese Academy of Sciences</p>
<p>University of Chinese Academy of Sciences</p>
<p>Yuanchun Zhou 
Computer Network Information Center
Chinese Academy of Sciences</p>
<p>University of Chinese Academy of Sciences</p>
<p>Hui Xiong 
Computer Network Information Center
Chinese Academy of Sciences</p>
<p>The Hong Kong University of Science and Technology (Guangzhou)</p>
<p>The Hong Kong University of Science and Technology</p>
<p>Hengshu Zhu 
Computer Network Information Center
Chinese Academy of Sciences</p>
<p>University of Chinese Academy of Sciences</p>
<p>SciHorizon: Benchmarking AI-for-Science Readiness from Scientific Data to Large Language Models
29 May 2025163086BFB83681C7A8AA4523925CA867arXiv:2503.13503v3[cs.LG]
Drawing on data resource papers published between 2018 and 2023 in peer-reviewed journals, we present recommendation lists of AI-ready datasets for Earth, Life, and Materials Sciences, making a novel and original contribution to the field.Concurrently, to assess the capabilities of LLMs across multiple scientific disciplines, we establish 16 assessment dimensions based on five core indicators-Knowledge, Understanding, Reasoning, Multimodality, and Values-spanning Mathematics, Physics, Chemistry, Life Sciences, and Earth and Space Sciences.Using the developed benchmark datasets, we have conducted a comprehensive evaluation of over 50 representative open-source and closed-source LLMs.All the results are publicly available and can be accessed online at www.scihorizon.cn/en.1 The platform was initially launched on January 22, 2025 [31].</p>
<p>In recent years, the rapid advancement of Artificial Intelligence (AI) technologies, particularly Large Language Models (LLMs), has revolutionized the paradigm of scientific discovery, establishing AI-for-Science (AI4Science) as a dynamic and evolving field.However, there is still a lack of an effective framework for the overall assessment of AI4Science, particularly from a holistic perspective on data quality and model capability.Therefore, in this study, we propose SciHorizon, a comprehensive assessment framework designed to benchmark the readiness of AI4Science from both scientific data and LLM perspectives.First, we introduce a generalizable framework for assessing AI-ready scientific data, encompassing four key dimensions-Quality, FAIRness, Explainability, and Compliance-which are subdivided into 15 sub-dimensions.</p>
<p>Introduction</p>
<p>Scientific data resources serve as the core driver propelling scientific discoveries and as a critical enabler for interdisciplinary research that integrates Artificial Intelligence (AI) with other disciplines.As AI technologies advance rapidly, particularly in Large Language Models (LLMs), AI-for-Science (AI4Science) has emerged as a frontier research hotspot [1][2][3][4][5][6][7].For instance, in structural biology, DeepMind's AlphaFold has revolutionized protein structure prediction by achieving unprecedented accuracy, effectively resolving a challenge that previously demanded decades of experimental effort [8].Likewise, LLMs, such as OpenAI's ChatGPT, are expanding AI's role in scientific research by not only enhancing data analysis capabilities [9][10][11][12], but also assisting in literature review [13], hypothesis generation [14,15], and complex reasoning [16,17].These models facilitate the synthesis of vast scientific knowledge, accelerate discovery processes, and foster interdisciplinary collaboration, thereby reshaping the landscape of modern scientific inquiry.</p>
<p>Despite rapid advancements in AI4Science, the field faces persistent challenges stemming from the reliance on large-scale, interdisciplinary datasets and the scarcity of AI-ready, high-quality data .Therefore, researchers advocate for systematic assessment frameworks that integrate metrics for data accuracy, completeness, and domain relevance.While traditional tools are instrumental in assessing data quality from different perspectives [18][19][20][21] or emphasizing specific aspects of data readiness [22][23][24][25], they fall short of addressing the unique requirements of AI applications in the scientific domain.This leads to two challenges in evaluating scientific data in AI4Science: 1) AI researchers struggle to efficiently extract valuable insights from vast, domain-specific datasets, leading to the underutilization or misapplication of high-potential data that could drive significant advances in AI-driven scientific discovery; and 2) Researchers across disciplines lack clear criteria to assess whether their datasets align with AI model requirements, hindering the development and optimization of high-quality, AI-ready data.</p>
<p>Meanwhile, after assessing the possibility of high-quality scientific data integration, a critical challenge lies in comprehensively and effectively assessing the application capabilities of AI4Science models at a fine-grained level.Despite the proliferation of diverse assessment frameworks for LLMs [26], those specifically designed for scientific applications remain scarce.Recently, several targeted benchmarks have been proposed, however, they exhibit notable limitations: 1) Most of them focus on specific disciplines [27,28], lacking a unified framework that accommodates multiple scientific fields; 2) Their capability assessments are relatively narrow-for instance, JEEBench [29] primarily evaluates reasoning abilities related to basic computations, while MultiMedQA focuses on assessing clinical knowledge [28], both of which fall short of providing a more comprehensive and fine-grained evaluation; and 3) No existing framework assesses whether LLMs embody the correct scientific research values [30], which is crucial for the responsible adoption of AI techniques in scientific tasks.</p>
<p>To bridge these critical gaps, here we present SciHorizon-an integrated assessment framework that evaluates AI4Science readiness from both AI-ready data and LLM perspectives.For AI-ready scientific data, we propose a generalizable AI-readiness assessment framework, across four principal dimensions-Quality, FAIRness, Explainability, and Compliance-operationalized through 15 sub-dimensions.To demonstrate its applicability, we analyze approximately 1,600 datasets published between 2018 and 2023, primarily consisting of those published in data resource papers in peer-reviewed journals (e.g., Scientific Data and ESSD), identifying dataset recommendations for Earth, Life, and Materials Sciences to support AI-driven scientific advancements.For the LLM capability on different disciplines, we develop a fine-grained assessment matrix spanning five core competencies-Knowledge, Understanding, Reasoning, Multimodality, and Values-granularized into 16 sub-dimensions.Our benchmark suite, covering Mathematics, Physics, Chemistry, Life Sciences, and Earth and Space Sciences, enables systematic comparison of more than 50 representative open-source and closed-source LLMs.All the results are publicly available and can be accessed online at www.scihorizon.cn/en 1 .</p>
<p>Related Works</p>
<p>The related works of this paper can be grouped into two categories, namely Data AI-Readiness and LLM Benchmarking.</p>
<p>Data AI-Readiness</p>
<p>The assessment of data AI-readiness has drawn more and more attention.Hiniduma et al. proposed a taxonomy of data readiness for AI metrics [32] and developed a quantitative assessment framework for data AI readiness [33].They are mainly from a data modality perspective and lack support and applicability for scientific data.Regarding the scientific data, the FAIR principles (Findable, Accessible, Interoperable, Reusable) presented in 2016 have been widely recognized as basic characteristics for scientific data sharing [34].The principles are also used for AI-readiness assessment for scientific data [35,36].Besides FAIRness evaluation, some further practices on AI-readiness assessment have also been carried out.ESIP Data Readiness Cluster published a checklist to examine AI-readiness for open environmental datasets [37], covering data quality, data documentation, data access, and data preparation.The checklist serves more as a guideline than an evaluation and has less involvement in the AI applications.NIH Bridge to Artificial Intelligence (Bridge2AI) Standards Working Group presented criteria and methods for assessing the AI-readiness of biomedical data [38], which is a good practice for a specific discipline.Although all these separate studies have been developed, comprehensive approaches for AI-readiness assessment for scientific data are still needed.</p>
<p>LLM Benchmarking</p>
<p>With the rapid advancement of LLMs [39][40][41], numerous benchmarks have emerged to assess their performance across scientific disciplines.Early evaluations focused primarily on mathematical reasoning, such as GSM8K [27] and MATH [42].Recently, specialized benchmarks such as TheoremQA [43] and MathVista [44] have been introduced to assess theorem-based reasoning and multimodal mathematical understanding.In addition to mathematics, researchers have developed broader scientific benchmarks to evaluate LLMs across multiple domains, such as ScienceQA [45], JEEBench [29], SciEval [46], and BIG-Bench [47].Despite these advancements, existing benchmarks are still limited in scope.Most focus on isolated disciplines [48], without providing a unified framework for cross-disciplinary evaluation.Furthermore, many evaluations prioritize factual recall [49,50], overlooking the need for fine-grained assessments of scientific problem-solving and advanced reasoning.More critically, no benchmark systematically evaluates whether LLMs adhere to fundamental scientific research values-such as academic integrity, fairness, and transparency-which are crucial for their responsible use in scientific workflows [30].</p>
<p>SciHorizon Framework</p>
<p>The framework of SciHorizon consists of two components: scientific data assessment and LLM assessment.Figure 1 presents an overview of the SciHorizon platform and its fine-grained assessment dimensions for both scientific data and LLMs.</p>
<p>Scientific Data Assessment</p>
<p>To systematically assess the AI readiness of scientific datasets, we have developed a comprehensive evaluation framework based on four key dimensions: Quality, FAIRness, Explainability, and Compliance.The following sections provide a detailed overview of this assessment framework and its practices.</p>
<p>Quality</p>
<p>This metric measures the Completeness, Accuracy, Consistency, and Timeliness of datasets to make sure the datasets are of high quality.We expand the traditional quality metrics by combining characteristics of scientific data.</p>
<p>• Completeness refers to the thorough recording of data elements and the inclusion of all necessary documents.This encompasses not only the data entity itself but also the corresponding data description files or metadata.• Accuracy requires minimal noise and redundancy.Moreover, for reproduced data, the processing methods should be clearly documented or supplemented with an accuracy analysis.• Consistency refers to both internal coherence within a dataset and external alignment with related datasets, ensuring that the same labels are used for identical variables.</p>
<p>• Timeliness refers to the prompt publication and continuous updating of data.For instance, time-sequenced data typically evolves over time.The timeliness metric evaluates whether datasets have been updated following their initial publication.</p>
<p>FAIRness</p>
<p>This metric measures the readiness of data for sharing by evaluating its FAIR principles, i.e., Findability, Accessibility, Interoperability, and Reusability.Given that these principles are well established within the research community, we do not reiterate the criteria here.However, our approach to measuring FAIRness differs from previous work in that we establish a recommended set of identifiers, vocabularies, formats, and standards, translating the principles into practical and actionable metrics.</p>
<p>Explainability</p>
<p>This metric measures the application-oriented explainability of data, which is essential for scientific AI models.To ensure that data accurately reflects scientific facts and enhances model explainability, it is important to assess their Diversity, Unbias, Domain Applicability, and Task Applicability.</p>
<p>• Diversity refers to the scale of dataset parameters and knowledge elements.Generally, the more parameters there are, the richer the knowledge information contained in the data.• Task Applicability refers to the suitability of data for AI tasks.Data should be structured and formatted in a way that aligns with the requirements of typical AI applications.</p>
<p>Compliance</p>
<p>Model training on vast and diverse data raises legal and ethical concerns.Provenance, Ethics, Safety, and Trustworthiness are important factors for ensuring the compliance of scientific AI models.</p>
<p>• Provenance requires clear documentation of data sources, authorship, licensing, and other relevant metadata to ensure transparency and traceability.</p>
<p>Assessment Practices</p>
<p>We set up an integrated framework for AI-Readiness evaluation, combining qualitative and quantitative evaluation.To evaluate the Quality, Explainability, and Compliance of the data governance framework, we implemented a hybrid human-in-the-loop review mechanism integrating computational pre-screening and Delphi process of expert consensus.Initially, pre-trained expert models generated sub-dimensional numerical outputs for each metric.These outputs combined quantifiable parameters and modelinferred values derived from historical expert knowledge.Subsequently, we invited 10 domain experts in data governance engaged in the Delphi iterative review : across multiple anonymized rounds, they critically evaluated the system-generated values, proposing adjustments to address semantic biases in explainability or recalibrate compliance weightings.This process continued until the system outputs and expert judgments converged to a unified consensus [51].To evaluate the FAIRness of scientific data, we utilized an evaluation toolkit developed by the Computer Network Information Center of the Chinese Academy of Sciences [52].This toolkit operationalized domain-specific FAIR principles through automated metadata validation and reproducibility tests, ensuring standardized and objective assessments of data assets across storage, sharing, and reuse workflows.</p>
<p>LLM Assessment</p>
<p>Assessing LLMs in AI4Science applications requires a comprehensive framework that systematically examines multiple dimensions of model capabilities.Existing benchmarks are either too broad to capture scientific depth or too specialized to cover multiple disciplines.To address these limitations, we propose a multi-dimensional assessment framework specifically designed for AI4Science across various disciplines.</p>
<p>Our benchmark assesses LLMs across five fundamental dimensions: Knowledge, Reasoning, Understanding, Multimodality, and Values.Each dimension represents a distinct yet complementary aspect of an LLM's scientific competence, ensuring a rigorous and actionable assessment.The following sections detail the design and assessment criteria for each dimension.</p>
<p>Knowledge</p>
<p>Knowledge aims to assess an LLM's ability to acquire and apply scientific knowledge across various domains.Traditional knowledge assessments primarily focus on factuality, often overlooking finer-grained aspects of knowledge application.To provide a more comprehensive assessment, we decompose knowledge into four key subdimensions: Factuality, Robustness, Externalization, and Helpfulness, as detailed below.</p>
<p>• • Factuality: Factuality refers to the extent to which generated text is free from factual errors [53].It serves as a fundamental criterion in knowledge assessment, ensuring that LLMs produce scientifically accurate responses to domain-specific questions.</p>
<p>We have extensively collected a large number of public datasets for scientific question answering, including AGIEval [54], C-Eval [55], CMB [56], CMMLU [57], Xiezhi [58], BB-GeoEval [59], and Replication [60].Additionally, we engaged experts to organize, translate, and refine these datasets to ensure linguistic consistency and domain accuracy.This process resulted in a unified set of multiple-choice questions.The unified dataset serves as the foundation for the entire knowledge dimension assessment.To quantify factual accuracy, we calculate the score as:
S k 1 = # Correct Responses # Questions
, where # denotes the count.S k 1 measures the LLM's ability to recognize domain-specific knowledge without reasoning.• • Robustness: Robustness refers to the ability of LLM to handle noisy input while maintaining accurate responses.To assess this capability, we apply perturbations to the question text through word dropout, synonym replacement, and character swaps.The model is then tasked with denoising the input and producing the correct answer.We denote the model's accuracy on altered input as S k and quantify Robustness by computing the ratio of S k 1 to S k 2 .A higher Robustness score indicates that the model effectively handles noisy or altered input while maintaining the ability to generate correct scientific answers.</p>
<p>• • Externalization: Externalization refers to an LLM's ability to articulate acquired scientific knowledge in a clear, logical, and coherent manner.This dimension assesses the model's capacity to convey domain knowledge in a structured and comprehensible way.</p>
<p>To assess this capability, we prompt the model to generate competing explanations for hypotheses following IBE-Eval [61].To enhance logical coherence and facilitate further analysis, the model's responses are constrained to an If-Then format.This approach ensures that the model generates structured and logically coherent knowledge.Since such knowledge is typically presented as long-form text with multiple sentences, we propose measuring sentence-level cohesion [62], which quantifies fluency and logical connectivity between sentences.Sentence-level cohesion is assessed by analyzing the perplexity of individual sentences using a GPT-based model:
S k 3 = 1 |x| |x| i=1 1 PPL(xi)
, where PPL(x i ) is computed by GPT-Neo [63], x represents the generated knowledge, |x| denotes the total number of sentences in the generated text, and PPL(x i ) represents the perplexity of sentence x i .A higher S k 3 indicates stronger externalization capabilities, meaning the model consistently produces sentences with lower perplexity.This ensures that scientific knowledge is not only accurate but also effectively communicated in AI4Science applications.</p>
<p>• • Helpfulness: Helpfulness refers to an LLM's ability to provide scientifically useful and relevant knowledge that aids in problem-solving.This dimension assesses whether the knowledge generated by the LLM is actionable and effective in guiding low-parameter LLMs in scientific question-answering and decision-making.</p>
<p>To assess Helpfulness, we employ a two-step assessment procedure.First, we generate domain-specific knowledge using the assessed LLM.Then, we inject generated knowledge into a low-parameter model, such as LLaMA3-8B [64], and assess its ability to utilize the provided information to answer a scientific multiple-choice question.Helpfulness is measured based on the accuracy of the low-parameter model in selecting the correct answer after receiving the additional knowledge, denoted as S k 4 .A higher Helpfulness score indicates that the knowledge provided by the LLM is relevant and useful for guiding problem-solving, ensuring that AI4Science models generate practical scientific insights.</p>
<p>Understanding</p>
<p>The assessment of Understanding in LLMs assesses their ability to comprehend and contextualize scientific content across diverse disciplines.The assessment primarily focuses on Scientific Fact Understanding tasks, which test the model's ability to interpret and understand complex scientific concepts beyond factual memorization.Understanding is crucial to ensuring that LLMs are not merely capable of recalling information but can also demonstrate deep comprehension of scientific content.</p>
<p>To assess LLMs' Understanding of scientific concepts across various disciplines, we curate domain-specific datasets for assessment.For Life Sciences, we use MedM-CQA [65], MedQA [66], and GPT-3 Clinical Vignettes [67].Chemistry assessment is conducted using ChEBI-20 [68], while Earth and Space Sciences are assessed with GeoBench [69].For Mathematics, we employ MathBench [70], and for Physics, Sci-Eval [46].To ensure these datasets effectively measure comprehension rather than simple recall, we enlist experts to refine and transform them into multiple-choice questions that assess the model's grasp of scientific content.</p>
<p>Reasoning</p>
<p>The assessment of reasoning capabilities in LLMs is crucial for assessing their ability to process information, analyze data, and derive logical conclusions across various scientific disciplines.However, existing benchmarks primarily focus on numerical reasoning, which is often limited to elementary arithmetic operations such as addition, subtraction, multiplication, and exponentiation, such as MMLU [42] and AGIEval [54].To address this gap, we propose a novel reasoning assessment that incorporates both numerical reasoning and deductive reasoning.</p>
<p>• • Numerical Reasoning: Numerical Reasoning assesses an LLM's ability to perform arithmetic operations, interpret quantitative data, and solve mathematical problems within scientific contexts.To assess this capability, we employ a curated set of expert-filtered datasets spanning multiple scientific disciplines.For Life Sciences and Chemistry, we utilize GPQA [71], MMLU [42], SciEval [46], and ScienceQA [45].The Earth and Space Sciences assessment is based on MMLU and ScienceQA, while Mathematics is assessed using AQuA [72], Big-Bench [47], MATHQA [73], and MMLU.Finally, Physics reasoning is tested using BigBench, GPQA, MMLU, SciEval, and ScienceQA.Each dataset consists of structured, multiple-choice numerical reasoning tasks covering mathematical modeling, quantitative estimation, and applied problem-solving in scientific contexts.Model performance is quantified by accuracy, denoted as S r 1 , which measures the correctness of the model's responses.
S r = 2 i=1 S r i /2.</p>
<p>Multimodality</p>
<p>The Multimodality dimension assesses LLMs based on their ability to comprehend and reason with multimodal scientific information.We define two primary sub-dimensions for assessing multimodal capabilities:</p>
<p>• • Scientific Chart Understanding: Assesses an LLM's ability to extract factual information, analyze data trends, and identify categorical relationships and key insights from scientific visual representations, such as line graphs and bar charts.• • Scientific Chart Reasoning: Assesses an LLM's capacity for higher-order reasoning, drawing conclusions, and applying scientific knowledge to predict outcomes based on visual data.Unlike basic data comprehension, this dimension requires logical inference.</p>
<p>To assess the Multimodal dimension, we collect a large number of scientific charts and employ domain experts for the construction of a multimodal benchmark dataset.The dataset consists of multiple-choice questions.Model performance is quantified by accuracy, denoted as S m 1 for understanding and S m 2 for reasoning, respectively.</p>
<p>Values</p>
<p>The assessment of value alignment aims to assess an LLM's adherence to ethical and moral standards, particularly in scientific contexts.We are the first to introduce the assessment in scientific research value.This dimension ensures that LLMs not only generate accurate outputs but also uphold integrity, fairness, and social responsibility in their applications.</p>
<p>Our value alignment framework is grounded in widely recognized ethical guidelines, including Compliance with Laws and Regulations, Academic Integrity, Commitment to Objective Truth, Respect for Humanity and Nature, Fairness and Justice, Privacy and Security Protection, and Transparency and Explainability.</p>
<p>To systematically assess Values in LLMs, we propose a GPT-based pipeline for constructing ethical assessment questions for each sub-dimension.This pipeline comprises four sequential steps:</p>
<p>• 1. Generating Research Scenarios We first create structured research contexts for different scientific disciplines as follows:</p>
<p>Assume you are a researcher in {Primary Discipline}, specializing in {Secondary Disci-pline}.Provide a structured list of research directions and subtopics, following established disciplinary classification standards.</p>
<p>• 2. Expanding Subtopic Information For each identified research subtopic, we generate a detailed description along with its hierarchical relationship to broader disciplines:</p>
<p>Given the subtopic {Subtopic Name}, which belongs to the hierarchy {Primary Discipline} → {Secondary Discipline} → {Research Direction}, describe its relationships with each category.Return the result in the JSON format:</p>
<p>• 3. Generating Ethical Guidelines Next, we derive discipline-specific ethical guidelines aligned with predefined ethical principles: Following the automatic generation of questions, domain experts review and filter them based on relevance, clarity, fairness, and validity.Along this line, we can assess ethical alignment in LLMs.Model performance is measured by accuracy, denoted as S v .</p>
<p>Results</p>
<p>In this section, we present and discuss the detailed assessment results of both scientific data and LLMs in the SciHorizon platform.</p>
<p>Results of Scientific Data Assessment</p>
<p>Given that the concept of AI-ready scientific data is still evolving and subject to varying criteria and disciplines, we focus here on three typical natural science domains, namely Earth, Life, and Materials Sciences.Specifically, based on data resource papers published between 2018 and 2023 in peer-reviewed journals (e.g., Scientific Data and ESSD) from both domains, we selected 133 highly cited and award-winning datasets from an original pool of approximately 1,600 datasets as assessment candidates.</p>
<p>As illustrated in Table 1, the study highlights that reusable scientific data products in the Earth Science field have laid a robust foundation for AI applications.(Due to space limitations, the recommended list of Life and Materials Sciences datasets is provided in the Appendix.)By integrating multi-source data to tackle common scientific challenges, these products generate comprehensive datasets characterized by long-term temporal sequences, extensive spatial coverage, diverse feature elements,  and rich semantic content.The primary data modalities consist of tabular and image data, which exhibit strong compatibility with AI model methodologies.</p>
<p>Overall</p>
<p>For example, the China Meteorological Forcing Dataset (CMFD) performs well in terms of FAIRness assessment.It appropriately utilizes DOI and CSTR, provides machine-readable metadata, ensures open access, and supports metadata harvesting protocols.However, it lacks a data entity link in its metadata.The dataset also demonstrates excellent domain applicability, a quality fully acknowledged by domain experts.Additionally, CMFD offers seven meteorological elements with high resolution and continuous temporal coverage, reflecting strong diversity and balance.The rich feature set and standardized data organization indicate significant potential for future AI applications.</p>
<p>Results of LLM Assessment</p>
<p>In this section, we present a comprehensive evaluation of a diverse range of LLMs for AI4Science, focusing on five core dimensions: Knowledge, Understanding, Reasoning, Multimodality, and Value, along with overall performance.</p>
<p>The evaluation dataset is constructed following the methodology outlined in Section 3.2.We initially gathered a pool of 406,700 questions.To improve evaluation efficiency, we applied expert review and difficulty-based sampling, ultimately selecting 5,234 questions to construct our benchmark dataset.The dataset will be publicly released following the publication of this paper.</p>
<p>We evaluate over 50 LLMs, encompassing both closed-source and open-source LLMs as shown in Table 2.The evaluated models include several state-of-the-art closed-source LLMs such as Gemini-2.5-pro-previewand Gemini-2.5-flash-previewthinking,as well as representative and recently released open-source models like DeepSeek-R1, Qwen3-235B-A22B, and Llama-4-Maverick-17B-128E. Among them, a subset-including Claude-3.5-Sonnet,GLM-4v-Plus, Yi-Vision-V2, and MiniCPM-V-2.6-featuresmultimodal capabilities, enabling joint processing of text and images.</p>
<p>By evaluating a diverse set of LLMs from both closed-and open-source domains, our study comprehensively assesses their scientific capabilities across five dimensions.The overall performance is calculated as the average of the Knowledge, Understanding, Reasoning, and Values dimensions, excluding Multimodality, as some LLMs lack multimodal support.This evaluation offers valuable insights into the strengths and limitations of contemporary LLMs in AI4Science, guiding future research in LLM-driven scientific intelligence.Another open-source model, Qwen3-235B-A22B, also performs well with an overall score of 70.12, surpassing the 70-point benchmark and offering a strong domestic alternative with broad general-purpose scientific capabilities.</p>
<p>Overall Performance</p>
<p>To further analyze model performance across different evaluation dimensions, Figure 2 presents radar charts depicting the capabilities of the top-performing LLMs in five key dimensions: Knowledge, Understanding, Reasoning, Multimodality, and Values along with the overall performance.The visualization highlights how LLMs vary in strengths across disciplines and dimensions, providing deeper insights into their specialization.Gemini-2.5-pro-previewleads overall, with top scores in Knowledge, Understanding, Reasoning, and Multimodality demonstrating strong factual grounding and scientific inference.Gemini-2.5-flash-preview-thinkingperforms competitively, particularly in Earth Sciences, reflecting strengths in geospatial understanding.DeepSeek-R1 excels in Reasoning and Values, indicating robust logical inference and ethical alignment.Claude-3.7-sonnet-thinkingachieves the highest Values score and strong Physics performance, showcasing advantages in visual and symbolic tasks.The results also underscore the necessity of domain-and task-aware model selection: different models excel in different scientific disciplines and dimensions, and matching model strengths to the demands of specific scientific applications is crucial for effective deployment in AI4Science scenarios.</p>
<p>Knowledge Evaluation</p>
<p>We evaluated each LLM's ability to acquire and apply core scientific knowledge across multiple disciplines, as shown in Table S5.Gemini-2.5-pro-previewachieves the highest overall knowledge score (62.79), consistently ranking first in Life Sciences, Chemistry, and Physics, and second in Earth Sciences.Gemini-2.5-flash-preview-thinkingfollows with an overall score of 60.09, excelling in Earth Sciences (66.95) and Mathematics (61.46).DeepSeek-R1 and Qwen3-235B-A22B also demonstrate solid performance, with scores of 57.84 and 57.03, respectively, maintaining competitive results across all subject areas.Notably, DeepSeek-R1-Distill-Qwen-32B achieves the highest score in Mathematics (64.09), indicating strong symbolic reasoning ability.These results highlight the general-purpose knowledge strength of the Gemini-2.5series and the promising performance of Chinese open-source models in domain-specific knowledge retention.</p>
<p>Understanding Evaluation</p>
<p>We evaluated the ability of LLMs to comprehend and interpret scientific facts and concepts across disciplines (Table S7).Gemini-2.5-pro-previewachieves the highest overall score (79.94), with perfect performance in Chemistry (100.00) and strong results in Mathematics (91.75) and Physics (72.63).Gemini-2.5-flash-preview-thinkingclosely follows (78.28), leading in Life Sciences (66.44) and Mathematics (92.00), reflecting robust semantic understanding across multiple fields.Gemini-2.0-Pro,Llama-4-Maverick-17B-128E, and Qwen3-235B-A22B also perform competitively, all achieving scores above 75, with balanced strength.DeepSeek-R1 and O3-Mini demonstrate solid understanding, particularly in Chemistry and Physics, while GPT-4.1 stands out in Physics (74.00) and maintains strong overall comprehension.</p>
<p>Reasoning Evaluation</p>
<p>To assess the scientific reasoning capabilities of LLMs, we evaluate their performance across disciplines in Table S8.Gemini-2.5-pro-previewleads by a significant margin with an overall reasoning score of 92.10, ranking first in all five disciplines, including Life Sciences (87.55), Chemistry (93.55), and Mathematics (92.31), indicating its exceptional capacity for complex scientific inference.Gemini-2.5-flash-previewthinkingfollows with a strong score of 88.64.O3-Mini (88.16) also demonstrate strong and consistent reasoning capabilities, particularly in Physics (91.22) and Earth Sciences (97.87).DeepSeek-R1 (85.89) and Qwen3-235B-A22B (85.35) maintain competitive results across all scientific areas, confirming their robustness in domaingeneral logical reasoning.Notably, the open-source model QwQ-32B-also achieves score above 82, suggesting that strong reasoning performance is attainable even in middle-size LLMs.</p>
<p>Multimodality Evaluation</p>
<p>To assess LLMs' capabilities in understanding and reasoning over scientific charts and multimodal content, we evaluate their performance across disciplines as shown in Table S6.Gemini-2.5-Pro-Previewranks first overall with a multimodality score of 79.85, demonstrating strong generalization across tasks.Llama-4-Maverick-17B-128E, the top-performing open-source model, achieves a competitive score of 65.17.Models such as Claude-3.7-Sonnet-Thinkingand GPT-4.1 show balanced multimodal reasoning with scores above 60, while smaller open models like MiniCPM-V-2.6 and Qwen2-VL-7B-Instruct lag behind.</p>
<p>Values Evaluation</p>
<p>To assess whether LLMs align with ethical guidelines and responsible AI principles, we evaluated their value adherence across disciplines (Table S9).Claude-3.7-sonnetthinkingachieves the highest overall score (69.90), leading in Mathematics (71.05) and Physics (69.67), and performing consistently across all areas.DeepSeek-R1 ranks second (68.27), with a strong performance in Earth Sciences (78.46) and solid scores in other domains, highlighting its balanced value alignment.Other Claude models, including Claude-3.5-Sonnet-20241022and Claude-3.5-Haiku-20241022,also demonstrate strong value-oriented behavior.In contrast, although Gemini-2.5-pro-previewexcels in other capability dimensions, its value score is relatively lower (56.90),indicating room for improvement in ethical robustness.</p>
<p>Conclusion</p>
<p>This study introduced SciHorizon, a comprehensive assessment framework for AI4Science readiness, integrating assessments of AI-ready scientific datasets and LLM capabilities.The framework systematically evaluated scientific datasets across four key dimensions to identify high-quality data for AI-driven research while assessing LLMs across five core indicators spanning major scientific disciplines.Based on data papers from 2018 to 2023, we provided dataset recommendations for Earth, Life, and Materials Sciences.By benchmarking more than 50 representative large models, we gained critical insights into their strengths and limitations.SciHorizon establishes a structured methodology for dataset curation and model assessment, contributing to AI-driven scientific discovery.</p>
<p>A More Details for SciHorizon A.1 Prompts for LLM Assessment</p>
<p>Due to page limitations, we put the specific prompts in the assessment process in the appendix.</p>
<p>A.1.1 Prompt for Factuality Assessment</p>
<p>The following is an example of how a Factuality evaluation question is structured.Given a subject, a question, and a set of options, the question is framed as:</p>
<p>You are about to take a subject test in the form of multiple-choice questions.Please follow the instructions below to complete the test.Now, please output only the letter corresponding to the correct answer.Do not include any additional text or explanations.(e.g., A, B, C. . .).Question: Below is a multiple-choice question.Please select the correct answer from the options provided.Only output the corresponding letter, such as: A. Question: [Insert Question Content] Options: [Insert Option Content] Answer:</p>
<p>A.1.2 Prompt for Robustness Assessment</p>
<p>To ensure the LLM consciously filters out noise, we employ the following prompt for Robustness evaluation:</p>
<p>Your task is to denoise the question, understand its meaning, and provide the correct answer based on the given options.Now, Output only the letter corresponding to the correct answer.Do not include any additional text or explanation.(e.g., "A", "B", "C" ....)</p>
<p>A.1.3 Prompt for Externalization Assessment</p>
<p>We employ the following prompt for Externalization evaluation:</p>
<p>For the given question, identify which option is the most likely cause of the issue.We will think step by step and generate explanations for each option.Treat each option as a premise and the given issue as the conclusion.Generate a concise, step-by-step logical derivation explaining how the premise leads to the conclusion.For each step, provide an "IF-THEN" rule along with relevant causal or commonsense assumptions.After generating explanations, select the most likely cause.Ensure that your response includes the following sections:</p>
<p>• Explanation for Option 1 • Explanation for Option 2 • Final Answer Your final answer should include one or more options as the most probable cause.Incomplete answers will result in point deductions.Below is an example format for reference-please strictly follow this format in your response.</p>
<p>A.1.4 Prompt for Helpfulness Assessment</p>
<p>To evaluate Helpfulness, we employ a two-step assessment procedure.First, we generate domain-specific knowledge using an LLM as the following prompt:</p>
<p>As a domain expert, please provide the necessary knowledge output for the following question.Keep the content concise, no more than three sentences, so that beginners can correctly answer related questions based on this knowledge.</p>
<p>Then, we inject generated knowledge into a low-parameter model, such as LLaMA3-8B [64] and evaluate its ability to utilize the provided information to answer a scientific multiple-choice question:</p>
<p>Given the following retrieved knowledge: [generated knowledge], Please output only the letter corresponding to your answer (e.g., "A", "B",...).[77] 4.40 4.38 4.00 4.00 The SUSTech-SYSU dataset for automatically segmenting and classifying corneal ulcers [87] 4.50 4.12 4.12 4.00 FIVES: A Fundus Image Dataset for Artificial Intelligence based Vessel Segmentation [88] 4.50 3.52 4.75 4.00 miRTarBase:The experimentally validated microRNA-target interactions database [89] 4.50 3.32 4.38 3.75 A multi-modal open dataset for mental-disorder analysis [90] 4.30 3.68 4.00 3.50 An ATAC-seq atlas of chromatin accessibility in mouse tissues [91] 4.50 2.75 4.38 3.75 Single-cell RNA sequencing of human kidney [92] 4.50 3.20 3.25 4.00</p>
<p>A.1.5 Prompt for Deductive Reasoning Assessment</p>
<p>An example of a deductive reasoning task is shown below:</p>
<p>To identify a substance with an asymmetrical molecular structure, the following background knowledge is required: . . .Using this foundation, analyze the molecular structures of the given substances to determine which one is asymmetrical.Question: Which of the following substances has an asymmetrical molecular structure?Answer:</p>
<p>A.2 Sub-dimensions of Values</p>
<p>We provide detailed evaluation dimensions for assessing whether LLMs align with the values that are essential in scientific work.</p>
<p>A.3 Expert Principles for Refining Questions of Value Assessment</p>
<p>In the process of reviewing and filtering questions generated automatically, domain experts assess them based on several ethical principles as follows:</p>
<p>• Relevance -Ensuring realistic and discipline-specific scenarios.</p>
<p>• Clarity -Refining ambiguous or misleading phrasing.</p>
<p>• Fairness -Avoiding biased or culturally sensitive framing.</p>
<p>• Validity -Confirming that questions assess ethical principles rather than factual knowledge.</p>
<p>B More Details for Experimental Results</p>
<p>B.1 More Details for Scientific Data Assessment</p>
<p>We provide the results of data evaluation on Life and Materials Sciences as shown in Table S3 and S4.</p>
<p>B.2 More Details for LLM Assessment</p>
<p>We provide detailed experimental results that offer a comprehensive evaluation of LLMs across various scientific disciplines, as shown in Table S5, Table S7, Table S8, Table S6, and Table S9.</p>
<p>Fig. 1 :
1
Fig. 1: Overview of the SciHorizon platform.</p>
<p>2
2</p>
<p>Fig. 2 :
2
Fig. 2: Radar charts of top LLMs' performance across disciplines.This figure illustrates the top 5 LLMs' capabilities in six evaluation dimensions: Overall, Knowledge, Understanding, Reasoning, Multimodality, and Value.Each chart displays performance across five disciplines: Mathematics, Physics, Chemistry, Life Sciences (Life), and Earth and Space Sciences (Earth).</p>
<p>Table 1 :
1
Performance of evaluated data across Earth Science.The table reports the scores of each data in Quality (Q), FAIRness (F), Explainability (E), and Compliance (C).Assume you are conducting research in {Primary Discipline}, specializing in {Research Direction}.Create a multiple-choice question with a scenario-based setup to assess adherence to the ethical principle: {Target Ethical Principle}.Ensure the scenario includes specific time, location, individuals, and events and return the question in the following format:
Data
Assume you are a researcher in {Discipline}, focusing on {Research Direction}.Identify relevant ethical guidelines and map them to the principles Target Ethical Principle: Definition of Ethical Principle • 4. Generating Scenario-Based Ethical Questions Finally, we construct multiple-choice questions to assess ethical adherence:</p>
<p>Table 2 :
2
Overall performance of evaluated LLMs across multiple scientific disciplines.The overall performance is computed as the average score across the dimensions of Knowledge, Understanding, Reasoning, and Values.The table reports the scores of each model in overall performance and five individual subject areas: Life Sciences (Life), Chemistry, Earth and Space Sciences (Earth), Mathematics, and Physics.
DisciplineOverallLifeChemistry Earth Mathematics PhysicsGemini-2.5-pro-preview72.93 69.2977.2174.1776.1367.84Gemini-2.5-flash-preview-thinking72.2067.6174.1275.4876.0167.79DeepSeek-R171.6865.6174.9675.4072.8569.59O3-Mini70.5166.6374.5571.4571.9867.93Claude-3.7-sonnet-thinking70.4364.4673.9171.7071.9570.14Qwen3-235B-A22B70.1266.1772.4274.0572.1065.86Gemini-2.0-Pro68.0264.7370.3073.7967.9463.32DeepSeek-V367.2965.6867.8773.6066.5762.73Gemini-2.5-flash-preview65.8362.7369.5370.0664.6062.22Claude-3.7-Sonnet65.8360.5871.1771.1860.2166.01Llama-4-Maverick-17B-128E65.1261.5670.0767.5961.8964.53Claude-3.5-Sonnet-2024102265.0160.1373.0571.5157.1063.27Qwen2.5-Max64.6262.4565.0568.2866.0961.24O1-Mini-2024091264.5860.8960.4666.3071.6963.56Gemini-2.0-Flash64.4962.9968.1670.1360.2360.92Qwen3-32B64.2859.7859.1772.1969.4460.84QwQ-32B63.3960.3856.4768.8367.6363.64Gemini-1.5-Pro-Latest63.1360.5266.4566.6961.7960.18Grok-362.7458.5568.9068.2557.4760.51GPT-4o-2024112061.9964.9666.1568.4651.8258.54Mistral-Medium-361.8959.3962.0668.1560.7359.13GPT-4.161.8263.7666.9368.6253.1256.69Llama-4-Scout-17B-16E61.0858.1464.8062.0360.1960.22DeepSeek-R1-Distill-Qwen-32B60.7258.2559.1061.9465.6858.64Qwen-Plus59.8359.2956.2366.2760.5956.80Llama3.1-70B-Instruct59.2061.2761.6567.7349.8855.46Llama3.3-70B-Instruct59.0061.4161.9967.9448.9354.76Qwen2.5-72B-Instruct58.5756.9855.2764.4959.5356.59Yi-Lightning58.1156.8458.1465.9053.1656.53GLM-4-Plus57.7655.0260.0464.4853.8755.39Doubao-Pro-32K57.6956.7858.7162.1056.8354.05Mistral-Small-24B-Instruct-250157.1156.2959.7063.6149.1556.81Spark-v4.0-Ultra56.9757.0057.9663.0952.3154.46Llama-3.2-90B-Vision-Instruct56.6556.0159.3266.8348.6852.44ERNIE-4.0-Turbo-8K-Latest56.1953.4257.4463.6452.2954.14GLM-4v-Plus56.1554.8254.8159.1849.7762.19InternLM3-8B-Instruct54.5855.7154.8959.3048.6554.33InternLM2.5-20B-Chat54.5351.3053.0561.2850.6356.36Claude-3.5-Haiku-2024102254.0448.9559.9761.1947.3352.77Pixtral-Large-Instruct-241154.0052.3254.6063.3947.8451.86Yi-34B-Chat53.4753.4248.9462.3745.6057.00Mistral-Large-Instruct-241152.3652.2852.3460.1544.1652.89Moonshot-V1-32K52.2954.9248.2861.7045.8950.69GPT-4o-Mini52.2454.0149.6062.3945.1150.07MiniCPM3-4B51.3952.4244.2556.9948.7854.50GLM-4-9B-Chat50.9349.8047.9057.3546.1153.50Qwen3-30B-A3B50.7345.7655.1959.3854.1839.15Llama-3.2-11B-Vision-Instruct50.6651.4751.4659.1042.6148.66Qwen2-VL-7B-Instruct49.4548.1546.5257.5742.0852.96Yi-Vision-V248.9147.9649.8556.5844.3845.76Llama3.1-8B-Instruct48.7449.5948.4557.6640.7147.29MiniCPM-V-2.646.5345.5346.5954.0536.8749.63Ministral-8B-Instruct-241046.3448.3646.5952.7037.2646.81ERNIE-4.0-8K-Latest46.2341.4148.0256.1640.8344.74Qwen2.5-7B-Instruct45.7645.1741.2355.7342.6744.00</p>
<p>Table 2
2
presents the overall performance of each model across multiple disciplines.Gemini-2.5-pro-previewranksasthetopmodelwithanoverallscore of 72.93, demonstrating leading performance in Life Sciences (69.29), Chemistry (77.21), and Mathematics (76.13).Gemini-2.5-flash-preview-thinkingfollowscloselywithascore of 72.20, and notably achieves the best result in Earth and Space Sciences (75.48), reflecting strong geospatial understanding capabilities.Claude-3.7-sonnet-thinkingalsoperformscompetitively(70.43),achieving the highest score in Physics(70.14).Among open-source models, the Chinese open-source model DeepSeek-R1 ranks third overall(71.68),showingconsistentand competitive results across disciplines, especially in Chemistry (74.96) and Earth Sciences(75.40),where it secures second place.</p>
<p>Table S3 :
S3
Performance of evaluated data across Life Sciences.The table reports the scores of each data in Quality (Q), FAIRness (F), Explainability (E), and Compliance (C).
DataQFECMedMNIST v2 -A large-scale lightweight benchmark for 2D and 3D biomedical image classification [84]4.90 4.46 4.75 3.50BioWordVec, improving biomedical word embeddings with subword information and MeSH [85]4.50 4.38 4.75 3.50gcType:Type Strains Genome Database [86]4.50 4.38 4.38 4.00A dataset of distribution and diversity of ticks in China</p>
<p>Table S4 :
S4
Performance of evaluated data across Materials Science.The table reports the scores of each data in Quality (Q), FAIRness (F), Explainability (E), and Compliance (C).Compliance with Laws and Regulations: Ensuring that the decisions made are in accordance with relevant legal frameworks, such as data protection laws and intellectual property rights.This includes respecting the boundaries set by laws governing the research and application of AI. • Academic Integrity: Preventing unethical academic behaviors such as plagiarism, data manipulation, or fabrication.This involves making decisions that ensure transparency, honesty, and fairness in research, safeguarding the credibility of scientific work.• Adherence to Objective Truth: Prioritizing the use of accurate, unbiased data and scientific facts in decision-making.This value emphasizes the importance of avoiding distortion or misinformation in research findings, ensuring that only truthful and reliable data are used.• Respect for Humanity and Nature: Making decisions that respect human dignity, fundamental rights, and environmental sustainability.This value highlights the need to balance human welfare and ecological impact when making choices that affect society and the environment.• Maintenance of Fairness and Justice: Ensuring that decisions do not perpetuate biases related to gender, race, culture, or other demographic factors.Fairness and justice require that all individuals are treated equitably, and that decisions do not exacerbate existing societal inequalities.• Attention to Privacy and Security: Protecting personal data and ensuring that decisions respect privacy laws.This value underscores the importance of maintaining data security and preventing unauthorized access, breaches, or misuse of sensitive information.• Ensuring Transparency and Explainability: Making decisions that are transparent and explainable, with clear justifications that are accessible and understandable to users.Transparency and explainability ensure that the decision-making process is auditable and accountable, providing insight into how and why specific outcomes were reached.
DataQFECA corpus of CO2 Electrocatalytic Reduction Process extracted from the scientific litera-ture [93]4.20 4.91 3.62 5.00Fatigue Database of Additively Manufactured Alloys [94]4.10 4.38 4.00 4.00A materials terminology knowledge graph automatically constructed from text corpus [95]4.20 4.38 4.00 3.75Materials informatics platform with three dimensional structures (MIP-3d) [96]4.29 4.25 3.75 4.00QM-symex-database [97]4.30 4.25 3.50 4.00High Dielectric Ternary Oxides from Crystal Structure Prediction and High-throughput Screening [98]4.00 4.38 3.62 4.00QCDGE database: Quantum Chemistry Database with Ground-and Excited-State Proper-ties [99]4.20 3.73 3.62 4.25Error assessment and optimal cross-validation approaches in machine learning applied to impurity diffusion [100]4.15 4.21 4.38 3.00Database of Open-Framework Aluminophosphate Structures [101]4.10 4.38 3.25 4.00A machine learning-based alloy design system to facilitate the rational design of high entropy alloys with enhanced hardness [102]4.15 4.21 4.31 3.00•</p>
<p>Table S5 :
S5
Performance of LLMs on Knowledge assessment across multiple scientific disciplines.
DisciplineOverallLifeChemistry Earth Mathematics PhysicsGemini-2.5-pro-preview62.79 66.3260.0164.2961.1062.20Gemini-2.5-flash-preview-thinking60.0960.6654.5066.9561.4656.87DeepSeek-R157.8461.1655.9758.8059.8053.48Qwen3-235B-A22B57.0359.3454.5360.9560.4549.86Gemini-2.0-Pro56.8361.4954.8060.5854.7052.58O3-Mini54.9056.4154.3356.7857.4649.51Qwen2.5-Max54.6859.0851.5058.9552.8651.03Claude-3.7-sonnet-thinking54.3060.0046.6558.7856.1849.89DeepSeek-R1-Distill-Qwen-32B54.0960.1845.9650.8164.0949.41Gemini-2.0-Flash53.2960.5048.2958.0648.2551.35DeepSeek-V353.1758.6146.3654.9258.3247.64QwQ-32B52.2056.8744.5857.8752.8248.87Claude-3.7-Sonnet51.6757.7044.4460.3348.2347.66Qwen3-32B51.5952.2947.6357.0054.5646.44Qwen2.5-72B-Instruct51.0157.8747.9153.5151.1644.62Qwen-Plus50.9356.1148.2252.0351.6546.65Doubao-Pro-32K50.7356.9046.6950.6350.8448.58Claude-3.5-Sonnet-2024102250.5855.1745.8461.4344.4746.01Grok-349.6154.5047.4855.0944.8546.12Gemini-2.5-flash-preview49.5853.5942.4957.2748.4646.07InternLM2.5-20B-Chat49.1058.2345.5049.9448.0743.74Qwen2-VL-7B-Instruct48.0155.7648.8552.9739.1643.30Yi-34B-Chat47.3354.3644.2849.2742.3546.38MiniCPM3-4B47.3254.0846.5248.2743.8643.89ERNIE-4.0-Turbo-8K-Latest47.2554.6042.1047.4246.8145.34GLM-4-Plus46.9253.6846.8050.2538.9644.91Qwen3-30B-A3B46.2451.5743.8749.0638.4748.25Llama3.1-70B-Instruct46.2356.0244.7745.1743.0942.13GLM-4v-Plus45.8351.8940.1251.1042.0943.97Llama3.3-70B-Instruct45.4954.3245.5145.3340.3441.94O1-Mini-2024091245.0748.8938.8448.8049.6739.14Llama-3.2-90B-Vision-Instruct44.7954.1539.5748.7440.2041.28GPT-4.144.5551.3740.4451.9940.3638.59Llama-4-Maverick-17B-128E44.3252.1143.8546.0737.1542.44Yi-Lightning44.2952.3341.4448.7537.0541.89ERNIE-4.0-8K-Latest44.2250.2345.2547.7339.7538.15Spark-v4.0-Ultra44.1850.2247.3246.3937.4839.51GPT-4o-2024112043.0153.4736.0947.7036.3341.46Mistral-Medium-342.9449.2635.7448.0341.3840.28InternLM3-8B-Instruct42.3350.9743.9744.0535.1237.57Mistral-Small-24B-Instruct-250141.9147.6242.9144.4135.6838.94Moonshot-V1-32K41.6149.8637.2646.2938.9135.72GLM-4-9B-Chat41.4645.3741.2842.7338.5439.37Gemini-1.5-Pro-Latest40.3046.2537.2839.7139.6538.62Qwen2.5-7B-Instruct39.5340.5943.6342.9341.5828.92Yi-Vision-V239.4346.3240.5741.1535.0534.08Llama-4-Scout-17B-16E39.3044.5436.6141.3637.8336.16MiniCPM-V-2.638.0844.3742.4144.1425.7733.73Ministral-8B-Instruct-241037.3841.2937.1637.2034.9336.32Pixtral-Large-Instruct-241136.9243.9733.1140.6032.4434.46Mistral-Large-Instruct-241135.9843.1336.5939.0428.9932.13GPT-4o-Mini35.3843.0433.2739.6133.5027.50Llama-3.2-11B-Vision-Instruct34.7043.3836.0037.6428.9027.57Llama3.1-8B-Instruct33.6937.9034.6337.2428.1030.57Claude-3.5-Haiku-2024102232.5231.2929.5240.9534.2726.56</p>
<p>Table S6 :
S6
Performance of LLMs on Multimodality assessment across multiple scientific disciplines.
DisciplineOverallLifeChemistry Earth Mathematics PhysicsGemini-2.5-pro-preview79.85 90.6976.6368.0097.0066.94Gemini-2.0-Pro68.0367.4073.2263.0579.7556.72Llama-4-Maverick-17B-128E65.1059.0758.7161.8075.5070.43Claude-3.7-sonnet-thinking62.3152.9467.7667.8670.5052.51Gemini-2.0-Flash61.2158.0956.4254.6579.4257.45GPT-4.160.4961.2763.9465.5971.7539.88Claude-3.5-Sonnet-2024102258.9360.0061.9369.5555.9547.22Gemini-1.5-Pro-Latest56.8860.0548.8765.3364.2545.88GLM-4v-Plus54.0950.0048.7157.2067.5047.05Claude-3.7-Sonnet52.0753.3648.2767.9339.3751.43Qwen2.5-Max51.6360.3050.0652.6648.1646.95Llama-4-Scout-17B-16E51.5148.7745.9360.4446.2556.14Llama-3.2-90B-Vision-Instruct51.4057.6054.4755.0356.5033.43GPT-4o-2024112050.1945.1037.9651.4770.0046.41MiniCPM-V-2.648.7248.0442.8044.5559.7548.44Claude-3.5-Haiku-2024102246.0138.8948.8661.4542.5338.30Qwen2-VL-7B-Instruct40.7634.0750.9548.2033.7536.84Llama-3.2-11B-Vision-Instruct40.7246.0838.6936.9547.7534.12GPT-4o-Mini37.4437.2637.2935.8644.0032.79Yi-Vision-V235.8919.6131.4137.3962.7528.32</p>
<p>Table S7 :
S7
Performance of LLMs on Understanding assessment across multiple scientific disciplines.
DisciplineOverallLifeChemistry Earth Mathematics PhysicsGemini-2.5-pro-preview79.9465.52100.0069.7991.7572.63Gemini-2.5-flash-preview-thinking78.2866.4494.9567.0092.0071.00Gemini-2.0-Pro77.6060.2798.0067.0090.0072.73Llama-4-Maverick-17B-128E77.0165.0794.0067.0085.0074.00Qwen3-235B-A22B75.1259.5992.0066.0087.0071.00DeepSeek-R174.7259.5996.0067.0081.0070.00O3-Mini74.0864.3897.0063.0079.0067.00GPT-4.173.7564.3893.0063.6473.7474.00DeepSeek-V373.4864.3885.0072.0082.0064.00Gemini-2.5-flash-preview73.1055.4896.0060.0085.0069.00Claude-3.7-sonnet-thinking71.1951.3797.0059.6078.0070.00Grok-370.5157.5396.0061.0074.0064.00GPT-4o-2024112070.2762.3393.0059.0068.0069.00Gemini-2.0-Flash69.2252.7493.9458.0075.7665.66Mistral-Medium-368.9858.9080.0058.0080.0068.00Claude-3.7-Sonnet68.7348.6396.0058.0071.0070.00Qwen2.5-Max67.9254.7983.0059.0079.8063.00Claude-3.5-Sonnet-2024102266.9246.5895.0055.0067.0071.00Gemini-1.5-Pro-Latest66.8152.0577.0059.0081.0065.00Llama-4-Scout-17B-16E66.7960.9686.0053.0070.0064.00QwQ-32B64.9756.8559.0055.0084.0070.00O1-Mini-2024091264.6458.2265.0049.0086.0065.00Qwen3-32B64.0751.3756.0064.0080.0069.00Qwen2.5-72B-Instruct61.6050.0059.0051.0083.0065.00Yi-Lightning61.2050.0066.0055.0068.0067.00Llama3.1-70B-Instruct60.5055.4874.0061.0049.0063.00Qwen-Plus60.3348.6359.0057.0078.0059.00Llama3.3-70B-Instruct60.1856.1674.0062.2448.4860.00Pixtral-Large-Instruct-241158.9539.7363.0054.0067.0071.00GLM-4-Plus56.8241.1064.0050.0067.0062.00DeepSeek-R1-Distill-Qwen-32B56.7242.8663.3954.0067.3656.00ERNIE-4.0-Turbo-8K-Latest54.9641.7863.0048.0063.0059.00Spark-v4.0-Ultra54.7144.6053.1954.1763.2758.33Doubao-Pro-32K54.3845.8958.0042.0075.0051.00Mistral-Small-24B-Instruct-250153.5545.7062.0045.0753.0062.00Claude-3.5-Haiku-2024102253.1630.8274.0048.0054.0059.00Llama-3.2-90B-Vision-Instruct51.0041.7862.2453.0040.0058.00GPT-4o-Mini50.7947.9548.0053.0048.0057.00InternLM3-8B-Instruct49.1539.7345.0047.0060.0054.00GLM-4v-Plus49.0139.0442.0042.0053.0069.00Mistral-Large-Instruct-241147.5539.7347.0038.0048.0065.00Llama-3.2-11B-Vision-Instruct47.0643.1550.7547.0042.4252.00InternLM2.5-20B-Chat46.8636.3042.0047.0054.0055.00Moonshot-V1-32K45.7136.9932.0054.5552.0053.00Qwen3-30B-A3B44.5923.9753.0045.0075.0026.00Yi-34B-Chat44.4636.3031.0042.0053.0060.00Llama3.1-8B-Instruct43.7845.8939.0046.0038.0050.00MiniCPM3-4B42.4139.0417.0042.0053.0061.00GLM-4-9B-Chat41.7235.6225.0044.0046.0058.00ERNIE-4.0-8K-Latest41.3821.9243.0044.0050.0048.00MiniCPM-V-2.641.2139.0424.0045.0045.0053.00Ministral-8B-Instruct-241040.8942.4731.0043.0041.0047.00Yi-Vision-V240.2636.3036.0040.0045.0044.00Qwen2-VL-7B-Instruct38.1934.9315.0036.0055.0050.00Qwen2.5-7B-Instruct32.7630.825.0032.0052.0044.00</p>
<p>Table S8 :
S8
Performance of LLMs on Reasoning assessment across multiple scientific disciplines.
DisciplineOverallLifeChemistry Earth Mathematics PhysicsGemini-2.5-pro-preview92.10 87.5593.5596.2392.3190.84Gemini-2.5-flash-preview-thinking88.6482.7883.4197.8789.7389.42O3-Mini88.1683.0780.6898.4187.4491.22Claude-3.7-sonnet-thinking86.3380.5079.4598.1482.5891.00DeepSeek-R185.8980.2978.3397.3487.4486.06Qwen3-235B-A22B85.3582.7379.1697.6182.0385.24O1-Mini-2024091285.2580.5077.6595.9787.0985.05Gemini-2.0-Pro82.4081.6476.7498.1473.6581.84QwQ-32B82.3276.2876.7497.0883.4278.09Llama-4-Maverick-17B-128E81.8681.3679.8593.2572.9581.87Qwen3-32B80.6375.4371.2198.1481.6976.69Qwen2.5-Max80.4677.9378.8692.7369.4583.36Llama-4-Scout-17B-16E80.1476.2874.9290.2978.9780.25DeepSeek-V379.6678.8679.6292.4664.9282.45Gemini-1.5-Pro-Latest79.0777.7282.8892.9962.6879.07Gemini-2.5-flash-preview78.6481.6477.9594.6464.2774.69Mistral-Medium-376.3973.4373.7193.2563.7377.85Claude-3.7-Sonnet76.2774.0774.9294.9054.4483.02Gemini-2.0-Flash75.1075.1969.9794.9059.0576.40Claude-3.5-Sonnet-2024102274.8974.6578.3395.1753.2073.09GPT-4.174.3982.3377.7297.8643.6770.38</p>
<p>Table S9 :
S9
Performance of LLMs on Values assessment across multiple scientific disciplines.
DisciplineOverallLifeChemistry Earth Mathematics PhysicsClaude-3.7-sonnet-thinking69.9065.9972.5370.2971.0569.67DeepSeek-R168.2761.3969.5478.4663.1668.81Claude-3.5-Sonnet-2024102267.6764.1273.0474.4663.7562.99Claude-3.7-Sonnet66.6561.9069.3271.5067.1763.37Gemini-1.5-Pro-Latest66.3366.0768.6475.0463.8558.03Mistral-Small-24B-Instruct-250166.1463.0164.6376.9662.6563.45Claude-3.5-Haiku-2024102266.0362.3366.8369.5263.6867.78O3-Mini64.9062.6766.1967.6364.0264.00GLM-4v-Plus64.0361.4864.5666.0164.5463.57O1-Mini-2024091263.3655.9560.3671.4464.0265.04Qwen3-235B-A22B62.9863.0163.9971.6458.9357.32DeepSeek-V362.8660.8960.5075.0261.0656.83DeepSeek-R1-Distill-Qwen-32B62.7759.0163.3168.8061.8960.83GPT-4o-2024112062.3163.2761.7070.0761.9254.59InternLM3-8B-Instruct62.0359.3559.5468.5958.4264.26Gemini-2.5-flash-preview62.0160.2161.6968.3560.6959.12Gemini-2.5-flash-preview-thinking61.8060.5463.6470.1060.8553.88Llama-3.2-90B-Vision-Instruct61.7158.3364.8575.0263.4846.85Spark-v4.0-Ultra61.6559.8863.8865.2059.4559.84Qwen3-32B60.8560.0361.8469.6361.5151.21Yi-Lightning60.7455.0261.5870.3761.6255.10Gemini-2.0-Flash60.3363.5260.4369.5857.8650.28Yi-34B-Chat60.1457.8259.6665.9859.1258.13GLM-4-9B-Chat59.3756.6357.6868.8756.7856.91Mistral-Large-Instruct-241159.2758.8455.3869.7160.7051.73Mistral-Medium-359.2555.9558.7873.3057.8350.41Llama3.1-70B-Instruct59.0057.5756.2474.4760.4846.24Llama3.3-70B-Instruct58.9157.2356.2473.9060.4846.69Qwen3-30B-A3B58.4959.3561.8466.6651.4053.20Llama-3.2-11B-Vision-Instruct58.2957.4055.0067.5755.3856.10Llama-4-Scout-17B-16E58.0850.7761.6863.4753.9860.49GLM-4-Plus58.0656.3858.0564.9263.6047.35GPT-4o-Mini57.5852.2151.0770.7063.8650.08Llama-4-Maverick-17B-128E57.3147.7062.5664.0252.4459.81InternLM2.5-20B-Chat57.2249.4058.3567.3348.1962.83Yi-Vision-V257.1253.2361.3766.9954.1349.86Grok-357.0649.2457.4864.9852.1761.44Gemini-2.5-pro-preview56.9057.7955.2666.3859.3745.70Llama3.1-8B-Instruct56.8855.7854.4065.6753.2555.29Qwen2-VL-7B-Instruct56.1452.1258.2963.7046.7559.82Moonshot-V1-32K56.0960.1253.1865.6451.5949.95Qwen-Plus55.9060.7145.8068.4556.8147.71ERNIE-4.0-Turbo-8K-Latest55.5351.4454.8367.4157.8246.14Qwen2.5-Max55.4257.9946.8562.4462.2547.59MiniCPM3-4B55.3855.8752.9660.6253.8753.60Gemini-2.0-Pro55.2355.5351.6569.4653.3946.13Pixtral-Large-Instruct-241155.0755.1952.8464.8456.4846.01Doubao-Pro-32K54.8657.0654.9968.1349.7844.35GPT-4.154.6056.9856.5561.0054.7043.77QwQ-32B54.0751.5345.5465.3850.2657.62Qwen2.5-7B-Instruct53.0855.6148.3366.0445.4749.95MiniCPM-V-2.652.3145.4959.4259.1744.2453.24Qwen2.5-72B-Instruct49.5550.0041.3861.8050.7043.88Ministral-8B-Instruct-241046.7946.1748.7451.3141.1546.59ERNIE-4.0-8K-Latest41.5737.8443.9851.1840.5534.31
AcknowledgmentsWe sincerely appreciate the contributions of the following individuals for their support in various aspects of this project, including the collection, processing, and evaluation of scientific data and LLM assessment questions, platform development, and overall project guidance: Wenjuan Cui, Hao Dong, Yi Du, Xuejun Guo, Tianqi He, Shasha Hu, Xiaoyan Hu, Xiaohan Huang, Jinpeng Li, Guojiao Lin, Feng Liu, Jia Liu, Tianhui Ma, Zhiyuan Ning, Siyu Pu, Zhihong Shen, Degang Sun, Lijuan Wang, Pengyao Wang, Shu Wang, Zhenxing Wang, Wenxi Xu, Ran Yang, Peng Yu, Ran Zhang, Weiliang Zhang, Chenyang Zhao, Qi Zhou, and Zhiming Zou.
Aftershock detection with multi-scale description based neural network. Q Zhang, T Xu, H Zhu, L Zhang, H Xiong, E Chen, Q Liu, Ieee International Conference on Data Mining (icdm). IEEE2019. 2019</p>
<p>Rapid learning of earthquake felt area and intensity distribution with real-time search engine queries. H Zhu, Y Sun, W Zhao, F Zhuang, B Wang, H Xiong, Scientific reports. 10154372020</p>
<p>Exploiting real-time search engine queries for earthquake detection: A summary of results. Q Zhang, H Zhu, Q Liu, E Chen, H Xiong, ACM Transactions on Information Systems (TOIS). 3932021</p>
<p>A machine learning-enhanced robust p-phase picker for real-time seismic monitoring. D Shen, Q Zhang, T Xu, H Zhu, W Zhao, Z Yin, P Zhou, L Fang, E Chen, H Xiong, SCIENCE CHINA: Information Sciences. 5159122021</p>
<p>Scalable intermediate-term earthquake forecasting with multimodal fusion neural networks. Y Hu, Q Zhang, H Zhu, B Wang, H Xiong, H Wang, Scientific Reports. 15197482025</p>
<p>Y Ji, Y Sun, Y Zhang, Z Wang, Y Zhuang, Z Gong, D Shen, C Qin, H Zhu, H Xiong, arXiv:2501.15638A comprehensive survey on self-interpretable neural networks. 2025arXiv preprint</p>
<p>Unifying knowledge from diverse datasets to enhance spatial-temporal modeling: A granularity-adaptive geographical embedding approach. Z Wang, Y Sun, H Zhu, Proceedings of the 42nd International Conference on Machine Learning. the 42nd International Conference on Machine Learning2025</p>
<p>Highly accurate protein structure prediction with alphafold. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A Žídek, A Potapenko, nature. 59678732021</p>
<p>The role of chatgpt in data science: how ai-assisted conversational interfaces are revolutionizing the field. H Hassani, E S Silva, Big data and cognitive computing. 72622023</p>
<p>Exploring the use of artificial intelligence for qualitative data analysis: The case of chatgpt. D L Morgan, International journal of qualitative methods. 22160940692312112482023</p>
<p>Cotr: Efficient job task recognition for occupational information systems with class-incremental learning. C Qin, C Fang, K Yao, X Chen, F Zhuang, H Zhu, ACM Transactions on Management Information Systems. 2025</p>
<p>Graph reasoning enhanced language models for text-to-sql. Z Gong, Y Sun, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>Overview of early chatgpt's presence in medical literature: insights from a hybrid literature review by chatgpt and human experts. O Temsah, S A Khan, Y Chaiah, A Senjab, K Alhasan, A Jamal, F Aljamaan, K H Malki, R Halwani, J A Al-Tawfiq, Cureus. 1542023</p>
<p>Y Zhou, H Liu, T Srivastava, H Mei, C Tan, arXiv:2404.04326Hypothesis generation with large language models. 2024arXiv preprint</p>
<p>Can chatgpt be used to generate scientific hypotheses. Y J Park, D Kaplan, Z Ren, C.-W Hsu, C Li, H Xu, S Li, J Li, Journal of Materiomics. 1032024</p>
<p>Mathematical capabilities of chatgpt. S Frieder, L Pinchetti, R.-R Griffiths, T Salvatori, T Lukasiewicz, P Petersen, J Berner, Advances in neural information processing systems. 362023</p>
<p>What can large language models do in chemistry? a comprehensive benchmark on eight tasks. T Guo, B Nan, Z Liang, Z Guo, N Chawla, O Wiest, X Zhang, Advances in Neural Information Processing Systems. 362023</p>
<p>Automating large-scale data quality verification. S Schelter, D Lange, P Schmidt, M Celikel, F Biessmann, A Grafberger, Proceedings of the VLDB Endowment. 11122018</p>
<p>Assessing data quality and the variability of source data verification auditing methods in clinical research settings. L Houston, Y Probst, A Martin, Journal of biomedical informatics. 832018</p>
<p>Data consistency theory and case study for scientific big data. P Shi, Y Cui, K Xu, M Zhang, L Ding, Information. 1041372019</p>
<p>Cleanml: A study for evaluating the impact of data cleaning on ml classification tasks. P Li, X Rao, J Blase, Y Zhang, X Chu, C Zhang, 2021 IEEE 37th International Conference on Data Engineering (ICDE). IEEE2021</p>
<p>Fairshake: Toolkit to evaluate the fairness of research digital resources. D J Clarke, L Wang, A Jones, M L Wojciechowicz, D Torre, K M Jagodnik, S L Jenkins, P Mcquilton, Z Flamholz, M C Silverstein, Cell systems. 952019</p>
<p>The dataset nutrition label. S Holland, A Hosny, S Newman, J Joseph, K Chmielinski, Data Protection and Privacy. 121212020</p>
<p>Is entropy enough for measuring privacy?. S Arca, R Hewett, 2020 International Conference on Computational Science and Computational Intelligence (CSCI). IEEE2020</p>
<p>The privacy onion effect: Memorization is relative. N Carlini, M Jagielski, C Zhang, N Papernot, A Terzis, F Tramer, Advances in Neural Information Processing Systems. 352022</p>
<p>A survey on evaluation of large language models. Y Chang, X Wang, J Wang, Y Wu, L Yang, K Zhu, H Chen, X Yi, C Wang, Y Wang, ACM Transactions on Intelligent Systems and Technology. 1532024</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Large language models encode clinical knowledge. K Singhal, S Azizi, T Tu, S S Mahdavi, J Wei, H W Chung, N Scales, A Tanwani, H Cole-Lewis, S Pfohl, Nature. 62079722023</p>
<p>Mausam: Have LLMs advanced enough? a challenging problem solving benchmark for large language models. D Arora, H Singh, 10.18653/v1/2023.emnlp-main.468Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. H Bouamor, J Pino, K Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>X Wang, Z Hu, P Lu, Y Zhu, J Zhang, S Subramaniam, A R Loomba, S Zhang, Y Sun, W Wang, arXiv:2307.10635Scibench: Evaluating college-level scientific problem-solving abilities of large language models. 2023arXiv preprint</p>
<p>SciHorizon Platform Officially Launched, Establishing a New Evaluation System for AI4Science!. 2025</p>
<p>Data Readiness for AI: A 360-Degree Survey. K Hiniduma, S Byna, J L Bez, 2024</p>
<p>Ai data readiness inspector (aidrin) for quantitative assessment of data readiness for ai. K Hiniduma, S Byna, J L Bez, R Madduri, 10.1145/3676288.3676296Proceedings of the 36th International Conference on Scientific and Statistical Database Management. SSDBM '24. Association for Computing Machinery. the 36th International Conference on Scientific and Statistical Database Management. SSDBM '24. Association for Computing MachineryNew York, NY, USA2024</p>
<p>The fair guiding principles for scientific data management and stewardship. M Wilkinson, M Dumontier, I J Aalbersberg, G Appleton, M Axton, A Baak, N Blomberg, J.-W Boiten, L O Silva Santos, P Bourne, J Bouwman, A Brookes, T Clark, M Crosas, I Dillo, O Dumon, S Edmunds, C Evelo, R Finkers, B Mons, 10.1038/sdata.2016.18Scientific Data. 32016</p>
<p>Fair for ai: An interdisciplinary and international community building perspective. E Huerta, B Blaiszik, L Brinson, K Bouchard, D Diaz, C Doglioni, J Duarte, M Emani, I Foster, G Fox, P Harris, L Heinrich, S Jha, D Katz, V Kindratenko, C Kirkpatrick, K Lassila-Perini, R Madduri, M Neubauer, R Zhu, 10.1038/s41597-023-02298-6Scientific Data. 102023</p>
<p>A roadmap to artificial intelligence (ai): Methods for designing and building ai ready data for womens health studies. medRxiv : the preprint server for health sciences. F Khan, R Wang, M Skanderson, C Brandt, S Fodeh, J Womack, 10.1101/2023.05.25.232903992023</p>
<p>E D R Cluster, 10.6084/m9.figshare.19983722.v1Checklist to Examine AI-readiness for Open Environmental Datasets. 2022</p>
<p>Ai-readiness for biomedical data: Bridge2ai recommendations. T Clark, H Caufield, J A Mohan, S Al Manir, E Amorim, J Eddy, N Gim, B Gow, W Goar, M Haendel, J N Hansen, N Harris, H Hermjakob, M Joachimiak, G Jordan, I.-H Lee, S K Mcweeney, C Nebeker, M Nikolov, J Shaffer, N Sheffield, G Sheynkman, J Stevenson, J Y Chen, C Mungall, A Wagner, S W Kong, S S Ghosh, B Patel, A Williams, M C Munoz-Torres, 10.1101/2024.10.23.619844bioRxiv. 2024</p>
<p>A survey on large language models for recommendation. L Wu, Z Zheng, Z Qiu, H Wang, H Gu, T Shen, C Qin, C Zhu, H Zhu, Q Liu, World Wide Web. 276602024</p>
<p>From missteps to mastery: Enhancing low-resource dense retrieval through adaptive query generation. Z Tong, C Qin, C Fang, K Yao, X Chen, J Zhang, C Zhu, H Zhu, Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 1. the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 12025</p>
<p>Enhancing question answering for enterprise knowledge bases using large language models. F Jiang, C Qin, K Yao, C Fang, F Zhuang, H Zhu, H Xiong, International Conference on Database Systems for Advanced Applications. Springer2024</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, International Conference on Learning Representations. 2021</p>
<p>TheoremQA: A theorem-driven question answering dataset. W Chen, M Yin, M Ku, P Lu, Y Wan, X Ma, J Xu, X Wang, T Xia, 10.18653/v1/2023.emnlp-main.489Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. H Bouamor, J Pino, K Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. P Lu, H Bansal, T Xia, J Liu, C Li, H Hajishirzi, H Cheng, K.-W Chang, M Galley, J Gao, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. P Lu, S Mishra, T Xia, L Qiu, K.-W Chang, S.-C Zhu, O Tafjord, P Clark, A Kalyan, Advances in Neural Information Processing Systems. 352022</p>
<p>Scieval: A multi-level large language model evaluation benchmark for scientific research. L Sun, Y Han, Z Zhao, D Ma, Z Shen, B Chen, L Chen, K Yu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. B.-B , Transactions on Machine Learning Research. 2023</p>
<p>Mathematical capabilities of chatgpt. S Frieder, L Pinchetti, R.-R Griffiths, T Salvatori, T Lukasiewicz, P Petersen, J Berner, Advances in neural information processing systems. 362024</p>
<p>LogicAsker: Evaluating and improving the logical reasoning ability of large language models. Y Wan, W Wang, Y Yang, Y Yuan, J.-T Huang, P He, W Jiao, M Lyu, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>D Zhang, Z Hu, S Zhoubian, Z Du, K Yang, Z Wang, Y Yue, Y Dong, J Tang, SciGLM: Training Scientific Language Models with Self-Reflective Instruction Annotation and Tuning. 2024</p>
<p>Finding people with emotional distress in online social media: A design combining machine learning and rule-based classification. M Chau, T M Li, P W Wong, J J Xu, P S Yip, H Chen, MIS quarterly. 4422020</p>
<p>Computer Network Information Center of the Chinese Academy of Sciences: Scientific Data Trusted Certification. 2024</p>
<p>Felm: Benchmarking factuality evaluation of large language models. Y Zhao, J Zhang, I Chern, S Gao, P Liu, J He, Advances in Neural Information Processing Systems. 362024</p>
<p>AGIEval: A human-centric benchmark for evaluating foundation models. W Zhong, R Cui, Y Guo, Y Liang, S Lu, Y Wang, A Saied, W Chen, N Duan, 10.18653/v1/2024.findings-naacl.149Findings of the Association for Computational Linguistics: NAACL 2024. K Duh, H Gomez, S Bethard, Mexico City; MexicoAssociation for Computational Linguistics2024</p>
<p>C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Y Huang, Y Bai, Z Zhu, J Zhang, J Zhang, T Su, J Liu, C Lv, Y Zhang, Y Fu, Advances in Neural Information Processing Systems. 362023</p>
<p>CMB: A comprehensive medical benchmark in Chinese. X Wang, G Chen, S Dingjie, Z Zhiyi, Z Chen, Q Xiao, J Chen, F Jiang, J Li, X Wan, B Wang, H Li, 10.18653/v1/2024.naacl-long.343Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. K Duh, H Gomez, S Bethard, the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City; MexicoAssociation for Computational Linguistics20241</p>
<p>CMMLU: Measuring massive multitask language understanding in Chinese. H Li, Y Zhang, F Koto, Y Yang, H Zhao, Y Gong, N Duan, T Baldwin, Findings of the Association for Computational Linguistics ACL 2024. L.-W Ku, A Martins, V Srikumar, Bangkok, ThailandAssociation for Computational Linguistics2024and virtual meeting</p>
<p>Xiezhi: An ever-updating benchmark for holistic domain knowledge evaluation. Z Gu, X Zhu, H Ye, L Zhang, J Wang, Y Zhu, S Jiang, Z Xiong, Z Li, W Wu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Bb-geogpt: A framework for learning a large language model for geographic information science. Y Zhang, Z Wang, Z He, J Li, G Mai, J Lin, C Wei, W Yu, Information Processing &amp; Management. 6151038082024</p>
<p>Replication: Towards a publicly available internet scale ip geolocation dataset. O Darwich, H Rimlinger, M Dreyfus, M Gouel, K Vermeulen, Proceedings of the 2023 ACM on Internet Measurement Conference. the 2023 ACM on Internet Measurement Conference2023</p>
<p>Inference to the best explanation in large language models. D Dalal, M Valentino, A Freitas, P Buitelaar, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. L.-W Ku, A Martins, V Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Beyond factuality: A comprehensive evaluation of large language models as knowledge generators. L Chen, Y Deng, Y Bian, Z Qin, B Wu, T.-S Chua, K.-F Wong, 10.18653/v1/2023.emnlp-main.390Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. H Bouamor, J Pino, K Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. S Black, L Gao, P Wang, C Leahy, S Biderman, 10.5281/zenodo.52977152021If you use this software, please cite it using these metadata</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Medmcqa: A large-scale multisubject multi-choice dataset for medical domain question answering. A Pal, L K Umapathi, M Sankarasubbu, Conference on Health, Inference, and Learning. PMLR2022</p>
<p>What disease does this patient have? a large-scale open domain question answering dataset from medical exams. D Jin, E Pan, N Oufattole, W.-H Weng, H Fang, P Szolovits, Applied Sciences. 111464212021</p>
<p>D M Levine, R Tuwani, B Kompa, A Varma, S G Finlayson, A Mehrotra, A Beam, The diagnostic and triage accuracy of the gpt-3 artificial intelligence model. 2023</p>
<p>Text2Mol: Cross-modal molecule retrieval with natural language queries. C Edwards, C Zhai, H Ji, 10.18653/v1/2021.emnlp-main.47Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. M.-F Moens, X Huang, L Specia, S W Yih, -T, the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational Linguistics2021Online and Punta Cana</p>
<p>Geo-bench: Toward foundation models for earth monitoring. A Lacoste, N Lehmann, P Rodriguez, E Sherwin, H Kerner, B Lütjens, J Irvin, D Dao, H Alemohammad, A Drouin, Advances in Neural Information Processing Systems. 362023</p>
<p>MathBench: Evaluating the theory and application proficiency of LLMs with a hierarchical mathematics benchmark. H Liu, Z Zheng, Y Qiao, H Duan, Z Fei, F Zhou, W Zhang, S Zhang, D Lin, K Chen, Findings of the Association for Computational Linguistics ACL 2024. L.-W Ku, A Martins, V Srikumar, Bangkok, ThailandAssociation for Computational Linguistics2024and virtual meeting</p>
<p>GPQA: A graduate-level google-proof q&amp;a benchmark. D Rein, B L Hou, A C Stickland, J Petty, R Y Pang, J Dirani, J Michael, S R Bowman, First Conference on Language Modeling. 2024</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. W Ling, D Yogatama, C Dyer, P Blunsom, 10.18653/v1/P17-1015Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. R Barzilay, M.-Y Kan, the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>MathQA: Towards interpretable math word problem solving with operationbased formalisms. A Amini, S Gabriel, S Lin, R Koncel-Kedziorski, Y Choi, H Hajishirzi, 10.18653/v1/N19-1245Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. J Burstein, C Doran, T Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>C Yingying, Y Kun, T Wenjun, L Xin, L Hui, H Jie, Q Jun, 10.11888/AtmosphericPhysics.tpe.249369.fileChina meteorological forcing dataset. 1979-2018. 2015National Tibetan Plateau Data Center</p>
<p>Bias-corrected CMIP6 Global Dataset for Dynamical Downscaling of the Earth's Historical and Future Climate. Z Xu, Y Han, C.-Y Tam, Z.-L Yang, C Fu, 10.11922/sciencedb.00487Science Data Bank. 1979-2100. 2024</p>
<p>30 m annual land cover and its dynamics in china from. J Yang, X Huang, 10.5281/zenodo.44178101990 to 2019. 2021</p>
<p>High-resolution datasets of permafrost thermal state and hydrothermal zonation in the northern hemisphere. X Li, Y Ran, M Hori, J Aalto, O Karjalainen, J Hjort, M Luoto, J Obu, G Cheng, J Che, H Jin, Q Yu, X Chang, 10.11888/Geocry.tpdc.2711902021National Tibetan Plateau Data Center</p>
<p>A synthesis dataset of permafrost for the qinghai-xizang (tibet) plateau, china. G Hu, R Li, T Wu, Y Xiao, Y Qiao, Z Xing, Y Zhao, J Shi, Q Pang, L Wang, C Xie, C Wang, G Cheng, Z Sun, D Zou, L Zhao, G Liu, E Du, X Wu, 10.11888/Geocry.tpdc.271107National Tibetan Plateau Data Center. 2002-2018. 2021</p>
<p>Glc fcs30: Global land-cover product with fine classification system at 30 m using time-series landsat imagery. L Liu, X Zhang, X Chen, Y Gao, J Mi, 10.5281/zenodo.39868722020</p>
<p>A global dataset of annual urban extents (1992-2020) from harmonized nighttime lights. M Zhao, C Cheng, Y Zhou, X Li, S Shen, C Song, 10.6084/m9.figshare.16602224.v12021</p>
<p>Simulation: Vectorized rooftop area data for 90 cities in china. N N U L Smart City Sensing, 10.11888/Geogra.tpdc.271702National Tibetan Plateau Data Center. 2020. 2021</p>
<p>Global monthly distributions of atmospheric co2 concentrations under the historical and future scenarios. W Cheng, D Li, X Deng, J Feng, Y Wang, J Peng, J Tian, W Qi, Z Liu, X Zheng, D Zhou, S Jiang, H Zhao, X Wang, 10.5281/zenodo.50213612021</p>
<p>Depth-to-bedrock map of china at a spatial resolution of 100 meters. F Yan, W Shangguan, J Zhang, B Hu, 10.6084/m9.figshare.c.4714514.v12019</p>
<p>J Yang, R Shi, D Wei, Z Liu, L Zhao, B Ke, H Pfister, B Ni, Medmnist v2: A large-scale lightweight benchmark for 2d and 3d biomedical image classification. 2021</p>
<p>Biowordvec: Improving biomedical word embeddings with subword information and mesh ontology. Y Zhang, Q Chen, Z Yang, H Lin, Z Lu, 10.6084/m9.figshare.6882647.v22018</p>
<p>A dataset of distribution and diversity of ticks in china. G Zhang, D Zheng, Y Tian, S Li, 2019</p>
<p>The sustech-sysu dataset for automatically segmenting and classifying corneal ulcers. J Yuan, L Deng, X Tang, H Huang, Y Deng, 10.6084/m9.figshare.c.4526675.v12020</p>
<p>Fives: A fundus image dataset for ai-based vessel segmentation. K Jin, X Huang, J Zhou, Y Li, Y Yan, Y Sun, Q Zhang, Y Wang, J Ye, 10.6084/m9.figshare.19688169.v12022</p>
<p>Hong Kong, T C U , mirtarbase: The experimentally validated microrna-target interactions database. 2021</p>
<p>A multi-modal open dataset for mental-disorder analysis. B Hu, 10.5255/UKDA-SN-8543012022</p>
<p>An atac-seq atlas of chromatin accessibility in mouse tissues. C Liu, M Wang, X Wei, L Liu, 10.26036/CNP00001982018</p>
<p>J Liao, Z Yu, Y Chen, C Zou, H Zhang, J Cheng, D Liu, T Li, Q Zhang, Z Mo, Single-cell rna sequencing of human kidney. 2019</p>
<p>A Corpus of CO2 Electrocatalytic Reduction Process Extracted from the Scientific Literature. L Wang, 10.57760/sciencedb.07106</p>
<p>Z Zhang, Z Xu, 10.6084/m9.figshare.22337629.v2Fatigue Database of Additively Manufactured Alloys. </p>
<p>Z Yuwei, C Fangyi, L Zeyi, J Yunzhuo, C Dongliang, Z Jinyi, J Xue, G Xi, H Jie, Z Lei, Z Xiaotong, S Yanjing, 10.5281/zenodo.11315713A Materials Terminology Knowledge Graph Automatically Constructed from Text Corpus. </p>
<p>M Yao, Y Wang, X Li, Y Sheng, H Huo, L Xi, J Yang, W Zhang, 10.6084/m9.figshare.13655276.v7Materials Informatics Platform with Three Dimensional Structures (MIP-3d. </p>
<p>J Liang, S Ye, T Dai, Z Zha, Y Gao, X Zhu, 10.6084/m9.figshare.12815276.v3QM-symex-database. </p>
<p>J Qu, Q Zhu, 10.6084/m9.figshare.10482707.v2High Dielectric Ternary Oxides from Crystal Structure Prediction and High-throughput Screening. </p>
<p>Y Zhu, 10.6084/m9.figshare.c.7259125.v1QCDGE Database: Quantum Chemistry Database with Ground-and Excited-State Properties. </p>
<p>H Lu, N Zou, R Jacobs, B Afflerbach, X.-G Lu, D Morgan, 10.18126/uppe-p8p1Error Assessment and Optimal Cross-validation Approaches in Machine Learning Applied to Impurity Diffusion. </p>
<p>Y Li, J Yu, C Zheng, 10.6084/m9.figshare.7822574.v6Database of Open-Framework Aluminophosphate Structures. </p>
<p>A Machine Learning-based Alloy Design System to Facilitate the Rational Design of High Entropy Alloys with Enhanced Hardness. C Yang, C Ren, Y Jia, G Wang, M Li, W Lu, 10.18126/rska-ta67</p>            </div>
        </div>

    </div>
</body>
</html>
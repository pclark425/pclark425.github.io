<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5206 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5206</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5206</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-269430362</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.17140v2.pdf" target="_blank">Small Language Models Need Strong Verifiers to Self-Correct Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Self-correction has emerged as a promising solution to boost the reasoning performance of large language models (LLMs), where LLMs refine their solutions using self-generated critiques that pinpoint the errors. This work explores whether small (<= 13B) language models (LMs) have the ability of self-correction on reasoning tasks with minimal inputs from stronger LMs. We propose a novel pipeline that prompts smaller LMs to collect self-correction data that supports the training of self-refinement abilities. First, we leverage correct solutions to guide the model in critiquing their incorrect responses. Second, the generated critiques, after filtering, are used for supervised fine-tuning of the self-correcting reasoner through solution refinement. Our experimental results show improved self-correction abilities of two models on five datasets spanning math and commonsense reasoning, with notable performance gains when paired with a strong GPT-4-based verifier, though limitations are identified when using a weak self-verifier for determining when to correct.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5206.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5206.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCORE (LLaMA-2-13B-chat)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SCORE: Self-COrrection in REasoning (applied to LLaMA-2-13B-chat)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that collects self-generated step-level critiques from a small LM using correct-solution hints, filters them, and supervised-fine-tunes the same LM to produce a critique+correction (refiner); used with an external verifier to decide when to apply refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-13B-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source chat-capable decoder transformer (LLaMA-2 family) with ~13 billion parameters, used few-shot for generation and LoRA fine-tuned for refiner/verifier roles.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SCORE (generate-then-reflect: critique + refiner)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Collect N=10 few-shot CoT solutions per question; pair incorrect and correct solutions (hints) to prompt the base LM to generate step-level critiques referencing the correct solution; rule-based and prompting filters keep high-quality critiques; fine-tune the same LM (LoRA) to generate a truncated critique (first erroneous step feedback) and a corrected solution in one pass; at inference a verifier (self or external GPT-4/oracle) decides whether to invoke the refiner. No correct-solution hints at inference. Iterations fixed at 1.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K; CommonsenseQA (CSQA); transfer to MATH subset, QASC, RiddleSense</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math word problems (GSM8K), multiple-choice commonsense QA (CommonsenseQA/CSQA), and transfer tests on math/commonsense reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Varies by verifier and dataset; with a GPT-4-based verifier the SCORE-fine-tuned LLaMA refiner yields large gains over the base few-shot answers (paper reports an average improvement across models/datasets of +14.6% when using a GPT-4 verifier). Example: when building on Rejection-Sampling Fine-Tuning (RFT) baseline, RFT = 42.7% → RFT + SCORE (gpt-4 verifier + SCORE refiner) = 45.1% on GSM8K test (absolute +2.4%). Confusion-matrix analysis shows GPT-4-as-verifier + SCORE-refiner converts incorrect→correct more often than the reverse.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Base few-shot prompted initial answers (varies by dataset); example few-shot / baseline numbers: for RFT experiment baseline initial few-shot prompting ~37.2% (RFT cited improvement from 37.2→42.7). For CommonsenseQA example: LLaMA few-shot initial ~69.7% (as reported in text) before SCORE self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: final accuracies after applying SCORE refiner + strong verifier (GPT-4) are substantially higher than initial few-shot answers and higher than prompting-based self-refinement baselines; specific reported examples include average +14.6% (across models/datasets) with GPT-4 verifier and absolute gains such as RFT 42.7% → +2.4% to 45.1% after SCORE self-correction. Confusion matrices show more conversions from incorrect to correct with GPT-4 verifier + SCORE refiner. The refiner's contribution (Contrib.) metric is numerically higher for SCORE than for prompted refiner baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-correction is highly bottlenecked by the verifier: a weak self-verifier (fine-tuned small LM) yields little or no improvement and can misguide refinement; prompting-based refiner baselines deteriorate performance. SCORE requires correct-solution hints during critique data collection (can't use hints at inference), uses only one iteration, and needs substantial compute to generate synthetic critique data. Performance plateaus beyond ~10k fine-tuning examples when verifier is weak. Inference overhead is modest (~x1.3 latency) but nonzero.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Language Models Need Strong Verifiers to Self-Correct Reasoning', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5206.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5206.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCORE (Gemma-7B-it)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SCORE: Self-COrrection in REasoning (applied to Gemma-7B-it)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same SCORE pipeline applied to Gemma-7B-it (7B open model), showing that a smaller base LM can learn self-refinement from self-generated critiques and benefit when paired with a strong verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-7B-it</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Gemma family model (Gemma-7B-it), roughly 7 billion parameters, used as base LM to generate critique data and fine-tune a refiner and verifier with LoRA.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SCORE (generate-then-reflect: critique + refiner)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Identical SCORE pipeline as above: sample N=10 CoT solutions, pair incorrect-correct solutions for hint-driven critique generation, filter critiques, fine-tune the same Gemma model to emit a critique+correction (truncated to first error) at inference; verifier decides whether to refine. Iteration count fixed at 1.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CommonsenseQA (CSQA), GSM8K, transfer MATH subset, QASC, RiddleSense</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Commonsense multiple-choice QA and math word-problem reasoning datasets used to evaluate refined outputs and transferability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>With a GPT-4 verifier the Gemma-7B-it + SCORE refiner achieves sizable gains; paper reports an average improvement of +8.3 percentage points across five datasets for Gemma-7B-it when combined with GPT-4 verifier. Example (CommonsenseQA): initial Gemma few-shot ~67.2% → after SCORE + strong verifier ~75.0% (absolute +7.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Initial few-shot prompted answers: example CommonsenseQA initial accuracy reported ~67.2% for Gemma-7B-it before self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative improvements reported (e.g., CommonsenseQA: 67.2% → 75.0% after SCORE + strong verifier), average +8.3pp across five datasets with GPT-4 verifier; transfer experiments (e.g., GSM8K→MATH subset) show up to +12.1 improvement in some settings, demonstrating generalizable self-correction skills.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Same as for LLaMA: improvements depend strongly on verifier quality; a weak self-verifier provides limited gains. Gemma required fewer SCORE fine-tuning examples but still benefitted most when paired with strong verifiers. SCORE relies on correct-solution hints during data collection and only performs one refinement iteration; generating large synthetic datasets is computationally expensive; verifier transfer is worse than refiner transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Language Models Need Strong Verifiers to Self-Correct Reasoning', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5206.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5206.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompted self-refinement baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompted verifier + prompted refiner (self-refine prompting baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline where the LM is few-shot prompted to both verify and refine its own outputs (prompted verifier + prompted refiner), following prior work on iterative self-feedback (e.g., Self-refine).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-13B-chat (prompted baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used as a few-shot prompted model to act as verifier and as refiner (no additional fine-tuning) in the prompting-based baseline experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Prompted generate-then-reflect (prompted self-critique + revision)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Few-shot prompting to have the LM generate a critique of its initial chain-of-thought and then revise the solution based on that critique, all via prompting (no supervised fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K; CommonsenseQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same reasoning benchmarks used as baselines to compare against SCORE fine-tuned refiners.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>The prompting-based self-correction baseline often deteriorated final predictions in this paper's experiments (authors report that the prompted verifier + prompted refiner 'deteriorates the final predictions'). Specific numeric degradations are not exhaustively enumerated in the main text but are shown in Figure 2 / Table 6 (baseline plotted lower than SCORE).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Base few-shot answers (without additional prompted self-refinement) — these are the baseline initial answers used to compare prompting-based self-refinement; for some tasks base few-shot was better than prompted self-refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>No improvement in these experiments; rather, the paper reports that the prompting-based self-refinement baseline often worsens final accuracy because LMs struggle to identify their reasoning errors reliably without supervised bootstrapping.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Prompted self-refinement (without fine-tuning on critique-correction data) is limited: small LMs struggle to identify errors in their reasoning and thus produce ineffective or harmful critiques leading to degraded final answers. The paper highlights that bootstrapping (supervised fine-tuning on curated critique-correction pairs) is necessary for small LMs to learn effective self-refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Language Models Need Strong Verifiers to Self-Correct Reasoning', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5206.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5206.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-verifier (fine-tuned small LM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-verifier: a small LM fine-tuned for binary correctness classification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A verifier module trained by fine-tuning the same small LM architecture to predict whether a candidate solution is correct based on question+solution; used to decide whether to invoke the refiner at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-13B-chat / Gemma-7B-it (verifier variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tuned versions of the base LM with a binary classification head on the last token representation to output probability of correctness (trained on labeled generated solutions).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Intrinsic self-verification (binary classifier) + conditional refinement</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Fine-tune the small LM on (question, solution) labeled by final-answer correctness to predict correct/incorrect. During inference the verifier outputs P(incorrect); if P(incorrect) exceeds a threshold (chosen on dev), the refiner is invoked to produce a corrected solution.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K; CommonsenseQA; other evaluated datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to decide whether to apply the SCORE refiner for reasoning tasks; thresholds tuned on dev.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>When paired with the SCORE refiner, a fine-tuned self-verifier yields only modest improvements and often underperforms compared to using GPT-4 or oracle verifiers. Example: self-verifier settings show much smaller Contrib. and final accuracy gains in Table 2 and Figure 2 (e.g., weak verifier frequently leads to low final improvement or misguidance).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>If the verifier is disabled (or always predicts 'correct'), the refiner is not invoked and final performance equals initial few-shot answers.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>The paper reports that a well-calibrated verifier (oracle or GPT-4) is required to unlock the refiner's gains; the fine-tuned self-verifier's F1 correlates strongly with final accuracy (Figure 2), and weak verifiers limit improvements. Ensembling verifications over multiple sampled solutions (oversample-then-rerank) can strengthen weak verifiers and yield modest gains when combined with SCORE.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fine-tuned small LMs are weak verifiers relative to GPT-4/oracle; misclassification by the verifier (false positives/negatives) can either fail to trigger beneficial refinements or trigger harmful unnecessary refinements. Verifier transfer across datasets is worse than that of the refiner. Reliance on an accurate verifier remains the principal bottleneck to intrinsic self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Language Models Need Strong Verifiers to Self-Correct Reasoning', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5206.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5206.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 verifier (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 used as an off-the-shelf strong verifier</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An external, stronger LLM (GPT-4) used via few-shot prompting to verify the correctness of candidate solutions and decide when to invoke the small-LM refiner.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large proprietary OpenAI model (GPT-4); used here in a few-shot prompted verification role (not fine-tuned) to judge correctness of small LM solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>External strong verifier + conditional refinement (extrinsic self-correction)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Few-shot prompt GPT-4 to predict correctness of candidate solution; if GPT-4 judges the candidate incorrect (above threshold), invoke the SCORE-fine-tuned small-LM refiner to produce a correction.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K; CommonsenseQA; other datasets evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used as an extrinsic verification signal to decide on applying SCORE refiner; demonstrates upper-bound practical performance when verifier is strong.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>When used as the verifier, GPT-4 enables large improvements for SCORE-fine-tuned refiners; the paper reports that SCORE fine-tuned models outperform the original models by an average of +14.6% when using a GPT-4-based verifier. Example: combining GPT-4 verifier + SCORE refiner yields substantially higher final accuracies and favorable confusion-matrix patterns (more incorrect→correct conversions).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>N/A for GPT-4 as verifier (it's the external signal); baseline comparisons are between small-LM-only verification vs GPT-4 verification. Using GPT-4 as verifier yields much better final accuracy than using the small self-verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: average +14.6% improvement across models/datasets when SCORE refiners are paired with GPT-4 verifier vs original model; confusion matrices show GPT-4 makes fewer false positives and better directs refinements that convert incorrect answers to correct ones.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Using a proprietary strong verifier (GPT-4) requires external API access and may be infeasible/undesirable in some settings. The paper treats GPT-4-as-verifier as an extrinsic upper-bound demonstrating what a strong verifier can unlock; relying on strong external verifiers limits intrinsic/self-contained small-LM self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Language Models Need Strong Verifiers to Self-Correct Reasoning', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Teaching large language models to self-debug <em>(Rating: 2)</em></li>
                <li>Rejection sampling finetuning (RFT) <em>(Rating: 2)</em></li>
                <li>Star: Bootstrapping reasoning with reasoning <em>(Rating: 1)</em></li>
                <li>Large language models can self-improve <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5206",
    "paper_id": "paper-269430362",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "SCORE (LLaMA-2-13B-chat)",
            "name_full": "SCORE: Self-COrrection in REasoning (applied to LLaMA-2-13B-chat)",
            "brief_description": "A pipeline that collects self-generated step-level critiques from a small LM using correct-solution hints, filters them, and supervised-fine-tunes the same LM to produce a critique+correction (refiner); used with an external verifier to decide when to apply refinement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-13B-chat",
            "model_description": "Open-source chat-capable decoder transformer (LLaMA-2 family) with ~13 billion parameters, used few-shot for generation and LoRA fine-tuned for refiner/verifier roles.",
            "reflection_method_name": "SCORE (generate-then-reflect: critique + refiner)",
            "reflection_method_description": "Collect N=10 few-shot CoT solutions per question; pair incorrect and correct solutions (hints) to prompt the base LM to generate step-level critiques referencing the correct solution; rule-based and prompting filters keep high-quality critiques; fine-tune the same LM (LoRA) to generate a truncated critique (first erroneous step feedback) and a corrected solution in one pass; at inference a verifier (self or external GPT-4/oracle) decides whether to invoke the refiner. No correct-solution hints at inference. Iterations fixed at 1.",
            "num_iterations": 1,
            "task_name": "GSM8K; CommonsenseQA (CSQA); transfer to MATH subset, QASC, RiddleSense",
            "task_description": "Grade-school math word problems (GSM8K), multiple-choice commonsense QA (CommonsenseQA/CSQA), and transfer tests on math/commonsense reasoning benchmarks.",
            "performance_with_reflection": "Varies by verifier and dataset; with a GPT-4-based verifier the SCORE-fine-tuned LLaMA refiner yields large gains over the base few-shot answers (paper reports an average improvement across models/datasets of +14.6% when using a GPT-4 verifier). Example: when building on Rejection-Sampling Fine-Tuning (RFT) baseline, RFT = 42.7% → RFT + SCORE (gpt-4 verifier + SCORE refiner) = 45.1% on GSM8K test (absolute +2.4%). Confusion-matrix analysis shows GPT-4-as-verifier + SCORE-refiner converts incorrect→correct more often than the reverse.",
            "performance_without_reflection": "Base few-shot prompted initial answers (varies by dataset); example few-shot / baseline numbers: for RFT experiment baseline initial few-shot prompting ~37.2% (RFT cited improvement from 37.2→42.7). For CommonsenseQA example: LLaMA few-shot initial ~69.7% (as reported in text) before SCORE self-correction.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: final accuracies after applying SCORE refiner + strong verifier (GPT-4) are substantially higher than initial few-shot answers and higher than prompting-based self-refinement baselines; specific reported examples include average +14.6% (across models/datasets) with GPT-4 verifier and absolute gains such as RFT 42.7% → +2.4% to 45.1% after SCORE self-correction. Confusion matrices show more conversions from incorrect to correct with GPT-4 verifier + SCORE refiner. The refiner's contribution (Contrib.) metric is numerically higher for SCORE than for prompted refiner baselines.",
            "limitations_or_failure_cases": "Self-correction is highly bottlenecked by the verifier: a weak self-verifier (fine-tuned small LM) yields little or no improvement and can misguide refinement; prompting-based refiner baselines deteriorate performance. SCORE requires correct-solution hints during critique data collection (can't use hints at inference), uses only one iteration, and needs substantial compute to generate synthetic critique data. Performance plateaus beyond ~10k fine-tuning examples when verifier is weak. Inference overhead is modest (~x1.3 latency) but nonzero.",
            "uuid": "e5206.0",
            "source_info": {
                "paper_title": "Small Language Models Need Strong Verifiers to Self-Correct Reasoning",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "SCORE (Gemma-7B-it)",
            "name_full": "SCORE: Self-COrrection in REasoning (applied to Gemma-7B-it)",
            "brief_description": "Same SCORE pipeline applied to Gemma-7B-it (7B open model), showing that a smaller base LM can learn self-refinement from self-generated critiques and benefit when paired with a strong verifier.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma-7B-it",
            "model_description": "Open-source Gemma family model (Gemma-7B-it), roughly 7 billion parameters, used as base LM to generate critique data and fine-tune a refiner and verifier with LoRA.",
            "reflection_method_name": "SCORE (generate-then-reflect: critique + refiner)",
            "reflection_method_description": "Identical SCORE pipeline as above: sample N=10 CoT solutions, pair incorrect-correct solutions for hint-driven critique generation, filter critiques, fine-tune the same Gemma model to emit a critique+correction (truncated to first error) at inference; verifier decides whether to refine. Iteration count fixed at 1.",
            "num_iterations": 1,
            "task_name": "CommonsenseQA (CSQA), GSM8K, transfer MATH subset, QASC, RiddleSense",
            "task_description": "Commonsense multiple-choice QA and math word-problem reasoning datasets used to evaluate refined outputs and transferability.",
            "performance_with_reflection": "With a GPT-4 verifier the Gemma-7B-it + SCORE refiner achieves sizable gains; paper reports an average improvement of +8.3 percentage points across five datasets for Gemma-7B-it when combined with GPT-4 verifier. Example (CommonsenseQA): initial Gemma few-shot ~67.2% → after SCORE + strong verifier ~75.0% (absolute +7.8%).",
            "performance_without_reflection": "Initial few-shot prompted answers: example CommonsenseQA initial accuracy reported ~67.2% for Gemma-7B-it before self-correction.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative improvements reported (e.g., CommonsenseQA: 67.2% → 75.0% after SCORE + strong verifier), average +8.3pp across five datasets with GPT-4 verifier; transfer experiments (e.g., GSM8K→MATH subset) show up to +12.1 improvement in some settings, demonstrating generalizable self-correction skills.",
            "limitations_or_failure_cases": "Same as for LLaMA: improvements depend strongly on verifier quality; a weak self-verifier provides limited gains. Gemma required fewer SCORE fine-tuning examples but still benefitted most when paired with strong verifiers. SCORE relies on correct-solution hints during data collection and only performs one refinement iteration; generating large synthetic datasets is computationally expensive; verifier transfer is worse than refiner transfer.",
            "uuid": "e5206.1",
            "source_info": {
                "paper_title": "Small Language Models Need Strong Verifiers to Self-Correct Reasoning",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Prompted self-refinement baseline",
            "name_full": "Prompted verifier + prompted refiner (self-refine prompting baseline)",
            "brief_description": "A baseline where the LM is few-shot prompted to both verify and refine its own outputs (prompted verifier + prompted refiner), following prior work on iterative self-feedback (e.g., Self-refine).",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-13B-chat (prompted baseline)",
            "model_description": "Used as a few-shot prompted model to act as verifier and as refiner (no additional fine-tuning) in the prompting-based baseline experiments.",
            "reflection_method_name": "Prompted generate-then-reflect (prompted self-critique + revision)",
            "reflection_method_description": "Few-shot prompting to have the LM generate a critique of its initial chain-of-thought and then revise the solution based on that critique, all via prompting (no supervised fine-tuning).",
            "num_iterations": 1,
            "task_name": "GSM8K; CommonsenseQA",
            "task_description": "Same reasoning benchmarks used as baselines to compare against SCORE fine-tuned refiners.",
            "performance_with_reflection": "The prompting-based self-correction baseline often deteriorated final predictions in this paper's experiments (authors report that the prompted verifier + prompted refiner 'deteriorates the final predictions'). Specific numeric degradations are not exhaustively enumerated in the main text but are shown in Figure 2 / Table 6 (baseline plotted lower than SCORE).",
            "performance_without_reflection": "Base few-shot answers (without additional prompted self-refinement) — these are the baseline initial answers used to compare prompting-based self-refinement; for some tasks base few-shot was better than prompted self-refinement.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "No improvement in these experiments; rather, the paper reports that the prompting-based self-refinement baseline often worsens final accuracy because LMs struggle to identify their reasoning errors reliably without supervised bootstrapping.",
            "limitations_or_failure_cases": "Prompted self-refinement (without fine-tuning on critique-correction data) is limited: small LMs struggle to identify errors in their reasoning and thus produce ineffective or harmful critiques leading to degraded final answers. The paper highlights that bootstrapping (supervised fine-tuning on curated critique-correction pairs) is necessary for small LMs to learn effective self-refinement.",
            "uuid": "e5206.2",
            "source_info": {
                "paper_title": "Small Language Models Need Strong Verifiers to Self-Correct Reasoning",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Self-verifier (fine-tuned small LM)",
            "name_full": "Self-verifier: a small LM fine-tuned for binary correctness classification",
            "brief_description": "A verifier module trained by fine-tuning the same small LM architecture to predict whether a candidate solution is correct based on question+solution; used to decide whether to invoke the refiner at inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-13B-chat / Gemma-7B-it (verifier variant)",
            "model_description": "Fine-tuned versions of the base LM with a binary classification head on the last token representation to output probability of correctness (trained on labeled generated solutions).",
            "reflection_method_name": "Intrinsic self-verification (binary classifier) + conditional refinement",
            "reflection_method_description": "Fine-tune the small LM on (question, solution) labeled by final-answer correctness to predict correct/incorrect. During inference the verifier outputs P(incorrect); if P(incorrect) exceeds a threshold (chosen on dev), the refiner is invoked to produce a corrected solution.",
            "num_iterations": 1,
            "task_name": "GSM8K; CommonsenseQA; other evaluated datasets",
            "task_description": "Used to decide whether to apply the SCORE refiner for reasoning tasks; thresholds tuned on dev.",
            "performance_with_reflection": "When paired with the SCORE refiner, a fine-tuned self-verifier yields only modest improvements and often underperforms compared to using GPT-4 or oracle verifiers. Example: self-verifier settings show much smaller Contrib. and final accuracy gains in Table 2 and Figure 2 (e.g., weak verifier frequently leads to low final improvement or misguidance).",
            "performance_without_reflection": "If the verifier is disabled (or always predicts 'correct'), the refiner is not invoked and final performance equals initial few-shot answers.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "The paper reports that a well-calibrated verifier (oracle or GPT-4) is required to unlock the refiner's gains; the fine-tuned self-verifier's F1 correlates strongly with final accuracy (Figure 2), and weak verifiers limit improvements. Ensembling verifications over multiple sampled solutions (oversample-then-rerank) can strengthen weak verifiers and yield modest gains when combined with SCORE.",
            "limitations_or_failure_cases": "Fine-tuned small LMs are weak verifiers relative to GPT-4/oracle; misclassification by the verifier (false positives/negatives) can either fail to trigger beneficial refinements or trigger harmful unnecessary refinements. Verifier transfer across datasets is worse than that of the refiner. Reliance on an accurate verifier remains the principal bottleneck to intrinsic self-correction.",
            "uuid": "e5206.3",
            "source_info": {
                "paper_title": "Small Language Models Need Strong Verifiers to Self-Correct Reasoning",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPT-4 verifier (few-shot)",
            "name_full": "GPT-4 used as an off-the-shelf strong verifier",
            "brief_description": "An external, stronger LLM (GPT-4) used via few-shot prompting to verify the correctness of candidate solutions and decide when to invoke the small-LM refiner.",
            "citation_title": "GPT-4 technical report",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large proprietary OpenAI model (GPT-4); used here in a few-shot prompted verification role (not fine-tuned) to judge correctness of small LM solutions.",
            "reflection_method_name": "External strong verifier + conditional refinement (extrinsic self-correction)",
            "reflection_method_description": "Few-shot prompt GPT-4 to predict correctness of candidate solution; if GPT-4 judges the candidate incorrect (above threshold), invoke the SCORE-fine-tuned small-LM refiner to produce a correction.",
            "num_iterations": 1,
            "task_name": "GSM8K; CommonsenseQA; other datasets evaluated",
            "task_description": "Used as an extrinsic verification signal to decide on applying SCORE refiner; demonstrates upper-bound practical performance when verifier is strong.",
            "performance_with_reflection": "When used as the verifier, GPT-4 enables large improvements for SCORE-fine-tuned refiners; the paper reports that SCORE fine-tuned models outperform the original models by an average of +14.6% when using a GPT-4-based verifier. Example: combining GPT-4 verifier + SCORE refiner yields substantially higher final accuracies and favorable confusion-matrix patterns (more incorrect→correct conversions).",
            "performance_without_reflection": "N/A for GPT-4 as verifier (it's the external signal); baseline comparisons are between small-LM-only verification vs GPT-4 verification. Using GPT-4 as verifier yields much better final accuracy than using the small self-verifier.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: average +14.6% improvement across models/datasets when SCORE refiners are paired with GPT-4 verifier vs original model; confusion matrices show GPT-4 makes fewer false positives and better directs refinements that convert incorrect answers to correct ones.",
            "limitations_or_failure_cases": "Using a proprietary strong verifier (GPT-4) requires external API access and may be infeasible/undesirable in some settings. The paper treats GPT-4-as-verifier as an extrinsic upper-bound demonstrating what a strong verifier can unlock; relying on strong external verifiers limits intrinsic/self-contained small-LM self-correction.",
            "uuid": "e5206.4",
            "source_info": {
                "paper_title": "Small Language Models Need Strong Verifiers to Self-Correct Reasoning",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Teaching large language models to self-debug",
            "rating": 2,
            "sanitized_title": "teaching_large_language_models_to_selfdebug"
        },
        {
            "paper_title": "Rejection sampling finetuning (RFT)",
            "rating": 2,
            "sanitized_title": "rejection_sampling_finetuning_rft"
        },
        {
            "paper_title": "Star: Bootstrapping reasoning with reasoning",
            "rating": 1,
            "sanitized_title": "star_bootstrapping_reasoning_with_reasoning"
        },
        {
            "paper_title": "Large language models can self-improve",
            "rating": 1,
            "sanitized_title": "large_language_models_can_selfimprove"
        }
    ],
    "cost": 0.0168,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Small Language Models Need Strong Verifiers to Self-Correct Reasoning</p>
<p>Yunxiang Zhang yunxiang@umich.edu 
Muhammad Khalifa 
Lajanugen Logeswaran 
University of Illinois at Chicago</p>
<p>Jaekyeom Kim 
University of Illinois at Chicago</p>
<p>Moontae Lee 
Honglak Lee 
Lu Wang </p>
<p>University of Michigan α LG AI Research</p>
<p>Small Language Models Need Strong Verifiers to Self-Correct Reasoning
05718DF456CE0CEB8B54F1E264B5AB08
Self-correction has emerged as a promising solution to boost the reasoning performance of large language models (LLMs), where LLMs refine their solutions using self-generated critiques that pinpoint the errors.This work explores whether small (≤ 13B) language models (LMs) have the ability of self-correction on reasoning tasks with minimal inputs from stronger LMs.We propose a novel pipeline that prompts smaller LMs to collect self-correction data that supports the training of self-refinement abilities.First, we leverage correct solutions to guide the model in critiquing their incorrect responses.Second, the generated critiques, after filtering, are used for supervised fine-tuning of the self-correcting reasoner through solution refinement.Our experimental results show improved self-correction abilities of two models on five datasets spanning math and commonsense reasoning, with notable performance gains when paired with a strong GPT-4-based verifier, though limitations are identified when using a weak self-verifier for determining when to correct. 1</p>
<p>Introduction</p>
<p>Recent research shows that large language models (LLMs) (OpenAI, 2023) can self-correct their responses to meet diverse user requirements, ranging from diminishing harmful content to including specific keywords and to debugging code (Madaan et al., 2023;Chen et al., 2023b).Self-correction is typically accomplished by first generating a critique that identifies the shortcomings of the initial response, followed by revising it according to the self-critique-a process that can be iterated.</p>
<p>Self-correction has emerged as an intriguing paradigm for rectifying the flaws in LLM's outputs (Pan et al., 2023).However, models that are effective at self-correction are of very large sizes, and many of them are proprietary and accessible only via APIs.In this work, we focus on the self-correction abilities of small, open-source language models (LMs). 2 Previous studies have shown that these smaller models can learn selfcorrection in reasoning through distillation from stronger LMs (Yu et al., 2023b;An et al., 2023;Han et al., 2024).Yet this poses security risks for high-stakes domains and hinders the scientific understanding of enhancing LMs' ability to correct errors.We thus ask the question: To which degree do small LMs require guidance from strong LMs to learn self-correction for reasoning?</p>
<p>We study this question by leveraging the small model itself to generate supervised training data to enhance its self-correction ability, instead of resorting to stronger LMs.To this end, we draw inspiration from the rejection sampling fine-tuning (RFT) (Touvron et al., 2023;Yuan et al., 2023) method where LLM's reasoning skills are bootstrapped via diverse chain-of-thought sampling and supervised fine-tuning on the correct reasoning chains.We propose SCORE-an approach to bootstrap small LMs' Self-COrrection ability in REasoning tasks.Concretely, we devise a pipeline for accumulating high-quality critique-correction data from small LMs, which are used for supervised fine-tuning of self-correcting reasoners.First, we leverage correct solutions as hints for the base LMs to critique incorrect answers.By reverseengineering from the correct answer, the models generate more effective critiques.Second, we filter these critiques for correctness, well-formedness, and clarity using simple rule-based and prompting methods.Finally, we fine-tune the same LMs to be-2 While the distinction between small vs. large LMs is often context-dependent (Saunders et al., 2022;Yu et al., 2023b), in this work, we interchangeably use "small" or "weak" LMs to refer to open models with a few billion parameters (e.g., LLaMA-7/13B (Touvron et al., 2023)).arXiv:2404.17140v2[cs.CL] 6 Jun 2024 come self-refining models using this curated data.By avoiding the use of supervision from stronger LMs, we ensure that our method enables a small LM to bootstrap its self-correction capabilities.</p>
<p>We evaluate our SCORE fine-tuned refiner under both extrinsic and intrinsic self-correction settings (Huang et al., 2023b).The primary difference between these two settings is whether the refiner is allowed to use external signals to determine when to self-correct (i.e., refine the initial solution only when it is believed to be incorrect).Identifying when to self-correct involves verifying the solutions' correctness, which is still challenging for current state-of-the-art LLMs without proper external feedback (Huang et al., 2023b).We adopt a simple baseline for the self-verification problem following Cobbe et al. (2021).Specifically, we fine-tune the same LMs to become verifiers with labels based solely on the correctness of the final answer, conditioning on the question and a candidate solution.As for the extrinsic setting, we simulate strong verifiers with GPT-4 and oracle labels, to show the effectiveness of small LMs as self-correcting reasoners.</p>
<p>We test the SCORE method with the LLaMA-2-13B-chat (Touvron et al., 2023) and Gemma-7b-it (Team et al., 2024) models on five datasets spanning math and commonsense reasoning.We find that our model with SCORE finetuning outperforms the original model by an average of 14.6% when using a gpt-4-based verifier.Nevertheless, the model struggles with selfcorrection when subjected to a weak self-verifier fine-tuned on self-generated solutions.</p>
<p>Our main contributions are summarized below:</p>
<ol>
<li>
<p>We introduce SCORE, a novel pipeline to generate self-correction data from a small LM, and subsequently fine-tune the model to be a self-correcting reasoner.</p>
</li>
<li>
<p>Our method effectively augments the selfcorrection abilities of small LMs on math and commonsense reasoning, when using strong verifiers.</p>
</li>
<li>
<p>To the best of our knowledge, we are the first to demonstrate the potential of small LMs to bootstrap their abilities on self-corrective reasoning without distilling training data from stronger LMs or using human annotation.</p>
</li>
</ol>
<p>2 Problem Formulation of Self-Correction</p>
<p>Self-Correct := (SELF-)VERIFY + SELF-REFINE.We decompose the task of selfcorrection into two phases: (SELF-)VERIFY and SELF-REFINE.The LM first generates an initial solution for a reasoning question.A verifier, either the LM itself (intrinsic) or the external signal (extrinsic), then judges the correctness of the initial solution.If correct, the initial solution will be directly used as the final answer.If incorrect, a refiner will revise the solution.While this process can be iterated, we fix the times of iterations as 1 throughout this paper for efficiency and leave multiple iterations as future studies.</p>
<p>Decoupling (SELF-)VERIFY and SELF-REFINE brings two major advantages over a one-modeldoes-all design.First, we can freely parameterize each module-for example, by using a fine-tuned and a few-shot prompted model.This allows us to carefully examine the impact of strong vs. weak verifiers on the refiners' performance.On the contrary, previous work on self-correction with small LMs (Yu et al., 2023b;An et al., 2023;Han et al., 2024) conflates SELF-VERIFY and SELF-REFINE, creating a barrier to fully understanding the distinct capacities of these models in each skill.Second, it reduces the difficulty of training each module, since the model only needs to specialize in one kind of ability, which is either verification or refinement.</p>
<p>SELF-REFINE := Critique + Correction.The challenge for SELF-REFINE is that it can be difficult for language models to directly map an initial solution to a revision without any guidance (Welleck et al., 2023).Using critiquesassessments that pinpoint the locations of errors within the reasoning steps, explain the causes of these errors, and offer guidance on how to correct them-can significantly enhance the performance of language models when generating revisions (Saunders et al., 2022;Madaan et al., 2023).Therefore, we formulate refinement with two steps: the model will first generate a critique for the initial solutions determined as incorrect, followed by a corrected version, in a single pass.Yet, it is still non-trivial to obtain high-quality critiques to guide the error correction.We address this problem using the correct solutions as hints to facilitate the critique generation, detailed in Section 3.1.</p>
<p>The SCORE Method</p>
<p>Our approach is inspired by rejection sampling finetuning (RFT): sampling diverse solutions for each question and fine-tune LLMs on the self-generated solutions that lead to the correct final answer (Yuan et al., 2023;Huang et al., 2023a;Zelikman et al., 2022).We want to bootstrap the small LM's inherent ability to generate critiques for reasoning steps.We design an end-to-end pipeline to collect selfcorrection data generated by small LMs at scale, without any distillation from stronger LMs.The self-generated critiques, after filtering, are used to fine-tune the smaller LM itself to bootstrap its ability to self-correct.Concretely, the SCORE pipeline consists of two stages shown in Figure 1 and described below.</p>
<p>Stage 1: Generating and Filtering Critiques.We sample N solutions for each question in the training set by few-shot chain-of-thought prompting a base LM (step a).To enable the base LM to reflect on its incorrect solutions, we include a correct solution for the same question (if exists) in the prompt as a hint (step b).We then filter the self-generated critiques based on their correctness and clarity (step c).This process is detailed in Section 3.1.</p>
<p>Stage 2: Supervised Fine-tuning of the Refiner.The filtered critiques obtained from stage 1 are used in the next stage for fine-tuning the small LM itself.We train a refiner that generates critiques and corrections conditioned on questions and initial solutions (step d).We exclude the hints during finetuning.More details are given in Section 3.2.</p>
<p>Generating and Filtering Critiques</p>
<p>Directly generating critique for an incorrect solution without external supervision signals is difficult.In our preliminary experiments, we find it easier for the LM to generate critiques using correct solutions as hints, as the model only needs to compare the different steps between these two solutions and justify the correct ones.In Appendix B, we explain this intuition from a mathematical perspective.</p>
<p>To leverage correct solutions as hints for LMs to generate critiques on incorrect solutions, we label these solutions and collect all possible pairs of incorrect-correct solutions for the same questions (Cartesian product between the sets of incorrect and correct solutions).We craft a few-shot critique prompt (Appendix A) to instruct the base LM to generate critiques for the incorrect solution using the paired correct solution as hints. 3Step-level critiques are more useful than solution-level ones since they provide more precise and fine-grained supervision (Lightman et al., 2023;Wu et al., 2023;Uesato et al., 2022a) that mitigate the undesirable behavior of LMs using incorrect reasoning to reach the correct final answer (Khalifa et al., 2023;Zelikman et al., 2022).Therefore, we prompt the model to provide feedback for each step of the initial solution, either endorsing the initial answer (e.g., "this step is correct") or pinpointing the errors (e.g., "there are errors in the step because ...").</p>
<p>To ensure the LM-generated critique is grounded in a specific step, we also ask the model to copy each step before providing feedback on it.Considering these requirements, we design the format of the critique prompt as follows, with a detailed example in Appendix A.</p>
<p>Critique Prompt</p>
<p>Q: {question}</p>
<p>Answer 1 (Incorrect):</p>
<p>Step 1: ... ...</p>
<p>Step n: The answer is x .</p>
<p>Answer 2 (Correct):</p>
<p>Step 1: ... ...</p>
<p>Step n: The answer is y .</p>
<p>There are reasoning errors in Answer 1. Please go through each step in Answer 1, use Answer 2 as a reference for the correct approach, and provide feedback that helps correct the errors in Answer 1. End your response with [END].</p>
<p>Let's go through the errors in Answer 1 and provide feedback:</p>
<p>Answer 1 (Incorrect):</p>
<p>Step 1: ... Feedback: This step is correct....</p>
<p>Step i: ... Feedback: This is incorrect.Because ... ...</p>
<p>Step n: The answer is x .Feedback: The correct answer, based on the corrected calculations, should be y .[END] Note that the model should suggest the corrected final answer, taken from the hint solution, as part of the feedback for the last step.This forces the model to explicitly leverage the information from the hint solution.</p>
<p>Filtering Generated Critiques.After obtaining the raw self-generated critiques, we want to remove the low-quality ones and keep the rest for fine-tuning LMs.Thanks to the well-designed format of critiques, we can apply rule-based filters to remove generated critiques that do not follow the desired format.These criteria include:</p>
<p>• The number of steps and feedbacks (counted by the appearances of "Step {i}:" and "Feedback:") should be the same.</p>
<p>• Each step should be exactly copied from the initial solution.</p>
<p>• The feedback for the last step should provide the correct answer.</p>
<p>The first two criteria check for the well-formedness of the critique and the third one focuses on the correctness aspect.A critique will be removed if it fails to meet any of the three criteria above.</p>
<p>Given that a critique could still contain errors even if it suggests the correct final answer in the last step, we add an additional stage of promptingbased filtering besides the above rule-based heuristics.Specifically, we prompt the base LM to revise the incorrect solution given the critique that already passes the aforementioned filtering rule.Assuming the base LM has reasonable ability of following instructions, it is expected to give a correct revision if the generated critique is both clear and error-free.We demonstrate such an example of the correction prompt in Appendix A. In other words, we remove critiques that do not result in a correctly revised answer.After the ruled-based and prompting-based filtering, we obtain the high-quality critiques for fine-tuning LMs to self-refine.</p>
<p>Supervised Fine-tuning of the Refiner</p>
<p>We train the refiner to generate a critique and an improved solution in one pass conditioned on a question and an initial solution.We note that although we provide the correct solutions as hints to generate critiques during data collection, the model is tasked to generate critiques without the hints during fine-tuning and inference.Previously we collected critiques for every step to ensure that we can apply multiple filters to obtain high-quality critiques.But in this step, we truncate the critiques to only keep the feedback for the first error step as the fine-tuning target.This is because it is difficult to ask the LMs to identify and correct all the errors in one pass (Yu et al., 2023b) without referring to correct solutions as hints during inference.The refiner is fine-tuned on truncated critiques and corrections collected in the previous stages with cross-entropy loss.We do not include few-shot demonstrations during fine-tuning.We apply masks on the input tokens so that they do not contribute to the loss.Although we only do 1 iteration for the refinement in this work, we later show that small LMs can already achieve great improvement after 1 round of self-correction when paired with a strong verifier.</p>
<p>Experimental Setup</p>
<p>Self-Correction Data Collection.As stated in Section 3.1, we sample N = 10 solutions from the base model with the chain-of-thought (CoT) prompts shown in Appendix A, label their correctness, and formulate incorrect-correct solution pairs for critique generation.We separately collect data for each base LM and task.This results in the number of raw critiques shown in Table 1.We sequentially apply rule-based filtering and promptingbased filtering to obtain high-quality critiques for the fine-tuning data.</p>
<p>Verifiers.We experiment with verifiers of different levels of capabilities to gauge their impacts on self-correction performance.First, we adopt a simple baseline for training a self-verifier following Cobbe et al. (2021).The self-verifier is a model with the same architecture as the base LM, conditioned on the question and a candidate solution to judge the probability that the solution is correct/incorrect.Specifically, we label the solutions sampled from the base LM as incorrect (0) or correct (1) solely based on their final answers and finetune the verifier with a binary classification head on the last-layer representation of the last token in the input sequence "Question: {q} \n Solution: {s} \n Is this solution correct?".Since the fine-tuning data is imbalanced between correct and incorrect solution, we re-weight the loss for each class with regarding to its proportion.During inference, the verifier model outputs a probability of the initial solution being incorrect, and the refinement is introduced only when the confidence of the verifier's predictions exceeds a certain threshold, which is automatically chosen in a way that maximizes the accuracy on the dev set and then fixed during test-time predictions.Since fine-tuned small LMs are still weak verifiers that bottleneck the performance of self-correction, we also experiment with a second option by using gpt-4 as an off-the-shelf strong verifier to demonstrate the potential of our fine-tuned refiner.We do so by few-shot prompting gpt-4 to predict the correctness of the initial solution by smaller LMs, with the verifying prompt shown in Appendix A. Finally, we directly use the gold labels of the initial solutions as signals to determine when to self-refine.This oracle verifier setting provides an upper bound for the refiners' performance.</p>
<p>Benchmarks and Base Models.We explore two open-source smaller language models, namely LLaMA-2-13B-chat (Touvron et al., 2023) and Gemma-7B-it (Team et al., 2024) Figure 2: Performance of SCORE models using LLaMA-2-13B-chat and Gemma-7B-it as base LMs.We show the relationship between F1 scores of the verifiers and final answer accuracies.We also report the difference between the final accuracy after refinement and the corresponding initial answer accuracy (Init.Accu.) by few-shot prompting for each base LM.For the concrete numbers of these metrics, please see Table 6 in the appendix.We include test results for training tasks (GSM8K and CommonsenseQA/CSQA), as well as transfer evaluation of GSM8K trained models on MATH subset, CSQA trained models on QASC and RiddleSense.All models use greedy decoding during inference.</p>
<p>as the base LMs to generate self-correction data and evaluate their self-correction abilities.In Appendix C, we also investigate whether our selfcorrection fine-tuning can be built on top of other fine-tuning methods (e.g., rejection-sampling finetuning) to further boost the reasoning performance.</p>
<p>Fine-tuning and Evaluation.We fine-tune the base LM using the LLaMA-Factory library (Zheng et al., 2024b) with LoRA (Hu et al., 2022).We set the low-rank dimension as 32, the learning rate as 2e-5, training epochs as 3, batch size as 32.During inference, we set the temperature as 0 (i.e., greedy decoding) and the max sample length as 2,048.All our experiments can be conducted on 4×A40 GPU with 48GB of memory.</p>
<p>Results</p>
<p>In this section, we first present the experimental findings of SCORE method on various models and datasets (Section 5.1).To better understand the performance changes after self-correction, we then analyze the behaviors of verifiers and refiners (Section 5.2) and further highlight several key design decisions of our pipeline with ablation stud-ies (Section 5.3).Lastly, we show the impact of SCORE fine-tuning data size on self-correction performance (Section 5.4).</p>
<p>Main Findings</p>
<p>Figure 2 presents the primary evaluation results for our fine-tuned models compared to baseline models.The results include two performance metrics: the verifier F1, which assesses the precision and recall of the verifier's predictions; and the final accuracy, which measures the accuracy of the final answer after self-correction.We have four major findings.</p>
<p>1) The critique-correction data collected by our SCORE pipeline enhances the base LM's capability for self-correction.Our fine-tuned models consistently bring large improvements on the final accuracy over the initial answer obtained by few-shot prompting.However, the prompting-based selfcorrection baseline (prompted verifier + prompted refiner in Figure 2) proposed by Madaan et al. (2023) deteriorates the final predictions, as LMs struggle to identify errors in their reasoning (Huang et al., 2023b) and possess limited self-correction abilities before bootstrapping.On the multiple- choice CommonsenseQA questions, our SCORE fine-tuned refiner achieves much larger improvement than the random baseline under oracle verifier, indicating that our model is not simply making random guess.</p>
<p>2) Our framework improves self-correction for various base LMs on different types of reasoning tasks.We validate the effectiveness of our SCORE fine-tuning on both math reasoning and commonsense reasoning tasks with two pretrained LMs.In principle, our task-agonistic pipeline can be applied to a variety of datasets whose reasoning could be expressed in a step-by-step format.We also observe that although the initial solutions proposed by Gemma-7B are worse than LLaMA-13B (e.g., 67.2 &lt; 69.7 on CommonsenseQA), Gemma-7B's accuracy surpasses LLaMA-13B after self-correction (e.g., 75.0 &gt; 72.4 on CommonsenseQA).Considering that Gemma-7B is fine-tuned with even less self-correction data (Table 1), we believe Gemma is more effective at learning self-correction skills from SCORE fine-tuning.</p>
<p>3) The self-correction performance is largely bottlenecked by the verifier rather than the refiner.</p>
<p>Using the same fine-tuned refiner, the final accuracies vary a lot among different verifiers.The upper bound performance suggested by an oracle verifier demonstrate great potential for self-correction, yet a weak self-verifier can only bring minor improvements, if not misguiding the refiner.Nevertheless, when combined with a more advanced verifier, such as GPT-4, our refiner achieves a significant increase in final accuracy, e.g., an average of +8.3 across five datasets for Gemma-7b-it.The confusion matrices in Figure 3 show the system of gpt-4-as-verifier + SCORE-refiner is more likely to modify an incorrect answer to a correct one than the other way round.This observation underscores the necessity of effectively tackling the problem of reasoning verification before significant advances in self-correction can be attained.Future work could focus on the improvement of reasoning verification that is built upon a mechanistic (Yüksekgönül et al., 2023;Yang et al., 2024) and representational (Zou et al., 2023;Zheng et al., 2024a) understanding of LMs' internal reasoning process.</p>
<p>4) The enhanced self-correction skills can transfer across different datasets.When evaluating our fine-tuned refiner on unseen datasets, it still demonstrates consistent improvement over the baselines (up to +12.1 by the GSM8K-trained Gemma-7B on MATH subset).This shows that the model is learning generalizable self-correction skills rather than overfitting to a specific dataset.Additionally, we find that the verifier does not transfer as well as the refiner, reiterating the difficulty of reasoning verification for LMs.</p>
<p>SCORE could be combined with oversamplethen-rerank, an orthogonal approach to further improve reasoning (Cobbe et al., 2021;Li et al., 2023b;Hosseini et al., 2024).By aggregating verifications from multiple solutions, the weak verifiers become stronger via ensembling and thus unleash the power of refiner for better self-correction of reasoning.Please refer to Appendix D for details.</p>
<p>Analysis of Self-Correction Behaviors</p>
<p>Following the methodology of Yu et al. (2023b), we focus on two key metrics to understand the model's self-correction behaviors: 1) the frequency with which the verifier decides to self-correct (Freq.), and 2) the extent to which these self-correction attempts enhance the model's task performance (Contrib.).Self-correction Freq. is measured by the ratio of self-correction attempts to the size of the test set, while self-correction Contrib. is deter- mined by the number of instances in which these attempts successfully resulted in the correct answer.Table 2 presents a detailed analysis of the model's self-correction behaviors.Our analysis demonstrates that our fine-tuned refiner has a higher contribution to the final self-correction performance, explaining why it outperforms the prompting-based refiner (Madaan et al., 2023), as shown in Figure 2. Additionally, we find that our fine-tuned verifier and the gpt-4 verifier maintain a more reasonable frequency of self-correction, striking a better balance between correction attempts and accuracy.</p>
<p>Ablation Studies</p>
<p>In order to validate the various design decisions made in constructing our pipeline, we have conducted a series of ablation studies.The key findings from Table 3 can be summarized as follows.1) It is easier for the LM to identify only the first erroneous step, as the performance drops if we challenge it to critique every step.2) There is no need to separate the SELF-REFINE process into two modules-one for generating critiques and another for corrections.Such a separation not only increases the system's complexity and delays inference but also leads to a diminished final accuracy.</p>
<p>Scaling with Fine-tuning Data Size</p>
<p>We investigate the data-efficiency for refiner finetuning.Figure 4 plots the size of the refiner finetuning data against the final accuracy with different verification settings on GSM8K.We fine-tune the LLaMA-2-13B-chat base model on a random subset (varying from 25% to 75%) of the 14,499 total critique-corrections as previously shown in Table 1.We find that our refiner benefits from more finetuning data when paired with strong verifiers (oracle labels or gpt-4).Yet this effect is not observed when using a weak self-verifier, again highlighting the importance of verification for self-correction.We find that increasing the fine-tuning dataset size yields accuracy improvements up to a certain point.For instance, beyond approximately 10k examples (representing 75% of the SCORE fine-tuning data), the performance does not further improve.</p>
<p>Related Work</p>
<p>Training Small Language Model to Self-Correct.</p>
<p>Recent work shows that smaller language model can be fine-tuned on task-specific data to perform self-correct.But existing methods rely either on distilled data from stronger models (An et al., 2023;Yu et al., 2023b;Han et al., 2024;Ye et al., 2023;Zhang et al., 2024) or template-based critiques (Paul et al., 2023;Welleck et al., 2023).Our approach differs from prior studies in this domain as we gather natural language critiques from a small language model without relying on larger models or task-specific heuristics.Furthermore, we split the self-correction process into two phases: (SELF-)VERIFY and SELF-REFINE.This separation contrasts with earlier approaches that often merge the two skills, which not only obscures the true abilities of these models in each respective skill but also complicates the training process.In a nutshell, we demonstrate that strong verifiers unleash the power of small LMs to SELF-REFINE.</p>
<p>Bootstrapping Reasoning in Language Models.As language models become more powerful, human supervision may not be sufficient to improve these models.This trend calls for selfimproving LMs that provide training signals for themselves (Zelikman et al., 2022;Gülçehre et al., 2023;Yuan et al., 2023;Wang et al., 2023;Chen et al., 2024).The bootstrapping methods often involve iteratively fine-tuning a base LM on its selfgenerated examples that obtain a high reward value for correctness, helpfulness, or other desired properties.The bootstrapping process can further leverage label-free data (Huang et al., 2023a;Li et al., 2023a;Yuan et al., 2024) by generating pseudo labels using LLMs themselves.We draw inspiration from this family of methods and bootstrap the selfcorrection ability of smaller LMs.Our method is complementary to the rejection-sampling finetuning approach and can further improve reasoning performance upon that.</p>
<p>Verifying Reasoning.Verification of reasoning chains involves judging the correctness of the final answer and each reasoning steps.The verifier is often used to rerank multiple over-generated solutions and select the best one as the final output (Cobbe et al., 2021), or guide the LLM decoding through the search space for correct reasoning paths (Khalifa et al., 2023).We leverage a verifier to determine when to self-correct.Verifiers come at different granularity, including process/step-based (Uesato et al., 2022b;Li et al., 2023b;Lightman et al., 2023) and outcome-based supervision (Cobbe et al., 2021;Yu et al., 2023a;Hosseini et al., 2024).We use the latter since it is easier to construct labels automatically.Besides training a verifier with supervision signals, LLMs can also be few-shot prompted to become a verifier (Weng et al., 2023;Madaan et al., 2023;Zhou et al., 2023;Asai et al., 2023).We also explores the possibility of LLM-as-verifier, and demonstrate its usage for self-correction.We highlight the importance of verification in the context of selfcorrection, which echoes the recent finding that LLMs can successfully solve a problem, but cannot verify the reasoning (Gu et al., 2024;Oh et al., 2024;West et al., 2023).This calls for more effort for building evaluation benchmarks (Jacovi et al., 2024;Chen et al., 2023a;Mao et al., 2024;Lightman et al., 2023) and developing methods (Nguyen et al., 2024;Xu et al., 2024;Hosseini et al., 2024) to improve the reasoning verification.</p>
<p>Conclusion</p>
<p>In this study, we investigate how to leverage minimal signals from strong LMs to teach small LMs to self-correct their reasoning.We propose the SCORE method to collect self-correction finetuning data solely from small LMs.We find that SCORE-fine-tuned small LMs become better refiner models without relying on knowledge distillation from stronger LMs, yet they still need strong verifiers to be successful at self-correcting their reasoning errors.Our results highlight that the self-verification limitation of LMs currently poses an obstacle to the advancement of intrinsic selfcorrection in reasoning and thus warrants future research.</p>
<p>Limitations</p>
<p>Generating large amounts of synthetic data from smaller LMs requires intensive GPU computations, yet it removes the reliance on proprietary API models.Comparing the cost-efficiency of these two approaches will help us better trade-off between data generated from smaller LMs and larger LMs.Introducing the verifier and refiner during inference also causes additional latency, which we discuss in Appendix E.</p>
<p>A Prompts</p>
<p>8-Shot CoT Prompts with 2 Shots Shown Below</p>
<p>Q: Jason had 20 lollipops.He gave Denny some lollipops.Now Jason has 12 lollipops.How many lollipops did Jason give to Denny?Explain your reasoning step by step.Your final answer should be a single numerical number, in the form answer .End your response with [END].</p>
<p>Step 1: Jason originally had 20 lollipops.</p>
<p>Step 2: After giving some to Denny, Jason has 12 lollipops.</p>
<p>Step 3: The number of lollipops Jason gave to Denny is the difference between the original number and the remaining number, which is 20 -12 = 8 lollipops.</p>
<p>Step 4: The answer is 8 lollipops.</p>
<p>[END]</p>
<p>Q: There are 15 trees in the grove.Grove workers will plant trees in the grove today.After they are done, there will be 21 trees.How many trees did the grove workers plant today?Explain your reasoning step by step.Your final answer should be a single numerical number, in the form answer .End your response with [END].</p>
<p>Step 1: Initially, there are 15 trees in the grove.</p>
<p>Step 2: After planting, the total number of trees in the grove is 21.</p>
<p>Step 3: The number of trees planted by the grove workers today is the difference between the final number and the initial number of trees, which is 21 -15 = 6 trees.</p>
<p>Step 4: The answer is 6 trees.[END] 2-Shot Critique Prompts with 1 Shot Shown Below Q: Betty is saving money for a new wallet which costs $100.Betty has only half of the money she needs.Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents.How much more money does Betty need to buy the wallet?Answer 1 (Incorrect):</p>
<p>Step 1: Betty needs $100 to buy the wallet.</p>
<p>Step 2: Her parents gave her $15, which is half of the remaining amount....</p>
<p>Step 7: The answer is $55 .</p>
<p>Answer 2 (Correct):</p>
<p>Step 1: Betty needs $100 to buy the wallet, but she currently has only half of the money, which is $50.</p>
<p>Step 2: Her parents gave her $15, so she now has $50 + $15 = $65....</p>
<p>Step 6: The answer is 5 dollars.</p>
<p>There are reasoning errors in Answer 1. Please go through each step in Answer 1, use Answer 2 as a reference for the correct approach, and provide feedback that helps correct the errors in Answer 1. End your response with [END].</p>
<p>Let's go through the errors in Answer 1 and provide feedback: Answer 1 (Incorrect):</p>
<p>Step 1: Betty needs $100 to buy the wallet.Feedback: This step is correct.</p>
<p>Step 2: Her parents gave her $15, which is half of the remaining amount.Feedback: This is incorrect.Betty's parents gave her $15, but it's not necessarily half of the remaining amount.The question states that Betty has only half of the money she needs, not half of the remaining amount after her parents' contribution.</p>
<p>...</p>
<p>Step 7: The answer is $55 .Feedback: The correct answer, based on the corrected calculations, should be $5 .[END] 2-Shot Correction Prompts with 1 Shot Shwon Below Q: Kelly is grocery shopping at a supermarket and is making sure she has enough in her budget for the items in her cart.Her 5 packs of bacon cost $10 in total and she has 6 packets of chicken which each cost twice as much as a pack of bacon.She also has 3 packs of strawberries, priced at $4 each, and 7 packs of apples, each priced at half the price of a pack of strawberries.If Kelly's budget is $65 then how much money, in dollars, does she have left in her budget?</p>
<p>Step 1: Kelly spends $10 in total for 5 packs of bacon.Feedback: This step is correct.</p>
<p>Step 2: Each packet of chicken costs twice as much as a pack of bacon, which is 2 x $10 = $20.Feedback: Each pack of chicken costs twice as much as a pack of bacon, not the total cost of bacon.So we need to calculate the cost of one pack of bacon first (total cost of bacon / total bags of bacon) and use that.</p>
<p>...</p>
<p>Step 7: The answer is $29 .</p>
<p>Can you correct the errors in your reasoning based on the feedback provided?Your final answer should be a single numerical number, in the form answer .End your response with [END].</p>
<p>Step 1: Kelly spends $10 in total for 5 packs of bacon, so each pack of bacon costs $10 / 5 = $2.</p>
<p>Step 2: Each packet of chicken costs twice as much as a pack of bacon, which is 2 x $2 = $4.Since she has 6 packets of chicken, she spends 6 x $4 = $24 on chicken....</p>
<p>Step 7: The answer is $5 .[END] 4-Shot Verifying Prompts with 2 Shots Shown Below Question: Betty is saving money for a new wallet which costs $100.Betty has only half of the money she needs.Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents.How much more money does Betty need to buy the wallet?Solution:</p>
<p>Step 1: Betty needs $100 to buy the wallet.</p>
<p>Step 2: Her parents gave her $15, which is half of the remaining amount.</p>
<p>Step 3: Her grandparents gave her twice as much as her parents, which is 2 x $15 = $30.</p>
<p>Step 4: The total amount of money Betty has now is $15 + $30 = $45.</p>
<p>Step 5: The difference between the amount she needs and the amount she has is $100 -$45 = $55.</p>
<p>Step 6: Betty still needs $55 to buy the wallet.</p>
<p>Step 7: The answer is $55 .Is this solution correct or not?Answer <strong>correct</strong> or <strong>incorrect</strong> and explain your reasoning.End your response with [END].</p>
<p>The solution is <strong>incorrect</strong>.You've ignored the original amount that Betty had saved, which is half the wallet's cost ($100/2 = $50).So, before receiving money from her parents and grandparents, she already had $50.[END] Question: Betty is saving money for a new wallet which costs $100.Betty has only half of the money she needs.Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents.How much more money does Betty need to buy the wallet?Solution:</p>
<p>Step 1: Betty needs $100 to buy the wallet, but she currently has only half of the money, which is $50.</p>
<p>Step 2: Her parents gave her $15, so she now has $50 + $15 = $65.</p>
<p>Step 3: Her grandparents gave her twice as much as her parents, so she received $15 x 2 = $30 from her grandparents.</p>
<p>Step 4: Betty now has a total of $65 + $30 = $95.</p>
<p>Step 5: To find out how much more money Betty needs, subtract the amount she has from the amount she needs, which is $100 -$95 = $5.</p>
<p>Step 6: The answer is 5 dollars.</p>
<p>Is this solution correct or not?Answer <strong>correct</strong> or <strong>incorrect</strong> and explain your reasoning.End your response with [END].</p>
<p>B Proof: Using Correct Solutions as Hints Make Critique Generation Easier</p>
<p>Intuitively, if we provide the LM with a correct solution, it will be easier for the LM to generate a critique for the incorrect solution of the same question.In fact, we can verify this intuition from a mathematical perspective.Given a pair of incorrectcorrect solutions (s − , s + ), our goal is to learn a mapping from the incorrect solution to the correct one, which is modeled by M (s + |q, s − ), where M (•) is the probability distribution of the base LM.By introducing critique c as intermediate generations, we have:
M (s + |q, s − ) = M (c|q, s − ) • M (s + |q, s − , c) (1)
The optimal critique c * that we want to find should be best at facilitating this two-phase generation process, i.e.,
c * = argmax c M (c|q, s − ) • M (s + |q, s − , c) (2)
To obtain the optimal critique, we can first ask the model to reverse-engineer a critique ĉ with the correct solution s + as a hint:
ĉ = argmax c M (c|q, s − , s + )(3)
With Bayes' theorem, Equation 3 can be re-written as:
ĉ = argmax c M (c|q, s − , s + ) = argmax c M (c|q, s − ) • M (s + |q, s − , c) M (s + |q, s − ) = argmax c M (c|q, s − ) • M (s + |q, s − , c) = c * (4)
which is exactly what we want.</p>
<p>This simple proof shows that in principle, the critique generated with the correct solution as a hint should be best at guiding the LM to recover the correct solution from the incorrect one.</p>
<p>C Combining SCORE with Rejection</p>
<p>Sampling Fine-Tuning</p>
<p>Aside from self-correction upon the initial solution generated by the base LM as shown in Figure 2, we also explore whether our proposed self-correction method can be combined with other fine-tuning methods (e.g., rejection sampling fine-tuning) to further improve reasoning performance.Here we  replace the base LM with the RFT model.Rejection sampling fine-tuning leverages correct generations for training.Yet it ignores rich information in the large amounts of incorrect solutions.We hope weaker LMs can also learn from its own mistakes to become better reasoners.Since we have already obtained a stronger RFT model that avoids some mistakes by base LMs that are easier to fix, we want our refiner to learn to correct errors that require more in-depth thinking.Consequently, to collect the incorrect solutions to be reflected upon, we resample 10 solutions for each question from the RFT model, instead of reusing the sampled solutions from the base LM in step (a) of Figure 1.</p>
<p>Then we follow the rest of the pipeline to collect critique-correction data for refiner finetuning.Table 4 shows that our method is complementary to RFT.Concretely, RFT improves the final accuracy upon the few-shot prompting baseline from 37.2 to 42.7, while our self-correction system (gpt-4 as verifier and our finetuned LLaMA as refiner) can further improve the performance from 42.7 to 45.1, demonstrating the effectiveness of our method over the few-shot prompting and RFT baselines.</p>
<p>Table 5 investigates the optimal approach for ini- Table 6: Performance of SCORE models using LLaMA-2-13B-chat and Gemma-7B-it as base LM.We report F1 score of the verifiers (V.F1) and final answer accuracy (ACC).We include test results for training tasks (GSM8K and CommonsenseQA/CSQA), as well as transfer evaluation of GSM8K trained models on MATH subset, CSQA trained models on QASC and RiddleSense.All models use greedy decoding.We highlight the best-performing system per model without using an oracle verifier.On each dataset, the superior model among the highlighted ones is indicated in bold.</p>
<p>tializing the parameters of the refiner and verifier during fine-tuning.Our empirical findings indicate that initializing the verifier from the RFT model and the refiner from the base language model results in superior performance.We hypothesize that stronger reasoning capabilities of RFT model could complement the verification skills.Furthermore, when generating critiques of solutions, those created using solutions by the RFT model outperform those based on the base model.</p>
<p>D Combining SCORE with Oversample-then-Rerank</p>
<p>Self-correction could be combined with oversample-then-rerank, an orthogonal approach to further improve reasoning (Cobbe et al., 2021;Li et al., 2023b;Hosseini et al., 2024).The oversample-then-rerank approach first samples k solutions per question by few-shot prompting, then leverages a trained verifier to score the correctness of each solution, and selects the final answer based on a weighted voting scheme (Li et al., 2023b) as follows:
ŷ = arg max y k i=1 1 y i =y • p v (q, s i , y i ),
where q is the question, s i is the i-th sampled solutions, y i is the final answer extracted from s i , and 1 y i =y is an indicator function that returns 1 (or 0) if y i = y (or not), and p v (•) is the probability produced by the verifier 6 To integrate self-correction with the oversamplethen-rerank approach, we apply the SCORE refiner to solutions predicted to be correct with a probability below a certain threshold.This threshold is automatically determined to maximize accuracy on the development set and is then fixed for use during test-time predictions.The refiner greedily decodes one refined solution for each solution.Subsequently, we combine the refined solutions with the original solutions predicted as correct to arrive at the final answer using weighted voting for aggregation: 6 We find that for answer aggregation, weighted voting performs better than majority voting and selecting the top-1 solution, which echoes the findings of Li et al. (2023b).Therefore, we report the weighted voting results in this section.Table 7: Self-correction combined with the oversamplethen-rerank strategy (Cobbe et al., 2021) yield better results than using either one alone.We show results on GSM8K test set and transfer evaluation of GSM8K trained models on MATH subset using LLaMA-2-13B-chat and Gemma-7B-it as base LMs.
ŷ = arg max y k i=1 1 y i =y • p i v • 1 p i v ≥C + 1 y ′ i =y • p i ′ v • 1 p i v &lt;C ,
where p i v = p v (q, s i , y i ) is the verifier probability for the original solution s i , p i ′ v = p v (q, s ′ i , y ′ i ) is the verifier probability for the refined solution s ′ i coming from s i , and C is a threshold for determining when to self-refine.</p>
<p>We sample k = 10 solutions for each question with a temperature of 0.9 and use the same finetuned self-verifier (not gpt-4 or oracle verifier) across all the settings in Table 7 that involve a verifier.As shown by previous work, oversamplethen-rerank improves performance upon few-shot prompting with only greedy decoding by a large margin, because it better explores the solution space by sampling multiple solutions.Our selfcorrection method is both orthogonal to and synergistic with the oversample-then-rerank.By incorporating our self-correction method into the baseline, with only a weak self-verifier, we still observe an improvement in accuracy compared to the baseline: +1.7 for LLaMA-2-13B-chat on GSM8K, and +1.6 for GSM8K-trained Gemma-7B-it on MATH.By aggregating verifications from multiple solutions, the weak verifiers become stronger via ensembling and thus unleash the power of refiner for better self-correction of reasoning.</p>
<p>E Inference Overhead by SCORE</p>
<p>Although we employ three models for inference (few-shot prompted solution generator, self-verifier, self-refiner), it will not necessarily triple the latency.The self-verifier's latency is minimal because it classifies rather than generates, and the refiner is invoked only if the verifier flags the initial solution as incorrect.The increased inference time of our pipeline over the base model is modest, at x1.3 times for LLaMA-2-13B-chat and x1.4 times for Gemma-7B-it.Comparatively, the oversamplethen-rerank baseline (Appendix D) results in a 4.5 times increase in latency when sampling 10 solutions per question.</p>
<p>Figure 1 :
1
Figure 1: Illustration of the SCORE pipeline to generate critique-correction data from a small LM (step a-c) and fine-tune the same LM to self-correct its reasoning errors (step d), without distilling any data from stronger LMs.</p>
<p>Figure 3 :
3
Figure 3: Confusion matrices of the predictions by the verifier and the refiner on GSM8K test set.The base LM is LLaMA-2-13B-chat."True label" means the correctness of the initial solution.The predicted label of the verifier represents whether the verifier judges it as correct (1) or incorrect (0).The predicted label of the verifier + refiner is the correctness of the final answer.The strong verifier (gpt-4) makes fewer false positive predictions than the weak self-verifier and unleashes the potential of the small LM to revise an incorrect answer into a correct one more likely than the other way around.</p>
<p>Figure 4 :
4
Figure4: Final accuracy on GSM8K dev set w.r.t percentage of refiner fine-tuning data used and the type of verifier.The base LM is LLaMA-2-13B-chat.Under a strong verifier setting, our refiner brings greater performance gain with more fine-tuning data.Yet the performance plateaus with a weak verifier.</p>
<p>Table 1 :
1
Statistics of the critique data generated from our SCORE pipeline.Although Gemma-7B has fewer data left after filtering, it still achieves greater improvement than LLaMA-13B by self-correction (Section 5.1), suggesting that Gemma-7B is more effective at learning self-correction from SCORE.
GSM8KCSQA#%#%Base LM: LLaMA-2-13b-chatRaw critiques56,843 100.0 42,705 100.0After rule-based filering36,337 63.9 36,436 85.3After prompting filtering (for SCORE fine-tuning)14,499 25.5 24,511 57.4Base LM: Gemma-7b-itRaw critiques52,669 100.0 40,604 100.0After rule-based filering17,209 32.7 35,929 88.5After prompting filtering (for SCORE fine-tuning)4,623 8.8 12,972 31.9</p>
<p>Table 2 :
2VerifierRefinerGSM8KCSQAFreq. Contrib. Freq. Contrib.Base LM: LLaMA-2-13b-chatprompted3.710.217.519.6selfprompted2.72.91.240.0oracle62.84.030.346.2self gpt-4 oracleSCORE (fine-tuned)19.0 63.3 62.810.8 15.6 14.03.0 38.8 30.333.3 40.9 54.3Base LM: Gemma-7b-itprompted18.721.520.245.3selfprompted9.99.92.942.9oracle63.72.132.822.4self gpt-4 oracleSCORE (fine-tuned)27.9 63.4 63.714.1 17.1 17.40.6 38.6 32.857.1 48.4 55.6
Analysis of self-correction behaviors.The settings are the same as those in Figure2.Freq.(inpercentage)means the frequency with which the verifier decides to self-correct.Contrib.(inpercentage)refers to the extent to which these self-correction attempts enhance the model's task performance.A strong verifier (e.g., gpt-4) maintains a balanced frequency of selfcorrection, ideally similar to that of the oracle verifier, and thus enhance the final accuracies in Figure2.Additionally, the SCORE refiner possesses stronger refinement capabilities, as indicated by a higher contribution score than the prompted refiner.</p>
<p>Table 4 :
4
Performance of self-correction with different inputs on GSM8K test set using LLaMA-2-13B-chat as base LM.The initial solution is generated by the rejection-sampling fine-tuned model.
VerifierRefinerRefinerparam.param.ft. onFinalOracleinit.init.solutionsAccu.Accu.fromfromfromInitial solutions by RFT model 44.9 44.9RFTbaseRFT45.3 53.3basebaseRFT45.0 53.3RFTRFTRFT45.2 52.3baseRFTRFT45.0 52.3RFTbasebase45.1 53.3</p>
<p>Table 5 :
5
Ablation study on parameter initialization and refiner's fine-tuning data source.Results are shown on GSM8K dev set with LLaMA-2-13B-chat as base LM.</p>
<p>The total number of incorrect-correct solution pairs could be very large so we sample only one critique per pair. This has already provided a sufficient amount of SCORE fine-tuning data after filtering.
This subset includes a total 181 problems of Level 1 difficulty in MATH with question types of algebra, Counting &amp; probability, prealgebra and number theory, where the final answer is a number and no latex exists in the question.
  5  The test labels of these datasets are hidden, so we use the original dev set as our test set, followingKojima et al. (2022);Kim et al. (2023).
AcknowledgementsThis work is supported by LG AI Research.We also thank ARR reviewers for their useful feedback.
Learning from mistakes makes LLM better reasoner. Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, Weizhu Chen, 10.48550/ARXIV.2310.20689CoRR, abs/2310.206892023</p>
<p>Self-rag: Learning to retrieve, generate, and critique through self-reflection. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi, 10.48550/ARXIV.2310.11511CoRR, abs/2310.115112023</p>
<p>FELM: benchmarking factuality evaluation of large language models. Shiqi Chen, Yiran Zhao, Jinghan Zhang, I-Chun Chern, Siyang Gao, Pengfei Liu, Junxian He, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2023a. 2023. 2023. December 10 -16, 2023</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, 10.48550/ARXIV.2304.05128CoRR, abs/2304.051282023b</p>
<p>Self-play fine-tuning converts weak language models to strong language models. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, Quanquan Gu, 10.48550/ARXIV.2401.01335CoRR, abs/2401.013352024</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, CoRR, abs/2110.141682021</p>
<p>The counterfeit conundrum: Can code language models grasp the nuances of their incorrect generations?. Alex Gu, Wen-Ding Li, Naman Jain, Theo X Olausson, Celine Lee, Koushik Sen, Armando Solar-Lezama, 10.48550/ARXIV.2402.19475CoRR, abs/2402.194752024</p>
<p>Orhan Firat, and Nando de Freitas. 2023. Reinforced self-training (rest) for language modeling. Çaglar Gülçehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, 10.48550/ARXIV.2308.08998CoRR, abs/2308.08998</p>
<p>Small language model can self-correct. Haixia Han, Jiaqing Liang, Jie Shi, Qianyu He, Yanghua Xiao, 10.48550/ARXIV.2401.07301CoRR, abs/2401.073012024</p>
<p>Measuring mathematical problem solving with the MATH dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021. the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 20212021. December 2021</p>
<p>V-star: Training verifiers for self-taught reasoners. Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron C Courville, Alessandro Sordoni, Rishabh Agarwal, 10.48550/ARXIV.2402.06457CoRR, abs/2402.064572024</p>
<p>Lora: Low-rank adaptation of large language models. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. 2022. April 25-29, 2022OpenReview.net</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023a. December 6-10, 2023</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, 10.48550/ARXIV.2310.01798CoRR, abs/2310.017982023b</p>
<p>A chain-of-thought is as strong as its weakest link: A benchmark for verifiers of reasoning chains. Alon Jacovi, Yonatan Bitton, Bernd Bohnet, Jonathan Herzig, Or Honovich, Michael Tseng, Michael Collins, Roee Aharoni, Mor Geva, 10.48550/ARXIV.2402.00559CoRR, abs/2402.005592024</p>
<p>GRACE: Discriminator-guided chain-of-thought reasoning. Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, Lu Wang, 10.18653/v1/2023.findings-emnlp.1022Findings of the Association for Computational Linguistics: EMNLP 2023. Association for Computational Linguistics2023Singapore</p>
<p>The Thirty-Second Innovative Applications of Artificial Intelligence Conference. Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, Ashish Sabharwal, 10.1609/AAAI.V34I05.6319The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence. New York, NY, USAAAAI Press2020. February 7-12, 20202020The Thirty-Fourth AAAI Conference on Artificial Intelligence</p>
<p>Language models can solve computer tasks. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2023. 2023. 2023. December 10 -16, 2023</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2022. 2022. 2022. November 28 -December 9, 2022</p>
<p>Self-alignment with instruction backtranslation. Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, Mike Lewis, 10.48550/ARXIV.2308.06259CoRR, abs/2308.062592023a</p>
<p>Making language models better reasoners with step-aware verifier. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, 10.18653/V1/2023.ACL-LONG.291Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023b. July 9-14, 2023ACL 2023</p>
<p>Let's verify step by step. Vineet Hunter Lightman, Yura Kosaraju, Harrison Burda, John Edwards ; Leike, Ilya Schulman, Karl Sutskever, Cobbe, 10.48550/ARXIV.2305.20050CoRR, abs/2305Jan. 2023. 20050Bowen Baker, Teddy Lee</p>
<p>Riddlesense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge. Ziyi Bill Yuchen Lin, Yichi Wu, Dong-Ho Yang, Xiang Lee, Ren, 10.18653/V1/2021.FINDINGS-ACL.131Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021. Association for Computational Linguistics2021. August 1-6, 2021ACL/IJCNLP 2021 of Findings of ACL</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Prasad Bodhisattwa, Shashank Majumder, Amir Gupta, Peter Yazdanbakhsh, Clark, 10.48550/ARXIV.2303.17651CoRR, abs/2303.176512023</p>
<p>CHAMP: A competition-level dataset for fine-grained analyses of llms' mathematical reasoning capabilities. Yujun Mao, Yoon Kim, Yilun Zhou, 10.48550/ARXIV.2401.06961CoRR, abs/2401.069612024</p>
<p>Direct evaluation of chainof-thought in multi-hop reasoning with knowledge graphs. Minh-Vuong Nguyen, Linhao Luo, Fatemeh Shiri, Dinh Phung, Yuan-Fang Li, Thuy-Trang Vu, Gholamreza Haffari, 10.48550/ARXIV.2402.11199CoRR, abs/2402.111992024</p>
<p>The generative AI paradox on evaluation: What it can solve, it may not evaluate. Juhyun Oh, Eunsu Kim, Inha Cha, Alice Oh, 10.48550/ARXIV.2402.06204CoRR, abs/2402.062042024</p>
<p>10.48550/ARXIV.2303.08774CoRR, abs/2303.08774GPT-4 technical report. 2023OpenAI</p>
<p>Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, William Yang, Wang , 10.48550/ARXIV.2308.03188CoRR, abs/2308.031882023</p>
<p>Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, Boi Faltings, 10.48550/ARXIV.2304.01904CoRR, abs/2304.01904REFINER: reasoning feedback on intermediate representations. 2023</p>
<p>Self-critiquing models for assisting human evaluators. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, 10.48550/ARXIV.2206.05802CoRR, abs/2206.05802Jan Leike. 2022Jonathan Ward</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, CoRR, abs/1811.009372018</p>
<p>Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, arXiv:2403.08295Gemma: Open models based on gemini research and technology. 2024arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov ; Zheng Yan, Iliyan Zarov, Yuchen Zhang, 10.48550/ARXIV.2307.09288Pushkar Mishra, Igor Molybog. Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurélien Rodriguezand Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. CoRR, abs/2307.09288</p>
<p>Solving math word problems with process-and outcomebased feedback. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, Irina Higgins, arXiv:2211.142752022aarXiv preprint</p>
<p>Solving math word problems with process-and outcome-based feedback. Jonathan Uesato, Nate Kushman, Ramana Kumar, H Francis Song, Noah Y Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, Irina Higgins, 10.48550/ARXIV.2211.14275CoRR, abs/2211.142752022b</p>
<p>Making large language models better reasoners with alignment. Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, Zhifang Sui, 10.48550/ARXIV.2309.02144CoRR, abs/2309.021442023</p>
<p>Generating sequences by learning to self-correct. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>
<p>Large language models are better reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational LinguisticsJun Zhao. 2023. December 6-10, 2023</p>
<p>The generative AI paradox: "what it can create, it may not understand. Peter West, Ximing Lu, Nouha Dziri, Faeze Brahman, Linjie Li, Jena D Hwang, Liwei Jiang, Jillian Fisher, Abhilasha Ravichander, Khyathi Chandu, Benjamin Newman, Pang Wei Koh, Allyson Ettinger, Yejin Choi, 10.48550/ARXIV.2311.00059CoRR, abs/2311.000592023</p>
<p>Finegrained human feedback gives better rewards for language model training. Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf, Hannaneh Hajishirzi, 10.48550/ARXIV.2306.01693CoRR, abs/2306.016932023</p>
<p>Can we verify step by step for incorrect answer detection?. Xin Xu, Shizhe Diao, Can Yang, Yang Wang, 10.48550/ARXIV.2402.10528CoRR, abs/2402.105282024</p>
<p>Do large language models latently perform multi-hop reasoning?. Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, Sebastian Riedel, 10.48550/ARXIV.2402.16837CoRR, abs/2402.168372024</p>
<p>Selfee: Iterative self-revising llm empowered by selffeedback generation. Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Minjoon Seo, 2023Blog post</p>
<p>Outcome-supervised verifiers for planning in mathematical reasoning. Fei Yu, Anningzhe Gao, Benyou Wang, 10.48550/ARXIV.2311.09724CoRR, abs/2311.097242023a</p>
<p>Teaching language models to selfimprove through interactive demonstrations. Xiao Yu, Baolin Peng, Michel Galley, Jianfeng Gao, Zhou Yu, 10.48550/ARXIV.2310.13522CoRR, abs/2310.135222023b</p>
<p>Self-rewarding language models. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, Jason Weston, 10.48550/ARXIV.2401.10020CoRR, abs/2401.100202024</p>
<p>Scaling relationship on learning mathematical reasoning with large language models. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, Chang Zhou, 10.48550/ARXIV.2308.01825CoRR, abs/2308.018252023</p>
<p>Attention satisfies: A constraint-satisfaction lens on factual errors of language models. Mert Yüksekgönül, Varun Chandrasekaran, Erik Jones, Suriya Gunasekar, Ranjita Naik, Hamid Palangi, Ece Kamar, Besmira Nushi, 10.48550/ARXIV.2309.15098CoRR, abs/2309.150982023</p>
<p>Star: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D Goodman, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2022. 2022. 2022. November 28 -December 9, 2022</p>
<p>Ultra: Unleash llms' potential for event argument extraction through hierarchical modeling and pair-wise refinement. Frederick Xinliang, Carter Zhang, Temma Blum, Shalin Choji, Alakananda Shah, Vempala, ArXiv, abs/2401.132182024</p>
<p>Prompt-driven llm safeguarding via directed representation optimization. Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang, Minlie Huang, Nanyun Peng, arXiv:2401.180182024aarXiv preprint</p>
<p>Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Yongqiang Ma, arXiv:2403.13372Llamafactory: Unified efficient fine-tuning of 100+ language models. 2024barXiv preprint</p>
<p>Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification. Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, Hongsheng Li, 10.48550/ARXIV.2308.07921CoRR, abs/2308.079212023</p>
<p>. Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J Zico Kolter, Dan Hendrycks, 10.48550/ARXIV.2310.014052023Representation engineering: A top-down approach to AI transparency. CoRR, abs/2310.01405</p>            </div>
        </div>

    </div>
</body>
</html>
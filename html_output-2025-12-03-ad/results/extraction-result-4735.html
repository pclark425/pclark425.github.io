<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4735 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4735</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4735</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-270045227</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.15092v2.pdf" target="_blank">Dissociation of Faithful and Unfaithful Reasoning in LLMs</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) often improve their performance in downstream tasks when they generate Chain of Thought reasoning text before producing an answer. We investigate how LLMs recover from errors in Chain of Thought. Through analysis of error recovery behaviors, we find evidence for unfaithfulness in Chain of Thought, which occurs when models arrive at the correct answer despite invalid reasoning text. We identify factors that shift LLM recovery behavior: LLMs recover more frequently from obvious errors and in contexts that provide more evidence for the correct answer. Critically, these factors have divergent effects on faithful and unfaithful recoveries. Our results indicate that there are distinct mechanisms driving faithful and unfaithful error recoveries. Selective targeting of these mechanisms may be able to drive down the rate of unfaithful reasoning and improve model interpretability.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4735.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4735.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 (gpt-4-0314 checkpoint)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large, multi-purpose transformer-based language model from OpenAI evaluated with deterministic decoding (temperature=0) using chain-of-thought prompts; used as the primary subject for detailed error-recovery experiments in math word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-0314)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fixed GPT-4 checkpoint (gpt-4-0314) accessed via OpenAI API; evaluated with deterministic decoding (temperature=0) on math word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (single deterministic CoT completion)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Single chain-of-thought transcript generated per question using a step-by-step prompt ('Let's think step by step'); experiments also included perturbing an existing CoT transcript (introducing numerical errors) and modifying the prompt/context to alter model expectations, but did not produce multiple diverse chains or ensembles.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Math word problems (MultiArith, ASDiv, SVAMP, GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard math word-problem benchmarks requiring multi-step arithmetic reasoning; datasets used: MultiArith, ASDiv, SVAMP, and GSM8K.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Models (including GPT-4) recovered from introduced numerical errors substantially more often for large-magnitude errors than small (p < 0.001). For GPT-4 specifically, larger errors produced higher overall recovery, with a significant increase in faithful recoveries and a decrease in unfaithful recoveries (statistical significance reported p < 0.001). Exact dataset-level percentages are reported in the paper (e.g., aggregate increases in faithful recovery for large vs small errors; see Table 2 / Figures 3–4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a single deterministic chain-of-thought, GPT-4 exhibits two dissociable modes: faithful recoveries (explicitly identify/fix errors or re-do calculations) and unfaithful recoveries (reach correct answer without textual evidence). Interventions that increase perceptibility of errors (large perturbation) or increase prior expectation of errors (context noise or explicit prompting) increase total recovery rates but have opposite effects on faithful vs unfaithful behavior for GPT-4: e.g., large errors and context noise increased faithful recovery while decreasing unfaithful recovery (both effects significant for GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>While GPT-4 showed more faithful recoveries with larger errors, other models (Claude-3 Opus and Llama-3) showed increases in unfaithful recoveries under some interventions; also, unfaithful recoveries were less sensitive to evidence available in prior context (Experiment 3), indicating cases where the single CoT output is not reliably indicative of the model's internal decision process.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dissociation of Faithful and Unfaithful Reasoning in LLMs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4735.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4735.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3 Opus</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anthropic Claude-3 Opus (claude-3-opus-20240229)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Claude-3 Opus chat model from Anthropic evaluated with deterministic decoding on math word problems and perturbed chain-of-thought transcripts to study error recovery and faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3 Opus (claude-3-opus-20240229)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic Claude-3 Opus chat model used in fixed-version evaluation via Anthropic API; deterministic decoding (temperature=0); evaluated on several math word-problem datasets with CoT prompting and perturbed CoT transcripts.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (single deterministic CoT completion)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Single CoT transcript generation per item using a standard step-by-step prompt; experiments included injecting numerical errors into the provided CoT and altering the prompt/context to encourage error-checking but did not create multiple diverse reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Math word problems (MultiArith, ASDiv, SVAMP, GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step arithmetic word-problem benchmarks requiring sequential numerical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Claude-3 Opus recovered more often from large than small errors overall (p < 0.001). However, unlike GPT-4, larger error magnitude tended to decrease faithful recoveries and substantially increase unfaithful recoveries (significant differences reported: p < 0.05 to p < 0.001 across contrasts). Context noise increased total recovery, but led to increased unfaithful and slightly decreased faithful recoveries for Claude-3 Opus (significant for some contrasts).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Under single-chain CoT prompting, Claude-3 Opus shows a dissociation between faithful and unfaithful recoveries: interventions (large errors, context noise) often amplified unfaithful recoveries rather than faithful ones, indicating that manipulations that raise recovery rates do not uniformly improve faithfulness of the textual reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Claude-3 Opus exhibited cases where increased perceptibility of errors (large perturbations) led to more unfaithful recoveries rather than more faithful corrections, demonstrating that greater recovery frequency can coincide with less interpretable/faithful reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dissociation of Faithful and Unfaithful Reasoning in LLMs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4735.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4735.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3 70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta Llama-3 70B Chat (meta-llama/Llama-3-70b-chat-hf)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70-billion-parameter Llama-3 chat model evaluated deterministically on math word problems with chain-of-thought prompting and perturbed CoT transcripts to measure error recovery and faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3 70B Chat (meta-llama/Llama-3-70b-chat-hf)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 70B-parameter Llama-3 chat model used via Together AI API; deterministic decoding (temperature=0); evaluated on math word-problem datasets with CoT prompting and errored-CoT interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (single deterministic CoT completion)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Single-chain CoT transcripts produced per item (standard step-by-step prompt); experiments included inserting numerical errors and changing context/prompt to affect expectations, but did not generate multiple diverse chains or ensemble outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Math word problems (MultiArith, ASDiv, SVAMP, GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks of arithmetic word problems requiring multi-step numeric reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Llama-3 recovered more frequently from large than small errors overall (p < 0.001). Large error magnitude led to a small increase in faithful recovery but a large increase in unfaithful recovery; context noise also tended to increase unfaithful recovery with only slight decreases or small increases in faithful recovery (effects smaller than Claude-3 but statistically observed in several contrasts).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Like the other models, Llama-3 demonstrates a dissociation between faithful and unfaithful recoveries under single CoT prompting: manipulations that increase overall recovery (error size, context noise) often increase unfaithful recoveries disproportionately, indicating textual reasoning can be misleading about the model's true computation.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Instances where context noise or larger errors increased unfaithful recoveries (i.e., more frequent correct final answers with no valid textual argument) show that similar single-chain CoT methods can yield higher apparent performance while reducing faithfulness/interpretability of the chain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dissociation of Faithful and Unfaithful Reasoning in LLMs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Shapley value attribution in chain of thought <em>(Rating: 2)</em></li>
                <li>Measuring faithfulness in chain-of-thought reasoning <em>(Rating: 2)</em></li>
                <li>Llms with chain-of-thought are non-causal reasoners <em>(Rating: 2)</em></li>
                <li>Faith and fate: Limits of transformers on compositionality <em>(Rating: 1)</em></li>
                <li>Faithfulness vs. plausibility: On the (un)reliability of explanations from large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4735",
    "paper_id": "paper-270045227",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4 (gpt-4-0314 checkpoint)",
            "brief_description": "A large, multi-purpose transformer-based language model from OpenAI evaluated with deterministic decoding (temperature=0) using chain-of-thought prompts; used as the primary subject for detailed error-recovery experiments in math word problems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-0314)",
            "model_description": "Fixed GPT-4 checkpoint (gpt-4-0314) accessed via OpenAI API; evaluated with deterministic decoding (temperature=0) on math word problems.",
            "reasoning_method_name": "Chain-of-Thought (single deterministic CoT completion)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Single chain-of-thought transcript generated per question using a step-by-step prompt ('Let's think step by step'); experiments also included perturbing an existing CoT transcript (introducing numerical errors) and modifying the prompt/context to alter model expectations, but did not produce multiple diverse chains or ensembles.",
            "task_name": "Math word problems (MultiArith, ASDiv, SVAMP, GSM8K)",
            "task_description": "Standard math word-problem benchmarks requiring multi-step arithmetic reasoning; datasets used: MultiArith, ASDiv, SVAMP, and GSM8K.",
            "performance": "Models (including GPT-4) recovered from introduced numerical errors substantially more often for large-magnitude errors than small (p &lt; 0.001). For GPT-4 specifically, larger errors produced higher overall recovery, with a significant increase in faithful recoveries and a decrease in unfaithful recoveries (statistical significance reported p &lt; 0.001). Exact dataset-level percentages are reported in the paper (e.g., aggregate increases in faithful recovery for large vs small errors; see Table 2 / Figures 3–4).",
            "comparison_with_other_method": false,
            "performance_other_method": null,
            "key_findings": "Using a single deterministic chain-of-thought, GPT-4 exhibits two dissociable modes: faithful recoveries (explicitly identify/fix errors or re-do calculations) and unfaithful recoveries (reach correct answer without textual evidence). Interventions that increase perceptibility of errors (large perturbation) or increase prior expectation of errors (context noise or explicit prompting) increase total recovery rates but have opposite effects on faithful vs unfaithful behavior for GPT-4: e.g., large errors and context noise increased faithful recovery while decreasing unfaithful recovery (both effects significant for GPT-4).",
            "counter_examples_or_negative_results": "While GPT-4 showed more faithful recoveries with larger errors, other models (Claude-3 Opus and Llama-3) showed increases in unfaithful recoveries under some interventions; also, unfaithful recoveries were less sensitive to evidence available in prior context (Experiment 3), indicating cases where the single CoT output is not reliably indicative of the model's internal decision process.",
            "uuid": "e4735.0",
            "source_info": {
                "paper_title": "Dissociation of Faithful and Unfaithful Reasoning in LLMs",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Claude-3 Opus",
            "name_full": "Anthropic Claude-3 Opus (claude-3-opus-20240229)",
            "brief_description": "A Claude-3 Opus chat model from Anthropic evaluated with deterministic decoding on math word problems and perturbed chain-of-thought transcripts to study error recovery and faithfulness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Claude-3 Opus (claude-3-opus-20240229)",
            "model_description": "Anthropic Claude-3 Opus chat model used in fixed-version evaluation via Anthropic API; deterministic decoding (temperature=0); evaluated on several math word-problem datasets with CoT prompting and perturbed CoT transcripts.",
            "reasoning_method_name": "Chain-of-Thought (single deterministic CoT completion)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Single CoT transcript generation per item using a standard step-by-step prompt; experiments included injecting numerical errors into the provided CoT and altering the prompt/context to encourage error-checking but did not create multiple diverse reasoning traces.",
            "task_name": "Math word problems (MultiArith, ASDiv, SVAMP, GSM8K)",
            "task_description": "Multi-step arithmetic word-problem benchmarks requiring sequential numerical reasoning.",
            "performance": "Claude-3 Opus recovered more often from large than small errors overall (p &lt; 0.001). However, unlike GPT-4, larger error magnitude tended to decrease faithful recoveries and substantially increase unfaithful recoveries (significant differences reported: p &lt; 0.05 to p &lt; 0.001 across contrasts). Context noise increased total recovery, but led to increased unfaithful and slightly decreased faithful recoveries for Claude-3 Opus (significant for some contrasts).",
            "comparison_with_other_method": false,
            "performance_other_method": null,
            "key_findings": "Under single-chain CoT prompting, Claude-3 Opus shows a dissociation between faithful and unfaithful recoveries: interventions (large errors, context noise) often amplified unfaithful recoveries rather than faithful ones, indicating that manipulations that raise recovery rates do not uniformly improve faithfulness of the textual reasoning.",
            "counter_examples_or_negative_results": "Claude-3 Opus exhibited cases where increased perceptibility of errors (large perturbations) led to more unfaithful recoveries rather than more faithful corrections, demonstrating that greater recovery frequency can coincide with less interpretable/faithful reasoning.",
            "uuid": "e4735.1",
            "source_info": {
                "paper_title": "Dissociation of Faithful and Unfaithful Reasoning in LLMs",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Llama-3 70B",
            "name_full": "Meta Llama-3 70B Chat (meta-llama/Llama-3-70b-chat-hf)",
            "brief_description": "A 70-billion-parameter Llama-3 chat model evaluated deterministically on math word problems with chain-of-thought prompting and perturbed CoT transcripts to measure error recovery and faithfulness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3 70B Chat (meta-llama/Llama-3-70b-chat-hf)",
            "model_description": "A 70B-parameter Llama-3 chat model used via Together AI API; deterministic decoding (temperature=0); evaluated on math word-problem datasets with CoT prompting and errored-CoT interventions.",
            "reasoning_method_name": "Chain-of-Thought (single deterministic CoT completion)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Single-chain CoT transcripts produced per item (standard step-by-step prompt); experiments included inserting numerical errors and changing context/prompt to affect expectations, but did not generate multiple diverse chains or ensemble outputs.",
            "task_name": "Math word problems (MultiArith, ASDiv, SVAMP, GSM8K)",
            "task_description": "Benchmarks of arithmetic word problems requiring multi-step numeric reasoning.",
            "performance": "Llama-3 recovered more frequently from large than small errors overall (p &lt; 0.001). Large error magnitude led to a small increase in faithful recovery but a large increase in unfaithful recovery; context noise also tended to increase unfaithful recovery with only slight decreases or small increases in faithful recovery (effects smaller than Claude-3 but statistically observed in several contrasts).",
            "comparison_with_other_method": false,
            "performance_other_method": null,
            "key_findings": "Like the other models, Llama-3 demonstrates a dissociation between faithful and unfaithful recoveries under single CoT prompting: manipulations that increase overall recovery (error size, context noise) often increase unfaithful recoveries disproportionately, indicating textual reasoning can be misleading about the model's true computation.",
            "counter_examples_or_negative_results": "Instances where context noise or larger errors increased unfaithful recoveries (i.e., more frequent correct final answers with no valid textual argument) show that similar single-chain CoT methods can yield higher apparent performance while reducing faithfulness/interpretability of the chain.",
            "uuid": "e4735.2",
            "source_info": {
                "paper_title": "Dissociation of Faithful and Unfaithful Reasoning in LLMs",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Shapley value attribution in chain of thought",
            "rating": 2,
            "sanitized_title": "shapley_value_attribution_in_chain_of_thought"
        },
        {
            "paper_title": "Measuring faithfulness in chain-of-thought reasoning",
            "rating": 2,
            "sanitized_title": "measuring_faithfulness_in_chainofthought_reasoning"
        },
        {
            "paper_title": "Llms with chain-of-thought are non-causal reasoners",
            "rating": 2,
            "sanitized_title": "llms_with_chainofthought_are_noncausal_reasoners"
        },
        {
            "paper_title": "Faith and fate: Limits of transformers on compositionality",
            "rating": 1,
            "sanitized_title": "faith_and_fate_limits_of_transformers_on_compositionality"
        },
        {
            "paper_title": "Faithfulness vs. plausibility: On the (un)reliability of explanations from large language models",
            "rating": 1,
            "sanitized_title": "faithfulness_vs_plausibility_on_the_unreliability_of_explanations_from_large_language_models"
        }
    ],
    "cost": 0.010343999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Dissociation of Faithful and Unfaithful Reasoning in LLMs
2 Sep 2024</p>
<p>Evelyn Yee eyee@ucsd.edu 
Alice Li 
Chenyu Tang 
Yeon Ho 
Ramamohan Paturi rpaturi@ucsd.edu 
Leon Bergen lbergen@ucsd.edu </p>
<p>Laboratory of Emerging Intelligence
University of California
San Diego</p>
<p>University of California
San Diego</p>
<p>Dissociation of Faithful and Unfaithful Reasoning in LLMs
2 Sep 20243AFBA122282EFEDE51D78BD403833118arXiv:2405.15092v2[cs.AI]
Large language models (LLMs) often improve their performance in downstream tasks when they generate Chain of Thought reasoning text before producing an answer.We investigate how LLMs recover from errors in Chain of Thought.Through analysis of error recovery behaviors, we find evidence for unfaithfulness in Chain of Thought, which occurs when models arrive at the correct answer despite invalid reasoning text.We identify factors that shift LLM recovery behavior: LLMs recover more frequently from obvious errors and in contexts that provide more evidence for the correct answer.Critically, these factors have divergent effects on faithful and unfaithful recoveries.Our results indicate that there are distinct mechanisms driving faithful and unfaithful error recoveries.Selective targeting of these mechanisms may be able to drive down the rate of unfaithful reasoning and improve model interpretability.</p>
<p>Introduction</p>
<p>Large language models have shown a remarkable ability to solve high-level reasoning tasks across different domains (Bubeck et al., 2023;Burnell et al., 2023;Chang &amp; Bergen, 2023;Yu et al., 2023).Chain of thought (CoT), a prompting strategy that involves breaking down complex tasks into smaller sub-tasks and using the outputs of one sub-task as the input for the next, has been crucial for improvements in model performance on these reasoning tasks (Wei et al., 2023).In both the few-shot and zero-shot prompting styles, chain of thought improves language model performance on multi-step reasoning, problem-solving, and Q&amp;A tasks (Kojima et al., 2022;Suzgun et al., 2022;Wei et al., 2023) in English and multilingual contexts (Shi et al., 2022).Additionally, chain of thought has been shown to work in multimodal contexts (Rose et al., 2023;Zhang et al., 2023).More generally, there may be fundamental reasons which explain the importance of chain of thought for language model reasoning (Feng et al., 2023).</p>
<p>Despite the significant empirical success of chain of thought and the beginnings of a theoretical understanding, there is still much unknown about the mechanistic reasons for its success (Madaan &amp; Yazdanbakhsh, 2022;Wang et al., 2023).Although chain of thought transcripts resemble human reasoning on a surface level (Dziri et al., 2023;Wei et al., 2023), it is unknown whether this observed output aligns with the language model's internal reasoning processes.</p>
<p>The current study investigates the conditions under which LLM reasoning is faithful, where reasoning text is considered faithful if it provides a valid or reasonable argument in support of the final conclusion.This question of "faithfulness" (Jacovi &amp; Goldberg, 2020) in chain of thought is fundamental for understanding whether their reasoning is a trustworthy source of information for human users.Faithful reasoning ensures that the model's output not only reaches the correct conclusion but does so in a logically valid manner, allowing for human verification.</p>
<p>We investigate the faithfulness of chain of thought by focusing on language models' ability to recover from errors in their chain of thought texts.The study makes use of the dissociation paradigm from psychology and neuroscience (Tulving, 1972;Shallice, 1988).We apply interventions to LLM reasoning and measure the effect on faithful and unfaithful error recoveries.If these behaviors respond differently to the interventions, this provides evidence for distinct mechanisms for faithful and unfaithful error recoveries.</p>
<p>Our contributions include: new methods for analyzing LLM reasoning, the identification of both faithful and unfaithful error recovery behaviors, and evidence of distinct mechanisms underlying faithful and unfaithful error reasoning.</p>
<p>2 Related work 2.1 Understanding chain of thought Alkhamissi et al. (2023) and Hebenstreit et al. (2023) empirically investigated the generalizability of chain of thought across distinct reasoning tasks.Feng et al. (2023) proposed a mathematical framework for understanding chain of thought in arithmetic, emphasizing its role in enhancing transformer model expressiveness.Wang et al. (2023) and Madaan &amp; Yazdanbakhsh (2022) conducted ablation studies to determine which information is critical for successful reasoning, with the former emphasizing the coherence of few-shot demonstrations and the latter focusing on symbols and structural patterns.Notably, Wu et al. (2023) found that chain of thought increases robustness to input perturbations.Mechanistically, Dutta et al. (2024) analyzed chain of thought generation as a composition of neural circuits in transformer models, tracing information flow through the model during reasoning generation.</p>
<p>Errors in chain of thought.</p>
<p>Language models have been observed to produce errors in chain of thought generation, and there have been some efforts to describe these errors.For example, Zhang et al. (2023) categorized chain of thought errors in a multi-modal system as "hallucination" or "other."Similarly, Nguyen et al. (2024) classify chain of though errors into factual errors, coherence errors, and answer errors, noting a discrepancy between chain of thought accuracy and answer accuracy, which may grow with model size.McKenna et al. (2023) presented an investigation of reasoning errors in LLMs and attributed hallucination errors to memorization from their training corpus.Dziri et al. (2023) provided more fine-grained categorizations of errors in generated reasoning chains, identifying that local errors can either propagate or be resolved in later reasoning steps.That work also provided theoretical arguments that the propagation effect should significantly overwhelm the recovery effect, making errors inevitable for arbitrarily long reasoning chains.This effect was similarly observed by Arora et al. (2022), which developed a theoretical model for error propagation in model-generated text.</p>
<p>Faithfulness in chain of thought.</p>
<p>Recent research points to instances of unfaithful behavior in chain of thought.Agarwal et al. (2024) distinguish "plausible" explanations from "faithful" ones and call for further research and development of both plausible and faithful systems.Turpin et al. (2023) showed that language models can use chain of thought to unfaithfully rationalize answers that are derived from superficial cues in the prompt.To a similar effect, Lanham et al. (2023) implemented counterfactual interventions on chain of thought texts, finding that LLMs can achieve the correct answer despite variations on their chain of thought, like early stopping and error introductions.Gao (2023) also used counterfactual interventions, similar to those in the current study, and employed Shapley values to assess the significance of tokens in the model's reasoning text.Importantly, both studies found that, despite logically disrupted reasoning, the model's conclusions often remained unaffected.This suggests the model's final answer is not always aligned with its generated reasoning text.To further understand this effect, Bao et al. (2024) identify the "instructions" of a task as a potential mediating factor in the disconnect between the chain of thought text and the model's final answer.</p>
<p>Additional works examine faithfulness in similar textual reasoning contexts, including Freetext Rationales (Wiegreffe et al., 2021) and Natural Language Explanations (Atanasova et al., 2023).Across both CoT and NLE contexts, Parcalabescu &amp; Frank (2024) remark that existing faithfulness tests can be framed as measuring output consistency rather than internal model functioning.<Questions, Target Answer> pairs are sampled from the original dataset.For a single evaluation, the same model is used for each "LLM" part of the pipeline.</p>
<p>Methods</p>
<p>Dataset</p>
<p>In our experiments, we measure the effect of introducing errors into chain of thought reasoning text.These errors are generated in several stages (see Figure 1).First, given a question from a dataset, an LLM is prompted with Kojima et al. (2022) Given a ground-truth CoT transcript, a number in the text is selected, and an error is introduced at this point.The LLM is provided with the question along with the previous CoT text up to the point of this error, and it responds by completing the reasoning text.In general, selected values were adjusted by random integer values in {-3, -2, -1, 1, 2, 3}, following the methodology of Gao (2023).Figure 2 shows an example transcript.</p>
<p>Appendix A.1 contains more details on our querying and evaluation pipeline.</p>
<p>Models</p>
<p>We tested fixed versions of OpenAI's GPT-4 OpenAI (2023) as well as Anthropic's Claude-3 Opus and Meta's Llama-3 70B Chat model.The GPT model responses were gathered using the OpenAI API (gpt-3.5-turbo-0301and gpt-4-0314), Claude responses were gathered using the Anthropic API (claude-3-opus-20240229), and LLama responses were gathered using the Together AI API (meta-llama/Llama-3-70b-chat-hf).Decoding was done with temperature 0.</p>
<p>Datasets</p>
<p>We used four math word problem datasets, MultiArith (Koncel-Kedziorski et al., 2016), ASDiv (Miao et al., 2020), SVAMP (Patel et al., 2021), and GSM8K (Cobbe et al., 2021).For each dataset, we originally evaluated each model on all available questions in the test set (see Table 1 for dataset size details) and recorded the output chain of thought text.</p>
<p>Dataset</p>
<p>Error Introduction</p>
<p>Numerical errors were introduced using regular expressions.In the context of math word problems, we targeted errors in critical calculation steps, which derive a new numerical value which is essential to achieving the correct final answer.For all experiments, we manually verified that the errors met these criteria, to guarantee that correct final answers truly indicated error recovery, as opposed to the error having no effect on the final answer if naively propagated through.This resulted in slightly varying sample sizes across experimental conditions.</p>
<p>Faithfulness Analysis</p>
<p>To understand the faithfulness of these error recovery behaviors, we manually annotated each error response to identify whether the model recovered from the error and whether the error recovery behavior was faithful or unfaithful, as demonstrated in Figure 2.</p>
<p>For more details on our annotation process, including stimulus validation and faithfulness annotation criteria, see Appendix A.4.</p>
<p>In order to estimate the effects of different experimental variables on error recovery and faithfulness, we use multinomial logistic regression with fixed effects for datasets.† Several previous studies have observed error recovery, where a model reaches a correct final answer despite a flaw in the intermediate reasoning, as a general phenomenon in chain of thought.(See Section 2 for an overview.)Generally, it is assumed that error recovery indicates unfaithfulness, as the model reaches an error-free answer despite errors in the reasoning text.However, it is not clear whether recovery always indicates unfaithfulness.For example, it could be possible that a model explicitly announces the presence of an error and state a plausible, complete process for recovery.In this case, the error recovery would present no evidence for unfaithfulness.</p>
<p>Figure 2 illustrates the different types of error recovery behavior that we may observe.</p>
<p>Faithful recoveries occur when the model explicitly identifies its error and then recovers from it.Unfaithful recoveries occur when the model recovers without generating any text identifying that an error occurred.</p>
<p>Because LLMs may not be able to accurately judge whether CoT transcripts are faithful, ‡ we perform extensive manual annotation of LLM transcripts.In Experiment 1, we manipulate the perceptibility of errors by changing their magnitude (i.e. the absolute numerical difference between the error value and the original value).Larger errors are expected to be more noticeable to the model, resulting in higher rates of recovery.</p>
<p>Experiment 1: Error magnitude</p>
<p>For the small magnitude condition, errors were introduced by increasing the selected numerical values by 1.In the large magnitude condition, errors were introduced by increasing the selected numerical values by 101.The stimuli in these two conditions were matched pairwise within each dataset and error position.As shown in 3, the different models' error recovery capabilities varied.All models showed higher error recovery rates for large magnitude errors.(p &lt; 0.001 for each model).</p>
<p>Results</p>
<p>Figure 4 shows the effect of error magnitude on faithful and unfaithful recoveries.For GPT-4, increased error size led to higher rates of faithful recovery and lower rates of unfaithful recovery.For Claude-3 Opus, it led to a small decrease in faithful error recovery and a large increase in unfaithful recovery.For Llama-3, it led to a small increase in faithful error recovery and a large increase in unfaithful error recovery.For all three models, we found the faithful response to error magnitude and the unfaithful response to error magnitude to be significantly different (p &lt; 0.001 for GPT-4 and Llama-3, and p &lt; 0.05 for Claude-3 Opus).</p>
<p>The results in this experiment provide evidence that large language models recover more frequently from large errors than small ones.Consistent across models of different families, we find a dissociation between faithful and unfaithful modes of error recovery, though the individual responses vary between models.</p>
<p>Experiment 2: Prior expectations</p>
<p>Experiment 2 evaluates the hypothesis that a model will recover more frequently if it expects that an error is likely to occur in its CoT transcript.We increase this prior expectation of an error using two methods: introducing noise into the transcript, or directly prompting the model with this information.</p>
<p>Context Noise</p>
<p>We introduced noise in the CoT transcripts by randomly replacing 10 nonnumerical characters in the text preceding the numerical error.This was intended to introduce a prior expectation of mistakes in the CoT text without affecting the logic of the reasoning.As a result, we expected error recovery to increase for the noisy condition, over the baseline with no textual noise.</p>
<p>Error Recovery Prompt To more explicitly induce error expectations in the model, we modified the chain of thought prompt.In all of the other experiments, we prefaced the CoT transcript with the sentence, "Let's think step by step."(Kojima et al., 2022).In contrast, in this condition, we remind the model to specifically look for errors, replacing the previous prompt with "Let's think step by step, being careful to notice and fix any mistakes."Published as a conference paper at COLM 2024</p>
<p>Results</p>
<p>See Appendix C.2 for full results from this experiment.</p>
<p>Figure 12 shows overall recovery behaviors.Context noise increased the recovery rate relative to baseline (p &lt; 0.001 for all models).The error recovery prompt had a less pronounced effect, producing a small increase in total error recovery from GPT-4 (p &lt; 0.05) and Claude-3 Opus (p &lt; 0.001) and no detectable change in total error recovery from Llama-3 (p &gt; 0.05).Our analyses focus primarily on the effect of context noise.</p>
<p>Figure 5 shows the effect of context noise on faithful and unfaithful recoveries.For GPT-4, introducing context noise increased the rate of faithful recovery and decreased the rate of unfaithful recovery.For Claude-3 Opus and Llama-3, we observed that context noise led to an increase in unfaithful recovery and a slight decrease in faithful recovery.We found this difference between faithful and unfaithful recovery to be significant for GPT-4 (p &lt; 0.001) and Claude-3 Opus (p &lt; 0.001), but not for Llama-3 (p = 0.07), due to the smaller overall response to context noise in Llama-3.</p>
<p>As shown in Figure 15, the error recovery prompt led to increased faithful recovery and generally decreased unfaithful recovery for all 3 models.This difference between faithful and unfaithful behavior was not significant for GPT-4 (p = 0.08), but it was significant for Claude-3 Opus (p &lt; 0.001) and Llama-3 (p &lt; 0.001).</p>
<p>The results provide evidence that language models recover from errors more frequently when the context provides evidence that errors will occur in the CoT transcript.Across all models, this contextual evidence has distinct effects on faithful recoveries compared to unfaithful recoveries.However, the direction of the effects varied across the contextual manipulations and models.</p>
<p>Experiment 3: Recoverability</p>
<p>Experiment 3 is motivated by the following observation: different types of errors will provide the model with different amounts of evidence regarding how to correctly recover.If the error is introduced into information that the model has previously observed, then it can identify that an error has occurred by noticing a discrepancy with the prior information, and recover by copying the old information into its current context.In contrast, if the error is propagated through the CoT text, occurring multiple times, then the model will have stronger evidence that no error has occurred and that its prior reasoning is correct.For each stimulus, the relevant section of the original chain of thought is shown on the left, and the corresponding section of the perturbed chain of thought is shown on the right.The selected value in the original chain is highlighted in blue, and the error is highlighted in red.</p>
<p>We hypothesize that error recovery will occur more often when an LLM has more evidence about how to correctly recover.We investigate this hypothesis by introducing errors on three distinct positions in the CoT text.See Figure 6 for an illustration Appendix A.3 for more examples for each error position.</p>
<p>• Copying errors affect numbers that have been mentioned accurately earlier, either in the question or in the prior reasoning.</p>
<p>• Calculation errors affect the first occurrence of a value that originates in the CoT text.These mimic calculation mistakes, where the model performs an incorrect calculation when deriving a new value.</p>
<p>• Propagated calculation errors affect numbers that originate in the CoT text and appear at least twice.Both the initial and the secondary occurrences of the selected value are altered, with the text in between remaining untouched.The error in this case is propagated through the CoT text.</p>
<p>Copying errors have the highest amount of evidence for the correct value, as the correct value can be directly retrieved from an earlier part of the text.Calculation errors cannot be fixed by retrieving from the previous text.Propagated calculation errors provide the most evidence for the incorrect value.As a result of the differences in evidence, we expect copying errors to lead to the highest rate of error recovery, followed by calculation errors, followed by propagated inference errors.</p>
<p>Results</p>
<p>Figure 16 shows the overall error recovery results for GPT-4, which was the only model evaluated in this experiment.See Appendix C.3 for full results from this experiment.GPT-4 had the highest error recovery rate in the copying error condition and the lowest error recovery rate in the propagated calculation error condition (p &lt; 0.001 for both comparisons).Figure 7 shows the rate of faithful and unfaithful recoveries in response to the error interventions.</p>
<p>The pattern of faithful recoveries matches the pattern of overall recoveries: faithful recoveries occur most often for copy errors, less often for calculation errors (p &lt; 0.001, and least often for propagated calculation errors (p &lt; 0.001).</p>
<p>A different pattern was observed for unfaithful recoveries.Compared to faithful recoveries, there was a significantly smaller effect of calculation errors (p &lt; 0.05) or propagated calculation errors (p &lt; 0.001) on the rate of unfaithful recoveries.</p>
<p>The results show that a larger amount of evidence for the correct value increases the rate of faithful recoveries, but has a smaller effect on unfaithful recoveries.This indicates a dissociation in the behavior of faithful and unfaithful recoveries.</p>
<p>Discussion</p>
<p>In this study, we have investigated the ability of LLMs to recover from errors in their reasoning.We developed a fine-grained annotation scheme for LLM errors, and performed extensive manual annotations of LLM errors across three models and four datasets.</p>
<p>We identified three key aspects of chain of thought transcripts which have an effect on these language models' ability to recover.First, Experiment 1 found that larger errors are easier to recover from.Experiment 2 found that stronger prior expectations that an error will occur increase the frequency of recovery.Experiment 3 found that evidence for the correct value in the prior context increases the error recovery rate.</p>
<p>The experiments also evaluated the effect of these interventions on faithful and unfaithful error recoveries.Across the experiments, faithful and unfaithful recoveries consistently diverged in their responses to these interventions.Factors that increased the rate of faithful recoveries decreased the rate of unfaithful recoveries, and vice-versa.</p>
<p>Conclusion</p>
<p>Our study provides evidence that LLMs operate with two distinct modes of reasoning.In one mode, the model generates text that is optimized for human interpretability, allowing a human to understand the reasoning that supports the conclusion.In the other mode, the LLM arrives at conclusions through internal processes that are not fully captured in the generated text.The generated text in this mode may appear plausible but does not provide a valid argument for the model's conclusions.</p>
<p>The second, unfaithful reasoning mode undermines our ability to reliably audit an LLM's decision-making process.The model-generated explanations may appear plausible but fail to accurately represent the true basis for the model's conclusions.This means that examining a model's chain of thought output is not sufficient for verifying its conclusions.</p>
<p>A key challenge is to develop methods which consistently elicit the interpretable mode of reasoning from LLMs.Future research should focus on understanding the mechanisms underlying these different modes of reasoning, identifying factors that influence which mode is activated, and developing techniques to encourage faithful reasoning.</p>
<p>Limitations</p>
<p>The study evaluated 3 models on 4 datasets, which consist of mathematical reasoning problems.We chose to focus on depth over breadth, leveraging costly manual annotations for each case.However, the study does not investigate non-mathematical reasoning errors.</p>
<p>Additionally, errors that these models naturally produce during text generation may be different than those introduced in the current study.To address this, we analyzed a sample of model-generated errors from the math word problem datasets (see Appendix B), and found natural behavior similar to the errors and recovery behaviors investigated in this study.Further work is required to understand the full range of naturally occurring errors in LLMs.</p>
<p>A Methodology Details</p>
<p>A.1 Model querying pipeline</p>
<p>For the baseline chain of thought evaluation, we use Kojima et al. (2022)'s 2-pass querying pipeline.During the first pass, we provide the model with the selected question and the chain of thought prompt.We then record the model's response (i.e. the chain of thought) and make a second query containing all of the previous information, with a second prompt designed to elicit a single numerical answer from the model.This allows us to somewhat normalize the model output and streamline the evaluation process.Our prompts are from Kojima et al. (2022), with some small adaptations to formatting for OpenAI's chat models.</p>
<p>For the errored chain of thought evaluation, we follow the same procedure, except we include the errored chain of thought with the question in the first pass.The model's response to the first pass contains its completion of the chain of thought, and we combine this text with the previous material for the second, answer extraction pass.</p>
<p>A.2 Chain of thought perturbations</p>
<p>To introduce numerical errors, we first used regular expressions to select all occurrences of numerical values in the question and chain of thought text.We then grouped these occurrences by their value and filtered these value groups depending on the error position.</p>
<p>For the copying error position, we filtered to values in the chain of thought which occurred at least twice (the first occurrence(s) may be in the text of the question itself).For the calculation error position, we filtered to values which occurred for the first time in the chain of thought.For the propagated calculation error position, we filtered to values which occurred for the first time in the chain of thought and occurred at least twice total.</p>
<p>Our value selection did not account for word forms of numbers, like "one," "half," or "third," and although we did make some effort to filter out step numbers, our filtering process did not account for other kinds of values which are non-essential to the reasoning, like numbers in names (e.g."horse #7").Additionally, our value grouping process relied on evaluating the selected strings as float values and comparing these, so this process did not account for the same value to appear in different number formats (e.g."0.7" vs "70%").For both calculation error conditions, we removed stimuli like these from our analysis, because they did not conceptually represent the type of reasoning error that we wanted to explore with these experiments.For the Copying Error conditions, we removed non-essential values but allowed the other kinds of ambiguous/error value repeats.This is the reason for our varying sample sizes across the different experimental conditions; although all conditions started with a sample of 300 errored chain of thought stimuli, some of the stimuli were deemed unfit and discarded, leaving different sample sizes of valid stimuli.</p>
<p>After filtering, our primary perturbation method was directly inspired by Gao (2023).We randomly selected one of the numerical groups for each error position for each chain of thought, and then we perturbed the selected value(s), randomly selecting an integer perturbation amount from {-3,-2,-1,1,2,3}.The selected value(s) were replaced in the chain of thought text, and any additional chain of thought after the final error (i.e. after the first error in the calculation and copying error positions, and after the second error in the propagated calculation error position) was discarded, to allow the model full freedom to adjust its response after the errors.</p>
<p>For Experiment 2, we performed the exact same process, but the perturbations were by a fixed amount (+1 and +101) rather than being randomly sampled.</p>
<p>A.3 Chain of thought prompts</p>
<p>Error Position</p>
<p>Prompt Format Example</p>
<p>Baseline</p>
<p>Chain of Thought</p>
<p>CoT Completion:</p>
<p>USER: Q: <Question> ASSISTANT: A: Let's think step by step.</p>
<p>ASSISTANT: <Original chain of thought, queried from the model> Q: Jennie is helping at her mom's office.She has a pile of 60 letters needing stamps, and a pile of letters already stamped.She puts stamps on one-third of the letters needing stamps.If there are now 30 letters in the pile of already-stamped letters, how many were in that pile when Jennie began?</p>
<p>A: Let's think step by step. 1. Jennie has a pile of 60 letters needing stamps.</p>
<ol>
<li>
<p>She puts stamps on one-third of the letters needing stamps, which is 60/3 = 20 letters.</p>
</li>
<li>
<p>After putting stamps on 20 letters, there are now 30 letters in the pile of already-stamped letters.4. Therefore, there were originally 30 -20 = 10 letters in the pile of already-stamped letters when Jennie began helping at her mom's office.</p>
</li>
</ol>
<p>Therefore, the answer (arabic numerals) is 10.</p>
<p>Answer Extraction:</p>
<p>USER: Q: <Question> Q: Josie grows grapes on her 10-acre farm.Each acre produces 5 tons of grapes per year, and each ton of grapes makes 2 barrels of wine.How many barrels of wine does her farm produce per year?</p>
<p>A: Let's think step by step.First, we need to find out how many tons of grapes Josie's farm produces per year.</p>
<p>To do this, we multiply the number of acres ( 10) by the number of tons per acre (7 5).So, 10 acres * 5 tons/acre = 50 tons of grapes per year.Now, we need to find out how many barrels of wine these 50 tons of grapes can produce.We know that 1 ton of grapes makes 2 barrels of wine.So, 50 tons * 2 barrels/ton = 100 barrels of wine.</p>
<p>Therefore, Josie's farm produces 100 barrels of wine per year.Therefore, the answer (arabic numerals) is 100.</p>
<p>Answer Extraction:</p>
<p>USER: Q: <Question> ASSISTANT: <final answer, queried from the model> the selected value and the copying error criteria for the second occurrence of the selected value.</p>
<p>A.4.2 Error recovery evaluation</p>
<p>After using the 2-pass querying pipeline to extract the final answer for each question, we use regular expressions to extract the first numerical value from the final answer and evaluate this string as a float, before comparing it against the target answer for the relevant question in the original dataset, similar to the process used by Kojima et al. (2022).If the model-produced answer equals the target answer, we consider this an accurate response.</p>
<p>A.4.3 Recovery behaviors / faithfulness</p>
<p>Completed Chain of Thought 22 legs.ops, I made a mistake!spiders have 3 x 8 = 24 legs.</p>
<p>, there are 24 spider legs in total.</p>
<p>erefore, the answer (arabic merals) is"❌ rror Recovery After confirming the accuracy of each response, we sorted the error recovery responses (i.e.where the model's final answer is correct despite our error in the reasoning) into 5 fine-grained categories of error recovery behavior which we observed in the data:</p>
<p>• Complete hallucination: model recovers correct value with no obvious textual evidence of recovery methods</p>
<p>• Partial hallucination: model produces tokens after the perturbation that are not interpretable/coherent and are not a comment identifying the error, a re-calculation, or the correct value.</p>
<p>• Explicitly identifies error: model makes a natural language comment explicitly identifying the error (e.g."I'm sorry, that's not correct.")before stating the correct answer and/or re-doing calculations.Also includes cases where the model recovers via explicit rounding.</p>
<p>• Directly re-does calculation without explicitly identifying error: model produces the correct calculations directly after the perturbation or completely re-does the entire chain of thought, without a natural language comment identifying the error</p>
<p>• States correct value directly after error: model produces the correct value directly after the perturbation, without a natural language comment For our primary analysis, "Complete hallucination" and "Partial hallucination" were grouped together as unfaithful error recoveries because they do not explicitly support the downstream calculations, and the remaining behaviors were grouped together as faithful error recoveries because of their clear acknowledgement of the error.</p>
<p>D Sensitivity Analysis</p>
<p>As described in Appendix A.4.3, our full annotation process included 5 fine-grained ery behaviors, which formed as a natural clustering of the data.Of these 5 fine-grained behaviors, we identified that 3 of the behaviors were clearly interpretable in their identification of the error and evidence for the correct value, while two behaviors, called "partial hallucination" and "complete hallucination," were not clearly interpretable.In the main body of the paper, we group these two unclear behaviors together as "unfaithful recovery."</p>
<p>In this section, we consider an alternate definition of "unfaithful recovery," which only includes complete hallucinations.Using this alternative definition, we draw similar qualitative and statistical conclusions to those in the main body of the paper, across all 3 experiments.This indicates that our observations are robust to this difference in definition.</p>
<p>D.1 Experiment 1</p>
<p>Figure 17: Sensitivity analysis for faithful vs. unfaithful recoveries, examining the difference in recovery rates between between large error and small error conditions.Negative values indicate recoveries occurred more often for small errors.Error bars are 95% confidence intervals.</p>
<p>In experiment 1, we examined the effect of error magnitude on the faithful and unfaithful error recovery rates of GPT-4, Claude-3 Opus, and Llama-3 70B.From Figure 17, we see that GPT-4 generally performed complete hallucination less frequently for large errors than small errors, whereas it performed faithful recovery more frequently for large errors than small errors.As in Section 5, there is a significant difference between the effects of error size of faithful and unfaithful recovery rates (p &lt; 0.001).Similarly, this dissociation between</p>
<p>D.2 Experiment 2</p>
<p>In experiment 2, we measured the effect of context noise on error recovery rates.The trends of the complete hallucinations are largely similar to those of the unfaithful responses presented in the primary analysis.For Claude-3 Opus, we identify a significant difference between faithful recovery and complete hallucination in response to both context noise (p &lt; 0.001) and the error recovery prompt (p &lt; 0.001).For Llama-3 70B, we find similar dissociations (p &lt; 0.05 for context noise and p &lt; 0.001 for prompting).However, under this sensitivity analysis, we were unable to detect a similarly significant differentiation for GPT-4's faithful recovery and complete hallucination behaviors (p &gt; 0.05 for both experimental comparisons).Figure 19 shows the complete hallucination rates against the faithful recovery rates for experiment 3. Consistent with our analysis from Section 7, we find that complete hallucination shows distinct behavior compared to faithful recovery (and the overall error recovery trend), from copy errors to calculation errors (p &lt; 0.005) or from calculation errors to propagated calculation errors (p &lt; 0.001).</p>
<p>Figure 1 :
1
Figure 1: Our querying and error recovery evaluation pipeline for errored chain of thought.<Questions,Target Answer> pairs are sampled from the original dataset.For a single evaluation, the same model is used for each "LLM" part of the pipeline.</p>
<p>Figure 2 :
2
Figure 2: An example stimulus from the ASDiv Calculation Error set for GPT-4 (lightly edited for clarity), with demonstrations of the potential error recovery behaviors.The error is highlighted in red, demonstration of faithful recovery is highlighted in green, and unfaithful recovery behaviors are highlighted in blue.The model's final answer to the question is boxed.</p>
<p>Figure 3 :
3
Figure 3: Overall error recovery rates (as a proportion of all responses) from small errors and large errors.Error bars indicate 95% binomial confidence intervals.</p>
<p>Figure 4 :
4
Figure 4: Difference between large error and small error recovery rates, as a proportion of all responses.Negative values indicate recoveries occurred more often for small errors.Error bars are 95% confidence intervals.See Appendix C.1 for full results from this experiment.</p>
<p>Figure 5 :
5
Figure 5: Difference in recovery rates (as a proportion of all responses) between context noise and baseline conditions.Negative values indicate recoveries occurred more often in the baseline condition.Error bars are 95% confidence intervals.</p>
<p>Figure 6 :
6
Figure6: Example stimuli from each error position in the GPT-4 test set.For each stimulus, the relevant section of the original chain of thought is shown on the left, and the corresponding section of the perturbed chain of thought is shown on the right.The selected value in the original chain is highlighted in blue, and the error is highlighted in red.</p>
<p>Figure 7 :
7
Figure 7: Faithful and unfaithful recovery rates (as a proportion of all responses) for GPT-4 for each error position.Error bars indicate 95% binomial confidence intervals.</p>
<p>"Figure 8 :
8
Figure 8: An example stimulus from the ASDiv Calculation Error set for GPT-4 (lightly edited for clarity), with demonstrations of each fine-grained error recovery behavior.The error is highlighted in red, demonstration of faithful recovery is highlighted in green, and unfaithful recovery behaviors are highlighted in blue.The model's final answer to the question is boxed.</p>
<p>Figure 10 :
10
Figure 10: Faithful recovery rates (as a proportion of all responses) from small and large errors.Error bars indicate 95% binomial confidence intervals.</p>
<p>Figure 11 :
11
Figure 11: Unfaithful recovery rates (as a proportion of all responses) from small and large errors.Error bars indicate 95% binomial confidence intervals.</p>
<p>Figure 12 :
12
Figure 12: Overall error recovery rates (as a proportion of all responses) from textual adjustments.Error bars indicate 95% binomial confidence intervals.</p>
<p>Figure 13 :
13
Figure 13: Faithful recovery rates (as a proportion of all responses) from textual adjustments.Error bars indicate 95% binomial confidence intervals.</p>
<p>Figure 14 :
14
Figure 14: Unfaithful recovery rates (as a proportion of all responses) from textual adjustments.Error bars indicate 95% binomial confidence intervals.</p>
<p>Figure 15 :
15
Figure 15: Difference recovery rates between error recovery prompt and baseline conditions, as a proportion of all responses.Negative values indicate recoveries occurred more often in the baseline condition.Error bars are 95% confidence intervals.</p>
<p>Figure 18 :Figure 19 :
1819
Figure 18: Sensitivity analysis for faithful vs. unfaithful recoveries, examining the difference in recovery rates between context noise and baseline conditions.Negative values indicate that recoveries occurred more often in the baseline condition.Error bars indicate 95% confidence intervals.</p>
<p>Table 1 :
1
The initial test set size for each dataset used in this work.
Test Set SizeMultiArith600ASDiv2096SVAMP1000GSM8K1319For each model in each dataset, we randomly sampled 300 <question, chain of thought,answer> triples for which the model achieved the correct answer. * These triples, whichwere collected separately for each model and dataset, make up our ground-truth data for allfurther experiments.</p>
<p>4 Error recovery behaviors
Question + Errored Chain of ThoughtCompleted Chain of Thought...22 legs....22 legs.Oops, I made a mistake!...22 legs.So, there are 22 spider legs in total.❌3 spiders have 3 x 8 = 24 legs.So, there are 24 spider legs in total.✔So, there are 24 spider legs in total.✔No Error RecoveryFaithful RecoveryUnfaithful Recoveryverymistake!3 x 8 = 24 legs. spider legs in total. nswer (arabic...$13 Now, let's find out how much money Adam had to contribute from his savings. We know that the total cost ofthe trousers was $25, and Adam'smother and father gave him $6 and$12, respectively. So, Adam had tocontribute $25 -$6 -$12 =$ 7 from his savings. ✔Unfaithful Recovery
"Q: There are 3 spiders.How many spider legs are there?A: Let's think step by step.Each spider has 8 legs.So, 1 spider has 8 legs.3 spiders have 3 x 8 = 22"</p>
<p>Table 2 :
2
Experiment 1 numerical results, performed on GPT-4.All percentages are rounded to 2 decimal places.
DatasetModelErrornTotal ErrorFaithfulUnfaithful Recovery (%)AmountRecovery (%)Recovery (%)CompletePartial Hal-Hallucina-lucinationtion (%)(%)GPT-4Small Error 247 Large Error 24777.33 100.0072.06 95.953.64 0.811.62 3.24MultiArithClaude-3Small Error 28867.364.8657.644.86OpusLarge Error 28896.533.1281.9411.46Llama-3Small Error 29045.526.5537.931.0370BLarge Error 29091.7217.5945.5228.62GPT-4Small Error 228 Large Error 22875.44 97.3746.93 85.9624.56 7.893.95 3.51ASDivClaude-3Small Error 19664.293.0653.068.16OpusLarge Error 19686.221.5372.4512.24Llama-3Small Error 24155.196.6447.301.2470BLarge Error 24191.2921.5855.1914.52GPT-4Small Error 257 Large Error 25772.76 93.7751.75 86.3819.07 6.231.95 1.17SVAMPClaude-3Small Error 27258.091.4747.069.56OpusLarge Error 27283.091.4770.2211.40Llama-3Small Error 28462.322.1156.693.5270BLarge Error 28493.3119.0150.3523.94GPT-4Small Error 229 Large Error 22779.04 97.8050.66 81.066.11 6.6122.27 10.13GSM8KClaude-3Small Error 22367.712.6962.782.24OpusLarge Error 22389.691.7980.727.17Llama-3Small Error 27131.732.5825.833.3270BLarge Error 27179.347.0140.9631.37</p>
<p>Experiment 2 full results</p>
<p>† Random effects models did not converge in this setting. ‡ We were unable to build an accurate LLM-driven annotation pipeline for error recovery.
Code and data for our experiments, including instructions for reproducing our results, will be made available at https://github.com/CoTErrorRecovery/CoTErrorRecovery.However, OpenAI has announced that access to the GPT-4 checkpoint that we evaluated (gpt-4-0314) may be permanently deprecated as early as June 2024(Eleti et al., 2023).A.4 AnnotationsA.4.1 Stimulus validationBecause the numerical errors were introduced with regular expressions, some of the resulting stimuli did not match our conceptual definitions of each error condition.We manually check for and remove these stimuli before continuing with the error recovery evaluation:• Unnecessary error: If the error to the chain of thought text was unnecessary to the final answer (e.g.introducing an error on a step number rather than a meaningful value in the chain of thought), then the stimulus was invalid.• Incorrect error position: For copying errors, if the previous occurrence of the selected value was not associated with the same logical quantity, the stimulus was invalid.For calculation errors, if the selected value occurred previously in a different form (e.g. as a word instead of numerically), the stimulus was invalid.For propagated calculation errors, we check the calculation error criteria for the first occurrence ofA.4.4 Annotation interfaceFigure9: A screenshot of the annotation interface.Annotation was performed through a custom web app.Annotators were provided with the question, chain of thought, and final answer, along with some courtesy formatting.For accuracy and stimulus validation, the interface presents best-guess values, based on metadata from the problem (e.g. the expected error position).The error recovery was left blank for annotators to fill in.See Figure9for a sample view of the annotation interface.A.4.5 AnnotatorsA team of 3 annotators performed all of the annotations for this work.All annotators were STEM undergraduate students with native-level proficiency in English.Annotators were trained together and were all provided with the same annotation guideline document, provided in our GitHub repository.Each experimental set (i.e. each combination of <dataset, model, error position, error amount>) was split evenly among the annotators, and annotators met weekly to review annotations for agreement.B Analysis of Natural ErrorsIn our experiments, we prompted LLMs to generate reasoning for solving math problems, and then introduced artificial errors into the reasoning transcripts.In this section, we analyze naturally generated LLM errors to determine whetherthey match the distribution of natural errors.We filtered these transcripts to ensure that they were valid chains of reasoning, and only introduced errors into the valid transcripts.We collected reasoning transcripts from GPT-4, and identified transcripts that contained incorrect reasoning.§ We then manually labeled 50 of these transcripts per dataset (200 total).After filtering out low-quality questions (e.g.where the ground-truth target answer is ambiguous or incorrect) and subjectively correct answers (e.g.where the model reached the correct answer at some point in its reasoning but did not output the answer in the correct format), we found 74 high-quality, naturally-generated incorrect chains of thought.Of these, 7 transcripts involved the introduction and propagation of calculation errors, as defined in our paper (see Section 7).Additionally, 6 more error transcripts demonstrated an attempt to skip intermediate reasoning steps, similar to the unfaithful error recovery behavior observed in the paper.These findings offer preliminary evidence that the errors introduced in the paper and the observed error recovery behaviors occur naturally.C Full error recovery resultsC.1 Experiment 1 full results § These invalid transcripts were excluded from all other analyses in the paper.Published as a conference paper at COLM 2024C.3 Experiment 3 full resultsFigure16: Total error recovery rates (as a proportion of all responses) for GPT-4 for each error position.Error bars indicate 95% binomial confidence intervals.Table5: Experiment 3 numerical Due to manual annotation costs, this experiment was only evaluated on GPT-4.The "Calculation Error" results are the "Baseline" error recovery results from Experiment 3 (Tables3 and 4).
Faithfulness vs. plausibility: On the (un)reliability of explanations from large language models. Chirag Agarwal, Sree Harsha Tanneru, Himabindu Lakkaraju, 2024</p>
<p>Opt-r: Exploring the role of explanations in finetuning and prompting for reasoning skills of large language models. Badr Alkhamissi, Siddharth Verma, Ping Yu, Zhijing Jin, Asli Celikyilmaz, Mona Diab, 10.18653/v1/2023.nlrse-1.10Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE). the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE)Association for Computational Linguistics2023</p>
<p>Why exposure bias matters: An imitation learning perspective of error accumulation in language generation. Kushal Arora, Layla El Asri, Hareesh Bahuleyan, Jackie Cheung, 10.18653/v1/2022.findings-acl.58Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational LinguisticsMay 2022</p>
<p>Faithfulness tests for natural language explanations. Pepa Atanasova, Oana-Maria Camburu, Christina Lioma, Thomas Lukasiewicz, Jakob Grue Simonsen, Isabelle Augenstein, 10.18653/v1/2023.acl-short.25Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20232</p>
<p>Llms with chain-of-thought are non-causal reasoners. Guangsheng Bao, Hongbo Zhang, Linyi Yang, Cunxiang Wang, Yue Zhang, 2024</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, Sparks of artificial general intelligence: Early experiments with gpt-4. 2023</p>
<p>Ryan Burnell, Han Hao, R A Andrew, Jose Conway, Orallo Hernandez, Revealing the structure of language model capabilities. 2023</p>
<p>Language model behavior: A comprehensive survey. Tyler A Chang, Benjamin K Bergen, 2023</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, 2021</p>
<p>How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning. Subhabrata Dutta, Joykirat Singh, Soumen Chakrabarti, Tanmoy Chakraborty, 2024</p>
<p>. Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Yuchen Jiang, Peter Lin, Chandra West, Bhagavatula, Le Ronan, Jena D Bras, Soumya Hwang, Sean Sanyal, Xiang Welleck, Allyson Ren, Zaid Ettinger, Yejin Harchaoui, Choi, 2023Faith and fate: Limits of transformers on compositionality</p>
<p>Function calling and other api updates. Atty Eleti, Jeff Harris, Logan Kilpatrick, Jul 2023</p>
<p>Towards revealing the mystery behind chain of thought: A theoretical perspective. Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, Liwei Wang, 2023</p>
<p>Shapley value attribution in chain of thought. Leo Gao, Apr 2023</p>
<p>An automatically discovered chain-of-thought prompt generalizes to novel models and datasets. Konstantin Hebenstreit, Robert Praas, Louis P Kiesewetter, Matthias Samwald, 2023</p>
<p>Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness?. Alon Jacovi, Yoav Goldberg, 10.18653/v1/2020.acl-main.386Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022</p>
<p>MAWPS: A math word problem repository. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, Hannaneh Hajishirzi, 10.18653/v1/N16-1136Proceedings of the 2016 Conference of the North American Chapter. Kevin Knight, Ani Nenkova, Owen Rambow, the 2016 Conference of the North American ChapterSan Diego, CaliforniaAssociation for Computational LinguisticsJune 2016</p>
<p>. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Karina Kamil Ė Lukoši Ūt Ė, Newton Nguyen, Nicholas Cheng, Nicholas Joseph, Oliver Schiefer, Robin Rausch, Sam Larson, Sandipan Mccandlish, Saurav Kundu, Shannon Kadavath, Thomas Yang, ; Henighan, R Samuel, Ethan Bowman, Perez, Jan Brauner,. 2023Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared KaplanMeasuring faithfulness in chain-of-thought reasoning</p>
<p>Text and patterns: For effective chain of thought, it takes two to tango. Aman Madaan, Amir Yazdanbakhsh, 2022</p>
<p>A diverse corpus for evaluating and developing english math word problem solvers. Nick Mckenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark Johnson, Mark Steedman, ; Shen-Yun Miao, Chao-Chun Liang, Keh-Yih Su, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2023. 2020Sources of hallucination by large language models on inference tasks</p>
<p>Direct evaluation of chain-of-thought in multi-hop reasoning with knowledge graphs. Minh-Vuong Nguyen, Linhao Luo, Fatemeh Shiri, Dinh Phung, Yuan-Fang Li, Thuy-Trang Vu, Gholamreza Haffari, 2024</p>
<p>. OpenAI. Gpt-4 technical report. 2023</p>
<p>On measuring faithfulness or self-consistency of natural language explanations. Letitia Parcalabescu, Anette Frank, 2024</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, 10.18653/v1/2021.naacl-main.168Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational LinguisticsJune 2021</p>
<p>Daniel Rose, Vaishnavi Himakunthala, Andy Ouyang, Ryan He, Alex Mei, Yujie Lu, Michael Saxon, Chinmay Sonar, Diba Mirza, William Yang, Wang , Visual chain of thought: Bridging logical gaps with multimodal infillings. 2023</p>
<p>From neuropsychology to mental structure. Tim Shallice, 1988Cambridge University Press</p>
<p>Language models are multilingual chain-of-thought reasoners. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, Jason Wei, 2022</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, Jason Wei, 2022</p>
<p>Episodic and semantic memory. Endel Tulving, Organization of memory. 111972</p>
<p>Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting. Miles Turpin, Julian Michael, Ethan Perez, Samuel R Bowman, 2023</p>
<p>Towards understanding chain-of-thought prompting: An empirical study of what matters. Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, Huan Sun, 10.18653/v1/2023.acl-long.153Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>Measuring association between labels and free-text rationales. Sarah Wiegreffe, Ana Marasović, Noah A Smith, 10.18653/v1/2021.emnlp-main.804Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Marie-Francine Moens, Xuanjing Huang, Lucia Specia, Scott , Wen-Tau Yih, the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsNovember 2021Online and Punta Cana</p>
<p>Analyzing chain-of-thought prompting in large language models via gradient-based feature attributions. Skyler Wu, Eric Meng Shen, Charumathi Badrinath, Jiaqi Ma, Himabindu Lakkaraju, 2023</p>
<p>Natural language reasoning, a survey. Fei Yu, Hongbo Zhang, Prayag Tiwari, Benyou Wang, 2023</p>
<p>Multimodal chain-of-thought reasoning in language models, 2023. faithful recovery and complete hallucination responses in this experiment also hold for Claude-3 Opus (p 0.05) and Llama-3 70B. Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, Alex Smola, 0010</p>            </div>
        </div>

    </div>
</body>
</html>
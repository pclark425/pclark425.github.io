<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8223 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8223</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8223</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-151.html">extraction-schema-151</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <p><strong>Paper ID:</strong> paper-278788765</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.16067v2.pdf" target="_blank">How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior</a></p>
                <p><strong>Paper Abstract:</strong> Memory is a critical component in large language model (LLM)-based agents, enabling them to store and retrieve past executions to improve task performance over time. In this paper, we conduct an empirical study on how memory management choices impact the LLM agents'behavior, especially their long-term performance. Specifically, we focus on two fundamental memory management operations that are widely used by many agent frameworks-memory addition and deletion-to systematically study their impact on the agent behavior. Through our quantitative analysis, we find that LLM agents display an experience-following property: high similarity between a task input and the input in a retrieved memory record often results in highly similar agent outputs. Our analysis further reveals two significant challenges associated with this property: error propagation, where inaccuracies in past experiences compound and degrade future performance, and misaligned experience replay, where some seemingly correct executions can provide limited or even misleading value as experiences. Through controlled experiments, we demonstrate the importance of regulating experience quality within the memory bank and show that future task evaluations can serve as free quality labels for stored memory. Our findings offer insights into the behavioral dynamics of LLM agent memory systems and provide practical guidance for designing memory components that support robust, long-term agent performance.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8223.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8223.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RegAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RegAgent (synthetic regression agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic, controllable LLM-agent environment that predicts y = w^T x by retrieving K nearest demonstrations from an episodic memory bank; designed to isolate and measure the impact of memory addition/deletion on long-term agent performance and error propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RegAgent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Synthetic regression agent that must predict a scalar y = w^T x given a 6-d input x by conditioning on K = 6 nearest stored demonstrations (input,guess) pairs; used to precisely control noise and measure imitation vs reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4o-mini (backbone for most experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>GPT-4o-mini is used as the backbone LLM for most experiments; experiments also report results on other LLM backbones (GPT-4o, DeepSeek-V3) to check robustness. (paper uses model as an in-context generator and as coarse evaluators in variants).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>RegAgent synthetic regression task</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Controlled regression prediction task: samples x from three Gaussians, agent must return y = w^T x + noise; success defined as |ŷ - y| ≤ 1. Retrieval uses cosine similarity on inputs; output similarity computed via an RBF-style function.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Memory bank of query-execution pairs (q, e). Stored as an expanding set of (input vector, predicted output) records; retrieval is nearest-neighbor (cosine for inputs) returning K demonstrations which are used as in-context examples. Addition and deletion are explicit operations controlled by evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Retrieved top-K demonstrations are concatenated/used as in-context few-shot demonstrations in the prompt for the LLM; agent then generates execution conditioned on these demonstrations. Retrieval influences the agent via demonstration-following rather than gradient updates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compares fixed-memory baseline (no additions) vs add-all vs selective addition with three coarse automatic evaluators (C1,C2,C3) and strict human-like evaluator; also compares deletion strategies (periodic, history-based, combined). Finds (i) experience-following: high input similarity → high output similarity (near Pearson r≈1 when demonstrations are very similar); (ii) add-all and coarse evaluators with noisy additions degrade long-run success rates due to error propagation, while strict/selective addition yields sustained improvement; history-based deletion removes misaligned/noisy records and can modestly improve or maintain performance.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Paper identifies strict selective addition (high-quality evaluator) combined with history-based or combined deletion as the most reliable strategy: only store high-utility trajectories and remove demonstrations that consistently yield low downstream utility.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Strong experience-following makes RegAgent highly susceptible to error propagation: noisy stored demonstrations cause the agent to imitate and amplify errors; naive add-all leads to flat or declining success rates. History-based deletion can remove useful entries if the utility evaluator is coarse/noisy. The synthetic environment purposely emphasizes reliance on demonstrations, so results may be exaggerated relative to other tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Use selective addition driven by a reliable trajectory evaluator (strict/human-like or fine-tuned judge) to avoid accumulating noisy executions; apply history-based deletion that removes records with low downstream utility (after sufficient retrieval counts) or a combined deletion (periodic + history-based) to balance memory size and utility; evaluator reliability is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8223.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8223.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EHRAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EHRAgent (EHR code-generation agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based code-generation agent that answers electronic health record queries by retrieving a small number of past query–execution pairs (4 experiences) from an episodic memory bank and using them as in-context demonstrations for SQL-like code generation and execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ehragent: Code empowers large language models for complex tabular reasoning on electronic health records.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>EHRAgent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Code-generation agent for clinical EHR queries; retrieves 4 past experiences via text-embedding similarity and uses them as demonstrations to drive code generation (SQL-like) for MIMIC-III tasks; evaluated with accuracy (ACC).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4o-mini (backbone), with coarse evaluators using GPT-4o-mini/GPT-4.1-mini variants</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Paper uses GPT-4o-mini as the primary backbone LLM; coarse evaluators include GPT-4o-mini and GPT-4.1-mini and a fine-tuned GPT-4.1-mini for stronger automatic judging.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>EHRAgent EHR code-generation benchmark (MIMIC-III subset)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Natural-language EHR queries mapped to code + execution on MIMIC-III; challenges include complex tabular reasoning and code correctness; evaluation via exact-match for strict judge and LLM-based termination signal for coarse judge.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Memory bank of (query, code-execution) records with embeddings (OpenAI text-embedding-3-large). Retrieval: top-4 by cosine similarity; stored records appended as in-context demonstrations. Addition determined by evaluator signal (automatic coarse LLM judge or strict exact-match human-like judge).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Concatenate retrieved past experiences (code + results) as in-context demonstrations to the prompt for the LLM code generator; the highest-input-similarity retrieved example is used to compute output similarity (pycode_similar used to measure code similarity).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compares fixed-memory baseline (no additions), add-all, coarse automatic evaluators (different LLMs and a fine-tuned judge), and strict addition. Finds fixed-memory can be strong initially; add-all and coarse addition sometimes lead to flat or declining ACC due to accumulation of flawed records; strict selective addition and fine-tuned coarse evaluator (C3) show consistent self-improvement over time. Deletion experiments show periodical deletion reduces memory size with small performance drops; history-based deletion helps when utility evaluator is reliable.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Strict selective addition (high-quality evaluator, e.g., exact-match human-like judge) combined with history-based or combined deletion when using a reliable utility evaluator; fine-tuning coarse evaluators (even with a small dataset) substantially improves addition quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Coarse LLM judges (vanilla LLMs) can be noisy and may worsen long-term performance if used for addition/deletion; fixed-memory is static and cannot adapt to distribution shifts; history-based deletion performance depends strongly on the evaluator's reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Prioritize evaluator quality (fine-tune judges or use strict human-like criteria) for memory addition; use history-based deletion guided by downstream utility with thresholds for minimum retrieval counts to avoid removing entries based on sparse data; combined deletion (periodic + history) is a practical tradeoff for stability under shift.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8223.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8223.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AgentDriver</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AgentDriver (autonomous driving language agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based autonomous driving agent that augments planning with episodic experiences (trajectories); retrieves a single most-relevant past trajectory and uses it as an exemplar to influence the predicted short-horizon trajectory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A language agent for autonomous driving.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>AgentDriver</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Autonomous-driving LLM agent evaluated on nuScenes; originally uses a two-step retrieval (top-3 then LLM picks), simplified here to top-1 vector-similarity retrieval; retrieved trajectory is used as an exemplar to guide planning; output evaluated by UniAD 3-second average L2 distance and a derived success metric.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4o-mini (backbone) with other backbones tested (GPT-4o, DeepSeek-V3); coarse evaluators use LLM binary (yes/no) judges; strict evaluator uses numeric L2 distance threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>LLM backbones used to generate trajectories and to serve as coarse evaluators; strict evaluator computes UniAD 3s L2 distance to ground truth (threshold-based). Experiments include backbone variants to validate generality.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AgentDriver autonomous driving benchmark (nuScenes sampled cases)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Short-horizon trajectory prediction and planning (6-step horizon), challenges include safety, drivable area constraints, and collision avoidance; performance measured by trajectory distance metrics and success criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Memory bank of (scene-query, predicted trajectory) experiences; retrieval by vector similarity (original two-stage simplified to top-1), memory stores sequences of state/trajectory; addition controlled by evaluators (LLM-based coarse judge or L2-based strict judge), deletion via periodic/history/combined strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Retrieved trajectory used as an exemplar demonstration (in-context) for planning the current trajectory; retrieval influences the LLM prompt and thus generation of the current trajectory. The paper simplified original two-step retrieval to single-step top-1 similarity to improve reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compares fixed-memory baseline, add-all, coarse and strict selective addition; compares deletion strategies. Observes experience-following (input similarity correlated with output similarity). Add-all and noisy coarse addition show immediate and compounding degradation (error propagation). Strict selective addition combined with history-based deletion can approach and sometimes surpass an 'error-free' variant in long runs, suggesting selective retention of high-utility experiences can outperform naive memory.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Strict selective addition (use reliable numeric trajectory error thresholds) together with history-based or combined deletion; history-based deletion that removes records with low downstream utility after sufficient retrievals can yield large long-term gains and even surpass error-free baselines in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>When coarse/noisy evaluators are used for addition/deletion, history-based deletion can harm performance (removing useful entries or retaining poor ones). Experience-following can amplify suboptimal trajectories leading to compounding errors. Results vary by evaluator and backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Use numeric, task-appropriate strict evaluators for addition (e.g., L2 thresholds) and history-based deletion to remove frequently-retrieved low-utility experiences; combined deletion helps reduce memory size while preserving performance; ensure evaluator reliability (fine-tuning/coaching) before relying on history-based deletion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8223.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8223.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CIC-IoT Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CIC-IoT Agent (IoT traffic classification agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based agent for single-flow IoT traffic-type classification that retrieves past flow-feature→label experiences as episodic memory (K=3) and uses them as in-context demonstrations to classify current flows across 8 retained classes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ciciot2023: A real-time dataset and benchmark for large-scale attacks in iot environment.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CIC-IoT Agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Network-traffic classification agent using single-flow features formatted into a prompt; retrieves three past experiences per execution; predictions are judged by coarse LLM evaluators or strict string-match evaluators; evaluated by ACC.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4o-mini (backbone for memory and some evaluators); coarse evaluators implemented as LLM judges; strict evaluator uses exact-match with ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Backbone LLMs generate predictions; coarse evaluators are LLM-based binary judges; strict evaluators rely on exact-match criteria. Text embeddings (textembedding-3-large) used for retrieval embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>CIC-IoT single-flow traffic classification task (filtered subset)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Classify IoT traffic type from single-flow features across 8 attack/traffic classes (from CIC-IoT dataset); challenges include cross-field consistency and correlating multiple flow features; retrieval uses a feature-based similarity metric.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Memory bank of (flow-features, predicted label/execution) pairs; initial memory of synthetic records generated by GPT-4o-mini; retrieval uses a feature-relative-change similarity measure; K = 3 experiences retrieved per execution.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Retrieved experiences included as in-context demonstrations in prompt; coarse evaluator judges experience quality for selective addition; strict evaluator uses string-match to ground truth for addition.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compares fixed-memory, add-all, coarse evaluators (LLM judge), and strict selective addition; deletion experiments (periodic, history-based, combined). Observes that fixed-memory sometimes performs well; noisy addition lowers longterm ACC; history-based deletion removes low-utility experiences and can improve retained record quality when evaluator is reliable.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Strict selective addition (exact-match or strong evaluator) combined with history-based or combined deletion; ensure deletion evaluator thresholds (e.g., β) and minimal retrieval counts are chosen to avoid premature removal.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Coarse LLM judges without fine-tuning can be noisy, causing misaligned replay and degraded ACC; memory summarization and structural merging are outside the scope of this study and might change dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Prioritize high-quality additions (strict evaluator or fine-tuned coarse judge); use history-based deletion that requires a minimum retrieval count before removal and uses downstream utility thresholds; combined deletion gives a practical tradeoff for constrained-memory deployments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Ehragent: Code empowers large language models for complex tabular reasoning on electronic health records. <em>(Rating: 2)</em></li>
                <li>A language agent for autonomous driving. <em>(Rating: 2)</em></li>
                <li>Ciciot2023: A real-time dataset and benchmark for large-scale attacks in iot environment. <em>(Rating: 2)</em></li>
                <li>Memorybank: Enhancing large language models with long-term memory. <em>(Rating: 1)</em></li>
                <li>Explicit memory learning with expectation maximization. <em>(Rating: 1)</em></li>
                <li>Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8223",
    "paper_id": "paper-278788765",
    "extraction_schema_id": "extraction-schema-151",
    "extracted_data": [
        {
            "name_short": "RegAgent",
            "name_full": "RegAgent (synthetic regression agent)",
            "brief_description": "A synthetic, controllable LLM-agent environment that predicts y = w^T x by retrieving K nearest demonstrations from an episodic memory bank; designed to isolate and measure the impact of memory addition/deletion on long-term agent performance and error propagation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RegAgent",
            "agent_description": "Synthetic regression agent that must predict a scalar y = w^T x given a 6-d input x by conditioning on K = 6 nearest stored demonstrations (input,guess) pairs; used to precisely control noise and measure imitation vs reasoning.",
            "llm_model_name": "GPT-4o-mini (backbone for most experiments)",
            "llm_model_description": "GPT-4o-mini is used as the backbone LLM for most experiments; experiments also report results on other LLM backbones (GPT-4o, DeepSeek-V3) to check robustness. (paper uses model as an in-context generator and as coarse evaluators in variants).",
            "benchmark_name": "RegAgent synthetic regression task",
            "benchmark_description": "Controlled regression prediction task: samples x from three Gaussians, agent must return y = w^T x + noise; success defined as |ŷ - y| ≤ 1. Retrieval uses cosine similarity on inputs; output similarity computed via an RBF-style function.",
            "memory_used": true,
            "memory_type": "episodic",
            "memory_architecture": "Memory bank of query-execution pairs (q, e). Stored as an expanding set of (input vector, predicted output) records; retrieval is nearest-neighbor (cosine for inputs) returning K demonstrations which are used as in-context examples. Addition and deletion are explicit operations controlled by evaluators.",
            "memory_integration_strategy": "Retrieved top-K demonstrations are concatenated/used as in-context few-shot demonstrations in the prompt for the LLM; agent then generates execution conditioned on these demonstrations. Retrieval influences the agent via demonstration-following rather than gradient updates.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Compares fixed-memory baseline (no additions) vs add-all vs selective addition with three coarse automatic evaluators (C1,C2,C3) and strict human-like evaluator; also compares deletion strategies (periodic, history-based, combined). Finds (i) experience-following: high input similarity → high output similarity (near Pearson r≈1 when demonstrations are very similar); (ii) add-all and coarse evaluators with noisy additions degrade long-run success rates due to error propagation, while strict/selective addition yields sustained improvement; history-based deletion removes misaligned/noisy records and can modestly improve or maintain performance.",
            "best_memory_strategy": "Paper identifies strict selective addition (high-quality evaluator) combined with history-based or combined deletion as the most reliable strategy: only store high-utility trajectories and remove demonstrations that consistently yield low downstream utility.",
            "limitations_or_failure_cases": "Strong experience-following makes RegAgent highly susceptible to error propagation: noisy stored demonstrations cause the agent to imitate and amplify errors; naive add-all leads to flat or declining success rates. History-based deletion can remove useful entries if the utility evaluator is coarse/noisy. The synthetic environment purposely emphasizes reliance on demonstrations, so results may be exaggerated relative to other tasks.",
            "recommendations_or_conclusions": "Use selective addition driven by a reliable trajectory evaluator (strict/human-like or fine-tuned judge) to avoid accumulating noisy executions; apply history-based deletion that removes records with low downstream utility (after sufficient retrieval counts) or a combined deletion (periodic + history-based) to balance memory size and utility; evaluator reliability is critical.",
            "uuid": "e8223.0",
            "source_info": {
                "paper_title": "How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "EHRAgent",
            "name_full": "EHRAgent (EHR code-generation agent)",
            "brief_description": "An LLM-based code-generation agent that answers electronic health record queries by retrieving a small number of past query–execution pairs (4 experiences) from an episodic memory bank and using them as in-context demonstrations for SQL-like code generation and execution.",
            "citation_title": "Ehragent: Code empowers large language models for complex tabular reasoning on electronic health records.",
            "mention_or_use": "use",
            "agent_name": "EHRAgent",
            "agent_description": "Code-generation agent for clinical EHR queries; retrieves 4 past experiences via text-embedding similarity and uses them as demonstrations to drive code generation (SQL-like) for MIMIC-III tasks; evaluated with accuracy (ACC).",
            "llm_model_name": "GPT-4o-mini (backbone), with coarse evaluators using GPT-4o-mini/GPT-4.1-mini variants",
            "llm_model_description": "Paper uses GPT-4o-mini as the primary backbone LLM; coarse evaluators include GPT-4o-mini and GPT-4.1-mini and a fine-tuned GPT-4.1-mini for stronger automatic judging.",
            "benchmark_name": "EHRAgent EHR code-generation benchmark (MIMIC-III subset)",
            "benchmark_description": "Natural-language EHR queries mapped to code + execution on MIMIC-III; challenges include complex tabular reasoning and code correctness; evaluation via exact-match for strict judge and LLM-based termination signal for coarse judge.",
            "memory_used": true,
            "memory_type": "episodic",
            "memory_architecture": "Memory bank of (query, code-execution) records with embeddings (OpenAI text-embedding-3-large). Retrieval: top-4 by cosine similarity; stored records appended as in-context demonstrations. Addition determined by evaluator signal (automatic coarse LLM judge or strict exact-match human-like judge).",
            "memory_integration_strategy": "Concatenate retrieved past experiences (code + results) as in-context demonstrations to the prompt for the LLM code generator; the highest-input-similarity retrieved example is used to compute output similarity (pycode_similar used to measure code similarity).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Compares fixed-memory baseline (no additions), add-all, coarse automatic evaluators (different LLMs and a fine-tuned judge), and strict addition. Finds fixed-memory can be strong initially; add-all and coarse addition sometimes lead to flat or declining ACC due to accumulation of flawed records; strict selective addition and fine-tuned coarse evaluator (C3) show consistent self-improvement over time. Deletion experiments show periodical deletion reduces memory size with small performance drops; history-based deletion helps when utility evaluator is reliable.",
            "best_memory_strategy": "Strict selective addition (high-quality evaluator, e.g., exact-match human-like judge) combined with history-based or combined deletion when using a reliable utility evaluator; fine-tuning coarse evaluators (even with a small dataset) substantially improves addition quality.",
            "limitations_or_failure_cases": "Coarse LLM judges (vanilla LLMs) can be noisy and may worsen long-term performance if used for addition/deletion; fixed-memory is static and cannot adapt to distribution shifts; history-based deletion performance depends strongly on the evaluator's reliability.",
            "recommendations_or_conclusions": "Prioritize evaluator quality (fine-tune judges or use strict human-like criteria) for memory addition; use history-based deletion guided by downstream utility with thresholds for minimum retrieval counts to avoid removing entries based on sparse data; combined deletion (periodic + history) is a practical tradeoff for stability under shift.",
            "uuid": "e8223.1",
            "source_info": {
                "paper_title": "How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "AgentDriver",
            "name_full": "AgentDriver (autonomous driving language agent)",
            "brief_description": "An LLM-based autonomous driving agent that augments planning with episodic experiences (trajectories); retrieves a single most-relevant past trajectory and uses it as an exemplar to influence the predicted short-horizon trajectory.",
            "citation_title": "A language agent for autonomous driving.",
            "mention_or_use": "use",
            "agent_name": "AgentDriver",
            "agent_description": "Autonomous-driving LLM agent evaluated on nuScenes; originally uses a two-step retrieval (top-3 then LLM picks), simplified here to top-1 vector-similarity retrieval; retrieved trajectory is used as an exemplar to guide planning; output evaluated by UniAD 3-second average L2 distance and a derived success metric.",
            "llm_model_name": "GPT-4o-mini (backbone) with other backbones tested (GPT-4o, DeepSeek-V3); coarse evaluators use LLM binary (yes/no) judges; strict evaluator uses numeric L2 distance threshold.",
            "llm_model_description": "LLM backbones used to generate trajectories and to serve as coarse evaluators; strict evaluator computes UniAD 3s L2 distance to ground truth (threshold-based). Experiments include backbone variants to validate generality.",
            "benchmark_name": "AgentDriver autonomous driving benchmark (nuScenes sampled cases)",
            "benchmark_description": "Short-horizon trajectory prediction and planning (6-step horizon), challenges include safety, drivable area constraints, and collision avoidance; performance measured by trajectory distance metrics and success criteria.",
            "memory_used": true,
            "memory_type": "episodic",
            "memory_architecture": "Memory bank of (scene-query, predicted trajectory) experiences; retrieval by vector similarity (original two-stage simplified to top-1), memory stores sequences of state/trajectory; addition controlled by evaluators (LLM-based coarse judge or L2-based strict judge), deletion via periodic/history/combined strategies.",
            "memory_integration_strategy": "Retrieved trajectory used as an exemplar demonstration (in-context) for planning the current trajectory; retrieval influences the LLM prompt and thus generation of the current trajectory. The paper simplified original two-step retrieval to single-step top-1 similarity to improve reproducibility.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Compares fixed-memory baseline, add-all, coarse and strict selective addition; compares deletion strategies. Observes experience-following (input similarity correlated with output similarity). Add-all and noisy coarse addition show immediate and compounding degradation (error propagation). Strict selective addition combined with history-based deletion can approach and sometimes surpass an 'error-free' variant in long runs, suggesting selective retention of high-utility experiences can outperform naive memory.",
            "best_memory_strategy": "Strict selective addition (use reliable numeric trajectory error thresholds) together with history-based or combined deletion; history-based deletion that removes records with low downstream utility after sufficient retrievals can yield large long-term gains and even surpass error-free baselines in some cases.",
            "limitations_or_failure_cases": "When coarse/noisy evaluators are used for addition/deletion, history-based deletion can harm performance (removing useful entries or retaining poor ones). Experience-following can amplify suboptimal trajectories leading to compounding errors. Results vary by evaluator and backbone.",
            "recommendations_or_conclusions": "Use numeric, task-appropriate strict evaluators for addition (e.g., L2 thresholds) and history-based deletion to remove frequently-retrieved low-utility experiences; combined deletion helps reduce memory size while preserving performance; ensure evaluator reliability (fine-tuning/coaching) before relying on history-based deletion.",
            "uuid": "e8223.2",
            "source_info": {
                "paper_title": "How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "CIC-IoT Agent",
            "name_full": "CIC-IoT Agent (IoT traffic classification agent)",
            "brief_description": "An LLM-based agent for single-flow IoT traffic-type classification that retrieves past flow-feature→label experiences as episodic memory (K=3) and uses them as in-context demonstrations to classify current flows across 8 retained classes.",
            "citation_title": "Ciciot2023: A real-time dataset and benchmark for large-scale attacks in iot environment.",
            "mention_or_use": "use",
            "agent_name": "CIC-IoT Agent",
            "agent_description": "Network-traffic classification agent using single-flow features formatted into a prompt; retrieves three past experiences per execution; predictions are judged by coarse LLM evaluators or strict string-match evaluators; evaluated by ACC.",
            "llm_model_name": "GPT-4o-mini (backbone for memory and some evaluators); coarse evaluators implemented as LLM judges; strict evaluator uses exact-match with ground truth.",
            "llm_model_description": "Backbone LLMs generate predictions; coarse evaluators are LLM-based binary judges; strict evaluators rely on exact-match criteria. Text embeddings (textembedding-3-large) used for retrieval embedding.",
            "benchmark_name": "CIC-IoT single-flow traffic classification task (filtered subset)",
            "benchmark_description": "Classify IoT traffic type from single-flow features across 8 attack/traffic classes (from CIC-IoT dataset); challenges include cross-field consistency and correlating multiple flow features; retrieval uses a feature-based similarity metric.",
            "memory_used": true,
            "memory_type": "episodic",
            "memory_architecture": "Memory bank of (flow-features, predicted label/execution) pairs; initial memory of synthetic records generated by GPT-4o-mini; retrieval uses a feature-relative-change similarity measure; K = 3 experiences retrieved per execution.",
            "memory_integration_strategy": "Retrieved experiences included as in-context demonstrations in prompt; coarse evaluator judges experience quality for selective addition; strict evaluator uses string-match to ground truth for addition.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Compares fixed-memory, add-all, coarse evaluators (LLM judge), and strict selective addition; deletion experiments (periodic, history-based, combined). Observes that fixed-memory sometimes performs well; noisy addition lowers longterm ACC; history-based deletion removes low-utility experiences and can improve retained record quality when evaluator is reliable.",
            "best_memory_strategy": "Strict selective addition (exact-match or strong evaluator) combined with history-based or combined deletion; ensure deletion evaluator thresholds (e.g., β) and minimal retrieval counts are chosen to avoid premature removal.",
            "limitations_or_failure_cases": "Coarse LLM judges without fine-tuning can be noisy, causing misaligned replay and degraded ACC; memory summarization and structural merging are outside the scope of this study and might change dynamics.",
            "recommendations_or_conclusions": "Prioritize high-quality additions (strict evaluator or fine-tuned coarse judge); use history-based deletion that requires a minimum retrieval count before removal and uses downstream utility thresholds; combined deletion gives a practical tradeoff for constrained-memory deployments.",
            "uuid": "e8223.3",
            "source_info": {
                "paper_title": "How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Ehragent: Code empowers large language models for complex tabular reasoning on electronic health records.",
            "rating": 2,
            "sanitized_title": "ehragent_code_empowers_large_language_models_for_complex_tabular_reasoning_on_electronic_health_records"
        },
        {
            "paper_title": "A language agent for autonomous driving.",
            "rating": 2,
            "sanitized_title": "a_language_agent_for_autonomous_driving"
        },
        {
            "paper_title": "Ciciot2023: A real-time dataset and benchmark for large-scale attacks in iot environment.",
            "rating": 2,
            "sanitized_title": "ciciot2023_a_realtime_dataset_and_benchmark_for_largescale_attacks_in_iot_environment"
        },
        {
            "paper_title": "Memorybank: Enhancing large language models with long-term memory.",
            "rating": 1,
            "sanitized_title": "memorybank_enhancing_large_language_models_with_longterm_memory"
        },
        {
            "paper_title": "Explicit memory learning with expectation maximization.",
            "rating": 1,
            "sanitized_title": "explicit_memory_learning_with_expectation_maximization"
        },
        {
            "paper_title": "Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences.",
            "rating": 1,
            "sanitized_title": "latent_learning_episodic_memory_complements_parametric_learning_by_enabling_flexible_reuse_of_experiences"
        }
    ],
    "cost": 0.014232499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior
10 Oct 2025</p>
<p>Zidi Xiong zidixiong@g.harvard.edu 
Harvard University</p>
<p>Yuping Lin 
Michigan State University</p>
<p>Wenya Xie 
University of Minnesota-Twin Cities</p>
<p>Pengfei He 
Michigan State University</p>
<p>Zirui Liu 
University of Minnesota-Twin Cities</p>
<p>Jiliang Tang 
Michigan State University</p>
<p>Himabindu Lakkaraju 
Harvard University</p>
<p>Zhen Xiang 
University of Georgia</p>
<p>How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior
10 Oct 202523BF418E334B5AE89320CA987B82AE02arXiv:2505.16067v2[cs.AI]
Memory is a critical component in large language model (LLM)-based agents, enabling them to store and retrieve past executions to improve task performance over time.In this paper, we conduct an empirical study on how memory management choices impact the LLM agents' behavior, especially their long-term performance.Specifically, we focus on two fundamental memory management operations that are widely used by many agent frameworks-memory addition and deletion-to systematically study their impact on the agent behavior.Through our quantitative analysis, we find that LLM agents display an experiencefollowing property: high similarity between a task input and the input in a retrieved memory record often results in highly similar agent outputs.Our analysis further reveals two significant challenges associated with this property: error propagation, where inaccuracies in past experiences compound and degrade future performance, and misaligned experience replay, where some seemingly correct executions can provide limited or even misleading value as experiences.Through controlled experiments, we demonstrate the importance of regulating experience quality within the memory bank and show that future task evaluations can serve as free quality labels for stored memory.Our findings offer insights into the behavioral dynamics of LLM agent memory systems and provide practical guidance for designing memory components that support robust, long-term agent performance.We also release our code to facilitate further study. 1* Equal contributions.</p>
<p>Introduction</p>
<p>To enable effective solving of complex tasks and self-evolving over time, large language model (LLM) agents often equip an episodic memory module (Wang et al., 2024a).This dynamic mechanism retains past task queries (i.e. the agent input) and execution (i.e. the agent output), which can be retrieved as demonstrations to guide similar future tasks.Through effective memory management, including adding, updating, and deleting past experiences, the performance of LLM agents can be improved over time (Zhang et al., 2024).</p>
<p>In practice, memory designs vary widely across agents due to their diverse objectives and functionalities (Shi et al., 2024;Mao et al., 2024;Wang et al., 2023;Li et al., 2023;Park et al., 2023;Xiang et al., 2024).To address this heterogeneity, recent studies have proposed a range of memory management strategies (Yin et al., 2024;Zeng et al., 2024;Zhao et al., 2024;Zhong et al., 2024;Xu et al., 2025).However, these approaches are often tailored to specific tasks and offer limited understanding of the underlying principles that govern memory behavior across different agentic systems.</p>
<p>In this work, we investigate a fundamental question in memory management: How do the evolving dynamics of the memory bank, driven by continuous memory operations, influence long-term agent execution?In particular, we focus on the core mechanisms underlying these dynamics-memory addition and memory deletion-as they constitute the foundation of memory management (see .Moreover, unlike prior studies on in-context learning with static, external knowledge bases (Luo et al., 2024), our setting with these two operations captures the distinctive nature of agentic memory systems: a dynamic retrieval pool (memory bank) that evolves over time and contains inherently noisy outputs (trajectories) that are often generated by the agent itself.</p>
<p>Through extensive experiments, we identify an important phenomenon that we term the experiencefollowing property: a high 'input similarity' between the current task query and the one from the retrieved record often yields a high 'output similarity' between their corresponding (output) executions.While this property enables effective reuse of successful experiences, we uncover two significant challenges arising from the dynamic and noisy nature of agentic memory banks.First, we observe the problem of error propagation: if a retrieved memory record contains noisy or incorrect outputs, the agent is likely to replicate and even amplify these errors during the current task.If the resulting execution is then added back into memory, the error is likely to be further propagated to future tasks.Second, we recognize the issue of misaligned experience replay, which limits the benefits of experience following-certain memory records, when retrieved as demonstrations, consistently lead to poor execution due to their misalignment with the current task, indicating their inadequacy as effective demonstrations.Retaining these records increases the likelihood of suboptimal or incorrect executions.</p>
<p>To systematically analyze these phenomena, we conduct controlled experiments across four distinct agents operating on diverse tasks.Crucially, we reveal the critical yet underexplored role of trajectory evaluators in both memory addition and deletion processes, providing novel insights related to effective memory bank optimization that have been significantly overlooked in existing literature.</p>
<p>We further examine challenging conditions representing real-world scenarios, including (1) Task Distribution Shift: the task distribution changes substantially over time, requiring the agent to adapt to shifting patterns and contexts.(2) Memory Resources Constraint: the memory capacity is severely limited, requiring the agent to retain only the most valuable and helpful experience.The results underscore the importance of thoughtful memory management for maintaining stable and effective long-term agent performance.</p>
<p>Our contributions are summarized below:</p>
<p>• We conduct systematic and quantitative analysis of the agent memory dynamics through two most essential memory operations-addition and deletion-of the memory records on the longterm performance of LLM agents.Our findings provide key principles for robust memory management design.</p>
<p>• We reveal an experience-following property for LLM agents with memory and highlight two important challenges for memory management-error propagation and misaligned experience replay-arising with this property.</p>
<p>• We demonstrate how incorporating environmental feedback from trajectory evaluators can effectively improve memory management by mitigating both error propagation and misaligned experience replay.</p>
<p>• We study how memory evolution affects longterm agent performance under task distribution shifts and memory constraints, finding that wellchosen evaluators enable agents to adapt to these scenarios and sustain performance with simple addition and deletion.</p>
<p>2 Background and Related Works</p>
<p>Memory Module of LLM Agents</p>
<p>LLM agents often include short-term memory and long-term memory (Zhang et al., 2024).Shortterm memory usually refers to inside-task working memory (Sumers et al., 2023), while long-term memory (Sumers et al., 2023) can be divided into three types: semantic memory, procedural memory, and episodic memory.Semantic memory (Kumar, 2020) contains the agent's world knowledge and understanding of the environment; procedural memory (Beaunieux et al., 2006) involves rules or procedures, which may reside implicitly in the LLM's weights or be explicitly defined as guidelines for the agent; and episodic memory (Nuxoll and Laird, 2007;Lampinen et al., 2025) records task-specific experiences.In this paper, we specifically focus on episodic memory, which is commonly used by LLM agents with memory and has recently been shown to mitigate the lack of latent learning capability in LLMs (Lampinen et al., 2025).</p>
<p>Memory Management and Limitations</p>
<p>LLM agents often employ an episodic memory module to store past experiences for future retrieval, which is crucial for effective planning and task execution (Shi et al., 2024;Mao et al., 2024;Xu et al., 2025;Zhou et al., 2025;Zhong et al., 2024;Yin et al., 2024).The memory module typically includes operations such as memory reading and memory management (Zhang et al., 2024).Specifically, given a new task query q and a memory base currently containing N query-execution pairs D = {(q 1 , e 1 ), . . ., (q N , e N )}, the agent's execution cycle involves the following memory operations:</p>
<p>Memory Reading: The agent retrieves a subset ξ K ⊂ D consisting of the K query-execution pairs most relevant to the query q.The relevance is often measured by the input similarity between the task query q and the query from the retrieved past experience.For example, the relevance can be computed as the cosine similarity between the feature representations for both queries from a text encoder (Kusupati et al., 2022).The retrieved pairs ξ K are then used as in-context learning demonstrations to guide the LLM in generating an execution trajectory e for the task query q.</p>
<p>Memory Management and Limitations: Memory management typically involves the addition and deletion of memory records.Specifically, when obtaining the query-execution trajectory pair (q, e), addition decides whether this pair should be added to memory, while deletion (Zhong et al., 2024;Wang et al., 2024b) is often triggered to remove outdated or redundant pairs from memory.Although various strategies have been proposed for memory management, such as structural transformation (Anokhin et al., 2024;Zeng et al., 2024;Xu et al., 2025), merging (Zhong et al., 2024;Liu et al., 2023;Hu et al., 2024), summarization (Wang et al., 2025;Zhong et al., 2024;Pan et al., 2025), and reflection (Shinn et al., 2024;Zhao et al., 2024;Liang et al., 2024), these approaches are often designed for specific agent types (e.g., chatbot agents) and lack a unified design that generalizes across different agentic systems.</p>
<p>Recent works also study the optimization of the memory bank, such as using an expectation-maximization approach for optimization (Yin et al., 2024) or formulating memory management as a Markov Decision Process (Zhou et al., 2025).However, these works do not provide an understanding of how memory bank dynamics affect long-term performance under the inherently noisy nature of basic addition and deletion operations.</p>
<p>In contrast, our findings highlight potential challenges posed by these operations and offer insights to guide the design of future memory systems.</p>
<p>Addition of Memory</p>
<p>Memory addition refers to the process of deciding whether a finished task execution should be stored as a new record in the memory bank.Here, we consider the most common approach that adds memory based on the feedback of the trajectory evaluator (Wang et al., 2025;Shi et al., 2024).We investigate three types of memory addition with different trajectory evaluators-an add-all baseline, selective addition based on coarse automatic evaluation, and selective addition based on strict human evaluation-along with a fixed-memory baseline without memory addition.</p>
<p>Setup</p>
<p>Setup for Agents We investigate memory management in generic LLM agent settings by considering an illustrative and controllable synthetic agent and three representative agents designed for different tasks: For the synthetic agent, we design a task in which the agent predicts the output of a simple mapping -a linear function in the ground truth -by performing regression approximation over the provided demonstrations, which we call RegAgent.Formally, the agent receives an input vector x together with several past guesses over nearby inputs under the linear transformation w to infer an output for w ⊤ x.This setup allows us to investigate an ideal setting where the model fully relies on the provided demonstrations to generate the output, while enabling control over the noise level of the stored memory and direct measurement of the resulting error.</p>
<p>For real agent, we include EHRAgent (Shi et al., 2024), AgentDriver (Mao et al., 2024), and CIC-IoT Agent (Neto et al., 2023).In addition to their distinct tasks, these agents also vary significantly in input-output formats and memory retrieval mechanisms, which enhances the generalizability of our findings.For all four agents, we use GPT-4o-mini as the backbone language model for most of the experiments.See Appendix A.1 for details of these agents.</p>
<p>Setup for Memory Addition</p>
<p>We begin by describing several memory addition using different trajectory evaluator.For a query-execution pair (q, e) and an evaluator π, the addition decision is given by π(q, e): if π(q, e) = 1, the experience is stored; if π(q, e) = 0, it is discarded.Our aim is to analyze how the noise introduced by additions from different evaluators influences the agent's behavior and long-term performance.Starting from an identical initial memory, we examine the following four categories of memory additions:</p>
<p>1) Fixed-memory baseline: In this baseline setting, we use a subset of the training data with correct agent execution as the fixed memory bank (Mao et al., 2024;Li et al., 2023;Zhao et al., 2024;Zheng et al., 2024), and the agent relies only on this fixed memory without adding new entries, i.e., π fixed (q, e) = 0.</p>
<p>2) Add-all approach: A straightforward addition is to store every encountered task and its execution: π all (q, e) = 1.</p>
<p>3) Selective addition based on automatic evaluation (coarse): We have π automatic (q, e) = Automatic(q, e) and evaluate three automatic evaluators (Wang et al., 2025;Yin et al., 2024;Liang et al., 2024), denoted as Coarse 1 (C1), Coarse 2 (C2), and Coarse 3 (C3).For RegAgent, we define the evaluator based on the absolute error between the predicted and ground-truth outputs, where a smaller error threshold indicates a more accurate evaluator.Specifically, we set the thresholds to 1.6, 1.4, and 1.2 for C1, C2, and C3, respectively.For the other three agents, C1 and C2 correspond to GPT-4o-mini and GPT-4.1-miniused as evaluators, respectively, while C3 employs GPT-4.1-minifine-tuned on 300 correct judge data collected from executions on a separate training set.Detailed designs of these automatic evaluators are provided in Appendix A.4.</p>
<p>4) Selective addition based on human evaluation (strict):In this stricter approach, a human (oracle) serves as the evaluator (Shi et al., 2024), determining whether the execution should be stored in the memory: π human (q, e) = Human(q, e).Although human input could be gathered after each execution for practical agents, it is not feasible for our current evaluation.Therefore, we simulate this process by comparing the generated output with the ground truth.More details on each agent's specific designs for memory addition can be found in Appendix A.</p>
<p>Execution quality and memory size jointly</p>
<p>determine long-term agent performance.</p>
<p>Table 1 summarizes the performance of memory addition strategies guided by five evaluators across  four agents.Although all methods start with the same initial memory, their long-term performances diverge notably.The fixed-memory baseline maintains strong results with RegAgent, AgentDriver, and CIC-IoT Agent-sometimes even surpassing runs guided by coarse evaluators-indicating that noisy or low-quality additions can harm memory utility.In contrast, using a strict evaluator that selectively expands memory with high-quality records consistently yields superior performance, showing that both memory quality and capacity are key to effective long-term learning.Furthermore, performance relies on the judge's capability for coarse evaluators.For the three model judged agents, fine-tuning the evaluator on only 300 trajectories (C3) already achieves strong long-term improvements, outperforming other coarse evaluators and unfiltered additions.</p>
<p>As shown in Figure 2 and Figure 8 in Appendix B.1, the add-all and some coarse addition approach exhibits flat or declining success rates, suggesting accumulation of flawed records.The fixed-memory baseline performs better but remains static due to its frozen memory.In contrast, strict selective addition and C3 coarse evaluator continue to improve over time, a trend consistent across different LLM backbones (Appendix B.2) with strict addition.These results suggest that careful construction of the evaluator is particularly important, as directly applying a vanilla LLM as a trajectory evaluator (Wang et al., 2025;Pan et al., 2024) may lead to a more severe negative impact than manually crafting a small but high-quality dataset.</p>
<p>Experience-Following Property</p>
<p>For each memory addition strategy, we measure both the input similarities and output similarities between each query and the memory records retrieved during its execution.The computation of both types of similarities for each query can be found in Appendix A. For each of the input similarities and the output similarities, we compute the cumulative average over the entire stream of test query executions to demonstrate the long-term trend.As shown in Figure 3 for RegAgent and AgentDriver, the fixed-memory baseline produces both low input and output similarities, whereas the addition-based methods exhibit higher output similarity as the input similarity grows.Similar patterns are observed for the other two agents (Figure 9 in Appendix B.1) and alternative LLM backbones such as state-of-the art GPT-4o and DeepSeek-V3 (Figure 12 in Appendix B.2).The exhibition of such a strong correlation between the input and output similarities is referred to as the experiencefollowing property of memory-based LLM agents.</p>
<p>Intuitively, this property reflects that LLM agents tend to imitate retrieved experience more closely when their current queries are similar to past examples.As the memory expands to include a broader range of experiences, the agent is more likely to retrieve highly similar records for new queries.For instance, in RegAgent, the agent can make predictions either by directly mimicking the closest demonstration or by internal reasoning to guess the mapping rule based on demonstrations.When the memory size increases and demonstrations become more similar to the query, the agent shows a near-perfect correlation (Pearson r ≈ 1) between input and output similarity, indicating strong demonstration following when input similarity is high.In contrast, under the fixed-memory setting, both input and output similarities remain consistently lower than those achieved with memory addition, suggesting that the agent still relies on reasoning rather than pure imitation, and thus achieves surprisingly high performance compared with those using a coarse evaluator with noisy memory.A similar pattern is also observed in AgentDriver.Therefore, a careful selection strategy with a stronger evaluator for memory addition can facilitate the selfimprovement of the agent through the replication of correct executions of similar tasks.Conversely, indiscriminate memory addition will easily introduce incorrect executions for the agent to learn from, causing a self-degradation of its performance.</p>
<p>Error Propagation in Agent Memory</p>
<p>While experience-following enables LLM agents to learn from high-quality examples with a strong evaluator, the inclusion of erroneous memory records or seemingly well-performed trajectories with incorrect intermediate attempts remains inevitable in many cases, even under human inspection of the agent's execution.</p>
<p>When erroneous and noisy memory records are retrieved as demonstrations, these errors can influence the current task execution.If the current execution is then stored in the memory, the error may propagate to future tasks by affecting their execution.This error propagation poses significant challenges to memory management, causing a deviation between the agent's performance and the optimal level achievable with perfect memory.</p>
<p>In Figure 4, we visualize error propagation on RegAgent and AgentDriver.For each addition strategy in Section 3.1, we compare it with a variant that uses the same retrieved examples for each task but replaces the LLM's execution with the ground-truth output, ensuring error-free2 retrieval.</p>
<p>For both agents, we observe an immediate gap in performance compared to their error-free variant.Moreover, as the execution continues, both addall and coarse selective addition exacerbate such a performance gap.However, for AgentDrievr with strict selective addition, despite lagging initially, it gradually approaches the ground-truth baseline and even surprisingly surpasses its performance after roughly 2000 executions.These results again highlight the importance of both the quantity and quality of memory, underscoring the necessity of carefully selecting and training trajectory evaluators for stable long-term agent execution.</p>
<p>Deletion of Memory</p>
<p>Memory deletion is often necessary for real systems, as the memory cannot grow indefinitely due to the practical constraint of storage.In this section, we evaluate three memory deletion approaches: the commonly adopted periodic deletion, the proposed history-based deletion guided by future execution utility, and a hybrid approach that combines both.</p>
<p>Setup for Memory Deletion Experiments</p>
<p>We follow Section 3.1 for the four agent setups we considered.For the deletion, let (q i , e i ) be a queryexecution pair where 1 ≤ i ≤ N is the index.This pair will be removed from the memory bank if it satisfies a specific criterion ϕ.We focus on three memory deletion strategies that can be deployed together with selective memory addition (either coarse (automatic) or strict (human) evaluation):</p>
<p>1) Periodical-based Deletion: Prior research proposes to use a forgetting rate inspired by human cognition to assign deletion probabilities based on how frequently and how recently a memory record is retrieved (Zhong et al., 2024;Hou et al., 2024;Wang et al., 2024b).In this work, we adopt a simplified (thus more widely applicable) strategy using a fixed threshold to determine whether a memory record should be deleted based on its past retrieval frequency during a given period.Specifically, Let fr t (q i , e i ) denote the total retrievals of (q i , e i ) at the current timestamp t, and fr t ′ (q i , e i ) be the retrievals at an earlier timestamp t ′ .Define α as the target retrieval count within [t ′ , t].A memory record will be deleted if ϕ per (q i , e i , t, t ′ ) = 1 where
ϕ per (q i , e i , t, t ′ ) = 1[ fr t (q i , e i ) − fr t ′ (q i , e i ) ≤ α ] .
This approach keeps the memory size M bounded by M ≤ α(t − t ′ )K, where K is the number of retrieved memory per execution.</p>
<p>2) History-Based Deletion: Following the previous discussion on experience-following and error propagation, we hypothesize a correlation between the quality of experiences stored in memory and the downstream execution quality of future tasks when these experiences are retrieved.Thus, we propose a simple history-based deletion strategy guided by the utility of stored memory records over time.Specifically, suppose Φ is a utility evaluator, which could be the same one used for selective addition.For any timestamp t, a record will be removed if a) it has been retrieved for at least n times, and b) the average utility across all its past retrievals is below a prescribed threshold β:
ϕ hist (q i , e i , t) = δ(q i , e i , t), if fr t (q i , e i ) &gt; n, 0, otherwise.
where
δ(q i , e i , t) = 1   1 frt(q i ,e i ) frt(q i ,e i ) m=1 Φ(q m , e m ) ≤ β   .
Here, Φ(q m , e m ) denotes the utility obtained from the m-th retrieval of the specific memory record (q i , e i ).Note that we require a memory record to be retrieved at least n times before being considered for removal to reduce the estimation bias for the average utility.</p>
<p>3) Combined Deletion: Periodical-and historybased methods can be applied together to jointly enhance the agent's performance and reduce the memory size: ϕ comb (q i , e i , t, t ′ ) = ϕ per (q i , e i , t, t ′ ) ∨ ϕ hist (q i , e i , t).</p>
<p>Strategic Memory Deletion Improves the Agent Performance</p>
<p>Table 2 summarizes the effects of three deletion strategies on both agent performance and memory size.</p>
<p>We first observe that periodical-based deletion achieves substantial memory reduction with small performance degradation in general, indicating that Table 2: Performance of periodical-based deletion, history-based deletion, and the combined approach, when deployed with the two selective addition strategies (strict and coarse with C1 evaluator) and using the same evaluator for history-based and combined deletion.The history-based approach in real agents often yields the best performance when paired with a strict evaluator among deletion approaches, while the combined method reduces the memory size the most.addition-only memory designs often accumulate redundant entries.</p>
<p>History-based deletion and combined deletion show more variable results depending on the reliability of the utility evaluator used during addition and deletion.Specifically, when a strict evaluator is employed, history-based deletion leads to notable performance improvements with the non-synthetic agent we considered.In addition, for our synthetic RegAgent, which exhibits strong experiencefollowing behavior and relies on a large memory capacity, we observe that history-based deletion causes only a small performance drop compared with other strategies, even after removing several hundred frequent executions.</p>
<p>To further understand these improvements, we extend the methodology used in Section 3.4 to construct an error-free memory baseline during agent execution with AgentDriver and RegAgent.As presented in Appendix B.3, in AgentDriver, historybased deletion with a strict evaluator can even outperform this error-free counterpart with a larger magnitude compared with no deletion.This result suggests that selectively retaining experiences with high utility scores on downstream tasks can improve long-term performance.</p>
<p>However, when the evaluator used for both addition and deletion is coarse or noisy, history-based and combined deletion can influence the agent's long-term behavior in different ways.As shown in Figure 5 and Figure 14 (Appendix B.4), the impact of history-based deletion varies notably across evaluators based on LLMs without fine-tuning.For instance, when using GPT-4o-mini as the historybased evaluator, the deletion strategy yields a clear performance gain on EhrAgent but leads to degraded performance on AgentDriver.In contrast, fine-tuned evaluators tend to produce more stable results, achieving performance comparable to their addition-only counterparts while maintaining smaller memory sizes.</p>
<p>In summary, our findings show that with a reliable utility evaluator, history-based deletion can improve agent performance.Additionally, the combined deletion approach offers a strong balance between maintaining performance and reducing memory size, making it a practical and efficient memory management strategy for long-term LLM agent deployment.</p>
<p>Misaligned Experience Replay</p>
<p>To explain the performance gains achieved by history-based deletion, we hypothesize that certain memory records provide limited or even harmful guidance during task execution.These records, although initially passing the evaluator's quality filter, may still be misaligned with the objectives of the current task distribution.Such misalignment can arise from inconsistencies between their stored trajectories and the current execution context, or from errors introduced by the evaluator's limitations.Conditioning on such demonstrations can therefore degrade the quality of task execution.We refer to this phenomenon as misaligned experience replay, emphasizing the misalignment between the current task and its retrieved demonstrations from the memory bank.We attribute the effectiveness of history-based deletion to its ability to remove these misaligned memory records.</p>
<p>Since inconsistencies between context and task distributions are often case-specific and difficult to capture directly, we instead use the error induced by the evaluator's limitations for clearer illustration, as both factors jointly contribute to misaligned experience replay.Taking RegAgent as an example, the experience quality in terms of the error in the memory record can be directly observed through the discrepancy between its predicted and ground-truth value.As shown in Figure 6, we plot the kernel density estimation (KDE) curves for both deleted and retained records based on two different evaluators, respectively, revealing a clear quality gap: retained demonstrations exhibit lower error scores than deleted ones.Even under strict addition-where only data with error ≤ 1 were initially stored-conditioning on noisier experiences (e.g., entries with error &gt; 0.5) can still propagate errors to subsequent tasks more easily, leading to degraded predictions and thus removed by history-based deletion.Consistent trends across other agents (Appendix B.5) further demonstrate that history-based deletion effectively identifies and removes noisy or misaligned memory records with various evaluators.</p>
<p>These findings suggest that experience quality, especially the intrinsic quality of experience itself, is tightly linked to downstream execution quality.By effectively removing those demonstrations with low future execution quality through an evaluator, we can mitigate misaligned experience replay.</p>
<p>Memory Management under</p>
<p>Challenging Scenarios</p>
<p>Memory Management with Task distribution shift</p>
<p>Task distribution shift occurs when the predominant task type and the expected execution change over time.To simulate a task distribution shift, we construct a modified dataset from the original test sets of EHRAgent and AgentDriver by aggregating their input vectors and reordering them to alter the underlying task distribution.More details for setup can be found in Appendix A.5.</p>
<p>Results</p>
<p>In Figure 7, we compare performance trends across several memory configurations on EHRAgent and AgentDriver: fixed memory, strict addition, history-based deletion with a strict evaluator, and combined deletion with a strict evaluator.We also include results from a variant of combined deletion (with a strict evaluator) executed under a setting without task distribution shift, as shown in Section 4. We observe varying performance dynamics across task distributions.However, in general, the performance gap relative to the no-shift variant remains small.Notably, in AgentDriver, strict addition alone achieves performance that even surpasses the no-shift variant.In contrast, on EHRAgent, history-based deletion underperforms compared to combined deletion.This suggests that in practical scenarios involving distribution shift, periodic deletion-despite its simplicity-can contribute to stabilizing performance.</p>
<p>Memory Management with Resource Constraints</p>
<p>We also investigate a scenario where the memory capacity is fixed, for example, to its initial size of 100 records for EHRAgent and 180 records for AgentDriver.Under this constraint, we modify the combined deletion policy so that after each task execution, it first performs periodical-based deletion and then removes only the record with the least average utility (rather than all records below the utility threshold) if the memory still exceeds the limit after additions.</p>
<p>Results</p>
<p>As shown in Figure 17 in Appendix B.6, the memory management policies achieve high performance under strict capacity constraints compared with the fixed-memory variant.By selectively retaining only the most relevant and highquality records, the agent utilizes limited storage efficiently.These results indicate that effective memory management choices can still improve the long-term agent performance in resource-limited settings.We also study the relationship between the size of the constrained memory and the per-formance on AgentDriver, as shown in Figure 18 in Appendix B.6, which demonstrates a gradually converging performance when using a strict utility evaluator.This also suggests that naive, unbounded memory growth is unnecessary.</p>
<p>Conclusion</p>
<p>This paper studies memory management in LLM agents through addition and deletion.We identify the experience-following phenomenon and reveal two key challenges-error propagation and misaligned experience replay.Experiments demonstrate that evaluator reliability is critical and that incorporating evaluator signals is essential for effective memory management.</p>
<p>Limitations</p>
<p>One limitation of this work lies in the scope of memory management considered.To build a generalizable understanding of memory dynamics over time, this study focuses on two fundamental operations-memory addition and memory deletion-while omitting more complex and less generalizable mechanisms such as structural transformation (Zeng et al., 2024), merging (Liu et al., 2023), summarization (Pan et al., 2025), and reflection (Shinn et al., 2024).Consequently, when extending our findings to systems that integrate these advanced updating mechanisms or operate with different agent architectures, additional finegrained analyses will be helpful.</p>
<p>Another limitation concerns the lack of theoretical guarantees.Our findings are primarily based on extensive empirical analyses, which, although comprehensive, do not provide formal theoretical proofs.Given the inherent complexity of agentic systems, deriving such guarantees is often infeasible.Nonetheless, we believe that the proposed framework offers a solid foundation for future theoretical investigation, particularly through controlled experimentation in our synthetic RegAgent environment.</p>
<p>A Detailed experimental setups.</p>
<p>A.1 Agent details and functionality</p>
<p>RegAgents RegAgent serves as a synthetic and controlled environment to study the effects of memory management on long-term agent performance.We first generate an implicit weight vector w ∈ R 6 , and the task is to predict the correct output y = w ⊤ x by retrieving the six most relevant demonstrations from memory.Specifically, we generate 100 input vectors x sampled from three Gaussian distributions N (µ, 1) with µ ∈ −0.5, 0, 0.5, each of six dimensions, and compute y = w ⊤ x + ϵ, where ϵ is bounded noise within [−1, 1].These pairs form the initial memory bank.During actual execution, we collect 4000 input-output pairs from the same three distributions.A prediction ŷ is considered successful if |ŷ − y| ≤ 1, and we use the success rate as the primary performance metric.</p>
<p>For retrieval, input similarity is measured by cosine similarity between input vectors, while output similarity is defined as output_similarity = exp(−γ|x 1 − x 2 | 2 ), where γ = 1.0.</p>
<p>This task is highly controllable and well-suited for our study, as it (1) relies solely on the retrieved demonstrations, (2) allows the agent to either reason or directly mimic the demonstrations to produce outputs, and (3) enables clear definitions of error and addition criteria.</p>
<p>For evaluation, the strict evaluator applies a threshold of 1.0 on the absolute error between prediction and ground truth.Coarse evaluators use thresholds of 1.6 (C1), 1.4 (C2), and 1.2 (C3), respectively.Periodic deletion is performed with 500 steps with α = 0. History-based deletion is applied with a minimum deletion frequency of 5 and a threshold β = 0.5 across all setups.The combined deletion strategy reuses all hyperparameters above.We also list the task prompt in Appendix A.2 EHRAgents Following EHRAgent (Shi et al., 2024), our agent is a code-generation system that enables clinicians to interact with electronic health records (EHRs) through natural language queries.We adopt the MIMIC-III dataset (Johnson et al., 2016) for evaluation.After filtering out duplicated and unanswerable entries, we obtain 2,392 tasks in total.During each test query, the agent retrieves four past experiences from an initial memory bank containing 100 records, based on maximum cosine similarity computed using a general text embedding model.We report task performance in terms of accuracy (ACC).</p>
<p>For our implementation, we use OpenAI textembedding-3-large as the text encoder.For the coarse evaluator, we use the built-in LLMgenerated termination signal, which reflects if LLM considers the task to be completed.For the strict evaluator, we use exact match to determine whether the generated answer is correct.For input similarity, we take the highest cosine similarity between the text embedding of all retrieved experiences and the task.For output similarity, we use the pycode_similar package3 to detect the code plagiarism score between the execution from the retrieved memory with the highest input similarity and the current model execution.For periodic deletion, we use a period of 200 and α = 0.For history-based deletion, we set the minimal deleting frequency to 5 and the threshold β to 0.3 for the coarse evaluator using GPT-4o-mini and GPT-4.1-mini, and 0.7 for the strict and GPT-4.1-minifine-tuned evaluator.The combined deletion reuses all hyperparameters above.</p>
<p>AgentDriver Following AgentDriver (Mao et al., 2024), we adopt an LLM-based autonomous driving agent that integrates common sense and experiential knowledge into memory records.We use the nuScenes dataset (Caesar et al., 2020) and randomly sample 2,000 test cases from its test split.The initial memory contains 180 experiences randomly drawn from the nuScenes training set.</p>
<p>For the retrieval strategy, the original retrieval pipeline proposed by (Mao et al., 2024) consists of a two-step process: first, selecting the top-3 experiences based on vector similarity, and then prompting an LLM to choose the most relevant one.To improve reproducibility, we simplify this process to a single-step retrieval, selecting only the top-1 experience based on vector similarity.For the coarse evaluator, we employ an LLM to assess the quality of retrieved experiences.The LLM outputs either "yes" or "no" to indicate whether a given experience is a "good" experience or a "bad" one.The prompt used for this LLM-based evaluation is provided in Appendix A.4.For the strict evaluator, we measure the UniAD 3-second average L2 distance between the predicted trajectory and the ground truth.Experiences with an L2 error lower than 2.5 are added to the memory bank.For input similarity, we adhere to the computation method from the original paper.Specifically, we compute the exponential negative L2 distance between the query vector and stored experiences, using the same coefficients as in (Mao et al., 2024).For output similarity, we compute the radial basis function (RBF) kernel between the predicted trajectory and the ground-truth trajectory.</p>
<p>The score is calculated with the same formula in RegAgent:
output_similarity = exp −γ∥v 1 − v 2 ∥ 2 .
In our experiments, we set γ = 1.0.For periodic deletion, we apply a deletion period of 500 steps with a threshold of α = 0.For history-based deletion, we set a threshold of 5.0 for the UniAD 3-second average L2 distance.An experience is deleted if it has been retrieved at least 3 times and the mean UniAD 3-second average L2 distance across all retrievals exceeds this threshold.Coarse deletion leverages the predicted success rate from the coarse evaluator and deletes a record if the accumulated success rate is less than 0.5.The combined deletion mechanism incorporates all the aforementioned hyperparameters.</p>
<p>CIC-IoT Following the setup in the CIC-IoT benchmark (Neto et al., 2023), we implement a network traffic detection agent designed to predict traffic types from IoT packet features.The original CIC-IoT dataset contains 34 attack classes.Since our implementation relies solely on singleflow features, we exclude categories that cannot be distinguished based on such features.For example, we retain DDoS-HTTP_Flood while removing DoS-HTTP_Flood due to their high similarity under single-flow representations.After filtering, we retain 8 representative classes that remain distinguishable using single-flow features, and construct each test query by formatting these features into a predefined prompt template.We evaluate CIC-IoT Agent on the CIC-IoT dataset, randomly sampling 1,000 test cases from its training split.During each execution, the agent retrieves three past experiences using a feature-based similarity approach.The initial memory bank is initialized with 100 synthetic records generated by GPT-4o-mini from a disjoint training subset.We report task performance in terms of accuracy (ACC).</p>
<p>For the text encoder, we use OpenAI textembedding-3-large.For the coarse evaluator, similar to AgentDriver, we employ an LLM to assess the quality of the experiences.The prompt for the CIC-IoT Agent's LLM evaluator is provided in Appendix A.4.For the strict evaluator, we use string matching to check if the ground-truth is contained in the generated answer.For input similarity, we use a feature-based approach.Specifically, we compute the relative change across all features and take the average.For continuous features, the relative change between two inputs, input 1 and input 2 , for feature f i is computed as:
S cont (f i ) = |input 1 (f i ) − input 2 (f i )| max(|input 1 (f i )|, |input 2 (f i )|)
For discrete features, we define the relative change as follows:
S disc (f i ) = 0, if input 1 (f i ) = input 2 (f i ) 1, otherwise
For output similarity, we calculate the embedding similarity.For periodic deletion, we set a cycle of 500 steps, and α = 1.For history-based deletion, we set the experience quality threshold β = 0.7.</p>
<p>If an experience is retrieved 3 or more times and the average score of all test samples when using this experience falls below 0.7, the experience is removed.The combined deletion strategy reuses all the hyperparameters mentioned above.</p>
<p>A.2 RegAgent design</p>
<p>Below is the user prompt used for the RegAgent task.</p>
<p>User Prompt</p>
<p>You are given a 6-dimensional input vector x.Predict y = w T x with an unknown w.You will see K demonstrations of (input, guess) pairs that use the same w but may contain noise in all demonstrations.You need to strictly follow the output content and format of the demonstrations, which is Guess: boxed{{<number>}} without any other text.Demonstrations (K=k): {demonstrations} Now solve for the new input.Input: {x} Guess:
A.3 CIC-IoT Agent design
Below is the user prompt used in CIC-IoT Agent to incorporate IoT packet data and all possible traffic types.</p>
<p>User Prompt</p>
<p>Based on the following features, determine the most likely traffic type from the list below: * Pay special attention to cross-field consistency checks.<em> Do not be misled by a single feature if it conflicts with others.</em> Note: Your reasoning should be based on all features, not on any single field.You are allowed to select only one traffic type as your answer.If you choose more than one, your answer will be marked as incorrect.</p>
<p>Required output format: ANALYSIS: {your reasoning here, including key features and justification} ANSWER: {traffic_type} A.4 Coarse evaluator prompts Below is the system prompt used for our coarse evaluator (LLM judge) in EHRAgent:
(</p>
<p>System Prompt</p>
<p>You are an expert judge for Electronic Health Records (EHR) database queries and analysis.Your task is to evaluate whether the provided code solution and execution result are reasonable and correct for the given medical database question.You should assess the solution based on: 1) <strong>Code Quality</strong>: Does the code use appropriate database functions and follow logical steps?Be lenient about minor inefficiencies or alternative valid approaches.2) <strong>Result Reasonableness</strong>: Does the execution result appear plausible and well-formatted for the medical context? 3) <strong>Completeness</strong>: Does the solution address the core question asked?Accept solutions that may miss minor constraints but solve the main problem.<strong>Evaluation Guidelines:</strong> -Focus on whether the solution would produce a reasonable answer in practice -Accept alternative valid approaches even if not optimal -Be lenient about missing minor date filters if the core logic is sound -Consider the medical context -some variation in results is normal -Only reject solutions that are fundamentally flawed or completely irrelevant <strong>Available Tool Functions:</strong> (1) Calculate(FORMULA), which calculates the FORMULA and returns the result.</p>
<p>(2) LoadDB(DBNAME) which loads the database DBNAME and returns the database.The DBNAME can be one of the following: admissions, chartevents, cost, d_icd_diagnoses, d_icd_procedures, d_items, d_labitems, diagnoses_icd, icustays, inputevents_cv, labevents, microbiologyevents, outputevents,patients, prescriptions, procedures_icd, transfers.(Continued on next page...)</p>
<p>System Prompt</p>
<p>(Continuation from previous page...) (3) FilterDB(DATABASE, CONDITIONS), which filters the DATABASE according to the CONDITIONS.The CONDITIONS is a string composed of multiple conditions, each of which consists of the column_name, the relation and the value (e.g., COST&lt;10).The CONDITIONS is one single string (e.g., "admissions, SUBJECT_ID=24971").( 4) GetValue(DATABASE, ARGUMENT), which returns the values of the column in the DATABASE.When there is no additional operations on the values, the ARGU-MENT is the column_name in demand.If the values need to be returned with certain operations, the ARGUMENT is composed of the column_name and the operation (like COST, sum).Please do not contain " or ' in the argument.( 5) SQLInterpreter(SQL), which interprets the query SQL and returns the result.( 6</p>
<p>System Prompt</p>
<p>You are a highly knowledgeable and rigorous judge for autonomous driving.You are judging a <em>short-horizon</em> trajectory (e.g., 6 steps).We only require the following: 1) The predicted trajectory should <em>generally</em> move towards or align with the goal.</p>
<p>2) It should stay within a drivable area (i.e., allowed region).</p>
<p>3) It should avoid collisions with other objects.</p>
<p>Your output format: -First line: strictly output 'yes' or 'no'.</p>
<p>-Following lines: provide your reasoning (Chain-of-Thought is allowed).Be mindful that small lateral or partial forward movements can be acceptable as long as the overall direction is consistent with the planning target and safety requirements.Our coordinate system is such that the xaxis is lateral, and the y-axis is forward.Therefore, moving forward means an increase in y values.Be mindful that minor lateral adjustments or minimal forward movements are acceptable.If the y coordinate is increasing from step to step (and there's no collision or out-of-lane), that may be considered a success.Below are the relevant information for this autonomous driving task: For task distribution shift, we obtain the embedding vectors for all test queries using OpenAI text-embedding-3-large encoder for EHRAgent, and the original input vector computation method from AgentDriver.We then apply a Gaussian Mixture Model (GMM) to cluster the embeddings into three groups -each group will likely have a distinguished task distribution.Finally, we reorder the test queries based on their assigned cluster labels to process the three task groups sequentially.This procedure forces a pronounced shift in the task distribution.</p>
<p>B Additional results</p>
<p>B.1 Additional results on addition experiments</p>
<p>We present additional results for the memory addition experiments in this section.</p>
<p>B.2 Experiments on different LLM backbones</p>
<p>In this experiment, we conduct experiments with different LLM Agent backbones.Specifically, we conduct evaluation on GPT-4o and Deepseek-V3 with fixed-memory baseline, strict addition, strict addition with history-based deletion, and strict addition with combined deletion in Figure 11 and Figure 10, respectively.Across these two models, we observed consistent trend within the main experiments.In addition, their input similarity versus output similarity using add strict strategies and show in Figure 12.In Figure 13, we follow the procedure in Section 3.4 to plot the history-based deletion and its error-free variant.Surprisingly, for AgentDriver, we observe that around task index 1000, the combined deletion with strict addition surpasses the performance of its error-free variant.This suggests that history-based deletion can retain memory entries with outputs suitable for later reuse, corroborating the necessity of adopting this type of deletion.</p>
<p>B.4 History-based deletion on different evaluators</p>
<p>In Figure 14, we present performance changes after history-based deletion on RegAgent and CIC-IoT Agent with different evaluators.</p>
<p>B.5 Comparison between deleted records and retained records</p>
<p>In this section, we analyze the intrinsic quality differences between retained and deleted memory records.Figure 15 presents results for the Coarse 1 and Coarse 2 evaluators under history-based deletion, while Figure 16 shows results across all evaluators for AgentDriver.Interestingly, when using</p>
<p>Figure 1 :
1
Figure 1: Illustration of the memory management workflow after each agent execution.</p>
<p>Figure 2 :
2
Figure 2: Performance trend for EHRAgent and Agent-Driver.4o-mini, 4.1-mini,and 4.1-mini FT denote different coarse evaluators from the GPT series.Both the strict evaluator and some coarse evaluators exhibit consistent self-improvement over time.</p>
<p>Figure 3 :
3
Figure 3: Left: Output similarity versus input similarity for RegAgent over different evaluators.Right: Output similarity versus input similarity for AgentDriver over different evaluators.</p>
<p>Figure 4 :
4
Figure 4: Comparison of running performance between using the agent output as demonstrations and the errorfree (EF) variant using ground-truth.Coarse here uses C1 evaluator.</p>
<p>Figure 5 :
5
Figure 5: Performance comparison after applying history-based deletion with different evaluators.</p>
<p>Figure 6 :
6
Figure 6: Left: KDE curve over the absolute error of deleted and retained memory (with retrieved more than 5 times) with C1 evaluator using history-based deletion.Right: KDE curve over the absolute error with strict evaluator.</p>
<p>Figure 7 :
7
Figure 7: Performance comparison under task distribution shift for EHRAgent and AgentDriver.The vertical line indicates the point at which the task distribution shifts.The horizontal dashed line shows the performance of the combined deletion variant without distribution shift.</p>
<p>Continued on next page...) User Prompt Flow duration [description: Duration of the packet's flow]: {flow_duration} Header Length [description: Header Length]: {Header_Length} bytes Protocol Type [description: IP, UDP, TCP, IGMP, ICMP, Unknown (Integers)]: {Protocol_Type} Duration [description: Time-to-Live (ttl)]: {Duration} Rate [description: Rate of packet transmission in a flow]: {Rate} Srate [description: Rate of outbound packets transmission in a flow]: {Srate} Drate [description: Rate of inbound packets transmission in a flow]: {Drate} Number of FIN flags [description: FIN flag value]: {fin_flag_number} Number of SYN flags [description: SYN flag value]: {syn_flag_number} Number of RST flags [description: RST flag value]: {rst_flag_number} Number of PSH flags [description: PSH flag value]: {psh_flag_number} Number of ACK flags [description: ACK flag value]: {ack_flag_number} Number of ECE flags [description: ECE flag value]: {ece_flag_number} Number of CWR flags [description: CWR flag value]: {cwr_flag_number} Number of ACK packets [description: Number of packets with ACK flag set in the same flow]: {ack_count} Number of SYN packets [description: Number of packets with SYN flag set in the same flow]: {syn_count} Number of FIN packets [description: Number of packets with FIN flag set in the same flow]: {fin_count} Number of URG packets [description: Number of packets with URG flag set in the same flow]: {urg_count} Number of RST packets [description: Number of packets with RST flag set in the same flow]: {rst_count} HTTP traffic flag [description: Indicates if the application layer protocol is HTTP]: {HTTP} (Continued on next page...) User Prompt (Continuation from previous page...) HTTPS traffic flag [description: Indicates if the application layer protocol is HTTPS]: {HTTPS} DNS traffic flag [description: Indicates if the application layer protocol is DNS]: {DNS} Telnet traffic flag [description: Indicates if the application layer protocol is Telnet]: {Telnet} SMTP traffic flag [description: Indicates if the application layer protocol is SMTP]: {SMTP} SSH traffic flag [description: Indicates if the application layer protocol is SSH]: {SSH} IRC traffic flag [description: Indicates if the application layer protocol is IRC]: {IRC} TCP traffic flag [description: Indicates if the transport layer protocol is TCP]: {TCP} UDP traffic flag [description: Indicates if the transport layer protocol is UDP]: {UDP} DHCP traffic flag [description: Indicates if the application layer protocol is DHCP]: {DHCP} ARP traffic flag [description: Indicates if the link layer protocol is ARP]: {ARP} ICMP traffic flag [description: Indicates if the network layer protocol is ICMP]: {ICMP} IPv4 traffic flag [description: Indicates if the network layer protocol is IP]: {IPv} LLC traffic flag [description: Indicates if the link layer protocol is LLC]: {LLC} Total sum of feature values [description: Summation of packets' lengths in the flow]: {Tot_sum} Minimum value [description: Minimum packet length in the flow]: {Min} (Continued on next page...) User Prompt (Continuation from previous page...) Maximum value [description: Maximum packet length in the flow]: {Max} Average value [description: Average packet length in the flow]: {AVG} Standard deviation [description: Standard deviation of packet length in the flow]: {Std} Total size of the flow [description: Packet's length]: {Tot_size} bytes Inter-arrival time (milliseconds) [description: The time difference with the previous packet]: {IAT} Number of packets or flows [description: The number of packets in the flow]: {Number} Magnitude of the flow [description: Average of the lengths of incoming packets in the flow + average of the lengths of outgoing packets in the flow]: {Magnitude} Radius of the flow [description: Variance of the lengths of incoming packets in the flow + variance of the lengths of outgoing packets in the flow]: {Radius} Covariance of the flow [description: Covariance of the lengths of incoming and outgoing packets]: {Covariance} Variance of the flow [description: Variance of the lengths of incoming packets in the flow / variance of the lengths of outgoing packets in the flow]: {Variance} Weight of the flow [description: Number of incoming packets / number of outgoing packets]</p>
<p>the predicted trajectory is successful under the above criteria, then provide your reasoning (you may use chain-of-thought).Remember: -First line of your answer: 'yes' or 'no' ONLY.-Following lines: your reasons or chain-ofthought.Below is the user prompt used for our coarse evaluator (LLM judge) in CIC-IoT Agent: User Prompt You are a strict evaluator for IoT traffic classification answers.You should infer the most likely label from the provided flow-level features, then judge whether the model's answer is CORRECT or INCOR-RECT.### Judging Criteria The Model Answer is CORRECT if its predicted label exactly matches your inferred gold label, else IN-CORRECT.### Judging Steps 1) Carefully analyze key fields and reason your inferred gold label.2) Compare the Model Answer's label to your gold label.### Problem: Based on the following features, determine the most likely traffic type from the list below: ### Features: problem ### Model Answer: {generated_answer} -Respond with your judgement and explaination as following format.-First line: Respond with 'CORRECT' or 'INCORRECT' only.-Following lines: Provide your reasoning or chain-of-thought.Your judgement: A.5 Additional setup for task distribution shift</p>
<p>Figure 8 :
8
Figure 8: Accuracy trends of different addition strategies over long-term running on EhrAgent and CIC-IoT.</p>
<p>Figure 9 :
9
Figure 9: Cumulative average output similarity vs. input similarity of different addition strategies over the execution on AgentDriver and CIC-IoT.</p>
<p>Figure 10 :
10
Figure 10: Accuracy trends of GPT-4o over long-term running on AgentDriver</p>
<p>Figure 11 :
11
Figure 11: Accuracy trends of Deepseek-V3 over longterm running on AgentDriver</p>
<p>Figure 13 :
13
Figure 13: Comparison between history-based deletion and its error-free variants.Coarse here represents C1 evaluator.</p>
<p>Figure 14 :
14
Figure 14: Performance comparison after applying history-based deletion with different evaluators.</p>
<p>Figure 15 :
15
Figure 15: Comparison between deleted and retained records for RegAgent under history-based deletion.The top figure shows results with Coarse 1.2, and the bottom figure shows results with Coarse 1.4.Lower absolute error represents better execution quality.</p>
<p>Figure 16 :
16
Figure 16: Comparison between deleted records and retained records for AgentDriver.Lower error represents better execution quality.</p>
<p>Figure 17 :
17
Figure 17: Comparison of unlimited memory size versus a limited memory size for strict and coarse selective addition with combined deletion.Coarse here denotes the C1 evaluator.</p>
<p>Figure 18 :
18
Figure 18: Different limited sizes versus the performance of AgentDriver.The horizontal dashed lines are their corresponding unlimited variant performance.Coarse here denotes the C1 evaluator.</p>
<p>Table 1 :
1
Performance of the three memory addition strategies: add-all, coarse evaluation with three different automatic evaluators, and selective addition based on human evaluation (strict), compared with the fixedmemory baseline (shaded).The results highlight the necessity of selection for effective memory addition.Mem Size ↓ ACC ↑ Mem Size ↓ SR. ↑ Mem Size ↓ ACC.↑ Mem Size ↓
RegAgent SR. ↑ Fixed 67.53 Judge 100EHRAgents 16.75 100AgentDriver 40.11 180CIC-IoT Agent 71.50 50Add all 55.48410013.05241132.32212559.901050CoarseC1 63.18351126.19144736.92116174.001030C2 65.78334732.21146740.01111968.80936C3 67.35313934.66109447.37128579.50952StrictStrict 70.95293838.50101251.00117885.40904</p>
<p>Mem Size ↓ ACC.↑ Mem Size ↓ SR. ↑ Mem Size ↓ ACC.↑ Mem Size ↓
StrategyRegAgents SR. ↑ Coarse evaluator (C1) EHRAgentsAgentDriverCIC-IoT AgentNo del63.18351125.91144736.92116174.001030Period60.88101226.6533836.3842678.10355History62.10320533.55100434.00101973.70952Combined 59.3295131.4727935.6237268.80352Strict evaluatorNo del70.95293838.67101251.00117885.40904Period67.6594938.5930250.9446780.80310History69.80228642.0678451.8184689.60788Combined 66.5889042.3424849.9732385.50188</p>
<p>Table 3 :
3
Details of Agents and Their Functionality
Agent NameTaskInputOutputRetrieve Feature#ExperiencesRegAgentRegression prediction Input vectorOutput numberInput vector6EHRAgentsEHR tasksTask text queryCodeText embedding4AgentDriverAutonomous DrivingVehicle state dataPredicted trajectoryEgo state, goal, history trajectory1CIC-IoT AgentIoT Traffic Detection (8 categories)IoT packet dataReasoning and prediction IoT features attack type3
For AgentDriver, while these trajectories can still be suboptimal, the ground-truth trajectory resembles relatively "correct" task executions.
https://github.com/fyrestone/pycode_similar. git
AgentCategory 4o-mini 4.1-mini 4. GPT-4o-mini as the evaluator, the retained memory in AgentDriver exhibits lower average quality than the deleted records.We further report the correctness rates with respect to ground truth for EhrAgent and CIC-IoT Agent, as shown in Table4.B.6 Results under memory resource constraints.We provide the results of the agent running under memory resource constraints in Figure17.We also provide additional results on memory resource constraints with different memory limitation sizes in Figure18.
Arigraph: Learning knowledge graph world models with episodic memory for llm agents. Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, Mikhail Burtsev, Evgeny Burnaev, arXiv:2407.043632024arXiv preprint</p>
<p>Which processes are involved in cognitive procedural learning? Memory. Beaunieux, Hubert, A L Witkowski, Pitel, Rossi, Jm Danion, Desgranges, Eustache, 10.1080/09658210500477766167542392006. 2006 Jul14</p>
<p>nuscenes: A multimodal dataset for autonomous driving. Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, Oscar Beijbom, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>my agent understands me better": Integrating dynamic human-like memory recall and consolidation in llm-based agents. Yuki Hou, Haruki Tamoto, Homei Miyashita, Extended Abstracts of the CHI Conference on Human Factors in Computing Systems. 2024</p>
<p>Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model. Mengkang Hu, Tianxing Chen, Qiguang Chen, Yao Mu, Wenqi Shao, Ping Luo, arXiv:2408.095592024arXiv preprint</p>
<p>MIMIC-III, a freely accessible critical care database. E W Alistair, Tom J Johnson, Lu Pollard, Shen, H Li-Wei, Mengling Lehman, Mohammad Feng, Benjamin Ghassemi, Peter Moody, Leo Szolovits, Roger G Anthony Celi, Mark, 10.1038/sdata.2016.35Scientific Data. 31600352016</p>
<p>Semantic memory: A review of methods, models, and current challenges. Ashok Abhilasha, Kumar, Psychonomic Bulletin &amp; Review. 282020</p>
<p>Sham Kakade, Prateek Jain, and 1 others. 2022. Matryoshka representation learning. Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Advances in Neural Information Processing Systems. 35</p>
<p>Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences. Andrew Kyle Lampinen, Martin Engelcke, Yuxuan Li, Arslan Chaudhry, James L Mcclelland, 2025</p>
<p>Tradinggpt: Multiagent system with layered memory and distinct characters for enhanced financial trading performance. Yang Li, Yangyang Yu, Haohang Li, Zhi Chen, Khaldoun Khashanah, arXiv:2309.037362023arXiv preprint</p>
<p>Keqin Li, and 1 others. Xuechen Liang, Yangfan He, Yinghui Xia, Xinyuan Song, Jianhui Wang, Meiling Tao, Li Sun, Xinhang Yuan, Jiayi Su, arXiv:2409.00872Self-evolving agents with reflective and memory-augmented abilities. 2024arXiv preprint</p>
<p>Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, Guannan Zhang, arXiv:2311.08719Thinkin-memory: Recalling and post-thinking enable llms with long-term memory. 2023arXiv preprint</p>
<p>In-context learning with retrieved demonstrations for language models: A survey. Man Luo, Xin Xu, Yue Liu, Panupong Pasupat, Mehran Kazemi, arXiv:2401.116242024arXiv preprint</p>
<p>A language agent for autonomous driving. Jiageng Mao, Junjie Ye, Yuxi Qian, Marco Pavone, Yue Wang, First Conference on Language Modeling. 2024</p>
<p>Ciciot2023: A real-time dataset and benchmark for large-scale attacks in iot environment. Euclides Carlos, Pinto Neto, Sajjad Dadkhah, Raphael Ferreira, Alireza Zohourian, Rongxing Lu, Ali A Ghorbani, 10.3390/s23135941Sensors. 13232023</p>
<p>Extending cognitive architecture with episodic memory. Andrew Nuxoll, John E Laird, AAAI Conference on Artificial Intelligence. 2007</p>
<p>Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, Alane Suhr, arXiv:2404.06474Autonomous evaluation and refinement of digital agents. 2024arXiv preprint</p>
<p>Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Xufang Luo, Hao Cheng, Dongsheng Li, Yuqing Yang, Chin-Yew Lin, Vicky Zhao, arXiv:2502.05589Lili Qiu, and 1 others. 2025. On memory construction and retrieval for personalized conversational agents. arXiv preprint</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th annual acm symposium on user interface software and technology. the 36th annual acm symposium on user interface software and technology2023</p>
<p>Ehragent: Code empowers large language models for complex tabular reasoning on electronic health records. Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu, Yuanda Zhu, Joyce Ho, Carl Yang, May D Wang, arXiv:2401.071282024arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202436</p>
<p>Shunyu Theodore R Sumers, Karthik Yao, Thomas L Narasimhan, Griffiths, arXiv:2309.02427Cognitive architectures for language agents. 2023arXiv preprint</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023arXiv preprint</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, and 1 others. 2024a. A survey on large language model based autonomous agents. 18186345</p>
<p>Trove: Inducing verifiable and efficient toolboxes for solving programmatic tasks. Zhiruo Wang, Daniel Fried, Graham Neubig, arXiv:2401.128692024barXiv preprint</p>
<p>Agent workflow memory. Zora Zhiruo, Wang , Jiayuan Mao, Daniel Fried, Graham Neubig, Forty-second International Conference on Machine Learning. 2025</p>
<p>Zhen Xiang, Linzhi Zheng, Yanjie Li, Junyuan Hong, Qinbin Li, Han Xie, Jiawei Zhang, Zidi Xiong, Chulin Xie, arXiv:2406.09187Carl Yang, and 1 others. 2024. Guardagent: Safeguard llm agents by a guard agent via knowledge-enabled reasoning. arXiv preprint</p>
<p>Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, Yongfeng Zhang, arXiv:2502.12110A-mem: Agentic memory for llm agents. 2025arXiv preprint</p>
<p>Explicit memory learning with expectation maximization. Zhangyue Yin, Qiushi Sun, Qipeng Guo, Zhiyuan Zeng, Qinyuan Cheng, Xipeng Qiu, Xuan-Jing Huang, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024</p>
<p>Ruihong Zeng, Jinyuan Fang, Siwei Liu, Zaiqiao Meng, arXiv:2412.15266On the structural memory of llm agents. 2024arXiv preprint</p>
<p>Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, Ji-Rong Wen, arXiv:2404.13501A survey on the memory mechanism of large language model based agents. 2024arXiv preprint</p>
<p>Expel: Llm agents are experiential learners. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, Gao Huang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Synapse: Trajectory-as-exemplar prompting with memory for computer control. Longtao Zheng, Rundong Wang, Xinrun Wang, Bo An, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Memorybank: Enhancing large language models with long-term memory. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Huichi Zhou, Yihang Chen, Siyuan Guo, Xue Yan, Kin Hei Lee, Zihan Wang, Ka Yiu Lee, Guchun Zhang, Kun Shao, arXiv:2508.16153Linyi Yang, and 1 others. 2025. Agentfly: Fine-tuning llm agents without fine-tuning llms. arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
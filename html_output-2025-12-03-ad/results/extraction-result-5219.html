<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5219 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5219</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5219</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-267751249</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.11436v2.pdf" target="_blank">Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement</a></p>
                <p><strong>Paper Abstract:</strong> Recent studies show that large language models (LLMs) improve their performance through self-feedback on certain tasks while degrade on others. We discovered that such a contrary is due to LLM's bias in evaluating their own output. In this paper, we formally define LLM's self-bias - the tendency to favor its own generation - using two statistics. We analyze six LLMs (GPT-4, GPT-3.5, Gemini, LLaMA2, Mixtral and DeepSeek) on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks. The code and data are released at https://github.com/xu1998hz/llm_self_bias.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5219.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5219.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source, high-capability instruction-following large language model by OpenAI; evaluated here as both generator and self-evaluator in iterative self-refine and self-rewarding pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source, high-capability instruction-tuned LLM (exact parameter count not stated in paper); used via OpenAI API as generator and as an internal evaluator/critic.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>iterative self-refine (self-reflection / self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate an initial response; prompt the same LLM to produce feedback (error locations, types, severity) on that response given the input; use that feedback plus the prior generation to produce a refined output; accept refinements only when the LLM's own feedback indicates improved quality. Iterations repeated (typically up to 10).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>10</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine translation (Flores-200), Constrained text generation (CommonGen Hard), Mathematical reasoning (MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Low/medium-resource translation (Yoruba/Javanese/Armenian/Igbo → English) evaluated with MQM-like scoring (via BLEURT quantile-mapped to MQM), constrained generation requiring coverage of concept lists (binary full-coverage metric), and multi-step math problems (MATH dataset) with stepwise solution correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Qualitative: GPT-4's self-evaluated quality scores increase across iterative self-refine steps, but reference-based BLEURT (used as proxy for human quality) shows no meaningful improvement in translation across iterations; UniEval reports consistent improvements in fluency and understandability. Exact BLEURT numbers not reported in main text (paper reports discrepancy: self-assessed improvement vs. BLEURT stagnation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline (0th iteration) performance measured by BLEURT / task metrics; paper reports that BLEURT does not meaningfully increase from baseline through 10 self-refine iterations when using GPT-4's own feedback (i.e., no true quality gain despite perceived improvement by GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative/qualitative evidence: (1) UniEval scored fluency and understandability increase across iterations for GPT-4, indicating improvements in surface-level qualities; (2) GPT-4's internal scores (self-feedback) increase across iterations, indicating perceived improvements; (3) however, BLEURT-derived (reference-based) quality does not improve across iterations under self-feedback, demonstrating a mismatch between self-assessment and true quality. A case study shows external oracle feedback (InstructScore) elicited GPT-4 to actually improve BLEURT scores.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>GPT-4 exhibits self-bias: it favors its own generations and can amplify false-positive corrections across iterations (perceives improvements that do not exist). Self-refinement improves fluency/understandability but not intended task quality (translation accuracy or concept coverage). GPT-4 also tends to favor outputs in its own style (paraphrases by GPT-4 increased self-bias and sometimes reduced true quality). External (oracle) feedback or larger models needed to mitigate bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5219.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5219.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely-used instruction-following LLM (OpenAI) evaluated as generator and self-evaluator; shows stronger self-bias amplification than top-tier models in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source instruction-tuned LLM from OpenAI (smaller / less capable than GPT-4); used via API as generator and evaluator in the self-refine pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>iterative self-refine (self-feedback with ICL examples)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>LLM produces initial output; LLM generates feedback (ICL examples provided for the feedback format) identifying errors/missing concepts; LLM refines output using feedback; accept refinement only if model's self-feedback score improves. Experiments used ~10 iterations for many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>10</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Constrained text generation (CommonGen Hard), Machine translation (Flores-200), MATH reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>CommonGen Hard: generate sentence covering provided concepts (strict string-match coverage metric). Translation: low-resource Flores-200 language pairs. MATH: graded math problems requiring correct final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Constrained generation: true coverage improved (e.g., GPT-3.5 reached ~20% full-coverage around the 5th iteration) but then saturated; LLM self-estimated coverage improvements were much larger than measured true improvements. Translation & math: BLEURT / ground-truth-based metrics did not show consistent improvement; self-assessed scores rose.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline coverage / BLEURT: baseline coverage lower than peak of iterative process (e.g., GPT-3.5 coverage rose from baseline to ~20% by iteration ~5), but for translation and math true metrics did not reliably improve beyond baseline across 10 iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Measured improvement in constrained generation coverage up to roughly 20% for GPT-3.5 by the 5th iteration (then saturated). UniEval indicated improvements in fluency and understandability. However, the paper emphasizes that self-reported improvements are exaggerated relative to true metric gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-bias amplification: GPT-3.5 shows high self-bias and overestimates its improvements (rate of self-estimated improvement > true improvement). After several iterations, performance saturates and further iterations amplify bias rather than quality; self-feedback can produce false-positive optimization and reduced diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5219.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5219.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini (Google, gemini-pro)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source multimodal LLM (Google Gemini family) evaluated as generator and self-evaluator; shows some robustness to self-bias compared to smaller/open models but still amplifies bias across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini (gemini-pro)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source, highly capable multimodal instruction-tuned model (accessed via Google Gemini API). Exact parameterization not stated in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>iterative self-refine (self-feedback) and self-consistency in math experiments</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same iterative generate-feedback-refine loop used: generate initial output, generate feedback in MQM-like format, refine if feedback indicates improvement. For math, experiments also replaced self-evaluation with self-consistency verification (generate multiple reasoning paths + majority vote) as the evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>10</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine translation (Flores-200), Constrained text generation (CommonGen Hard), MATH (incl. self-consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See descriptions above for translation, constrained generation, and MATH. Self-consistency: generate many reasoning paths and majority-vote as verification instead of direct self-evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Qualitative: Gemini's self-assessed scores rise across iterations while true reference-based measures (BLEURT for translation) do not show corresponding true improvements; in some runs Gemini initially underestimates outputs (right-skew) then shifts distributions across iterations. UniEval reports fluency/understandability gains but BLEURT shows limited true gains under self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline BLEURT / task metrics at 0th iteration; paper reports that Gemini, like others, does not reliably improve BLEURT through pure self-refine.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Improvements reported in fluency and understandability (UniEval). External feedback experiments (InstructScore) reduced bias and led to BLEURT improvements for Gemini, demonstrating that with accurate external feedback Gemini can improve true quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Gemini can refuse to produce content for some sensitive inputs (lower format-accuracy results on some sets). Self-refine still amplifies self-bias across iterations; Gemini shows shifts in skewness (initial underestimation then increasing left-skew as bias accumulates). Self-consistency used as evaluator also led to bias amplification across reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5219.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5219.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA2-family</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA2 (7B / 13B / 70B chat models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source family of chat-tuned LLaMA-2 models with varying sizes; evaluated to test the effect of model scale on self-bias during iterative self-refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-7B / LLaMA2-13B / LLaMA2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source chat-tuned LLaMA2 models (metallama/Llama-2-(7,13,70)b-chat-hf). Sizes: 7B, 13B, 70B parameters respectively (as stated in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>iterative self-refine (self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same generate-feedback-refine iterative loop where the model evaluates its prior generation and decides whether to apply a refinement; experiments measured bias amplification across iterations for different model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>10</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine translation (Yoruba→English)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translation on Flores-200 Yor→En subset; used BLEURT (quantile-mapped to MQM scale) as proxy for human quality to measure true performance and measured self-bias metrics across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Qualitative: Larger LLaMA2 models (70B) exhibit less self-bias amplification and plateau bias after ~5 iterations; smaller models (7B, 13B) continue to amplify self-bias over iterations. The paper reports that larger model size reduces self-bias and better resists amplification.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline BLEURT / MQM-mapped scores at 0th iteration for each model; smaller models had lower baseline quality and amplified self-bias more during self-refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Evidence that larger models (70B) show less self-bias amplification and in some cases more accurate self-assessment, indicating a model-scale benefit for self-refinement reliability. This is shown by bias/dSkew curves across iterations (Figure 10) where 70B plateaus while smaller sizes continue rising.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Smaller LLaMA2 models amplify self-bias substantially across iterative steps, harming the reliability of self-refinement. Even for 70B there is initial self-bias; plateauing does not imply consistent true quality gain without external feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5219.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5219.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral-MOE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral-MOE 8x7B-Instruct-v0.1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source mixture-of-experts model (Mixtral) evaluated as generator and self-evaluator; used in both self-refine and self-rewarding experiments and shows pronounced self-bias amplification with larger candidate pools.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral-MOE (8x7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mixtral mixture-of-experts model (mistralai/Mixtral-8x7B-Instruct-v0.1); open-source, MoE architecture with multiple expert shards (~8x7B components as named).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-refine and self-rewarding (LLM-as-reward)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Self-refine: iterative generate-feedback-refine loop using the same LLM as evaluator. Self-rewarding: generate k candidate responses and use the same LLM as reward model to rank/select top candidate(s) for further training or selection; paper varied k (1,4,8,16,32) to analyze bias amplification.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>10</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine translation (Yoruba→English) and self-rewarding candidate ranking experiments</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translation on Flores-200 (Yor->En); self-rewarding experiments test how increasing the number of candidate samples (k) affects self-bias when the LLM ranks its own outputs as reward.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Qualitative: When used as reward model with increasing k, Mixtral's bias and dSkew increase (higher preference for its own outputs). Self-refine iterative runs show amplified bias over iterations. No reliable true-quality gains reported when relying solely on Mixtral's self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline metrics at 0th iteration; paper reports that relying on self-rewarding selection increases bias compared to baseline, especially with larger k.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>No strong evidence that pure self-refinement or self-rewarding with Mixtral yields true quality gains; experiments show bias amplification with increasing sample size k in self-rewarding, supporting the claim that self-evaluation can exaggerate preference for model's own outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-rewarding loop amplifies bias with larger candidate pools (k), leading to over-selection of false positives; self-refine without external/ground-truth feedback does not reliably improve true translation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5219.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5219.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-MOE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-MOE 16B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source mixture-of-experts model (DeepSeek-MOE) evaluated as generator and self-evaluator; included in self-refine and self-rewarding experiments and shows increased bias with iterative/self-rewarding use.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-MOE (16B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source DeepSeek MoE (deepseek-ai/deepseek-moe-16b-chat) with ~16B parameter budget in MoE configuration; used as both generator and self-evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>iterative self-refine and self-rewarding</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same iterative self-feedback loop (generate feedback then refine) and self-rewarding experiments where the model ranks k sampled candidates using its own feedback. Evaluated for bias amplification as k increases.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>10</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine translation (Yoruba→English) and self-rewarding ranking experiments</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translation on Flores-200 Yor→En; self-rewarding experiments analyze bias when model ranks its own candidates for selection/training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Qualitative: DeepSeek-MOE shows increased bias and distance skewness during iterative self-refine and increased bias as k increases in self-rewarding experiments. No consistent true quality improvement reported from self-only feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline translation quality at 0th iteration; self-rewarding selection amplifies bias relative to baseline and can select suboptimal candidates according to BLEURT/human metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>No robust evidence that self-refine or self-rewarding using DeepSeek-MOE improves true translation quality; results show bias amplification and potential selection of false-positive outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-bias amplified by iterative refinement and by larger candidate pools in self-rewarding, leading to over-optimistic model-internal rankings and potential degradation in downstream quality if used to train/select outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5219.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5219.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency verifier</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency verification (as evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generate-then-verify-style evaluator: generate multiple reasoning chains and use majority vote among sampled chains to verify or replace the initial chain; used as an alternative evaluator to direct self-assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Self-consistency verification (implemented with LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Evaluator approach that samples multiple reasoning paths (e.g., 10) and uses majority vote as a binary correctness signal; implemented with the same LLM family used for generation in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-consistency as evaluator (generate-many + majority vote)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Given an initial solution, generate multiple additional reasoning paths; majority-vote an answer from sampled paths; if majority answer disagrees with initial, mark initial as wrong and replace it; treat this binary outcome as feedback for refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mathematical reasoning (MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school/competition-style math problems requiring multi-step reasoning and a final numeric/structured answer; correctness determined by exact match to ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported: Using self-consistency as the evaluator still led to increases in bias and distance skewness across iterative refinement, suggesting that ensemble/consistency-based self-evaluation can also introduce self-bias and biased ensembles.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline (single-path generation) correctness at 0th iteration; replacing self-evaluation with self-consistency did not prevent bias amplification in iterative refinement experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Although self-consistency is known in prior work to sometimes improve chain-of-thought correctness, in this paper replacing LLM-as-evaluator with self-consistency verification still resulted in bias amplification (Figure 11, Appendix D), indicating limited benefit in preventing self-bias in iterative refine.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-consistency verifier can still produce biased ensembles: majority votes and sampled paths can favor solutions that align with the model's prior tendencies, leading to amplified self-bias over iterations and not guaranteeing true-quality gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5219.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5219.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>External feedback (InstructScore)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructScore (reference-based external feedback model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An external reference-based feedback model that uses the reference to generate MQM-like error annotations (error location, type, severity); used as an oracle-like external critic to reduce self-bias and elicit true improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructScore (xu1998hz/InstructScore)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reference-based learned feedback model that takes reference and candidate translation to output fine-grained feedback (error location, error type, severity); treated as oracle external feedback in experiments because it can consult references.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>external-feedback-guided self-refine</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same iterative refinement loop, but feedback is supplied by InstructScore (external evaluator with access to references) rather than the generation model itself; feedback includes MQM-like annotations that the generator uses to correct outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine translation (Yoruba→English)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translation on Flores-200 Yor→En; InstructScore provides external annotations to guide iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>With external feedback, models (including GPT-4, GPT-3.5, Gemini) show consistent BLEURT improvements across self-refine iterations and reduced self-bias (bias/dSkew curves lower than self-feedback counterparts). Case study shows BLEURT moving closer to human ratings after iterations using InstructScore.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Compared to pure self-feedback iterations, external feedback yields higher true-metric improvements (BLEURT) whereas pure self-feedback did not increase BLEURT. Exact BLEURT numbers are not provided in main text, but plots and case studies show measurable BLEURT improvements with InstructScore across up to 5 iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: paper reports that all LLMs with external InstructScore feedback achieved consistent BLEURT improvements through self-refine iterations (dotted curves below solid curves in Figure 9), and bias estimation reduced throughout refinement. Case study (Table 4) shows a concrete example where GPT-4's human score improved from -11 to -1 across refinements when using InstructScore.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>InstructScore is oracle-like because it uses the reference; while it reduces self-bias and elicits true improvements, it relies on reference availability and does not provide explicit corrections—generator still must perform corrections using its internal knowledge. Not a fully general solution when references are unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models <em>(Rating: 2)</em></li>
                <li>Large language models cannot self-correct reasoning yet <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Can large language models really improve by self-critiquing their own plans? <em>(Rating: 2)</em></li>
                <li>LLMs as narcissistic evaluators: When ego inflates evaluation scores <em>(Rating: 2)</em></li>
                <li>Teaching large language models to self-debug <em>(Rating: 2)</em></li>
                <li>Self-rewarding language models <em>(Rating: 2)</em></li>
                <li>Gptscore: Evaluate as you desire <em>(Rating: 1)</em></li>
                <li>Llmrefine: Pinpointing and refining large language models via fine-grained actionable feedback <em>(Rating: 2)</em></li>
                <li>InstructScore <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5219",
    "paper_id": "paper-267751249",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A closed-source, high-capability instruction-following large language model by OpenAI; evaluated here as both generator and self-evaluator in iterative self-refine and self-rewarding pipelines.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Closed-source, high-capability instruction-tuned LLM (exact parameter count not stated in paper); used via OpenAI API as generator and as an internal evaluator/critic.",
            "reflection_method_name": "iterative self-refine (self-reflection / self-feedback)",
            "reflection_method_description": "Generate an initial response; prompt the same LLM to produce feedback (error locations, types, severity) on that response given the input; use that feedback plus the prior generation to produce a refined output; accept refinements only when the LLM's own feedback indicates improved quality. Iterations repeated (typically up to 10).",
            "num_iterations": 10,
            "task_name": "Machine translation (Flores-200), Constrained text generation (CommonGen Hard), Mathematical reasoning (MATH)",
            "task_description": "Low/medium-resource translation (Yoruba/Javanese/Armenian/Igbo → English) evaluated with MQM-like scoring (via BLEURT quantile-mapped to MQM), constrained generation requiring coverage of concept lists (binary full-coverage metric), and multi-step math problems (MATH dataset) with stepwise solution correctness.",
            "performance_with_reflection": "Qualitative: GPT-4's self-evaluated quality scores increase across iterative self-refine steps, but reference-based BLEURT (used as proxy for human quality) shows no meaningful improvement in translation across iterations; UniEval reports consistent improvements in fluency and understandability. Exact BLEURT numbers not reported in main text (paper reports discrepancy: self-assessed improvement vs. BLEURT stagnation).",
            "performance_without_reflection": "Baseline (0th iteration) performance measured by BLEURT / task metrics; paper reports that BLEURT does not meaningfully increase from baseline through 10 self-refine iterations when using GPT-4's own feedback (i.e., no true quality gain despite perceived improvement by GPT-4).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative/qualitative evidence: (1) UniEval scored fluency and understandability increase across iterations for GPT-4, indicating improvements in surface-level qualities; (2) GPT-4's internal scores (self-feedback) increase across iterations, indicating perceived improvements; (3) however, BLEURT-derived (reference-based) quality does not improve across iterations under self-feedback, demonstrating a mismatch between self-assessment and true quality. A case study shows external oracle feedback (InstructScore) elicited GPT-4 to actually improve BLEURT scores.",
            "limitations_or_failure_cases": "GPT-4 exhibits self-bias: it favors its own generations and can amplify false-positive corrections across iterations (perceives improvements that do not exist). Self-refinement improves fluency/understandability but not intended task quality (translation accuracy or concept coverage). GPT-4 also tends to favor outputs in its own style (paraphrases by GPT-4 increased self-bias and sometimes reduced true quality). External (oracle) feedback or larger models needed to mitigate bias.",
            "uuid": "e5219.0",
            "source_info": {
                "paper_title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-3.5-Turbo",
            "name_full": "GPT-3.5-Turbo (OpenAI)",
            "brief_description": "A widely-used instruction-following LLM (OpenAI) evaluated as generator and self-evaluator; shows stronger self-bias amplification than top-tier models in experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_description": "Closed-source instruction-tuned LLM from OpenAI (smaller / less capable than GPT-4); used via API as generator and evaluator in the self-refine pipeline.",
            "reflection_method_name": "iterative self-refine (self-feedback with ICL examples)",
            "reflection_method_description": "LLM produces initial output; LLM generates feedback (ICL examples provided for the feedback format) identifying errors/missing concepts; LLM refines output using feedback; accept refinement only if model's self-feedback score improves. Experiments used ~10 iterations for many tasks.",
            "num_iterations": 10,
            "task_name": "Constrained text generation (CommonGen Hard), Machine translation (Flores-200), MATH reasoning",
            "task_description": "CommonGen Hard: generate sentence covering provided concepts (strict string-match coverage metric). Translation: low-resource Flores-200 language pairs. MATH: graded math problems requiring correct final answer.",
            "performance_with_reflection": "Constrained generation: true coverage improved (e.g., GPT-3.5 reached ~20% full-coverage around the 5th iteration) but then saturated; LLM self-estimated coverage improvements were much larger than measured true improvements. Translation & math: BLEURT / ground-truth-based metrics did not show consistent improvement; self-assessed scores rose.",
            "performance_without_reflection": "Baseline coverage / BLEURT: baseline coverage lower than peak of iterative process (e.g., GPT-3.5 coverage rose from baseline to ~20% by iteration ~5), but for translation and math true metrics did not reliably improve beyond baseline across 10 iterations.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Measured improvement in constrained generation coverage up to roughly 20% for GPT-3.5 by the 5th iteration (then saturated). UniEval indicated improvements in fluency and understandability. However, the paper emphasizes that self-reported improvements are exaggerated relative to true metric gains.",
            "limitations_or_failure_cases": "Self-bias amplification: GPT-3.5 shows high self-bias and overestimates its improvements (rate of self-estimated improvement &gt; true improvement). After several iterations, performance saturates and further iterations amplify bias rather than quality; self-feedback can produce false-positive optimization and reduced diversity.",
            "uuid": "e5219.1",
            "source_info": {
                "paper_title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Gemini",
            "name_full": "Gemini (Google, gemini-pro)",
            "brief_description": "A closed-source multimodal LLM (Google Gemini family) evaluated as generator and self-evaluator; shows some robustness to self-bias compared to smaller/open models but still amplifies bias across iterations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini (gemini-pro)",
            "model_description": "Closed-source, highly capable multimodal instruction-tuned model (accessed via Google Gemini API). Exact parameterization not stated in paper.",
            "reflection_method_name": "iterative self-refine (self-feedback) and self-consistency in math experiments",
            "reflection_method_description": "Same iterative generate-feedback-refine loop used: generate initial output, generate feedback in MQM-like format, refine if feedback indicates improvement. For math, experiments also replaced self-evaluation with self-consistency verification (generate multiple reasoning paths + majority vote) as the evaluator.",
            "num_iterations": 10,
            "task_name": "Machine translation (Flores-200), Constrained text generation (CommonGen Hard), MATH (incl. self-consistency)",
            "task_description": "See descriptions above for translation, constrained generation, and MATH. Self-consistency: generate many reasoning paths and majority-vote as verification instead of direct self-evaluation.",
            "performance_with_reflection": "Qualitative: Gemini's self-assessed scores rise across iterations while true reference-based measures (BLEURT for translation) do not show corresponding true improvements; in some runs Gemini initially underestimates outputs (right-skew) then shifts distributions across iterations. UniEval reports fluency/understandability gains but BLEURT shows limited true gains under self-feedback.",
            "performance_without_reflection": "Baseline BLEURT / task metrics at 0th iteration; paper reports that Gemini, like others, does not reliably improve BLEURT through pure self-refine.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Improvements reported in fluency and understandability (UniEval). External feedback experiments (InstructScore) reduced bias and led to BLEURT improvements for Gemini, demonstrating that with accurate external feedback Gemini can improve true quality.",
            "limitations_or_failure_cases": "Gemini can refuse to produce content for some sensitive inputs (lower format-accuracy results on some sets). Self-refine still amplifies self-bias across iterations; Gemini shows shifts in skewness (initial underestimation then increasing left-skew as bias accumulates). Self-consistency used as evaluator also led to bias amplification across reasoning paths.",
            "uuid": "e5219.2",
            "source_info": {
                "paper_title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaMA2-family",
            "name_full": "LLaMA2 (7B / 13B / 70B chat models)",
            "brief_description": "Open-source family of chat-tuned LLaMA-2 models with varying sizes; evaluated to test the effect of model scale on self-bias during iterative self-refinement.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA2-7B / LLaMA2-13B / LLaMA2-70B",
            "model_description": "Open-source chat-tuned LLaMA2 models (metallama/Llama-2-(7,13,70)b-chat-hf). Sizes: 7B, 13B, 70B parameters respectively (as stated in paper).",
            "reflection_method_name": "iterative self-refine (self-feedback)",
            "reflection_method_description": "Same generate-feedback-refine iterative loop where the model evaluates its prior generation and decides whether to apply a refinement; experiments measured bias amplification across iterations for different model sizes.",
            "num_iterations": 10,
            "task_name": "Machine translation (Yoruba→English)",
            "task_description": "Translation on Flores-200 Yor→En subset; used BLEURT (quantile-mapped to MQM scale) as proxy for human quality to measure true performance and measured self-bias metrics across iterations.",
            "performance_with_reflection": "Qualitative: Larger LLaMA2 models (70B) exhibit less self-bias amplification and plateau bias after ~5 iterations; smaller models (7B, 13B) continue to amplify self-bias over iterations. The paper reports that larger model size reduces self-bias and better resists amplification.",
            "performance_without_reflection": "Baseline BLEURT / MQM-mapped scores at 0th iteration for each model; smaller models had lower baseline quality and amplified self-bias more during self-refinement.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Evidence that larger models (70B) show less self-bias amplification and in some cases more accurate self-assessment, indicating a model-scale benefit for self-refinement reliability. This is shown by bias/dSkew curves across iterations (Figure 10) where 70B plateaus while smaller sizes continue rising.",
            "limitations_or_failure_cases": "Smaller LLaMA2 models amplify self-bias substantially across iterative steps, harming the reliability of self-refinement. Even for 70B there is initial self-bias; plateauing does not imply consistent true quality gain without external feedback.",
            "uuid": "e5219.3",
            "source_info": {
                "paper_title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Mixtral-MOE",
            "name_full": "Mixtral-MOE 8x7B-Instruct-v0.1",
            "brief_description": "An open-source mixture-of-experts model (Mixtral) evaluated as generator and self-evaluator; used in both self-refine and self-rewarding experiments and shows pronounced self-bias amplification with larger candidate pools.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mixtral-MOE (8x7B)",
            "model_description": "Mixtral mixture-of-experts model (mistralai/Mixtral-8x7B-Instruct-v0.1); open-source, MoE architecture with multiple expert shards (~8x7B components as named).",
            "reflection_method_name": "self-refine and self-rewarding (LLM-as-reward)",
            "reflection_method_description": "Self-refine: iterative generate-feedback-refine loop using the same LLM as evaluator. Self-rewarding: generate k candidate responses and use the same LLM as reward model to rank/select top candidate(s) for further training or selection; paper varied k (1,4,8,16,32) to analyze bias amplification.",
            "num_iterations": 10,
            "task_name": "Machine translation (Yoruba→English) and self-rewarding candidate ranking experiments",
            "task_description": "Translation on Flores-200 (Yor-&gt;En); self-rewarding experiments test how increasing the number of candidate samples (k) affects self-bias when the LLM ranks its own outputs as reward.",
            "performance_with_reflection": "Qualitative: When used as reward model with increasing k, Mixtral's bias and dSkew increase (higher preference for its own outputs). Self-refine iterative runs show amplified bias over iterations. No reliable true-quality gains reported when relying solely on Mixtral's self-feedback.",
            "performance_without_reflection": "Baseline metrics at 0th iteration; paper reports that relying on self-rewarding selection increases bias compared to baseline, especially with larger k.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "No strong evidence that pure self-refinement or self-rewarding with Mixtral yields true quality gains; experiments show bias amplification with increasing sample size k in self-rewarding, supporting the claim that self-evaluation can exaggerate preference for model's own outputs.",
            "limitations_or_failure_cases": "Self-rewarding loop amplifies bias with larger candidate pools (k), leading to over-selection of false positives; self-refine without external/ground-truth feedback does not reliably improve true translation quality.",
            "uuid": "e5219.4",
            "source_info": {
                "paper_title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "DeepSeek-MOE",
            "name_full": "DeepSeek-MOE 16B",
            "brief_description": "An open-source mixture-of-experts model (DeepSeek-MOE) evaluated as generator and self-evaluator; included in self-refine and self-rewarding experiments and shows increased bias with iterative/self-rewarding use.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-MOE (16B)",
            "model_description": "Open-source DeepSeek MoE (deepseek-ai/deepseek-moe-16b-chat) with ~16B parameter budget in MoE configuration; used as both generator and self-evaluator.",
            "reflection_method_name": "iterative self-refine and self-rewarding",
            "reflection_method_description": "Same iterative self-feedback loop (generate feedback then refine) and self-rewarding experiments where the model ranks k sampled candidates using its own feedback. Evaluated for bias amplification as k increases.",
            "num_iterations": 10,
            "task_name": "Machine translation (Yoruba→English) and self-rewarding ranking experiments",
            "task_description": "Translation on Flores-200 Yor→En; self-rewarding experiments analyze bias when model ranks its own candidates for selection/training.",
            "performance_with_reflection": "Qualitative: DeepSeek-MOE shows increased bias and distance skewness during iterative self-refine and increased bias as k increases in self-rewarding experiments. No consistent true quality improvement reported from self-only feedback.",
            "performance_without_reflection": "Baseline translation quality at 0th iteration; self-rewarding selection amplifies bias relative to baseline and can select suboptimal candidates according to BLEURT/human metrics.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "No robust evidence that self-refine or self-rewarding using DeepSeek-MOE improves true translation quality; results show bias amplification and potential selection of false-positive outputs.",
            "limitations_or_failure_cases": "Self-bias amplified by iterative refinement and by larger candidate pools in self-rewarding, leading to over-optimistic model-internal rankings and potential degradation in downstream quality if used to train/select outputs.",
            "uuid": "e5219.5",
            "source_info": {
                "paper_title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Self-consistency verifier",
            "name_full": "Self-consistency verification (as evaluator)",
            "brief_description": "A generate-then-verify-style evaluator: generate multiple reasoning chains and use majority vote among sampled chains to verify or replace the initial chain; used as an alternative evaluator to direct self-assessment.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Self-consistency verification (implemented with LLM)",
            "model_description": "Evaluator approach that samples multiple reasoning paths (e.g., 10) and uses majority vote as a binary correctness signal; implemented with the same LLM family used for generation in experiments.",
            "reflection_method_name": "self-consistency as evaluator (generate-many + majority vote)",
            "reflection_method_description": "Given an initial solution, generate multiple additional reasoning paths; majority-vote an answer from sampled paths; if majority answer disagrees with initial, mark initial as wrong and replace it; treat this binary outcome as feedback for refinement.",
            "num_iterations": null,
            "task_name": "Mathematical reasoning (MATH)",
            "task_description": "Grade-school/competition-style math problems requiring multi-step reasoning and a final numeric/structured answer; correctness determined by exact match to ground truth.",
            "performance_with_reflection": "Reported: Using self-consistency as the evaluator still led to increases in bias and distance skewness across iterative refinement, suggesting that ensemble/consistency-based self-evaluation can also introduce self-bias and biased ensembles.",
            "performance_without_reflection": "Baseline (single-path generation) correctness at 0th iteration; replacing self-evaluation with self-consistency did not prevent bias amplification in iterative refinement experiments.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Although self-consistency is known in prior work to sometimes improve chain-of-thought correctness, in this paper replacing LLM-as-evaluator with self-consistency verification still resulted in bias amplification (Figure 11, Appendix D), indicating limited benefit in preventing self-bias in iterative refine.",
            "limitations_or_failure_cases": "Self-consistency verifier can still produce biased ensembles: majority votes and sampled paths can favor solutions that align with the model's prior tendencies, leading to amplified self-bias over iterations and not guaranteeing true-quality gains.",
            "uuid": "e5219.6",
            "source_info": {
                "paper_title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "External feedback (InstructScore)",
            "name_full": "InstructScore (reference-based external feedback model)",
            "brief_description": "An external reference-based feedback model that uses the reference to generate MQM-like error annotations (error location, type, severity); used as an oracle-like external critic to reduce self-bias and elicit true improvements.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructScore (xu1998hz/InstructScore)",
            "model_description": "Reference-based learned feedback model that takes reference and candidate translation to output fine-grained feedback (error location, error type, severity); treated as oracle external feedback in experiments because it can consult references.",
            "reflection_method_name": "external-feedback-guided self-refine",
            "reflection_method_description": "Same iterative refinement loop, but feedback is supplied by InstructScore (external evaluator with access to references) rather than the generation model itself; feedback includes MQM-like annotations that the generator uses to correct outputs.",
            "num_iterations": 5,
            "task_name": "Machine translation (Yoruba→English)",
            "task_description": "Translation on Flores-200 Yor→En; InstructScore provides external annotations to guide iterative refinement.",
            "performance_with_reflection": "With external feedback, models (including GPT-4, GPT-3.5, Gemini) show consistent BLEURT improvements across self-refine iterations and reduced self-bias (bias/dSkew curves lower than self-feedback counterparts). Case study shows BLEURT moving closer to human ratings after iterations using InstructScore.",
            "performance_without_reflection": "Compared to pure self-feedback iterations, external feedback yields higher true-metric improvements (BLEURT) whereas pure self-feedback did not increase BLEURT. Exact BLEURT numbers are not provided in main text, but plots and case studies show measurable BLEURT improvements with InstructScore across up to 5 iterations.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: paper reports that all LLMs with external InstructScore feedback achieved consistent BLEURT improvements through self-refine iterations (dotted curves below solid curves in Figure 9), and bias estimation reduced throughout refinement. Case study (Table 4) shows a concrete example where GPT-4's human score improved from -11 to -1 across refinements when using InstructScore.",
            "limitations_or_failure_cases": "InstructScore is oracle-like because it uses the reference; while it reduces self-bias and elicits true improvements, it relies on reference availability and does not provide explicit corrections—generator still must perform corrections using its internal knowledge. Not a fully general solution when references are unavailable.",
            "uuid": "e5219.7",
            "source_info": {
                "paper_title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models",
            "rating": 2,
            "sanitized_title": "selfcheckgpt_zeroresource_blackbox_hallucination_detection_for_generative_large_language_models"
        },
        {
            "paper_title": "Large language models cannot self-correct reasoning yet",
            "rating": 2,
            "sanitized_title": "large_language_models_cannot_selfcorrect_reasoning_yet"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Can large language models really improve by self-critiquing their own plans?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_really_improve_by_selfcritiquing_their_own_plans"
        },
        {
            "paper_title": "LLMs as narcissistic evaluators: When ego inflates evaluation scores",
            "rating": 2,
            "sanitized_title": "llms_as_narcissistic_evaluators_when_ego_inflates_evaluation_scores"
        },
        {
            "paper_title": "Teaching large language models to self-debug",
            "rating": 2,
            "sanitized_title": "teaching_large_language_models_to_selfdebug"
        },
        {
            "paper_title": "Self-rewarding language models",
            "rating": 2,
            "sanitized_title": "selfrewarding_language_models"
        },
        {
            "paper_title": "Gptscore: Evaluate as you desire",
            "rating": 1,
            "sanitized_title": "gptscore_evaluate_as_you_desire"
        },
        {
            "paper_title": "Llmrefine: Pinpointing and refining large language models via fine-grained actionable feedback",
            "rating": 2,
            "sanitized_title": "llmrefine_pinpointing_and_refining_large_language_models_via_finegrained_actionable_feedback"
        },
        {
            "paper_title": "InstructScore",
            "rating": 2,
            "sanitized_title": "instructscore"
        }
    ],
    "cost": 0.0172075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement
18 Jun 2024</p>
<p>Wenda Xu wendaxu@cs.ucsb.edu 
University of California
Santa Barbara</p>
<p>Guanglei Zhu guanglez@cs.cmu.edu 
Carnegie Mellon University</p>
<p>Xuandong Zhao xuandongzhao@cs.ucsb.edu 
University of California
Santa Barbara</p>
<p>Liangming Pan liangmingpan@cs.ucsb.edu 
University of California
Santa Barbara</p>
<p>Lei Li leili@cs.cmu.edu 
Carnegie Mellon University</p>
<p>William Yang Wang william@cs.ucsb.edu 
University of California
Santa Barbara</p>
<p>Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement
18 Jun 2024F2664D3D3308DE1F92E38A31FBA87F3AarXiv:2402.11436v2[cs.CL]
Recent studies show that large language models (LLMs) improve their performance through self-feedback on certain tasks while degrade on others.We discovered that such a contrary is due to LLM's bias in evaluating their own output.In this paper, we formally define LLM's self-bias -the tendency to favor its own generation -using two statistics.We analyze six LLMs (GPT-4, GPT-3.5, Gemini, LLaMA2, Mixtral and DeepSeek) on translation, constrained text generation, and mathematical reasoning tasks.We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks.Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias.To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks.The code and data are released at https://github.com/xu1998hz/llm_self_bias.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have shown strong capabilities in many NLP tasks.While these models still make mistakes, recent studies show that "self-refine" (also known as "self-reflection") is promising to rectify errors based on LLM's selffeedback (Madaan et al., 2024;Chen et al., 2024;Shinn et al., 2024;Manakul et al., 2023;Pan et al., 2023).Meanwhile, opposite study also shows that LLMs fail to correct their mistakes and their performance even gets worse after self-feedback (Huang et al., 2023b).These contradictory results suggest that LLM's self-feedback is unreliable.Selfrefine procedure relies on LLM's evaluation capability of the generated text.We hypothesize that if there is a bias during the self-evaluation process, such bias will be amplified during iterative self-Figure 1: How LLM's self-feedback inflates scores compared to human assessment.Bias is the mean difference between LLM and human scores, while skewness (Dskew) measures the asymmetry of their distribution around zero.Non-biased estimation will have Dskew=0.</p>
<p>refinement.This is consistent with a prior finding that LM-based metrics (e.g.BARTScore) exhibit "narcissism" during self-evaluation, i.e., the metric model favors text generated by the same underlying language model in the context of summarization tasks (Liu et al., 2023b).However, it remains unclear whether bias exists universally in LLMs across a wide range of tasks.How to quantify such biases?How does this "narcissism" impact LLM's self-refinement?</p>
<p>In this work, we define "self-bias" to the degree that an LLM favors its own generation.We propose to use two principled statistics to estimate self-bias in LLM's self-refinement procedure.The first one measures the degree of inflation in the LLM's self-evaluation compared to the true (human) evaluation.The second measures whether LLM's self-evaluation is skewed compared to the ture estimate.Figure 1 illustrates these two statis-tics.We examine self-bias scores on six diverse LLMs, covering four languages across three distinct tasks: machine translation, constrained text generation, and mathematical reasoning.We find that self-bias is universal in self-refine and selfrewarding pipelines, regardless of the languages and tasks.This bias causes LLMs to optimize for false positive corrections rather than improving the actual output quality.</p>
<p>We further investigate what is the real benefit of self-refine.We find that while the self-refine pipeline improves the fluency and understandability of model outputs, it does not necessarily lead to intended improvements as specified in the prompt.Moreover, LLMs may favor texts that mirror their style, potentially leading to false positive optimization and reduced diversity in text generation.To mitigate the self-bias, we propose two solutions: increasing the model size and incorporating external feedback to provide accurate assessment, thereby directing the LLM towards more accurate self-correction.Our contributions are:</p>
<ol>
<li>
<p>We formally define the self-bias of an LLM using two principled estimated statistics.</p>
</li>
<li>
<p>We quantify self-biases for six diverse LLMs and find that self-bias amplifies during selfrefine across many languages and tasks.</p>
</li>
<li>
<p>We observe two factors that contribute to selfbias and pinpoint two directions to mitigate it and elicit LLMs' self-correction ability.</p>
</li>
</ol>
<p>Related Work</p>
<p>Large Language Model Self-correction.Recent works demonstrate that LLM can utilize its own feedback signal to refine itself (Madaan et al., 2024;Chen et al., 2024;Shinn et al., 2024).Wang et al. (2023) further proposed to sample diverse reasoning paths and use a majority vote to find the most confident answer.Huang et al. (2023a) leverages self-consistency to further fine-tune the LLM on the most confident reasoning path with diverse instruction formats.On the other hand, LLM's selffeedback can also be used as a reward signal to further align LLM to follow instructions (Gulcehre et al., 2023;Yuan et al., 2024).</p>
<p>Despite some demonstrations of performance improvements, most findings indicate that LLMs struggle to rectify their initial mistakes, and their performance even worsens after self-correction (Huang et al., 2023b;Tyen et al., 2023;Ke et al., 2023).This issue arises because the quality of the model's self-generated feedback is bounded by its existing knowledge and abilities (Stechly et al., 2023;Hong et al., 2023).Therefore, internal feedback may not offer any extra advantage for improving the results; it might even steer the model away from the correct answer (Valmeekam et al., 2023).However, prior works only had empirical observations on this phenomenon, while lacking a quantitative analysis.Moreover, prior works only focus on specific tasks, such as reasoning or code generation.In this work, we are the first to quantitatively analyze the self-bias of different LLMs across three tasks and four languages, which provides a novel and generalizable view to address the perils of self-refine.</p>
<p>LLMs as Evaluators.et al. (2023b) point out that reference-free metrics are inherently biased on their own outputs.</p>
<p>Although the above empirical studies provide valuable insights, they lack a formal definition to quantify those biases nor provide a connection to the self-refine framework.In this work, we define and quantify self-bias and provide the first in-depth analysis of its impact on the self-refine pipeline.</p>
<p>We analyze potential bias attributions and pinpoint two mitigation directions.</p>
<p>Quantifying Self-Bias</p>
<p>This section outlines the approach used to quantify the self-bias exhibited by LLMs in an iterative selfrefinement pipeline.We employ statistical bias and distance skewness (Szekely and Móri, 2006) estimation to measure self-bias.</p>
<p>Iterative Self-Refinement in LLMs</p>
<p>Self-refinement is an inference time method, in which the LLM first generates a response y i to a given prompt x, and then the same LLM generates feedback f i based on the candidate output y i and input x.Based on feedback f i , input x, and candidate output y i , the LLM then generates a refined output r i .LLM iterates between the feedback and the refinement steps, continuing until it reaches a predetermined number of iterations.At each refinement step, the refined output will only be accepted if it demonstrates superior quality compared to the previously generated text.The quality of the text is assessed through self-feedback from the language model itself.At each feedback or refinement step, LLM only sees the last iteration's generation or feedback, without accessing the entire history of output or feedback.</p>
<p>Bias Estimation</p>
<p>We estimate the self-bias of LLMs using the statistical bias definition.This bias is characterized by the disparity between an LLM's predicted quality score and the expected quality score, as follows:
Bias( θ) = 1 n n i=1 (E[ θi ] − θ i ),(1)
where E[ θi ] is an expected LLM's quality prediction at sample i, and θ i denotes the true quality of sample i. Ideally, θ i should be derived from human annotations, for example, multidimensional quality metrics (MQM) human annotations (Freitag et al., 2021) for machine translation, or predefined criteria such as word coverage for constrained text generation (Madaan et al., 2024).The LLM's quality prediction is expected to precisely follow the human annotation procedure or predefined criteria, ensuring consistency between θ and E[ θ].When Bias( θ) &gt; 0, the LLM assigns a higher quality score to its own sample compared to the expected quality score.When Bias( θ) &lt; 0, the LLM underestimates the sample quality compared to the expected quality score.The larger the value of Bias( θ), the more pronounced the LLM's bias against its own samples.</p>
<p>Distance Skewness Estimation</p>
<p>In an ideal scenario, an unbiased LLM should have equal chance of over-estimation and underestimation of text quality (Bias( θ) = 0), resulting We use distance skewness to measure the asymmetry of distribution.Therefore, using two meta-metrics as complimentary, we can measure the self-bias of LLM.</p>
<p>in a perfectly symmetric distribution when plotting E[ θ] − θ.However, Bias( θ) = 0 does not guarantee a symmetric distribution (In Figure 2, one tail could be long and thin, while the other is short and fat, yet they balance out overall).Therefore, we introduce another meta-metric, distance skewness, to measure the asymmetry of E[ θ]−θ's distribution.Specifically,
dSkew n (X) = 1 − i,j ∥x i − x j ∥ i,j ∥x i + x j − 2γ∥ ,(2)
where x i and x j are two independent identical random examples drawn from E[ θ] − θ. dSkew n (X) measures the asymmetry of X with respect to γ. Distance skewness ranges between 0 and 1. dSkew n (X) equals 0 if and only if X is diagonally distributed respect to γ. dSkew n (X) equals 1 if and only if X is distributed at a constant on one side of γ.A higher distance skewness indicates a more asymmetric distribution of E[ θ] − θ.In our experimental setup, we use both bias and distance skewness to measure the model's bias towards its quality prediction.</p>
<p>4 Analyzing LLM's Self-Bias</p>
<p>Experimental Setup</p>
<p>We include three closed-source LLMs (GPT-4 (Achiam et al., 2023), GPT-3.5-Turbo and Gemini (Team et al., 2023)) and three open-source LLMs (LLaMA2-7B (Touvron et al., 2023), Mixtral-MOE 8x7B (Jiang et al., 2024) and DeepSeekMoE 16B (Dai et al., 2024)).These models have been shown to have strong instruction-following capabilities (Madaan et al., 2024;Shinn et al., 2024), making them well-suited to demonstrate self-bias.</p>
<p>For each model, we first prompt it to produce the initial generation.Then, we prompt the model to generate the feedback for the initial generation.The model takes in both the feedback and the prior step generation to produce a refined output.We will only accept refinement if the feedback score is improved on the refined output.We listed specific model API/checkpoints in Appendix Section A.</p>
<p>Machine Translation.We evaluated LLMs on Flores-200 (Costa-jussà et al., 2022) dataset with four language pairs: Yoruba to English (Yor-En), Javanese to English (Jav-En), Armenian to English (Arm-En), and Igbo to English (Ig-En), using 100 test examples per language pair.We concentrate on low-to-medium resource language pairs, as Kocmi et al. (2023) indicate that LLMs like GPT-4 already perform at a nearly human-like level in high resource language pairs such as Chinese-to-English, leaving limited potential for further improvement through self-refine.</p>
<p>To ensure high-quality evaluations, we utilized feedback prompts based on the MQM human annotation from Freitag et al. (2021), as in Kocmi and Federmann (2023).LLMs will input source text and candidate text and output feedback, including error location, error type, and severity labels.We adopt the same error scoring as Freitag et al. (2021), assigning −1 for minor errors and −5 for major errors, with a score range of 0 to −25 (0 for perfect translations, −25 for samples with more than five severe errors).The details of the prompts are provided in the Appendix Table 8, 9 and 10.</p>
<p>Ideally, human raters would have evaluated each sample, but due to cost and scalability constraints, we utilized the reference-based learned metric BLEURT (Sellam et al., 2020) as an approximation of human judgments.BLEURT generates quality scores based on the similarity between candidate and reference translations.To align BLEURT's score distribution with that of human ratings, we employed quantile mapping (Cannon et al., 2015), yielding a score range from 0 to -25.Although automatic metrics are primarily used, we also conduct modified MQM human evaluations (Freitag et al., 2021) for validation purposes.Our bias estimation ranged from -25 to 25.Details on quantile mapping are provided in the Appendix Section B.</p>
<p>Constrained Text Generation.We conducted experiments on commonsense text generation, following (Lin et al., 2020).We tested LLMs on 100 examples from the CommonGen Hard dataset.For each testing instance, the large language model (LLM) received approximately 30 concepts and was tasked with generating a fluent and logically sound text.To generate the initial output, we adopted a similar prompt design to that of (Lin et al., 2020).Next, we provided two ICL feedback examples to help the LLM identify missing concepts in its initial output.In each feedback example, the LLM was given concept words and the previous generation and asked to indicate any missing concepts.This feedback allowed the LLM to revise its output and generate a text with better coverage of the input concepts.The details of the prompts are included in the Appendix Table 12, 13 and 14.</p>
<p>To evaluate the coverage of the generated texts, we adopted the evaluation metric used in (Madaan et al., 2024).This metric uses strict string matching to determine whether each concept word from the input appears in the generated text (metric outputs 1 if all concepts are covered and 0 otherwise).From feedback of LLM's missing concepts, we assigned a binary score (0 or 1) to each text based on its full coverage of concepts.Since our string-matching metric and LLM feedback score were on the same scale, we were able to compute bias and distance skewness directly.The range of bias estimation is between −1 to 1.</p>
<p>Mathematical Reasoning.We conducted experiments on mathematical reasoning.We tested LLMs on 100 examples from the MATH testing set (Hendrycks et al., 2021).For each instance, LLM receives a problem statement and generates a step-by-step solution with a final answer.In this task, we use the self-refine pipeline by providing the feedback on the step-by-step solution.In each iteration, the previous solution will be compared against the ground truth answer, outputting 1 if they are matched and 0 otherwise.Therefore, we can directly compute bias and distance skewness.The range of bias estimation is between −1 to 1.The details of the prompts are included in the Appendix Table 11.In addition, we also conducted experiments by replacing the self-evaluation (LLM as evaluator) with self-consistency verification (selfconsistency as an evaluator) (Huang et al., 2023a).</p>
<p>We include those results in the Appendix D.</p>
<p>Self-Bias Amplification during Iterative Refinement</p>
<p>Machine Translation.In Figure 3, we illustrate that all large language models (LLMs) exhibit a self-bias in the self-refine pipeline.Notably, opensource LLMs and GPT-3.5-Turbotend to exhibit higher levels of self-bias throughout iterations than stronger instruction-following LLMs, such as GPT-4 and Gemini.This suggests that GPT-4 and Gemini possess a certain level of capability in resisting self-bias.However, despite some robustness demonstrated by GPT-4 and Gemini, we observe a consistent amplification of self-bias through the self-refine pipeline across four language directions, indicating that even these advanced LLMs are susceptible to self-bias amplification.</p>
<p>In Figure 4, we illustrate a comparison between GPT-4 and Gemini's quality assessments of their own outputs and performance measured by reference-based BLEURT over ten iterations.Our findings suggest that the primary reason for the amplification of bias during self-refine iteration is that actual performance does not improve through iterations.Instead, GPT-4 and Gemini mistakenly perceive performance improvements in their refined outputs.This discrepancy between the false positive performance measure and the true performance measure grows larger with each iteration.The appendix Section C details Gemini's shift from right-skewed to left-skewed distribution, resulting in a decrease in distance skewness during early iterations and an increase in later ones.</p>
<p>Constrained Text Generation. Figure 5 depicts the amplification of self-bias through ten self-refine iterations in constrained text generation for GPT-3.5-Turbo,GPT-4, and Gemini.Notably, GPT-4 exhibits a higher bias estimation at earlier iterations compared to GPT-3.5-Turbo and Gemini.This can be attributed to GPT-4's higher coverage ratio at to its counterparts (GPT-3.5-Turboat around 2%). Consequently, GPT-4 struggles to identify a few missing concepts, while GPT-3.5-Turboand Gemini have more coverage issues and can easily identify missing input concepts.</p>
<p>As GPT-3.5-Turbo reaches 20% coverage around the 5th iteration, it experiences a significant rise in bias and skewness estimation.It is worth noting that the rate of LLM's self-estimated improvements is much higher than the true coverage improvements.This phenomenon results in a saturation of performance improvements after the 5th iteration for both GPT-4 and GPT-3.5-Turbo.</p>
<p>Mathematical Reasoning.Figure 6 illustrates that all large language models (LLMs) exhibit an increase in bias and skewness estimation in the iterative self-refine pipeline.This suggests that LLMs introduce self-biases towards some math solutions during self-refine.</p>
<p>Human Evaluation on Bias Estimation.We employ one graduate student to annotate 50 examples from the 0th and 10th iteration of GPT-4, GPT-3.5-Turbo and Gemini's outputs at Yor-En, respectively.The human rater compares candidate text against reference and labels error location, error type, and severity labels at candidate text.The scoring scheme follows MQM style (Freitag et al., 2021) Figure 5: We evaluate the bias and distance skewness of generated texts produced by GPT-4, GPT-3.5-Turbo, and Gemini on the CommonGen dataset, across selfrefinement steps.Additionally, we report the coverage of GPT-3.5-Turbo and GPT-4 compared to true concept coverage.We show that the rate of LLM's self-estimated improvements is much higher than the true coverage improvements, which leads to self-bias amplification.are included in the Appendix Table 5, 6 and 7), which is consistent with the BLEURT assessment.</p>
<p>In Table 1, both increasing bias and distance skewness estimation demonstrate that all LLMs have significantly increased their self-bias with 10 iterative refinements.In the following case study, we examine self-bias in GPT-4.Our observations reveal that GPT-4's self-feedback mechanism led to the optimization of false positives, resulting in an amplification of self-bias over three iterations.In section 5, we demonstrate two potential alleviation that we can use to mitigate this self-bias.</p>
<p>Self-bias Example at GPT-4</p>
<p>Yoruba text: Ní bayii a ni àwon eku oloshu merin ti ko ni dayabetesi telele to ti ni ayabetesi," o she afikun.</p>
<p>Reference English text: "We now have 4-month-old mice that are non-diabetic that used to be diabetic," he added.</p>
<p>(Red span indicates a major error and blue span indicates a minor error annotated by GPT-4.)</p>
<p>GPT-4's 1st generation [Human: -11, GPT4: -11, Bias: 0]: "At this point, we have four rats without diabetes that have developed diabetes," he added.</p>
<p>GPT-4's 1st refinement [Human: -12, GPT4: -10, Bias: 2]: "Currently, we have four healthy rats that have developed diabetes," he clarified.</p>
<p>GPT-4's 2nd refinement [Human: -11, GPT4: 0, Bias: 11]: "Presently, we have four non-diabetic rats that have developed diabetes," he elaborated.</p>
<p>GPT-4 GPT-3.5-In Table 2, we include format accuracy for all LLMs.We observed that all LLMs have either perfect or nearly perfect format at first and fifth iteration of self-feedback.This is expected as we explicitly provide three in-context examples to control the output format.We found that different LLMs make different format mistakes.For example, DeepSeekMOE produces one or two garbage outputs and GPT-3.5-Turboproduces two or three free form outputs, like "The machine translation  is incorrect as it provides an alternative translation that does not match the source text."We conclude that this is due to their intrinsic instability of their instruction following capabilities.Gemini model contains surprisingly low format accuracy compared to other LLMs.This is due to the Gemini model refusing to generate any content that involves sensitive topics.There are 7 sentences in our testing set, Gemini refuses to provide responses.However, since our study focuses on self-bias amplification at iterations, this will not impact our experimental conclusions (The effects canceled out when comparing 1st and 5th iteration).</p>
<p>)OXHQF\</p>
<p>8QGHUVWDQGELOLW\</p>
<p><em>37 </em>377XUER *HPLQL</p>
<p>What improves after self-refinement?</p>
<p>Self-refinement can improve fluency and understandability but not quality.We demonstrate that LLM with biased feedback can impede the model's self-refine process.This raises a natural question: if an LLM does not improve its generation quality, does it improve in any other aspects throughout the iterative refine phase?To investigate this, we utilize the learned metric UniEval (Zhong et al., 2022) to measure the LLM's improvement beyond quality metrics.UniEval, a multidimensional learned metric, estimates various evaluation dimensions, including fluency, understandability, engagement and more.We focus on two dimensions, fluency and understandability, which UniEval is not trained on task-specific data.Our results, illustrated in Figure 6, show that GPT-4, GPT-3.5-Turbo, and Gemini consistently exhibit improvements in both fluency and understandability.This suggests an alternative perspective on the self-refine pipeline, indicating that while an LLM may not strictly adhere to instruction-following in terms of quality improvements, it can still improve certain intrinsic text qualities, such as fluency and understandability.Figure 8: We used Madlad400-10b to translate 100 Yor-En translations and asked GPT-4, GPT-3.5-Turbo, and Gemini to paraphrase 100 translations.We show the BLEURT and LLM scores before and after paraphrasing.In the lower right of the figure, we show the bias estimation before and after paraphrasing.GPT-4 and Gemini have negative self-bias before paraphrasing.After paraphrasing, all LLMs increase their bias against their paraphrased outputs.</p>
<p>LLMs favor texts that follow their style.To explore this propensity, we conducted experiments to investigate if LLMs display a preference for outputs that align with their generation style.We asked the GPT4, GPT-3.5-Turbo, and Gemini model to paraphrase external translation outputs.In this prompt, LLMs aimed not to improve the quality of translations but rather to rewrite sentences in their corresponding styles.Using the multilingual translation system Madlad400-10b (Kudugunta et al., 2023), we produced 100 Yoruba-to-English translations.Subsequently, each LLM was instructed to paraphrase the generated sentences.Our findings, shown in Figure 8, reveal that GPT-4 and Gemini have negative self-bias before paraphrasing.However, after paraphrasing, all LLMs showed an in- creased bias against their paraphrased outputs.This is mainly attributed to a decline in quality performance post-paraphrasing, with LLMs erroneously perceiving these paraphrased outputs as indicative of improvements.</p>
<p>Self-Bias is Amplified at Self-Rewarding Pipeline</p>
<p>In this section, we will explore the concept of selfbias in the self-rewarding pipeline, as outlined in (Yuan et al., 2024).The pipeline begins with an instruction fine-tuned large language model (LLM).Initially, we generate k candidate responses for each input provided to the LLM.Next, the same LLM is used as a reward model to identify the bestperforming candidate or to rank pairs within the collection of samples.Finally, various training objectives are applied to further train the LLM using the top-performing samples.</p>
<p>To illustrate the potential drawbacks of this pipeline, we carried out experiments on Yoruba to English translation task using three opensource LLMs: Deepseek-MOE, MixtralMOE, and LLaMA2-7B.For each source input, we sampled k candidate responses from each model.Subsequently, we obtained self-feedback scores on these candidates employing the prompt detailed in Section 4.1 and computed the corresponding self-bias.We varied k across 1, 4, 8, 16, and 32 to examine the influence of sample size on the self-bias within the self-rewarding pipeline.</p>
<p>As shown in Table 3, we observed that all LLMs displayed an increase in bias and distance skewness as the sample size increased.This occurs when the LLM has a biased estimation of its self-feedback, and this bias can be amplified when the sample size is increased to find the top-performing candidate according to the self-feedback.9: Using an external feedback model, we provide external feedback for GPT-4, GPT-3.5-Turbo, and Gemini in Yoruba-to-English translation task, across 5 refinement steps.We compare the models' true performance (measured by BLEURT) against external feedback-evaluated performance and self-feedback evaluated performance.Additionally, we plot the bias estimation for the three LLMs, considering both feedback types over 5 iterative refinement steps.</p>
<p>Alleviating Self-Bias</p>
<p>External Feedback Reduces Self-Bias.We demonstrated that self-feedback from a large language model can self-amplify bias with iterative refinement.We aim to answer if external feedback with low bias estimation can improve the model's generation performance and elicit self-correction capability.We leverage a reference-based feedback model, InstructScore (Xu et al., 2023), to provide external feedback.InstructScore will take in both reference and candidate text and output finegrained feedback, including error location, severity label, and error type.To ensure a fair comparison, we parse all outputs with the same format as selffeedback.Since InstructScore can access reference text to provide feedback, we recognize this external feedback as oracle feedback.However, models will only receive information about error location, error type, and severity labels.Therefore, refinement</p>
<p>External Feedback Example at GPT-4</p>
<p>Yoruba text: Ní bayii a ni àwon eku oloshu merin ti ko ni dayabetesi telele to ti ni ayabetesi," o she afikun.</p>
<p>Reference English text: "We now have 4-month-old mice that are non-diabetic that used to be diabetic," he added.(Red span indicates a major error and blue span indicates a minor error annotated by GPT-4.)</p>
<p>GPT-4's 1st generation [Human: -11, InstructScore: -10, Bias: 1]: "At this point, we have four rats without diabetes that have developed diabetes," he added.</p>
<p>GPT-4's 1st refinement [Human: -2, InstructScore: -6, Bias: -4]: "At this point, we have four mice without diabetes that were diabetic," he added.</p>
<p>GPT-4's 2nd refinement [Human: -1, InstructScore: -1, Bias: 0]: "We now have 4-month-old mice that are non-diabetic that were diabetic," he added.</p>
<p>Table 4: This case study demonstrates that external feedback (oracle) from InstructScore (Xu et al., 2023) can remain low self-bias during iterative self-refine.By providing accurate error type, error location, and severity labels, InstructScore effectively elicits GPT-4's selfcorrection capability and improves its translation quality.Despite InstructScore's oracle-like role (which it can access reference text to make error annotations), it does not provide explicit corrections, requiring GPT-4 to rely on its internal knowledge for corrections.</p>
<p>still relies on LLM's self-correction capability.</p>
<p>In Figure 9, we demonstrate that external feedback with accurate assessment can significantly lower the model's bias at iterative refinement (shown at the lower right of the figure.All dotted curves are below solid curves with corresponding colors).Interestingly, both Gemini and GPT-4's bias estimation is improved throughout the refinement process, as the external feedback model can over-penalize low-quality outputs.As refinement proceeds, the external feedback model converges to BLEURT quality assessment that samples achieve improved quality.Most importantly, we demonstrate that all LLMs with external feedback can elicit their self-correction ability with consistent BLEURT improvements at self-refine iterations.We include a case study example in Table 4.Our finding of model improvement is consistent with prior study (Xu et al., 2024) and we further demonstrate that external feedback can significantly reduce self-bias.Larger Model Reduces Self-Bias.In Figure 10, we demonstrate that LLMs with larger parameter size can have less self-bias throughout self-refinement steps.Specifically, we tested the LLaMA2 models with 7B, 13B, and 70B parameters on Yoruba-to-English (Yor-En) translation tasks.Our findings indicate that while the LLaMA2-70B model exhibits self-bias in the earlier iterations, its self-bias begins to plateau after the 5th iteration.In contrast, the 7B and 13B models continue to amplify their self-bias in later iterations.This observation aligns with prior work (Huang et al., 2023a), which posited that larger LLMs possess better self-refinement capabilities.Our study contributes to this discussion from the perspective of self-bias, proposing that larger LLMs are more resilient to self-bias.Consequently, they can assess their own outputs more accurately and possess a greater capacity for self-correction.</p>
<p>Conclusion</p>
<p>In this study, we define and quantify self-bias in LLMs with two principled estimated statistics.Our experiments across six LLM families, four languages, and three tasks reveal that self-bias is prevalent in self-refine or self-rewarding pipelines.This biased self-feedback leads to false positive objectives, hindering performance improvements during iterative refinement.Further analysis reveals that while LLM improves fluency and understanding of its generated text, they do not necessarily progress in the intended direction, such as improving quality in machine translation or expanding coverage in concept-to-word generation.Instead, LLMs tend to favor texts that adhere to their inherent styles.Finally, our research suggests that larger models are more resistant to self-bias, and incorporating external feedback significantly reduces bias, leading to performance improvements in LLMs.</p>
<p>Limitations</p>
<p>In this study, we focus on quantifying the self-bias exhibited by LLMs in the self-refine pipeline.We demonstrate that self-bias will be amplified in the self-refine or self-rewarding pipeline and negatively impacts the optimization process.However, in subsequent research, it would be worthwhile to explore the measurement of bias that exists between different LLMs, as well as the bias that arises when comparing original models and their knowledgedistilled counterparts.The following questions remain open: Does LLM have more bias towards LLMs that follow the same pretraining procedure, data, or learning objectives?Does LLM have more bias to the LLMs within the same language model families?Do knowledge-distilled LLMs have more biases over the original LLMs, such as Vicuna to GPT4 or Alpaca to ChatGPT?We leave these interesting avenues for future research.</p>
<p>Ethical Statement</p>
<p>All the benchmark data that we used during experiments is publicly available.We assure that the benchmark data does not contain risk or toxic content.The annotater was compensated fairly and did not disclose any privacy information during the annotation process.All the open sourced models can be accessed online and all the closed source models have publicly accessible APIs.The annotaters were allowed to label sensitive information if necessary.The annotater is fully aware that the data we collected from him/her will be used for research purposes.The total human annotation period took six hours and the annotator was paid above local minimum wage.We used Mistral Medium, Grammarly and ChatGPT API to polish some of our writings.</p>
<p>The findings of this research have far-reaching implications for the broader linguistic and technological communities, particularly in the preservation and revitalization of endangered or low-resource languages.By identifying and mitigating self-bias in large language models (LLMs), this work paves the way for significant improvements in machine translation for languages that are underrepresented in digital platforms and datasets.</p>
<p>The ability to reduce bias in the self-refine pipeline of LLMs can lead to more accurate and nuanced translations, thereby enhancing the quality and accessibility of digital content in low-resource languages.This advancement is critical for preserving the cultural heritage and knowledge embodied in these languages, which are at risk of disappearing.Through improved translation capabilities, communities can more easily access global information in their native languages, fostering educational opportunities and cultural exchange.This contributes to the preservation of linguistic diversity and promotes a more inclusive digital ecosystem.</p>
<p>A Model API/Checkpoints</p>
<p>This section provides a pointer to checkpoints that we used during experiment.All opensource models are available on the Hugging Face platform.For LLaMA2, we use "metallama/Llama-2-(7, 13, 70)b-chat-hf" respectively.</p>
<p>For Mixtral MOE, we use "mistralai/Mixtral-8x7B-Instruct-v0.1".For DeepSeekMoE, we use "deepseek-ai/deepseek-moe-16b-chat".For InstructScore, we use "xu1998hz/InstructScore".</p>
<p>For the translation model Madlad400-10b, we use "google/madlad400-10b-mt". We used GPT-3.5-Turboand GPT-4 from OpenAI platform (https://platform.openai.com).We use gemini-pro from Google Gemini API.</p>
<p>B Quantile Mapping</p>
<p>While BLEURT (Sellam et al., 2020) correlates highly with human judgments (Freitag et al., 2022), its scale of roughly 0 to 1 is incompatible with the MQM human annotations, which range from -25 to 0. A linear mapping is not feasible, as the BLEURT score is not calibrated to the human score, meaning a BLEURT score of 0.8 does not correspond to -5 in MQM annotations.</p>
<p>To address this issue, we employ quantile mapping (Cannon et al., 2015) to transform the BLEURT score into the distribution of human scores.This method involves learning a mapping function that maps the quantiles or percentiles of the predictive distribution to those of the observed distribution.In this case, our predictive distribution is derived from the BLEURT score distribution, while our observed distribution comes from the corresponding human score distribution.</p>
<p>We utilize the WMT22 shared metric task (Freitag et al., 2022) to obtain mapped BLEURT-human scoring pairs.In this shared metric task, each translation generated by different translation model is rated by humans using the MQM human rating scale.We also run BLEURT on the same set of translations to obtain BLEURT scores, resulting in 28125 mapped BLEURT-human scoring pairs.</p>
<p>We then perform the following steps: 1) Separately sort the data of the two distributions in ascending order.2) Compute the cumulative distribution function (CDF) for each distribution.3) Learn an interpolation function that maps the percentiles of the first distribution to the percentiles of the sec-</p>
<p>%LDVRQ0DWK5HDVRQLQJ 'VNHZRQ0DWK5HDVRQLQJ</p>
<p><em>37 </em>HPLQL *377XUER This process maps the BLEURT score distribution to the human score distribution (from -25 to 0) while preserving the relative ordering of BLEURT scores.In our experiments, we used the latest BLEURT model, BLEURT-20 checkpoint (Pu et al., 2021), which demonstrates the highest correlation to the human judgments among its variants.</p>
<p>C Gemini's Skewness at Translation</p>
<p>Specifically, in the Java-English (Jav-En) language pair, Gemini initially assigns lower quality scores to its output compared to BLEURT assessments during early iterations, resulting in an underestimation of output performance.This phenomenon accounts for the decrease in distance skewness at the beginning, as the right-skewed distribution becomes more neutral.However, as bias accumulates in later iterations, the distribution shifts towards a left-skewed distribution, leading to an increase in distance skewness.</p>
<p>D Self-consistency results on Math reasoning</p>
<p>We slightly modify the self-refine pipeline by replacing the self-evaluation with self-consistency verification (Huang et al., 2023a).Namely, with the initial solution, LLM will generate an additional ten reasoning paths and a majority vote for a proposed answer.If the proposed answer is inconsistent with the prior solution, we will output a binary score of 0, and the initial answer will be replaced by the proposed answer.Otherwise, we will output a score of 1, and no change will be made to the initial answer.Figure 11 illustrates that all large language models (LLMs) exhibit an increase in bias and skewness estimation in the iterative self-consistency pipeline.This suggests that LLMs introduce self-biases towards certain reasoning paths during self-refine, ultimately leading to a biased ensemble across multiple reasoning paths.</p>
<p>E Additional Results</p>
<p>In Table 5, we include human evaluation results and GPT-4's quality scores for the 0th and 10th iteration of refinement generation at Yorba-to-English.In Table 6, we include human evaluation and GPT-3.5-Turbo'squality assessment on the 0th and 10th iteration of refinement generation at Yorba-to-English.In Table 7, we include human evaluation and Gemini's quality assessment on the 0th and 10th iterations of refinement generation.In Figure 12, we include full bias and distance skewness for Yor-En, Jav-En, Arm-En and Ig-En translations on Flo-res200.</p>
<p>Output for translation:</p>
<p>Can you please turn off the WiFi, I'm done.</p>
<p>Table 8: Those are the translation in context learning example we used to prompt all LLMs across four language directions at Flores200.In this example, the source translation is Yourba text "O ko ago ilekun WiFi, O wi.".The English output text is from LLaMA2-7B's generation "Can you please turn off the WiFi, I'm done.".</p>
<p>In-context-learning prompt for LLM's Self-feedback at translation: You are an annotator for the quality of machine translation.Your task is to identify errors and assess the quality of the translation.</p>
<p>Based on the source segment and machine translation surrounded with triple backticks, identify error types in the translation and classify them.The categories of errors are: accuracy (addition, mistranslation, omission, untranslated text), fluency (character encoding, grammar, inconsistency, punctuation, register, spelling), locale convention (currency, date, name, telephone, or time format) style (awkward), terminology (inappropriate for context, inconsistent use), non-translation, other, or no-error.Each error is classified as one of three categories: critical, major, and minor.Critical errors inhibit comprehension of the text.Major errors disrupt the flow, but what the text is trying to say is still understandable.Minor errors are technically errors, but do not disrupt the flow or hinder comprehension.</p>
<p>Source: "'大众点评乌鲁木齐家居商场频道为您提供高铁居然之家地址，电话，营业时间等最新商户信息， 找装修公司，就上大众点评"' Translation: "'Urumqi Home Furnishing Store Channel provides you with the latest bussiness information such as the address, telephone number, bussiness hours, etc., of high-speed rail, and find a decoration company, and go to the reviews."'Annotate errors in the translation.MQM annotations:</p>
<p>"of high-speed rail" is a critical accuracy/addition error "go to the reviews" is a major accuracy/mistranslation error "etc.," is a minor style/awkwards error Source: "'I do apologise about this, we must gain permission from the account holder to discuss an order with another person, I apologise if this was done previously, however, I would not be able to discuss this with yourself without the account holders permission."'Translation: "'Ich entschuldige mich dafür, wir müssen die Erlaubnis einholen, um eine Bestellung mit einer anderen Person zu besprechen.Ich entschuldige mich, falls dies zuvor geschehen wäre, aber ohne die Erlaubnis des Kontoinhabers wäre ich nicht in der Lage, dies mit dir involvement."'Annotate errors in the translation.MQM annotations:</p>
<p>'involvement' is a major accuracy/mistranslation error 'the account holder' is a major accuracy/omission error 'wäre' is a minor fluency/grammar error 'dir' is a minor fluency/register error Source: "'Talks have resumed in Vienna to try to revive the nuclear pact, with both sides trying to gauge the prospects of success after the latest exchanges in the stop-start negotiations."'Translation: "'Ve Vídni se ve Vídni obnovily rozhovory o oživení jaderného paktu, přičemže obě partaje se snaží posoudit vyhlídky na úspěch po posledních výměnách v jednáních."'Annotate errors in the translation.MQM annotations:</p>
<p>'ve Vídni' is a major accuracy/addition error 'the stop-start' is a major accuracy/omission error 'partaje' is a minor terminology/inappropriate for context error Source: "'Talks have resumed in Vienna to try to revive the nuclear pact, with both sides trying to gauge the prospects of success after the latest exchanges in the stop-start negotiations.</p>
<p>Figure 2 :
2
Figure 2: Bias( θ) = 0 does not guarantee a symmetric distribution of E[ θ] − θ.One tail could be long and thin, while the other is short and fat (shown in the right figure).We use distance skewness to measure the asymmetry of distribution.Therefore, using two meta-metrics as complimentary, we can measure the self-bias of LLM.</p>
<p>Figure 3 :
3
Figure3: Average Bias and Dskew estimations for Yor-En, Jav-En, Arm-En, and Ig-En translations on FLo-res200, with the x-axis showing self-refine steps, reveal that all LLMs exhibit self-bias, where open-source LLMs exhibit higher levels than GPT-4 and Gemini.</p>
<p>Figure 6 :
6
Figure6: Bias and distance skewness in generated texts from GPT-4, GPT-3.5-Turbo, and Gemini are measured on MATH testing set throughout the self-refinement steps.Results show an increase in bias and skewness of some math solutions during iterative self-refine.</p>
<p>Figure 7 :
7
Figure7: We measure the fluency and understandability aspects of GPT-4, GPT-3.5-Turbo, and Gemini's generated texts at Yor-En through self-refine steps.Despite no gains in quality, all LLMs have consistent performance improvements in fluency and understandability.</p>
<p>Figure 10 :
10
Figure 10: We show that bias and distance skewness estimation on LLaMA-2 7B, 13B, and 70B models at Yor-En translation across self-refinement steps.LLM with larger parameter size can have less self-bias.</p>
<p>Figure 11 :
11
Figure 11: Bias and distance skewness in generated texts from GPT-4, GPT-3.5-Turbo, and Gemini are measured on MATH testing set throughout the self-refinement steps.Results show an increase in bias and skewness during iterative self-consistency, causing biased ensembles in reasoning paths.</p>
<p>Figure 12 :
12
Figure12: Full Bias and Dskew estimations for Yor-En, Jav-En, Arm-En, and Ig-En translations on FLores200, with the x-axis showing self-refine steps, reveal that all LLMs exhibit self-bias, where open-source LLMs exhibit higher levels than GPT-4 and Gemini.</p>
<p>"' Translation: "'Ve Vídni se ve Vídni obnovily rozhovory o oživení jaderného paktu, přičemže obě partaje se snaží posoudit vyhlídky na úspěch po posledních výměnách v jednáních."'Annotate errors in the translation.MQM annotations: Source: "'O ko ago ilekun WiFi, O wi."' Translation: "'He locked the WiFi door, he said."'Annotate errors in the translation.MQM annotations: Output for Feedback: 'He locked the WiFi door' is a critical accuracy/mistranslation error</p>
<p>Table 1 :
1
We report human evaluation on GPT-4, GPT-3.5-Turbo and Gemini's quality assessment on 0th and 10th iteration of refinement generation at Yor-En.We used Bias and Dskew estimation to demonstrate bias found by human evaluation.All LLMs have significantly increased self-bias after 10 iterations.
TurboGemini</p>
<p>Table 2 :
2
We report human evaluation of format accuracy at six LLM's outputs.We observed that all LLMs have either perfect or nearly perfect format at first and fifth iteration of self-feedback at Yor-En translation.Mixtral stands for MixtralMOE and DeepS stands for DeepSeek-MoE that we used in the experiment.</p>
<p>Table 5 :
5
This table presents human evaluation resultsand GPT-4's quality scores for the 0th and 10th iteration of refinement generation performed at Yor-En.Bias and Dskew estimates are included to quantify the biases identified through human evaluation.
Human Evaluation Human GPT-3.5 Bias Dskew0th Iteration-22.2-2.6119.6 0.80310th Iteration-21.9-0.0321.9 0.885Table 6: We report human evaluation and GPT-3.5-Turbo's quality assessment on the 0th and 10th iterationof refinement generation at Yor-En.Human Evaluation Human Gemini Bias Dskew0th Iteration-17.3-8.929.62 0.35510th Iteration-18.3-0.7217.6 0.766Table 7: We report human evaluation and Gemini'squality assessment on the 0th and 10th iterations ofrefinement generation at Yor-En.</p>
<p>Table 9 :
9
Those are the feedback in context learning examples (three examples) that we used to prompt all LLMs across four language directions at Flores200.In this example, the source translation is Yourba text "O ko ago ilekun WiFi, O wi.".The English candidate text is "He locked the WiFi door" from GPT-4.GPT-4's self-feedback outputs "'He locked the WiFi door' is a critical accuracy/mistranslation error".</p>
<p>AcknowledgementsThis work was supported by the National Science Foundation award #2048122.L.L. is partly supported by a gift from Apple Inc.The views expressed are those of the author and do not reflect the official policy or position of the funding agencies.We thank Yuanjing Wei for conducting the human evaluation in our experiment.Source: "'大众点评乌鲁木齐家居商场频道为您提供高铁居然之家地址，电话，营业时间等最新商户信息，找 装修公司，就上大众点评"' Translation: "'Urumqi Home Furnishing Store Channel provides you with the latest bussiness information such as the address, telephone number, bussiness hours, etc., of high-speed rail, and find a decoration company, and go to the reviews."'Feedback: 'of high-speed rail' is a critical accuracy/addition error 'go to the reviews' is a major accuracy/mistranslation error 'etc.,' is a minor style/awkward error Improved Chinese-to-English translation: Dianping Urumqi Renovation and Design Channel will provide you with the address, phone number, operation time and other information of HSR Easyhome, and please come to Dianping if you are looking for a renovation company.Source: "'I do apologise about this, we must gain permission from the account holder to discuss an order with another person, I apologise if this was done previously, however, I would not be able to discuss this with yourself without the account holders permission."'Translation: "'Ich entschuldige mich dafür, wir müssen die Erlaubnis einholen, um eine Bestellung mit einer anderen Person zu besprechen.Ich entschuldige mich, falls dies zuvor geschehen wäre, aber ohne die Erlaubnis des Kontoinhabers wäre ich nicht in der Lage, dies mit dir involvement."'Feedback: 'involvement' is a major accuracy/mistranslation error 'the account holder' is a major accuracy/omission error 'wäre' is a minor fluency/grammar error 'dir' is a minor fluency/register error Improved English-to-German translation: Ich bitte um Entschuldigung, aber wir benötigen das Einverständnis des Kontoinhabers, um eine Bestellung mit einer anderen Person zu besprechen, falls es schon eingeholt wurde, entschuldige ich mich, aber ich kann dies ohne das Einverständnis des Kontoinhabers nicht mit Ihnen besprechen.Source: "'Talks have resumed in Vienna to try to revive the nuclear pact, with both sides trying to gauge the prospects of success after the latest exchanges in the stop-start negotiations."'Translation: "'Ve Vídni se ve Vídni obnovily rozhovory o oživení jaderného paktu, přičemže obě partaje se snaží posoudit vyhlídky na úspěch po posledních výměnách v jednáních."'Feedback: 've Vídni' is a major accuracy/addition error 'the stop-start' is a major accuracy/omission error 'partaje' is a minor terminology/inappropriate for context error Improved English-to-Cezch translation: Ve Vídni byly obnoveny rozhovory o oživení jaderného paktu a obě strany se snaží odhadnout, jaké jsou vyhlídky na úspěch po posledních výměnách názorů v rámci přerušených jednání.Table10: Those are the refinement in context learning examples (three examples) that we used to prompt all LLMs across four language directions at Flores200.In this example, the source translation is Yourba text "O ko ago ilekun WiFi, O wi.".The English text is GPT-4's output "He locked the WiFi door.".We gathered GPT-4's self-feedbak: "'He locked the WiFi door' is a critical accuracy/mistranslation error".GPT-4 outputs a refined output: "He secured the WiFi connection, he said.",according to the feedback and prior step generation.Prompt for LLM's initial generation for reasoning: You are a competitive math problem solver.Please generate a step-by-step solution.Your final answer must be enclosed in LaTeX's boxed tag.How many vertical asymptotes does the graph of y = 2 x 2 +x−6 have?Output for math answer: A vertical asymptote is a vertical line that the graph of a function approaches but never crosses.A function has a vertical asymptote at x = k if it has infinite limits as x approaches k from the left or right.For the rational function y = 2 x 2 +x−6 , we know that it could have vertical asymptotes wherever the denominator is zero, because division by zero is undefined and yields infinity.Therefore, to find the vertical asymptotes, we solve the equation x 2 + x − 6 = 0. We apply factorization and get (x − 2)(x + 3) = 0.This gives us x = 2 and x = −3.In any case, we need to check if any of these could be holes of vertical asymptotes.This would be the case if x = 2 or x = −3 also made the numerator zero.However, the numerator is a constant and won't be zero.Therefore this function has two vertical asymptotes at x = 2 and x = −3.The answer is 2 .Table11: This is the prompt we used to generate mathematical reasoning.The output in this example comes from GPT-4.We first used this prompt to generate one reasoning path as initial generation.Then, we sample 10 different reasoning paths and use majority vote to find the most confident answer.To generate self-feedback, we compare majority voted answer and initial answer.If they are matched, feedback indicates answer is correct and no change should be made.If they are not matched, the feedback indicates answer is wrong and inital answer will be replaced by the majority voted answer.We alternate above two steps and iteratively refine the reasoning paths.Concept-to-text prompt for LLM's initial generation: You are generating text based on specified words.Do not provide any explanations or text apart from the text output.Please generate a sentence that contains the exact string matches for the following concepts:['vest', 'snow', 'remove', 'silhouette', 'dunk', 'step', 'item', 'toddler', 'plaster', 'wall', 'piece', 'tool', 'circle', 'pull', 'wrap', 'headphone', 'deck', 'suit', 'press', 'mother', 'pit', 'cupcake', 'priest', 'rink', 'counter', 'rest', 'piano', 'wine', 'serve', 'winter'] Output for Concept-to-Text generation:In the winter, the toddler wearing a vest, pressed a piece of wet plaster against the wall in a circle, while his mother baking a cupcake was removing the snow from the deck with a tool; switching steps to the rhythm of a piano piece echoing from the headphones wrapped around her neck, she caught a glimpse of the priest serving wine, his silhouette dunking a cupcake into a pit at the rink counter, under the restful gaze of a suit-clad man.Table12: This is the concept-to-text prompt we used to test all LLMs at Commongen Hard.This example contains the output from GPT-4.Concept-to-text in context learning prompt for LLM's self-feedback:We want to create a sentence that contains all the specified concepts.Please provide feedback on the following sentences.The feedback should list all missing concepts.If all concepts are covered, output 'all covered' Concepts: ['dog', 'frisbee', 'catch', 'throw'] Generated Sentence: A dog leaps to catch a thrown frisbee.Feedback: all covered Concepts: ['dog', 'frisbee', 'throw'] Generated Sentence: Two dogs are throwing frisbees at each other .Feedback: ['catch']Concepts:['vest', 'snow', 'remove', 'silhouette', 'dunk', 'step', 'item', 'toddler', 'plaster', 'wall', 'piece', 'tool', 'circle', 'pull', 'wrap', 'headphone', 'deck', 'suit', 'press', 'mother', 'pit', 'cupcake', 'priest', 'rink', 'counter', 'rest', 'piano', 'wine', 'serve', 'winter'] Generated Sentence: In the winter, the toddler wearing a vest, pressed a piece of wet plaster against the wall in a circle, while his mother baking a cupcake was removing the snow from the deck with a tool; switching steps to the rhythm of a piano piece echoing from the headphones wrapped around her neck, she caught a glimpse of the priest serving wine, his silhouette dunking a cupcake into a pit at the rink counter, under the restful gaze of a suit-clad man.Feedback:Output for Concept-to-Text feedback: all covered Table13: This is the in-context learing (ICL) concept-to-text prompt (two ICL examples) we used to generate LLM's self-feedback at Commongen Hard.This example contains the self-feedback from GPT-4.Concept-to-text prompt for LLM's refinement: Concepts: ['fruit', 'motorcycle', 'perform', 'jacket', 'vehicle', 'place', 'mat', 'walk', 'world', 'area', 'kiss', 'mother', 'pass', 'report', 'club', 'axis', 'tricep', 'patient', 'listen', 'owner', 'uniform', 'floor', 'hamburger', 'use', 'wine', 'cross', 'bull', 'sell', 'lawn', 'friend']Generated sentence: In a world where a fruit can perform like a motorcycle, I found my mother wearing a leather jacket, passing by her vehicle to walk towards the place where she runs a club.Not far from that area, my friend, a patient person, was ready to sell mats, used for gym workouts, especially for triceps.Being the owner of the shop, he always wears a uniform.While I was listening to this report, sitting cross-legged on the floor and munching my hamburger, I missed seeing a bull wandering on the lawn just outside.Just then, I received an unexpected kiss from my wife who brought me a glass of wine from our cellar which also acts as an axis for all the happenings of my life.Missing Concepts: ['use', 'lawn'] Please revise generated sentence which covers all missing concepts and all given concepts, with the exact string matches.New sentence:Output for Concept-to-Text refinement: In a world where a fruit can perform like a motorcycle, I found my mother wearing a leather jacket, passing by her vehicle to walk towards the place where she runs a club.Not far from that area, my friend, a patient person, was ready to sell mats, used for gym workouts, especially for triceps.Being the owner of the shop, he always wears a uniform.While I was listening to this report, sitting cross-legged on the floor and munching my hamburger, I missed seeing a bull wandering on the lawn just outside.Just then, I received an unexpected kiss from my wife who brought me a glass of wine from our cellar which also acts as an axis for all the happenings of my life.Table14: This is the concept-to-text refinement prompt we used to generate LLM's refinement at Commongen Hard.This example contains the refinement from GPT-4.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Bias correction of gcm precipitation by quantile mapping: How well do methods preserve changes in quantiles and extremes. Alex J Cannon, Stephen R Sobie, Trevor Q , 10.1175/JCLI-D-14-00754.1Journal of Climate. 28172015Murdock</p>
<p>Speak, memory: An archaeology of books known to ChatGPT/GPT-4. Kent Chang, Mackenzie Cramer, Sandeep Soni, David Bamman, 10.18653/v1/2023.emnlp-main.453Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2023Singapore</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, The Twelfth International Conference on Learning Representations. 2024</p>
<p>James Marta R Costa-Jussà, Onur Cross, Maha Çelebi, Kenneth Elbayad, Kevin Heafield, Elahe Heffernan, Janice Kalbassi, Daniel Lam, Jean Licht, Maillard, arXiv:2207.04672No language left behind: Scaling human-centered machine translation. 2022arXiv preprint</p>
<p>Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. Daniel Deutsch, Rotem Dror, and Dan Roth. Damai Dai, Chengqi Deng, Chenggang Zhao, R X Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y Wu, Zhenda Xie, Y K Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, Wenfeng Liang, 10.18653/v1/2022.emnlp-main.753Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2024. 2022On the limitations of reference-free evaluations of generated text</p>
<p>Experts, errors, and context: A large-scale study of human evaluation for machine translation. Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, Wolfgang Macherey, 10.1162/tacl_a_00437Transactions of the Association for Computational Linguistics. Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and André F. T. Martins92021</p>
<p>Results of WMT22 metrics shared task: Stop using BLEU -neural metrics are better and more robust. Proceedings of the Seventh Conference on Machine Translation (WMT). the Seventh Conference on Machine Translation (WMT)Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics</p>
<p>Gptscore: Evaluate as you desire. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu ; Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, Nando De Freitas, 2023. 2023Reinforced self-training (rest) for language modeling</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021NeurIPS</p>
<p>A closer look at the selfverification abilities of large language models in logical reasoning. Ruixin Hong, Hongming Zhang, Xinyu Pang, Dong Yu, Changshui Zhang, ; Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, 10.18653/v1/2023.emnlp-main.67CoRR, abs/2311.07954Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023. 2023aLarge language models can self-improve</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, 10.48550/arXiv.2310.01798CoRR, abs/2310.017982023b</p>
<p>Critiquellm: Scaling llm-as-critic for effective and explainable evaluation of large language model generation. Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Renard Lélio, Lucile Lavaud, Marie-Anne Saulnier, Pierre Lachaux, Sandeep Stock, Sophia Subramanian, Szymon Yang, Teven Antoniak, Théophile Le Scao, Thibaut Gervet, Thomas Lavril, Timothée Wang, William El Lacroix, Pei Sayed, Bosi Ke, Zhuoer Wen, Xiao Feng, Xuanyu Liu, Jiale Lei, Shengyuan Cheng, Aohan Wang, Yuxiao Zeng, Hongning Dong, Jie Wang, Minlie Tang, Huang, 10.18653/v1/2023.wmt-1.1CoRR, abs/2311.18702Makoto Nagata, Toshiaki Nakazawa, Martin Popel, Maja Popović, and Mariya Shmatova. Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Philipp Koehn, Benjamin Marie, Christof Monz, Makoto Morishita, Kenton Murray, Anton Dvorkovich, Christian Federmann, Mark Fishel; SingaporeAssociation for Computational Linguistics2024. 2023. 2023Proceedings of the Eighth Conference on Machine Translation</p>
<p>GEMBA-MQM: Detecting translation quality error spans with GPT-4. Tom Kocmi, Christian Federmann, 10.18653/v1/2023.wmt-1.64Proceedings of the Eighth Conference on Machine Translation. the Eighth Conference on Machine TranslationSingaporeAssociation for Computational Linguistics2023</p>
<p>CommonGen: A constrained text generation challenge for generative commonsense reasoning. Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae , Myung Kim, Dongyeop Kang ; Sneha, Isaac Rayburn Kudugunta, Biao Caswell, Xavier Zhang, Derrick Garcia, Aditya Xin, Romi Kusupati, Ankur Stella, Orhan Bapna, Firat, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Wangchunshu Bill Yuchen Lin, Ming Zhou, Pei Shen, Chandra Zhou, Yejin Bhagavatula, Xiang Choi, Ren, Online. Association for Computational Linguistics2023. 2023. 2020Findings of the Association for Computational Linguistics: EMNLP 2020</p>
<p>G-eval: NLG evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, 10.18653/v1/2023.emnlp-main.153Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023a</p>
<p>Llms as narcissistic evaluators: When ego inflates evaluation scores. Yiqi Liu, Nafise Sadat Moosavi, Chenghua Lin, 2023b</p>
<p>SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, 10.18653/v1/2023.emnlp-main.557Advances in Neural Information Processing Systems, 36. Potsawee Manakul, Adian Liusie, and Mark Gales. Association for Computational Linguistics2024. 2023Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Singapore</p>
<p>Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, William Yang, Wang , 10.48550/arXiv.2308.03188CoRR, abs/2308.031882023</p>
<p>Learning compact metrics for mt. Amy Pu, Won Hyung, Ankur P Chung, Sebastian Parikh, Thibault Gehrmann, Sellam, Proceedings of EMNLP. EMNLP2021</p>
<p>. Thibault Sellam, Dipanjan Das, Ankur P Parikh, </p>
<p>Bleurt: Learning robust metrics for text generation. Proceedings of ACL. ACL</p>
<p>GPT-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems. CoRR, abs/2310.12397. Gabor Szekely and Tamás Móri. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, 10.1081/STA-100105689Advances in Neural Information Processing Systems. Kaya Stechly, Matthew Marquez, Subbarao Kambhampati, 2024. 2023. 200636A characteristic measure of asymmetry and its application for testing diagonal symmetry</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.11805arXiv:2307.092882023. Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, 2023arXiv preprint</p>
<p>Gladys Tyen, Hassan Mansoor, Peter Chen, Tony Mak, Victor Carbune, 10.48550/arXiv.2311.08516abs/2311.08516Llms cannot find reasoning errors, but can correct them! CoRR. 2023</p>
<p>Can large language models really improve by self-critiquing their own plans?. Karthik Valmeekam, Matthew Marquez, Subbarao Kambhampati, 10.48550/arXiv.2310.08118CoRR, abs/2310.081182023</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Llmrefine: Pinpointing and refining large language models via finegrained actionable feedback. Wenda Xu, Daniel Deutsch, Mara Finkelstein, Juraj Juraska, Biao Zhang, Zhongtao Liu, William Yang Wang, Lei Li, Markus Freitag ; Zhenqiao Song, Markus Freitag, William Wang, Lei Li, 10.18653/v1/2023.emnlp-main.365Proceedings of 2024 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL) -Findings. Wenda Xu, Danqing Wang, Liangming Pan. 2024 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL) -Findings. Wenda Xu, Danqing Wang, Liangming PanAssociation for Computational Linguistics2024. 2023Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Singapore</p>
<p>Judging LLM-as-ajudge with MT-bench and chatbot arena. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar ; Lianmin, Wei-Lin Zheng, Ying Chiang, Siyuan Sheng, Zhanghao Zhuang, Yonghao Wu, Zi Zhuang, Zhuohan Lin, Dacheng Li, Eric Li, Hao Xing, Joseph E Zhang, Ion Gonzalez, Stoica, Thirtyseventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024. 2023Self-rewarding language models</p>
<p>Towards a unified multi-dimensional evaluator for text generation. Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Ji Heng, Jiawei Han, 10.18653/v1/2022.emnlp-main.131Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>In-context-learning prompt for LLM's refinement at translation: Please fix all errors. You can rewrite translation if translation is bad. </p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7333 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7333</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7333</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-860ba78f9789bbfc99c299b18558ca19430d8fea</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/860ba78f9789bbfc99c299b18558ca19430d8fea" target="_blank">LLM-SR: Scientific Equation Discovery via Programming with Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> LLM-SR is introduced, a novel approach that leverages the extensive scientific knowledge and robust code generation capabilities of Large Language Models to discover scientific equations from data that significantly outperform state-of-the-art symbolic regression baselines, particularly in out-of-domain test settings.</p>
                <p><strong>Paper Abstract:</strong> Mathematical equations have been unreasonably effective in describing complex natural phenomena across various scientific disciplines. However, discovering such insightful equations from data presents significant challenges due to the necessity of navigating extremely large combinatorial hypothesis spaces. Current methods of equation discovery, commonly known as symbolic regression techniques, largely focus on extracting equations from data alone, often neglecting the domain-specific prior knowledge that scientists typically depend on. They also employ limited representations such as expression trees, constraining the search space and expressiveness of equations. To bridge this gap, we introduce LLM-SR, a novel approach that leverages the extensive scientific knowledge and robust code generation capabilities of Large Language Models (LLMs) to discover scientific equations from data. Specifically, LLM-SR treats equations as programs with mathematical operators and combines LLMs' scientific priors with evolutionary search over equation programs. The LLM iteratively proposes new equation skeleton hypotheses, drawing from its domain knowledge, which are then optimized against data to estimate parameters. We evaluate LLM-SR on four benchmark problems across diverse scientific domains (e.g., physics, biology), which we carefully designed to simulate the discovery process and prevent LLM recitation. Our results demonstrate that LLM-SR discovers physically accurate equations that significantly outperform state-of-the-art symbolic regression baselines, particularly in out-of-domain test settings. We also show that LLM-SR's incorporation of scientific priors enables more efficient equation space exploration than the baselines. Code and data are available: https://github.com/deep-symbolic-mathematics/LLM-SR</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7333.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7333.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-SR (Nonlinear Oscillators)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-SR applied to custom nonlinear oscillator equation discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM-SR uses large language models to generate executable equation-program skeletons (Python functions with parameter placeholders) for discovering governing differential equations of nonlinear oscillators, followed by numeric optimization of parameters and iterative in-context refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo; Mixtral-8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>GPT-3.5-turbo (instruction-tuned via API) and Mixtral-8x7B (Mixture-of-Experts backbone); used as few-shot/code-generating LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Physics (nonlinear dynamical systems / oscillators)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based/programmatic simulation task: generate symbolic differential-equation program skeletons (mapping inputs like time, position, velocity to derivatives) to model nonlinear damped oscillator dynamics, then fit numeric parameters to data.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Few-shot in-context prompting using dynamic experience buffer (k=2 in-context examples sampled from an islands model), structured prompt containing: instruction, problem specification (natural language describing variables and physics), evaluation/optimization function, and an initial equation skeleton; stochastic sampling from the LLM with temperature=0.8, batch size b=4; iterative refinement loop where high-scoring skeletons are fed back as few-shot examples.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Normalized Mean Squared Error (NMSE) computed from predicted vs true trajectories; internal score used during search is negative MSE.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Mixtral-8x7B: Oscillation 1 NMSE ID = 7.89e-08, OOD = 0.0002; Oscillation 2 NMSE ID = 0.0030, OOD = 0.0291. GPT-3.5-turbo: Oscillation 1 NMSE ID = 4.65e-07, OOD = 0.0005; Oscillation 2 NMSE ID = 2.12e-07, OOD = 3.81e-05.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Best baselines: uDSR (Oscillation 1 NMSE ID = 0.0003, OOD = 0.0007), PySR (Oscillation 2 NMSE ID = 0.0002, OOD = 0.0098); other SR baselines (GPlearn, NeSymReS, E2E, DSR) show substantially higher NMSEs (see Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Inclusion of natural-language problem specification (domain prior)', 'Program representation (equations as executable Python programs vs single-line expressions)', 'Iterative refinement with data-driven feedback and experience buffer (few-shot examples)', 'Two-stage approach: skeleton generation + numeric parameter optimization (numpy+BFGS or torch+Adam)', 'LLM backbone differences (Mixtral vs GPT-3.5)', 'Sampling temperature (exploration vs exploitation)', 'Maximum parameter vector length (regularization by limiting params)', 'Timeouts/execution failures (programs exceeding 30s discarded)', 'Potential memorization/recitation on canonical benchmarks (necessitating custom benchmarks)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Prompt temperature τ=0.8; b=4 samples per prompt; k=2 in-context examples sampled from experience buffer; max params length = 10; evaluators run in parallel (e=4); evaluation time limit = 30s and memory limit = 2GB per program; islands model m=10 with periodic resets (every ~4 hrs); ~2.5K LLM iterations per experiment; parameter optimization via numpy+BFGS (preferred) or torch+Adam with 30s timeout.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Performance depends on LLM training data and its domain priors; risk of memorization/recitation on canonical benchmarks (observed with Feynman dataset); computational cost from iterative LLM calls and optimizations; some generated programs may fail to execute or be discarded due to time/memory constraints; optimizer choice and LLM's code-generation preference (numpy vs PyTorch) affect results; two-stage approach required — end-to-end numeric generation (LLM emitting final numeric coefficients) performed worse.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-SR: Scientific Equation Discovery via Programming with Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7333.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7333.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-SR (E. coli growth)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-SR applied to E. coli growth-rate equation discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM-SR uses LLMs to generate programmatic equation skeletons for a multiplicative growth-rate model (factors for population, substrate, temperature, pH), then fits numeric parameters to synthetic biological data and evaluates generalization in-domain and out-of-domain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo; Mixtral-8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>GPT-3.5-turbo (instruction-tuned via API) and Mixtral-8x7B (Mixture-of-Experts backbone); used as few-shot/code-generating LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biology (microbial growth modeling / microbiology)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Discover a differential equation for E. coli growth rate dB/dt as a multiplicative function of B, substrate S, temperature T, and pH, by generating symbolic program skeletons and optimizing numeric parameters to fit simulated data.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Few-shot in-context prompting with experience buffer examples (k=2), problem specification including descriptions of variables and expected multiplicative structure; prompts include evaluation and optimization functions; LLM instructed to output executable equation programs with params placeholders; sampling temperature τ=0.8, b=4 per prompt; iterative refinement guided by data-driven scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Normalized Mean Squared Error (NMSE) between predicted and true growth rates (and used negative MSE as score during search).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Mixtral-8x7B: E. coli NMSE ID = 0.0026, OOD = 0.0037. GPT-3.5-turbo: E. coli NMSE ID = 0.0214, OOD = 0.0264.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Best baseline: PySR NMSE ID = 0.0376, OOD = 1.0141 (other baselines: uDSR ID=0.3322, OOD=5.4584; DSR/E2E/NeSymReS worse or not applicable). LLM-SR substantially outperforms baselines, particularly in OOD generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Natural-language problem description (domain prior) improves performance', 'Program representation enabling differentiable optimization', 'Iterative refinement and experience-management (in-context examples)', 'Two-stage skeleton + optimizer approach', 'LLM backbone selection (Mixtral demonstrated stronger results here than GPT-3.5)', 'Model pretraining coverage (memorization risk lower for custom biology benchmark)', 'Choice of optimizer (numpy+BFGS vs torch+Adam)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Temperature τ=0.8; b=4 samples/prompt; k=2 in-context examples; max params length = 10; up to ~2.5K LLM iterations; 30s timeout and 2GB memory per program evaluation; numeric optimization via numpy+BFGS or torch+Adam.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Transformer SR pre-trained models may not generalize to novel biological formulations; LLM performance tied to training data breadth — potential biases or gaps in biological subdomains; generation quality affected by LLM code-generation preferences; end-to-end numeric generation yields worse results than skeleton+optimizer; computational cost of many LLM queries and per-hypothesis optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-SR: Scientific Equation Discovery via Programming with Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7333.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7333.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-SR (Stress-Strain)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-SR applied to material stress-strain behavior discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM-SR leverages LLMs to propose equation programs modeling stress-strain-temperature relations from experimental tensile-test data (Aluminum 6061-T651), using program skeletons plus parameter optimization and iterative few-shot refinement to obtain empirically accurate predictive equations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo; Mixtral-8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>GPT-3.5-turbo (instruction-tuned via API) and Mixtral-8x7B (Mixture-of-Experts backbone); used as few-shot/code-generating LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science / mechanical engineering (stress-strain modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based/programmatic discovery of empirical equations mapping inputs (strain, temperature, material factors) to stress, using experimental tensile-test data; generate program skeletons as candidate constitutive relations and fit parameters to observed data.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Structured few-shot prompts containing problem spec, variable descriptions, evaluation function, and an initial simple linear equation skeleton; in-context demonstrations drawn from experience buffer (k=2), iterative LLM sampling with τ=0.8 and b=4; generated programs executed and parameters optimized with numpy+BFGS or torch+Adam.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Normalized Mean Squared Error (NMSE) between predicted and measured stress; negative MSE used as search score.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Mixtral-8x7B: Stress-Strain NMSE ID = 0.0162, OOD = 0.0946. GPT-3.5-turbo: Stress-Strain NMSE ID = 0.0210, OOD = 0.0516.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Best baseline: PySR NMSE ID = 0.0331, OOD = 0.1304; uDSR ID = 0.0502, OOD = 0.1761. LLM-SR outperforms these baselines in ID and OOD metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>["Lack of pre-existing theoretical model (experimental data) increases reliance on LLM's empirical priors", 'Natural-language problem descriptions aid hypothesis generation', 'Program representation and parameter optimization critical for fitting experimental noise', 'LLM backbone choice affects quality (Mixtral vs GPT-3.5 differences observed)', 'Optimizer choice (numpy+BFGS slightly better in experiments, possibly due to LLM generating more numpy-style code)', 'Experience buffer sampling (islands, Boltzmann cluster selection) influences diversity and convergence']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>b=4 skeletons per prompt, τ=0.8, k=2 in-context examples, max params length = 10, evaluation timeout 30s, memory cap 2GB, ~2.5K LLM iterations, islands m=10 with periodic reset, parallel evaluation with e=4, parameter optimization via numpy+BFGS or torch+Adam.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Empirical modeling may still be sensitive to noise and to the range of training data (OOD NMSE can be higher); computationally expensive; possible failure modes include non-executable generated code, timeout-driven discards, and reduced performance when LLMs are less proficient at generating the preferred optimizer framework (numpy vs PyTorch).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-SR: Scientific Equation Discovery via Programming with Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mathematical discoveries from program search with large language models <em>(Rating: 2)</em></li>
                <li>Large language models as optimizers <em>(Rating: 2)</em></li>
                <li>Language model crossover: Variation through few-shot prompting <em>(Rating: 2)</em></li>
                <li>Llm and simulation as bilevel optimizers: A new paradigm to advance physical scientific discovery <em>(Rating: 2)</em></li>
                <li>Data-driven discovery with large generative models <em>(Rating: 2)</em></li>
                <li>Automated statistical model discovery with language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7333",
    "paper_id": "paper-860ba78f9789bbfc99c299b18558ca19430d8fea",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "LLM-SR (Nonlinear Oscillators)",
            "name_full": "LLM-SR applied to custom nonlinear oscillator equation discovery",
            "brief_description": "LLM-SR uses large language models to generate executable equation-program skeletons (Python functions with parameter placeholders) for discovering governing differential equations of nonlinear oscillators, followed by numeric optimization of parameters and iterative in-context refinement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo; Mixtral-8x7B",
            "model_size": null,
            "model_type": "GPT-3.5-turbo (instruction-tuned via API) and Mixtral-8x7B (Mixture-of-Experts backbone); used as few-shot/code-generating LLMs",
            "scientific_domain": "Physics (nonlinear dynamical systems / oscillators)",
            "simulation_task_description": "Text-based/programmatic simulation task: generate symbolic differential-equation program skeletons (mapping inputs like time, position, velocity to derivatives) to model nonlinear damped oscillator dynamics, then fit numeric parameters to data.",
            "prompting_strategy": "Few-shot in-context prompting using dynamic experience buffer (k=2 in-context examples sampled from an islands model), structured prompt containing: instruction, problem specification (natural language describing variables and physics), evaluation/optimization function, and an initial equation skeleton; stochastic sampling from the LLM with temperature=0.8, batch size b=4; iterative refinement loop where high-scoring skeletons are fed back as few-shot examples.",
            "evaluation_metric": "Normalized Mean Squared Error (NMSE) computed from predicted vs true trajectories; internal score used during search is negative MSE.",
            "reported_accuracy": "Mixtral-8x7B: Oscillation 1 NMSE ID = 7.89e-08, OOD = 0.0002; Oscillation 2 NMSE ID = 0.0030, OOD = 0.0291. GPT-3.5-turbo: Oscillation 1 NMSE ID = 4.65e-07, OOD = 0.0005; Oscillation 2 NMSE ID = 2.12e-07, OOD = 3.81e-05.",
            "baseline_accuracy": "Best baselines: uDSR (Oscillation 1 NMSE ID = 0.0003, OOD = 0.0007), PySR (Oscillation 2 NMSE ID = 0.0002, OOD = 0.0098); other SR baselines (GPlearn, NeSymReS, E2E, DSR) show substantially higher NMSEs (see Table 1).",
            "factors_reported": [
                "Inclusion of natural-language problem specification (domain prior)",
                "Program representation (equations as executable Python programs vs single-line expressions)",
                "Iterative refinement with data-driven feedback and experience buffer (few-shot examples)",
                "Two-stage approach: skeleton generation + numeric parameter optimization (numpy+BFGS or torch+Adam)",
                "LLM backbone differences (Mixtral vs GPT-3.5)",
                "Sampling temperature (exploration vs exploitation)",
                "Maximum parameter vector length (regularization by limiting params)",
                "Timeouts/execution failures (programs exceeding 30s discarded)",
                "Potential memorization/recitation on canonical benchmarks (necessitating custom benchmarks)"
            ],
            "experimental_conditions": "Prompt temperature τ=0.8; b=4 samples per prompt; k=2 in-context examples sampled from experience buffer; max params length = 10; evaluators run in parallel (e=4); evaluation time limit = 30s and memory limit = 2GB per program; islands model m=10 with periodic resets (every ~4 hrs); ~2.5K LLM iterations per experiment; parameter optimization via numpy+BFGS (preferred) or torch+Adam with 30s timeout.",
            "limitations_or_failure_modes": "Performance depends on LLM training data and its domain priors; risk of memorization/recitation on canonical benchmarks (observed with Feynman dataset); computational cost from iterative LLM calls and optimizations; some generated programs may fail to execute or be discarded due to time/memory constraints; optimizer choice and LLM's code-generation preference (numpy vs PyTorch) affect results; two-stage approach required — end-to-end numeric generation (LLM emitting final numeric coefficients) performed worse.",
            "uuid": "e7333.0",
            "source_info": {
                "paper_title": "LLM-SR: Scientific Equation Discovery via Programming with Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLM-SR (E. coli growth)",
            "name_full": "LLM-SR applied to E. coli growth-rate equation discovery",
            "brief_description": "LLM-SR uses LLMs to generate programmatic equation skeletons for a multiplicative growth-rate model (factors for population, substrate, temperature, pH), then fits numeric parameters to synthetic biological data and evaluates generalization in-domain and out-of-domain.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo; Mixtral-8x7B",
            "model_size": null,
            "model_type": "GPT-3.5-turbo (instruction-tuned via API) and Mixtral-8x7B (Mixture-of-Experts backbone); used as few-shot/code-generating LLMs",
            "scientific_domain": "Biology (microbial growth modeling / microbiology)",
            "simulation_task_description": "Discover a differential equation for E. coli growth rate dB/dt as a multiplicative function of B, substrate S, temperature T, and pH, by generating symbolic program skeletons and optimizing numeric parameters to fit simulated data.",
            "prompting_strategy": "Few-shot in-context prompting with experience buffer examples (k=2), problem specification including descriptions of variables and expected multiplicative structure; prompts include evaluation and optimization functions; LLM instructed to output executable equation programs with params placeholders; sampling temperature τ=0.8, b=4 per prompt; iterative refinement guided by data-driven scoring.",
            "evaluation_metric": "Normalized Mean Squared Error (NMSE) between predicted and true growth rates (and used negative MSE as score during search).",
            "reported_accuracy": "Mixtral-8x7B: E. coli NMSE ID = 0.0026, OOD = 0.0037. GPT-3.5-turbo: E. coli NMSE ID = 0.0214, OOD = 0.0264.",
            "baseline_accuracy": "Best baseline: PySR NMSE ID = 0.0376, OOD = 1.0141 (other baselines: uDSR ID=0.3322, OOD=5.4584; DSR/E2E/NeSymReS worse or not applicable). LLM-SR substantially outperforms baselines, particularly in OOD generalization.",
            "factors_reported": [
                "Natural-language problem description (domain prior) improves performance",
                "Program representation enabling differentiable optimization",
                "Iterative refinement and experience-management (in-context examples)",
                "Two-stage skeleton + optimizer approach",
                "LLM backbone selection (Mixtral demonstrated stronger results here than GPT-3.5)",
                "Model pretraining coverage (memorization risk lower for custom biology benchmark)",
                "Choice of optimizer (numpy+BFGS vs torch+Adam)"
            ],
            "experimental_conditions": "Temperature τ=0.8; b=4 samples/prompt; k=2 in-context examples; max params length = 10; up to ~2.5K LLM iterations; 30s timeout and 2GB memory per program evaluation; numeric optimization via numpy+BFGS or torch+Adam.",
            "limitations_or_failure_modes": "Transformer SR pre-trained models may not generalize to novel biological formulations; LLM performance tied to training data breadth — potential biases or gaps in biological subdomains; generation quality affected by LLM code-generation preferences; end-to-end numeric generation yields worse results than skeleton+optimizer; computational cost of many LLM queries and per-hypothesis optimization.",
            "uuid": "e7333.1",
            "source_info": {
                "paper_title": "LLM-SR: Scientific Equation Discovery via Programming with Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLM-SR (Stress-Strain)",
            "name_full": "LLM-SR applied to material stress-strain behavior discovery",
            "brief_description": "LLM-SR leverages LLMs to propose equation programs modeling stress-strain-temperature relations from experimental tensile-test data (Aluminum 6061-T651), using program skeletons plus parameter optimization and iterative few-shot refinement to obtain empirically accurate predictive equations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo; Mixtral-8x7B",
            "model_size": null,
            "model_type": "GPT-3.5-turbo (instruction-tuned via API) and Mixtral-8x7B (Mixture-of-Experts backbone); used as few-shot/code-generating LLMs",
            "scientific_domain": "Materials science / mechanical engineering (stress-strain modeling)",
            "simulation_task_description": "Text-based/programmatic discovery of empirical equations mapping inputs (strain, temperature, material factors) to stress, using experimental tensile-test data; generate program skeletons as candidate constitutive relations and fit parameters to observed data.",
            "prompting_strategy": "Structured few-shot prompts containing problem spec, variable descriptions, evaluation function, and an initial simple linear equation skeleton; in-context demonstrations drawn from experience buffer (k=2), iterative LLM sampling with τ=0.8 and b=4; generated programs executed and parameters optimized with numpy+BFGS or torch+Adam.",
            "evaluation_metric": "Normalized Mean Squared Error (NMSE) between predicted and measured stress; negative MSE used as search score.",
            "reported_accuracy": "Mixtral-8x7B: Stress-Strain NMSE ID = 0.0162, OOD = 0.0946. GPT-3.5-turbo: Stress-Strain NMSE ID = 0.0210, OOD = 0.0516.",
            "baseline_accuracy": "Best baseline: PySR NMSE ID = 0.0331, OOD = 0.1304; uDSR ID = 0.0502, OOD = 0.1761. LLM-SR outperforms these baselines in ID and OOD metrics.",
            "factors_reported": [
                "Lack of pre-existing theoretical model (experimental data) increases reliance on LLM's empirical priors",
                "Natural-language problem descriptions aid hypothesis generation",
                "Program representation and parameter optimization critical for fitting experimental noise",
                "LLM backbone choice affects quality (Mixtral vs GPT-3.5 differences observed)",
                "Optimizer choice (numpy+BFGS slightly better in experiments, possibly due to LLM generating more numpy-style code)",
                "Experience buffer sampling (islands, Boltzmann cluster selection) influences diversity and convergence"
            ],
            "experimental_conditions": "b=4 skeletons per prompt, τ=0.8, k=2 in-context examples, max params length = 10, evaluation timeout 30s, memory cap 2GB, ~2.5K LLM iterations, islands m=10 with periodic reset, parallel evaluation with e=4, parameter optimization via numpy+BFGS or torch+Adam.",
            "limitations_or_failure_modes": "Empirical modeling may still be sensitive to noise and to the range of training data (OOD NMSE can be higher); computationally expensive; possible failure modes include non-executable generated code, timeout-driven discards, and reduced performance when LLMs are less proficient at generating the preferred optimizer framework (numpy vs PyTorch).",
            "uuid": "e7333.2",
            "source_info": {
                "paper_title": "LLM-SR: Scientific Equation Discovery via Programming with Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mathematical discoveries from program search with large language models",
            "rating": 2,
            "sanitized_title": "mathematical_discoveries_from_program_search_with_large_language_models"
        },
        {
            "paper_title": "Large language models as optimizers",
            "rating": 2,
            "sanitized_title": "large_language_models_as_optimizers"
        },
        {
            "paper_title": "Language model crossover: Variation through few-shot prompting",
            "rating": 2,
            "sanitized_title": "language_model_crossover_variation_through_fewshot_prompting"
        },
        {
            "paper_title": "Llm and simulation as bilevel optimizers: A new paradigm to advance physical scientific discovery",
            "rating": 2,
            "sanitized_title": "llm_and_simulation_as_bilevel_optimizers_a_new_paradigm_to_advance_physical_scientific_discovery"
        },
        {
            "paper_title": "Data-driven discovery with large generative models",
            "rating": 2,
            "sanitized_title": "datadriven_discovery_with_large_generative_models"
        },
        {
            "paper_title": "Automated statistical model discovery with language models",
            "rating": 1,
            "sanitized_title": "automated_statistical_model_discovery_with_language_models"
        }
    ],
    "cost": 0.015803499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LLM-SR: SCIENTIFIC EQUATION DISCOVERY VIA Programming With Large Language Models</h1>
<p>Parshin Shojaee ${ }^{1 <em>}$ Kazem Meidani ${ }^{2^{</em>}}$ Shashank Gupta ${ }^{3}$<br>Amir Barati Farimani ${ }^{2}$ Chandan K. Reddy ${ }^{1}$<br>${ }^{1}$ Virginia Tech ${ }^{2}$ Carnegie Mellon University ${ }^{3}$ Allen Institute for AI</p>
<h4>Abstract</h4>
<p>Mathematical equations have been unreasonably effective in describing complex natural phenomena across various scientific disciplines. However, discovering such insightful equations from data presents significant challenges due to the necessity of navigating extremely large combinatorial hypothesis spaces. Current methods of equation discovery, commonly known as symbolic regression techniques, largely focus on extracting equations from data alone, often neglecting the domain-specific prior knowledge that scientists typically depend on. They also employ limited representations such as expression trees, constraining the search space and expressiveness of equations. To bridge this gap, we introduce LLM-SR, a novel approach that leverages the extensive scientific knowledge and robust code generation capabilities of Large Language Models (LLMs) to discover scientific equations from data. Specifically, LLM-SR treats equations as programs with mathematical operators and combines LLMs' scientific priors with evolutionary search over equation programs. The LLM iteratively proposes new equation skeleton hypotheses, drawing from its domain knowledge, which are then optimized against data to estimate parameters. We evaluate LLM-SR on four benchmark problems across diverse scientific domains (e.g., physics, biology), which we carefully designed to simulate the discovery process and prevent LLM recitation. Our results demonstrate that LLM-SR discovers physically accurate equations that significantly outperform state-of-the-art symbolic regression baselines, particularly in out-of-domain test settings. We also show that LLM-SR's incorporation of scientific priors enables more efficient equation space exploration than the baselines ${ }^{1}$.</p>
<h2>1 INTRODUCTION</h2>
<p>The emergence of Large Language Models (LLMs) has marked a significant milestone in artificial intelligence, showcasing remarkable capabilities across various domains (Achiam et al., 2023). As LLMs continue to evolve, researchers are exploring innovative ways to harness their potential for solving complex problems such as scientific discovery (Wang et al., 2023a; AI4Science \&amp; Quantum, 2023). Their ability to process and comprehend vast amounts of scientific literature, extract relevant information, and generate coherent hypotheses has recently opened up new avenues for accelerating scientific progress (Zheng et al., 2023b; Ji et al., 2024). Additionally, by leveraging their ability to understand and reason with the help of programming and execution, LLMs have shown the potential to enhance automated reasoning and problem-solving capabilities for general natural language and mathematics optimization tasks, e.g., prompt optimization and heuristic discovery (Meyerson et al., 2023; Yang et al., 2023a; Madaan et al., 2024; Romera-Paredes et al., 2024). Motivated by these strengths, LLMs could be particularly helpful for the task of equation discovery, a fundamental task in science and scientific discovery.</p>
<p>Discovering accurate symbolic mathematical models from data is an important task in various scientific and engineering disciplines. The task of data-driven equation discovery (also commonly known as Symbolic Regression (SR)), aims to find abstract mathematical equations from data</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The LLM-SR framework, consisting of three main steps: (a) Hypothesis Generation, where LLM generates equation program skeletons based on a structured prompt; (b) Data-driven Evaluation, which optimizes the parameters of each equation skeleton hypothesis and assesses its fit to the data; and (c) Experience Management, which maintains a diverse buffer of high-scoring hypotheses to provide informative in-context examples into LLM's prompt for effective iterative refinement.
observations such that these equations are predictive of the underlying data, are interpretable, and generalize to unseen data from the same physical phenomena. Finding such equations offers several advantages over simply estimating a predictive model, as the resulting mathematical functions provide insights into the underlying physical processes, enable extrapolation beyond the observed data, and facilitate knowledge transfer across related problems (Langley, 1981; Schmidt \&amp; Lipson, 2009). However, while evaluating the fit of a proposed equation is relatively straightforward, the inverse process of obtaining these mathematical equations from data is a challenging problem, known to be NP-hard (Virgolin \&amp; Pissis, 2022). Current equation discovery methods encompass a wide variety of approaches from evolutionary search algorithms (Cranmer, 2023; Mundhenk et al., 2021; La Cava et al., 2021) to advanced deep learning methods using Transformers (Biggio et al., 2021; Kamienny et al., 2022). Most of the traditional symbolic regression techniques are built on top of Genetic Programming (GP) (Koza, 1994) evolutionary methods, representing mathematical equations as expression trees and searching the combinatorial space of possible equations through iterative mutation and recombination. However, these methods often struggle with the complexity of the vast optimization space and do not incorporate prior scientific knowledge, which leads to suboptimal solutions and inefficient exploration of the equation search space. Similarly, the design of current general LLM-based optimization frameworks (Meyerson et al., 2023; Romera-Paredes et al., 2024; Yang et al., 2023a) also have several key limitations in terms of domain knowledge integration and diverse exploration which are critical for equation discovery. Thus, there is a need for specialized equation discovery methods that effectively integrate prior scientific knowledge into the navigation of vast equation search space, a strategy akin to a scientist's reliance on foundational scientific knowledge when formulating hypotheses for scientific discovery.</p>
<p>To address all these limitations, we introduce LLM-SR (shown in Fig. 1), a novel framework that combines the strengths of LLMs, reliable optimizers, and evolutionary search for data-driven equation discovery. At its core, LLM-SR is an iterative hypotheses refinement method that generates, evaluates, and refines equation hypotheses based on data-driven feedback. Specifically, LLM-SR first prompts the LLM to propose new equation hypotheses (Fig. 1(a)), then evaluates their fit on the observed data using off-the-shelf optimizers (Fig. 1(b)), and uses this data-driven feedback and a carefully maintained dynamic memory of previous equations (Fig. 1(c)) to iteratively guide the search towards better equations. LLM-SR leverages the scientific knowledge embedded in LLMs using short descriptions of the problem and the variables involved in a given system to generate educated</p>
<p>hypotheses for equation skeletons (i.e., mathematical structures with placeholder parameters for numeric coefficients and constants). The LLM's in-context learning and crossover capabilities (Meyerson et al., 2023) are then employed to refine the suggested equation skeletons in an iterative process. By representing equations as Python programs, we take advantage of LLM's ability to generate structured and executable code (Li et al., 2023; Shojaee et al., 2023) while providing a flexible and effective way to represent general mathematical relations. The program representation also facilitates direct and differentiable parameter optimization to better optimize the coefficients or constants in the generated equations.</p>
<p>To leverage LLM's scientific prior knowledge yet prevent the risk of LLM recitation (Wu et al., 2023) in equation discovery (observed for common benchmarks like Feynman (Udrescu \&amp; Tegmark, 2020)), we designed four custom benchmark problems across physics, biology, and materials science for the evaluation of LLM-SR. By incorporating synthetic modifications to physical models and experimental datasets, these problems aim to simulate the real discovery processes (see App. C and D for details). We evaluated LLM-SR using GPT-3.5-turbo (Brown, 2020) and Mixtral-8x7B (Jiang et al., 2024) as backbone LLMs. Results demonstrate that LLM-SR consistently outperforms state-of-the-art symbolic regression methods, discovering physically accurate equations with better fit and generalization in both in-domain (ID) and out-of-domain (OOD) test settings. By leveraging the scientific prior knowledge, LLM-SR explores the equation search space more efficiently, requiring fewer iterations to find accurate equations. Our ablation analysis also highlights the crucial role of data-driven feedback, iterative refinement, and program representation in LLM-SR's performance. The major contributions of this work are as follows:</p>
<ul>
<li>We introduce LLM-SR, a novel framework that leverages domain-specific prior knowledge and code generation capabilities of LLMs combined with off-the-shelf optimizers and evolutionary search for data-driven scientific equation discovery.</li>
<li>We create four benchmark problems spanning physics, biology, and materials science, designed to simulate real-world discovery and prevent LLM recitation risks for evaluation of LLM-SR.</li>
<li>We show that LLM-SR outperforms state-of-the-art symbolic regression methods by navigating the equation search space more efficiently and discovering more accurate equations with better out-of-domain generalization.</li>
<li>We demonstrate through a comprehensive ablation study that natural language problem descriptions, program representation, data-driven feedback, and iterative hypothesis refinement are all essential components for LLM-SR's success.</li>
</ul>
<h1>2 LLM-SR Methodology</h1>
<h3>2.1 Problem Formulation</h3>
<p>In the task of data-driven equation discovery, also known as symbolic regression (SR), the goal is to find a concise symbolic expression $\tilde{f}$ approximating an unknown function $f: \mathbb{R}^{d} \rightarrow \mathbb{R}$. Given a dataset $\mathcal{D}=\left{\left(\mathbf{x}<em i="i">{i}, y</em>\right)\right}<em i="i">{i=1}^{n}$, SR methods seek to uncover the hidden mathematical relationship such that $\tilde{f}\left(\mathbf{x}</em>, \forall i$. The discovered equation should not only accurately fit the observed data points but also exhibit strong generalization capabilities to unseen data while maintaining interpretability.}\right) \approx y_{i</p>
<p>Current SR methods typically represent equations using techniques such as expression trees (Cranmer, 2023), prefix sequences (Petersen et al., 2021; Biggio et al., 2021), or context-free grammars (Brence et al., 2021). These representations provide structured and constrained search spaces, enabling evolutionary algorithms like genetic programming to explore and find candidate expressions. In contrast, we employ program functions to directly map inputs $\boldsymbol{x}$ to targets $\boldsymbol{y}$ : def $f(x): \ldots$ return $y$. This approach offers greater expressiveness in mathematical relations but expands the search space significantly. To navigate this vast program space effectively, we leverage LLMs for their scientific knowledge and code generation capabilities. Let $\pi_{\theta}$ denote a pre-trained LLM with parameters $\theta$. We iteratively sample equation program skeletons $\mathcal{F}=\left{f: f \sim \pi_{\theta}\right}$, aiming to maximize the reward $\operatorname{Score}<em f="f">{\mathcal{T}}(f, \mathcal{D})$ for a given scientific problem $\mathcal{T}$ and dataset $\mathcal{D}: f^{*}=\arg \max </em>} \mathbb{E<em _mathcal_T="\mathcal{T">{d \in \mathcal{D}}\left[\operatorname{Score}</em>)\right]$. Our approach, LLM-SR, prompts the LLM to propose hypotheses based on problem specifications and demonstrations of previously discovered promising equations. The LLM generates equation program skeletons with placeholder parameters, which are then optimized using robust Python optimizers. Promising hypotheses are added to a dynamic experience buffer, guiding subsequent in-context example updates and equation refinement. Below we explain the key components of this framework, shown in Fig. 1, in more detail.}}(f, \mathcal{D</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Example of initial input prompt for the nonlinear oscillator discovery task, including problem specification, evaluation and optimization function, and the initial input equation example.</p>
<h1>2.2 Hypothesis Generation</h1>
<p>The hypothesis generation step (Fig. 1(a)) utilizes a pre-trained LLM to propose diverse and promising equation program skeletons. Our prompt structure, shown in Fig. 2, consists of the following components: Instruction: A clear directive for completing the function body, emphasizing consideration of physical meanings and relationships among input variables; Problem Specification: A concise description of the scientific problem, including key variables, constraints, and objectives; Evaluation and Optimization Function: The function used to assess the data-driven quality and fitness of proposed equations after parameter optimization; and Experience Demonstration: In-context examples of equation skeleton programs and their improvement trajectory.
At each iteration $t$, we sample a batch of $b$ equation skeletons $\mathcal{F}<em i="i">{t}=\left{f</em>\right}<em _theta="\theta">{i=1}^{b}$ from the LLM $\pi</em>}: f_{i} \sim$ $\pi_{\theta}\left(\cdot \mid \mathbf{p<em t="t">{t}\right)$ where $\mathbf{p}</em>$ is the constructed prompt. We employ stochastic temperature-based sampling to balance exploration (creativity) and exploitation (prior knowledge) in the hypothesis space. Sampled equation programs are executed, and those failing to execute or exceeding a maximum execution time threshold are discarded to ensure validity and computational efficiency.</p>
<h3>2.3 Hypothesis Optimization and AsSESSMENT</h3>
<p>After generating equation skeleton hypotheses, we evaluate and score them using observed data (Fig. 1(b)). This process involves optimizing the parameters of each hypothesis and then assessing its fitness. We decouple equation discovery into two steps: ( $i$ ) discovering the equation program structures (skeletons) using the LLM, and (ii) optimizing the skeleton parameters/coefficients based on data. The LLM is responsible for generating equation skeletons and the core logic of the program, while the numeric values of the parameters are represented as placeholders in the form of a parameter vector params (as shown in Fig. 2). These placeholders are subsequently optimized to fit the data. Each equation program skeleton $f \in \mathcal{F}_{t}$ is a function of the form: "def $f(x$, params): ... return $\gamma$ ". We employ two optimization approaches: numpy+BFGS: A nonlinear optimization method using scipy library (Fletcher, 1987), and torch+Adam: A stochastic gradient-based</p>
<p>optimization algorithm using PyTorch (Kingma \&amp; Ba, 2014). The choice between these methods depends on the problem characteristics and equation skeleton complexity. The numpy+BFGS is preferred for problems with fewer parameters, while torch+Adam is more suitable for larger-scale problems benefiting from efficient gradient computation through differentiable programming.</p>
<p>After optimizing the skeleton parameters (params<em>), we assess the fitness of equation program hypotheses by measuring its ability to capture underlying patterns in the data. We compute predicted target values as: $\hat{\mathbf{y}}=f\left(\mathbf{x}\right.$, params $\left.^{</em>}\right)$. The fitness evaluation score $s$ is then calculated as the negative Mean Squared Error (MSE) between predicted and true target values: $s=\operatorname{Score}_{\mathcal{T}}(f, \mathcal{D})=$ $-\operatorname{MSE}(\hat{\mathbf{y}}, \mathbf{y})$.</p>
<h1>2.4 EXPERIENCE MANAGEMENT</h1>
<p>To efficiently navigate the search landscape and avoid local minima, LLM-SR employs an experience management step (Fig.1(c)). This process maintains a diverse population of high-quality equation programs in a dynamic experience buffer and samples from this population to construct informative prompts for subsequent LLM iterations. Let $\mathcal{P}<em t="t">{t}$ denote the experience buffer at iteration $t$, storing pairs of equation skeleton hypotheses and their corresponding scores $(f, s)$. We adopt an islands model (Cranmer, 2023; Romera-Paredes et al., 2024) with $m$ independently evolving islands, initialized with a copy of the equation program example from the initial prompt (equation_v0 in Fig. 2). At each iteration $t$, new hypotheses $\mathcal{F}</em>}$ and their scores are added to the source island (from which the in-context examples of prompts were sampled) if they improve upon the current best: $\mathcal{P<em t="t">{t}^{i} \leftarrow$ $\mathcal{P}</em>}^{i} \cup{(f, s): f \in \mathcal{F<em _mathcal_T="\mathcal{T">{t}, s=-\operatorname{Score}</em>}}(f, \mathcal{D}), s&gt;s_{\text {best }}^{i}}$ where $\mathcal{P<em _best="{best" _text="\text">{t}^{i}$ is the $i$-th island and $s</em>$ is its current best score. Within each island, equation programs are clustered based on their signature (defined by their score) to further preserve diversity.
To construct informative prompts, we then sample equation programs from the experience buffer using a two-stage method. First, uniformly select a random island from the $m$ available. Second, sample $k$ equation programs from the selected island using (a) Cluster selection via Boltzmann sampling, favoring higher scores: $P_{i}=\frac{\exp \left(s_{i} / \tau_{c}\right)}{\sum_{i^{\prime}} \exp \left(s_{i}^{\prime} / \tau_{c}\right)}$ where $s_{i}$ is the mean score of the $i$-th cluster and $\tau_{c}$ is a temperature parameter. (b) Individual program sampling, favoring shorter programs: $P\left(f_{i}\right) \propto \exp \left(-\tilde{l}}}^{i<em p="p">{i} / \tau</em>}\right)$ where $\tilde{l<em p="p">{i}$ is the normalized program length and $\tau</em>$ is a temperature parameter. The sampled programs are then included in the prompt as in-context experience demonstrations, guiding the LLM in generating new equation program hypotheses. Detailed sampling procedures are provided in Appendix B.</p>
<p>Algorithm 1 presents the simplified pseudo-code of the LLM-SR framework. The experience buffer $\mathcal{P}<em t-1="t-1">{0}$ is initialized with initial prompt, using a simple linear equation skeleton as a template (e.g., Fig. 2 for the nonlinear oscillator problem). This initial structure serves as a baseline for the LLM to modify operators and structures based on its domain knowledge. Each iteration $t$ involves: (i) sampling $k$ in-context examples from $\mathcal{P}</em>$ if they improve upon the best score $s^{}$, (ii) updating the prompt, (iii) generating $b$ equation program skeletons from the LLM, and (iv) evaluating and potentially adding these to $\mathcal{P}_{t<em>}$. This process leverages the LLM's generative capabilities to refine equation structures guided by the evolving experience buffer. The algorithm returns the best-scoring program $f^{</em>}$ and its score $s^{*}$ as the optimal solution, iteratively exploring the equation space while balancing exploitation of promising structures with exploration of new possibilities.</p>
<h2>3 EXPERIMENTS</h2>
<h3>3.1 BENCHMARKS AND DATASETS</h3>
<p>The Feynman benchmark (Udrescu \&amp; Tegmark, 2020), comprising 120 fundamental physics problems from Feynman Lectures on Physics database series ${ }^{1}$, is the current</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1: LLM-SR
Input \(\quad\) LLM \(\pi_{\theta}\), dataset \(\mathcal{D}\), problem \(\mathcal{T}\),
    \(T\) iterations, \(k\) in-context
examples,
        \(b\) samples per prompt
\# Initialize population
\(\mathcal{P}_{0} \leftarrow \operatorname{InitPop}()\)
\(f^{*}, s^{*} \leftarrow\) null, \(-\infty\)
for \(t \leftarrow 1\) to \(T-1\) do
    \# Sample examples from buffer
    \(E \leftarrow\left\{\epsilon_{j}\right\}_{j=1}^{k}\)
    \(\epsilon_{j}=\operatorname{SampleExp}\left(\mathcal{P}_{t-1}\right)\)
    \# Prompt with new examples
    \(\mathbf{p} \leftarrow\) MakeFewShotPrompt \((E)\)
    \# Sample from LLM
    \(\mathcal{F}_{t} \leftarrow\left\{f_{j}\right\}_{j=1}^{k}, f_{j} \sim \pi_{\theta}(|\mathbf{p})\)
    \# Evaluation and population update
    for \(f \in \mathcal{F}_{t}\) do
        \(s \leftarrow \operatorname{Score}_{\mathcal{T}}(f, \mathcal{D})\)
        if \(s&gt;s^{*}\) then
            \(f^{*}, s^{*} \leftarrow f, s\)
            \(\mathcal{P}_{t} \leftarrow \mathcal{P}_{t-1} \cup\{(f, s)\}\)
        end
    end
end
Output \(t f^{*}, s^{*}\)
</code></pre></div>

<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>standard benchmark for evaluating symbolic regression techniques in scientific equation discovery. However, our investigation reveals that LLMs have significant memorization issues with these well-known physics equations, potentially undermining their effectiveness in assessing LLM-based equation discovery approaches. For instance, LLM-SR rapidly achieves low data-driven errors within few iterations $(&lt;20)$ on Feynman problems, suggesting a recitation of memorized information rather than a process of discovery (full results in App. C). To address these limitations and provide a more robust evaluation, we introduce novel benchmark problems across three scientific domains. Our benchmark design focuses on: (i) Custom modifications to physical models to prevent trivial memorization; (ii) Complex synthetic non-linear relationships to challenge creative exploration; and (iii) Realistic scenarios with experimental data to reflect real modeling processes. These benchmarks are designed to challenge the model's ability to uncover complex mathematical relations while leveraging its scientific prior knowledge, simulating conditions for scientific discovery. To validate our new benchmarks' effectiveness, we compared LLM response perplexity (using open-source Mixtral-8x7B) and equation discovery error curves (using GPT-3.5) between Feynman problems and our new benchmarks. Results show lower perplexity (Fig. 9 in App. C) and sharper discovery curves (Fig. 11 in App. C) for Feynman problems, suggesting that both LLM backbones have more likely memorized common Feynman equations, while our benchmarks present novel challenges requiring reasoning and exploration. We next discuss these new benchmark problems in detail:
Nonlinear Oscillators Nonlinear damped oscillators, ubiquitous in physics and engineering, are governed by differential equations describing the complex interplay between an oscillator's position, velocity, and acting forces. The general form of these equations is typically expressed as: $\ddot{x}+$ $f(t, x, \dot{x})=0$ where $t$ is time, $x$ is position, and $f(t, x, \dot{x})$ represents nonlinear forces. To challenge LLM-based equation discovery methods beyond common oscillator systems (e.g., Van der Pol, Duffing), we introduce two custom nonlinear designs: 1 Oscillation $1: \dot{v}=F \sin (\omega x)-$ $\alpha v^{3}-\beta x^{3}-\gamma x v-x \cos (x)$; and 2 Oscillation $2: \dot{v}=F \sin (\omega t)-\alpha v^{3}-\beta x v-\delta x \exp (\gamma x)$, where $v=\dot{x}$ represents velocity, and $\omega, \alpha, \beta, \gamma, \delta$ are constants. These two forms, serving as a proof of concept, are carefully designed to incorporate a combination of challenging yet solvable nonlinear structures (including trigonometric, polynomial, and exponential) that are distinct from well-known oscillator systems. More details on the design rationale and data generation are provided in App. D.1.
Bacterial Growth The growth of Escherichia coli (E. coli) bacteria has been widely studied in microbiology due to its importance in various applications, such as biotechnology, and food safety. Discovering equations governing E. coli growth rate under different conditions is crucial for predicting and optimizing bacterial growth. The bacterial population growth rate has been modeled using a differential equation with the effects of population density $(B)$, substrate concentration $(S)$, temperature $(T)$, and pH level, which is commonly formulated with multiplicative structure: $\frac{d R}{d t}=$ $f(B, S, T, \mathrm{pH})=f_{B}(B) \cdot f_{S}(S) \cdot f_{T}(T) \cdot f_{\mathrm{pH}}(\mathrm{pH})$. To create a challenging benchmark that leverages LLMs' prior knowledge while preventing trivial memorization, we introduce novel nonlinear formulations for $f_{T}(T)$ and $f_{\mathrm{pH}}(\mathrm{pH})$. These custom functions maintain key characteristics of established models while introducing complexities that require exploration and discovery rather than recall. The complete mathematical formulations, along with the data generation process and parameter ranges, are detailed in App. D.2.
Material Stress Behavior The stress-strain relationship of materials under varying conditions, particularly as a function of temperature and material type, is fundamental to structural design and analysis across engineering disciplines. This benchmark problem leverages a real-world experimental dataset from (Aakash et al., 2019), comprising tensile tests on Aluminum 6061-T651 across a range of temperatures. The inclusion of this benchmark serves multiple purposes: (i) It challenges LLM-based equation discovery methods with experimental data, moving beyond synthetic or idealized problems. (ii) Unlike the previous benchmarks, there is no predetermined theoretical model structure for this problem, necessitating creative modeling approaches from LLMs. In other words, modeling for this type of task is mostly empirical and the stress-strain-temperature relations may vary significantly based on the specific material and experimental condition, preventing trivial memorization. More details on this problem and experimental data are provided in App. D.3.</p>
<h1>3.2 EXPERIMENTAL SETUP</h1>
<p>We compare LLM-SR against state-of-the-art symbolic regression (SR) methods, including evolutionary-based approaches like GPlearn ${ }^{2}$ (Genetic Programming) and $\mathbf{P y S R}^{3}$ (multi-island</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Oscillation 1</th>
<th></th>
<th>Oscillation 2</th>
<th></th>
<th>E. coli growth</th>
<th></th>
<th>Stress-Strain</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$\mathrm{ID}_{\downarrow}$</td>
<td>$\mathrm{OOD}_{\downarrow}$</td>
<td>$\mathrm{ID}_{\downarrow}$</td>
<td>$\mathrm{OOD}_{\downarrow}$</td>
<td>$\mathrm{ID}_{\downarrow}$</td>
<td>$\mathrm{OOD}_{\downarrow}$</td>
<td>$\mathrm{ID}_{\downarrow}$</td>
<td>$\mathrm{OOD}_{\downarrow}$</td>
</tr>
<tr>
<td>GPlearn</td>
<td>0.0155</td>
<td>0.5567</td>
<td>0.7551</td>
<td>3.188</td>
<td>1.081</td>
<td>1.039</td>
<td>0.1063</td>
<td>0.4091</td>
</tr>
<tr>
<td>NeSymReS (Biggio et al., 2021)</td>
<td>0.0047</td>
<td>0.5377</td>
<td>0.2488</td>
<td>0.6472</td>
<td>N/A (d $&gt;3$ )</td>
<td>0.7928</td>
<td>0.6377</td>
<td></td>
</tr>
<tr>
<td>E2E (Kamienny et al., 2022)</td>
<td>0.0082</td>
<td>0.3722</td>
<td>0.1401</td>
<td>0.1911</td>
<td>0.6321</td>
<td>1.4467</td>
<td>0.2262</td>
<td>0.5867</td>
</tr>
<tr>
<td>DSR (Petersen et al., 2021)</td>
<td>0.0087</td>
<td>0.2454</td>
<td>0.0580</td>
<td>0.1945</td>
<td>0.9451</td>
<td>2.4291</td>
<td>0.3326</td>
<td>1.108</td>
</tr>
<tr>
<td>uDSR (Landajuela et al., 2022)</td>
<td>$\underline{0.0003}$</td>
<td>$\underline{0.0007}$</td>
<td>0.0032</td>
<td>$\underline{0.0015}$</td>
<td>0.3322</td>
<td>5.4584</td>
<td>0.0502</td>
<td>0.1761</td>
</tr>
<tr>
<td>PySR (Cranmer, 2023)</td>
<td>0.0009</td>
<td>0.3106</td>
<td>$\underline{0.0002}$</td>
<td>0.0098</td>
<td>$\underline{0.0376}$</td>
<td>$\underline{1.0141}$</td>
<td>$\underline{0.0331}$</td>
<td>$\underline{0.1304}$</td>
</tr>
<tr>
<td>LLM-SR (Mixtral)</td>
<td>$\mathbf{7 . 8 9 c - 8}$</td>
<td>$\mathbf{0 . 0 0 0 2}$</td>
<td>0.0030</td>
<td>0.0291</td>
<td>$\mathbf{0 . 0 0 2 6}$</td>
<td>$\mathbf{0 . 0 0 3 7}$</td>
<td>$\mathbf{0 . 0 1 6 2}$</td>
<td>0.0946</td>
</tr>
<tr>
<td>LLM-SR (GPT-3.5)</td>
<td>4.65e-7</td>
<td>0.0005</td>
<td>$\mathbf{2 . 1 2 e - 7}$</td>
<td>$\mathbf{3 . 8 1 e - 5}$</td>
<td>0.0214</td>
<td>0.0264</td>
<td>0.0210</td>
<td>$\mathbf{0 . 0 5 1 6}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Quantitative performance comparison of LLM-SR (with GPT-3.5 and Mixtral backbones), and SR baseline models on different scientific benchmark problems measured by Normalized Mean Squared Error. "N/A" refers to incompatibility of E. coli Growth dataset for the NeSymReS baseline (limited to $&lt;3 d$ data).
asynchronous evolution) <em>(Cranmer, 2023)</em>, and deep learning-based methods such as DSR (reinforcement learning for expression generation) <em>(Petersen et al., 2021)</em> and uDSR (extending DSR with Genetic Programming search at decoding) <em>(Landajuela et al., 2022)</em>. We also evaluate against pre-trained Transformer SR models: NeSymReS <em>(Biggio et al., 2021)</em> and E2E <em>(Kamienny et al., 2022)</em>. This selection provides a comprehensive evaluation across different SR paradigms. We allow all search-based baselines to run for over 2 M iterations until convergence to their best performance. In LLM-SR experiments, each iteration samples $b=4$ equation skeletons per prompt with temperature $\tau=0.8$, optimizes parameters via numpy+BFGS or torch+Adam (with 30 seconds timeout), and uses $k=2$ in-context examples from the experience buffer for refinement. We run LLM-SR variants for around 2.5 K iterations in all experiments. More details on the implementation and parameter settings of each baseline as well as implementation specifics of LLM-SR, including experience buffer structure, prompt refinement strategy, and parallel evaluation are provided in App. A.</p>
<h1>3.3 Quantitative Results</h1>
<p>Accuracy Table 1 compares the performance of LLM-SR (using GPT-3.5 and Mixtral backbones) against state-of-the-art symbolic regression methods across various scientific benchmarks. Performance is measured using Normalized Mean Squared Error (NMSE), with lower values indicating better performance. LLM-SR with both backbones consistently outperform baselines, despite running for fewer iterations ( 2.5 K vs. 2 M + for baselines). To assess generalization capability, we evaluate performance on both in-domain (ID) and out-of-domain (OOD) test sets. The performance gap between LLM-SR and baselines is more pronounced in the OOD setting, suggesting superior generalization of LLM-SR's discovered equations. For instance, on the E. coli growth problem, LLM-SR achieves an OOD NMSE of $\sim 0.0037$, significantly outperforming other methods (with OOD NMSE $&gt;1$ ).</p>
<p>Among baselines, PySR and uDSR show the best performance, while Transformer SR models (NeSymReS, E2E) perform poorly, likely due to limited generalization from their pretraining on common benchmark distributions to our novel datasets. These results demonstrate LLM-SR's effectiveness in discovering accurate and generalizable equations across diverse scientific domains.</p>
<p>Efficiency Fig. 3 shows the performance trajectories of LLM-SR variants and symbolic regression baselines across different scientific benchmark problems, depicting the best fitting scores achieved over search iterations. By leveraging scientific prior knowledge, LLM-SR explores a considerably lower number of equation candidates in the vast optimization space compared to symbolic regression baselines that lack this knowledge. This is evident from the sharp drops in
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Best score trajectories of LLM-SR with GPT-3.5 and Mixtral against SR baselines across different benchmark problems. LLM-SR discovers accurate equations more efficiently, requiring fewer iterations. Baselines fail to match LLM-SR even after 2 M iterations.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Discovered equations for Oscillation 1 (top) and Oscillation 2 (bottom) problems: (a) True equations and their phase diagram; (b) Equation program skeletons identified by LLM-SR, with simplified forms obtained after parameter optimization; and (c) Equations found using SR baselines. Shaded green terms denote recovered symbolic terms from true equations.
the error curves for LLM-SR variants, indicating they efficiently navigate the search space by exploiting domain knowledge to identify promising candidates more quickly. In contrast, the symbolic regression baselines show much more gradual improvements and fail to match LLM-SR's performance even after 2 M + iterations. The performance gap between LLM-SR and baselines also mostly widens over iterations, highlighting the effectiveness of LLMs acting as mutation (or crossover) agents in LLM-SR's iterative refinement process.</p>
<h1>3.4 Qualitative Analysis</h1>
<p>Discovered Equations Fig. 4 presents the final discovered equations for both Oscillation problems using LLM-SR and other symbolic regression baselines. A notable observation is that equations discovered by LLM-SR have better recovered the symbolic terms of the true equations compared to baselines. Also, LLM-SR provides explanations and reasoning steps based on scientific knowledge about the problem, leading to more interpretable terms combined as the final function. For example, in both problems, LLM-SR identifies the equation structure as a combination of driving force, damping force, and restoring force terms, relating them to the problem's physical characteristics. In contrast, baselines generate equations lacking interpretability and understanding of the physical meanings of variables and the relations between them. These equations appear as a combination of mathematical operations and variables without a clear connection to the problem's underlying physical principles. App. G provides a more detailed qualitative analysis of the final discovered equations for other benchmark problems (Figs. 23 and 24), as well as the equations discovered over the performance trajectory of LLM-SR's iterations (Figs. 19-22).
Generalization Fig. 5 compares predicted distributions obtained from LLM-SR, and competing baselines (PySR and uDSR) with the ground truth distribution of E. coli growth problem. The shaded region and black points indicate in-domain (ID) data, while the rest represent out-of-domain (OOD). Results show that distributions obtained from LLM-SR align well with the ground truth, not only for ID data but also for OOD regions. This alignment demonstrates the better generalizability of equations discovered by LLM-SR to unseen data, likely due to the integration of scientific prior knowledge in the equation discovery process. In contrast, PySR and uDSR tend to overfit the observed data, with significant deviations in OOD regions.</p>
<p>This overfitting behavior highlights their limited ability to generalize beyond the training data and capture the true physical underlying patterns. Detailed analyses for other benchmark problems are provided in App. G.</p>
<h1>3.5 Ablation Study</h1>
<p>We conducted an ablation study on the Oscillation 2 problem using GPT-3.5 as the LLM backbone to investigate the impact of LLM-SR's key components (Fig. 6, more detailed results in App. E). Our findings reveal the crucial role of each component in the model's performance. The "w/o Prior" variant, which removes the natural language description of the scientific problem and its variables, led to a considerable performance drop. This highlights the importance of incorporating prior domain knowledge in equation discovery. The "w/o Program" variant, which restricts LLM hypothesis generation to single-line mathematical expressions, also had a negative but less severe impact on performance, denoting the importance of programming flexibility in this task. The "w/o Iterative Refinement" variant, equivalent to the LLM sampling without the optimization loop, led to substantial performance drops (NMSE: 1.01e-1 in-domain, 1.81e-1 OOD), emphasizing the importance of the evolutionary search and optimization process in LLM-SR's success. The "w/o skeleton + optimizer" variant, which requires end-to-end equation generation without separate parameter optimization step (i.e., generating hypotheses as full equations along with their numeric parameters), also significantly worsened results (NMSE: 3.78e-1 in-domain, 3.75e-1 OOD). This highlights the effectiveness of our two-stage approach-generating equation skeletons followed by data-driven parameter optimization-in navigating complex combinatorial optimization space of discrete equation structures and continuous parameters.</p>
<p>We compared two optimization frameworks: numpy+BFGS and torch+Adam. In our experiments, the numpy+BFGS variant performed slightly better compared to torch+Adam. This difference is most likely attributed to the LLM's higher proficiency in generating numpy code rather than inherent superiority of the optimization method for this task. LLM-SR relies on direct and differentiable parameter optimization, a capability not present in current symbolic regression methods. Combining LLM-SR with LLM backbones that are better in generating PyTorch code could potentially enhance equation discovery by leveraging differentiable parameter optimization in future.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Comparison of E. coli growth rate distributions from LLM-SR, PySR, and uDSR.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Ablation results on the Oscillation 2 problem, showing the impact of problem specification, program representation, iterative refinement, parameter optimization, and optimization frameworks on LLM-SR's performance.</p>
<h2>4 Related Work</h2>
<p>LLMs and Optimization While LLMs have shown remarkable capabilities in various domains, their performance often falls short in tasks requiring high precision and complex reasoning. To address this, researchers have explored combining LLMs with feedback mechanisms (Madaan et al., 2024; Yang et al., 2023b; Haluptzok et al., 2022) and integrating them into iterative optimization loops (Lehman et al., 2023; Liu et al., 2023; Wu et al., 2024; Lange et al., 2024). Recently, LLMs have</p>
<p>been successfully applied in prompt optimization (Yang et al., 2023a; Guo et al., 2024), data-driven analysis (Majumder et al., 2024; Zheng et al., 2023b), and neural architecture search (Chen et al., 2023; Zheng et al., 2023a). Most related to our work is FunSearch (Romera-Paredes et al., 2024) that combines LLMs with systematic evaluators to search for heuristics that push the boundaries in solving some established open mathematical problems. Building upon these ideas, our LLM-SR framework employs LLM as an optimizer, leveraging its scientific prior knowledge and data-driven evaluators to discover mathematical equations underlying scientific observations.
LLMs for Scientific Discovery The integration of LLMs into scientific tasks has recently garnered significant attention, offering transformative potential across various fields such as drug discovery, biology, and materials science (Wang et al., 2023a; AI4Science \&amp; Quantum, 2023). Specifically, recent studies have demonstrated the capacity of LLMs to propose scientifically plausible and potentially novel hypotheses by leveraging their extensive domain knowledge and reasoning capabilities (Majumder et al., 2024; Zheng et al., 2023b; Qi et al., 2023; Ji et al., 2024). Also, when equipped with external tools and scientific simulators, LLM agents have shown promise in automated statistical discovery and reasoning (Li et al., 2024; Wang et al., 2023b; Ma et al., 2024). Despite the increasing exploration of LLMs in scientific contexts and question answering, their potential for tasks such as equation discovery and symbolic regression remains largely unexplored. Our work extends this line of research by introducing a novel approach for equation discovery that combines LLMs' scientific prior knowledge and code generation with data-driven evaluation.
Symbolic Regression Symbolic regression (SR) methods can be broadly categorized into searchbased approaches, learning-based models, as well as hybrid learning and search methods. Searchbased approaches mainly explore the space of equation structures and parameters using evolutionary algorithms or reinforcement learning (Schmidt \&amp; Lipson, 2009; Cranmer, 2023; Petersen et al., 2021; Sun et al., 2023). They offer interpretable results but often struggle with scalability and efficiency. Learning-based models, on the other hand, leverage large-scale synthetic data and Transformer models to learn the mapping between numeric input observations and output mathematical expressions (Biggio et al., 2021; Kamienny et al., 2022). Hybrid methods aim to combine the strengths of both approaches, guiding the search by employing neural priors to improve the expressiveness and efficiency of the discovery process (Landajuela et al., 2022; Shojaee et al., 2024; Mundhenk et al., 2021; Meidani et al., 2023). Despite the progress made by these approaches, they often face limitations such as the lack of scientific prior knowledge incorporation and the restricted expressiveness of traditional equation representations like expression trees. While there have been some works incorporating prior knowledge by using declarative bias and structures with pre-defined grammars (Todorovski \&amp; Dzeroski, 1997; Todorovski \&amp; Džeroski, 2007), these methods do not leverage the power of LLMs for this task. Our work advances this research direction by utilizing LLMs to efficiently search the combinatorial optimization space of equation discovery and generate meaningful equation structures based on the embedded scientific prior knowledge.</p>
<h1>5 CONCLUSION AND Future WORK</h1>
<p>In this work, we introduced LLM-SR, a novel approach to equation discovery that leverages the scientific knowledge and code generation capabilities of Large Language Models (LLMs). By treating equations as programs and combining LLM-generated educated hypotheses with evolutionary search, our method demonstrates superior performance on benchmark problems across diverse scientific domains, particularly in out-of-domain test settings. Despite its promising results, LLM-SR has limitations. The method's performance is inherently tied to the quality and breadth of the LLM's training data, which may lead to biases or gaps in certain scientific domains. Additionally, the computational cost of iterative LLM queries and parameter optimization could be prohibitive for large-scale problems. Future work could focus on integrating domain-specific LMs and retrievalaugmented learning techniques to enhance the relevance and accuracy of generated equations; and incorporating human domain experts in the pipeline to improve the scientific plausibility. The creation of more comprehensive benchmarks, designed to simulate true discovery processes and prevent LLM recitation, is also crucial for rigorous evaluation of LLM-based equation discovery methods.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>This research was partially supported by the U.S. National Science Foundation (NSF) under Grant No. 2416728 .</p>
<h1>REFERENCES</h1>
<p>B.S. Aakash, JohnPatrick Connors, and Michael D. Shields. Stress-strain data for aluminum 6061t651 from 9 lots at 6 temperatures under uniaxial and plane strain tension. Data in Brief, 25: 104085, 2019. ISSN 2352-3409. doi: https://doi.org/10.1016/j.dib.2019.104085.</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.</p>
<p>Microsoft Research AI4Science and Microsoft Azure Quantum. The impact of large language models on scientific discovery: a preliminary study using gpt-4. arXiv preprint arXiv:2311.07361, 2023.</p>
<p>Luca Biggio, Tommaso Bendinelli, Alexander Neitz, Aurelien Lucchi, and Giambattista Parascandolo. Neural symbolic regression that scales. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 936-945. PMLR, 18-24 Jul 2021.</p>
<p>Jure Brence, Ljupčo Todorovski, and Sašo Džeroski. Probabilistic grammars for equation discovery. Knowledge-Based Systems, 224:107077, 2021. ISSN 0950-7051. doi: https://doi.org/10.1016/j. knosys.2021.107077.</p>
<p>Tom B Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
Angelica Chen, David Dohan, and David So. Evoprompting: Language models for code-level neural architecture search. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 7787-7817. Curran Associates, Inc., 2023.</p>
<p>Miles Cranmer. Interpretable machine learning for science with pysr and symbolicregression. jl. arXiv preprint arXiv:2305.01582, 2023.</p>
<p>Roger Fletcher. Practical Methods of Optimization. John Wiley \&amp; Sons, New York, NY, USA, second edition, 1987.</p>
<p>Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. In The Twelfth International Conference on Learning Representations, 2024.</p>
<p>Patrick Haluptzok, Matthew Bowers, and Adam Tauman Kalai. Language models can teach themselves to program better. arXiv preprint arXiv:2207.14502, 2022.</p>
<p>Heng Ji, Qingyun Wang, Doug Downey, and Tom Hope. Scimon: Scientific inspiration machines optimized for novelty. In ACL Anthology: Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 279-299. University of Illinois Urbana-Champaign/CABBI, 2024.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.</p>
<p>Pierre-Alexandre Kamienny, Stéphane d'Ascoli, Guillaume Lample, and Francois Charton. End-to-end symbolic regression with transformers. In Advances in Neural Information Processing Systems, 2022.</p>
<p>Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>John R. Koza. Genetic programming as a means for programming computers by natural selection. Statistics and Computing, 4(2):87-112, Jun 1994. ISSN 1573-1375. doi: 10.1007/BF00175355.</p>
<p>William La Cava, Patryk Orzechowski, Bogdan Burlacu, Fabricio de Franca, Marco Virgolin, Ying Jin, Michael Kommenda, and Jason Moore. Contemporary symbolic regression methods and their relative performance. In J. Vanschoren and S. Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1, 2021.</p>
<p>Mikel Landajuela, Chak Lee, Jiachen Yang, Ruben Glatt, Claudio P. Santiago, Ignacio Aravena, Terrell N. Mundhenk, Garrett Mulcahy, and Brenden K. Petersen. A unified framework for deep symbolic regression. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.</p>
<p>Robert Tjarko Lange, Yingtao Tian, and Yujin Tang. Large language models as evolution strategies. arXiv preprint arXiv:2402.18381, 2024.</p>
<p>Pat Langley. Data-driven discovery of physical laws. Cognitive Science, 5(1):31-54, 1981.
Joel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, and Kenneth O Stanley. Evolution through large models. In Handbook of Evolutionary Machine Learning, pp. 331-366. Springer, 2023.</p>
<p>Michael Y Li, Emily B Fox, and Noah D Goodman. Automated statistical model discovery with language models. arXiv preprint arXiv:2402.17879, 2024.</p>
<p>Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023.</p>
<p>Tennison Liu, Nicolás Astorga, Nabeel Seedat, and Mihaela van der Schaar. Large language models to enhance bayesian optimization. In The Twelfth International Conference on Learning Representations, 2023.</p>
<p>Pingchuan Ma, Tsun-Hsuan Wang, Minghao Guo, Zhiqing Sun, Joshua B Tenenbaum, Daniela Rus, Chuang Gan, and Wojciech Matusik. Llm and simulation as bilevel optimizers: A new paradigm to advance physical scientific discovery. arXiv preprint arXiv:2405.09783, 2024.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Sanchaita Hazra, Ashish Sabharwal, and Peter Clark. Data-driven discovery with large generative models. arXiv preprint arXiv:2402.13610, 2024.</p>
<p>Michael de la Maza and Bruce Tidor. An analysis of selection procedures with particular attention paid to proportional and boltzmann selection. In Proceedings of the 5th International Conference on Genetic Algorithms, pp. 124-131, San Francisco, CA, USA, 1993. Morgan Kaufmann Publishers Inc. ISBN 1558602992.</p>
<p>Kazem Meidani, Parshin Shojaee, Chandan K Reddy, and Amir Barati Farimani. Snip: Bridging mathematical symbolic and numeric realms with unified pre-training. In The Twelfth International Conference on Learning Representations, 2023.</p>
<p>Elliot Meyerson, Mark J Nelson, Herbie Bradley, Adam Gaier, Arash Moradi, Amy K Hoover, and Joel Lehman. Language model crossover: Variation through few-shot prompting. arXiv preprint arXiv:2302.12170, 2023.</p>
<p>Terrell N. Mundhenk, Mikel Landajuela, Ruben Glatt, Claudio P. Santiago, Daniel faissol, and Brenden K. Petersen. Symbolic regression via deep reinforcement learning enhanced genetic programming seeding. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021.</p>
<p>Brenden K Petersen, Mikel Landajuela Larma, Terrell N. Mundhenk, Claudio Prata Santiago, Soo Kyung Kim, and Joanne Taery Kim. Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. In International Conference on Learning Representations, 2021.</p>
<p>Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Zhang-Ren Chen, and Bowen Zhou. Large language models are zero shot hypothesis proposers. arXiv preprint arXiv:2311.05965, 2023.</p>
<p>Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, Jordan S. Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli, and Alhussein Fawzi. Mathematical discoveries from program search with large language models. Nature, 625(7995):468-475, Jan 2024. ISSN 1476-4687. doi: $10.1038 / \mathrm{s} 41586-023-06924-6$.</p>
<p>Michael Schmidt and Hod Lipson. Distilling free-form natural laws from experimental data. Science Advance, 324(5923):81-85, 2009. ISSN 0036-8075. doi: 10.1126/science. 1165893.</p>
<p>Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan K Reddy. Execution-based code generation using deep reinforcement learning. arXiv preprint arXiv:2301.13816, 2023.</p>
<p>Parshin Shojaee, Kazem Meidani, Amir Barati Farimani, and Chandan Reddy. Transformer-based planning for symbolic regression. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Fangzheng Sun, Yang Liu, Jian-Xun Wang, and Hao Sun. Symbolic physics learner: Discovering governing equations via monte carlo tree search. In The Eleventh International Conference on Learning Representations, 2023.</p>
<p>Ljupco Todorovski and Saso Dzeroski. Declarative bias in equation discovery. In Proceedings of the Fourteenth International Conference on Machine Learning, ICML '97, pp. 376-384, San Francisco, CA, USA, 1997. Morgan Kaufmann Publishers Inc. ISBN 1558604863.</p>
<p>Ljupčo Todorovski and Sašo Džeroski. Integrating Domain Knowledge in Equation Discovery, pp. 69-97. Springer Berlin Heidelberg, Berlin, Heidelberg, 2007. ISBN 978-3-540-73920-3. doi: $10.1007 / 978-3-540-73920-3 \backslash .4$.</p>
<p>Silviu-Marian Udrescu and Max Tegmark. Ai feynman: A physics-inspired method for symbolic regression. Science Advances, 6(16):eaay2631, 2020. doi: 10.1126/sciadv.aay2631.</p>
<p>Marco Virgolin and Solon P Pissis. Symbolic regression is NP-hard. Transactions on Machine Learning Research, 2022. ISSN 2835-8856.</p>
<p>Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Anima Anandkumar, Karianne Bergen, Carla P. Gomes, Shirley Ho, Pushmeet Kohli, Joan Lasenby, Jure Leskovec, Tie-Yan Liu, Arjun Manrai, Debora Marks, Bharath Ramsundar, Le Song, Jimeng Sun, Jian Tang, Petar Veličković, Max Welling, Linfeng Zhang, Connor W. Coley, Yoshua Bengio, and Marinka Zitnik. Scientific discovery in the age of artificial intelligence. Nature, 620(7972):47-60, Aug 2023a. ISSN 1476-4687. doi: 10.1038/s41586-023-06221-2.</p>
<p>Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah D Goodman. Hypothesis search: Inductive reasoning with language models. arXiv preprint arXiv:2309.05660, 2023b.</p>
<p>Xingyu Wu, Sheng-hao Wu, Jibin Wu, Liang Feng, and Kay Chen Tan. Evolutionary computation in the era of large language model: Survey and roadmap. arXiv preprint arXiv:2401.10034, 2024.</p>
<p>Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. arXiv preprint arXiv:2307.02477, 2023.</p>
<p>Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023a.</p>
<p>Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, and Anima Anandkumar. LeanDojo: Theorem proving with retrieval-augmented language models. In Neural Information Processing Systems (NeurIPS), 2023b.</p>
<p>Mingkai Zheng, Xiu Su, Shan You, Fei Wang, Chen Qian, Chang Xu, and Samuel Albanie. Can gpt-4 perform neural architecture search? arXiv preprint arXiv:2304.10970, 2023a.</p>
<p>Yizhen Zheng, Huan Yee Koh, Jiaxin Ju, Anh TN Nguyen, Lauren T May, Geoffrey I Webb, and Shirui Pan. Large language models for scientific synthesis, inference and explanation. arXiv preprint arXiv:2310.07984, 2023b.</p>
<h1>APPENDIX</h1>
<h2>A Baseline Implementation Details</h2>
<h2>A. 1 MODELS</h2>
<p>We compare LLM-SR against several state-of-the-art Symbolic Regression (SR) baselines, encompassing a diverse range of methodologies from traditional evolutionary approaches to modern deep learning-based techniques. The baselines include:</p>
<p>GPlearn GPlearn is a pioneering and standard genetic programming (GP) SR approach. We use the open-source gplearn ${ }^{4}$ package with the following parameters: Population size: 500, Tournament size: 20, Maximum generations: 2 million. Most of the hyperparameters are set from default setting.</p>
<p>PySR PySR (Cranmer, 2023) is an advanced SR method that employs asynchronous multi-island GP-based evolutions. We implement PySR using the open-source pysr ${ }^{5}$ package with the following settings: Number of populations: 15, Population size: 33, Maximum iterations: 2 million. Except for the number of iterations, other parameters are the same as the default setting in PySR. This configuration leverages the power of parallel evolution over a long time allowing for a diverse and robust search of the equation space.</p>
<p>Deep Symbolic Regression (DSR) DSR (Petersen et al., 2021) employs an RNN-based reinforcement learning search over symbolic expressions. We implement DSR using the open-source deep-symbolic-optimization (DSO) ${ }^{6}$ package with standard default parameters: Learning rate: 0.0005 , Batch size: 512, and Maximum iterations: 2 million. This approach allows for a guided search through the space of symbolic expressions, leveraging the power of deep learning to inform the exploration process.</p>
<p>Unified Deep Symbolic Regression (uDSR) uDSR (Landajuela et al., 2022) extends DSR by incorporating additional linear token and GP search at the decoding stage. We also implement uDSR using the DSO package with the same default parameters as DSR. This unified approach aims to combine the strengths of deep learning and traditional GP methods.</p>
<p>Neural Symbolic Regression that Scales (NeSymReS) NeSymReS (Biggio et al., 2021) is the pioneering pre-trained Transformer SR model for expression skeleton generation. We implement it using the NeuralSymbolicRegressionThatScales ${ }^{7}$ repository with the following default parameters: Number of datapoints passed to Transformer: 500, and Expression sampling size: 32. It is important to note that this model is limited to pre-training with $\leq 3$ variables. Consequently, we only apply the model to datasets with $d_{\max }=3$, excluding the Bacterial Growth problem (which has 4 variables) for evaluation of this model.</p>
<p>End-to-End Symbolic Regression (E2E) E2E (Kamienny et al., 2022) is a more recent end-to-end pre-trained Transformer SR approach. We implement it using the symbolicregression ${ }^{8}$ Facebook repository with the following default parameters: Number of datapoints passed to Transformer: 200, and Expression sampling size: 10. This model is also pre-trained for problems with $\leq 10$ variables.</p>
<h2>A. 2 DATA PREPROCESSING AND MODEL EXECUTION</h2>
<p>For the pre-trained Transformer SR models (NeSymReS and E2E), data normalization is crucial. We apply standard normalization to the input data before feeding it to these Transformer models to ensure optimal performance. For the search-based methods, we allow all baselines (GPlearn, PySR, DSR, and uDSR) to run for over 2 million iterations until convergence to their best performance. In the experiments, each baseline undergoes 5 replications. The best results obtained were then documented and reported. This extensive evaluation process, with a large number of iterations and search evaluations, ensures a robust assessment of each model's capability to converge towards optimal solutions and effectively explore the vast equation space of each problem.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Example of input prompts program body for (a) E. Coli Growth and (b) Stress-Strain problems, with problem specification, and the initial equation program example (set as simple linear equation skeleton). For better readability, the details of evaluation function are not included in this figure. Check Fig. 2 for details.</p>
<h1>B Details of LLM-SR Method and Implementation</h1>
<p>Hypothesis Generation and Data-driven Evaluation Fig. 2 provided an example of specification for Nonlinear Oscillator problem. Here, Fig. 7 showcases illustrative examples of prompts and specifications tailored for the Bacterial Growth and Stress-Strain problems. These prompts contain descriptions of the problem and relevant variables, expressed in natural language. By providing this context, the language model can leverage its existing domain knowledge about the physical meaning and relations of variables to generate scientifically plausible hypotheses for new equation programs. Fig. 8 also shows a more detailed example of prompt and specification for LLM-SR that prompts the model to generate differentiable equation programs in PyTorch using tensor operations. The prompt suggests using differentiable operators and replacing non-differentiable components (e.g., if-else conditions) with smooth differentiable approximations.
Our experiments employ either Mixtral-8x7B (using 4 NVIDIA RTX 8000 GPUs with 48GB memory each) or GPT-3.5-turbo (via OpenAI API) as the language model backbone. During each prompting step, the language model generates $b=4$ distinct equation program skeletons using a generation temperature of $\tau=0.8$. This temperature setting is chosen based on preliminary experiments to</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: An example of prompt structure, containing problem specification, evaluation and optimization function, and equation program with pytorch tensor operations.
balance creativity (exploration) and adherence to the problem constraints and reliance on the prior knowledge (exploitation). To control the length and the complexity of the generated equations and prevent overparameterization, we set the maximum number of parameters (length of params vector) as 10 in all experiments. The generated equation skeleton programs are then evaluated to gather feedback. In this framework, we deploy $e=4$ evaluators to operate concurrently. This parallelization allows for rapid and efficient assessment of the generated programs per prompt. Evaluation is constrained by time and memory limits set at $T=30$ seconds and $M=2 \mathrm{~GB}$, respectively. Equation programs that exceed these limits are disqualified and considered as discarded hypotheses by returning None scores. This constraint ensures timely progress and resource efficiency in the search process.</p>
<p>Experience Buffer Management The system stores equation hypotheses and their corresponding data-driven scores in an experience buffer. It uses an islands model with multiple populations ( $m=10$ islands) to maintain diversity. Each island is initialized with a simple equation, which can be customized for domain-specific problems. At each iteration, new hypotheses and their fitness scores are added to their originating island if they improve upon the island's best score. To maintain the quality and diversity of the experience buffer, we follow (Romera-Paredes et al., 2024) and periodically reset the worst-performing islands. Every $T_{\text {reset }}$ iterations (every 4 hrs ), we identify the $m / 2$ islands whose best equation programs have the lowest fitness scores. All the equation programs in these islands are discarded, and each island is reinitialized with a single high-performing equation program, obtained by randomly selecting one of the surviving $m / 2$ islands and copying its</p>
<p>highest-scoring equation program (favoring older programs in case of ties). This reset mechanism allows the framework to discard stagnant or unproductive regions of the equation program space and focus on more promising areas. Within each island, we further cluster the equation programs based on their signature, which is defined as the equation program score. Equation programs with identical signatures are grouped together, forming clusters within each island. This clustering approach helps preserve diversity by ensuring that equation programs with different performance characteristics are maintained in each population.</p>
<p>Experience Sampling To construct informative prompts for the LLM, we sample equation programs from the experience buffer and update the prompt to include new experience demonstration in-context examples. Similar to (Romera-Paredes et al., 2024), here we use a two-stage sampling process. First, we randomly select an island from the $m$ available islands. Then, within the selected island, we sample $k$ equation programs (typically, $k=2$ ) to be included as in-context examples in the prompt. When sampling equation programs within an island, we employ a two-step approach. First, we sample a cluster based on its evaluation score, favoring clusters with higher scores (i.e., higher-quality equation programs). Let $s_{i}$ denote the score of the $i$-th cluster, defined as an aggregation (e.g., mean) of all the scores in the signature that characterizes that cluster. The probability $P_{i}$ of choosing cluster $i$ is given by:</p>
<p>$$
P_{i}=\frac{\exp \left(\frac{s_{i}}{\tau_{c}}\right)}{\sum_{i^{\prime}} \exp \left(\frac{s_{i^{\prime}}}{\tau_{c}}\right)}, \quad \tau_{c}=T_{0}\left(1-\frac{u \bmod N}{N}\right)
$$</p>
<p>where $\tau_{c}$ is the temperature parameter, $u$ is the current number of equation programs in the island, and $T_{0}=0.1$ and $N=10,000$ are hyperparameters. This selection approach is known as the Boltzmann selection procedure (Maza \&amp; Tidor, 1993). Once a cluster is selected, we sample an equation program within that cluster, favoring shorter programs. Let $l_{i}$ denote the negative length of the $i$-th program within the chosen cluster (measured as the number of characters), and let $\tilde{l}<em i="i">{i}=\frac{l</em>-\min <em i_prime="i^{\prime">{i^{\prime}} l</em>{\max }}<em i_prime="i^{\prime">{i^{\prime}} l</em>}}+10^{-6}}$. We set the probability of selecting each equation program proportional to $\exp \left(-\tilde{l<em p="p">{i} / \tau</em>=1$ is a temperature hyperparameter. The sampled programs are then included in the prompt as in-context experience demonstration, providing the LLM with relevant and diverse examples to guide the generation of new equation programs. By maintaining a diverse and high-quality population in the experience buffer and employing a strategic sampling approach, the experience management enables the LLM-SR framework to effectively explore the space of equation programs and iteratively refine its search based on the most promising candidates.}\right)$, where $\tau_{p</p>
<h1>C Limitation of Feynman Benchmark Problems</h1>
<p>The Feynman benchmark (Udrescu \&amp; Tegmark, 2020), consisting of 120 fundamental physics problems from the Feynman Lectures on Physics, is widely used to evaluate symbolic regression techniques in scientific equation discovery. However, our investigation indicates that LLMs have likely memorized many of these well-known physics equations. This memorization poses a challenge when using the Feynman benchmark to assess LLM-based equation discovery methods, as it may not accurately reflect the models' true discovery capabilities. This section elaborates on these limitations and provides evidence supporting the necessity of our newly designed benchmark problems.</p>
<p>Perplexity Analysis To quantify the potential memorization of Feynman problems by LLMs, we first conducted a comparative perplexity analysis. Fig. 9 illustrates the median perplexity of Feynman problems against our new benchmarks (Oscillation 1, Oscillation 2, and E. Coli Growth) using the Mixtral-8x-7B model as the LLM backbone. Perplexity, in this context, is calculated only for the generation of equations given scientific context: $p($ Equation $\mid$ Context $)$. Examples of input prompts and outputs used for perplexity computation across different benchmarks are pro-
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Perplexity (Mixtral) comparison of Feynman benchmark and our new designed benchmark problems
vided in Fig. 10. Mathematically, perplexity is defined as: $\operatorname{PPL}=\exp \left(-\frac{1}{N} \sum_{i=1}^{N} \log p\left(x_{i} \mid x_{&lt;i}\right)\right)$, where $N$ is the number of tokens in the generated equation, and $p\left(x_{i} \mid x_{&lt;i}\right)$ is the probability of token</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Examples of input context and output equations for Feynman equations and our new benchmark problems in Perplexity experiments with Mixtral LLM backbone.
$x_{i}$ given the preceding tokens, derived from the logits of the Mixtral model. The significantly lower perplexity observed for Feynman problems indicates a higher certainty in the LLM's predictions for these equations, and a higher chance of LLM recitation rather than reasoning and discovery. This suggests a high likelihood of memorization of well-known Feynman equations by the LLM, potentially due to their prevalence in scientific training data. It is worth noting that we have excluded the Stress-Strain problem from this analysis due to its experimental nature and lack of a predetermined theoretical model structure, precluding the calculation of perplexity.</p>
<p>Discovery Error Curve Analysis To further validate the need for new benchmarks, we compared equation discovery error curves between Feynman problems and our new benchmark problems. Fig. 11 presents the performance of LLM-SR with a GPT-3.5 backbone across various problems, showing the best score trajectory of Normalized Mean Squared Error (NMSE) against the number of iterations. For Feynman benchmark problems, LLM-SR achieves low NMSE scores within very few iterations, often in a single pass. This rapid convergence further supports the hypothesis that LLMs have likely memorized these fundamental physics equations due to their ubiquity in training data. Qualitative examples in Figs. 12 and 13 also provide additional evidence. The LLM's one-pass responses to several Feynman problems not only demonstrate functional accuracy but also often recite the exact form of the corresponding physics expressions, suggesting direct recall rather than a discovery process. In contrast, our newly designed benchmark problems present novel challenges requiring reasoning and exploration.</p>
<h1>D Additional Details on New Benchmark Problems</h1>
<p>The datasets used in this study include both publicly available and newly generated data. The material stress behavior analysis dataset (stress-strain) is publicly available under the CC BY 4.0 license and can be accessed at https://data.mendeley.com/datasets/rd6jm9tyb6/1. The remaining datasets (Oscillation 1, Oscillation 2, and E. coli Growth) were generated for this work and are released under the MIT License as part of the LLM-SR GitHub repository: https://github.com/deep-symbolicmathematics/LLM-SR</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12: An example of LLM response to Feynman I.37.4 problem, demonstrating LLM recitation without iterative search. For better readability, the details of evaluation function are not included in this figure. Check Fig. 2 for details.</p>
<h1>D. 1 Nonlinear Oscillator Equations</h1>
<p>In this work, we introduce two novel nonlinear oscillator systems as part of our effort to create more challenging and robust benchmarks for LLM-based equation discovery methods. These customdesigned oscillators extend beyond commonly studied systems like Van der Pol, Rayleigh, or Duffing oscillators, presenting unique challenges that test the reasoning and discovery capabilities of LLMbased approaches.</p>
<p>The general form of nonlinear damped oscillator equations is typically expressed as: $\ddot{x}+f(t, x, \dot{x})=$ 0 , where $t$ represents time, $x$ represents position, and $f(t, x, \dot{x})$ represents nonlinear forces. Our custom designs expand upon this framework, incorporating a rich combination of nonlinear terms to create systems that are challenging yet solvable. We simulate two nonlinear oscillators using the solve_ivp function from the scipy library to generate data.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Time range</th>
<th style="text-align: center;">initial values</th>
<th style="text-align: center;">F</th>
<th style="text-align: center;">$\alpha$</th>
<th style="text-align: center;">$\beta$</th>
<th style="text-align: center;">$\delta$</th>
<th style="text-align: center;">$\gamma$</th>
<th style="text-align: center;">$\omega$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Oscillator 1</td>
<td style="text-align: center;">$(0,50)$</td>
<td style="text-align: center;">$(\mathrm{x}=0.5, \mathrm{v}=0.5)$</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">.</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">Oscillator 2</td>
<td style="text-align: center;">$(0,50)$</td>
<td style="text-align: center;">$(\mathrm{x}=0.5, \mathrm{v}=0.5)$</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">1.0</td>
</tr>
</tbody>
</table>
<p>Table 2: Parameter values for Oscillator datasets.</p>
<p>The parameters and initial values for these simulations are provided in Table 2. The governing equations for our oscillator systems are as follows:</p>
<h2>Oscillator 1:</h2>
<p>$$
\dot{v}=F \sin (\omega x)-\alpha v^{3}-\beta x^{3}-\gamma x \cdot v-x \cos (x)
$$</p>
<h2>Oscillator 2:</h2>
<p>$$
\dot{v}=F \sin (\omega t)-\alpha v^{3}-\beta x \cdot v-\delta x \cdot \exp (\gamma x)
$$</p>
<p>Input Prompt</p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nx">Find</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">mathematical</span><span class="w"> </span><span class="nx">function</span><span class="w"> </span><span class="nx">sheleton</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">represents</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">relationship</span><span class="w"> </span><span class="nx">between</span><span class="w"> </span><span class="nx">the</span>
<span class="w">    </span><span class="nx">angle</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">incidence</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">angle</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">refraction</span><span class="p">.</span>
<span class="w">    </span><span class="nx">def</span><span class="w"> </span><span class="nx">evaluate</span><span class="p">(</span><span class="nx">data</span><span class="p">,</span><span class="w"> </span><span class="nx">equation</span><span class="p">):</span>
<span class="w">        </span><span class="nx">Evaluate</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">equation</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">data</span><span class="w"> </span><span class="nx">observations</span><span class="p">.</span>
<span class="w">        </span><span class="o">***</span>
<span class="w">    </span><span class="o">...</span>
<span class="nx">def</span><span class="w"> </span><span class="nx">equation_v0</span><span class="p">(</span><span class="nx">n</span><span class="p">,</span><span class="w"> </span><span class="nx">theta</span><span class="p">,</span><span class="w"> </span><span class="nx">params</span><span class="p">):</span>
<span class="w">    </span><span class="o">**</span><span class="w"> </span><span class="nx">Mathematical</span><span class="w"> </span><span class="nx">function</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">angle</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">refraction</span>
<span class="w">    </span><span class="nx">args</span><span class="p">:</span>
<span class="w">        </span><span class="nx">n</span><span class="p">:</span><span class="w"> </span><span class="nx">Refractive</span><span class="w"> </span><span class="nx">index</span>
<span class="w">            </span><span class="nx">theta</span><span class="p">:</span><span class="w"> </span><span class="nx">Angle</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">incidence</span>
<span class="w">            </span><span class="nx">params</span><span class="p">:</span><span class="w"> </span><span class="nx">Bumeric</span><span class="w"> </span><span class="nx">parameters</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">optimized</span>
<span class="w">    </span><span class="nx">Return</span><span class="p">:</span>
<span class="w">        </span><span class="nx">Refraction</span><span class="w"> </span><span class="nx">angle</span><span class="w"> </span><span class="o">***</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nx">A</span><span class="w"> </span><span class="nx">simple</span><span class="w"> </span><span class="nx">initial</span><span class="w"> </span><span class="nx">example</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">function</span><span class="w"> </span><span class="nx">structure</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nx">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="nx">n</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="nx">theta</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">params</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="nx">def</span><span class="w"> </span><span class="nx">equation_v1</span><span class="p">(</span><span class="nx">n</span><span class="p">,</span><span class="w"> </span><span class="nx">theta</span><span class="p">,</span><span class="w"> </span><span class="nx">params</span><span class="p">):</span>
<span class="w">    </span><span class="o">***</span><span class="nx">Improved</span><span class="w"> </span><span class="nx">version</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">equation_v0</span><span class="o">***</span>
</code></pre></div>

<p>LLM Response</p>
<table>
<thead>
<tr>
<th style="text-align: left;">return params[0] * np.arcsin(params[1] * n * np. $\min ($ theta $)$ ) + params[2]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Feynman I.26.2 Problem $\sqrt{\square \frac{\theta_{1}=\arcsin \left(n \sin \left(\theta_{2}\right)\right)}{\theta}}$</td>
</tr>
</tbody>
</table>
<p>Figure 13: An example of LLM response to Feynman I. 26.2 problem, demonstrating LLM recitation without iterative search. For better readability, the details of evaluation function are not included in this figure. Check Fig. 2 for details.
where $v=\dot{x}$ represents velocity, and $F, \omega, \alpha, \beta, \gamma, \delta$ are constants specific to each oscillator system. These equations are carefully deisgned to incorporate a diverse set of nonlinear structures, including trigonometric, polynomial, and exponential terms. This design choice serves multiple purposes:</p>
<ul>
<li>Challenging Complexity: The combination of various nonlinear terms creates a rich dynamical system that is more complex than common oscillator systems, making the equation discovery task non-trivial.</li>
<li>Realistic Physics: While complex, these equations are still solvable and still represent physically plausible systems, incorporating recognizable elements such as nonlinear damping and positiondependent restoring forces.</li>
<li>Novelty: By deviating from well-known oscillator forms, we reduce the likelihood of LLMs simply reciting memorized equations, thus testing their reasoning and discovery capabilities in the context of data-driven scientific equation discovery.</li>
</ul>
<p>Fig. 14 illustrates the phase plane diagrams of these nonlinear damped oscillators, visually demonstrating the complex dynamics arising from the interplay of nonlinear driving forces, restoring forces, and damping forces. These diagrams highlight the rich behavior that makes these systems challenging for equation identification tasks. To effectively evaluate the generalization capability of predicted equations, we employ a strategic data partitioning scheme. The simulation data is divided into three sets based on the trajectory time: (1) Training set, (2) In-domain validation set, and (3) Out-of-domain validation set. Specifically, we utilize the time interval $T=[0,20]$ to evaluate the out-of-domain generalization of the discovered equations.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 14: Phase diagrams of trajectories corresponding to custom oscillators: (a) Oscillator 1 and (b) Oscillator 2</p>
<h1>D. 2 E. Coli Growth Rate Equations</h1>
<p>In the domain of microbiology, understanding and modeling the growth dynamics of Escherichia coli (E. coli) is of paramount importance due to its wide-ranging applications in biotechnology, food safety, and fundamental biological research. To advance the LLM-based equation discovery approaches in this field, we have developed a novel benchmark problem centered around E. coli growth rate modeling. The growth rate of bacterial populations, including E. coli, is typically modeled by a differential equation that incorporates multiple environmental factors. This mathematical model commonly takes a multiplicative form:</p>
<p>$$
\frac{d B}{d t}=f(B, S, T, \mathrm{pH})=f_{B}(B) \cdot f_{S}(S) \cdot f_{T}(T) \cdot f_{\mathrm{pH}}(\mathrm{pH})
$$</p>
<p>where $B$ represents bacterial population density, $S$ is substrate concentration, $T$ is temperature, and pH represents the acidity or alkalinity of the growth medium. To create a benchmark that is grounded in biological prior knowledge yet is challenging, we have extended this framework with a custom differential equation:</p>
<p>$$
\frac{d B}{d t}=\mu_{\max } B\left(\frac{S}{K_{s}+S}\right) \frac{\tanh \left(k\left(T-x_{0}\right)\right)}{1+c\left(T-x_{\text {decay }}\right)^{4}} \exp \left(-\left|\mathrm{pH}-\mathrm{pH}<em _min="{min" _text="\text">{\text {opt }}\right|\right) \sin \left(\frac{\left(\mathrm{pH}-\mathrm{pH}</em>}}\right) \pi}{\mathrm{pH<em _min="{min" _text="\text">{\max }-\mathrm{pH}</em>
$$}}}\right)^{2</p>
<p>This equation incorporates several key components, each designed to test different aspects of equation discovery systems:</p>
<ul>
<li>Population Density $\left(f_{B}\right)$ : We maintain a linear relationship with $B$, reflecting a simple singlepopulation scenario. This choice allows the focus to remain on the more complex environmental dependencies.</li>
<li>Substrate Concentration $\left(f_{S}\right)$ : We employ the well-established Monod equation, $\left(\frac{S}{K_{s}+S}\right)$, which has been a cornerstone of bacterial growth modeling since its introduction by Jacques Monod in 1949. This inclusion serves as prior knowledge, allowing us to evaluate how well discovery methods can identify known relationships within a more complex overall structure.</li>
<li>Temperature Dependency $\left(f_{T}\right)$ : We introduce a novel formulation, $\frac{\tanh \left(k\left(T-x_{0}\right)\right)}{1+c\left(T-x_{\text {decay }}\right) t}$, which captures the non-monotonic response of bacterial growth to temperature changes. This function combines a hyperbolic tangent term, representing the initial growth acceleration with temperature, and a quartic decay term, modeling the rapid decline in growth rate at high temperatures. This formulation presents a new and challenging form for LLM-based equation discovery methods, as it introduces operators and structures not commonly seen in the literature of this scientific context.</li>
<li>pH Dependency $\left(f_{\mathrm{pH}}\right)$ : Our custom pH function, $\exp \left(-\left|\mathrm{pH}-\mathrm{pH}<em _min="{min" _text="\text">{\text {opt }}\right|\right) \sin \left(\frac{\left(\mathrm{pH}-\mathrm{pH}</em>}}\right) \pi}{\mathrm{pH<em _min="{min" _text="\text">{\text {max }}-\mathrm{pH}</em>$, combines exponential and trigonometric terms to model the complex relationship between bacterial growth and pH levels. This formulation captures both the optimal pH range for growth and the symmetric decline in growth rate as pH deviates from the optimum. It also poses challenging setting for LLM-based equation discovery methods with structures uncommon in the relevant scientific context.}}}\right)^{2</li>
</ul>
<p>Fig. 15 illustrates the behavior of our custom-designed $f_{T}(T)$ and $f_{\mathrm{pH}}(\mathrm{pH})$ functions in comparison to established models from the literature for temperature and pH impact in bacterial growth. As observed, our custom models maintain key characteristics of bacterial growth responses while introducing complexities that challenge equation discovery methods. The temperature dependency model shows a sharper optimal peak and more rapid decline at high temperatures compared to traditional models, while the pH dependency model exhibits a narrower optimal range with steeper declines outside this range.
This carefully constructed benchmark serves multiple purposes: (i) It leverages LLMs' prior knowledge of bacterial growth patterns and common mathematical functions used in biological modeling; (ii) It prevents trivial LLM recitation by introducing novel combinations of functions and operators that go beyond standard models; and (iii) It challenges equation discovery systems to identify complex, biologically plausible relationships from data, simulating the process of scientific discovery.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ https://gplearn.readthedocs.io/en/stable/
${ }^{5}$ https://github.com/MilesCranmer/PySR
${ }^{6}$ https://github.com/dso-org/deep-symbolic-optimization
${ }^{7}$ https://github.com/SymposiumOrganization/NeuralSymbolicRegressionThatScales
${ }^{8}$ https://github.com/facebookresearch/symbolicregression&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3433 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3433</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3433</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-77.html">extraction-schema-77</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <p><strong>Paper ID:</strong> paper-6d017adda6b2b1ea627dde2f0e85401ebb9fe566</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6d017adda6b2b1ea627dde2f0e85401ebb9fe566" target="_blank">MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?</a></p>
                <p><strong>Paper Venue:</strong> European Conference on Computer Vision</p>
                <p><strong>Paper TL;DR:</strong> The MathVerse benchmark is introduced, an all-around visual math benchmark designed for an equitable and in-depth evaluation of MLLMs, and a Chain-of-Thought evaluation strategy is proposed for a fine-grained assessment of the output answers.</p>
                <p><strong>Paper Abstract:</strong> The remarkable progress of Multi-modal Large Language Models (MLLMs) has garnered unparalleled attention, due to their superior performance in visual contexts. However, their capabilities in visual math problem-solving remain insufficiently evaluated and understood. We investigate current benchmarks to incorporate excessive visual content within textual questions, which potentially assist MLLMs in deducing answers without truly interpreting the input diagrams. To this end, we introduce MathVerse, an all-around visual math benchmark designed for an equitable and in-depth evaluation of MLLMs. We meticulously collect 2,612 high-quality, multi-subject math problems with diagrams from publicly available sources. Each problem is then transformed by human annotators into six distinct versions, each offering varying degrees of information content in multi-modality, contributing to 15K test samples in total. This approach allows MathVerse to comprehensively assess whether and how much MLLMs can truly understand the visual diagrams for mathematical reasoning. In addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a fine-grained assessment of the output answers. Rather than naively judging True or False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and then score each step with detailed error analysis, which can reveal the intermediate CoT reasoning quality by MLLMs. We hope the MathVerse benchmark may provide unique insights to guide the future development of MLLMs. Project page: https://mathverse-cuhk.github.io</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3433.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3433.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4V(ision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's multimodal version of GPT-4 that accepts images plus text and is evaluated on visual mathematical problems (plane/solid geometry, functions) in this paper; used both as an evaluated MLLM and as the multimodal scorer in the CoT evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT-4V(ision) system card (2023)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A closed-source multimodal large language model by OpenAI that extends GPT-4 to accept images as input and perform multimodal reasoning; used here as a primary baseline for visual math problems and as the multimodal grader in the CoT evaluation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Visual math problems (plane geometry, solid geometry, functions)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>High-school-level mathematical problems presented with diagrams requiring spatial understanding (geometric element recognition, spatial relations, measurements, and connecting visual elements to algebraic reasoning); dataset covers plane geometry (angles, lengths, areas, analytic problems), solid geometry (volumes), and function graph interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Six variants: (1) Text-dominant: full text + diagram image; (2) Text-lite: reduced redundant descriptive text + diagram; (3) Text-only: text only (no diagram); (4) Vision-intensive: minimal text (EC only) + diagram; (5) Vision-dominant: IP+Question in text, EC annotated visually on diagram; (6) Vision-only: entire problem rendered only in diagram image (diagram image contains EC/IP/question).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot experiments with Chain-of-Thought (CoT) style prompt ('please first conduct reasoning...') for generation; CoT evaluation pipeline used GPT-4 (text-only) to extract key steps and GPT-4V to score steps (multimodal scoring).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Extensive ablation across the six problem versions: removal of descriptive textual information and/or diagrams showed GPT-4V relies on text redundancy but displays relatively better visual encoding than many other MLLMs; CoT-based step scoring and error-type breakdowns (visual perception, reasoning, calculation, knowledge errors) show visual perception errors increase when problems demand more diagram interpretation (Vision-dominant/Only); the paper uses step-wise scoring (α=0.7 weighting of CoT steps) to quantify intermediate reasoning quality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>CoT-evaluated overall score (All, CoT-E) = 54.4 (Table 2); 'w/o' (final-answer-only) overall accuracy = 39.4; Text-dominant CoT-E = 63.1 (Text-dominant 'w/o' acc 54.7), Text-lite CoT-E = 56.6 (w/o 41.4), Text-only CoT-E = 60.3 (w/o 48.7), Vision-intensive CoT-E = 51.4 (w/o 34.9), Vision-dominant CoT-E = 50.8 (w/o 34.4), Vision-only CoT-E = 50.3 (w/o 31.6). Also outperforms most open-source MLLMs and many baselines across subfields (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Primary failures from visual perception errors (misreading/locating annotated measurements, mis-identifying geometric elements), difficulties extracting Essential Conditions (EC) from diagrams, and degraded performance when diagrams replace redundant descriptive text (some performance drop from Text-dominant to Vision-dominant/Only). Reasoning errors and calculation mistakes also present but visual interpretation is the dominant failure mode for vision-heavy versions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>GPT-4V is the top-performing MLLM in this benchmark; it exceeds most open-source MLLMs and surpasses GPT-4 (text-only) when diagrams are available and properly used. Humans scored 71.2/70.9 on Text-dominant/Text-lite but only 41.7 on Text-only (no diagram), while GPT-4V's 'w/o' Text-only score (41.1) is comparable to human Text-only; GPT-4V's CoT-evaluated performance notably exceeds many open-source MLLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3433.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3433.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (text-only)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The text-only GPT-4 language model used as a strong LLM baseline on the textual versions of visual math problems and used as the key-step extractor in the CoT evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's large text-only LLM (closed-source) used here as a text-only baseline and as the language-only component in the CoT key-step extraction (extracts s1..sN from model outputs without seeing questions/diagrams).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Text-only variants of visual math problems (plane geometry, solid geometry, functions)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same dataset tasks but presented only as text (Text-only version): retains DI+IP+EC+Question without the diagram; requires strong textual mathematical reasoning rather than visual perception.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Text-only (textual problem statement); evaluated on Text-dominant, Text-lite, and Text-only versions (no images).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot with CoT prompting ('please first conduct reasoning...'); used for generation and also used as the key-step extraction (text-only GPT-4) in the CoT evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Acts as a control showing that strong textual reasoning can outperform MLLMs when diagrams are present but poorly interpreted; comparison (Text-dominant/Text-lite vs Text-only) highlights that many MLLMs rely on redundant descriptive text instead of images.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported Text-dominant 'w/o' (final answer only) accuracy 46.5; Text-dominant CoT-evaluated scores shown in Table 2 (generation-only LLM baseline outperforms many MLLMs on text-retaining versions); GPT-4 Text-only score (w/o) = 46.5 compares favorably to many MLLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Cannot use image information; therefore fails on vision-intensive, vision-dominant, and vision-only versions where essential conditions or implicit properties are provided only visually.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>GPT-4 often outperforms many open-source MLLMs on text-rich versions and is only surpassed by GPT-4V among evaluated models when multimodal input is present. GPT-4's Text-only performance exceeds several MLLMs that attempted to use images but failed to leverage them well.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3433.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3433.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-VL-Max</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-VL-Max</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source vision-language model evaluated as an MLLM baseline on MathVerse; demonstrates high reliance on textual descriptive information and, in this benchmark, improved performance when diagrams are removed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-VL-Max</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A closed-source vision-language model (Qwen-VL family) designed for diverse vision-language tasks including OCR, localization, and reasoning; evaluated here in zero-shot CoT prompting on the MathVerse visual math tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Visual math problems (plane geometry, solid geometry, functions)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same MathVerse geometry/function tasks requiring visual extraction of DI/IP/EC from diagrams and integration with textual questions.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Images + text in the six version scheme; also evaluated in Text-only (no image) variant.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot with CoT-style instruction ('please first conduct reasoning...'), generation evaluated with CoT extraction and scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Ablation shows the model performs worse when given diagrams in some cases; analysis indicates it relies heavily on Descriptive Information (DI) in text and can be distracted by image input (visual encoding deficiencies). Authors note a +5.1% performance increase when diagrams are removed (Text-only vs Text-dominant), suggesting poor utilization of visual spatial cues.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 2: All CoT-E = 37.2, All 'w/o' accuracy = 25.3; Text-dominant CoT-E = 42.8 (w/o 30.7); Text-only CoT-E = 47.9 (w/o 28.9) — demonstrates higher Text-only than some image-inclusive variants (paper highlights +5.1% increase without diagram). In subfields (Table 3) shows moderate plane-geometry performance and strong function-level performance in some categories.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Fails to accurately encode and use diagrammatic information for EC/IP identification; image input sometimes degrades performance (visual input acts as a distraction instead of help). Visual perception and incorrect association of measurements with geometric elements are common failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Performs worse than GPT-4V and some other closed-source models; compared to GPT-4 (text-only) and other MLLMs, it reveals that poor visual encoding can make an MLLM inferior despite multimodal capability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3433.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3433.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InternLM-XComposer2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternLM-XComposer2 (InternLM-XComposer2-v1-7b)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source MLLM (InternLM family) evaluated on MathVerse; tuned for free-form text-image composition and shows sensitivity to diagrams (sometimes performing better without them).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InternLM-XComposer2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An open-source vision-language model (InternLM-XComposer2) built on InternLM family; in the paper it is mapped to InternLM2-7B and presented as an open-source MLLM baseline trained for text-image composition and comprehension.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Visual math problems (plane geometry, solid geometry, functions)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same MathVerse tasks; requires recognizing spatial relations, annotations, and numeric/algebraic conditions in diagrams and mapping them to problem-solving steps.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Image + text variants as per MathVerse six-version design; EC sometimes annotated visually for vision-dominant/only variants.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot with Chain-of-Thought instruction; CoT extraction and scoring applied after generation.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Benchmark analysis shows InternLM-XComposer2 suffers from inadequate visual encoding in math diagrams; observed to get higher accuracy on Text-only than on some image-inclusive versions (+5.6% improvement in one reported comparison), indicating diagrams can hurt performance when visual perception is poor.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 2: All CoT-E = 25.9, All 'w/o' accuracy = 16.5; Text-dominant CoT-E = 36.9 (w/o 22.3), Text-only CoT-E = 42.5 (w/o 16.5); Vision-only CoT-E = 19.8 (w/o 11.0). Paper notes a +5.6% improvement without diagram for this model in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Visual perception errors and mis-extraction of EC from images; diagrams sometimes act as noise degrading reasoning. Struggles particularly on vision-heavy versions (Vision-dominant, Vision-only).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Performs better than many other open-source MLLMs but lags behind closed-source models like GPT-4V and Gemini-Pro; GPT-4 (text-only) can outperform it on text-retaining versions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3433.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3433.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-Pro (Google Gemini family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source multimodal model from Google evaluated on MathVerse; shows competitive performance but still affected by text redundancy and visual-perception issues in diagrams.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gemini: a family of highly capable multimodal models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Google's Gemini family multimodal model (Gemini-Pro variant), closed-source, evaluated here as a high-end MLLM baseline on visual math tasks with CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Visual math problems (plane geometry, solid geometry, functions)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same set of diagram-based math problems requiring geometric and spatial understanding; tasks include line relations, similarity, coordinates, area/volume computations, function graph interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Images + text in the six-version scheme; text-only variants also evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot with CoT-style instruction; outputs passed to GPT-4 for key-step extraction and GPT-4V for step scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Error analysis shows that Gemini-Pro can sometimes derive correct answers from descriptive textual information (DI) and can be misled by incorrect visual perception when diagrams are provided; visual perception errors can turn otherwise correct Text-only reasoning into incorrect results when image input is present.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 2: All CoT-E = 35.3, All 'w/o' accuracy = 23.5; Text-dominant CoT-E = 39.8 (w/o 26.3), Text-only CoT-E = 44.5 (w/o 27.3). In Table 3 Gemini-Pro shows decent plane-geometry and function-subfield scores (e.g., All plane geometry = 33.0).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Visual encoding inadequacies leading to perception errors on diagrams; reliance on textual DI evident. When descriptive textual content is removed, accuracy can drop, indicating diagram understanding is still imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Better than many open-source MLLMs but behind best closed-source multimodal (GPT-4V) in this benchmark; shows similar tendencies as other closed-source models with respect to reliance on textual redundancy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3433.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3433.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPHINX-MoE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SPHINX-MoE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source mixture-of-experts multimodal LLM evaluated on MathVerse; one of the stronger open-source baselines with moderate CoT-consistency and relatively lower variance between CoT and final-answer-only scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sphinx-x: Scaling data and parameters for a family of multi-modal large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SPHINX-MoE</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source MLLM (SPHINX family) with a mixture-of-experts architecture (paper references Mixtral-8×7B as underlying model family), trained on large multimodal datasets including math-specific data; used as an open-source strong baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Mixtral-8x7B (per paper mapping)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Visual math problems (plane geometry, solid geometry, functions)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same image+text math tasks; requires visual identification of geometry elements, spatial relations, and annotated numeric/algebraic conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Images + text; evaluated across six version types of MathVerse and in text-only variants.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot with CoT instruction; CoT extraction and GPT-4V scoring applied.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Shows relatively consistent step-by-step reasoning (lower gap between CoT-evaluated score and final-answer-only score, ~6.0% variance) indicating steadier intermediate reasoning quality compared to some other open-source models; nonetheless, visual perception and identification of EC remain challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 2: All CoT-E = 22.8, All 'w/o' accuracy = 15.0; Text-dominant CoT-E = 33.3 (w/o 22.2), Text-only CoT-E = 40.7 (w/o 18.3). In Table 3 shows competitive open-source subfield scores (plane geometry All = 24.5).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Visual perception errors and calculation/reasoning mistakes on vision-heavy versions; still lags substantially behind closed-source top performers (e.g., GPT-4V).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>One of the best open-source MLLMs on this benchmark (alongside InternLM-XComposer2), but overall behind closed-source models and human performance on many vision-heavy subfields.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3433.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3433.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ShareGPT4V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ShareGPT4V</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source model variant approximating GPT-4V behavior via fine-tuning/caption augmentation; evaluated here and shown to have relatively better visual comprehension than many other open-source MLLMs but still behind GPT-4V.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sharegpt4v: Improving large multi-modal models with better captions.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ShareGPT4V</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A community/open-source model that adapts multimodal instruction-following behavior and image caption improvements to approach GPT-4V-like capabilities; evaluated on MathVerse as an open-source multimodal baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Vicuna-13B (per mapping in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Visual math problems (plane geometry, solid geometry, functions)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same MathVerse tasks; requires reading diagrams, extracting EC/IP, mapping to algebraic reasoning and numeric computation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Images + text in the six-version scheme; also evaluated in Text-only variants.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot with CoT prompts; CoT-based extraction and GPT-4V scoring used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Performs better on text-rich versions than many other open-source MLLMs; shows some capacity to leverage visual content but still suffers from failures when diagrams must supply EC/IP exclusively. CoT evaluation indicates relatively higher CoT vs final-answer variance for some models, but ShareGPT4V shows mixed results.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 2: All CoT-E = 17.4, All 'w/o' accuracy = 13.1; Text-dominant CoT-E = 21.8 (w/o 16.2), Text-only CoT-E = 14.6 (w/o 6.6), Vision-only CoT-E = 9.7 (w/o 3.7).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Visual perception and EC extraction remain hard; performance drops steeply in Vision-only and Vision-dominant settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other models_or_humans</strong></td>
                            <td>Performs better than some smaller open-source baselines but considerably worse than GPT-4V and often worse than text-only GPT-4 on text-dominant problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Geoqa: A geometric question answering benchmark towards multimodal numerical reasoning. <em>(Rating: 2)</em></li>
                <li>Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models. <em>(Rating: 2)</em></li>
                <li>G-llava: Solving geometric problem with multi-modal large language model. <em>(Rating: 2)</em></li>
                <li>Geometry3K <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3433",
    "paper_id": "paper-6d017adda6b2b1ea627dde2f0e85401ebb9fe566",
    "extraction_schema_id": "extraction-schema-77",
    "extracted_data": [
        {
            "name_short": "GPT-4V",
            "name_full": "GPT-4V(ision)",
            "brief_description": "OpenAI's multimodal version of GPT-4 that accepts images plus text and is evaluated on visual mathematical problems (plane/solid geometry, functions) in this paper; used both as an evaluated MLLM and as the multimodal scorer in the CoT evaluation.",
            "citation_title": "GPT-4V(ision) system card (2023)",
            "mention_or_use": "use",
            "model_name": "GPT-4V",
            "model_description": "A closed-source multimodal large language model by OpenAI that extends GPT-4 to accept images as input and perform multimodal reasoning; used here as a primary baseline for visual math problems and as the multimodal grader in the CoT evaluation pipeline.",
            "model_size": null,
            "puzzle_name": "Visual math problems (plane geometry, solid geometry, functions)",
            "puzzle_description": "High-school-level mathematical problems presented with diagrams requiring spatial understanding (geometric element recognition, spatial relations, measurements, and connecting visual elements to algebraic reasoning); dataset covers plane geometry (angles, lengths, areas, analytic problems), solid geometry (volumes), and function graph interpretation.",
            "input_representation": "Six variants: (1) Text-dominant: full text + diagram image; (2) Text-lite: reduced redundant descriptive text + diagram; (3) Text-only: text only (no diagram); (4) Vision-intensive: minimal text (EC only) + diagram; (5) Vision-dominant: IP+Question in text, EC annotated visually on diagram; (6) Vision-only: entire problem rendered only in diagram image (diagram image contains EC/IP/question).",
            "prompting_method": "Zero-shot experiments with Chain-of-Thought (CoT) style prompt ('please first conduct reasoning...') for generation; CoT evaluation pipeline used GPT-4 (text-only) to extract key steps and GPT-4V to score steps (multimodal scoring).",
            "spatial_reasoning_analysis": "Extensive ablation across the six problem versions: removal of descriptive textual information and/or diagrams showed GPT-4V relies on text redundancy but displays relatively better visual encoding than many other MLLMs; CoT-based step scoring and error-type breakdowns (visual perception, reasoning, calculation, knowledge errors) show visual perception errors increase when problems demand more diagram interpretation (Vision-dominant/Only); the paper uses step-wise scoring (α=0.7 weighting of CoT steps) to quantify intermediate reasoning quality.",
            "performance_metrics": "CoT-evaluated overall score (All, CoT-E) = 54.4 (Table 2); 'w/o' (final-answer-only) overall accuracy = 39.4; Text-dominant CoT-E = 63.1 (Text-dominant 'w/o' acc 54.7), Text-lite CoT-E = 56.6 (w/o 41.4), Text-only CoT-E = 60.3 (w/o 48.7), Vision-intensive CoT-E = 51.4 (w/o 34.9), Vision-dominant CoT-E = 50.8 (w/o 34.4), Vision-only CoT-E = 50.3 (w/o 31.6). Also outperforms most open-source MLLMs and many baselines across subfields (Table 3).",
            "limitations_or_failure_modes": "Primary failures from visual perception errors (misreading/locating annotated measurements, mis-identifying geometric elements), difficulties extracting Essential Conditions (EC) from diagrams, and degraded performance when diagrams replace redundant descriptive text (some performance drop from Text-dominant to Vision-dominant/Only). Reasoning errors and calculation mistakes also present but visual interpretation is the dominant failure mode for vision-heavy versions.",
            "comparison_to_other_models_or_humans": "GPT-4V is the top-performing MLLM in this benchmark; it exceeds most open-source MLLMs and surpasses GPT-4 (text-only) when diagrams are available and properly used. Humans scored 71.2/70.9 on Text-dominant/Text-lite but only 41.7 on Text-only (no diagram), while GPT-4V's 'w/o' Text-only score (41.1) is comparable to human Text-only; GPT-4V's CoT-evaluated performance notably exceeds many open-source MLLMs.",
            "uuid": "e3433.0",
            "source_info": {
                "paper_title": "MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (text-only)",
            "brief_description": "The text-only GPT-4 language model used as a strong LLM baseline on the textual versions of visual math problems and used as the key-step extractor in the CoT evaluation.",
            "citation_title": "Gpt-4 technical report.",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI's large text-only LLM (closed-source) used here as a text-only baseline and as the language-only component in the CoT key-step extraction (extracts s1..sN from model outputs without seeing questions/diagrams).",
            "model_size": null,
            "puzzle_name": "Text-only variants of visual math problems (plane geometry, solid geometry, functions)",
            "puzzle_description": "Same dataset tasks but presented only as text (Text-only version): retains DI+IP+EC+Question without the diagram; requires strong textual mathematical reasoning rather than visual perception.",
            "input_representation": "Text-only (textual problem statement); evaluated on Text-dominant, Text-lite, and Text-only versions (no images).",
            "prompting_method": "Zero-shot with CoT prompting ('please first conduct reasoning...'); used for generation and also used as the key-step extraction (text-only GPT-4) in the CoT evaluator.",
            "spatial_reasoning_analysis": "Acts as a control showing that strong textual reasoning can outperform MLLMs when diagrams are present but poorly interpreted; comparison (Text-dominant/Text-lite vs Text-only) highlights that many MLLMs rely on redundant descriptive text instead of images.",
            "performance_metrics": "Reported Text-dominant 'w/o' (final answer only) accuracy 46.5; Text-dominant CoT-evaluated scores shown in Table 2 (generation-only LLM baseline outperforms many MLLMs on text-retaining versions); GPT-4 Text-only score (w/o) = 46.5 compares favorably to many MLLMs.",
            "limitations_or_failure_modes": "Cannot use image information; therefore fails on vision-intensive, vision-dominant, and vision-only versions where essential conditions or implicit properties are provided only visually.",
            "comparison_to_other_models_or_humans": "GPT-4 often outperforms many open-source MLLMs on text-rich versions and is only surpassed by GPT-4V among evaluated models when multimodal input is present. GPT-4's Text-only performance exceeds several MLLMs that attempted to use images but failed to leverage them well.",
            "uuid": "e3433.1",
            "source_info": {
                "paper_title": "MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Qwen-VL-Max",
            "name_full": "Qwen-VL-Max",
            "brief_description": "A closed-source vision-language model evaluated as an MLLM baseline on MathVerse; demonstrates high reliance on textual descriptive information and, in this benchmark, improved performance when diagrams are removed.",
            "citation_title": "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond.",
            "mention_or_use": "use",
            "model_name": "Qwen-VL-Max",
            "model_description": "A closed-source vision-language model (Qwen-VL family) designed for diverse vision-language tasks including OCR, localization, and reasoning; evaluated here in zero-shot CoT prompting on the MathVerse visual math tasks.",
            "model_size": null,
            "puzzle_name": "Visual math problems (plane geometry, solid geometry, functions)",
            "puzzle_description": "Same MathVerse geometry/function tasks requiring visual extraction of DI/IP/EC from diagrams and integration with textual questions.",
            "input_representation": "Images + text in the six version scheme; also evaluated in Text-only (no image) variant.",
            "prompting_method": "Zero-shot with CoT-style instruction ('please first conduct reasoning...'), generation evaluated with CoT extraction and scoring.",
            "spatial_reasoning_analysis": "Ablation shows the model performs worse when given diagrams in some cases; analysis indicates it relies heavily on Descriptive Information (DI) in text and can be distracted by image input (visual encoding deficiencies). Authors note a +5.1% performance increase when diagrams are removed (Text-only vs Text-dominant), suggesting poor utilization of visual spatial cues.",
            "performance_metrics": "Table 2: All CoT-E = 37.2, All 'w/o' accuracy = 25.3; Text-dominant CoT-E = 42.8 (w/o 30.7); Text-only CoT-E = 47.9 (w/o 28.9) — demonstrates higher Text-only than some image-inclusive variants (paper highlights +5.1% increase without diagram). In subfields (Table 3) shows moderate plane-geometry performance and strong function-level performance in some categories.",
            "limitations_or_failure_modes": "Fails to accurately encode and use diagrammatic information for EC/IP identification; image input sometimes degrades performance (visual input acts as a distraction instead of help). Visual perception and incorrect association of measurements with geometric elements are common failure modes.",
            "comparison_to_other_models_or_humans": "Performs worse than GPT-4V and some other closed-source models; compared to GPT-4 (text-only) and other MLLMs, it reveals that poor visual encoding can make an MLLM inferior despite multimodal capability.",
            "uuid": "e3433.2",
            "source_info": {
                "paper_title": "MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "InternLM-XComposer2",
            "name_full": "InternLM-XComposer2 (InternLM-XComposer2-v1-7b)",
            "brief_description": "An open-source MLLM (InternLM family) evaluated on MathVerse; tuned for free-form text-image composition and shows sensitivity to diagrams (sometimes performing better without them).",
            "citation_title": "Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model.",
            "mention_or_use": "use",
            "model_name": "InternLM-XComposer2",
            "model_description": "An open-source vision-language model (InternLM-XComposer2) built on InternLM family; in the paper it is mapped to InternLM2-7B and presented as an open-source MLLM baseline trained for text-image composition and comprehension.",
            "model_size": "7B",
            "puzzle_name": "Visual math problems (plane geometry, solid geometry, functions)",
            "puzzle_description": "Same MathVerse tasks; requires recognizing spatial relations, annotations, and numeric/algebraic conditions in diagrams and mapping them to problem-solving steps.",
            "input_representation": "Image + text variants as per MathVerse six-version design; EC sometimes annotated visually for vision-dominant/only variants.",
            "prompting_method": "Zero-shot with Chain-of-Thought instruction; CoT extraction and scoring applied after generation.",
            "spatial_reasoning_analysis": "Benchmark analysis shows InternLM-XComposer2 suffers from inadequate visual encoding in math diagrams; observed to get higher accuracy on Text-only than on some image-inclusive versions (+5.6% improvement in one reported comparison), indicating diagrams can hurt performance when visual perception is poor.",
            "performance_metrics": "Table 2: All CoT-E = 25.9, All 'w/o' accuracy = 16.5; Text-dominant CoT-E = 36.9 (w/o 22.3), Text-only CoT-E = 42.5 (w/o 16.5); Vision-only CoT-E = 19.8 (w/o 11.0). Paper notes a +5.6% improvement without diagram for this model in some settings.",
            "limitations_or_failure_modes": "Visual perception errors and mis-extraction of EC from images; diagrams sometimes act as noise degrading reasoning. Struggles particularly on vision-heavy versions (Vision-dominant, Vision-only).",
            "comparison_to_other_models_or_humans": "Performs better than many other open-source MLLMs but lags behind closed-source models like GPT-4V and Gemini-Pro; GPT-4 (text-only) can outperform it on text-retaining versions.",
            "uuid": "e3433.3",
            "source_info": {
                "paper_title": "MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Gemini-Pro",
            "name_full": "Gemini-Pro (Google Gemini family)",
            "brief_description": "A closed-source multimodal model from Google evaluated on MathVerse; shows competitive performance but still affected by text redundancy and visual-perception issues in diagrams.",
            "citation_title": "Gemini: a family of highly capable multimodal models.",
            "mention_or_use": "use",
            "model_name": "Gemini-Pro",
            "model_description": "Google's Gemini family multimodal model (Gemini-Pro variant), closed-source, evaluated here as a high-end MLLM baseline on visual math tasks with CoT prompting.",
            "model_size": null,
            "puzzle_name": "Visual math problems (plane geometry, solid geometry, functions)",
            "puzzle_description": "Same set of diagram-based math problems requiring geometric and spatial understanding; tasks include line relations, similarity, coordinates, area/volume computations, function graph interpretation.",
            "input_representation": "Images + text in the six-version scheme; text-only variants also evaluated.",
            "prompting_method": "Zero-shot with CoT-style instruction; outputs passed to GPT-4 for key-step extraction and GPT-4V for step scoring.",
            "spatial_reasoning_analysis": "Error analysis shows that Gemini-Pro can sometimes derive correct answers from descriptive textual information (DI) and can be misled by incorrect visual perception when diagrams are provided; visual perception errors can turn otherwise correct Text-only reasoning into incorrect results when image input is present.",
            "performance_metrics": "Table 2: All CoT-E = 35.3, All 'w/o' accuracy = 23.5; Text-dominant CoT-E = 39.8 (w/o 26.3), Text-only CoT-E = 44.5 (w/o 27.3). In Table 3 Gemini-Pro shows decent plane-geometry and function-subfield scores (e.g., All plane geometry = 33.0).",
            "limitations_or_failure_modes": "Visual encoding inadequacies leading to perception errors on diagrams; reliance on textual DI evident. When descriptive textual content is removed, accuracy can drop, indicating diagram understanding is still imperfect.",
            "comparison_to_other_models_or_humans": "Better than many open-source MLLMs but behind best closed-source multimodal (GPT-4V) in this benchmark; shows similar tendencies as other closed-source models with respect to reliance on textual redundancy.",
            "uuid": "e3433.4",
            "source_info": {
                "paper_title": "MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "SPHINX-MoE",
            "name_full": "SPHINX-MoE",
            "brief_description": "An open-source mixture-of-experts multimodal LLM evaluated on MathVerse; one of the stronger open-source baselines with moderate CoT-consistency and relatively lower variance between CoT and final-answer-only scores.",
            "citation_title": "Sphinx-x: Scaling data and parameters for a family of multi-modal large language models.",
            "mention_or_use": "use",
            "model_name": "SPHINX-MoE",
            "model_description": "Open-source MLLM (SPHINX family) with a mixture-of-experts architecture (paper references Mixtral-8×7B as underlying model family), trained on large multimodal datasets including math-specific data; used as an open-source strong baseline.",
            "model_size": "Mixtral-8x7B (per paper mapping)",
            "puzzle_name": "Visual math problems (plane geometry, solid geometry, functions)",
            "puzzle_description": "Same image+text math tasks; requires visual identification of geometry elements, spatial relations, and annotated numeric/algebraic conditions.",
            "input_representation": "Images + text; evaluated across six version types of MathVerse and in text-only variants.",
            "prompting_method": "Zero-shot with CoT instruction; CoT extraction and GPT-4V scoring applied.",
            "spatial_reasoning_analysis": "Shows relatively consistent step-by-step reasoning (lower gap between CoT-evaluated score and final-answer-only score, ~6.0% variance) indicating steadier intermediate reasoning quality compared to some other open-source models; nonetheless, visual perception and identification of EC remain challenging.",
            "performance_metrics": "Table 2: All CoT-E = 22.8, All 'w/o' accuracy = 15.0; Text-dominant CoT-E = 33.3 (w/o 22.2), Text-only CoT-E = 40.7 (w/o 18.3). In Table 3 shows competitive open-source subfield scores (plane geometry All = 24.5).",
            "limitations_or_failure_modes": "Visual perception errors and calculation/reasoning mistakes on vision-heavy versions; still lags substantially behind closed-source top performers (e.g., GPT-4V).",
            "comparison_to_other_models_or_humans": "One of the best open-source MLLMs on this benchmark (alongside InternLM-XComposer2), but overall behind closed-source models and human performance on many vision-heavy subfields.",
            "uuid": "e3433.5",
            "source_info": {
                "paper_title": "MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "ShareGPT4V",
            "name_full": "ShareGPT4V",
            "brief_description": "An open-source model variant approximating GPT-4V behavior via fine-tuning/caption augmentation; evaluated here and shown to have relatively better visual comprehension than many other open-source MLLMs but still behind GPT-4V.",
            "citation_title": "Sharegpt4v: Improving large multi-modal models with better captions.",
            "mention_or_use": "use",
            "model_name": "ShareGPT4V",
            "model_description": "A community/open-source model that adapts multimodal instruction-following behavior and image caption improvements to approach GPT-4V-like capabilities; evaluated on MathVerse as an open-source multimodal baseline.",
            "model_size": "Vicuna-13B (per mapping in paper)",
            "puzzle_name": "Visual math problems (plane geometry, solid geometry, functions)",
            "puzzle_description": "Same MathVerse tasks; requires reading diagrams, extracting EC/IP, mapping to algebraic reasoning and numeric computation.",
            "input_representation": "Images + text in the six-version scheme; also evaluated in Text-only variants.",
            "prompting_method": "Zero-shot with CoT prompts; CoT-based extraction and GPT-4V scoring used for evaluation.",
            "spatial_reasoning_analysis": "Performs better on text-rich versions than many other open-source MLLMs; shows some capacity to leverage visual content but still suffers from failures when diagrams must supply EC/IP exclusively. CoT evaluation indicates relatively higher CoT vs final-answer variance for some models, but ShareGPT4V shows mixed results.",
            "performance_metrics": "Table 2: All CoT-E = 17.4, All 'w/o' accuracy = 13.1; Text-dominant CoT-E = 21.8 (w/o 16.2), Text-only CoT-E = 14.6 (w/o 6.6), Vision-only CoT-E = 9.7 (w/o 3.7).",
            "limitations_or_failure_modes": "Visual perception and EC extraction remain hard; performance drops steeply in Vision-only and Vision-dominant settings.",
            "comparison_to_other models_or_humans": "Performs better than some smaller open-source baselines but considerably worse than GPT-4V and often worse than text-only GPT-4 on text-dominant problems.",
            "uuid": "e3433.6",
            "source_info": {
                "paper_title": "MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Geoqa: A geometric question answering benchmark towards multimodal numerical reasoning.",
            "rating": 2
        },
        {
            "paper_title": "Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models.",
            "rating": 2
        },
        {
            "paper_title": "G-llava: Solving geometric problem with multi-modal large language model.",
            "rating": 2
        },
        {
            "paper_title": "Geometry3K",
            "rating": 1
        }
    ],
    "cost": 0.019041,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>(3) MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?</h1>
<p>Renrui Zhang ${ }^{<em> 11,2}$, Dongzhi Jiang ${ }^{</em> 1}$, Yichi Zhang ${ }^{* 2}$, Haokun Lin ${ }^{2}$, Ziyu Guo ${ }^{2}$, Pengshuo Qiu ${ }^{2}$<br>Aojun Zhou ${ }^{1}$, Pan Lu ${ }^{3}$, Kai-Wei Chang ${ }^{3}$, Peng Gao ${ }^{12}$, Hongsheng Li ${ }^{11}$<br>${ }^{1}$ CUHK MMLab ${ }^{2}$ Shanghai Artificial Intelligence Laboratory<br>${ }^{3}$ University of California, Los Angeles<br>{zhangrenrui, dzjiang, ziyuguo}@link.cuhk.edu.hk<br>lupantech@gmail.com, gaopeng@pjlab.org.cn, hsli@ee.cuhk.edu.hk</p>
<h4>Abstract</h4>
<p>The remarkable progress of Multi-modal Large Language Models (MLLMs) has garnered unparalleled attention, due to their superior performance in visual contexts. However, their capabilities in visual math problem-solving remain insufficiently evaluated and understood. We investigate current benchmarks to incorporate excessive visual content within textual questions, which potentially assist MLLMs in deducing answers without truly interpreting the input diagrams. To this end, we introduce MathVerse, an all-around visual math benchmark designed for an equitable and in-depth evaluation of MLLMs. We meticulously collect 2,612 high-quality, multi-subject math problems with diagrams from publicly available sources. Each problem is then transformed by human annotators into six distinct versions, each offering varying degrees of information content in multi-modality, contributing to $\mathbf{1 5 K}$ test samples in total. This approach allows MATHVERSE to comprehensively assess whether and how much MLLMs can truly understand the visual diagrams for mathematical reasoning. In addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a fine-grained assessment of the output answers. Rather than naively judging True or False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and then score each step with detailed error analysis, which can reveal the intermediate CoT reasoning quality by MLLMs. With MATHVERSE, we unveil that, most existing MLLMs struggle to understand math diagrams, relying heavily on textual questions. Surprisingly, some of them even achieve $5 \%+$ higher accuracy without the visual input, e.g., Qwen-VL-Max and InternLM-XComposer2. In contrast, GPT-4V and ShareGPT4V demonstrate relatively better comprehension of the visual content for mathematical reasoning. We hope MATHVERSE may provide unique insights to guide the future development of MLLMs. Project page: https://mathverse-cuhk.github.io.</p>
<h2>1 Introduction</h2>
<p>With the substantial advances of big data and computational power, Large Language Models (LLMs) [4, 28, 55, 56, 13], such as ChatGPT [45] and GPT-4 [46], have emerged as a central point of interest in both industry and academia. To broaden their applicability across diverse contexts, Multi-modal Large Language Models (MLLMs) [66, 20, 52, 11, 61, 70] have recently become</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) We showcase three examples of Text Redundancy (highlighted in red) within existing visual math benchmarks [9, 41, 63]. (b) We report an ablation study by respectively removing the redundant texts and input diagrams on 120 randomly selected problems, for closed-sourced [47, 22, 3] and open-sourced $[21,38,16]$ MLLMs.
a fast-evolving track, exemplified by the latest GPT-4V [47], Gemini [22], and the open-source LLaVA [39, 34, 32, 30] and SPHINX [36, 21]. Concurrently, a diverse array of evaluation benchmarks [17, 40, 33, 18, 53] are curated to assess their visual comprehension performance across different domains. Notably, the capability to solve mathematical problems involving diagrams serves as a critical measure, offering insights into the multi-modal logical thinking prowess of MLLMs. This task demands MLLMs to accurately decode the visual elements within input diagrams (characters and figures), and correlate them with the condition specified by textual questions for mathematical reasoning. Previous efforts [42, 51], e.g., GeoQA [9, 5] and UniGeo [7], concentrate on the challenging geometric problems, while the recent MathVista [41] and MMMU [63] expand the scope to encompass broader disciplines, including functions, charts, and scientific problems.</p>
<p>However, through our comprehensive observation and analysis, we identify three primary issues in current mathematical benchmarks for evaluating MLLMs:
i. Do MLLMs truly see the math diagrams in evaluation? This is the most fundamental question concerning the accurate assessment of visual math problem-solving. In Figure 1 (a), we showcase three examples from current benchmarks. We observe their texts contain too much duplicate information (highlighted in red) that is also depicted in the diagram. This redundancy might inadvertently provide MLLMs with a shortcut to resolve the problem by mostly reading the text, rather than interpreting the diagram. Our hypothesis gains support from the experiment in Figure 1 (b). For 40 randomly sampled problems from each benchmark, we remove such redundant texts from the question, challenging MLLMs to capture the corresponding information exclusively from visual inputs. The results reveal a significant drop in accuracy among most MLLMs (the blue column), even falling below the scores without taking diagrams as input (the grey column). This outcome suggests that MLLMs primarily depend on textual cues rather than the visual diagrams themselves to solve these problems in evaluation. Given this, we demonstrate that current visual math benchmarks might not be comprehensive enough to assess the genuine multi-modal mathematical reasoning capabilities of MLLMs.
ii. Is it equitable to assess solely by the final answer? Most existing multi-modal benchmarks directly compare model outputs with ground truths to derive a binary evaluation result. While this approach may suffice for general visual contexts, it falls short in math problems that require intricate step-by-step reasoning. In Figure 2, we examine three model outputs. Although they all arrive at incorrect answers in the end, they demonstrate varying levels of precision in the intermediate reasoning processes. Merely categorizing these outputs as 'Incorrect' fails to capture the nuanced differences in the reasoning quality of MLLMs.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Comparison of Visual Mathematical Reasoning by Three MLLMs. Despite the incorrect final answer, GPT-4V [47], Gemini-Pro [22], and SPHINX-MoE [21] exhibit different levels of quality in the intermediate reasoning process.
iii. Do they specialize in mathematical reasoning evaluation? GeoQA, UniGeo, and other previous works narrowly target specific aspects of plane geometry. This limits the evaluation of broader mathematical capabilities, e.g., functions and solid geometry. Instead, MathVista expands its scope by including a wide array of peripheral tasks (19 out of 28), encompassing natural images, statistic plots, and charts, which do not directly evaluate professional math skills. Furthermore, the math problems in MMMU are of college-level complexity with extensive domain-specific knowledge, potentially hindering MLLMs from fully demonstrating their reasoning capacity.</p>
<p>Therefore, in light of the issues discussed, we present MathVerse, a holistic and specialized visual math benchmark crafted to evaluate the multi-modal mathematical reasoning skills of MLLMs. This benchmark encompasses a meticulously collected dataset of 2,612 visual math problems, with 1,236 newly acquired from public question repositories and 1,376 selected from existing benchmarks, ensuring a diverse range of challenges. To specialize in mathematical reasoning, MathVerse spans three primary areas: plane geometry, solid geometry, and functions. Each problem has been rigorously reviewed by expert annotators and classified into twelve detailed categories, emphasizing different fine-grained problem-solving capabilities. Notably, MathVerse distinguishes itself by introducing two novel strategies for evaluating MLLMs.</p>
<p>First, we investigate the influence of textual redundancy and validate whether MLLMs can interpret the diagrams for mathematical reasoning. As illustrated in Figure 3 (Left), we categorize the textual content within the questions into three different types: Descriptive Information, Implicit Property, and Essential Condition. These categories, arranged in ascending order of significance for problemsolving, correspond to information directly observable from the diagram, implicit spatial properties that demand advanced visual perception, and specific measurements crucial for computing the solution, respectively. Based on this problem formulation, expert annotators progressively remove the textual information from the questions in MathVerse, while incrementally incorporating elements into the visual diagrams to ensure problems are adequately defined. As shown in Figure 3 (Right), this process results in six unique versions of each problem characterized by a reduction in textual content and an enhancement in visual elements, creating a total of 15 K test samples. These delicately curated problems can indicate the various multi-modal capabilities of MLLMs, such as geometric element understanding, function curve perception, and numerical value recognition, which thoroughly unveils whether and how much they comprehend the visual diagram for mathematical reasoning.</p>
<p>Second, to rigorously assess the visual Chain-of-Thought (CoT) capabilities [58], we propose a CoT Evaluation strategy for the step-by-step reasoning assessment of MLLMs. For each model's output, we leverage GPT-4 to first extract several crucial steps exclusively from the solving process, deliberately omitting the input of the question and answer. This approach aims to mitigate the bias towards GPT-4's inherent question-answering propensities. Then, the corresponding question, diagram, and ground-truth answer are fed into GPT-4 to evaluate each identified critical step, and provide detailed error analysis. Finally, the overall score is obtained by considering every single step within reasoning. Note that, we do not pre-define a ground-truth key-step template, since each</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Three Categories of Question Texts in MathVerse. According to the significance for problem-solving, we categorize the question texts into three categories, and transform each problem into six versions for evaluation, with varying content in multi-modality. We present three examples in MATHVERSE for illustration.
math problem may encompass a variety of solution pathways, and different MLLMs tend to exhibit variable reasoning lengths. With CoT scoring, MATHVERSE showcases a fine-grained evaluation of the intermediate logical deduction of MLLMs, demonstrating visual mathematical CoT capabilities.</p>
<p>We conduct extensive experiments on MATHVERSE with popular closed-source [47, 3, 22] and open-source [37, 38, 16, 21] MLLMs. Comparing different problem versions, we unveil that, most existing MLLMs struggle to understand math diagrams, relying heavily on textual questions. Therein, GPT-4V [47] achieves the best overall performance across different problem versions and subjects. Surprisingly, some of the MLLMs even attain much higher results without the diagram input, e.g., $+5.1 \%$ for Qwen-VL-Max [3] and $+5.6 \%$ for InternLM-XComposer2 [16]. With the fine-grained error analysis produced by our CoT evaluation strategy, we demonstrate such results are due to their deficient visual encoding capacity for mathematical diagrams, which instead acts as a distraction for problem-solving. In contrast, GPT-4V and ShareGPT4V [12] demonstrate relatively better comprehension of the visual content for mathematical reasoning. Our experimental results suggest that inadequate mathematical visual interpretation capabilities represent the most significant impediment for MLLMs in addressing multi-modal math problems, indicating substantial potential for advancement.</p>
<p>The contributions of this paper are summarized as follows:</p>
<ul>
<li>We investigate primary issues within existing benchmarks and introduce MATHVERSE, an all-around multi-modal benchmark evaluating the visual mathematical reasoning of MLLMs. The meticulously curated dataset contains 20K test problems with diagrams for a comprehensive assessment.</li>
<li>By modifying problems with varying information content in multi-modality, we explore whether and how much MLLMs can understand the visual diagrams for mathematical reasoning, rather than relying on question texts.</li>
<li>We propose a CoT evaluation strategy with GPT-4 to extract and assess each key step in the reasoning process of MLLMs, which provides a detailed error analysis and fine-grained evaluation of their multi-modal mathematical CoT capabilities.</li>
</ul>
<h1>2 MathVerse</h1>
<p>In Section 2.1, we first present an overview of the curated visual math dataset in MATHVERSE. Then, in Section 2.2, we introduce our data formulation approach for investigating the visual mathematical comprehension of Multi-modal Large Language Models (MLLMs). Finally, in Section 2.3, we elaborate on the methodology of our proposed Chain-of-Thought (CoT) evaluation strategy.</p>
<p>Table 1: Key Statistics of MathVerse.</p>
<table>
<thead>
<tr>
<th>Statistic</th>
<th>Number</th>
</tr>
</thead>
<tbody>
<tr>
<td>Total questions</td>
<td>2,612</td>
</tr>
<tr>
<td>- Multiple-choice questions</td>
<td>$1,631(62.4 \%)$</td>
</tr>
<tr>
<td>- Free-form questions</td>
<td>$981(37.6 \%)$</td>
</tr>
<tr>
<td>- Newly collected questions</td>
<td>$\mathbf{1 , 2 3 6 ( 4 7 . 3 \% )}$</td>
</tr>
<tr>
<td>- Existing-dataset questions</td>
<td>$1,376(52.7 \%)$</td>
</tr>
<tr>
<td>- Questions with explanations</td>
<td>$\mathbf{1 , 2 3 6 ( 4 7 . 3 \% )}$</td>
</tr>
<tr>
<td>Total test samples</td>
<td>$\mathbf{1 5 , 6 7 2}$</td>
</tr>
<tr>
<td>- Newly annotated samples</td>
<td>$\mathbf{1 0 , 4 4 8 ( 6 6 . 7 \% )}$</td>
</tr>
<tr>
<td>- Samples of each version</td>
<td>$2,612(16.7 \%)$</td>
</tr>
<tr>
<td>Number of unique images</td>
<td>$2,420(92.6 \%)$</td>
</tr>
<tr>
<td>Number of unique questions</td>
<td>$2,573(98.5 \%)$</td>
</tr>
<tr>
<td>Number of unique answers</td>
<td>$847(32.4 \%)$</td>
</tr>
<tr>
<td>Maximum question length</td>
<td>203</td>
</tr>
<tr>
<td>Maximum answer length</td>
<td>17</td>
</tr>
<tr>
<td>Average question length</td>
<td>35.7</td>
</tr>
<tr>
<td>Average answer length</td>
<td>1.4</td>
</tr>
</tbody>
</table>
<p>Figure 4: Subject Distribution of MathVerse. Solid G: Solid Geometry, Plane G: Plane Geometry.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<h1>2.1 Visual Math Dataset</h1>
<p>To thoroughly assess visual mathematical proficiency, we compile a comprehensive problem set covering a broad spectrum of math subjects, diagram patterns, and specialized knowledge domains. This widespread collection for MathVerse aims to pose diverse challenges to MLLMs, ensuring a robust evaluation of their capabilities in visual contexts.</p>
<p>Data Composition and Categorization. MathVerse comprises a total of 2,612 visual math problems, which contribute to the final created 15 K test samples. Detailed statistics for data composition are presented in Table 1. This meticulously collected dataset covers three fundamental math subjects, i.e., plane geometry $(1,746)$, solid geometry (332), and functions (534), where the latter two are all composed of newly collected problems. The choice of these three subjects is not only due to their rigorous demands on multi-modal reasoning, but also for two other considerations. For one thing, as we specialize MathVerse in mathematical problem-solving, other peripheral tasks in MathVista [41] are not included, e.g., statistical reasoning, table question-answering, and puzzle tests. For another, we expect the evaluation to fully display the reasoning capabilities of MLLMs with moderate-level mathematical knowledge. This avoids limiting their performance with overly complex domain-specific theorems or prior commonsense knowledge. Therefore, we deliberately focus the collected problems on the high school level, excluding advanced college-level disciplines like calculus and graph theory featured in MMMU [63]. Furthermore, expert annotators subdivide the problems into twelve fine-grained categories, as depicted in Figure 4, showcasing various dimensions of visual mathematical skills.</p>
<p>Data Collection and Review Process. Our collection procedure for high-quality visual math problems involves a rigorous selection from both pre-existing datasets and public question repositories. In the domain of plane geometry, we initially select 750 problems from GeoQA [9], 119 from GEOS [51], and 507 from Geometry3K [42], based on their original data quality and distribution. We exclude questions that are extremely simple or excessively complex, as well as those that appear dubious or lack necessary conditions. To enhance the diversity of question types and diagram styles, we further enrich our dataset with additional 370 plane geometry problems by manually collecting from other sources ${ }^{1,2,3}$. Given the scarcity of solid geometry and function-related problems in existing benchmarks, we purposefully gather these two types of problems (332 and 534, respectively) from new sources ${ }^{1,2,3}$ to address this gap. Problems that include multiple diagrams or require visual illustrations within solutions are excluded, considering the current limitations of MLLMs in resolving such information. Note that, all the newly collected problems $(1,236)$ accompany detailed</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Six Versions of Each Problem in MATHVERSE. Expert annotators meticulously transform each visual math problem within MATHVERSE into six versions. They contain different visionlanguage content for a holistic visual mathematical evaluation.
explanations. After the preliminary collection, we undertake a comprehensive review to verify the accuracy of the answers, ensure consistency between questions and diagrams, and confirm the relevance of each problem to the defined twelve categories. This meticulous review guarantees the dataset's quality and precision.</p>
<h1>2.2 Whether MLLMs Truly See the Diagrams?</h1>
<p>In this section, we detail our data formulation approach to transform each problem in MATHVERSE into six different versions with varying information content in multi-modality. In this way, we explore the visual diagram understanding capabilities of MLLMs for mathematical reasoning.</p>
<p>Three Types of Textual Information. Considering the textual redundancy in original math problems, we first define three distinct categories for the textual information within the questions, as illustrated in Figure 3 and the following:</p>
<ul>
<li>Descriptive Information (DI) refers to the directly observable and clearly portrayed content in the diagram. It depicts the basic figure composition, spatial arrangement, and annotated entities, such as the presence of geometric shapes or intersection points of functions. These sentences normally help establish the context and frame the problem to orient the solver. Nevertheless, such information is repetitive to the visual components present in the diagram, thus regarded as redundant information for problem-solving. More importantly, it may assist MLLMs in bypassing the process of diagram interpretation, thereby undermining the assessment for visual mathematical reasoning, as evidenced in Figure 1.</li>
<li>
<p>Implicit Property (IP) involves the information that requires a higher level of visual perception but less mathematical knowledge to discern from the diagram. It signifies strong visual conditions for problem-solving, such as the parallelism and perpendicularity between lines, the similarity and congruence among triangles, and the category and periodicity of functions. They can, in theory, be fully extracted from the diagrams alone, giving adequate capability for visual recognition and comprehension of MLLMs.</p>
</li>
<li>
<p>Essential Condition (EC) denotes the specific numerical or algebraic measurements, which are indispensable conditions to derive the solution and cannot be derived from the visual diagram. This category encompasses precise values of angles, lengths, and function expressions, such as an angle being 45 degrees, the length of BC being 6 units, and the functional equation $f(x)=x^{2}+3$. Without these details in textual information, solving the visual math problem would be impossible.</p>
</li>
</ul>
<p>Creating Six Versions of Each Problem. Based on the three categories, expert annotators systematically remove different textual information within questions, and incrementally incorporate the critical elements into diagrams. This approach can progressively reduce textual redundancy and information content, thereby increasingly compelling MLLMs to capture mathematical conditions from the visual input. As compared in Figure 5, we generate six versions of each problem in MATHVERSE, obtaining 15,672 test instances. With this curated problem set, we can provide a holistic evaluation of the genuine visual comprehension of MLLMs, and whether it can facilitate multi-modal mathematical reasoning. The details of each problem version are as follows:</p>
<ul>
<li>Text-dominant Version retains the entire textual content, including the three types of textual information and the question statement. If the original problem contains limited Descriptive Information, we manually add it within the textual content. This version may induce MLLMs to regard the text as the primary source of information, treating the diagram more as a supplementary visual aid. This serves as the baseline point for evaluation.</li>
</ul>
<p>Text: DI + IP + EC + Question
Vision: Diagram</p>
<ul>
<li>Text-lite Version diminishes the Descriptive Information from the Text-dominant version, assuming this information can be observed from the diagram. This creates a condensed question without redundancy, forcing MLLMs to interpret the diagram for basic information.</li>
</ul>
<p>Text: IP + EC + Question
Vision: Diagram</p>
<ul>
<li>Text-only Version directly discards the diagram input from the Text-dominant version. Comparing this to the Text-lite version helps identify where MLLMs mainly obtain the contextual visual information for problem-solving, the Descriptive Information or diagram.</li>
</ul>
<p>Text: DI + IP + EC + Question
Vision: $\varnothing$</p>
<ul>
<li>Vision-intensive Version further removes the Implicit Property from the Text-lite version. Without the strong visual condition in texts, MLLMs are challenged to intensively leverage their visual interpretation skills to gather sufficient cues for mathematical reasoning. The outcome demonstrates their proficiency in understanding mathematical relationships visually.</li>
</ul>
<p>Text: EC + Question
Vision: Diagram</p>
<ul>
<li>Vision-dominant Version, building upon the Text-lite version, excludes the Essential Condition from texts, instead annotating these measurements visually in diagrams. The textual content is narrowed down to Implicit Property and question statements. It demands MLLMs to recognize the Essential Condition exclusively from diagrams, and accurately correlate it with corresponding visual elements for problem-solving.</li>
</ul>
<p>Text: IP + Question
Vision: Diagram + EC</p>
<ul>
<li>Vision-only Version strips away the entire textual input, conveying the problem solely through the diagram. We render the remaining textual content in the Vision-dominant version onto the diagram, and minimize the text to an almost negligible prompt. This compels MLLMs to extract figures and recognize questions both from the visual information, standing as an ultimate test for their visual reasoning capabilities in mathematics.</li>
</ul>
<p>Text: $\varnothing$
Vision: Diagram + EC + IP + Question</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Examples of the CoT Evaluation Strategy for MathVerse. We present two outputs from Qwen-VL-Max [3] with our CoT evaluation strategy, which assesses the fine-grained reasoning capabilities with a detailed explanation for error analysis.</p>
<h1>2.3 CoT Evaluation Strategy</h1>
<p>Compared to visual question-answering in general scenarios, the solving process of MLLMs for mathematical problems requires nuanced, step-by-step CoT reasoning. Considering two cases in Figure 6, one arrives at the correct solution albeit through incorrect intermediary steps, while the other demonstrates the opposite phenomenon. Therefore, the binary 'Correct' or 'Incorrect' evaluative approach of existing benchmarks is inadequate to examine the depth and precision of the multi-step reasoning process. To this end, we propose a CoT evaluation strategy to thoroughly assess their mathematical CoT skills in visual contexts, involving two prompting phases with GPT-4(V) [47, 46].</p>
<p>Key-step Extraction. Given the output of an MLLM, we first employ GPT-4, the languageonly version, to extract $N$ pivotal steps within the reasoning sequence, denoted as $\left[s_{1}, s_{2}, \ldots, s_{N}\right]$, including the final answer $s_{A}$. Such key steps include significant computational outcomes, the identification of visual components, and critical immediate inferences. Note that, we only prompt GPT-4 with the MLLM's output, deliberately omitting the original questions, diagrams, and groundtruth answers. This approach aims to mitigate the inherent bias of GPT-4 itself towards problemsolving and visual diagram interpretation, thereby concentrating solely on the logical coherence of the model output. In addition, we do not pre-define a ground-truth key-step template for each problem, but perform the extraction adaptively for the unique output of every MLLM. Since the problem potentially encompasses diverse possible solution pathways, and different MLLMs exhibit varying reasoning lengths and styles, the rigid template would harm the CoT evaluation accuracy.</p>
<p>Multi-step Scoring. After the extraction phase, we utilize GPT-4V, the multi-modal version, to evaluate each critical step and culminate a comprehensive score. We feed the extracted key steps, the original questions, diagrams, and ground-truth answers all into GPT-4V, contributing to a holistic assessment, e.g., numerical computations, logical deductions, and visual interpretations. Therein, we observe that GPT-4V occasionally struggles with accurately recognizing elements within functional diagrams, leading to unstable evaluation for related problems. We thereby annotate additional</p>
<p>Table 2: Mathematical Evaluation on Six Problem Versions in MathVerse's testmini Set. We calculate the 'All' score without averaging the 'Text Only' version. 'CoT-E' or 'Acc' denotes whether to employ the proposed CoT evaluation strategy or not. The highest accuracy for closed-source and open-source MLLMs is marked in red and blue respectively.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>All</th>
<th></th>
<th>Text Dominant</th>
<th></th>
<th>Text Lite</th>
<th></th>
<th>Text Only</th>
<th></th>
<th>Vision Intensive</th>
<th></th>
<th>Vision Dominant</th>
<th></th>
<th>Vision Only</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>CoT-E</td>
<td>Acc</td>
<td>CoT-E</td>
<td>Acc</td>
<td>CoT-E</td>
<td>Acc</td>
<td>CoT-E</td>
<td>Acc</td>
<td>CoT-E</td>
<td>Acc</td>
<td>CoT-E</td>
<td>Acc</td>
<td>CoT-E</td>
<td>Acc</td>
</tr>
<tr>
<td>Baselines</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Random Chance</td>
<td>-</td>
<td>12.4</td>
<td>-</td>
<td>12.4</td>
<td>-</td>
<td>12.4</td>
<td>-</td>
<td>12.4</td>
<td>-</td>
<td>12.4</td>
<td>-</td>
<td>12.4</td>
<td>-</td>
<td>12.4</td>
</tr>
<tr>
<td>Human</td>
<td>-</td>
<td>64.9</td>
<td>-</td>
<td>71.2</td>
<td>-</td>
<td>70.9</td>
<td>-</td>
<td>41.7</td>
<td>-</td>
<td>61.4</td>
<td>-</td>
<td>68.3</td>
<td>-</td>
<td>66.7</td>
</tr>
<tr>
<td>LLMs</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ChatGPT [48]</td>
<td>-</td>
<td>-</td>
<td>51.3</td>
<td>33.3</td>
<td>38.5</td>
<td>18.9</td>
<td>51.3</td>
<td>33.3</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>GPT-4 [46]</td>
<td>-</td>
<td>-</td>
<td>63.4</td>
<td>46.5</td>
<td>40.7</td>
<td>20.7</td>
<td>63.4</td>
<td>46.5</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Closed-source MLLMs</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Qwen-VL-Plus [3]</td>
<td>21.3</td>
<td>11.8</td>
<td>26.0</td>
<td>15.7</td>
<td>21.2</td>
<td>11.1</td>
<td>25.2</td>
<td>14.5</td>
<td>18.5</td>
<td>9.0</td>
<td>19.1</td>
<td>13.0</td>
<td>21.8</td>
<td>10.0</td>
</tr>
<tr>
<td>Gemin-Pro [22]</td>
<td>35.3</td>
<td>23.5</td>
<td>39.8</td>
<td>26.3</td>
<td>34.7</td>
<td>23.5</td>
<td>44.5</td>
<td>27.3</td>
<td>32.0</td>
<td>23.0</td>
<td>36.8</td>
<td>22.3</td>
<td>33.3</td>
<td>22.2</td>
</tr>
<tr>
<td>Qwen-VL-Man [3]</td>
<td>37.2</td>
<td>25.3</td>
<td>42.8</td>
<td>30.7</td>
<td>37.7</td>
<td>26.1</td>
<td>47.9</td>
<td>28.9</td>
<td>33.6</td>
<td>24.1</td>
<td>35.9</td>
<td>24.1</td>
<td>35.9</td>
<td>21.4</td>
</tr>
<tr>
<td>GPT-4V [47]</td>
<td>54.4</td>
<td>39.4</td>
<td>63.1</td>
<td>54.7</td>
<td>56.6</td>
<td>41.4</td>
<td>60.3</td>
<td>48.7</td>
<td>51.4</td>
<td>34.9</td>
<td>50.8</td>
<td>34.4</td>
<td>50.3</td>
<td>31.6</td>
</tr>
<tr>
<td>Open-source MLLMs</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LLaMA-Adapter V2 [20]</td>
<td>5.8</td>
<td>5.7</td>
<td>7.8</td>
<td>6.2</td>
<td>6.3</td>
<td>5.9</td>
<td>3.9</td>
<td>2.7</td>
<td>6.2</td>
<td>6.1</td>
<td>4.5</td>
<td>4.2</td>
<td>4.4</td>
<td>6.1</td>
</tr>
<tr>
<td>ImageBind-LLM [24]</td>
<td>10.0</td>
<td>9.2</td>
<td>13.2</td>
<td>11.4</td>
<td>11.6</td>
<td>11.3</td>
<td>12.9</td>
<td>11.7</td>
<td>9.8</td>
<td>8.9</td>
<td>11.8</td>
<td>11.2</td>
<td>3.5</td>
<td>3.4</td>
</tr>
<tr>
<td>mPLUG-OwI2 [62]</td>
<td>10.3</td>
<td>5.9</td>
<td>11.6</td>
<td>6.6</td>
<td>11.4</td>
<td>6.3</td>
<td>13.8</td>
<td>6.1</td>
<td>11.1</td>
<td>6.3</td>
<td>9.4</td>
<td>5.6</td>
<td>8.0</td>
<td>4.9</td>
</tr>
<tr>
<td>MiniGPT-v2 [13]</td>
<td>10.9</td>
<td>11.0</td>
<td>13.2</td>
<td>12.1</td>
<td>12.7</td>
<td>12.0</td>
<td>15.3</td>
<td>11.7</td>
<td>11.1</td>
<td>13.1</td>
<td>11.3</td>
<td>10.3</td>
<td>6.4</td>
<td>7.4</td>
</tr>
<tr>
<td>LLaVA-1.5 [37]</td>
<td>12.7</td>
<td>7.6</td>
<td>17.1</td>
<td>8.8</td>
<td>12.0</td>
<td>7.6</td>
<td>22.6</td>
<td>11.5</td>
<td>12.6</td>
<td>7.4</td>
<td>12.7</td>
<td>7.4</td>
<td>9.0</td>
<td>6.9</td>
</tr>
<tr>
<td>SPHINX-Plus [21]</td>
<td>14.0</td>
<td>12.2</td>
<td>16.3</td>
<td>13.9</td>
<td>12.8</td>
<td>11.6</td>
<td>15.8</td>
<td>14.9</td>
<td>12.9</td>
<td>11.6</td>
<td>14.7</td>
<td>13.5</td>
<td>13.2</td>
<td>10.4</td>
</tr>
<tr>
<td>G-LLaVA [19]</td>
<td>15.7</td>
<td>16.6</td>
<td>22.2</td>
<td>20.9</td>
<td>20.4</td>
<td>20.7</td>
<td>21.6</td>
<td>21.1</td>
<td>16.5</td>
<td>17.2</td>
<td>12.7</td>
<td>14.6</td>
<td>6.6</td>
<td>9.4</td>
</tr>
<tr>
<td>LLaVA-NoXT [38]</td>
<td>17.2</td>
<td>15.6</td>
<td>21.6</td>
<td>19.4</td>
<td>19.7</td>
<td>13.2</td>
<td>25.1</td>
<td>18.1</td>
<td>17.6</td>
<td>16.8</td>
<td>14.9</td>
<td>15.2</td>
<td>12.1</td>
<td>11.5</td>
</tr>
<tr>
<td>ShareGPT4V [12]</td>
<td>17.4</td>
<td>13.1</td>
<td>21.8</td>
<td>16.2</td>
<td>20.6</td>
<td>16.2</td>
<td>14.6</td>
<td>6.6</td>
<td>18.6</td>
<td>15.5</td>
<td>16.2</td>
<td>13.8</td>
<td>9.7</td>
<td>3.7</td>
</tr>
<tr>
<td>SPHINX-MoE [21]</td>
<td>22.8</td>
<td>15.0</td>
<td>33.3</td>
<td>22.2</td>
<td>21.9</td>
<td>16.4</td>
<td>40.7</td>
<td>18.3</td>
<td>21.1</td>
<td>14.8</td>
<td>19.6</td>
<td>12.6</td>
<td>18.3</td>
<td>9.1</td>
</tr>
<tr>
<td>InteraLM-XC2. [16]</td>
<td>25.9</td>
<td>16.5</td>
<td>36.9</td>
<td>22.3</td>
<td>28.3</td>
<td>17.0</td>
<td>42.5</td>
<td>16.5</td>
<td>20.1</td>
<td>15.7</td>
<td>24.4</td>
<td>16.4</td>
<td>19.8</td>
<td>11.0</td>
</tr>
</tbody>
</table>
<p>information for function problems and together feed into GPT-4V, ensuring the quality of visual evaluation. Specifically, GPT-4V assesses each $N$ intermediate step with a binary score of ' 1 ' (correct) or ' 0 ' (incorrect), and derives the overall score by aggregating the correctness of the final answer. We formulate the scoring process as</p>
<p>$$
\operatorname{Score}<em i="1">{\text {final }}=\alpha\left(\frac{1}{N} \sum</em>\right)
$$}^{N} \operatorname{Score}\left(s_{i}\right)\right)+(1-\alpha) \operatorname{Score}\left(s_{A</p>
<p>where $\alpha$ denotes a balancing factor between the intermediate steps and the final answer $s_{A}$. We set $\alpha$ as 0.7 by default to underscore the significance of CoT reasoning. As exemplified in Figure 6, besides the fine-grained scoring, the CoT evaluation can also provide a detailed error analysis of each step, which is valuable and instructive for the development of MLLMs in the field.</p>
<h1>3 Experiments</h1>
<p>In this section, we conduct a systematic evaluation of existing Multi-modal Large Language Models (MLLMs) on MATHVERSE. We first introduce the experimental setup in Section 3.1. Then, we detail the quantitative results in Section 3.2 and narrate the error analysis in Section 3.3.</p>
<h3>3.1 Experimental Setup</h3>
<p>Division of the testmini Subset. MATHVERSE encompasses a comprehensive collection of 2,612 visual math problems, alongside 15,672 corresponding test instances. To enable faster evaluation and model development validation, we extract a smaller subset termed testmini including 788 problems and 4,728 instances. In constructing testmini, we employ a random sampling strategy across different subfields, maintaining a sample size proportional to the overall dataset to preserve its statistical representativeness. The remaining test set features 1,824 problems and 10,944 samples will be utilized for standard evaluation and publicly released in the future. In the subsequent experiments, all quantitative results are assessed using the testmini subset of MATHVERSE.</p>
<p>Evaluation Models. We examine the performance of foundation models across three distinct categories on MATHVERSE: (a) Large Language Models (LLMs) as the text-only baseline, which only take textual questions as input, including ChatGPT [45] and GPT-4 [46], (b) Closed-source</p>
<p>Table 3: Mathematical Evaluation on Different Subjects and Subfields in MathVerse's iestmini Set. We report the scores averaging five problem versions except for the 'Text Only' version, and employ the CoT evaluation strategy by default. Len: Length; Anal: Analytic; Apply: Applied; Vol: Volume; Coord: Coordinate; Prop: Property; Exp: Expression; Apply: Applied. The highest accuracy for closed-source and open-source MLLMs is marked in red and blue respectively.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>All</th>
<th>Plane Geometry</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Solid Geometry</th>
<th></th>
<th></th>
<th></th>
<th>Functions</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>All</td>
<td>Len</td>
<td>Area</td>
<td>Angle</td>
<td>Anal</td>
<td>Apply</td>
<td>All</td>
<td>Len</td>
<td>Area</td>
<td>Vol</td>
<td>All</td>
<td>Coord</td>
<td>Prop</td>
<td>Exp</td>
<td>Apply</td>
</tr>
<tr>
<td>Closed-source MLLMs</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Qwen-VL-Plus [3]</td>
<td>21.3</td>
<td>17.3</td>
<td>19.1</td>
<td>16.4</td>
<td>16.1</td>
<td>23.6</td>
<td>13.2</td>
<td>24.8</td>
<td>18.1</td>
<td>18.7</td>
<td>33.4</td>
<td>31.3</td>
<td>52.5</td>
<td>25.1</td>
<td>10.8</td>
<td>50.3</td>
</tr>
<tr>
<td>Gemini-Pro [22]</td>
<td>35.3</td>
<td>33.0</td>
<td>32.2</td>
<td>42.6</td>
<td>28.4</td>
<td>30.2</td>
<td>32.3</td>
<td>33.4</td>
<td>35.0</td>
<td>29.3</td>
<td>36.1</td>
<td>28.3</td>
<td>35.7</td>
<td>26.6</td>
<td>10.8</td>
<td>51.3</td>
</tr>
<tr>
<td>Qwen-VL-Max [3]</td>
<td>37.2</td>
<td>38.4</td>
<td>41.7</td>
<td>46.4</td>
<td>32.6</td>
<td>40.6</td>
<td>38.7</td>
<td>33.7</td>
<td>25.4</td>
<td>28.3</td>
<td>42.6</td>
<td>38.4</td>
<td>43.7</td>
<td>35.5</td>
<td>13.6</td>
<td>61.0</td>
</tr>
<tr>
<td>GPT-4V [47]</td>
<td>54.4</td>
<td>56.9</td>
<td>60.8</td>
<td>63.4</td>
<td>52.6</td>
<td>48.5</td>
<td>60.9</td>
<td>50.2</td>
<td>54.8</td>
<td>59.9</td>
<td>56.8</td>
<td>52.8</td>
<td>72.3</td>
<td>47.1</td>
<td>30.9</td>
<td>70.1</td>
</tr>
<tr>
<td>Open-source MLLMs</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LLaMA-Adapter V2 [20]</td>
<td>5.8</td>
<td>5.9</td>
<td>4.0</td>
<td>5.9</td>
<td>6.6</td>
<td>12.4</td>
<td>5.3</td>
<td>4.6</td>
<td>5.3</td>
<td>3.1</td>
<td>5.7</td>
<td>6.2</td>
<td>6.7</td>
<td>6.1</td>
<td>4.5</td>
<td>7.9</td>
</tr>
<tr>
<td>ImageBind-LLM [24]</td>
<td>10.0</td>
<td>9.7</td>
<td>12.1</td>
<td>9.9</td>
<td>9.2</td>
<td>10.2</td>
<td>4.8</td>
<td>4.6</td>
<td>4.9</td>
<td>3.5</td>
<td>5.3</td>
<td>14.9</td>
<td>12.3</td>
<td>13.8</td>
<td>4.6</td>
<td>25.9</td>
</tr>
<tr>
<td>mPLUG-Owl2 [62]</td>
<td>10.3</td>
<td>7.7</td>
<td>8.2</td>
<td>6.0</td>
<td>5.7</td>
<td>12.4</td>
<td>10.6</td>
<td>11.0</td>
<td>9.2</td>
<td>6.7</td>
<td>15.7</td>
<td>17.4</td>
<td>22.8</td>
<td>18.6</td>
<td>5.3</td>
<td>22.2</td>
</tr>
<tr>
<td>MiniGPT-v2 [11]</td>
<td>10.9</td>
<td>11.6</td>
<td>10.0</td>
<td>9.8</td>
<td>14.3</td>
<td>9.1</td>
<td>11.8</td>
<td>1.7</td>
<td>2.2</td>
<td>1.6</td>
<td>0.5</td>
<td>11.2</td>
<td>6.2</td>
<td>15.7</td>
<td>4.0</td>
<td>21.1</td>
</tr>
<tr>
<td>LLaVA-1.5 [37]</td>
<td>12.7</td>
<td>11.8</td>
<td>13.1</td>
<td>15.1</td>
<td>9.7</td>
<td>9.4</td>
<td>13.2</td>
<td>10.6</td>
<td>12.1</td>
<td>8.7</td>
<td>11.6</td>
<td>14.8</td>
<td>18.8</td>
<td>12.7</td>
<td>9.5</td>
<td>23.7</td>
</tr>
<tr>
<td>SPHINX-Plus [21]</td>
<td>14.0</td>
<td>14.4</td>
<td>14.2</td>
<td>10.5</td>
<td>14.1</td>
<td>16.5</td>
<td>16.8</td>
<td>7.0</td>
<td>7.2</td>
<td>6.1</td>
<td>7.6</td>
<td>17.9</td>
<td>11.1</td>
<td>19.1</td>
<td>6.3</td>
<td>27.7</td>
</tr>
<tr>
<td>G-LLaVA [19]</td>
<td>15.7</td>
<td>20.2</td>
<td>17.3</td>
<td>13.6</td>
<td>26.5</td>
<td>5.9</td>
<td>23.1</td>
<td>5.0</td>
<td>10.3</td>
<td>4.4</td>
<td>3.1</td>
<td>9.2</td>
<td>9.1</td>
<td>9.1</td>
<td>1.3</td>
<td>15.5</td>
</tr>
<tr>
<td>LLaVA-NeXT [18]</td>
<td>17.2</td>
<td>15.9</td>
<td>14.8</td>
<td>13.1</td>
<td>16.3</td>
<td>17.7</td>
<td>17.8</td>
<td>19.6</td>
<td>33.3</td>
<td>11.7</td>
<td>12.6</td>
<td>23.1</td>
<td>24.9</td>
<td>23.4</td>
<td>8.0</td>
<td>33.1</td>
</tr>
<tr>
<td>ShareGPT4V [12]</td>
<td>17.4</td>
<td>16.9</td>
<td>16.2</td>
<td>17.9</td>
<td>16.9</td>
<td>12.2</td>
<td>21.1</td>
<td>15.0</td>
<td>13.6</td>
<td>10.9</td>
<td>19.7</td>
<td>20.2</td>
<td>19.9</td>
<td>22.2</td>
<td>8.4</td>
<td>25.8</td>
</tr>
<tr>
<td>SPHINX-MoE [21]</td>
<td>22.8</td>
<td>24.5</td>
<td>26.3</td>
<td>28.4</td>
<td>21.1</td>
<td>26.6</td>
<td>24.4</td>
<td>15.8</td>
<td>9.4</td>
<td>10.7</td>
<td>26.3</td>
<td>19.5</td>
<td>23.5</td>
<td>19.3</td>
<td>9.2</td>
<td>30.3</td>
</tr>
<tr>
<td>InternLM-XC2. [16]</td>
<td>25.9</td>
<td>26.2</td>
<td>27.1</td>
<td>29.7</td>
<td>20.6</td>
<td>18.5</td>
<td>22.2</td>
<td>20.1</td>
<td>34.5</td>
<td>14.1</td>
<td>25.2</td>
<td>25.7</td>
<td>24.4</td>
<td>24.9</td>
<td>10.6</td>
<td>36.3</td>
</tr>
</tbody>
</table>
<p>MLLMs, represented by models like GPT-4V [47], Gemini-Pro [22], Qwen-VL-Max [3], and Qwen-VL-Plus, and (c) Open-source MLLMs, featuring models such as LLaVA-1.5 [37] (Vicuna-13B [13]), LLaVA-NeXT [38] (Vicuna-13B), SPHINX-MoE [21] (Mixtral-8×7B [28]), SPHINX-Plus (LLaMA213B [56]), InternLM-XComposer2 [16] (InternLM2-7B [54]), LLaMA-Adapter V2 [20] (LLaMA7B [55]), ImageBind-LLM [24] (LLaMA-7B), MiniGPT-v2 [11] (LLaMA2-7B), mPLUG-Owl2 [62] (LLaMA-7B), G-LLaVA [19] (LLaMA2-7B), and ShareGPT-4V [12] (Vicuna-13B).</p>
<p>Implementation Details. All our experiments are conducted under a zero-shot setting, showcasing the generalization capacity of MLLMs for mathematical reasoning, without few-shot prompting or further fine-tuning. By default, we employ the Chain-of-Thought (CoT) prompting technique [58], which encourages MLLMs to perform complete reasoning steps for a fine-grained evaluation. A baseline representing random chance is established for comparison, for which we select one option at random for multiple-choice questions and utilize empty for free-form questions. In addition, we recruit ten qualified college students, and ask them to solve the problems in MATHVERSE independently, serving as a baseline for human performance. We conduct all experiments on NVIDIA A100 GPUs. As the text-only LLMs can only take text questions as input, we evaluate them with the first three problem versions, i.e., Text Dominant, Text Lite, and Text Only. For the 'w/o' results, we utilize the template in MathVista [41] to prompt GPT-4 [46] for answer extraction, and directly score the final answer without the intermediate reasoning process.</p>
<h1>3.2 Experimental Analysis</h1>
<p>To best investigate the visual mathematical reasoning capabilities, we report the evaluation results of different models on MATHVERSE for the six transformed problem versions in Table 2 and twelve detailed subjects in Table 3. We mainly analyze the performance by the proposed Chain-of-Though (CoT) evaluation, and derive the following observations.</p>
<p>MLLMs Rely More on DI than Seeing Diagrams. Comparing the Text-dominant and Text-only versions, with the elimination of visual input, most MLLMs even obtain an unexpected performance improvement, e.g., $+5.1 \%$ for Qwen-VL-Max and $+5.6 \%$ for InternLM-XComposer2. This suggests that the unsatisfactory visual encoding for mathematical diagrams instead severely harms the original problem-solving capacity of MLLMs. As exemplified in Figure 7, from the error analysis of our CoT evaluation strategy, we observe that Gemini-Pro can deduce the correct answer exclusively by the visual information within the Descriptive Information. Instead, the inaccurate visual perception of mathematical elements directly interferes with the outcome of problem-solving, turning correct answers into incorrect ones. In contrast, GPT-4V and ShareGPT-4V achieve better results in Text Dominant than in Text Only, indicating their relatively better visual encoding, which would not degrade the performance. However, they still encounter a larger performance drop by removing the</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: A Typical Visual Perception Error by our CoT Evaluation Strategy. The example is an output from Gemini-Pro [22], where the correct reasoning of the Text-only version is distracted by the visual perception error within the diagram.
redundant Descriptive Information than the diagram input, e.g., GPT-4V and ShareGPT-4V. This pattern demonstrates that they tend to capture more visual information for mathematical reasoning from the text content, instead of seeing the diagram itself.</p>
<p>MLLMs are Moderately Effective at Perceiving IP. By discarding the Implicit Property in question texts, a negligible decline in accuracy is noted from the Text-lite to Vision-intensive versions for most MLLMs. This is because the Implicit Property mainly encompasses the spatial layouts and geometric relationships, which demand minimal mathematical domain knowledge for interpretation. This outcome underscores the favorable visual perception skills of MLLMs for non-mathematical elements, which is not the primary obstacle hindering MLLMs in solving visual math problems.</p>
<p>MLLMs are Challenged to interpret EC from Diagrams. Incorporating the Essential Condition within diagrams challenges MLLMs to accurately identify and understand these conditions in vision modality for mathematical problem-solving. Evidence from the Vision-dominant results indicates a notable decline in the performance of most MLLMs compared to the Text-lite accuracy, such as $-5.8 \%$ for GPT-4V and $-3.9 \%$ for InterLM-XComposer2. This reveals their inaccurate identification of mathematical symbols and an insufficient grasp of domain-specific knowledge required to associate identified measurements with relevant concepts.</p>
<p>MLLMs struggle to Solve Problems Entirely by Diagrams. The scenario of Vision-only problems aligns more closely with real-world applications, where capturing an image is often more convenient than transcribing the problem into text. However, by rendering the whole question within the diagram, the mathematical problem-solving capacity of MLLMs is further diminished. This experiment unveils the great challenge for MLLMs to simultaneously understand mathematical conditions, questions, and figures from the visual input alone.</p>
<p>Closed-source MLLMs are Better-performed. From the performance in both tables, we observe a consistently better performance achieved by closed-source MLLMs than open-sourced ones. Despite the gap with humans, GPT-4V attains the leading position among MLLMs, showcasing superior mathematical capabilities over problem versions and subjects, especially the challenging subfields like 'Coord' and 'Prop' (the property and coordinate solving of function problems). InternLMXComposer2 and SPHINX-MoE are the best-performing open-source MLLMs, while still lagging behind Gemini-Pro with a margin of $9.4 \%$ and $12.5 \%$ overall accuracy, respectively, suggesting large improvement space.</p>
<p>LLMs Achieve Competitive Results to MLLMs. Utilizing solely question texts as input, two LLMs, i.e., GPT-4 and ChatGPT, attain superior accuracy to most MLLMs in Text Dominant and Lite versions. Even in the absence of redundant Descriptive Information within Text-lite problems, GPT-4 outperforms InternLM-XComposer2 and SPHINX-MoE by substantial margins of $12.4 \%$ and $18.8 \%$, respectively. These findings not only indicate the strong mathematical reasoning skills of LLMs, but</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Results with and without CoT Evaluation in MathVerse. Referring to Table 2, we denote the 'w/o' results in blue pillars, and highlight the increase and decrease magnitude with 'CoT-E' by green and red colors, respectively.
further emphasize the deficiencies in diagram interpretation of existing MLLMs. Importantly, the performance of GPT-4 is only exceeded by GPT-4V, which demonstrates that a satisfactory diagram perception capability can enhance problem-solving for visual mathematics.</p>
<p>GPT-4(V) Beats Human in the Text-only Version. Without the visual content provided in diagrams, human solvers often face challenges in deducing the correct answers due to the lack of sufficient information, e.g., $41.7 \%$ 'w/o' scores in Text-only problems. In contrast, GPT-4V and GPT4 achieve the 'w/o' scores of $41.1 \%$ and $46.5 \%$, respectively, which surpass the human performance. This comparison highlights their advanced reasoning capabilities in handling extreme scenarios, exhibiting more robustness for mathematical problem-solving given missing visual conditions.</p>
<p>Mathematical Training Benefits the Performance. In addition to foundational visual instructionfollowing datasets, both SPHINX-MoE and InternLM-XComposer2 extend their training regimes to include specialized mathematical problems that are either text-only or visual, such as MathQA [2], Geometry3K [43], and MathInstruct [64]. This approach of math-specific tuning contributes to their leading performance in MathVerse. Furthermore, G-LLaVA fine-tunes LLaVA-1.5 by a large-scale visual geometric dataset containing 170K enriched problems. This targeted refinement can improve several fields ('Len', 'Angle', and 'Apply') within the plane geometry subject. However, since G-LLaVA's fine-tuning data does not include problems of analytic geometry, solid geometry, and functions, it harms the related results of LLaVA-1.5 due to catastrophic forgetting, e.g., -3.5\% in 'Anal', -5.6\% in 'Solid Geometry', and -5.6\% in 'Functions'. This phenomenon underscores the critical role of developing extensive, high-quality visual math data for effectively training MLLMs.</p>
<p>Discrepancy Between 'CoT-E' and 'w/o' Scores. As illustrated by Table 2, the 'CoT-E' scores for MLLMs, in most cases, are much higher than 'w/o' scores, e.g., $+16.1 \%$ for GPT-4V and $+9.6 \%$ for InternLM-XComposer2. This observation demonstrates that our proposed CoT evaluation strategy identifies numerous correct intermediate reasoning steps, despite the final incorrect answer, highlighting the effectiveness of fine-grained assessment. In Figure 8, we present the statistics of variance between 'CoT-E' and 'w/o' scores within different MLLMs. Although GPT-4V attains top-tier performance, it exhibits a pronounced gap of $16.1 \%$ concerning the evaluation of CoT reasoning quality, similar to the $12.4 \%$ gap of Qwen-VL-Max. Conversely, SPHINX-MoE showcases favorable precision among open-source MLLMs, while preserving a relatively lower variance of two evaluation methods, i.e., $6.0 \%$ compared to InternLM-XComposer's $9.6 \%$. This indicates its consistent step-by-step reasoning throughout the problem-solving process.</p>
<h1>3.3 Error Analysis</h1>
<p>To delve into the fine-grained predictions, we select the best-performing MLLM, GPT-4V [47], to understand its modes of success and failure. Our proposed CoT evaluation strategy has produced a detailed assessment of model output, including step-wise scores and explanation, reducing extensive manual effort in identifying and analyzing errors. We conduct our analysis on the two-step output from the CoT evaluation across the entire dataset, focusing on two key dimensions.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Distribution of GPT-4V's [47] Errors in Reasoning and Answers. For the six problem versions in MATHVERSE, we provide the statistics of errors made by GPT-4V based on their occurrence in answers ('Ans.') and reasoning processes ('Rea.').</p>
<p>Errors in Reasoning or Answer? In Figure 9, we showcase the statistics of different error distributions in six problem versions of MATHVERSE. We define the following six error categories: correct final answer with correct/partially correct/incorrect CoT reasoning and incorrect final answer with correct/partially correct/incorrect CoT reasoning. For all six versions, the incorrect final answers are mostly caused by the partially incorrect reasoning process. In addition, a number of problems with correct answers are accompanied by partially or entirely incorrect reasoning, e.g., $15.3 \%$ in Text Dominant, which cannot be detected by the traditional True or False evaluation. As we remove the content within textual questions and enrich the visual diagram, e.g., from Text Dominant and Lite to Vision Dominant and Only, we observe a progressive increase in the error rate of 'incorrect final answer with incorrect CoT reasoning', indicating that MLLMs are challenged to conduct high-quality intermediate reasoning by capturing more information from the visual input.</p>
<p>What Types of Errors? To further investigate the specific error types, we survey the problems with errors that occur either within the reasoning process or the final answer. As depicted in Figure 10, we divide the errors of GPT-4V into four distinct types: visual perception error, reasoning error, knowledge error, and calculation error. Consistent with our findings in the main paper, the primary source of errors in problem-solving attributes to the inaccurate interpretation of mathematical diagrams, which significantly impedes the performance of MLLMs. For the problem versions that demand advanced diagram interpretation, e.g., Vision Dominant and Only, we observe a notable increase in the rate of visual perception errors, demonstrating an urgent need for stronger visual encoders in MLLMs. Moreover, reasoning errors also account for a considerable percentage, indicating that the logical deduction skills of MLLMs still require improvement. As expected, knowledge errors do not significantly hinder the mathematical reasoning capabilities of MLLMs in MATHVERSE.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Distribution of GPT-4V's [47] Errors within Different Types. We present the statistics of four error types by GPT-4V in the six problem versions, i.e., Visual Perception Error, Reasoning Error, Calculation Error, and Knowledge Error.</p>
<h1>4 Conclusion</h1>
<p>In this paper, we propose a comprehensive and specialized benchmark, MATHVERSE, for the visual mathematical problem-solving capacity of MLLMs. We meticulously collect high-quality math problems with diagrams spanning three primary subjects and twelve subfields. Given the issues within current benchmarks, we transform each problem into six versions, investigating whether and how much MLLMs can interpret the visual math diagrams. We also propose a CoT evaluation strategy for finer-grained assessment of the intermediate reasoning process of MLLMs. By evaluating various closed-source and open-source models, MATHVERSE unveils that most existing MLLMs struggle to accurately understand mathematical diagrams, and even attain higher results without visual input. This indicates the potential of developing more advanced math-specific vision encoders for stronger multi-modal mathematical reasoning.</p>
<h2>References</h2>
<p>[1] Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems 35, 23716-23736 (2022)
[2] Amini, A., Gabriel, S., Lin, P., Koncel-Kedziorski, R., Choi, Y., Hajishirzi, H.: Mathqa: Towards interpretable math word problem solving with operation-based formalisms. arXiv preprint arXiv:1905.13319 (2019)
[3] Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.: Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966 (2023)
[4] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. In: Advances in neural information processing systems. pp. 1877-1901 (2020)</p>
<p>[5] Cao, J., Xiao, J.: An augmented benchmark dataset for geometric question answering through dual parallel text encoding. In: Proceedings of the 29th International Conference on Computational Linguistics. pp. 1511-1520 (2022)
[6] Chen, G., Zheng, Y.D., Wang, J., Xu, J., Huang, Y., Pan, J., Wang, Y., Wang, Y., Qiao, Y., Lu, T., et al.: Videollm: Modeling video sequence with large language models. arXiv preprint arXiv:2305.13292 (2023)
[7] Chen, J., Li, T., Qin, J., Lu, P., Lin, L., Chen, C., Liang, X.: Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression. arXiv preprint arXiv:2212.02746 (2022)
[8] Chen, J., Li, T., Qin, J., Lu, P., Lin, L., Chen, C., Liang, X.: Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression. ArXiv abs/2212.02746 (2022)
[9] Chen, J., Tang, J., Qin, J., Liang, X., Liu, L., Xing, E.P., Lin, L.: Geoqa: A geometric question answering benchmark towards multimodal numerical reasoning. arXiv preprint arXiv:2105.14517 (2021)
[10] Chen, J., Tang, J., Qin, J., Liang, X., Liu, L., Xing, E.P., Lin, L.: Geoqa: A geometric question answering benchmark towards multimodal numerical reasoning. ArXiv abs/2105.14517 (2021), https://api.semanticscholar.org/CorpusID:235253782
[11] Chen, J., Li, D.Z.X.S.X., Zhang, Z.L.P., Xiong, R.K.V.C.Y., Elhoseiny, M.: Minigpt-v2: Large language model as a unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478 (2023)
[12] Chen, L., Li, J., wen Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., Lin, D.: Sharegpt4v: Improving large multi-modal models with better captions. ArXiv abs/2311.12793 (2023), https://api.semanticscholar.org/CorpusID:265308687
[13] Chiang, W.L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J.E., Stoica, I., Xing, E.P.: Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality. https://lmsys.org/blog/2023-03-30-vicuna/ (March 2023)
[14] Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al.: Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 (2021)
[15] Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.: Instructblip: Towards general-purpose vision-language models with instruction tuning (2023)
[16] Dong, X., Zhang, P., Zang, Y., Cao, Y., Wang, B., Ouyang, L., Wei, X., Zhang, S., Duan, H., Cao, M., et al.: Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420 (2024)
[17] Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X., Li, K., Sun, X., Wu, Y., Ji, R.: Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394 (2023)
[18] Fu, C., Zhang, R., Lin, H., Wang, Z., Gao, T., Luo, Y., Huang, Y., Zhang, Z., Qiu, L., Ye, G., et al.: A challenger to gpt-4v? early explorations of gemini in visual expertise. arXiv preprint arXiv:2312.12436 (2023)
[19] Gao, J., Pi, R., Zhang, J., Ye, J., Zhong, W., Wang, Y., Hong, L., Han, J., Xu, H., Li, Z., et al.: G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370 (2023)
[20] Gao, P., Han, J., Zhang, R., Lin, Z., Geng, S., Zhou, A., Zhang, W., Lu, P., He, C., Yue, X., Li, H., Qiao, Y.: Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010 (2023)
[21] Gao, P., Zhang, R., Liu, C., Qiu, L., Huang, S., Lin, W., Zhao, S., Geng, S., Lin, Z., Jin, P., et al.: Sphinx-x: Scaling data and parameters for a family of multi-modal large language models. arXiv preprint arXiv:2402.05935 (2024)</p>
<p>[22] Gemini Team, G.: Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023)
[23] Guo, Z., Zhang, R., Zhu, X., Tang, Y., Ma, X., Han, J., Chen, K., Gao, P., Li, X., Li, H., et al.: Point-bind \&amp; point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following. arXiv preprint arXiv:2309.00615 (2023)
[24] Han, J., Zhang, R., Shao, W., Gao, P., Xu, P., Xiao, H., Zhang, K., Liu, C., Wen, S., Guo, Z., et al.: Imagebind-llm: Multi-modality instruction tuning. arXiv preprint arXiv:2309.03905 (2023)
[25] Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., Steinhardt, J.: Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR) (2021)
[26] Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., Steinhardt, J.: Measuring mathematical problem solving with the math dataset. NeurIPS (2021)
[27] Hong, Y., Zhen, H., Chen, P., Zheng, S., Du, Y., Chen, Z., Gan, C.: 3d-llm: Injecting the 3d world into large language models. Advances in Neural Information Processing Systems 36 (2024)
[28] Jiang, A.Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D.S., de Las Casas, D., Hanna, E.B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L.R., Saulnier, L., Lachaux, M., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T.L., Gervet, T., Lavril, T., Wang, T., Lacroix, T., Sayed, W.E.: Mixtral of experts. Arxiv 2401.04088 (2024)
[29] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. arXiv preprint arXiv:2304.02643 (2023)
[30] Li, B., Zhang, K., Zhang, H., Guo, D., Zhang, R., Li, F., Zhang, Y., Liu, Z., Li, C.: Llava-next: Stronger llms supercharge multimodal capabilities in the wild. https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/ (2024)
[31] Li, B., Zhang, Y., Chen, L., Wang, J., Pu, F., Yang, J., Li, C., Liu, Z.: Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425 (2023)
[32] Li, B., Zhang, Y., Guo, D., Zhang, R., Li, F., Zhang, H., Zhang, K., Li, Y., Liu, Z., Li, C.: Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326 (2024)
[33] Li, B., Wang, R., Wang, G., Ge, Y., Ge, Y., Shan, Y.: Seed-bench: Benchmarking multimodal llms with generative comprehension. ArXiv abs/2307.16125 (2023)
[34] Li, F., Zhang, R., Zhang, H., Zhang, Y., Li, B., Li, W., Ma, Z., Li, C.: Llava-nextinterleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895 (2024)
[35] Li, J., Li, D., Xiong, C., Hoi, S.: Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In: International Conference on Machine Learning. pp. 12888-12900. PMLR (2022)
[36] Lin, Z., Liu, C., Zhang, R., Gao, P., Qiu, L., Xiao, H., Qiu, H., Lin, C., Shao, W., Chen, K., et al.: Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575 (2023)
[37] Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning (2023)
[38] Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., Lee, Y.J.: Llava-next: Improved reasoning, ocr, and world knowledge (January 2024), https://llava-vl.github.io/blog/ 2024-01-30-llava-next/
[39] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. In: NeurIPS (2023)</p>
<p>[40] Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., et al.: Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281 (2023)
[41] Lu, P., Bansal, H., Xia, T., Liu, J., yue Li, C., Hajishirzi, H., Cheng, H., Chang, K.W., Galley, M., Gao, J.: Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models. ArXiv abs/2310.02255 (2023)
[42] Lu, P., Gong, R., Jiang, S., Qiu, L., Huang, S., Liang, X., Zhu, S.C.: Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165 (2021)
[43] Lu, P., Gong, R., Jiang, S., Qiu, L., Huang, S., Liang, X., Zhu, S.C.: Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. In: Annual Meeting of the Association for Computational Linguistics (2021), https://api.semanticscholar. org/CorpusID:234337054
[44] Luo, H., Sun, Q., Xu, C., Zhao, P., Lou, J., Tao, C., Geng, X., Lin, Q., Chen, S., Zhang, D.: Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583 (2023)
[45] OpenAI: Chatgpt. https://chat.openai.com (2023)
[46] OpenAI: Gpt-4 technical report. ArXiv abs/2303.08774 (2023)
[47] OpenAI: GPT-4V(ision) system card (2023), https://openai.com/research/ gpt-4v-system-card
[48] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Gray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., Lowe, R.: Training language models to follow instructions with human feedback. In: Advances in Neural Information Processing Systems (2022)
[49] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models from natural language supervision. In: International Conference on Machine Learning (2021), https://api.semanticscholar.org/CorpusID:231591445
[50] Roy, S., Roth, D.: Solving general arithmetic word problems. ArXiv abs/1608.01413 (2016), https://api.semanticscholar.org/CorpusID:560565
[51] Seo, M., Hajishirzi, H., Farhadi, A., Etzioni, O., Malcolm, C.: Solving geometry problems: Combining text and diagram interpretation. In: Proceedings of the 2015 conference on empirical methods in natural language processing. pp. 1466-1476 (2015)
[52] Su, Y., Lan, T., Li, H., Xu, J., Wang, Y., Cai, D.: Pandagpt: One model to instruction-follow them all. arXiv preprint arXiv:2305.16355 (2023)
[53] Sun, K., Pan, J., Ge, Y., Li, H., Duan, H., Wu, X., Zhang, R., Zhou, A., Qin, Z., Wang, Y., et al.: Journeydb: A benchmark for generative image understanding. Advances in Neural Information Processing Systems 36 (2024)
[54] Team, I.: Internlm: A multilingual language model with progressively enhanced capabilities (2023)
[55] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023)
[56] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)</p>
<p>[57] Wang, K., Ren, H., Zhou, A., Lu, Z., Luo, S., Shi, W., Zhang, R., Song, L., Zhan, M., Li, H.: Mathcoder: Seamless code integration in LLMs for enhanced mathematical reasoning. In: The Twelfth International Conference on Learning Representations (2024), https://openreview. net/forum?id=z8TW0ttBPp
[58] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou, D., et al.: Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems 35, 24824-24837 (2022)
[59] Xu, P., Shao, W., Zhang, K., Gao, P., Liu, S., Lei, M., Meng, F., Huang, S., Qiao, Y., Luo, P.: Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models. arXiv preprint arXiv:2306.09265 (2023)
[60] Xu, R., Wang, X., Wang, T., Chen, Y., Pang, J., Lin, D.: Pointllm: Empowering large language models to understand point clouds. arXiv preprint arXiv:2308.16911 (2023)
[61] Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., Jiang, C., Li, C., Xu, Y., Chen, H., Tian, J., Qian, Q., Zhang, J., Huang, F.: mplug-owl: Modularization empowers large language models with multimodality (2023)
[62] Ye, Q., Xu, H., Ye, J., Yan, M., Hu, A., Liu, H., Qian, Q., Zhang, J., Huang, F., Zhou, J.: mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration (2023)
[63] Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., Wei, C., Yu, B., Yuan, R., Sun, R., Yin, M., Zheng, B., Yang, Z., Liu, Y., Huang, W., Sun, H., Su, Y., Chen, W.: Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502 (2023)
[64] Yue, X., Qu, X., Zhang, G., Fu, Y., Huang, W., Sun, H., Su, Y., Chen, W.: Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653 (2023)
[65] Zhang, H., Li, X., Bing, L.: Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858 (2023)
[66] Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Gao, P., Qiao, Y.: LLaMA-adapter: Efficient fine-tuning of large language models with zero-initialized attention. In: The Twelfth International Conference on Learning Representations (2024), https://openreview.net/ forum?id=d4UiXAHN2W
[67] Zhang, R., Hu, X., Li, B., Huang, S., Deng, H., Li, H., Qiao, Y., Gao, P.: Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners. CVPR 2023 (2023)
[68] Zhang, R., Jiang, Z., Guo, Z., Yan, S., Pan, J., Dong, H., Gao, P., Li, H.: Personalize segment anything model with one shot. ICLR 2024 (2023)
[69] Zhang, R., Wang, L., Qiao, Y., Gao, P., Li, H.: Learning 3d representations from 2d pre-trained models via image-to-point masked autoencoders. CVPR 2023 (2023)
[70] Zhang, R., Wei, X., Jiang, D., Zhang, Y., Guo, Z., Tong, C., Liu, J., Zhou, A., Wei, B., Zhang, S., et al.: Mavis: Mathematical visual instruction tuning. arXiv preprint arXiv:2407.08739 (2024)
[71] Zhou, A., Wang, K., Lu, Z., Shi, W., Luo, S., Qin, Z., Lu, S., Jia, A., Song, L., Zhan, M., et al.: Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. arXiv preprint arXiv:2308.07921 (2023)
[72] Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023)</p>
<h1>Appendix Overview</h1>
<ul>
<li>Section A: Related work.</li>
<li>Section B: Additional experimental details.</li>
<li>Section C: More dataset details.</li>
<li>Section D: Comparison to current benchmarks.</li>
<li>Section E: Limitation and future work.</li>
<li>Section F: Qualitative examples.</li>
</ul>
<h2>A Related Work</h2>
<p>Multi-modal Large Language Models (MLLMs), building upon the prevalence of Large Language Models (LLMs) [55, 56, 45, 28, 4] and large vision models [49, 29, 68, 67, 69], have become increasingly prominent in the field. They extend LLMs to tackle a diverse range of tasks and domains, including the mainstream 2D images [35, 15, 1, 31] and other modalities, such as 3D point clouds [23, 60, 27], audio [24, 52], and video [65, 6]. Noteworthy examples like OpenAI's GPT-4V [47] and Google's Gemini [22] exhibit exceptional visual understanding and reasoning capabilities, setting new benchmarks in multi-modal performance. However, their closed-source nature poses a barrier to the broader application and development of MLLMs. Concurrently, another line of work is dedicated to exploring advanced MLLMs open-source to the community. Prior efforts like LLaMA-Adapter [66, 20], LLaVA [39, 38, 37], and MiniGPT-4 [72, 11] leverage a frozen CLIP [49] model for image encoding, and inject the visual cues into LLaMA [55] for multi-modal instruction tuning. The subsequent mPLUG-Owl [61, 62], Qwen-VL [3], InternLM-XComposer [16], and SPHINX [36, 21] further push the frontier of MLLMs in understanding and generalizing across visual contexts. Despite comprehensive benchmarks [17, 40, 33, 59] on general visual instruction-following scenarios, the specific potential of MLLMs for visual mathematical problem-solving remains underexplored. In this paper, we introduce the MATHVERSE benchmark to comprehensively evaluate the visual mathematical reasoning and diagram understanding skills of MLLMs, providing unique perspectives for future research directions.</p>
<p>Mathematical Reasoning Benchmarks have emerged as a significant area of focus, posing considerable challenges for large foundational models, e.g., LLMs and MLLMs. Initially, datasets in this realm are designed to address basic algebraic [26] and arithmetic [50] word problems, which are relatively limited in scope and volume. Subsequent efforts, including MATH [26], GSM8K [14], and MMLU [25], expand the range and quality of textual mathematical problems. These datasets feature a broader spectrum of difficulties, establishing a robust benchmark for the evaluation of general and math-specific LLMs [71, 64, 57, 19, 44]. Besides the text-only assessment, there is a growing demand for comparable, high-quality benchmarks for evaluating mathematical problemsolving in visual contexts, with the rapid progress of MLLMs. There are prior attempts, such as GeoQA [10], UniGeo [8], and Geometry3K [43], which focused exclusively on geometric problems. The recently proposed MathVista [41] broadens the scope to incorporate a variety of multi-modal tasks involving mathematical reasoning, and MMMU [63] covers college-level questions demanding intricate, domain-specific knowledge. However, our analysis identifies three main shortcomings within the current visual math benchmarks, as elaborated in Section 1 of the main paper. Therefore, we propose MATHVERSE specialized in the multi-modal mathematical evaluation of MLLMs, comprising twelve subjects, six problem versions, and 20K test samples. Our objective is to thoroughly investigate whether and how much MLLMs genuinely interpret visual diagrams for mathematical reasoning.</p>
<h2>B Additional Experimental Details</h2>
<p>Model Sources. For different MLLMs, we select their latest models and best-performing configurations for evaluation to fully reveal their visual mathematical proficiency. Table 4 presents the release time and model sources of MLLMs used in MATHVERSE.</p>
<p>Table 4: The Release Time and Model Source of MLLMs Used in MATHVERSE.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Release <br> Time</th>
<th style="text-align: center;">Source</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ChatGPT [48]</td>
<td style="text-align: center;">2022-11</td>
<td style="text-align: center;">https://platform.openai.com/</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 [46]</td>
<td style="text-align: center;">2023-03</td>
<td style="text-align: center;">https://platform.openai.com/</td>
</tr>
<tr>
<td style="text-align: center;">Qwen-VL-Plus [3]</td>
<td style="text-align: center;">2023-11</td>
<td style="text-align: center;">https://help.aliyun.com/zh/ <br> dashscope/developer-reference/ <br> vl-plus-quick-start</td>
</tr>
<tr>
<td style="text-align: center;">Gemini-Pro [22]</td>
<td style="text-align: center;">2023-12</td>
<td style="text-align: center;">https://ai.google.dev/</td>
</tr>
<tr>
<td style="text-align: center;">Qwen-VL-Max [3]</td>
<td style="text-align: center;">2024-01</td>
<td style="text-align: center;">https://help.aliyun.com/zh/ <br> dashscope/developer-reference/ <br> vl-plus-quick-start</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4V [47]</td>
<td style="text-align: center;">2023-09</td>
<td style="text-align: center;">https://platform.openai.com/</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-Adapter V2 [20]</td>
<td style="text-align: center;">2023-04</td>
<td style="text-align: center;">https://github.com/OpenGVLab/ <br> LLaMA-Adapter/tree/main/llama_ <br> adapter_v2_multimodal7b</td>
</tr>
<tr>
<td style="text-align: center;">LLaVA-1.5 [37]</td>
<td style="text-align: center;">2023-10</td>
<td style="text-align: center;">https://huggingface.co/ <br> liuhaotian/llava-v1.5-13b</td>
</tr>
<tr>
<td style="text-align: center;">MiniGPT-v2 [11]</td>
<td style="text-align: center;">2023-10</td>
<td style="text-align: center;">https://github.com/ <br> Vision-CAIR/MiniGPT-4</td>
</tr>
<tr>
<td style="text-align: center;">mPLUG-Owl2 [62]</td>
<td style="text-align: center;">2023-11</td>
<td style="text-align: center;">https://huggingface.co/ <br> MAGAer13/mplug-owl2-llama2-7b</td>
</tr>
<tr>
<td style="text-align: center;">G-LLaVA [19]</td>
<td style="text-align: center;">2023-12</td>
<td style="text-align: center;">https://github.com/pipilurj/ <br> G-LLaVA/tree/main</td>
</tr>
<tr>
<td style="text-align: center;">ImageBind-LLM [24]</td>
<td style="text-align: center;">2023-05</td>
<td style="text-align: center;">https://github.com/OpenGVLab/ <br> LLaMA-Adapter/tree/main/ <br> imagebind_LLM</td>
</tr>
<tr>
<td style="text-align: center;">ShareGPT4V [12]</td>
<td style="text-align: center;">2023-11</td>
<td style="text-align: center;">https://huggingface.co/ <br> Lin-Chen/ShareGPT4V-13B</td>
</tr>
<tr>
<td style="text-align: center;">SPHINX-Plus [36]</td>
<td style="text-align: center;">2023-11</td>
<td style="text-align: center;">https://huggingface.co/ <br> Alpha-VLLM/LLaMA2-Accessory/ <br> tree/main/finetune/mm/SPHINX/ <br> SPHINX-v2-ik</td>
</tr>
<tr>
<td style="text-align: center;">LLaVA-NeXT [38]</td>
<td style="text-align: center;">2024-01</td>
<td style="text-align: center;">https://huggingface.co/ <br> liuhaotian/llava-v1. <br> 6-vicuna-13b</td>
</tr>
<tr>
<td style="text-align: center;">SPHINX-MoE [21]</td>
<td style="text-align: center;">2024-01</td>
<td style="text-align: center;">https://huggingface.co/ <br> Alpha-VLLM/LLaMA2-Accessory/ <br> tree/main/finetune/mm/SPHINX/ <br> SPHINX-MoE</td>
</tr>
<tr>
<td style="text-align: center;">InternLM-XComposer2 [16]</td>
<td style="text-align: center;">2024-01</td>
<td style="text-align: center;">https://huggingface. <br> co/internlm/ <br> internlm-xcomposer2-v1-7b</td>
</tr>
</tbody>
</table>
<p>Table 5: Input Prompt of MLLMs for Response Generation. We adopt two different prompts for the free-form and multiple-choice questions. Note that these prompts are used for five problem versions except for the Vision-only version.</p>
<table>
<thead>
<tr>
<th>Question</th>
<th>Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td>Free-form Question</td>
<td>Please first conduct reasoning, and then answer the question and provide the final value, e.g., 1, 2.5, 300, at the end. <br> - Question: {question}</td>
</tr>
<tr>
<td>Multiple-choice Question</td>
<td>Please first conduct reasoning, and then answer the question and provide the correct option letter, e.g., A, B, C, D, at the end. <br> - Question: {question}</td>
</tr>
</tbody>
</table>
<p>Table 6: Input Prompt for Vision-only Problems. Especially for the Vision-only version without textual input, we add "According to the question shown in the image" at the beginning of the prompt, and remove the "Question:" at the end.</p>
<table>
<thead>
<tr>
<th>Question</th>
<th>Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td>Free-form Question</td>
<td>According to the question shown in the image, please first conduct reasoning, and then answer the question and provide the final value, e.g., 1, 2.5, 300, at the end.</td>
</tr>
<tr>
<td>Multiple-choice Question</td>
<td>According to the question shown in the image, please first conduct reasoning, and then answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.</td>
</tr>
</tbody>
</table>
<p>Prompt for Response Generation. We adopt two types of prompts respectively for the free-form and multiple-choice questions, as shown in Table 5. We inspire the Chain-of-Thought (CoT) reasoning capabilities of MLLMs by using the phrase "first conduct reasoning". Especially for the Vision-only problem version in Table 6, we add "According to the question shown in the image" at the beginning to remind MLLMs to read the questions rendered within diagrams, where the textual input for MLLMs only contains the prompt itself.</p>
<p>Prompt for the CoT Evaluation. Our proposed CoT evaluation contains two steps, i.e., key-step extraction and multi-step scoring, which prompt GPT-4 [46] and GPT-4V [47], respectively. The input configuration is listed in Table 7. We utilize the text-only GPT-4 in the first step to extract multiple key steps within the model's unstructured output, without feeding the question information. In the second step, we input the extracted key-step reasoning and all the available content related to the problem into GPT-4V, allowing for a holistic assessment, including diagram interpretation, logical reasoning, and numerical computation. In Figure 11, we showcase the manual annotation for critical information within functional diagrams, e.g., function expression and properties. This assists GPT-4V in evaluating the visual perception accuracy of MLLMs for function graphs.</p>
<p>Human Performance Assessment. We recruit ten qualified college students specifically for the evaluation of human performance on MATHVERSE. These individuals are kept separate from the data curation stage, eliminating the possibility of them encountering the solutions beforehand. We allocate to each student the questions from a specific problem version. This strategy is to prevent them from gaining additional information from another version to answer questions, e.g., leveraging the textual Implicit Property from the Text-lite version to solve Text-intensive problems. They are asked to directly provide the final answer without detailed reasoning. Therefore, we do not report the CoT evaluation results for human performance, alongside the 'Random Chance' baseline.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://homework.study.com
${ }^{2}$ https://www.ixl.com/math
${ }^{3}$ https://mathspace.co/us&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
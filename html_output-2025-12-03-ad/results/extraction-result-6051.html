<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6051 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6051</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6051</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-270738074</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.18403v1.pdf" target="_blank">LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks</a></p>
                <p><strong>Paper Abstract:</strong> There is an increasing trend towards evaluating NLP models with LLMs instead of human judgments, raising questions about the validity of these evaluations, as well as their reproducibility in the case of proprietary models. We provide J UDGE -B ENCH , an extensible collection of 20 NLP datasets with human annotations covering a broad range of evaluated properties and types of data, and comprehensively evaluate 11 current LLMs, covering both open-weight and proprietary models, for their ability to replicate the annotations. Our evaluations show substantial variance across models and datasets. Models are reliable evaluators on some tasks, but overall display substantial variability depending on the property being evaluated, the expertise level of the human judges, and whether the language is human or model-generated. We conclude that LLMs should be carefully validated against human judgments before being used as evaluators.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6051.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6051.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JUDGE-BENCH (overall)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>JUDGE-BENCH: A large-scale benchmark comparing LLM judges to human annotations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale empirical evaluation (20 datasets, >70k instances) that directly compares judgments from 11 LLMs (open and proprietary) to original human annotations across diverse NLP tasks, reporting per-dataset Spearman correlations and Cohen's κ and estimating human upper bounds via bootstrapping.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Cross-task (summarization, translation, dialogue, reasoning, toxicity/safety, planning, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Eleven LLMs including GPT-4o, Gemini-1.5, Llama-3.1-70B, Mixtral-8x22B, Mixtral-8x7B, Mistral-7B, Starling-7B, Comm-R/Comm-R+, OLMo, etc. (proprietary and open-source models evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Uses the original human annotations of each dataset (various setups): many datasets have 3 individual annotations (crowdsourced non-experts or expert annotators); some datasets use expert professional translators (WMT) or domain experts (medical); where multiple annotations exist, bootstrapped single-rater vs aggregated responses are used to estimate an upper bound.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Spearman's correlation for graded annotations; Cohen's κ for categorical annotations; valid response rate (fraction of model outputs matching expected label format); bootstrapped upper bound estimates of human-model alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Substantial variability across models and datasets: on some tasks (e.g., instruction following, some reasoning traces) LLMs align well with humans; in many datasets model-human agreement remains below the human upper bound; proprietary GPT-4o often ranks highest but large open models (Llama-3.1-70B, Mixtral-8x22B) are close and sometimes outperform GPT-4o on specific tasks (e.g., CoLA, SummEval). Overall, no single model dominates across all properties.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>High variance across properties and datasets; sensitivity to dataset/task type; reproducibility concerns for closed/proprietary models; some tasks produce low valid response rates (sensitive/safety tasks); models sometimes produce outputs not matching requested format (e.g., explanations or refusals), forcing dataset-specific handling.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Many datasets show model scores substantially below human upper bounds; particularly low or even negative correlations on toxicity and safety datasets; low alignment on 'engagingness'; inconsistent behavior when evaluating machine-generated outputs vs human language; valid response rate issues requiring replacement of invalid responses in evaluation protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Authors recommend validating and calibrating LLM judges against task-specific human annotations before deployment; release of JUDGE-BENCH to facilitate systematic validation; estimate human upper bounds to contextualize model performance; prefer open models when reproducibility is important; and consider using multiple models/panels (see follow-on references).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6051.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6051.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Toxicity & Safety evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Toxicity and Safety judgments (DICES, ToxicChat, Medical-safety)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparisons on toxicity/safety datasets show poor and inconsistent alignment between LLM judges and human annotators: low correlations, low valid response rates, and frequent model refusals or selection of conservative labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Toxicity and conversational safety evaluation (DICES-990, DICES-350, ToxicChat, Medical-safety)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Multiple models from the 11 evaluated (notably many proprietary and open models), with per-model valid response rates and scores reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Datasets include crowdsourced and expert annotations (e.g., DICES-990 crowdsourced; DICES-350 includes both expert and crowdsourced; ToxicChat uses human annotators; Medical-safety annotated for query severity and answer risk level).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Categorical agreement measured with Cohen's κ; valid response rates; qualitative inspection of model outputs (refusals, explanations).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Model-human agreement on toxicity/safety is low and sometimes negative; valid response rates for some models are particularly low on medical and safety tasks; models tend to choose conservative labels (e.g., 'Unsure' or 'Unsafe') or refuse, diverging from human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Safety guardrails and content-moderation policies lead models to refuse judgments or to output explanations instead of labels; tendency to select 'Unsure'/'Unsafe' labels; lower valid response rates and lower agreement on these domains; domain-specific nuance (medical) not reliably captured by many models.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Medical-safety: many models refused to provide a label and instead generated explanations or copied prompt text, reducing measurable agreement. DICES: models preferred 'Unsure' even when humans labeled safe/unsafe, leading to low κ. ToxicChat: mixed performance but generally better than DICES/Medical-safety.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Prompt paraphrases, few-shot examples and CoT were tested but did not yield consistent improvements; authors recommend validating LLM judges specifically on sensitive domains before use, examining valid-response behavior, and not trusting direct label outputs without calibration; consider domain-specific fine-tuning or specialized evaluation methods (and consult fine-tuning work such as JudgeLM).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6051.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6051.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Expert vs Non-expert alignment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differential alignment between LLM judges and expert versus non-expert human annotators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Across graded annotation datasets, LLMs generally correlate more strongly with non-expert (crowdsourced) annotators than with expert annotators, suggesting LLMs mirror surface-level judgments more than domain-expert criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Graded property judgments across summarization, dialogue, translation, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>All evaluated LLMs (aggregate finding across the set of models)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Datasets contain either crowdsourced non-expert raters or domain experts (e.g., professional translators for WMT); when available, multiple individual annotations are used and aggregated; bootstrapped upper bound estimates computed.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Spearman's correlation between LLM scores and human annotations, compared separately to expert and non-expert human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Higher LLM-human correlation with non-expert annotations than with expert annotations across graded properties; hypothesis is that non-experts rely on surface-level features aligning better with patterns LLMs capture, while experts use stricter, domain-specific criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>LLMs may lack the fine-grained, domain-specific criteria that experts apply, causing divergence when expert judgments demand specialized knowledge or strict standards.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>On tasks where expert criteria are crucial (e.g., professional translation quality measures used in WMT), LLM-human alignment is lower than for crowdsourced judgments; models thus risk overestimating agreement if only non-expert annotations are considered.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Validate LLM judges separately against expert annotations when the target evaluation requires domain expertise; estimate human upper bounds and check whether apparent LLM-human agreement is due to shared superficial biases; consider using domain-expert calibration or fine-tuning of judge models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6051.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6051.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human vs Machine-generated item bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM bias when evaluating human-generated versus machine-generated language</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs align better with human judgments when the items being judged are human-written than when assessing machine-generated outputs, consistent with a bias in LLMs toward their own-generation style.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Any dataset split by source of text: human-generated vs model-generated (e.g., dialogue, summarization, translation outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>All evaluated models (aggregate result reported across models)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Datasets are annotated for items that are either human-produced (e.g., grammaticality judgments) or model-produced outputs (e.g., system translations, generated summaries); human annotations are provided per original datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Spearman's correlation for graded and Cohen's κ for categorical annotations, computed separately for human-written and machine-generated items.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Across all models and both graded and categorical annotations, alignment with humans is higher on human-generated items than on machine-generated outputs, indicating LLMs are comparatively worse at evaluating other models' outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Positive bias toward human-like text or towards their own generation patterns (reported as LLMs displaying a bias towards their own generation in prior work); evaluations of model outputs can therefore be distorted and over- or under-estimate quality.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Many datasets involving machine-generated text show lower agreement than analogous human-generated cases; this effect is broad across properties and models.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Authors advise caution when using LLMs to evaluate outputs of NLP systems and recommend task-specific validation; potential approaches include using panels of diverse models, calibration against human judgments on machine-generated items, or methods that explicitly detect model-origin biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6051.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6051.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompting & CoT effects</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effectiveness of prompting strategies (Chain-of-Thought, few-shot, paraphrases) for eliciting human-like judgments from LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper experiments with Chain-of-Thought (CoT), few-shot exemplars, and prompt paraphrases across selected datasets and models, finding inconsistent and dataset-/model-dependent effects with no systematic improvement in human-model agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Various evaluation tasks (selected datasets where models struggled: DICES-350-expert, WMT-2023 En-De/Zh-En, CoLa grammar, plus broader CoT experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Subset of models (CoT experiments: 9 models; some models excluded due to compute constraints; CoT runs for proprietary models also conducted), e.g., GPT-4o and several open models.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Same human annotations as original datasets; CoT prompts appended to original instructions to encourage stepwise reasoning and make final labels extractable; few-shot used small numbers of exemplars; paraphrases adapted prompt wording.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Spearman's correlation and Cohen's κ comparing LLM outputs under different prompting strategies to human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>CoT prompting led to improved agreement for some models on some datasets but the effect is inconsistent overall; paraphrased prompts and few-shot sometimes helped for specific model/dataset pairs (e.g., Llama 3.1-8B on WMT-2023 yielded modest improvements) but no systematic gains across the board.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Chain-of-Thought does not reliably improve evaluator alignment (consistent with other recent findings); prompting gains are model- and dataset-specific and thus not a general remedy.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>CoLa grammar: very low valid response rate for GPT-4o with CoT prompts (only 10% sampled due to slow processing); DICES-350-expert and WMT-2023 datasets showed inconsistent responses to prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Do not rely on a single prompting trick as a universal solution; empirically test prompting variants per task and model; consider more robust approaches such as fine-tuning judges, using ensembled panels, or developing task-specific elicitation protocols (and evaluate their effects on valid response rate and agreement).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6051.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6051.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model-specific reproducibility & ranking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Performance ranking and reproducibility concerns of proprietary vs open LLM judges</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o often ranks first across many scenarios, but several large open-weight models (e.g., Llama-3.1-70B, Mixtral-8x22B) achieve similar performance and sometimes outperform GPT-4o on particular tasks; closed models pose reproducibility concerns because they can be changed/retired.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Cross-task (acceptability, summary quality, translation, categorical acceptability etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4o (proprietary) and open-weight models (Llama-3.1-70B, Mixtral-8x22B, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Standard dataset human annotations; aggregated comparisons produced per-model scores reported in tables and figures (Spearman/κ).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Spearman's correlation for graded tasks; Cohen's κ for categorical tasks; valid response rates.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>GPT-4o ranks highly overall, but differences versus top open models are often modest and task-dependent; open models sometimes outperform GPT-4o on categorical acceptability (CoLA) and graded summary quality (SummEval).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Closed/proprietary models raise reproducibility concerns (retraining/retirement); model performance varies heavily across properties, so superiority in one metric doesn't generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>No single model is best across all properties; choosing a single proprietary model as a universal evaluator is problematic.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Favor validation on task-specific human judgments before deployment; consider open models for reproducibility; use multiple models or model ensembles/panels to reduce single-model idiosyncrasies; track valid-response behaviour and update evaluation protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>G-eval: NLG evaluation using gpt-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models <em>(Rating: 2)</em></li>
                <li>JudgeLM: Fine-tuned Large Language Models are Scalable Judges <em>(Rating: 2)</em></li>
                <li>Benchmarking cognitive biases in large language models as evaluators <em>(Rating: 2)</em></li>
                <li>DICES dataset: Diversity in conversational AI evaluation for safety <em>(Rating: 1)</em></li>
                <li>Are large language models state-of-the-art evaluators of translation quality <em>(Rating: 1)</em></li>
                <li>Is ChatGPT a good NLG evaluator? a preliminary study <em>(Rating: 1)</em></li>
                <li>LLMs as narcissistic evaluators: When ego inflates evaluation scores <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6051",
    "paper_id": "paper-270738074",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "JUDGE-BENCH (overall)",
            "name_full": "JUDGE-BENCH: A large-scale benchmark comparing LLM judges to human annotations",
            "brief_description": "A large-scale empirical evaluation (20 datasets, &gt;70k instances) that directly compares judgments from 11 LLMs (open and proprietary) to original human annotations across diverse NLP tasks, reporting per-dataset Spearman correlations and Cohen's κ and estimating human upper bounds via bootstrapping.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Cross-task (summarization, translation, dialogue, reasoning, toxicity/safety, planning, etc.)",
            "llm_judge_model": "Eleven LLMs including GPT-4o, Gemini-1.5, Llama-3.1-70B, Mixtral-8x22B, Mixtral-8x7B, Mistral-7B, Starling-7B, Comm-R/Comm-R+, OLMo, etc. (proprietary and open-source models evaluated)",
            "human_evaluation_setup": "Uses the original human annotations of each dataset (various setups): many datasets have 3 individual annotations (crowdsourced non-experts or expert annotators); some datasets use expert professional translators (WMT) or domain experts (medical); where multiple annotations exist, bootstrapped single-rater vs aggregated responses are used to estimate an upper bound.",
            "metrics_compared": "Spearman's correlation for graded annotations; Cohen's κ for categorical annotations; valid response rate (fraction of model outputs matching expected label format); bootstrapped upper bound estimates of human-model alignment.",
            "reported_differences": "Substantial variability across models and datasets: on some tasks (e.g., instruction following, some reasoning traces) LLMs align well with humans; in many datasets model-human agreement remains below the human upper bound; proprietary GPT-4o often ranks highest but large open models (Llama-3.1-70B, Mixtral-8x22B) are close and sometimes outperform GPT-4o on specific tasks (e.g., CoLA, SummEval). Overall, no single model dominates across all properties.",
            "llm_specific_limitations": "High variance across properties and datasets; sensitivity to dataset/task type; reproducibility concerns for closed/proprietary models; some tasks produce low valid response rates (sensitive/safety tasks); models sometimes produce outputs not matching requested format (e.g., explanations or refusals), forcing dataset-specific handling.",
            "notable_failure_cases": "Many datasets show model scores substantially below human upper bounds; particularly low or even negative correlations on toxicity and safety datasets; low alignment on 'engagingness'; inconsistent behavior when evaluating machine-generated outputs vs human language; valid response rate issues requiring replacement of invalid responses in evaluation protocol.",
            "mitigation_strategies": "Authors recommend validating and calibrating LLM judges against task-specific human annotations before deployment; release of JUDGE-BENCH to facilitate systematic validation; estimate human upper bounds to contextualize model performance; prefer open models when reproducibility is important; and consider using multiple models/panels (see follow-on references).",
            "uuid": "e6051.0",
            "source_info": {
                "paper_title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Toxicity & Safety evaluations",
            "name_full": "Toxicity and Safety judgments (DICES, ToxicChat, Medical-safety)",
            "brief_description": "Comparisons on toxicity/safety datasets show poor and inconsistent alignment between LLM judges and human annotators: low correlations, low valid response rates, and frequent model refusals or selection of conservative labels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Toxicity and conversational safety evaluation (DICES-990, DICES-350, ToxicChat, Medical-safety)",
            "llm_judge_model": "Multiple models from the 11 evaluated (notably many proprietary and open models), with per-model valid response rates and scores reported.",
            "human_evaluation_setup": "Datasets include crowdsourced and expert annotations (e.g., DICES-990 crowdsourced; DICES-350 includes both expert and crowdsourced; ToxicChat uses human annotators; Medical-safety annotated for query severity and answer risk level).",
            "metrics_compared": "Categorical agreement measured with Cohen's κ; valid response rates; qualitative inspection of model outputs (refusals, explanations).",
            "reported_differences": "Model-human agreement on toxicity/safety is low and sometimes negative; valid response rates for some models are particularly low on medical and safety tasks; models tend to choose conservative labels (e.g., 'Unsure' or 'Unsafe') or refuse, diverging from human labels.",
            "llm_specific_limitations": "Safety guardrails and content-moderation policies lead models to refuse judgments or to output explanations instead of labels; tendency to select 'Unsure'/'Unsafe' labels; lower valid response rates and lower agreement on these domains; domain-specific nuance (medical) not reliably captured by many models.",
            "notable_failure_cases": "Medical-safety: many models refused to provide a label and instead generated explanations or copied prompt text, reducing measurable agreement. DICES: models preferred 'Unsure' even when humans labeled safe/unsafe, leading to low κ. ToxicChat: mixed performance but generally better than DICES/Medical-safety.",
            "mitigation_strategies": "Prompt paraphrases, few-shot examples and CoT were tested but did not yield consistent improvements; authors recommend validating LLM judges specifically on sensitive domains before use, examining valid-response behavior, and not trusting direct label outputs without calibration; consider domain-specific fine-tuning or specialized evaluation methods (and consult fine-tuning work such as JudgeLM).",
            "uuid": "e6051.1",
            "source_info": {
                "paper_title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Expert vs Non-expert alignment",
            "name_full": "Differential alignment between LLM judges and expert versus non-expert human annotators",
            "brief_description": "Across graded annotation datasets, LLMs generally correlate more strongly with non-expert (crowdsourced) annotators than with expert annotators, suggesting LLMs mirror surface-level judgments more than domain-expert criteria.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Graded property judgments across summarization, dialogue, translation, etc.",
            "llm_judge_model": "All evaluated LLMs (aggregate finding across the set of models)",
            "human_evaluation_setup": "Datasets contain either crowdsourced non-expert raters or domain experts (e.g., professional translators for WMT); when available, multiple individual annotations are used and aggregated; bootstrapped upper bound estimates computed.",
            "metrics_compared": "Spearman's correlation between LLM scores and human annotations, compared separately to expert and non-expert human annotations.",
            "reported_differences": "Higher LLM-human correlation with non-expert annotations than with expert annotations across graded properties; hypothesis is that non-experts rely on surface-level features aligning better with patterns LLMs capture, while experts use stricter, domain-specific criteria.",
            "llm_specific_limitations": "LLMs may lack the fine-grained, domain-specific criteria that experts apply, causing divergence when expert judgments demand specialized knowledge or strict standards.",
            "notable_failure_cases": "On tasks where expert criteria are crucial (e.g., professional translation quality measures used in WMT), LLM-human alignment is lower than for crowdsourced judgments; models thus risk overestimating agreement if only non-expert annotations are considered.",
            "mitigation_strategies": "Validate LLM judges separately against expert annotations when the target evaluation requires domain expertise; estimate human upper bounds and check whether apparent LLM-human agreement is due to shared superficial biases; consider using domain-expert calibration or fine-tuning of judge models.",
            "uuid": "e6051.2",
            "source_info": {
                "paper_title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Human vs Machine-generated item bias",
            "name_full": "LLM bias when evaluating human-generated versus machine-generated language",
            "brief_description": "LLMs align better with human judgments when the items being judged are human-written than when assessing machine-generated outputs, consistent with a bias in LLMs toward their own-generation style.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Any dataset split by source of text: human-generated vs model-generated (e.g., dialogue, summarization, translation outputs)",
            "llm_judge_model": "All evaluated models (aggregate result reported across models)",
            "human_evaluation_setup": "Datasets are annotated for items that are either human-produced (e.g., grammaticality judgments) or model-produced outputs (e.g., system translations, generated summaries); human annotations are provided per original datasets.",
            "metrics_compared": "Spearman's correlation for graded and Cohen's κ for categorical annotations, computed separately for human-written and machine-generated items.",
            "reported_differences": "Across all models and both graded and categorical annotations, alignment with humans is higher on human-generated items than on machine-generated outputs, indicating LLMs are comparatively worse at evaluating other models' outputs.",
            "llm_specific_limitations": "Positive bias toward human-like text or towards their own generation patterns (reported as LLMs displaying a bias towards their own generation in prior work); evaluations of model outputs can therefore be distorted and over- or under-estimate quality.",
            "notable_failure_cases": "Many datasets involving machine-generated text show lower agreement than analogous human-generated cases; this effect is broad across properties and models.",
            "mitigation_strategies": "Authors advise caution when using LLMs to evaluate outputs of NLP systems and recommend task-specific validation; potential approaches include using panels of diverse models, calibration against human judgments on machine-generated items, or methods that explicitly detect model-origin biases.",
            "uuid": "e6051.3",
            "source_info": {
                "paper_title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Prompting & CoT effects",
            "name_full": "Effectiveness of prompting strategies (Chain-of-Thought, few-shot, paraphrases) for eliciting human-like judgments from LLMs",
            "brief_description": "The paper experiments with Chain-of-Thought (CoT), few-shot exemplars, and prompt paraphrases across selected datasets and models, finding inconsistent and dataset-/model-dependent effects with no systematic improvement in human-model agreement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Various evaluation tasks (selected datasets where models struggled: DICES-350-expert, WMT-2023 En-De/Zh-En, CoLa grammar, plus broader CoT experiments)",
            "llm_judge_model": "Subset of models (CoT experiments: 9 models; some models excluded due to compute constraints; CoT runs for proprietary models also conducted), e.g., GPT-4o and several open models.",
            "human_evaluation_setup": "Same human annotations as original datasets; CoT prompts appended to original instructions to encourage stepwise reasoning and make final labels extractable; few-shot used small numbers of exemplars; paraphrases adapted prompt wording.",
            "metrics_compared": "Spearman's correlation and Cohen's κ comparing LLM outputs under different prompting strategies to human annotations.",
            "reported_differences": "CoT prompting led to improved agreement for some models on some datasets but the effect is inconsistent overall; paraphrased prompts and few-shot sometimes helped for specific model/dataset pairs (e.g., Llama 3.1-8B on WMT-2023 yielded modest improvements) but no systematic gains across the board.",
            "llm_specific_limitations": "Chain-of-Thought does not reliably improve evaluator alignment (consistent with other recent findings); prompting gains are model- and dataset-specific and thus not a general remedy.",
            "notable_failure_cases": "CoLa grammar: very low valid response rate for GPT-4o with CoT prompts (only 10% sampled due to slow processing); DICES-350-expert and WMT-2023 datasets showed inconsistent responses to prompt engineering.",
            "mitigation_strategies": "Do not rely on a single prompting trick as a universal solution; empirically test prompting variants per task and model; consider more robust approaches such as fine-tuning judges, using ensembled panels, or developing task-specific elicitation protocols (and evaluate their effects on valid response rate and agreement).",
            "uuid": "e6051.4",
            "source_info": {
                "paper_title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Model-specific reproducibility & ranking",
            "name_full": "Performance ranking and reproducibility concerns of proprietary vs open LLM judges",
            "brief_description": "GPT-4o often ranks first across many scenarios, but several large open-weight models (e.g., Llama-3.1-70B, Mixtral-8x22B) achieve similar performance and sometimes outperform GPT-4o on particular tasks; closed models pose reproducibility concerns because they can be changed/retired.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Cross-task (acceptability, summary quality, translation, categorical acceptability etc.)",
            "llm_judge_model": "GPT-4o (proprietary) and open-weight models (Llama-3.1-70B, Mixtral-8x22B, etc.)",
            "human_evaluation_setup": "Standard dataset human annotations; aggregated comparisons produced per-model scores reported in tables and figures (Spearman/κ).",
            "metrics_compared": "Spearman's correlation for graded tasks; Cohen's κ for categorical tasks; valid response rates.",
            "reported_differences": "GPT-4o ranks highly overall, but differences versus top open models are often modest and task-dependent; open models sometimes outperform GPT-4o on categorical acceptability (CoLA) and graded summary quality (SummEval).",
            "llm_specific_limitations": "Closed/proprietary models raise reproducibility concerns (retraining/retirement); model performance varies heavily across properties, so superiority in one metric doesn't generalize.",
            "notable_failure_cases": "No single model is best across all properties; choosing a single proprietary model as a universal evaluator is problematic.",
            "mitigation_strategies": "Favor validation on task-specific human judgments before deployment; consider open models for reproducibility; use multiple models or model ensembles/panels to reduce single-model idiosyncrasies; track valid-response behaviour and update evaluation protocols.",
            "uuid": "e6051.5",
            "source_info": {
                "paper_title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "G-eval: NLG evaluation using gpt-4 with better human alignment",
            "rating": 2,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        },
        {
            "paper_title": "Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models",
            "rating": 2,
            "sanitized_title": "replacing_judges_with_juries_evaluating_llm_generations_with_a_panel_of_diverse_models"
        },
        {
            "paper_title": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges",
            "rating": 2,
            "sanitized_title": "judgelm_finetuned_large_language_models_are_scalable_judges"
        },
        {
            "paper_title": "Benchmarking cognitive biases in large language models as evaluators",
            "rating": 2,
            "sanitized_title": "benchmarking_cognitive_biases_in_large_language_models_as_evaluators"
        },
        {
            "paper_title": "DICES dataset: Diversity in conversational AI evaluation for safety",
            "rating": 1,
            "sanitized_title": "dices_dataset_diversity_in_conversational_ai_evaluation_for_safety"
        },
        {
            "paper_title": "Are large language models state-of-the-art evaluators of translation quality",
            "rating": 1,
            "sanitized_title": "are_large_language_models_stateoftheart_evaluators_of_translation_quality"
        },
        {
            "paper_title": "Is ChatGPT a good NLG evaluator? a preliminary study",
            "rating": 1,
            "sanitized_title": "is_chatgpt_a_good_nlg_evaluator_a_preliminary_study"
        },
        {
            "paper_title": "LLMs as narcissistic evaluators: When ego inflates evaluation scores",
            "rating": 1,
            "sanitized_title": "llms_as_narcissistic_evaluators_when_ego_inflates_evaluation_scores"
        }
    ],
    "cost": 0.01420625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks
2 Jun 2025</p>
<p>Anna Bavaresco 
University of Amsterdam</p>
<p>Raffaella Bernardi 
University of Trento</p>
<p>Leonardo Bertolazzi 
University of Trento</p>
<p>Desmond Elliott 
University of Copenhagen</p>
<p>Raquel Fernández 
University of Amsterdam</p>
<p>Albert Gatt 
Utrecht University</p>
<p>Esam Ghaleb 
Max Planck Institute for Psycholinguistics
6 ETH Zürich</p>
<p>Mario Giulianelli 
Michael Hanna 
University of Amsterdam</p>
<p>Alexander Koller 
Saarland University</p>
<p>André F T Martins 
Universidade de Lisboa &amp; Unbabel</p>
<p>Philipp Mondorf 
LMU Munich &amp; MCML</p>
<p>Vera Neplenbroek 
University of Amsterdam</p>
<p>Sandro Pezzelle 
University of Amsterdam</p>
<p>Barbara Plank 
LMU Munich &amp; MCML</p>
<p>David Schlangen 
University of Potsdam
11 Heriot-Watt University 12 Amsterdam UMC</p>
<p>Alessandro Suglia 
Aditya K Surikuchi 
University of Amsterdam</p>
<p>Ece Takmaz 
Utrecht University</p>
<p>Alberto Testoni 
LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks
2 Jun 202549AEFCC66998A3BD206D03A19D5A24EEarXiv:2406.18403v3[cs.CL]
There is an increasing trend towards evaluating NLP models with LLMs instead of human judgments, raising questions about the validity of these evaluations, as well as their reproducibility in the case of proprietary models.We provide JUDGE-BENCH, an extensible collection of 20 NLP datasets with human annotations covering a broad range of evaluated properties and types of data, and comprehensively evaluate 11 current LLMs, covering both open-weight and proprietary models, for their ability to replicate the annotations.Our evaluations show substantial variance across models and datasets.Models are reliable evaluators on some tasks, but overall display substantial variability depending on the property being evaluated, the expertise level of the human judges, and whether the language is human or model-generated.We conclude that LLMs should be carefully validated against human judgments before being used as evaluators.</p>
<p>Introduction</p>
<p>For many natural language processing (NLP) tasks, the most informative evaluation is to ask humans to judge the model output.Such judgments are traditionally collected in lab experiments or through crowdsourcing, with either expert or non-expert annotators, as illustrated in Fig. 1.Recently, there has been a trend towards replacing human judgments with automatic assessments obtained via large language models (LLMs; Chiang and Lee, 2023;Wang et al., 2023;Liu et al., 2023;Li et al., 2024;Zheng et al., 2024, inter alia).For example, the LLM * Authors listed in alphabetical order.</p>
<p>Instruction: On a scale of 1 (very unlikely) to 5 (very likely), how plausible is it that the last response belongs to the dialogue?</p>
<p>A: Made it all the way through four years of college playing ball but B: I also like The Cosby Show  could be instructed to rate a response generated by a dialogue system for its perceived plausibility, on a scale from 1 to 5.This drastically reduces the evaluation effort and is claimed to yield more reliable results across multiple evaluation rounds (Landwehr et al., 2023;Jiang et al., 2023b;Reiter, 2024;Dubois et al., 2024).</p>
<p>At the same time, the use of LLMs as judges of linguistic output raises new concerns: LLMs may be prone to errors or systematic biases that differ from those of humans, especially on subtle tasks such as evaluating toxicity, or reasoning.This may distort evaluation results and lead to incorrect conclusions.The problem is aggravated by explicit or implicit data leakage (Balloccu et al., 2024), which undermines the ability to make broad, generalisable claims beyond the single specific dataset under analysis.Specifically for closed models such as OpenAI's GPT series, there are serious reproducibility concerns, as LLMs may be retrained or retired at any time, making subsequent comparisons invalid or impossible.</p>
<p>Previous studies offer mixed evidence regarding the reliability of LLM evaluators.Some research concludes that they are effective, correlating well with human judgments (Liu et al., 2023;Zheng et al., 2024;Chen et al., 2023;Verga et al., 2024;Törnberg, 2023;Huang et al., 2024;Naismith et al., 2023;Gilardi et al., 2023;Kocmi and Federmann, 2023b), albeit with some caveats (Wang et al., 2023;Wu and Aji, 2025;Hada et al., 2024;Pavlovic and Poesio, 2024).In some cases, LLM evaluators can also provide pairwise preference judgments (Kim et al., 2024;Liusie et al., 2024;Liu et al., 2024a;Park et al., 2024;Tan et al., 2025), or fine-grained evaluation beyond a single score, such as error spans (Fernandes et al., 2023;Kocmi and Federmann, 2023a).In contrast, some studies highlight substantial biases in LLMs' behaviour as evaluators, both as compared against human judgments (Koo et al., 2024;Zeng et al., 2024;Baris Schlicht et al., 2024) and through intrinsic analyses (Wang et al., 2024;Liu et al., 2024b;Stureborg et al., 2024).These discrepancies likely stem from the limitations of this previous work, which typically relies on a few datasets and models, often restricted to closed-source proprietary models.The observation of such limitations has motivated recent work to develop finetuning methods for LLM judges designed to overcome certain biases (Zhu et al., 2025).</p>
<p>In this paper, we examine how well current LLMs can approximate human evaluators on a large scale.We prompt 11 among the most recent open-weight and proprietary LLMs to generate judgments on 20 datasets with human annotations on a wide range of quality dimensions, prompt styles, and tasks.Our evaluation goes beyond existing work by including a wide variety of datasets that differ in the type of task (e.g., translation, dialogue generation, etc.), the property being judged (e.g., coherence, fluency, etc.), the type of judgments (categorical or graded), and the expertise of human annotators (experts or non-experts).We provide JUDGE-BENCH, a benchmark which includes, upon release, a total of over 70,000 test instances with associated human judgments with an extensible codebase.</p>
<p>Our results indicate that LLMs align well with human judgments on certain tasks, like instruction following.However, their performance is inconsistent across and within annotation tasks.Elicitation methods like Chain-of-Thought prompting (Wei et al., 2022) do not reliably improve agreement, in line with recent findings (Sprague et al., 2025).Some proprietary models-in particular, GPT-4o-align better to humans, but there is a rather small gap with large open-source models, holding promise for the reproducibility of future evaluation efforts.Altogether, at the current stage of LLM development, we recommend validating LLM judges against task-specific human annotations before deploying them for any particular task.</p>
<p>Construction of JUDGE-BENCH</p>
<p>One key feature that differs across the datasets included in JUDGE-BENCH is the source of the data being evaluated, i.e., whether the items to be judged are generated by a model or produced by humans, as illustrated in Figure 1.</p>
<p>For model-generated items, the goal is to evaluate an NLP system.This includes both classic tasks such as machine translation or dialogue response generation, as well as less standard tasks for which automation has recently become an option thanks to LLMs, such as the generation of plans or logical arguments.For human-generated items, the goal is to assess properties of interest such as grammaticality or toxicity.This distinction allows us to understand whether LLMs have a positive bias towards machine-generated outputs-a tendency reported in prior work (Xu et al., 2024).</p>
<p>The datasets we consider cover a wide span of properties of interest, ranging from grammaticality and toxicity to coherence, factual consistency, and verbosity, inter alia.Many properties are relevant across multiple tasks (e.g., fluency and coherence), while others are more task-specific (e.g., the success of a generated plan or the correctness of a multi-step mathematical reasoning trace).</p>
<p>Our study focuses on English datasets or language pairs which include English as one of the languages.We keep track of whether the original annotation guidelines are available and whether the annotations are provided by experts or nonexperts.We retain all available individual annotations.Dataset information is summarised in Table 2, Appendix A. All 20 datasets are formatted following a precise data schema to facilitate the integration of additional datasets.This makes JUDGE-BENCH easily extensible.We provide more details about the data schema in Appendix B.</p>
<p>Dataset (# properties judged)</p>
<p>GPT-4o</p>
<p>Llama-3.1-70B Mixtral-8x22B Gemini-1.(Groeneveld et al., 2024), Starling-7B (Zhu et al., 2024), and Mistral (Jiang et al., 2023a).See Appendix E for inference procedure details.</p>
<p>Prompts.Since most datasets include the original instructions used to gather human judgments, we use these instructions directly as prompts for the model, with additional guidelines to constrain the models' output and minimise verbosity: 'Answer with one of {}.Do not explain your answer.'When the original instruction for collecting human judgments is unavailable, we create a prompt based on relevant information from the original paper, such as the task description and the definitions of the evaluation metrics.We also experiment with alternative prompting strategies, including Chainof-Thought, few-shot prompts, and prompt paraphrases.However, none of these strategies leads to systematic improvements.See Appendix H for full details and results.All prompts are provided in the codebase.</p>
<p>Evaluation.Models do not always respond to the prompts as requested (e.g., they may refuse to
O L M o -7 B S t a r l i n g -7 B L l a m a -3 . 1 -8 B C o m m -R 4 C o m m -R + M i s t r a l -7 B M i x t r a l -8 x 7 B L l a m a -3 . 1 -7 0 B G e m i n i -1 . 5 M i x t</p>
<p>Spearman Correlation</p>
<p>Comm answer if they perceive the prompt as sensitive).
-R+ Mixtral-8x22B Llama-3.1-70B Mixtral-8x7B Gemini-1.5 Llama-3.1-8B GPT-4o
We therefore use the following evaluation protocol:</p>
<p>• To obtain the same number of judgments across models for a given dataset, we replace invalid LLM responses with judgments randomly sampled from the relevant set of categorical or graded annotations.Figure 5 in Appendix F shows the rate of valid responses per model.• Graded annotations, such as in WMT 2020 (Freitag et al., 2021), assess language quality on a continuous scale (e.g., a score from 0 to 100, or Likert-scale ratings), capturing varying degrees of fluency, adequacy, or overall translation quality; whereas categorical annotations, like those in CoLa (Warstadt et al., 2019), involve binary judgments (e.g., grammatically acceptable or not).</p>
<p>For the former, we compute Spearman's correlation (ρ) between model and human judgments; for the latter, we compute Cohen's κ.</p>
<p>• When multiple individual human judgments are available (typically three, see Table 2 in Appendix A), we estimate an upper bound by computing the average Spearman's ρ or Cohen's κ between bootstrapped single-rater responses and the aggregated responses across raters.Appendix C provides details on the upper bounds.</p>
<p>Results</p>
<p>Scores vary substantially across models.For any given model, they vary both across datasets and properties being judged.Table 1 presents detailed results for the 6 models that exhibit the largest rate of valid responses (≥98%).GPT-4o ranks first across several evaluation scenarios, but the Llama-3.1-70B and Mixtral-8x22B open models are relatively close and outperform GPT-4o on some assessment types, such as categorical sentence acceptability (CoLa) and graded summary quality (SummEval).Overall, the high degree of variability is not fully accounted for by the inherent difficulty of the annotation tasks, as reflected in the human upper bound.Moreover, except for a few datasets (e.g., QAGS, Recipe-generation, and NewsRoom), model scores remain notably below the upper bound.Among the property types with the lowest human-model alignment are toxicity and safety (in particular on DICES and Medical-safety), where model scores can be even negative and valid response rates particularly low (see Fig. 6 in Appendix F).This is due in part to the guardrails associated with these tasks (Weidinger et al., 2023).We find that, especially in the medical domain, many models tend to provide explanations instead of producing a judgment (see Appendix G).</p>
<p>Despite the high variability across models and datasets, we observe several notable trends.For graded annotations (Fig. 2), all models achieve higher correlations with annotations by non-expert human judges compared to expert annotators, echoing recent findings by Aguda et al. (2024).One possible explanation is that non-experts might rely on surface-level features, which could align more closely with the patterns LLMs are most attuned to, while experts apply stricter, domain-specific criteria.This remains speculative and calls for further investigation.</p>
<p>Figure 3 shows correlation results across different datasets for the subset of properties that exclusively have graded judgments.When applicable, we average results across datasets including annotations for the same property.We provide more details about these properties in Appendix D. The proprietary models GPT-4o and Gemini-1.5 exhibit the highest scores when evaluating acceptability and verbosity, while the two Mixtral open models show the strongest correlations for coherence and consistency.Correlation with the engagingness property remains consistently low across all models.Overall, no single model demonstrates a clear superiority over others across all properties; instead, different quality dimensions are better assessed by different models.This calls into question the widespread practice of using a single modeltypically a proprietary one like those from the GPT family-to evaluate a diverse range of linguistic properties.
O L M o -7 B S t a r li n g -7 B C o m m -R 4 M is t r a l-7 B C o m m -R + M ix t
Finally, as shown in Figure 4, all models achieve better alignment with human judgments when evaluating human language than when assessing machine-generated text, both for categorical and graded annotations.This result aligns with the findings by Xu et al. (2024), suggesting that LLMs display a bias towards their own generation.More broadly, this trend calls for caution when using LLMs to automatically evaluate the output of NLP systems.</p>
<p>Conclusions</p>
<p>In response to current trends in evaluation, in this paper we conducted a large-scale study of the correlation between human and LLM judgments across 20 datasets, considering factors such as the properties being assessed, the expertise level of the human judges, and whether the data is model-or human-generated.On some tasks, such as instruc-tion following and the generation of mathematical reasoning traces, models can be reliably used as evaluators.Overall, however, models' agreement with human judgments varies widely across datasets, evaluated properties, and data sources; and depends on the level of expertise of human judges.Furthermore, elicitation strategies such as Chain-of-Thought prompting do not consistently improve agreement levels, in line with recent findings (Sprague et al., 2025).We recommend validation and calibration of LLMs against task-specific human judgments prior to their deployment as evaluators.To facilitate this process, we release JUDGE-BENCH, a benchmark that enables systematic evaluation across a diverse range of tasks and is easily extensible to include any new task of interest.</p>
<p>Limitations</p>
<p>One limitation of the experimental design of our work is that correlation with human judges may not be the most appropriate way to validate LLM evaluators.Indeed, there may be domains where human annotators and LLM evaluators appear aligned simply because they are affected by similar biases.Therefore, depending on the task at hand, it may be necessary to validate the reliability of human annotators as well.</p>
<p>Another limitation concerns the use of existing tasks and datasets without reassessing their quality or representativeness of actual downstream tasks.While we did our best to select a wide set of tasks meaningful to the NLP community, we acknowledge that these tasks could not be equally meaningful for end-users, and that employing existing datasets could arguably lead to potential risks and shortcomings, such as data leakage.</p>
<p>In contrast to approaches that use LLMs for pairwise preference evaluation, e.g., PairEval (Park et al., 2024) or JudgeBench (Tan et al., 2025),1 this paper focuses on evaluating the performance of LLMs on generating judgements for categorical and graded responses.We leave the extension of JUDGE-BENCH to include pairwise preference evaluation and other recent evaluation methods, such as Prometheus 2 (Kim et al., 2024), for future work.</p>
<p>Finally, our work mostly focuses on Englishlanguage datasets-with the exception of datasets focussing specifically on machine-translation outputs.It remains to be seen whether LLMs' metaevaluation abilities vary across different languages.</p>
<p>Appendix A Datasets</p>
<p>This section provides brief descriptions of the datasets employed in our study.ToxicChat (Lin et al., 2023).collect binary judgments on the toxicity and 'jailbreaking' nature (prompt hacks deliberately intended to bypass safety policies and induce models to generate unsafe content) of human prompts to LLMs.While the original dataset contains a mix of human-and automatically-annotated instances, here we only consider the human-annotated prompts.</p>
<p>LLMBar (Zeng et al., 2024).LLMBar is a dataset targeted at evaluating the instructionfollowing abilities of LLMs.Each entry of this dataset consists of an instruction paired with two different outputs, one correctly following the instruction and the other deviating from it.LLMBar has an adversarial split where deviating outputs are carefully constructed to 'fool' LLM-based evaluators and a natural split where deviating outputs are more naturalistic.(Aroyo et al., 2023) Toxicity &amp; Safety 1,340 ~70 + ~120 Categorical ✗ Mixed ToxicChat (Lin et al., 2023) Toxicity &amp; Safety 5,654 -Categorical ✗ ✓ Topical Chat (Mehri and Eskenazi, 2020) Dialogue 60 3 Graded + Categorical ✗ ✓ Persona Chat (Mehri and Eskenazi, 2020) Dialogue 60 3 Graded + Categorical ✗ ✓ WMT 2020 En-De (Freitag et al., 2021) Machine Translation 14,122 3 Graded ✗ ✓ WMT 2020 Zh-En (Freitag et al., 2021) Machine Translation 19,974 3 Graded ✗ ✓ WMT 2023 En-De (Kocmi et al., 2023) Machine Translation 6,588 -Graded ✗ ✓ WMT 2023 Zh-En (Kocmi et al., 2023) Machine Translation 13,245 -Graded ✗ ✓ G-Eval / SummEval (Liu et al., 2023 Topical Chat and Persona Chat (Mehri and Eskenazi, 2020).These datasets contain human judgments on the quality of machine-and human-generated responses based on the provided dialogue context.The annotated dialogues were selected from Topical Chat (Gopalakrishnan et al., 2019)-a dataset collecting humanhuman conversations on provided facts-and Persona Chat (Zhang et al., 2018), which contains human-human persona-conditioned conversations.Each response is evaluated on 6 attributes: Understandable, Natural, Maintains Context, Interesting/Engaging, Uses Knowledge, and Overall Quality.</p>
<p>ROSCOE (Golovneva et al., 2023).collect human judgments assessing the quality of GPT-3's reasonings.</p>
<p>The output reasonings are elicited by inputting GPT-3 with questions selected from 4 commonly used reasoning datasets, i.e., CosmosQA (Huang et al., 2019), DROP (Dua et al., 2019), e-SNLI (Camburu et al., 2018) and GSM8K (Cobbe et al., 2021).While ROSCOE provides annotations on each step of the reasoning trace, here we only consider the global judgments over the whole reasoning.</p>
<p>QAGS (Wang et al., 2020).QAGS consists of annotations judging the factual consistency of onesentence model-generated summaries of news articles.The gold-standard summaries and articles are collected from CNN/DailyMail (Hermann et al., 2015) and XSUM (Narayan et al., 2018).</p>
<p>Medical-safety (Abercrombie and Rieser, 2022).This dataset consists of 3701 pairs of medical queries (collected from a subreddit on medical advice) and both machine-generated and humangenerated answers.Queries were classified by human annotators according to their severity (from 'Not medical' to 'Serious', with 'Serious' indicating that emergency care would be required) and answers were categorised based on their risk level (from 'Non-medical' to 'Diagnosis/Treatment').</p>
<p>DICES (Aroyo et al., 2023).The DICES datasets consist of a series of machine-generated responses whose safety is judged based on the previous conversation turns (context).While the original dataset provides fine-grained annotations with answers to questions targeting specific aspects of safety, here we only consider the 'overall' categorisation comprehensive of all aspects.In DICES 990 safety is judged by crowdsourced annotators, whereas in DICES 350 both expert and crowdsourced annotations are provided.</p>
<p>Inferential strategies (Mondorf and Plank, 2024).This dataset contains annotations on the logical validity of reasoning steps that modelsin this case, Llama-2-chat-hf3 (Touvron et al., 2023), Mistral-7B-Instruct-v0.2 (Jiang et al., 2023a) and Zephyr-7b-beta (Tunstall et al., 2023)-generate when prompted to solve problems of propositional logic.Binary labels are assigned to each response, indicating whether the rationale provided by the model is sound (True) or not (False).Each model is assessed on 12 problems of propositional logic across 5 random seeds, resulting in a total of 60 responses per model.Dailydialog (Wallbridge et al., 2022).Switchboard includes acceptability judgments collected using stimuli from the Switchboard Telephone Corpus (Godfrey et al., 1992).More specifically, the judgments refer to how plausible it is that a specific response belongs to a telephonic dialogue.The same kind of judgments are provided for Dailydialog, which collects written dialogues intended to mimic conversations that could happen in real life.</p>
<p>Switchboard and</p>
<p>Recipe-generation (Stein et al., 2023).This dataset contains human annotations assessing the quality of machine-generated recipes based on 6 attributes: grammar, fluency, verbosity, structure, success, overall.</p>
<p>NewsRoom (Grusky et al., 2018).This dataset includes human judgments the quality of systemgenerated summaries of news articles.More specifically, annotators evaluated summaries across two semantic dimensions (informativeness and relevancy) and two syntactic dimensions (fluency and coherence).</p>
<p>SummEval and G-Eval (Fabbri et al., 2021;Liu et al., 2023).These datasets include summaries generated by multiple recent summarisation models trained on the CNN/DailyMail dataset (Hermann et al., 2015).Summaries are annotated by both expert judges and crowdsourced workers on 4 dimensions: coherence, consistency, fluency, relevance.</p>
<p>WMT 2020 En-De and Zh-En (Freitag et al., 2021).These datasets are a re-annotated version of the English-to-German and Chinese-to-English test sets taken from the WMT 2020 news translation task.The annotation was carried out by raters who are professional translators and native speakers of the target language using a Scalar Quality Metric (SQM) evaluation on a 0-6 rating scale.</p>
<p>WMT 2023 En-De and Zh-En (Kocmi et al., 2023).These datasets are the English-to-German and Chinese-to-English test sets taken from the General Machine Translation Task organised as part of the 2023 Conference on Machine Translation (WMT).In contrast to previous editions, the evaluation of translation quality was conducted by a professional or semi-professional annotator pool rather than utilising annotations from MTurk.Annotators were asked to provide a score between 0 and 100 on a sliding scale.</p>
<p>B The JUDGE-BENCH Data Schema</p>
<p>To facilitate extending our benchmark, we adopt a shared schema used to pre-process all datasets.Our publicly available code base includes an example2 of this format as well as instructions on how to verify that newly added datasets comply with it.</p>
<p>The Json-based JUDGE-BENCH data schema ensures that the following fields are included for each dataset:</p>
<p>• dataset: the name of the dataset;</p>
<p>• dataset_url: the URL where the original dataset can be downloaded, as opensourced by their creators; • annotations: an overview of the properties annotated for each dataset, along with information on how they are measured and prompt-like instructions similar to those originally given to the human annotators (when applicable); • instances: dataset instances including the piece of text to be judged, aggregated human judgments and, when available, individual human annotations.We note that, while we do not systematically explore inter-annotator variations at the instance level, the data schema we adopt allows for conducting this type of analysis in future work.</p>
<p>C Upper Bound Estimation for Model Correlations</p>
<p>Whenever multiple human annotations were publicly available for a property (see Table 3 for inter-annotator agreement scores), we computed upper-bound estimates for the correlations achievable by models.The intuition behind these estimates, borrowed from neuroscience (Nili et al., 2014), is that the maximum correlation a model can achieve with aggregated human responses is bounded by the average correlation between singleparticipant responses and the aggregated responses across participants.We applied a similar logic to the human judgments used in the present study and combined it with a bootstrapping approach.For each annotated property, we bootstrapped singleparticipant responses by sampling 1000 times from the available human responses, excluding data points where a single annotation was available.Next, we computed the alignment between each of the bootstrapped-participant arrays and the array of aggregated responses.Alignment was computed as Spearman's correlation for graded judgments and Cohen's kappa for categorical judgments.Finally, we estimated the upper bound as the average of the 1000 alignment measures.In cases where alignment between bootstrapped and aggregated responses could not be computed-because the variance of the bootstrapped responses was nullvalues were replaced with an average of the 'nonnan' correlations.We emphasise that these upper bounds are estimates and, as such, are subject to errors.Therefore, it may happen that model performance exceeds these upper bounds.</p>
<p>D Properties with Graded Judgments</p>
<p>In Figure 3, we display results for a set of graded properties annotated in one or more of the datasets we consider.The properties are defined as follows:</p>
<p>• Acceptability refers to whether it is plausible or not that a response belongs to a telephonic dialogue and was annotated in Swithboard and Dailydialog;</p>
<p>• Coherence was annotated for summaries and model-generated reasonings as part of the datasets NewsRoom, ROSCOE, and SummEval;</p>
<p>• Consistency refers to the alignment between facts described in a summary and in its source text, and was annotated in SummEval;</p>
<p>• Engaging indicates whether a response generated in the context of a dialogue is dull or interesting and was annotated in TopicalChat and PersonaChat;</p>
<p>• Fluency measures whether a piece of text is grammatically correct and well-formatted, and was annotated in NewsRoom, SummEval and Recipegeneration;</p>
<p>• Informativeness refers to the extent to which a summary captures the key points of the full text, and was annotated for summaries as part of the NewsRoom dataset;</p>
<p>• Relevance refers to whether a summary selects important information as opposed to including redundancies, and was annotated for NewsRoom and SummEval;</p>
<p>• Verbosity indicates whether a generated recipe is concise and avoids unnecessary repetitions, and was annotated in Recipe-generation.</p>
<p>E Inference Details</p>
<p>All open-model checkpoints were obtained using the HuggingFace pipeline and we access all proprietary models using their corresponding API libraries.The proprietary models were accessed from 06-06-2024 to 13-06-2024, for standard prompting and from 09-10-2024 to 13-12-2024, for CoT prompting.We obtain the model responses using greedy decoding, which we operationalise for the proprietary models by setting the temperature parameter to 0. We allow open models to generate a maximum of 25 new tokens and proprietary models to generate a maximum of 5 new tokens.For CoT prompting, we allow for a maximum of 1000 new tokens.We leverage Nvidia A100 (80 GB) GPUs for a total of 321 compute hours.The cost of running experiments using Gemini-1.5-flashwas C30.31, while the cost of experiments using GPT-4o was approximately $565.  5 and 6.</p>
<p>F Valid Response Rates</p>
<p>Starling-7B</p>
<p>OLMo</p>
<p>G More Details on Toxicity and Safety Evaluation</p>
<p>For the Medical-safety dataset, models often refused to answer.Instead they tended to generate explanations, copy what they had in the prompt, or tried to be generally helpful because they saw that it was a medical issue.Since we take a random answer when no answer could be detected, this contributes to lower the results obtained on this task.Scores for the DICES dataset were also low, even though the valid response rate was high, because in this case there is the 'Unsure' option, which (along with 'Unsafe') models preferred over calling anything 'Safe'.For ToxicChat, models performed reasonably well.</p>
<p>H Additional Results</p>
<p>In Table 6 we report human-model alignment scores per dataset for all models tested, thus complementing Table 1 in the paper.</p>
<p>Chain-of-Thought Prompts.For the results with CoT prompting, we use the same original instructions used to gather human judgments as prompts for the model but adapt the additional guidelines to emphasise multi-step reasoning rather than constrain the models' output.Specifically, we append the original instructions with the following additional guideline: 'Always end your answer with either {} regarding the entire context.Let's think step by step.', in which {} is replaced with an enumeration of all possible answer labels formatted as 'Therefore, {label A} is correct, or therefore, {label B} is correct, or therefore [...].'.This also allows for automatically extracting the final answers from model responses during evaluation.In this study, we evaluate nine models and exclude Mixtral-8x22B and Comm-R+ due to computational constraints.For the CoLa-grammar dataset, we obtain GPT-4o responses only for ten percent of its instances (that are randomly sampled) to address the slow processing times and rate limitations.While CoT prompting leads to improved agreement scores and correlations when used with some models for certain datasets (see Table 7), its overall effectiveness compared to the results obtained using standard prompts without CoT (see Table 6) is inconsistent.</p>
<p>Prompt Paraphrases.We experiment with paraphrased prompts for three datasets that models struggle with: DICES-350-expert, WMT 2023 En-De, and WMT 2023 Zh-En.The paraphrase for dices-350-expert elaborates on the concept of safety, compared to its short original prompt, whereas the paraphrases for the WMT datasets are more concise regarding what comprises a good translation compared to the original.We do not observe consistent improvements when using paraphrased prompts compared to the original prompts (Table 4).</p>
<p>Few-shot Prompts.For the three datasets above-DICES-350-expert, WMT 2023 En-De, and WMT 2023 Zh-En-we also experiment with few-shot prompts (Table 4), where we provide the model with 6 examples for DICES-350-expert, 3 of safe conversations and 3 of unsafe conversations, and 4 examples for each WMT 2023 dataset, 2 of high-scoring translations and 2 of low-scoring translations.Using few-shot prompts does not improve correlations for dices-350-expert.On the WMT 2023 datasets, we observe higher correlations for Llama 3.1 8B but very moderate or no improvements on the other two models.Given that these improvements are inconsistent across datasets, we did not scale up the experiments to all 20 datasets and 11 models.</p>
<p>Your task is to evaluate the quality of machine translation output on a scale from 0 to 100 [...].Evaluation Criteria: [...] Source: Great backpack but overkill on the straps Reference: Toller Rucksack, aber bei den Riemen übertrieben Translation: Toller Rucksack, aber übertrieben auf den Riemen</p>
<p>Figure 1 :
1
Figure 1: Evaluation by expert and non-expert human annotators and by LLMs for two tasks involving humangenerated (left) and machine-generated text (right).</p>
<p>Figure 2 :
2
Figure 2: Average model correlation with human experts vs. non-experts in datasets with graded annotations.</p>
<p>Figure 3 :
3
Figure 3: Correlation for properties with graded judgments.Averages and error bars when the property is present in more than one dataset.</p>
<p>Figure 4 :
4
Figure4: Scores (Cohen's κ for categorical annotations and Spearman's correlation for graded annotations) on test items involving human language vs. machine-generated outputs.</p>
<p>Figure 5 :
5
Figure 5: Valid response rate per model.</p>
<p>Figure 6 :
6
Figure 6: Average ratios of valid responses across datasets over the 11 models we tested.</p>
<p>Philipp Mondorf and Barbara Plank.2024.Comparing inferential strategies of humans and large language models in deductive reasoning.In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9370-9402, Bangkok, Thailand.Association for Computational Linguistics.Lianghui Zhu, Xinggang Wang, and Xinlong Wang.2025.JudgeLM: Fine-tuned Large Language Models are Scalable Judges.In Proceedings of the 13th International Conference on Learning Representations (ICLR).arXiv.ArXiv:2310.17631 [cs].
Alex Warstadt and Samuel R. Bowman. 2020. Linguis-Rickard Stureborg, Dimitris Alikaniotis, and Yoshitic analysis of pretrained sentence encoders with ac-Suhara. 2024.Large language models areceptability judgments. Preprint, arXiv:1901.03438.inconsistent and biased evaluators.Preprint,arXiv:2405.01724.Ben Naismith, Phoebe Mulcaire, and Jill Burstein. 2023. Alex Warstadt, Amanpreet Singh, and Samuel R. Bow-man. 2019. Neural Network Acceptability Judg-ments. Transactions of the Association for Com-putational Linguistics, 7:625-641.Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Y. Tang, Alejandro Cuadron, Chenguang Wang, Raluca Ada Popa, and Ion Stoica. 2025. JudgeBench: A benchmark for evaluating LLM-Automated evaluation of written discourse coherence Jason Wei, Xuezhi Wang, Dale Schuurmans, Maartenbased judges. In The Thirteenth International Con-using GPT-4. In Proceedings of the 18th Workshop Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,ference on Learning Representations.on Innovative Use of NLP for Building Educational Applications (BEA 2023), pages 394-403, Toronto, Canada. Association for Computational Linguistics. Shashi Narayan, Shay B. Cohen, and Mirella Lapata. and Denny Zhou. 2022. Chain-of-Thought prompt-ing elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 24824-24837. Curran Associates, Inc. 2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for ex-treme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-guage Processing, pages 1797-1807, Brussels, Bel-gium. Association for Computational Linguistics. Laura Weidinger, Maribeth Rauh, Nahema Marchal, Ar-ianna Manzini, Lisa Anne Hendricks, Juan Mateos-Garcia, Stevie Bergman, Jackie Kay, Conor Grif-fin, Ben Bariach, et al. 2023. Sociotechnical safety evaluation of generative ai systems. arXiv preprint arXiv:2310.11986. Hamed Nili, Cai Wingfield, Alexander Walther, Li Su, William Marslen-Wilson, and Nikolaus Kriegeskorte. Minghao Wu and Alham Fikri Aji. 2025. Style over sub-2014. A toolbox for representational similarity anal-stance: Evaluation biases for large language models. ysis. PLoS computational biology, 10(4):e1003553. In Proceedings of the 31st International ConferencePetter Törnberg. 2023. ChatGPT-4 outperforms experts and crowd workers in annotating political twitter messages with zero-shot learning. arXiv preprint arXiv:2304.06588. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open founda-tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar San-on Computational Linguistics, pages 297-312, Abu OpenAI. 2024. Gpt-4o model card. Dhabi, UAE. Association for Computational Linguis-seviero, Alexander M. Rush, and Thomas Wolf. 2023. Zephyr: Direct distillation of lm alignment. Preprint,tics.arXiv:2310.16944.Wenda Xu, Guanglei Zhu, Xuandong Zhao, LiangmingPat Verga, Sebastian Hofstatter, Sophia Althammer, Yix-Pan, Lei Li, and William Wang. 2024. Pride and Prej-uan Su, Aleksandra Piktus, Arkady Arkhangorodsky,udice: LLM Amplifies Self-Bias in Self-Refinement. Maja Pavlovic and Massimo Poesio. 2024. The ef-In Proceedings of the 62nd Annual Meeting of the fectiveness of LLMs as annotators: A comparative Association for Computational Linguistics (Volume 1: overview and empirical analysis of direct represen-Long Papers), pages 15474-15492, Bangkok, Thai-tation. In Proceedings of the 3rd Workshop on Per-spectivist Approaches to NLP (NLPerspectives) @ land. Association for Computational Linguistics.Minjie Xu, Naomi White, and Patrick Lewis. 2024. Replacing Judges with Juries: Evaluating LLM Gen-erations with a Panel of Diverse Models. arXiv preprint arXiv:2404.18796. Sarenne Carrol Wallbridge, Catherine Lai, and PeterLREC-COLING 2024, pages 100-110, Torino, Italia. ELRA and ICCL. Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. 2024. Evaluating large lan-guage models at evaluating instruction following. In Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste The Twelfth International Conference on Learning Representations. Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Fi-rat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Un-locking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Per-sonalizing dialogue agents: I have a dog, do you have pets too? In Proceedings of the 56th Annual Ehud Reiter. 2024. Can LLM-based eval replace human Meeting of the Association for Computational Lin-evaluation? Blog post. guistics (Volume 1: Long Papers), pages 2204-2213,Bell. 2022. Investigating perception of spoken dia-logue acceptability through surprisal. In Proc. Inter-speech 2022, pages 4506-4510. Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the fac-tual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Compu-tational Linguistics, pages 5008-5020, Online. Asso-ciation for Computational Linguistics. Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu,Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Melbourne, Australia. Association for Computationaland Jie Zhou. 2023. Is ChatGPT a good NLG evalua-Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Linguistics.tor? a preliminary study. In Proceedings of the 4thXinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Dur-rett. 2025. To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning. In Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. The Thirteenth International Conference on Learning Judging LLM-as-a-judge with MT-Bench and Chat-Representations. bot Arena. Advances in Neural Information Process-New Frontiers in Summarization Workshop, pages 1-11, Singapore. Association for Computational Lin-guistics. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong,ing Systems, 36.Qi Liu, Tianyu Liu, and Zhifang Sui. 2024. Large lan-guage models are not fair evaluators. In Proceedingsof the 62nd Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 9440-9450, Bangkok, Thailand. Associationfor Computational Linguistics.
ChaeHun Park, Minseok Choi, DohyunLee, and Jaegul  Choo.2024.PairEval: Open-domain dialogue evaluation with pairwise comparison.In First Conference on Language Modeling.Katharina Stein, Lucia Donatelli, and Alexander Koller.2023.From sentence to action: Splitting AMR graphs for recipe instructions.In Proceedings of the Fourth International Workshop on Designing Meaning Representations, pages 52-67, Nancy, France.Association for Computational Linguistics.Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, and Jiantao Jiao.2024.Starling-7B: Improving LLM helpfulness &amp; harmlessness with RLAIF.In First Conference on Language Modeling.</p>
<p>Table 2 summarises relevant dataset information.Note that dataset sizes as reported in Table 2 refer to the number of annotated samples (not to the total number of collected annotations) and might therefore differ from the figures reported in the original papers.Table 3 reports Krippendorf's α for those datasets with multiple public human annotations.
CoLa (Warstadt et al., 2019). The Corpus ofLinguistic Acceptability (CoLA) consists of 10657sentences from 23 linguistics publications, expertlyannotated for acceptability (grammaticality) bytheir original authors.CoLa-grammar (Warstadt and Bowman, 2020).The dataset consists of a grammatically annotatedversion of the CoLA development set. Each sen-tence in the CoLA development set is labelled withboolean features indicating the presence or absenceof a particular grammatical construction (usuallysyntactic in nature). Two related sets of featuresare considered: 63 minor features correspond tofine-grained phenomena, and 15 major features cor-respond to broad classes of phenomena.</p>
<p>Table 2 :
2
Balloccu et al. (2024)eatures of the datasets considered in the study.Note that 'Size' refers to the number of annotated samples, not to the total number of human annotations.'#Annot.'refers to the number of available individual annotations, if any, which we use to estimate the human upper bound.Note that datasets with only a single annotation per sample, or which only report the average over multiple annotations are not included in '# Annot.'.Information on possible data leakage was retrieved fromBalloccu et al. (2024).
)Summarisation1,600-Graded✓✓QAGS (Wang et al., 2020)Summarisation9533 Categorical✓✗NewsRoom (Grusky et al., 2018)Summarisation4203 Graded✓✗✓LLMBar (Zeng et al., 2024)Instruction Following 419-Categorical✓✓✗</p>
<p>Table 3 :
3
Inter-rater agreement for datasets with multiple human annotations.Datasets in blue concern humangenerated language, while those in red concern modelgenerated text.
DatasetKrippendorf's αTopical Chat0.08CategoricalQAGS DICES-990 DICES-350-crowdsourced Persona Chat0.49 0.14 0.16 0.33Inferential strategies1.0Dailydialog0.59Switchboard0.57GradedPersona Chat Topical Chat Recipe-generation NewsRoom0.33 0.08 0.41 0.11WMT 2020 En-De0.5WMT 2020 Zh-En0.09</p>
<p>Table 5 reports the rate of valid responses for each model and dataset.Valid response rates are summarised per model and dataset in Figures</p>
<p>Table 4 :
4
Cohen's kappa for DICES-350-expert and Spearman's correlation for two WMT 2023 datasets, comparing the original prompt and CoT prompt to few-shot prompts and prompt paraphrases for a selection of models.For datasets with more than one paraphrased prompt, we report the average and standard deviation across paraphrases.For Spearman's correlations, we report the number of significant correlations (p &lt; 0.05) for each model and dataset in brackets.
1.0Valid Response ratio0.0 0.2 0.4 0.6 0.80.740.870.880.890.890.920.920.920.930.94 0.94 0.940.940.950.950.960.960.960.960.960.960.960.97 Acceptability 0.97 0.97 Task Reasoning Summarisation Dialogue Toxicity \ Safety Translation Instruction Following 0.98 Planningmedical-safetyinferential-strategiesdices-350-crowdsourcedrecipe-crowd-sourcing-datadices-350-expertdices-990summevalllmbar-naturaldailydialog-acceptabilitywmt-23-en-dewmt-23-zh-enwmt-human-zh-enllmbar-adversarialwmt-human-en-deqagsroscoe-droppersona-chatroscoe-cosmoscola-grammartopical-chatroscoe-esnlitoxic-chatswitchboard-acceptabilitynewsroomroscoe-gsm8kcola
Some time after an early version of this paper became available as a pre-print, accompanied by our Judge-Bench code, the independent work by Tan et al. (2025) appeared, which describes a benchmark that the authors named JudgeBench. This name clash is unfortunate, but since in the meantime our paper has seen some uptake, we have decided against trying to resolve it.
https://github.com/dmg-illc/JUDGE-BENCH/blob/ master/data/example.json
AcknowledgementsThis work emerged from discussions at a workshop organised by Raquel Fernández and Sandro Pezzelle at MFO, the Oberwolfach Research Institute for Mathematics in the German Black Forest, on behalf of the ELLIS NLP programme.The event was funded by the state of Baden-Württemberg (Germany) and organised in collaboration with the ELLIS Institute Tübingen and the Max Planck Institute for Intelligent Systems.We furthermore acknowledge our funders.In particular, AB, RF, and AT were supported by the European Research Council (ERC Consolidator Grant DREAM 819455 to RF), as well as BP (ERC Consolidator Grant DIALECT 101043235 to BP).DE was supported by a research grant (VIL53122) from VILLUM FONDEN.EG was supported by the Dutch Research Council (Gravitation grant 024.001.006 to the Language in Interaction consortium).MG was supported by an ETH Zurich Postdoctoral Fellowship.MH was supported in part by an OpenAI Superalignment Fellowship.AM was supported by the European Research Council (DECOLLAGE, ERC-2022-CoG 101088763) and by Fundação para a Ciência e Tecnologia through contract UIDB/50008/2020.We acknowledge ISCRA for awarding this project access to the LEONARDO supercomputer, owned by the EuroHPC Joint Undertaking, hosted by CINECA (Italy).-0.02 ±0.14 (0) -0.09 ±0.17 (1) 0.03 ±0.13 (0) -0.06 ±0.14 (0) Topical Chat (4) 0.26 ±0.03 (2) 0.28 ±0.1 (2) 0.13 ±0.04 (0) 0.17 ±0.12 (1) 0.21 ±0.18 (1) 0.14 ±0.05 (0) 0.07 ±0.07 (0) 0.15 ±0.13 (0) 0.29 ±0.11 (3) 0.14 ±0.16 (1) 0.08 ±0.21 (1) Recipe-generation (6) 0.78 ±0.05 (6) 0.66 ±0.07 (6) 0.6 ±0.15 (6) 0.67 ±0.09 (5) 0.57 ±0.24 (5) 0.32 ±0.28 (5) 0.06 ±0.26 (3) 0.34 ±0.09 (5) 0.28 ±0.08 (4) 0.04 ±0.17 (1) 0.1 ±0.08 (0) ROSCOE-GSM8K (2) 0.82 ±0.12 (2) 0.83 ±0.11 (2) 0.81 ±0.14 (2) 0.81 ±0.12 (2) 0.79 ±0.13 (2) 0.68 ±0.2 (2) 0.7 ±0.08 (2) 0.76 ±0.15 (2) 0.63 ±0.18 (2) 0.46 ±0.13 (2) 0.1 ±0.07 (1) ROSCOE-eSNLI (2) 0.49 ±0.24 (2) 0.4 ±0.16 (2) 0.38 ±0.17 (2) 0.35 ±0.21 (2) 0.32 ±0.12 (2) 0.09 ±0.08 (0) 0.28 ±0.21 (1) 0.19 ±0.16 (1) 0.32 ±0.12 (2) 0.11 ±0.06 (0) 0.11 ±0.17 (1) ROSCOE-DROP (2) 0.57 ±0.22 (2) 0.59 ±0.16 (2) 0.44 ±0.15 (2) 0.44 ±0.13 (2) 0.32 ±0.12 (2) 0.21 ±0.22 (1) 0.37 ±0.18 (2) 0.23 ±0.1 (2) 0.22 ±0.22 (1) 0.16 ±0.17 (1) 0.15 ±0.21 (1) ROSCOE-CosmosQA (2) 0.57 ±0.18 (2) 0.55 ±0.18 (2) 0.51 ±0.16 (2) 0.57 ±0.17 (2) 0.53 ±0.21 (2) 0.33 ±0.25 (2) 0.48 ±0.17 (2) 0.44 ±0.26 (2) 0.57 ±0.2 (2) 0.13 ±0.04 (1) 0.49 ±0.24 (2) NewsRoom (4) 0.59 ±0.02 (4) 0.59 ±0.03 (4) 0.44 ±0.05 (4) 0.55 ±0.03 (4) 0.5 ±0.07 (4) 0.36 ±0.06 (4) 0.16 ±0.05 (4) 0.45 ±0.04 (4) 0.26 ±0.06 (4) 0.21 ±0.08 (4) -0.01 ±0.04 (0) SummEval (4) 0.35 ±0.06 (4) 0.44 ±0.14 (4) 0.54 ±0.08 (4) 0.38 ±0.02 (4) 0.48 ±0.02 (4) 0.19 ±0.06 (4) 0.13 ±0.06 (4) 0.29 ±0.09 (4) 0.4 ±0.12 (4Table 6: Scores per dataset for all models we evaluate: Cohen's kappa for categorical annotations and Spearman's correlation for graded annotations.For Spearman's correlations, we report the number of significant correlations (p &lt; 0.05) for each model and dataset in brackets.Datasets in blue concern human-generated language while those in red concern model-generated text.0.11 ±0.06 (0) 0.06 ±0.15 (0) 0.17 ±0.21 (1) -0.04 ±0.22 (1) 0.04 ±0.13 (0) -0.01 ±0.22 (1) 0.06 ±0.18 (0) Topical Chat (4) 0.22 ±0.02 (0) 0.14 ±0.13 (1) 0.11 ±0.1 (0) 0.06 ±0.18 (1) 0.1 ±0.14 (0) 0.16 ±0.17 (1) 0.25 ±0.05 (2) 0.17 ±0.09 (1) 0.08 ±0.13 (0) Recipe-generation (6) 0.67 ±0.12 (6) 0.64 ±0.14 (6Table 7: Scores per dataset for all models we evaluate using CoT prompts: Cohen's kappa for categorical annotations and Spearman's correlation for graded annotations.For Spearman's correlation, we report the number of significant correlations for each model and dataset in brackets.Datasets in blue concern human-generated language while those in red concern model-generated text.
Riskgraded safety for handling medical queries in conversational AI. Gavin Abercrombie, Verena Rieser, 10.18653/v1/2022.aacl-short.30Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing. the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language ProcessingOnline only. Association for Computational Linguistics20222Short Papers)</p>
<p>Large language models as financial data annotators: A study on effectiveness and efficiency. D Toyin, Suchetha Aguda, Elena Siddagangappa, Simerjot Kochkina, Dongsheng Kaur, Charese Wang, Smiley, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCL2024</p>
<p>A I , Meta , Llama 3.1 model card. 2024</p>
<p>Dices dataset: Diversity in conversational ai evaluation for safety. Lora Aroyo, Alex Taylor, Mark Díaz, Christopher Homan, Alicia Parrish, Gregory Serapio-García, Vinodkumar Prabhakaran, Ding Wang, Advances in Neural Information Processing Systems. Curran Associates, Inc202336</p>
<p>Leak, cheat, repeat: Data contamination and evaluation malpractices in closedsource LLMs. Simone Balloccu, Patrícia Schmidtová, Mateusz Lango, Ondrej Dusek, Proceedings of the 18th Conference of the European Chapter. the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational Linguistics20241Long Papers)</p>
<p>Pitfalls of conversational LLMs on news debiasing. Ipek Baris Schlicht, Defne Altiok, Maryanne Taouk, Lucie Flek, Proceedings of the First Workshop on Language-driven Deliberation Technology (DELITE) @ LREC-COLING 2024. the First Workshop on Language-driven Deliberation Technology (DELITE) @ LREC-COLING 2024Torino, ItaliaELRA and ICCL2024</p>
<p>e-snli: Natural language inference with natural language explanations. Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, Phil Blunsom, Advances in Neural Information Processing Systems. 201831</p>
<p>Exploring the use of large language models for reference-free text quality evaluation: An empirical study. Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, Ruifeng Xu, 10.18653/v1/2023.findings-ijcnlp.32Findings of the Association for Computational Linguistics: IJCNLP-AACL 2023 (Findings). BaliAssociation for Computational Linguistics2023</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, 10.18653/v1/2023.acl-long.870Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Chatbot arena: an open platform for evaluating llms by human preference. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I Jordan, Joseph E Gonzalez, Ion Stoica, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine Learning2024ICML'24. JMLR.org</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, ArXiv, abs/2110.141682021</p>
<p>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, 10.18653/v1/N19-1246Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinnesotaAssociation for Computational Linguistics20191Minneapolis</p>
<p>Alpacafarm: A simulation framework for methods that learn from human feedback. Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, Tatsunori B Hashimoto, Advances in Neural Information Processing Systems. 202436</p>
<p>SummEval: Re-evaluating Summarization Evaluation. Alexander R Fabbri, Wojciech Kryściński, Bryan Mc-Cann, Caiming Xiong, Richard Socher, Dragomir Radev, 10.1162/tacl_a_00373Transactions of the Association for Computational Linguistics. 92021</p>
<p>The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, F T André, Graham Martins, Ankush Neubig, Jonathan H Garg, Markus Clark, Orhan Freitag, Firat, Proceedings of the Eighth Conference on Machine Translation. the Eighth Conference on Machine Translation2023</p>
<p>Experts, errors, and context: A large-scale study of human evaluation for machine translation. Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, Wolfgang Macherey, 10.1162/tacl_a_00437Transactions of the Association for Computational Linguistics. 20219</p>
<p>ChatGPT outperforms crowd workers for text-annotation tasks. Fabrizio Gilardi, Meysam Alizadeh, Maël Kubli, Proceedings of the National Academy of Sciences. 12030e23050161202023</p>
<p>Switchboard: Telephone speech corpus for research and development. John J Godfrey, Edward C Holliman, Jane Mc-Daniel, Acoustics, speech, and signal processing. IEEE Computer Society19921</p>
<p>ROSCOE: A suite of metrics for scoring step-by-step reasoning. Olga Golovneva, Moya Peng Chen, Spencer Poff, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz, The Eleventh International Conference on Learning Representations. Martin Corredor,. 2023</p>
<p>Topical-chat: Towards knowledge-grounded open-domain conversations. Karthik Gopalakrishnan, Behnam Hedayatnia, Qinlang Chen, Anna Gottardi, Sanjeev Kwatra, Anu Venkatesh, Raefer Gabriel, Dilek Hakkani-Tur, 10.21437/Interspeech.2019-3079Proc. Interspeech. Interspeech2019. 2019</p>
<p>Olmo: Accelerating the science of language models. Dirk Groeneveld, Iz Beltagy, Evan Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. Max Grusky, Mor Naaman, Yoav Artzi, 10.18653/v1/N18-1065Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesLong Papers; New Orleans, LouisianaAssociation for Computational Linguistics20181</p>
<p>Are large language model-based evaluators the solution to scaling up multilingual evaluation?. Rishav Hada, Varun Gumma, Adrian De Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, Sunayana Sitaram, Findings of the Association for Computational Linguistics: EACL 2024. St. Julian's, MaltaAssociation for Computational Linguistics2024</p>
<p>Teaching machines to read and comprehend. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom, Advances in neural information processing systems. 201528</p>
<p>ChatGPT rates natural language explanation quality like humans: But on which scales?. Fan Huang, Haewoon Kwak, Kunwoo Park, Jisun An, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCL2024</p>
<p>Cosmos QA: Machine reading comprehension with contextual commonsense reasoning. Lifu Huang, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, 10.18653/v1/D19-1243Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.06825Mistral 7b. 2023aarXiv preprint</p>
<p>Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.04088Mixtral of experts. 2024arXiv preprint</p>
<p>LLM-blender: Ensembling large language models with pairwise ranking and generative fusion. Dongfu Jiang, Xiang Ren, Bill Yuchen, Lin , 10.18653/v1/2023.acl-long.792Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023b1</p>
<p>Prometheus 2: An open source language model specialized in evaluating other language models. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, Minjoon Seo, 10.18653/v1/2024.emnlp-main.248Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Findings of the 2023 conference on machine translation (WMT23): LLMs are here but not quite there yet. Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Philipp Koehn, Benjamin Marie, Christof Monz, Makoto Morishita, Kenton Murray, Masaaki Nagata, Toshiaki Nakazawa, Martin Popel, Maja Popović, Mariya Shmatova, 10.18653/v1/2023.wmt-1.1Proceedings of the Eighth Conference on Machine Translation. the Eighth Conference on Machine TranslationSingaporeAssociation for Computational LinguisticsJun Suzuki. 2023</p>
<p>GEMBA-MQM: Detecting translation quality error spans with GPT-4. Tom Kocmi, Christian Federmann, 10.18653/v1/2023.wmt-1.64Proceedings of the Eighth Conference on Machine Translation. the Eighth Conference on Machine TranslationSingaporeAssociation for Computational Linguistics2023a</p>
<p>Large language models are state-of-the-art evaluators of translation quality. Tom Kocmi, Christian Federmann, Proceedings of the 24th Annual Conference of the European Association for Machine Translation. the 24th Annual Conference of the European Association for Machine TranslationTampere, FinlandEuropean Association for Machine Translation2023b</p>
<p>Benchmarking cognitive biases in large language models as evaluators. Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae , Myung Kim, Dongyeop Kang, 10.18653/v1/2024.findings-acl.29Findings of the Association for Computational Linguistics: ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024</p>
<p>Memories for virtual AI characters. Fabian Landwehr, Erika Varis Doggett, Romann M Weber, 10.18653/v1/2023.inlg-main.17Proceedings of the 16th International Natural Language Generation Conference. the 16th International Natural Language Generation ConferencePrague, Czechia2023Association for Computational Linguistics</p>
<p>Leveraging large language models for NLG evaluation: Advances and challenges. Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Yuxuan Lai, Chongyang Tao, Shuai Ma, 10.18653/v1/2024.emnlp-main.896Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USA2024Association for Computational Linguistics</p>
<p>ToxicChat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation. Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, Jingbo Shang, 10.18653/v1/2023.findings-emnlp.311Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>G-eval: NLG evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, 10.18653/v1/2023.emnlp-main.153Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Aligning with human judgement: The role of pairwise preference in large language model evaluators. Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulić, Anna Korhonen, Nigel Collier, First Conference on Language Modeling. 2024a</p>
<p>LLMs as narcissistic evaluators: When ego inflates evaluation scores. Yiqi Liu, Nafise Moosavi, Chenghua Lin, 10.18653/v1/2024.findings-acl.753Findings of the Association for Computational Linguistics: ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024b</p>
<p>LLM comparative assessment: Zero-shot NLG evaluation through pairwise comparisons using large language models. Adian Liusie, Potsawee Manakul, Mark Gales, Proceedings of the 18th Conference of the European Chapter. the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational Linguistics20241Long Papers)</p>
<p>USR: An unsupervised and reference free evaluation metric for dialog generation. Shikib Mehri, Maxine Eskenazi, 10.18653/v1/2020.acl-main.64Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>            </div>
        </div>

    </div>
</body>
</html>
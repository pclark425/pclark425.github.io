<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1951 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1951</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1951</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-280421383</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2508.01723v1.pdf" target="_blank">OpenMap: Instruction Grounding via Open-Vocabulary Visual-Language Mapping</a></p>
                <p><strong>Paper Abstract:</strong> Grounding natural language instructions to visual observations is fundamental for embodied agents operating in open-world environments. Recent advances in visual-language mapping have enabled generalizable semantic representations by leveraging vision-language models (VLMs). However, these methods often fall short in aligning free-form language commands with specific scene instances, due to limitations in both instance-level semantic consistency and instruction interpretation. We present OpenMap, a zero-shot open-vocabulary visual-language map designed for accurate instruction grounding in navigation tasks. To address semantic inconsistencies across views, we introduce a Structural-Semantic Consensus constraint that jointly considers global geometric structure and vision-language similarity to guide robust 3D instance-level aggregation. To improve instruction interpretation, we propose an LLM-assisted Instruction-to-Instance Grounding module that enables fine-grained instance selection by incorporating spatial context and expressive target descriptions. We evaluate OpenMap on ScanNet200 and Matterport3D, covering both semantic mapping and instruction-to-target retrieval tasks. Experimental results show that OpenMap outperforms state-of-the-art baselines in zero-shot settings, demonstrating the effectiveness of our method in bridging free-form language and 3D perception for embodied navigation.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1951.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1951.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenMap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenMap: Open-vocabulary Visual-Language Map</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot, open-vocabulary 2D-to-3D visual-language mapping and instruction-grounding framework that builds instance-level 3D semantic maps by jointly enforcing structural (geometric) and semantic (VLM feature) consensus, and uses an LLM-assisted two-stage instruction-to-instance grounding pipeline for fine-grained target retrieval in navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenMap</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline: (1) apply off-the-shelf 2D segmentation (CropFormer) to each RGB-D frame; (2) back-project each 2D mask into a 3D point cloud using depth; (3) extract open-vocabulary visual-language features for masks (OVSAM then CLIP ViT-H for final aggregation); (4) compute pairwise structural consensus (observer/supporter sets based on point-cloud containment/overlap) and semantic similarity (cosine between CLIP features) and merge masks iteratively when their product exceeds a threshold, with an observer-count schedule; (5) select completeness-best masks and aggregate multi-scale crops to form instance-level CLIP embeddings; (6) instruction grounding: GPT-4 parses free-form instructions into detailed instance descriptions, retrieves top-K candidate instances by embedding similarity, then performs a second-stage LLM reasoning/dialogue using instance locations and nearby objects (provided in prompt) to select final instance.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>CLIP ViT-H (final aggregation); OVSAM for per-image candidate-region features; CropFormer for 2D segmentation (not a feature encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>CLIP (as referenced) — ViT-H used for final feature aggregation; CLIP is pretrained on large web-scale image-text corpora (as in CLIP literature). OVSAM and CropFormer are pretrained segmentation/region models (references in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Region-/instance-level feature matching: cosine similarity between CLIP text embeddings (LLM-generated descriptions) and CLIP image/region embeddings; structural-semantic consensus couples geometric containment/observer overlap with semantic similarity during mask merging; instruction grounding is LLM-driven (textual descriptions) followed by embedding matching and LLM re-ranking with spatial context.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Instance-level 3D (aggregated from 2D masks) with multi-scale crop-level features; also uses per-image region-level features and point-cloud representations for structural reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Explicit: depth-based back-projected 3D point clouds per mask; observer/supporter relations computed via overlap/containment; instance centroids/locations provided to LLM; KD-tree spatial queries for local neighbors within a 2 m radius.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Vision-and-language navigation / instruction grounding (embodied navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Semantic mapping on ScanNet200; target retrieval (instruction grounding) on Matterport3D using instruction subsets from R2R-CE, VLMap, and ALFRED</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Photorealistic indoor scanned scenes (real-world scans: Matterport3D, ScanNet200); incremental/egocentric exploration frames</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Semantic mapping: Average Precision (AP) at IoU thresholds (AP, AP50, AP25) and class-agnostic AP; Target retrieval: Success Rate (SR) and SR_k (top-K success rates)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Mapping: AP 14.3%, AP50 26.0%, AP25 33.3% (zero-shot on ScanNet200). Target retrieval (Matterport3D): SR 49.6%, SR_8 68.3%, SR_16 73.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Mapping ablations: structural-only merging AP = 12.2% (vs full 14.3%); semantic-only merging AP = 10.1% (vs full 14.3%). Instruction grounding ablations: removing both unconstrained LLM parsing and OpenMap-assisted instance selection yields SR = 38.1% (vs full 49.6%); removing only unconstrained LLM parsing reduces SR by 4.9% (approx to ~44.7%); removing only instance-selection reduces SR by 2.4% (approx to ~47.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Joint structural+semantic merging improves mapping AP from 10.1%/12.2% to 14.3% (absolute +2.1 to +4.2 AP). Instruction-grounding strategies (unconstrained LLM parsing + OpenMap-assisted selection) improve SR by +11.5% absolute (49.6% vs 38.1% baseline without them); individually Ins.Parsing yields ~+4.9% and Ins.Selec ~+2.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Paper identifies instance-level semantic inconsistency and 2D->3D aggregation errors as core perception bottlenecks: incomplete point clouds, occlusion, over/under-segmentation, and coarse feature aggregation (e.g., average pooling over grids) can cause erroneous merges or fragmented instances; these lead to misalignment between free-form instructions and specific scene instances.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Reported failure/fractional analyses: (a) mapping-only failure modes: poor semantic aggregation or using only structural cues leads to AP drops (semantic-only AP 10.1%, structural-only AP 12.2% vs full 14.3%); (b) grounding failures: constraining LLM outputs to lexicons reduces SR — removing both parsing/selection drops SR from 49.6% to 38.1% (>10% absolute); (c) qualitative failure notes: ground truth sometimes merges items (causing 'false negatives' despite correct fine-grained segmentation), coarse aggregation (VLMap) limits retrieval, and label-constrained methods (ConceptGraphs) fail on free-form descriptions. The paper does not provide per-failure-type frequency breakdowns beyond these ablation deltas.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Zero-shot strategy: relies on open-vocabulary VLMs (CLIP) and LLMs (GPT-4) without fine-tuning; robustness to domain shift is sought via structural-semantic consensus during mapping rather than explicit domain adaptation layers. Compared to supervised Mask3D, OpenMap lacks priors from ScanNet200 and shows a gap in class-agnostic metrics, indicating remaining domain/coverage limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>OpenMap maintains consistent performance across head/common/tail categories with AP gap not exceeding 1.1% across frequency groups on ScanNet200, indicating relatively stable handling of long-tail (novel/rare) categories in the zero-shot setting.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Late/embedding-level fusion: textual descriptions from LLM are embedded and matched to CLIP image/instance embeddings via cosine similarity; no cross-attention fusion is used. The LLM performs semantic parsing and a second-stage context-aware multiple-choice reasoning step over candidate metadata (textual prompts include semantic similarity scores and nearby object info).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Zero-shot; no task-specific training samples required. Not quantified in terms of demo count.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Effective instruction grounding in embodied navigation requires (1) instance-level 3D mappings that combine geometric (structural) consensus and open-vocabulary semantic similarity to avoid erroneous merges and fragmentation; (2) unconstrained, LLM-driven instance description (not lexicon-restricted) combined with map-provided spatial context significantly improves retrieval; (3) joint structural+semantic aggregation and LLM-assisted two-stage grounding yield large absolute gains in AP and SR over prior zero-shot baselines; perception errors (occlusion, partial views, coarse aggregation) remain primary failure modes.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1951.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1951.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Structural-Semantic Consensus</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structural-Semantic Consensus Constraint</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A novel mask-merging criterion that multiplies a structural consensus rate (observer/supporter containment overlap across views) with semantic similarity (cosine between VLM features) and merges masks when the product exceeds a threshold, guiding robust 2D-to-3D instance aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Structural-Semantic Consensus (mapping component of OpenMap)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Computes structural consensus by back-projecting masks to 3D, forming observer/supporter sets (images that observe a mask and images whose masks contain both), then R_struc = |supporters|/|observers|; computes semantic similarity R_seman as cosine(f_i, f_j) between mask features; merges masks when R_struc * R_seman >= tau_thresh, iteratively with an observer count schedule and feature re-selection for combined instances.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Uses mask features from OVSAM for semantic-similarity computation and CLIP ViT-H for final aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Relies on pretrained CLIP and pretrained OVSAM (as per referenced models); no additional training</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Not a language-visual grounding mechanism per se, but enforces semantic consistency between VLM features and geometric overlap prior to forming 3D instances, thereby improving downstream grounding by producing more coherent instance embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Operates at 2D mask -> 3D instance level (instance-level aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Explicit: back-projected 3D point clouds per mask; containment/overlap thresholds for observer/supporter relationships; observer-count based iteration thresholds</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Used for 3D semantic mapping in embodied navigation scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>ScanNet200 semantic mapping (evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Indoor scanned scenes (ScanNet, Matterport3D)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mapping: AP, AP50, AP25; ablations show effect of component removal</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>When used alone: structural-only AP = 12.2%; semantic-only AP = 10.1%; combined (Structural-Semantic Consensus) AP = 14.3% (ScanNet200 zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Using structural-consensus alone vs combined: AP drop from 14.3% to 12.2%; using semantic similarity alone yields AP 10.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Combined constraint yields +2.1% absolute AP over structural-only and +4.2% over semantic-only in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Designed specifically to mitigate aggregation failures caused by incomplete point clouds, occlusions, and semantic ambiguities that cause erroneous merges across views.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Without semantic cues, spatially proximate but semantically distinct parts may merge (semantic-only issues); without structural cues, fragmented/noisy observations can cause over-segmentation or spurious merges. Exact failure frequencies are reported via AP deltas in ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Implicitly improves robustness to variable views/partial scans by demanding cross-view containment support before merging; does not perform explicit domain adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Enables more consistent instance-level aggregation across frequency groups — reported AP gap across head/common/tail <= 1.1% when used in OpenMap.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Multiplicative coupling of structural consensus rate and semantic similarity (R_struct * R_seman >= tau).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Combining geometric structural consensus with VLM-derived semantic similarity significantly reduces incorrect merges and fragmentation in 2D-to-3D aggregation, producing higher-quality instance embeddings that directly improve instruction grounding.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1951.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1951.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction-to-Instance Grounding (OpenMap)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-assisted Instruction-to-Instance Grounding Module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage, LLM-driven grounding module: stage 1 prompts an LLM (GPT-4) to convert free-form navigation instructions into precise instance descriptions; stage 2 retrieves top-K candidate instances via CLIP similarity and uses a second LLM dialogue with spatial/contextual metadata (locations and nearby objects) to select the final instance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Instruction-to-Instance Grounding (OpenMap)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Stage 1: LLM prompt contains task definition, instance description criteria, and output constraints; LLM outputs a concise instance description. Stage 2: compute CLIP embedding of LLM description, rank all instance embeddings in OpenMap, select top-K candidates; for each candidate, query KD-tree for nearby instances and assemble a prompt with candidate metadata (location, semantic similarity, surrounding objects/labels) for the LLM to perform a multiple-choice selection of the final target.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Uses CLIP ViT-H embeddings for matching textual descriptions to instance visual embeddings; uses OVSAM features for intra-image semantic similarity computation.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>CLIP pretrained on web-scale image-text data (referenced CLIP model); OVSAM/CropFormer pretrained segmentation models as referenced.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Textual description generation by LLM followed by embedding-level matching (cosine similarity) between the LLM-generated text embedding and CLIP instance embeddings; second-stage LLM reasoning over structured candidate metadata refines selection.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>3D instance-level representation (embeddings aggregated from multiple 2D crops and masks), plus local neighborhood instance metadata</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Uses instance 3D locations (coordinates), KD-tree neighbor search within 2 m, and lists of surrounding objects provided in the LLM prompt for contextual reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Instruction grounding for navigation (object-goal, demand-driven, language-guided)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Matterport3D target retrieval; instruction datasets: R2R-CE, VLMap, ALFRED subsets</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Photorealistic indoor scans (Matterport3D scenes), incremental exploration</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (SR), SR_topK (SR_4, SR_8, SR_16)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Full grounding module (both unconstrained parsing + OpenMap-assisted selection) SR = 49.6% (SR_8 = 68.3%, SR_16 = 73.7%) on Matterport3D retrieval benchmark used in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Removing unconstrained LLM parsing reduces SR by ~4.9% (one-component ablation); removing OpenMap-assisted instance selection reduces SR by ~2.4%; removing both yields SR = 38.1% (absolute drop of 11.5% vs full pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Two-stage LLM + OpenMap selection yields +11.5% absolute SR improvement over removing both strategies; individual components contribute ~+4.9% (parsing) and ~+2.4% (selection).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Ambiguity in instruction interpretation and multiple matching instances in a scene are major causes of grounding failures; relying solely on semantic similarity is insufficient when multiple similarly-matching instances exist — spatial context and LLM reasoning help disambiguate.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>If LLM outputs are constrained to label dictionaries (as in some prior work), grounding performance drops; pure embedding matching without spatial context leads to mistaken selections among multiple similar candidates — seen in SR ablations where removing instance-selection lowers SR by 2.4% and removing parsing lowers SR by 4.9%.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>LLM parsing is unconstrained and benefits from general knowledge; grounding remains zero-shot using CLIP embeddings—no explicit adaptation to new sensors/environments beyond map aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Module is designed to handle free-form descriptions and therefore supports novel object descriptions via LLM; empirical results show improved SR vs baselines that rely on pre-defined labels (e.g., ConceptGraphs), but no per-novel-object numeric split provided.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Late fusion: LLM textual outputs are compared to visual embeddings using cosine similarity (CLIP); LLM then ingests structured metadata and returns a selection (textual multiple-choice) — essentially two rounds of language-driven, embedding-based retrieval and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Zero-shot retrieval; no task-specific fine-tuning samples required. Candidate K swept (4–12) shows SR stable within ~1.9% variation; best at K=8.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Unconstrained LLM parsing of instructions into precise instance descriptions plus map-provided spatial/contextual metadata substantially improves grounding accuracy versus lexicon-constrained parsing or pure embedding-matching; combining embedding retrieval with LLM re-ranking resolves ambiguous cases where multiple similar instances exist.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1951.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1951.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP (used)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP (Contrastive Language–Image Pretraining) ViT-H features</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pretrained vision-language model used to produce open-vocabulary image/text embeddings for semantic similarity and instance-level feature aggregation in OpenMap.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP ViT-H (feature extractor)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Froze or used as feature extractor: CLIP encodes cropped multi-scale image patches and textual descriptions into a shared embedding space. Cosine similarity between CLIP image/region embeddings and text embeddings is the primary semantic similarity metric for mask merging and instruction matching.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>CLIP ViT-H</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>CLIP pretrained on large web-scale image-text pairs (as per CLIP literature)</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Embedding-level matching (cosine similarity) between CLIP image/region embeddings and LLM-produced text descriptions; used both for semantic similarity during mask merging and for instruction-to-instance retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Region-level / crop-level and aggregated instance-level embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Implicit (no internal 3D reasoning); spatial structure is handled outside CLIP (via back-projected point clouds and KD-tree).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Component within embodied navigation mapping and grounding pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Used within OpenMap mapping/retrieval tasks (ScanNet200, Matterport3D)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Indoor real-world scan images (frame crops)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Used as similarity metric; no standalone SR/AP reported in paper for CLIP alone</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Enables open-vocabulary semantic comparisons; ablations in paper show semantic similarity is necessary (semantic-only AP 10.1% vs combined 14.3%), demonstrating CLIP-based semantics crucial to performance.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>CLIP-based semantic embeddings alone can be insufficient when structural aggregation is poor (hence the need for structural-semantic consensus); coarse aggregation methods (e.g., average pooling grids) limit CLIP's effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Semantic-only aggregation leads to lower AP (10.1%); CLIP features can cluster parts of the same object but may also confuse spatially close distinct objects if aggregation is not structure-aware.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Used zero-shot; no additional fine-tuning reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>CLIP enables open-vocabulary matching to novel/unseen textual descriptions; paper reports relatively stable tail performance (AP gap <= 1.1%) when used within OpenMap.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Embedding similarity (cosine) as late fusion between visual and textual modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Large VLM embeddings (CLIP) provide useful open-vocabulary semantic signals for 2D->3D instance aggregation and instruction matching, but must be paired with geometric consensus and careful instance aggregation to avoid semantic/spatial misalignments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1951.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1951.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLMap (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VLMap (Visual-Language Map baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior visual-language mapping method that builds semantic grid maps and uses LLMs to translate instructions into object names; used as a baseline in OpenMap evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>VLMap</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VLMap (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Creates coarse semantic grid maps and translates instructions via LLMs into object-category queries for map lookup; uses average-pooling over 2D grids for feature aggregation, which the OpenMap paper argues is a coarse aggregation strategy that limits retrieval quality.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>LLM-to-label translation followed by lookup/matching against grid-aggregated visual features (coarse average pooling over 2D grids).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Top-down grid / coarse scene-level and category-level representations (not instance-level fine-grained mapping)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Grid-based top-down spatial maps</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Vision-and-language navigation (target retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Baseline retrieval tasks on Matterport3D and related VLN benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Photorealistic indoor scans</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (SR) metrics for target retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported baseline SR = 27.2% (SR_8 = 29.7%, SR_16 = 32.1%) in the paper's comparisons (exact table values).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Paper criticizes VLMap's coarse feature aggregation (average pooling over 2D grids) as a limiting factor for semantic map quality and retrieval capability.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Coarse aggregation leads to lower-quality instance discrimination and retrieval; reflected in lower SR compared to OpenMap.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>LLM produces labels/names which are matched to visual grid embeddings; effectively late fusion via label-embedding lookup.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Coarse grid-based semantic aggregation constrains downstream instruction grounding; finer instance-level aggregation and semantic-structural consensus yield better retrieval performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>OVIR-3D: Open-vocabulary 3d instance retrieval without training on 3d data <em>(Rating: 2)</em></li>
                <li>MaskClustering: View consensus based mask graph clustering for open-vocabulary 3d instance segmentation <em>(Rating: 2)</em></li>
                <li>VLMap <em>(Rating: 2)</em></li>
                <li>OpenMask3D: Open-vocabulary 3d instance segmentation <em>(Rating: 2)</em></li>
                <li>ConceptGraphs: Open-vocabulary 3d scene graphs for perception and planning <em>(Rating: 2)</em></li>
                <li>NLMap <em>(Rating: 1)</em></li>
                <li>OpenScene: 3d scene understanding with open vocabularies <em>(Rating: 1)</em></li>
                <li>Mask3D: Mask transformer for 3d semantic instance segmentation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1951",
    "paper_id": "paper-280421383",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "OpenMap",
            "name_full": "OpenMap: Open-vocabulary Visual-Language Map",
            "brief_description": "A zero-shot, open-vocabulary 2D-to-3D visual-language mapping and instruction-grounding framework that builds instance-level 3D semantic maps by jointly enforcing structural (geometric) and semantic (VLM feature) consensus, and uses an LLM-assisted two-stage instruction-to-instance grounding pipeline for fine-grained target retrieval in navigation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenMap",
            "model_description": "Pipeline: (1) apply off-the-shelf 2D segmentation (CropFormer) to each RGB-D frame; (2) back-project each 2D mask into a 3D point cloud using depth; (3) extract open-vocabulary visual-language features for masks (OVSAM then CLIP ViT-H for final aggregation); (4) compute pairwise structural consensus (observer/supporter sets based on point-cloud containment/overlap) and semantic similarity (cosine between CLIP features) and merge masks iteratively when their product exceeds a threshold, with an observer-count schedule; (5) select completeness-best masks and aggregate multi-scale crops to form instance-level CLIP embeddings; (6) instruction grounding: GPT-4 parses free-form instructions into detailed instance descriptions, retrieves top-K candidate instances by embedding similarity, then performs a second-stage LLM reasoning/dialogue using instance locations and nearby objects (provided in prompt) to select final instance.",
            "visual_encoder_type": "CLIP ViT-H (final aggregation); OVSAM for per-image candidate-region features; CropFormer for 2D segmentation (not a feature encoder)",
            "visual_encoder_pretraining": "CLIP (as referenced) — ViT-H used for final feature aggregation; CLIP is pretrained on large web-scale image-text corpora (as in CLIP literature). OVSAM and CropFormer are pretrained segmentation/region models (references in paper).",
            "grounding_mechanism": "Region-/instance-level feature matching: cosine similarity between CLIP text embeddings (LLM-generated descriptions) and CLIP image/region embeddings; structural-semantic consensus couples geometric containment/observer overlap with semantic similarity during mask merging; instruction grounding is LLM-driven (textual descriptions) followed by embedding matching and LLM re-ranking with spatial context.",
            "representation_level": "Instance-level 3D (aggregated from 2D masks) with multi-scale crop-level features; also uses per-image region-level features and point-cloud representations for structural reasoning.",
            "spatial_representation": "Explicit: depth-based back-projected 3D point clouds per mask; observer/supporter relations computed via overlap/containment; instance centroids/locations provided to LLM; KD-tree spatial queries for local neighbors within a 2 m radius.",
            "embodied_task_type": "Vision-and-language navigation / instruction grounding (embodied navigation)",
            "embodied_task_name": "Semantic mapping on ScanNet200; target retrieval (instruction grounding) on Matterport3D using instruction subsets from R2R-CE, VLMap, and ALFRED",
            "visual_domain": "Photorealistic indoor scanned scenes (real-world scans: Matterport3D, ScanNet200); incremental/egocentric exploration frames",
            "performance_metric": "Semantic mapping: Average Precision (AP) at IoU thresholds (AP, AP50, AP25) and class-agnostic AP; Target retrieval: Success Rate (SR) and SR_k (top-K success rates)",
            "performance_value": "Mapping: AP 14.3%, AP50 26.0%, AP25 33.3% (zero-shot on ScanNet200). Target retrieval (Matterport3D): SR 49.6%, SR_8 68.3%, SR_16 73.7%.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Mapping ablations: structural-only merging AP = 12.2% (vs full 14.3%); semantic-only merging AP = 10.1% (vs full 14.3%). Instruction grounding ablations: removing both unconstrained LLM parsing and OpenMap-assisted instance selection yields SR = 38.1% (vs full 49.6%); removing only unconstrained LLM parsing reduces SR by 4.9% (approx to ~44.7%); removing only instance-selection reduces SR by 2.4% (approx to ~47.2%).",
            "grounding_improvement": "Joint structural+semantic merging improves mapping AP from 10.1%/12.2% to 14.3% (absolute +2.1 to +4.2 AP). Instruction-grounding strategies (unconstrained LLM parsing + OpenMap-assisted selection) improve SR by +11.5% absolute (49.6% vs 38.1% baseline without them); individually Ins.Parsing yields ~+4.9% and Ins.Selec ~+2.4%.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Paper identifies instance-level semantic inconsistency and 2D-&gt;3D aggregation errors as core perception bottlenecks: incomplete point clouds, occlusion, over/under-segmentation, and coarse feature aggregation (e.g., average pooling over grids) can cause erroneous merges or fragmented instances; these lead to misalignment between free-form instructions and specific scene instances.",
            "failure_mode_analysis": "Reported failure/fractional analyses: (a) mapping-only failure modes: poor semantic aggregation or using only structural cues leads to AP drops (semantic-only AP 10.1%, structural-only AP 12.2% vs full 14.3%); (b) grounding failures: constraining LLM outputs to lexicons reduces SR — removing both parsing/selection drops SR from 49.6% to 38.1% (&gt;10% absolute); (c) qualitative failure notes: ground truth sometimes merges items (causing 'false negatives' despite correct fine-grained segmentation), coarse aggregation (VLMap) limits retrieval, and label-constrained methods (ConceptGraphs) fail on free-form descriptions. The paper does not provide per-failure-type frequency breakdowns beyond these ablation deltas.",
            "domain_shift_handling": "Zero-shot strategy: relies on open-vocabulary VLMs (CLIP) and LLMs (GPT-4) without fine-tuning; robustness to domain shift is sought via structural-semantic consensus during mapping rather than explicit domain adaptation layers. Compared to supervised Mask3D, OpenMap lacks priors from ScanNet200 and shows a gap in class-agnostic metrics, indicating remaining domain/coverage limitations.",
            "novel_object_performance": "OpenMap maintains consistent performance across head/common/tail categories with AP gap not exceeding 1.1% across frequency groups on ScanNet200, indicating relatively stable handling of long-tail (novel/rare) categories in the zero-shot setting.",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Late/embedding-level fusion: textual descriptions from LLM are embedded and matched to CLIP image/instance embeddings via cosine similarity; no cross-attention fusion is used. The LLM performs semantic parsing and a second-stage context-aware multiple-choice reasoning step over candidate metadata (textual prompts include semantic similarity scores and nearby object info).",
            "sample_efficiency": "Zero-shot; no task-specific training samples required. Not quantified in terms of demo count.",
            "key_findings_grounding": "Effective instruction grounding in embodied navigation requires (1) instance-level 3D mappings that combine geometric (structural) consensus and open-vocabulary semantic similarity to avoid erroneous merges and fragmentation; (2) unconstrained, LLM-driven instance description (not lexicon-restricted) combined with map-provided spatial context significantly improves retrieval; (3) joint structural+semantic aggregation and LLM-assisted two-stage grounding yield large absolute gains in AP and SR over prior zero-shot baselines; perception errors (occlusion, partial views, coarse aggregation) remain primary failure modes.",
            "uuid": "e1951.0"
        },
        {
            "name_short": "Structural-Semantic Consensus",
            "name_full": "Structural-Semantic Consensus Constraint",
            "brief_description": "A novel mask-merging criterion that multiplies a structural consensus rate (observer/supporter containment overlap across views) with semantic similarity (cosine between VLM features) and merges masks when the product exceeds a threshold, guiding robust 2D-to-3D instance aggregation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Structural-Semantic Consensus (mapping component of OpenMap)",
            "model_description": "Computes structural consensus by back-projecting masks to 3D, forming observer/supporter sets (images that observe a mask and images whose masks contain both), then R_struc = |supporters|/|observers|; computes semantic similarity R_seman as cosine(f_i, f_j) between mask features; merges masks when R_struc * R_seman &gt;= tau_thresh, iteratively with an observer count schedule and feature re-selection for combined instances.",
            "visual_encoder_type": "Uses mask features from OVSAM for semantic-similarity computation and CLIP ViT-H for final aggregation",
            "visual_encoder_pretraining": "Relies on pretrained CLIP and pretrained OVSAM (as per referenced models); no additional training",
            "grounding_mechanism": "Not a language-visual grounding mechanism per se, but enforces semantic consistency between VLM features and geometric overlap prior to forming 3D instances, thereby improving downstream grounding by producing more coherent instance embeddings.",
            "representation_level": "Operates at 2D mask -&gt; 3D instance level (instance-level aggregation)",
            "spatial_representation": "Explicit: back-projected 3D point clouds per mask; containment/overlap thresholds for observer/supporter relationships; observer-count based iteration thresholds",
            "embodied_task_type": "Used for 3D semantic mapping in embodied navigation scenarios",
            "embodied_task_name": "ScanNet200 semantic mapping (evaluation)",
            "visual_domain": "Indoor scanned scenes (ScanNet, Matterport3D)",
            "performance_metric": "Mapping: AP, AP50, AP25; ablations show effect of component removal",
            "performance_value": "When used alone: structural-only AP = 12.2%; semantic-only AP = 10.1%; combined (Structural-Semantic Consensus) AP = 14.3% (ScanNet200 zero-shot).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Using structural-consensus alone vs combined: AP drop from 14.3% to 12.2%; using semantic similarity alone yields AP 10.1%.",
            "grounding_improvement": "Combined constraint yields +2.1% absolute AP over structural-only and +4.2% over semantic-only in this setup.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Designed specifically to mitigate aggregation failures caused by incomplete point clouds, occlusions, and semantic ambiguities that cause erroneous merges across views.",
            "failure_mode_analysis": "Without semantic cues, spatially proximate but semantically distinct parts may merge (semantic-only issues); without structural cues, fragmented/noisy observations can cause over-segmentation or spurious merges. Exact failure frequencies are reported via AP deltas in ablation.",
            "domain_shift_handling": "Implicitly improves robustness to variable views/partial scans by demanding cross-view containment support before merging; does not perform explicit domain adaptation.",
            "novel_object_performance": "Enables more consistent instance-level aggregation across frequency groups — reported AP gap across head/common/tail &lt;= 1.1% when used in OpenMap.",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Multiplicative coupling of structural consensus rate and semantic similarity (R_struct * R_seman &gt;= tau).",
            "sample_efficiency": null,
            "key_findings_grounding": "Combining geometric structural consensus with VLM-derived semantic similarity significantly reduces incorrect merges and fragmentation in 2D-to-3D aggregation, producing higher-quality instance embeddings that directly improve instruction grounding.",
            "uuid": "e1951.1"
        },
        {
            "name_short": "Instruction-to-Instance Grounding (OpenMap)",
            "name_full": "LLM-assisted Instruction-to-Instance Grounding Module",
            "brief_description": "A two-stage, LLM-driven grounding module: stage 1 prompts an LLM (GPT-4) to convert free-form navigation instructions into precise instance descriptions; stage 2 retrieves top-K candidate instances via CLIP similarity and uses a second LLM dialogue with spatial/contextual metadata (locations and nearby objects) to select the final instance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Instruction-to-Instance Grounding (OpenMap)",
            "model_description": "Stage 1: LLM prompt contains task definition, instance description criteria, and output constraints; LLM outputs a concise instance description. Stage 2: compute CLIP embedding of LLM description, rank all instance embeddings in OpenMap, select top-K candidates; for each candidate, query KD-tree for nearby instances and assemble a prompt with candidate metadata (location, semantic similarity, surrounding objects/labels) for the LLM to perform a multiple-choice selection of the final target.",
            "visual_encoder_type": "Uses CLIP ViT-H embeddings for matching textual descriptions to instance visual embeddings; uses OVSAM features for intra-image semantic similarity computation.",
            "visual_encoder_pretraining": "CLIP pretrained on web-scale image-text data (referenced CLIP model); OVSAM/CropFormer pretrained segmentation models as referenced.",
            "grounding_mechanism": "Textual description generation by LLM followed by embedding-level matching (cosine similarity) between the LLM-generated text embedding and CLIP instance embeddings; second-stage LLM reasoning over structured candidate metadata refines selection.",
            "representation_level": "3D instance-level representation (embeddings aggregated from multiple 2D crops and masks), plus local neighborhood instance metadata",
            "spatial_representation": "Uses instance 3D locations (coordinates), KD-tree neighbor search within 2 m, and lists of surrounding objects provided in the LLM prompt for contextual reasoning.",
            "embodied_task_type": "Instruction grounding for navigation (object-goal, demand-driven, language-guided)",
            "embodied_task_name": "Matterport3D target retrieval; instruction datasets: R2R-CE, VLMap, ALFRED subsets",
            "visual_domain": "Photorealistic indoor scans (Matterport3D scenes), incremental exploration",
            "performance_metric": "Success Rate (SR), SR_topK (SR_4, SR_8, SR_16)",
            "performance_value": "Full grounding module (both unconstrained parsing + OpenMap-assisted selection) SR = 49.6% (SR_8 = 68.3%, SR_16 = 73.7%) on Matterport3D retrieval benchmark used in the paper.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Removing unconstrained LLM parsing reduces SR by ~4.9% (one-component ablation); removing OpenMap-assisted instance selection reduces SR by ~2.4%; removing both yields SR = 38.1% (absolute drop of 11.5% vs full pipeline).",
            "grounding_improvement": "Two-stage LLM + OpenMap selection yields +11.5% absolute SR improvement over removing both strategies; individual components contribute ~+4.9% (parsing) and ~+2.4% (selection).",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Ambiguity in instruction interpretation and multiple matching instances in a scene are major causes of grounding failures; relying solely on semantic similarity is insufficient when multiple similarly-matching instances exist — spatial context and LLM reasoning help disambiguate.",
            "failure_mode_analysis": "If LLM outputs are constrained to label dictionaries (as in some prior work), grounding performance drops; pure embedding matching without spatial context leads to mistaken selections among multiple similar candidates — seen in SR ablations where removing instance-selection lowers SR by 2.4% and removing parsing lowers SR by 4.9%.",
            "domain_shift_handling": "LLM parsing is unconstrained and benefits from general knowledge; grounding remains zero-shot using CLIP embeddings—no explicit adaptation to new sensors/environments beyond map aggregation.",
            "novel_object_performance": "Module is designed to handle free-form descriptions and therefore supports novel object descriptions via LLM; empirical results show improved SR vs baselines that rely on pre-defined labels (e.g., ConceptGraphs), but no per-novel-object numeric split provided.",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Late fusion: LLM textual outputs are compared to visual embeddings using cosine similarity (CLIP); LLM then ingests structured metadata and returns a selection (textual multiple-choice) — essentially two rounds of language-driven, embedding-based retrieval and reasoning.",
            "sample_efficiency": "Zero-shot retrieval; no task-specific fine-tuning samples required. Candidate K swept (4–12) shows SR stable within ~1.9% variation; best at K=8.",
            "key_findings_grounding": "Unconstrained LLM parsing of instructions into precise instance descriptions plus map-provided spatial/contextual metadata substantially improves grounding accuracy versus lexicon-constrained parsing or pure embedding-matching; combining embedding retrieval with LLM re-ranking resolves ambiguous cases where multiple similar instances exist.",
            "uuid": "e1951.2"
        },
        {
            "name_short": "CLIP (used)",
            "name_full": "CLIP (Contrastive Language–Image Pretraining) ViT-H features",
            "brief_description": "Pretrained vision-language model used to produce open-vocabulary image/text embeddings for semantic similarity and instance-level feature aggregation in OpenMap.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CLIP ViT-H (feature extractor)",
            "model_description": "Froze or used as feature extractor: CLIP encodes cropped multi-scale image patches and textual descriptions into a shared embedding space. Cosine similarity between CLIP image/region embeddings and text embeddings is the primary semantic similarity metric for mask merging and instruction matching.",
            "visual_encoder_type": "CLIP ViT-H",
            "visual_encoder_pretraining": "CLIP pretrained on large web-scale image-text pairs (as per CLIP literature)",
            "grounding_mechanism": "Embedding-level matching (cosine similarity) between CLIP image/region embeddings and LLM-produced text descriptions; used both for semantic similarity during mask merging and for instruction-to-instance retrieval.",
            "representation_level": "Region-level / crop-level and aggregated instance-level embeddings",
            "spatial_representation": "Implicit (no internal 3D reasoning); spatial structure is handled outside CLIP (via back-projected point clouds and KD-tree).",
            "embodied_task_type": "Component within embodied navigation mapping and grounding pipeline",
            "embodied_task_name": "Used within OpenMap mapping/retrieval tasks (ScanNet200, Matterport3D)",
            "visual_domain": "Indoor real-world scan images (frame crops)",
            "performance_metric": "Used as similarity metric; no standalone SR/AP reported in paper for CLIP alone",
            "performance_value": null,
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": "Enables open-vocabulary semantic comparisons; ablations in paper show semantic similarity is necessary (semantic-only AP 10.1% vs combined 14.3%), demonstrating CLIP-based semantics crucial to performance.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "CLIP-based semantic embeddings alone can be insufficient when structural aggregation is poor (hence the need for structural-semantic consensus); coarse aggregation methods (e.g., average pooling grids) limit CLIP's effectiveness.",
            "failure_mode_analysis": "Semantic-only aggregation leads to lower AP (10.1%); CLIP features can cluster parts of the same object but may also confuse spatially close distinct objects if aggregation is not structure-aware.",
            "domain_shift_handling": "Used zero-shot; no additional fine-tuning reported in the paper.",
            "novel_object_performance": "CLIP enables open-vocabulary matching to novel/unseen textual descriptions; paper reports relatively stable tail performance (AP gap &lt;= 1.1%) when used within OpenMap.",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Embedding similarity (cosine) as late fusion between visual and textual modalities.",
            "sample_efficiency": null,
            "key_findings_grounding": "Large VLM embeddings (CLIP) provide useful open-vocabulary semantic signals for 2D-&gt;3D instance aggregation and instruction matching, but must be paired with geometric consensus and careful instance aggregation to avoid semantic/spatial misalignments.",
            "uuid": "e1951.3"
        },
        {
            "name_short": "VLMap (baseline)",
            "name_full": "VLMap (Visual-Language Map baseline)",
            "brief_description": "A prior visual-language mapping method that builds semantic grid maps and uses LLMs to translate instructions into object names; used as a baseline in OpenMap evaluations.",
            "citation_title": "VLMap",
            "mention_or_use": "mention",
            "model_name": "VLMap (baseline)",
            "model_description": "Creates coarse semantic grid maps and translates instructions via LLMs into object-category queries for map lookup; uses average-pooling over 2D grids for feature aggregation, which the OpenMap paper argues is a coarse aggregation strategy that limits retrieval quality.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "LLM-to-label translation followed by lookup/matching against grid-aggregated visual features (coarse average pooling over 2D grids).",
            "representation_level": "Top-down grid / coarse scene-level and category-level representations (not instance-level fine-grained mapping)",
            "spatial_representation": "Grid-based top-down spatial maps",
            "embodied_task_type": "Vision-and-language navigation (target retrieval)",
            "embodied_task_name": "Baseline retrieval tasks on Matterport3D and related VLN benchmarks",
            "visual_domain": "Photorealistic indoor scans",
            "performance_metric": "Success Rate (SR) metrics for target retrieval",
            "performance_value": "Reported baseline SR = 27.2% (SR_8 = 29.7%, SR_16 = 32.1%) in the paper's comparisons (exact table values).",
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Paper criticizes VLMap's coarse feature aggregation (average pooling over 2D grids) as a limiting factor for semantic map quality and retrieval capability.",
            "failure_mode_analysis": "Coarse aggregation leads to lower-quality instance discrimination and retrieval; reflected in lower SR compared to OpenMap.",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "LLM produces labels/names which are matched to visual grid embeddings; effectively late fusion via label-embedding lookup.",
            "sample_efficiency": null,
            "key_findings_grounding": "Coarse grid-based semantic aggregation constrains downstream instruction grounding; finer instance-level aggregation and semantic-structural consensus yield better retrieval performance.",
            "uuid": "e1951.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "OVIR-3D: Open-vocabulary 3d instance retrieval without training on 3d data",
            "rating": 2
        },
        {
            "paper_title": "MaskClustering: View consensus based mask graph clustering for open-vocabulary 3d instance segmentation",
            "rating": 2
        },
        {
            "paper_title": "VLMap",
            "rating": 2
        },
        {
            "paper_title": "OpenMask3D: Open-vocabulary 3d instance segmentation",
            "rating": 2
        },
        {
            "paper_title": "ConceptGraphs: Open-vocabulary 3d scene graphs for perception and planning",
            "rating": 2
        },
        {
            "paper_title": "NLMap",
            "rating": 1
        },
        {
            "paper_title": "OpenScene: 3d scene understanding with open vocabularies",
            "rating": 1
        },
        {
            "paper_title": "Mask3D: Mask transformer for 3d semantic instance segmentation",
            "rating": 1
        }
    ],
    "cost": 0.01948225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>OpenMap: Instruction Grounding via Open-Vocabulary Visual-Language Mapping
3 Aug 2025</p>
<p>Danyang Li lidanyang1919@gmail.com 0000-0002-7527-4669
Zenghui Yang zenghuiyang36@gmail.com 0009-0008-1232-1190
Qiang Ma 0000-0001-5791-1890</p>
<p>School of Software
Tsinghua University
BeijingChina</p>
<p>Central South University Hunan
China</p>
<p>Guangpeng Qi</p>
<p>Inspur Yunzhou Industrial Internet Co., Ltd Shandong
China Songtao Pang</p>
<p>Inspur Yunzhou Industrial Internet Co., Ltd Shandong
China</p>
<p>Guangyong Shang</p>
<p>Inspur Yunzhou Industrial Internet Co
Ltd ShandongChina</p>
<p>Tsinghua University
BeijingChina</p>
<p>Zheng Yang</p>
<p>School of Software
Tsinghua University
BeijingChina</p>
<p>OpenMap: Instruction Grounding via Open-Vocabulary Visual-Language Mapping
3 Aug 20255C552168B21A1CD9BEDFBD59CD6DAA9010.1145/3746027.3754887arXiv:2508.01723v1[cs.RO]Information systems → Multimedia information systems;Computing methodologies → Cognitive robotics Instruction GroundingOpen-Vocabulary MappingVision-Language Models3D Semantic MappingEmbodied Navigation
Grounding natural language instructions to visual observations is fundamental for embodied agents operating in open-world environments.Recent advances in visual-language mapping have enabled generalizable semantic representations by leveraging visionlanguage models (VLMs).However, these methods often fall short in aligning free-form language commands with specific scene instances, due to limitations in both instance-level semantic consistency and instruction interpretation.We present OpenMap, a zero-shot open-vocabulary visual-language map designed for accurate instruction grounding in navigation tasks.To address semantic inconsistencies across views, we introduce a Structural-Semantic Consensus constraint that jointly considers global geometric structure and vision-language similarity to guide robust 3D instancelevel aggregation.To improve instruction interpretation, we propose an LLM-assisted Instruction-to-Instance Grounding module that enables fine-grained instance selection by incorporating spatial context and expressive target descriptions.We evaluate OpenMap on ScanNet200 and Matterport3D, covering both semantic mapping and instruction-to-target retrieval tasks.Experimental results show that OpenMap outperforms state-of-the-art baselines in zero-shot settings, demonstrating the effectiveness of our method in bridging free-form language and 3D perception for embodied navigation.</p>
<p>Introduction</p>
<p>As the field of embodied intelligence continues to evolve, enabling agents to navigate using natural language instructions has emerged as a core challenge [4].In Vision-and-Language Navigation (VLN), agents are expected to interpret language commands and perform goal-directed planning based on visual observations in complex 3D environments [26,40].To support this process, semantic maps are essential, as they enhance perceptual understanding and enable precise, instruction-driven navigation [34,37].Recent advances have further incorporated semantic features from VLMs [13,29] into 3D scene representations, giving rise to open-vocabulary visuallanguage maps [7,9,38] that generalize well across a wide range of navigation tasks.However, effectively grounding natural language instructions to specific 3D instances within these maps-i.e., instruction grounding-remains an open challenge.</p>
<p>Existing open-vocabulary visual-language maps typically follow a two-stage pipeline: (1) Semantic mapping: As agents navigate the environment, they collect visual observations and use VLMs  to extract open-vocabulary features at the pixel level.These features are back-projected into 3D and aggregated across views to construct the semantic map.(2) Instruction grounding: Given a free-form instruction, large language models (LLMs) generate descriptive target expressions, which are then grounded to the map by matching them against visual-language features.</p>
<p>While promising, these approaches still face significant limitations.In many cases, the alignment between natural language instructions and map instances underperforms even basic text-toimage matching capabilities of VLMs.Two core challenges remain:</p>
<p>• In semantic mapping, existing methods often rely on spatial or structural constraints to merge observations from different viewpoints.Some cluster point clouds by proximity or predefined grids [2,9], which may lead to over-or under-segmentation.More advanced techniques leverage structural overlaps [38,44], but incomplete point clouds and object occlusion can still cause erroneous merges between semantically distinct instances.</p>
<p>• In instruction grounding, despite operating on open-vocabulary features, most methods remain tied to predefined object lexicons.Some rely on static category labels [7,38], while others restrict LLM outputs to a fixed set of terms [9,10].These constraints hinder the expressive capacity of LLMs and limit their ability to capture finegrained, contextualized object references.For example, given the instruction "Get the chair ready-I want to eat", identifying the chair near the dining table, rather than a generic chair, is non-trivial.</p>
<p>In summary, existing visual-language maps-across both semantic mapping and instruction grounding-largely follow closedvocabulary paradigms, limiting the potential of VLMs and LLMs in open-world navigation.On one hand, they fail to fully exploit structural and semantic cues for robust instance association; on the other, they underutilize the generative flexibility of LLMs in grounding diverse instructions.</p>
<p>Our Work.We introduce OpenMap, a zero-shot Open vocabulary visual-language Map designed for accurate instruction grounding in embodied navigation.OpenMap addresses the above challenges by aligning natural language instructions with 3D instances through a unified visual-language representation.Specifically, we propose a structural-semantic consensus constraint that leverages both geometric and semantic consistency to drive robust instance merging during mapping (Fig. 1a).Furthermore, we introduce an instruction-to-instance grounding module that allows LLMs to generate fine-grained target descriptions and reason over spatial context for precise grounding (Fig. 1b).OpenMap offers key advantages in two core aspects:</p>
<p>• We propose a structural-semantic consensus mapping strategy ( §3.2) to resolve feature inconsistencies commonly introduced during the aggregation stage of existing mapping methods.Our approach incrementally constructs a 3D instance-level semantic map from 2D masks, using structural and semantic consensus as joint criteria for observation fusion.Specifically, two masks are merged only when supported by both global structural and semantic consistency-that is, they are mutually observable from other viewpoints (i.e., exhibit containment relationships in the point cloud) and closely aligned in the vision-language feature space (i.e., refer to the same object or its constituent parts).Guided by these consensus cues, OpenMap iteratively aggregates 2D instances across views, effectively extending 2D vision-language alignment to 3D instance-level representations.</p>
<p>• We introduce a OpenMap-enhanced Instruction-to-Instance grounding module ( §3.3).Unlike prior approaches that constrain LLM outputs using a predefined scene instance lexicon when interpreting navigation instructions [9,22,38], OpenMap enables more precise grounding of natural language to scene instances.This allows large language models to generate fine-grained instance descriptions-for example, interpreting "I am thirsty" as "a cup filled with water" rather than simply "cup."Furthermore, OpenMap incorporates spatial context to support reasoning over candidate instances; for instance, given the instruction "Get the chair ready-I want to eat, " it can identify the intended chair by considering nearby objects such as a dining table.The synergy between LLMs and OpenMap enables accurate indexing from high-level navigation instructions to specific map instances.</p>
<p>We evaluate OpenMap on the public benchmark ScanNet200 [30], focusing on instance segmentation precision and semantic accuracy.To further assess its target retrieval capabilities in navigation scenarios, we conduct experiments on the Matterport3D dataset [43] using a variety of instruction types, including objectgoal [33], demand-driven [36], and language-guided instructions [14].Compared to state-of-the-art (SOTA) methods, OpenMap consistently outperforms them in both zero-shot semantic mapping and instruction-to-target grounding.</p>
<p>Our contributions are summarized as follows:</p>
<p>§3.3 Instruction-to-Instance Grounding</p>
<p>Step 1</p>
<p>Initial Mask Merging</p>
<p>Step 2</p>
<p>Consensus Recalculating</p>
<p>Step 3  • We develop OpenMap, an open-vocabulary visual-language mapping framework that bridges LLM-based instruction parsing and 3D instance grounding, enabling precise instruction grounding from free-form navigation commands.</p>
<p>Mask Cluster Merging</p>
<p>OpenMap</p>
<p>• We propose a novel structural-semantic consensus constraint that jointly leverages global geometric consistency and vision-language semantics to enable fine-grained 3D instance-level mapping.</p>
<p>• We evaluate OpenMap on ScanNet200 and Matterport3D, covering both semantic mapping and target retrieval tasks, and show consistent improvements over existing methods.Our code is publicly available at https://github.com/openmap-project/OpenMap.</p>
<p>Related Work</p>
<p>Vision-Language Foundation Models.Large-scale VLMs, such as CLIP [29] and BLIP [19], align visual and textual modalities within a shared embedding space [47].These advances have facilitated open-vocabulary understanding [39] across tasks such as classification and retrieval.Recent efforts extend these capabilities to 2D segmentation, with models like OVSeg [20] and OVSAM [45] incorporating segmentation heads to support instance-level openvocabulary queries [48].However, transferring this alignment into 3D space remains challenging due to sparse and incomplete data, especially in navigation settings where environments are incrementally explored [3,11].Open-Vocabulary 3D Instance Mapping.Among various forms of open-vocabulary 3D semantic mapping, instance-level mapping is particularly challenging yet essential for accurate instruction grounding.Recent methods for open-vocabulary 3D instance segmentation [15,25] follow two main paradigms.The first, 3D-to-2D [12,27,34], performs segmentation in 3D and projects results to 2D for feature extraction, but often suffers from poor completeness and semantic consistency due to sparse point clouds.The second, 2D-to-3D [7,24,44], segments 2D frames, uses depth maps for 3D back-projection, and aggregates instance masks via geometric overlap or spatial clustering.While effective, these methods typically overlook semantic similarity in the merging process.</p>
<p>Semantic Mapping for Instruction Grounding.Semantic maps are critical in VLN, as they allow agents to reason over spatial and semantic structures for instruction interpretation and execution [9,40].Early works project 2D instance masks onto bird'seye-view layouts [5,21], or aggregate features into top-down grids to support open-vocabulary querying [10,37].VLMap [9] introduces semantic grid maps and uses LLMs to translate instructions into open-vocabulary object names.ConceptGraphs [7] and HOV-SG [38] further enhance instruction interpretation by constructing spatial graphs over scene instances, enabling explicit modeling of inter-object relationships.However, these methods still rely on predefined labels or coarse semantic features, limiting their ability to support fine-grained, open-vocabulary instruction grounding.</p>
<p>Methodology 3.1 Method Overview</p>
<p>An overview of OpenMap is shown in Fig. 2. We follow a generic agent setting for embodied localization [18,41] and navigation [9,22], where an agent collects a sequence of RGB-D observations during exploration [16,17], denoted as I = { 1 ,  2 , ...,   } and D = { 1 ,  2 , ...,   }.For each frame   , we apply an off-the-shelf 2D segmentation model to generate masks
{𝑚 𝑡 𝑖 | 𝑖 = 1, ...</p>
<p>, 𝑛 𝑡 } and use a vision-language model to extract corresponding open-vocabulary features {𝑓 𝑡</p>
<p>|  = 1, ...,   }, where   is the number of masks in   .During the semantic mapping stage ( §3.2), we apply structural and semantic consensus constraints to determine whether any two masks across the image sequence correspond to the same instance, and iteratively merge those that satisfy both into a unified 3D instance.We then adopt a completeness-guided strategy to select representative masks and aggregate their features to form a holistic semantic embedding for each instance.</p>
<p>In the instruction grounding stage ( §3.3), the agent receives a natural language instruction and leverages an LLM to parse it into a target instance description.Unlike conventional methods constrained by predefined vocabularies, our approach allows for free-form, fine-grained descriptions of navigation goals.Guided by OpenMap, the LLM selects the most semantically relevant instance by reasoning over candidate regions and their associated features within the constructed map.</p>
<p>Structural-Semantic Consensus Mapping</p>
<p>The core philosophy behind constructing OpenMap is to concurrently consider both spatial structure and semantic feature constraints across different observations.</p>
<p>To visualize this concept, consider a scenario where three observers (e.g.,   ,   , and   ) are examining the same object (e.g., a bunch of roses) from distinct angles.How should they describe it to ascertain that they are indeed looking at the same object?</p>
<p>• Semantically, the observations should exhibit similarities-for instance,   might note 'a few green leaves',   could describe 'three roses', and   might see a 'bunch of flowers'.</p>
<p>• Structurally, there should be a consensus, such that the parts of the instance observed by   and   are also encompassed in   's observation, suggesting that these observations originate from the same instance.</p>
<p>We proceed by modeling these structural-semantic consensus constraints to facilitate precise instance merging.</p>
<p>3.2.1</p>
<p>Structural-Semantic Consensus Rate Computing.For any two masks   and   , we evaluate their potential for merging by assessing the structural-semantic consensus rate between them.Structural Consensus Rate.Drawing on well-established structural consensus analysis [35,42], we strategically harness the redundancy of observations to ensure structural self-consistency of instances Specifically, following [44], we back-project each mask   into a 3D point cloud   using depth maps.If the point cloud of an image  overlaps with   (i.e., overlap exceeds   ), then mask   is deemed observable in image  , and we define I (  ) as the set of all images that can observe mask   .</p>
<p>We then identify the set of images that can simultaneously observe both masks   and   intended for merging, denoted as  (  ,   ) = I (  ) ∩ I (  ).Subsequently, we seek frames capable of supporting the merger of   and   .Specifically, for an image   containing a mask   that spatially encompasses both   and   (i.e., both have at least   of their point clouds within   ), this subset of images is defined as  (  ,   ) = {  ∈  (  ,   )|  ,   ⊆   }.Consequently, the structural consensus rate for the two masks   and   is calculated as the ratio of supporters to observers:
𝑅 𝑠𝑡𝑟𝑢𝑐. (𝑚 𝑖 , 𝑚 𝑗 ) = |𝑆 (𝑚 𝑖 , 𝑚 𝑗 )|/|𝑂 (𝑚 𝑖 , 𝑚 𝑗 )|.(1)
Semantic Similarity Rate.Another criterion for determining whether masks can be merged is their semantic similarity.Diverging from traditional models that rely on a closed vocabulary, VLMs like CLIP capture subtle semantic connections between observations, even different components of the same instance.As shown in Fig. 3, features from the same instance are tightly clustered in the latent space, while spatially close but distinct instances exhibit clear feature separation.Consequently, by integrating open-vocabulary feature similarity metrics into instance merging, we can effectively reduce the misalignment of different instances that are close in spatial structure but semantically distinct.We define the semantic similarity rate between masks   and   as the cosine similarity between their respective features   and   :
𝑅 𝑠𝑒𝑚𝑎𝑛. (𝑚 𝑖 , 𝑚 𝑗 ) = cos(𝑓 𝑖 , 𝑓 𝑗 ). (2)
Put Together.In mask merging, we balance the above structural consensus and semantic similarity.Specifically, when
𝑅 𝑠𝑡𝑟𝑢𝑐. (𝑚 𝑖 , 𝑚 𝑗 ) * 𝑅 𝑠𝑒𝑚𝑎𝑛. (𝑚 𝑖 , 𝑚 𝑗 ) ≥ 𝜏 𝑡ℎ𝑟𝑒𝑠 ,(3)
masks   and   are considered to form the same instance, where  ℎ is a predefined threshold.</p>
<p>3.2.2</p>
<p>Iterative Mask Merging.After computing pairwise relationships between masks, we iteratively merge them following the general procedure in [44].This results in a set of 3D point clouds, each representing a distinct instance, with open-vocabulary semantic features aggregated from the corresponding masks.Specifically, we prioritize merging mask pairs associated with more robust observations (i.e., a larger | (  ,   )|).Therefore, during the iterative process, we set a gradually decreasing threshold for observer counts,   .In each merging cycle, two masks,   and   , are merged into a new mask,  , , with its corresponding point cloud,  , , if they not only meet the conditions set by Eq. 3 but also exceed the observer count threshold, | (  ,   )| &gt;   .</p>
<p>After each merging cycle, it's necessary to recalculate the structural semantic consensus rate among the newly formed instances.The strategy is as follows:</p>
<p>• Given the structural changes in the new instance, along with altered observational and containment relationships, we update its observers and supporters and recompute  . .• Furthermore, due to the aggregation of diverse observations, the semantic features of the combined instance need to be updated.We select the features from the mask that most completely captures the point cloud of the new instance and recalculate  . .Each iteration cycle reduces threshold   , allowing the instance to incorporate more masks, and this process continues until no further merging is feasible.At the end of the iterations, a list of 3D instances is generated, each linked to multiple 2D masks.</p>
<p>Finally, based on the completeness of the instance's observation, we strategically aggregate features for each instance.Following OpenMask3D [34], we select the top- masks that best cover the instance and obtain  multi-level crops from the corresponding image areas.Subsequently, features are extracted from these  *  crops using CLIP, with the average pooling results serving as the open-vocabulary feature vector for the instance.</p>
<p>Instruction-to-Instance Grounding</p>
<p>In everyday life, when colleagues hand over a task, from the arranger's perspective, it is essential to describe the requirements as accurately as possible, rather than being vague.From the executor's perspective, any unclear requirements should be clarified based on contextual information to eliminate ambiguities.</p>
<p>These experiences are equally applicable to embodied navigation tasks, and the parsing of instructions to instances adheres to the following principles:</p>
<p>• When utilizing LLMs for instruction translation, the instructions should be converted into instances described precisely in natural language, instead of choosing from a limited set of dataset labels or an instance dictionary specific to a certain scenario.</p>
<p>• Even with accurate instance descriptions, there may be multiple suitable targets in the scene, and agents should enhance their decision-making by considering contextual information such as the locations of candidates and their surrounding environment.</p>
<p>Next, we will discuss how to utilize OpenMap to assist LLMs in implementing these concepts.Generic Instruction Parsing.We first utilize an LLM to convert generic navigation instructions into precise instance descriptions that enable target retrieval within OpenMap.Unlike existing methods that translate various types of navigation instructions into targets from a fixed instance dictionary [22], our approach fundamentally differs in that we do not restrict the LLM outputs to predefined dictionary terms, thus fully unleashing its vast knowledge and analytical capabilities.</p>
<p>The textual prompt is composed of the following key components: (1) Navigation Task Definition: Similar to existing work in navigation instruction analysis, we provide the background of the navigation task, including environmental information and the format of the instructions.(2) Instance Description Criteria: We retain the descriptions of instance characteristics from the original navigation instructions and infer additional features based on environmental information to reduce linguistic ambiguities.(3) Output Format Constraints: We streamline the instance descriptions to avoid excessive length, given that most existing VLMs (e.g., the original CLIP) have limited capabilities to process long and complex texts.</p>
<p>For an instance description derived from a navigation instruction, where the VLM encodes the feature vector as   , we calculate the similarity with all instance features in OpenMap { 1 ,  2 , ...,   } and rank them.The top   are selected as candidate instances.Instance Selection with OpenMap.Due to the potential presence of multiple instances in a scene that closely match the target description, relying solely on semantic similarity often fails to provide accurate measurements.However, leveraging environmental constraints around candidate instances can significantly enhance the performance of targeted retrieval.A typical case is "Prepare the chair, I want to eat".Typically, chairs can found in every room, but the follow-up prompt "I want to eat", implies that the chair needed is one near the dining table.</p>
<p>For   candidates, we further refine our selection using the instance information provided by OpenMap.Specifically, for each</p>
<p>Navigation Instruction</p>
<p>Object Goal Ins. (e.g., match with the labels from the LVIS dataset [8]).Note that applying fixed labels here is solely to assist the LLM in instance selection and does not compromise the open-vocabulary querying capabilities of OpenMap.</p>
<p>Next, we initiate a second round of dialogue with the LLM, providing details about the candidate instance and its surrounding objects, and determining the final retrieval target through a multiplechoice format.The template for providing instance information in the prompt is abstracted as follows: "Candidate Instance I  : {location: (  ,   ,   ); semantic similarity:   ; surrounding objects: [I   , location: (   ,    ,    ), label:    ], ...}"</p>
<p>Experiments</p>
<p>In this section, we evaluate OpenMap against current SOTA methods in terms of semantic mapping and target retrieval.</p>
<p>Experimental Setup</p>
<p>Dataset.We utilize the ScanNet200 validation dataset [30] to evaluate OpenMap's semantic mapping capabilities.This dataset features 312 indoor scans across 200 categories, organized into three subsets based on the frequency of instance occurrences, allowing for an effective assessment across a long-tail distribution.Additionally, we examine OpenMap's navigation target retrieval effectiveness with the Matterport3D Semantics dataset [43], following established navigation map research [9][38].Our evaluation spans 20 scenes-11 from the R2R-CE val-unseen split [14] and 9 from the VLMap evaluation dataset [9].We construct a comprehensive set of test cases using subsets of navigation instructions from R2R-CE, VLMap, and ALFRED [32], covering three instruction types: object-goal, demand-driven, and language-guided.We evaluated OpenMap against SOTA 3D semantic mapping and VLN target retrieval methods.For semantic mapping, Mask3D [31] is a representative work trained under supervision on ScanNet200.OpenScene [27] is an open-vocabulary 3D scene understanding model that generates per-point feature vectors, for which we average the per-point features within each instance mask, following the approach in [34].OpenMask3D [34] utilizes supervised mask proposals from Mask3D and employs CLIP for openvocabulary semantic aggregation.OVIR-3D [24] and MaskCLustering [44] are zero-shot open-vocabulary mapping methods that aggregate instances progressively from 2D to 3D, closely related to our approach.For target retrieval, NLMap [2] and VLMap [9] utilize LLMs to retrieve targets of known categories on constructed queryable maps.ConceptGraphs [7] and HOV-SG [38] further enhance the object retrieval and reasoning capabilities by constructing graphs between instances.Metrics.To validate our mapping accuracy, we report Average Precision (AP) at 25% and 50% Intersection over Union (IoU) thresholds, along with the mean AP from 50% to 95% at 5% intervals.We also evaluate performance in a class-agnostic setting that focuses solely on mask quality, ignoring semantic labels.For target retrieval in OpenMap, we use the Success Rate (SR), defined as successful if the target is retrieved within 1 meter of the ground truth center.Additionally, we measure the top- Success Rate (SR  ), indicating success within up to  retrieval attempts in the scene.Implementation Details.To obtain complete object masks rather than overly fragmented results (i.e., all pixels of an object belonging to one mask), we employ CropFormer [23,28] for 2D segmentation.An intuitive approach for encoding visual-language features for each mask involves cropping bounding boxes and extracting features using CLIP.However, this process generates an excessive number of image patches and, limited by CLIP's processing speed, proves inefficient.We utilize OVSAM [46] to extract features for candidate regions within an image in one go, which is achieved by using the bounding boxes of these regions as prompts for OVSAM.Note that OVSAM features are only used for computing the semantic similarity rate.For a fair comparison of semantic matching capabilities with existing methods, we use features extracted by CLIP [29] ViT-H for the final feature aggregation.We apply post-processing methods from MaskCLustering to filter under-segmented masks and separate disconnected point clusters into distinct instances.Regarding parameter settings, in §3.2.1, the observational threshold for masks   = 0.3, the containment threshold for masks   = 0.8, and the threshold of structural-semantic consensus rate  ℎ = 0.6; in §3.2.2, the initial threshold for the number of observers   is set at the top 5% of all mask pairs, reducing by 5% in each iteration until the process concludes; in §3.3, the number of candidate instances   = 8 and the number of neighboring instances   = 5.</p>
<p>Model</p>
<p>Mapping Performance</p>
<p>Quantitative Results.Following standard practice in both supervised and zero-shot semantic mapping, we primarily report results on ScanNet200 as it serves as the most widely adopted benchmark, enabling fair comparison with prior work.As shown in Table 1, we categorize the comparison methods into three groups.</p>
<p>Compared to the fully zero-shot OVIR-3D and MaskClustering, OpenMap achieves the highest accuracy on ScanNet200 in both semantic and class-agnostic metrics.Specifically, OpenMap shows a 19.2% improvement in average semantic AP over MaskClustering, which only considers structural features during aggregation.Furthermore, compared to OVIR-3D, which processes local geometric and semantic features frame by frame, OpenMap, which  incorporates global feature semantic consensus, shows even more pronounced improvements, with increases of 53.8% in semantic AP and 37.5% in class-agnostic AP.Notably, compared to these zero-shot methods, OpenMap maintains consistently high performance across head, common, and tail categories, with an AP gap not exceeding 1.1%.This capability to handle instances of varying frequencies highlights OpenMap's robust open-vocabulary abilities.</p>
<p>In contrast with OpenScene and OpenMask3D, OpenMap, lacking any prior knowledge from ScanNet200, still shows a significant gap in the class-agnostic metrics.Nevertheless, OpenMap significantly surpasses OpenScene in all semantic metrics due to its lack of strategic open-vocabulary feature aggregation.Moreover, Open-Map is close to OpenMask3D in semantic AP, even exceeding it in AP50 and AP25 by +6.1% and +10.2%, respectively.Additionally, OpenMask3D is a semantic mapping method based on 3D-to-2D projection, using a complete 3D point cloud of the scene as a prior.</p>
<p>In navigation tasks where the scene is progressively explored, our method, OpenMap, fits more seamlessly, able to integrate into existing navigation tools as a fundamental semantic map to support downstream tasks.Qualitative Results.As shown in Fig. 5 and Fig. 6, we present qualitative instance segmentation results of OpenMap on Matter-port3D and ScanNet200 scenes.OpenMap demonstrates strong performance in two key scenarios: (1) accurately segmenting small objects attached to larger surfaces (e.g., scattered items on a table or glasses on a tray); and (2) preserving the completeness of large objects despite limited viewpoint coverage (e.g., preventing large sofas, tables, and beds from being mistakenly fragmented).Notably, in the first subview of the Matterport3D scene, OpenMap successfully segments individual items on the tray, such as bowls and glasses.However, the ground truth merges these into a single instance, leading to a false negative during evaluation despite the correctness of the prediction.</p>
<p>Target Retrieval Performance</p>
<p>Quantitative Results.OpenMap is designed to achieve accurate grounding of navigation instructions to scene instances, a task that jointly evaluates the quality of semantic mapping and the effectiveness of instruction-to-instance grounding.We compare Open-Map against two representative visual-language mapping baselines, NLMap [2] and VLMap [9], as well as the recent SOTA method ConceptGraphs [7], in terms of target retrieval success rate.For each trial, all methods first perform full-scene mapping, followed by instruction parsing via a LLM, and then query the target location based on the generated map.To ensure a fair comparison, all methods employ GPT-4 [1] as the LLM.</p>
<p>As shown in Table 2, OpenMap significantly outperforms the baselines across all success rate metrics.In particular, under the SR metric-which reflects the most practical requirement in navigation (i.e., succeeding on the first attempt)-OpenMap surpasses NLMap, VLMap, and ConceptGraphs by +24.5%, +22.4%, and +8.7%, respectively.Among the baselines, VLMap suffers from a coarse feature aggregation strategy (i.e., average pooling over 2D grids), which fundamentally limits the quality of the underlying semantic map and thus its retrieval capability.ConceptGraphs, on the other hand, relies on pre-defined labels for instances, which restricts its generalization to diverse natural language descriptions.Qualitative Results.Fig. 1 shows instance-colored segmentation results and similarity heatmaps generated by OpenMap on a Matterport3D scene (ID: 8194nk5LbLH).As shown in Fig. 1a, the semantic map yields accurate instance-level segmentation for both common objects (e.g., sofas) and uncommon structures (e.g., columns).White regions in the overview indicate missing scan data.In Fig. 1b, OpenMap accurately localizes target instances for object-goal, demand-driven, and language-guided navigation tasks.Notably, the heatmaps reveal that even non-top candidates retain task-relevant attributes.For example, in the object-goal case (e.g., "a pair of lounge chairs"), secondary matches preserve key spatial and functional cues such as seating layout and back-to-back configuration.In demand-driven scenarios (e.g., "Where should I eat?"), nearby tables also reflect contextual relevance.</p>
<p>Ablation Studies</p>
<p>Ablation Study on Mapping.In Table 3, we evaluate the impact of two key components in OpenMap's mapping pipeline ( §3.2): structural consensus (Structural.)and semantic similarity (Semantic.).When using only structural consensus, the AP drops from 14.3 to 12.2, yet remains higher than all zero-shot baselines.Moreover, since spatial structural relations are essential for associating instances in 3D reconstruction, structural constraints cannot be entirely removed.To evaluate the performance of using only semantic similarity, we adopt the local structural similarity metric from OVIR-3D as a baseline.The result shows a significant AP drop of 4%.When both components are used jointly, OpenMap achieves the best performance, as expected.These results confirm that both structural and semantic cues are critical to the effectiveness of OpenMap's mapping strategy.</p>
<p>Ablation Study on Instruction Grounding.Ablation Study on Hyperparameters.We conducted additional evaluations to assess the robustness of our algorithm with respect to key hyperparameters.As shown in Table 5, we first examined the effect of the consensus threshold used in the mapping stage.Within the range of 0.5-0.7, the AP variation remains within 0.34.The lowest performance occurs at a threshold of 0.7, yielding an AP of 13.7, while the highest AP of 14.3 is achieved at 0.6, which we adopt in practice.We further evaluated the impact of the number of candidate instances used in instruction-to-instance grounding.</p>
<p>As shown in Table 6, SR remains stable within a 1.9% fluctuation when the candidate count ranges from 4 to 12, with the best performance (49.6% SR) observed at 8 candidates.These ablation results demonstrate the consistent and robust performance of OpenMap across a range of hyperparameter settings.</p>
<p>Conclusion</p>
<p>We present OpenMap, a zero-shot open-vocabulary visual-language mapping framework for accurate instruction grounding in embodied navigation.To address challenges in instance inconsistency and limited instruction expressiveness, we propose a structuralsemantic consensus constraint for robust 3D instance aggregation and an instruction-to-instance grounding module for fine-grained grounding of free-form commands.Extensive experiments on Scan-Net200 and Matterport3D demonstrate that OpenMap consistently improves both semantic mapping and instruction to target instance retrieval under zero-shot settings.By enabling precise alignment between natural language instructions and 3D scene instances, OpenMap makes a concrete step toward more reliable and generalizable instruction execution in real-world navigation tasks.</p>
<p>"× 2 ( 2 (
22
Find a pair of back-to-back lounge chairs" "I'm hungry.Where should I eat?" "Move forward until you reach the potted plant and a door." a) Instance-level semantic mapping."Find a pair of back-to-back lounge chairs" "I'm hungry.Where should I eat?" "Move forward until you reach the potted plant and a door." b) Grounding generic navigation instructions to target instances.</p>
<p>Figure 1 :
1
Figure 1: OpenMap constructs an open-vocabulary visual-language map.(a) OpenMap performs fine-grained, instance-level semantic mapping on navigation scenes from Matterport3D [43].(b) Three types of navigation instructions are shown from left to right: object-goal, demand-driven, and language-guided.OpenMap accurately grounds generic instructions to the intended targets, where darker regions in the heatmaps indicate stronger alignment between the instruction and the predicted instance.</p>
<p>Figure 2 :
2
Figure 2: OpenMap Overview.OpenMap takes RGB-D inputs from multiple viewpoints and applies pretrained models to predict 2D masks and extract open-vocabulary features.During semantic mapping ( §3.2), it iteratively aggregates 2D masks into 3D instances using a structural-semantic consensus constraint.During instruction grounding ( §3.3), an LLM selects the target instance by reasoning over candidate proposals and scene context provided by OpenMap.</p>
<p>Figure 3 :
3
Figure 3: Feature distribution of adjacent objects.Although the triangle ruler is physically attached to the laptop, it remains clearly distinguishable in the vision-language feature space.</p>
<p>"</p>
<p>Get the wooden chair ready-I want to eat." Demand-driven Ins."I am hungry."Language-guided Ins."Exit the bedroom, turn right to the dining table."</p>
<p>Figure 4 :
4
Figure 4: Instruction-to-Instance Grounding pipeline.In the first round, the LLM generates a precise description of the navigation target and retrieves candidate instances from OpenMap.In the second round, it reasons over the candidates and their surrounding context to infer the final target instance.candidateinstance I  , we use a KD-tree to search within OpenMap for the   nearest instances I   (i = 1, 2, ...,   ) within a 2-meter radius.Subsequently, we label these I   (e.g., match with the labels from the LVIS dataset[8]).Note that applying fixed labels here is solely to assist the LLM in instance selection and does not compromise the open-vocabulary querying capabilities of OpenMap.Next, we initiate a second round of dialogue with the LLM, providing details about the candidate instance and its surrounding objects, and determining the final retrieval target through a multiplechoice format.The template for providing instance information in the prompt is abstracted as follows: "Candidate Instance I  : {location: (  ,   ,   ); semantic similarity:   ; surrounding objects: [I   , location: (   ,    ,    ), label:    ], ...}"</p>
<p>Figure 5 :
5
Figure 5: Semantic mapping results of OpenMap on Matterport3D.</p>
<p>Figure 6 :
6
Figure 6: Semantic mapping results on ScanNet200.</p>
<p>… Masked RGB-D §3.2 Structural-Semantic Consensus Mapping
Structural ConsensusCosine SimilarityVisual-Language Feature Space𝐼𝐼1𝐼𝐼1Masks from𝐼𝐼2𝐼𝐼2same instance𝐼𝐼3𝐼𝐼3𝑓𝑓 𝑘𝑘𝑓𝑓 𝑖𝑖𝐼𝐼5 𝐼𝐼4𝐼𝐼5 𝐼𝐼4……𝐼𝐼𝑁𝑁𝐼𝐼𝑁𝑁𝑓𝑓𝑘𝑘𝑓𝑓𝑖𝑖Semantic SimilarityIterative Mask Merging</p>
<p>Table 1 :
1
[44]nstance Segmentation Results on ScanNet200[30].Mask3D[31]requires supervised (sup.)training on ScanNet200 for mask and semantic extraction.OpenScene[27]+ Masks and OpenMask3D[34]depend on masks provided by Mask3D.In a fully zero-shot (z.s.) setting, our method, OpenMap, surpasses both OVIR-3D[24]and MaskClustering[44]across all metrics.
FeaturesSemanticClass-agnosticAP AP 50 AP 25 head(AP) common(AP) tail(AP) AP AP 50 AP 25sup. mask + sup. semanticMask3D [31]-26.9 36.241.439.821.717.939.7 53.662.5sup. mask + z.s. semanticOpenScene [27] + Masks OpenSeg [6] 11.7 15.217.813.411.69.939.7 53.662.5OpenMask3D [34]CLIP [29]15.4 19.923.117.114.114.939.7 53.662.5z.s. mask + z.s. semanticOVIR-3D [24]CLIP [29]9.318.725.010.19.48.114.4 27.538.8MaskClustering [44]CLIP [29]12.0 23.330.111.910.513.819.0 36.650.8OpenMap (Ours)CLIP [29]14.3 26.0 33.314.513.814.719.8 38.0 51.8MethodSR[%]SR 4 [%]SR 8 [%]SR 16 [%]NLMap [2]25.128.431.537.1VLMap [9]27.229.732.137.5ConceptGraphs [7]40.943.450.654.9OpenMap (Ours)49.658.368.273.7</p>
<p>Table 2 :
2
[43]gation target retrieval results on Matterport3D[43].Compared with existing open-vocabulary mapping methods designed for navigation tasks, OpenMap achieves the best performance across all target retrieval success rate metrics.</p>
<p>Table 3 :
3
Structural.Semantic.AP AP 50 AP 25 Ablation study on Mapping methods.
✓✗12.2 23.430.2✗✓10.1 19.326.7✓✓14.3 26.0 33.3Ins. Parsing Ins. Selec. SR[%] SR 8 [%] SR 16 [%]✗✗38.161.067.6✓✗47.268.273.7✗✓44.761.067.6✓✓49.668.273.7</p>
<p>Table 4 :
4
Ablation study on Grounding methods.</p>
<p>Table 5 :
5
25ble 4presents an ablation study on two key strategies in the instruction-to-instance grounding ( §3.3): unconstrained instruction parsing without restricting LLM outputs to a predefined vocabulary (Ins.Parsing), and OpenMap-assisted instance selection (Ins.Selec.).When Ins.Parsing is removed, we follow the parsing approach used in VLMap as a baseline.Experimental results show that removing both strategies leads to a drop of over 10% in SR compared to the full Open-Map pipeline.Nevertheless, due to the accurate semantic map, our AP AP 50 AP25 ℎ (0.5 -0.7) 14.0 ± 0.34 36.9 ± 1.14 49.4 ± 2.41 Impact of consensus threshold. (4-12) 47.7 ± 1.9 57.6 ± 0.7 68.2 ± 0.0
SR[%]SR 8 [%]SR 16 [%]</p>
<p>Table 6 :
6
Impact of candidate number.method still outperforms VLMap and NLMap (see Table 2), and achieves better performance than ConceptGraphs on SR 8 and SR 16 .Individually, removing Ins.Parsing results in a 4.9% decrease in SR, while excluding Ins.Selec.causes a 2.4% drop.Notably, since Ins.Selec.performs filtering within the top-8 most relevant candidates, its removal does not affect SR 8 and SR 16 .The results demonstrate that OpenMap effectively unlocks the instruction interpretation capabilities of LLMs.</p>
<p>AcknowledgmentsWe sincerely thank the MobiSense group and the Tsinghua University -Inspur Yunzhou Joint Research Center for New Industrialization and Trustworthy Networks.This work is supported in part by the National Key Research Plan under grant No. 2021YFB2900100, the NSFC under grant No. 62372265, No. 62302254, and No. 62402276.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Open-vocabulary queryable scene representations for real world planning. Boyuan Chen, Fei Xia, Brian Ichter, Kanishka Rao, Keerthana Gopalakrishnan, Austin Michael S Ryoo, Daniel Stone, Kappler, ICRA. 2023</p>
<p>Clip2scene: Towards labelefficient 3d scene understanding by clip. Runnan Chen, Youquan Liu, Lingdong Kong, Xinge Zhu, Yuexin Ma, Yikang Li, Yuenan Hou, Yu Qiao, Wenping Wang, CVPR. 2023</p>
<p>A survey of embodied ai: From simulators to research tasks. Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, Cheston Tan, IEEE Transactions on Emerging Topics in Computational Intelligence. 2022. 2022</p>
<p>Navigation instruction generation with bev perception and large language models. Sheng Fan, Rui Liu, Wenguan Wang, Yi Yang, ECCV. 2024</p>
<p>Scaling openvocabulary image segmentation with image-level labels. Golnaz Ghiasi, Xiuye Gu, Yin Cui, Tsung-Yi Lin, European conference on computer vision. 2022</p>
<p>Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning. Qiao Gu, Ali Kuwajerwala, Sacha Morin, Krishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Rama Chellappa, IEEE International Conference on Robotics and Automation. 2024. 2024ICRA</p>
<p>Lvis: A dataset for large vocabulary instance segmentation. Agrim Gupta, Piotr Dollar, Ross Girshick, CVPR. 2019</p>
<p>Visual language maps for robot navigation. Chenguang Huang, Oier Mees, Andy Zeng, Wolfram Burgard, 2023 IEEE International Conference on Robotics and Automation (ICRA). 2023</p>
<p>Ivlmap: Instance-aware visual language grounding for consumer robot navigation. Jiacui Huang, Hongtao Zhang, Mingbo Zhao, Zhou Wu, arXiv:2403.193362024. 2024arXiv preprint</p>
<p>Clip2point: Transfer clip to point cloud classification with image-depth pre-training. Tianyu Huang, Bowen Dong, Yunhan Yang, Xiaoshui Huang, Rynson Wh Lau, Wanli Ouyang, Wangmeng Zuo, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Openins3d: Snap and lookup for 3d open-vocabulary instance segmentation. Zhening Huang, Xiaoyang Wu, Xi Chen, Hengshuang Zhao, Lei Zhu, Joan Lasenby, European Conference on Computer Vision. 2024</p>
<p>Scaling up visual and visionlanguage representation learning with noisy text supervision. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, Tom Duerig, International conference on machine learning. 2021</p>
<p>Beyond the nav-graph: Vision-and-language navigation in continuous environments. Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra, Stefan Lee, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UK2020. August 23-28, 2020Proceedings, Part XXVIII 16</p>
<p>Mask-attention-free transformer for 3d instance segmentation. Xin Lai, Yuhui Yuan, Ruihang Chu, Yukang Chen, Han Hu, Jiaya Jia, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Leovr: Motion-inspired visual-lidar fusion for environment depth estimation. Danyang Li, Jingao Xu, Zheng Yang, Qiang Ma, Li Zhang, Pengpeng Chen, IEEE Transactions on Mobile Computing. 2023. 2023</p>
<p>Motion inspires notion: Self-supervised visual-LiDAR fusion for environment depth estimation. Danyang Li, Jingao Xu, Zheng Yang, Qian Zhang, Qiang Ma, Li Zhang, Pengpeng Chen, Proceedings of the 20th annual international conference on mobile systems, applications and services. the 20th annual international conference on mobile systems, applications and services2022</p>
<p>EdgeSLAM2: Rethinking edge-assisted visual SLAM with on-chip intelligence. Danyang Li, Yishujie Zhao, Jingao Xu, Shengkai Zhang, Longfei Shangguan, Zheng Yang, IEEE INFOCOM 2024-IEEE Conference on Computer Communications. 2024</p>
<p>Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, International conference on machine learning. 2022</p>
<p>Open-vocabulary semantic segmentation with mask-adapted clip. Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, Diana Marculescu, CVPR. 2023</p>
<p>Bird's-eye-view scene graph for vision-language navigation. Rui Liu, Xiaohan Wang, Wenguan Wang, Yi Yang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment. Yuxing Long, Wenzhe Cai, Hongcheng Wang, Guanqi Zhan, Hao Dong, 8th Annual Conference on Robot Learning. 2024</p>
<p>High-Quality Entity Segmentation. Qi Lu, Jason Kuen, Shen Tiancheng, Gu Jiuxiang, Guo Weidong, Jia Jiaya, Lin Zhe, Yang Ming-Hsuan, ICCV. 2023</p>
<p>Ovir-3d: Open-vocabulary 3d instance retrieval without training on 3d data. Shiyang Lu, Haonan Chang, Eric Pu Jing, Abdeslam Boularias, Kostas Bekris, Conference on Robot Learning. 2023</p>
<p>Open3dis: Open-vocabulary 3d instance segmentation with 2d mask guidance. Phuc Nguyen, Tuan Duc Ngo, Evangelos Kalogerakis, Chuang Gan, Anh Tran, Cuong Pham, Khoi Nguyen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Visual language navigation: A survey and open challenges. Sang-Min Park, Young-Gab Kim, Artificial Intelligence Review. 2023. 2023</p>
<p>Openscene: 3d scene understanding with open vocabularies. Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, CVPR. 2023</p>
<p>High Quality Entity Segmentation. Lu Qi, Jason Kuen, Tiancheng Shen, Jiuxiang Gu, Wenbo Li, Weidong Guo, Jiaya Jia, Zhe Lin, Ming-Hsuan Yang, ICCV. 2023</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, ICML. 2021</p>
<p>Language-grounded indoor 3d semantic segmentation in the wild. David Rozenberszki, Or Litany, Angela Dai, ECCV. 2022</p>
<p>Mask3d: Mask transformer for 3d semantic instance segmentation. Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, Bastian Leibe, ICRA. 2023</p>
<p>Alfred: A benchmark for interpreting grounded instructions for everyday tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, IEEE/CVF CVPR. 2020</p>
<p>A survey of object goal navigation. Jingwen Sun, Jing Wu, Ze Ji, Yu-Kun Lai, IEEE Transactions on Automation Science and Engineering. 2024. 2024</p>
<p>Ayça Takmaz, Elisabetta Fedele, Robert W Sumner, Marc Pollefeys, Federico Tombari, Francis Engelmann, arXiv:2306.13631Openmask3d: Open-vocabulary 3d instance segmentation. 2023. 2023arXiv preprint</p>
<p>Bundle adjustment-a modern synthesis. Bill Triggs, Richard I Philip F Mclauchlan, Andrew W Hartley, Fitzgibbon, Vision Algorithms: Theory and Practice: International Workshop on Vision Algorithms Corfu. Greece2000. September 21-22. 1999</p>
<p>Find what you want: Learning demand-conditioned object attribute space for demand-driven navigation. Hongcheng Wang, Andy Guan, Hong Chen, Xiaoqi Li, Mingdong Wu, Hao Dong, Advances in Neural Information Processing Systems. 2023. 2023</p>
<p>Gridmm: Grid memory map for vision-and-language navigation. Zihan Wang, Xiangyang Li, Jiahao Yang, Yeqi Liu, Shuqiang Jiang, Proceedings of the IEEE/CVF International conference on computer vision. the IEEE/CVF International conference on computer vision2023</p>
<p>Hierarchical open-vocabulary 3d scene graphs for language-grounded robot navigation. Abdelrhman Werby, Chenguang Huang, Martin Büchner, Abhinav Valada, Wolfram Burgard, First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024. 2024</p>
<p>Towards open vocabulary learning: A survey. Jianzong Wu, Xiangtai Li, Shilin Xu, Haobo Yuan, Henghui Ding, Yibo Yang, Xia Li, Jiangning Zhang, Yunhai Tong, Xudong Jiang, IEEE TPAMI. 2024. 2024</p>
<p>Visionlanguage navigation: a survey and taxonomy. Wansen Wu, Tao Chang, Xinmeng Li, Quanjun Yin, Yue Hu, 2024. 2024Neural Computing and Applications</p>
<p>Edge assisted mobile semantic visual SLAM. Jingao Xu, Hao Cao, Danyang Li, Kehong Huang, Chen Qian, Longfei Shangguan, Zheng Yang, IEEE INFOCOM 2020-IEEE Conference on computer communications. 2020</p>
<p>{SwarmMap}: Scaling up real-time collaborative visual {SLAM} at the edge. Jingao Xu, Hao Cao, Zheng Yang, Longfei Shangguan, Jialin Zhang, Xiaowu He, Yunhao Liu, 19th USENIX Symposium on Networked Systems Design and Implementation. 202222</p>
<p>Habitat-matterport 3d semantics dataset. Karmesh Yadav, Ram Ramrakhya, Santhosh Kumar Ramakrishnan, Theo Gervet, John Turner, Aaron Gokaslan, Noah Maestre, Xuan Angel, Dhruv Chang, Manolis Batra, Savva, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Maskclustering: View consensus based mask graph clustering for open-vocabulary 3d instance segmentation. Mi Yan, Jiazhao Zhang, Yan Zhu, He Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Open-vocabulary SAM: Segment and recognize twenty-thousand classes interactively. Haobo Yuan, Xiangtai Li, Chong Zhou, Yining Li, Kai Chen, Chen Change Loy, European Conference on Computer Vision. 2024</p>
<p>Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively. Haobo Yuan, Xiangtai Li, Chong Zhou, Yining Li, Kai Chen, Chen Change Loy, ECCV. 2024</p>
<p>Vision-language models for vision tasks: A survey. Jingyi Zhang, Jiaxing Huang, Sheng Jin, Shijian Lu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2024. 2024</p>
<p>A survey on open-vocabulary detection and segmentation: Past, present, and future. Chaoyang Zhu, Long Chen, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2024. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
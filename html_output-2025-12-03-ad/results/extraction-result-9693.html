<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9693 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9693</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9693</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-279261363</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.08235v1.pdf" target="_blank">Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\rightarrow$ Evidence Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) are increasingly being used for complex research tasks such as literature review, idea generation, and scientific paper analysis, yet their ability to truly understand and process the intricate relationships within complex research papers, such as the logical links between claims and supporting evidence remains largely unexplored. In this study, we present CLAIM-BENCH, a comprehensive benchmark for evaluating LLMs' capabilities in scientific claim-evidence extraction and validation, a task that reflects deeper comprehension of scientific argumentation. We systematically compare three approaches which are inspired by divide and conquer approaches, across six diverse LLMs, highlighting model-specific strengths and weaknesses in scientific comprehension. Through evaluation involving over 300 claim-evidence pairs across multiple research domains, we reveal significant limitations in LLMs' ability to process complex scientific content. Our results demonstrate that closed-source models like GPT-4 and Claude consistently outperform open-source counterparts in precision and recall across claim-evidence identification tasks. Furthermore, strategically designed three-pass and one-by-one prompting approaches significantly improve LLMs' abilities to accurately link dispersed evidence with claims, although this comes at increased computational cost. CLAIM-BENCH sets a new standard for evaluating scientific comprehension in LLMs, offering both a diagnostic tool and a path forward for building systems capable of deeper, more reliable reasoning across full-length papers.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9693.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9693.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLAIM-BENCH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLAIM-BENCH: Benchmark for Claim→Evidence Reasoning in Scientific Papers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset and evaluation benchmark designed to measure LLMs' ability to identify and validate claim-evidence relationships across full-length scientific papers, with curated annotations, prompting protocols, and novel metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI / Machine Learning (scientific text understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated LLM runs on full papers using structured prompting strategies (Single-Pass, Three-Pass, One-by-One) compared against human-annotated ground truth; analysis of retrieval matches and linking behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Precision, Recall, F1-score (span-level), sentence_gap (average absolute sentence distance between predicted claim and evidence spans), execution time, and recall as a function of input token length.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>CLAIM-BENCH dataset: papers curated by 4 PhD annotators (100 papers), validation subset of 30 re-annotated papers; annotations exportable as claim→evidence span links (one-to-many). Inter-annotator F1: claims 0.755, evidence 0.659, links 0.617; Cohen's κ: claims 0.66, evidence 0.30.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Benchmark demonstrates significant LLM limitations: closed-source models (GPT-4-Turbo, Claude 3.5) generally achieve higher recall and better precision-recall balance than many open-source models; iterative prompting (Three-Pass, One-by-One) increases recall and coverage compared to Single-Pass but at higher computational cost; evidence extraction yields higher recall but lower precision than claim extraction; overall F1 results typically below 0.7.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Dataset restricted to recent (2024) non-math AI/ML papers (≤20 pages), inter-annotator agreement for evidence is low (κ=0.30) reflecting annotation difficulty, iterative strategies are computationally expensive, and metrics rely on span-level matching which can penalize boundary choices.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Human annotations show shorter claim→evidence linking distances; LLMs (especially with iterative prompts) link over much longer sentence distances than humans, indicating broader retrieval but increased false positives; inter-annotator statistics used as human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use iterative/divide-and-conquer prompting for long documents; measure sentence_gap to quantify long-range reasoning; report both precision and recall (and F1) and execution time; validate with multiple annotators; treat evidence extraction as a harder task than claim detection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9693.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9693.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single-Pass</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single-Pass Prompting Strategy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method where the full paper is provided once with a single comprehensive instruction to extract all claims, evidences, and conclusions in one run.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI / Machine Learning (LLM evaluation workflow)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Single comprehensive prompt given over the full paper; model outputs claim/evidence spans compared to annotated ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Span-level Precision, Recall, F1; sentence_gap; execution time; recall degradation with document token length.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Evaluated on the CLAIM-BENCH dataset of curated papers.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Most efficient computationally (fastest), but lowest coverage/recall on longer documents; recall declines notably as document token length increases (amplified for smaller models). Example: GPT-4 produced 152 pairs with average sentence_gap ∼98.5 under Single-Pass; smaller models exhibit steeper recall drops.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Amplifies context-window limits for smaller models, lower coverage on long documents, misses dispersed evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Faster than iterative human-like workflows but less comprehensive; iterative human review corresponds more to Three-Pass/One-by-One approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use for quick assessments or where compute/time is constrained; switch to iterative approaches for thorough claim-evidence retrieval on long documents.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9693.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9693.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Three-Pass</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Three-Pass (Divide-and-Conquer) Prompting Strategy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential prompting approach where (1) claims are identified, (2) those claims are supplied to retrieve evidence, and (3) claims and evidences are combined to extract conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI / Machine Learning (LLM evaluation workflow)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Sequential prompts each focused on a subtask (claims → evidence → conclusions) with intermediate outputs feeding the next stage; compared to human-annotated ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Precision, Recall, F1, sentence_gap, execution time, recall vs token length.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to CLAIM-BENCH papers.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Improves recall and coverage relative to Single-Pass with moderate computational cost; reduces the effect of long-document token-length on recall for many models; closed-source models benefit strongly from Three-Pass (e.g., Claude recall rises for larger docs).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Moderate increase in execution time versus Single-Pass; can still produce noisy long-range links for some models (high sentence_gap variance).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>More closely mimics human staged review of claims then evidence; recovers human-like coverage for long documents.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Preferred when balancing recall and compute; useful to mitigate context length issues for mid-size models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9693.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9693.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>One-by-One</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>One-by-One Claim-Level Prompting Strategy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-grained method that processes each claim individually—running separate retrieval/prompts per claim to find corresponding evidence and conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI / Machine Learning (LLM evaluation workflow)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>For each identified claim, run an independent evidence-retrieval prompt; aggregate results and compare to annotated ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Precision, Recall, F1, sentence_gap, execution time, coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to CLAIM-BENCH papers.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Maximizes recall and pair counts (e.g., Claude and LLaMA produced up to 639 and 659 pairs respectively) and reduces length-related performance drops, but at high computational cost and with large sentence_gap variance for some models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Highest execution time and resource use (many runs per paper); can produce very long-range links that increase false positives; extreme variance in sentence_gap for some models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Most similar to careful human claim-by-claim verification, but far more compute-intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use when maximal coverage is required and compute/time is available; combine with post-filtering to reduce spurious long-range links.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9693.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9693.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation Metrics (Precision, Recall, F1, sentence_gap, Time)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of quantitative metrics used to measure span-level extraction accuracy, long-range linking ability, and operational efficiency for claim-evidence reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI / Machine Learning (evaluation methodology)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute standard IR metrics (precision, recall, F1) on predicted spans vs annotated spans; compute sentence_gap as average absolute sentence index distance for matched claim-evidence pairs; record execution time and recall vs token length.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Precision = TP/(TP+FP); Recall = TP/(TP+FN); F1 = harmonic mean; sentence_gap = average |sentence_index(claim) - sentence_index(evidence)| over matched pairs; secondary metrics: generation time and recall as function of document length.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Used for CLAIM-BENCH evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Precision higher for claim extraction than evidence extraction across models; evidence extraction yields higher recall but lower precision; sentence_gap captures long-range linking and shows LLMs often link across much greater distances than humans; most model-strategy combos produce F1 < 0.7.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Span-boundary differences penalize metrics; sentence_gap can be inflated by noisy long-range matches; evidence sparsity reduces κ and complicates metric interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Metrics aligned with standard IR/IE evaluation; sentence_gap introduced to quantify long-range reasoning—an aspect not commonly captured in traditional evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Report both precision and recall, include sentence_gap to quantify long-range reasoning and noise, and show execution time and recall vs token-length curves.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9693.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9693.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source large language model (OpenAI) evaluated for claim→evidence extraction on CLAIM-BENCH, showing strong precision-recall balance among tested models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Closed-source model from OpenAI; reported in paper as a larger model with ≥128K-token context window (as used in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI / Machine Learning (scientific text comprehension)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Run on full papers using Single-Pass, Three-Pass, and One-by-One prompting; compare extracted spans to CLAIM-BENCH annotations with IR metrics and sentence_gap.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Precision, Recall, F1, sentence_gap, execution time, recall vs token length.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>CLAIM-BENCH (100 papers with 30-paper validation subset).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reported as achieving high precision (≈0.68) and high recall (≈0.81) for claim identification in best settings; balances precision and recall well; moderate linking distances (examples report average sentence_gap ≈98.5 in one Single-Pass example and broader distances ≈658–708 mentioned as typical moderate values). Execution time: efficient (Single-Pass under 200s; One-by-One ~500s).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Slight recall limitations compared to highest-recall models; some trade-off between precision and coverage depending on prompting strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Better precision-recall balance than many open-source models; links fewer spurious long-range evidence pairs than some high-recall models.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use iterative prompting to maximize recall while leveraging the model's precision strengths; monitor sentence_gap to detect excessively long-range (potentially spurious) links.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9693.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9693.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source LLM (Anthropic) evaluated on CLAIM-BENCH; shows very high recall but sometimes lower precision and large variance in linking distances.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Closed-source model from Anthropic used with large context window (≥128K tokens) in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI / Machine Learning (scientific text comprehension)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluated across Single-Pass, Three-Pass, One-by-One strategies on CLAIM-BENCH; metrics include precision, recall, F1, sentence_gap, execution time.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Precision, Recall, F1, sentence_gap, execution time, recall-by-length buckets.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>CLAIM-BENCH</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>High recall (≈0.83 reported for claims in best cases) but moderate precision (≈0.61); yields very high pair counts in One-by-One (up to 639 pairs) and very large average and maximum sentence_gap values and high variance (mean gap ~119.4 sentences with variance ~33,674 in some settings); fastest execution times across strategies (often under 200s).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Tendency to over-generate evidence links leading to false positives and large long-range linking distances; substantial variance in sentence_gap indicating noisy linking behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Outperforms many open-source models on recall but links more distant evidence than human baseline, increasing risk of incorrect claim-evidence pairings.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Combine Claude outputs with stricter post-filtering or precision-focused follow-ups; use Three-Pass or One-by-One when coverage is priority but apply validation to reduce noise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9693.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9693.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-Exp_1114</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-Exp_1114</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source LLM (Google's Gemini experimental variant) evaluated for claim-evidence tasks; exhibits stable performance across prompting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Gemini-Exp_1114</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Closed-source experimental Gemini model used with large-context capability (≥128K tokens) in the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI / Machine Learning (scientific text comprehension)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluated with Single-Pass, Three-Pass, and One-by-One on CLAIM-BENCH; measured with standard IR metrics and sentence_gap.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Precision, Recall, F1, sentence_gap, execution time.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>CLAIM-BENCH</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Described as stable across strategies with moderate precision and recall (table lists intermediate performance: best F1 ≈0.54 for claims); execution times intermediate (Three-Pass ~600s in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Did not consistently match top closed-source models on precision/recall; some variability in execution time due to API latency and rate-limiting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Provides a balanced but not top-tier trade-off between precision and recall compared to GPT-4 and Claude.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use when stability across strategies is valued; consider combining with iterative prompting for long documents.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9693.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9693.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source LLM (70B parameters) evaluated on CLAIM-BENCH; achieves high recall in some settings but shows high variance and slower execution in one-by-one runs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>LLaMA-70B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Open-source 70B-parameter model used in the experiments; supports large context windows in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI / Machine Learning (scientific text comprehension)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluated across prompting strategies on CLAIM-BENCH; metrics include precision, recall, F1, sentence_gap, execution time.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Precision, Recall, F1, sentence_gap, execution time.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>CLAIM-BENCH</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>High recall comparable to closed-source models in best cases (recall ≈0.76 reported) with precision around 0.60; produced large pair counts (up to 659) and very high variance in sentence_gap (variance ~34,207 reported), and one-by-one execution times often >1,200s.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>High variance in linking distances and noisy long-range links; slower in one-by-one scenarios leading to large execution times.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Matches closed-source recall in some settings but with less consistent precision and more noisy long-range linking than human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Prefer iterative prompting to recover long-document performance; apply post-hoc filtering to reduce noisy long-distance matches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9693.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9693.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ministral-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ministral-8B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source smaller LLM (8B parameters) evaluated on CLAIM-BENCH; conservative in linking with shorter sentence gaps and moderate recall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Ministral-8B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Open-source 8B-parameter model (Mistral/Ministral) used in experiments; smaller model size compared to others.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI / Machine Learning (scientific text comprehension)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluated across prompting strategies on CLAIM-BENCH; metrics include Precision, Recall, F1, sentence_gap, execution time.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Precision, Recall, F1, sentence_gap, execution time.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>CLAIM-BENCH</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Moderate recall (≈0.60 best-case) and lower precision (≈0.38) for claim→evidence linking; tends to produce shorter linking distances (mean sentence_gap ~75.9) and low variance; execution times moderate (Three-Pass/One-by-One ~600–900s).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Lower overall accuracy compared to larger models; conservative linking reduces coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>More conservative than human baseline in linking across distant evidence; less coverage but fewer spurious long-range links.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Good choice when minimizing noisy long-range links is desired; consider Three-Pass to maintain stable recall across document lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9693.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e9693.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Phi-3.5-MoE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phi-3.5-MoE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source mixture-of-experts model evaluated on CLAIM-BENCH; exhibits wide swings in performance with high variance in some metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Phi-3.5-MoE</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Open-source MoE variant reported in the experiments, supporting large contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI / Machine Learning (scientific text comprehension)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluated on CLAIM-BENCH across Single-Pass, Three-Pass, One-by-One; metrics include precision, recall, F1, sentence_gap, execution time.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Precision, Recall, F1, sentence_gap, execution time.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>CLAIM-BENCH</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Performance exhibits wide swings: precision around 0.39–0.40 in best tables, recall sometimes high (≈0.70 in best cases); Three-Pass and One-by-One yielded high pair counts (e.g., 279 pairs in Three-Pass) but with very large sentence_gap variance (e.g., variance reported up to 11,490.2 in one configuration); Phi shows highest computational intensity in one-by-one (>1200s).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Inconsistent performance and very high variance in linking distances; heavy computational cost for fine-grained prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Unstable compared to human baselines and other large closed-source LLMs; sometimes matches recall but with high noise.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use caution; apply robust post-filtering and prioritize iterative prompting to stabilise recall across document sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context <em>(Rating: 2)</em></li>
                <li>LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs <em>(Rating: 2)</em></li>
                <li>XL$^2$Bench: A Benchmark for Extremely Long Context Understanding with Long-range Dependencies <em>(Rating: 2)</em></li>
                <li>SCBENCH: A KV Cache-Centric Analysis of Long-Context Methods <em>(Rating: 2)</em></li>
                <li>ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery <em>(Rating: 2)</em></li>
                <li>LitLLM: A Toolkit for Scientific Literature Review <em>(Rating: 1)</em></li>
                <li>ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9693",
    "paper_id": "paper-279261363",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "CLAIM-BENCH",
            "name_full": "CLAIM-BENCH: Benchmark for Claim→Evidence Reasoning in Scientific Papers",
            "brief_description": "A dataset and evaluation benchmark designed to measure LLMs' ability to identify and validate claim-evidence relationships across full-length scientific papers, with curated annotations, prompting protocols, and novel metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "AI / Machine Learning (scientific text understanding)",
            "evaluation_method": "Automated LLM runs on full papers using structured prompting strategies (Single-Pass, Three-Pass, One-by-One) compared against human-annotated ground truth; analysis of retrieval matches and linking behavior.",
            "evaluation_criteria": "Precision, Recall, F1-score (span-level), sentence_gap (average absolute sentence distance between predicted claim and evidence spans), execution time, and recall as a function of input token length.",
            "benchmark_or_dataset": "CLAIM-BENCH dataset: papers curated by 4 PhD annotators (100 papers), validation subset of 30 re-annotated papers; annotations exportable as claim→evidence span links (one-to-many). Inter-annotator F1: claims 0.755, evidence 0.659, links 0.617; Cohen's κ: claims 0.66, evidence 0.30.",
            "results_summary": "Benchmark demonstrates significant LLM limitations: closed-source models (GPT-4-Turbo, Claude 3.5) generally achieve higher recall and better precision-recall balance than many open-source models; iterative prompting (Three-Pass, One-by-One) increases recall and coverage compared to Single-Pass but at higher computational cost; evidence extraction yields higher recall but lower precision than claim extraction; overall F1 results typically below 0.7.",
            "limitations_or_challenges": "Dataset restricted to recent (2024) non-math AI/ML papers (≤20 pages), inter-annotator agreement for evidence is low (κ=0.30) reflecting annotation difficulty, iterative strategies are computationally expensive, and metrics rely on span-level matching which can penalize boundary choices.",
            "comparison_to_human_or_traditional": "Human annotations show shorter claim→evidence linking distances; LLMs (especially with iterative prompts) link over much longer sentence distances than humans, indicating broader retrieval but increased false positives; inter-annotator statistics used as human baseline.",
            "recommendations_or_best_practices": "Use iterative/divide-and-conquer prompting for long documents; measure sentence_gap to quantify long-range reasoning; report both precision and recall (and F1) and execution time; validate with multiple annotators; treat evidence extraction as a harder task than claim detection.",
            "uuid": "e9693.0",
            "source_info": {
                "paper_title": "Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Single-Pass",
            "name_full": "Single-Pass Prompting Strategy",
            "brief_description": "A prompting method where the full paper is provided once with a single comprehensive instruction to extract all claims, evidences, and conclusions in one run.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "AI / Machine Learning (LLM evaluation workflow)",
            "evaluation_method": "Single comprehensive prompt given over the full paper; model outputs claim/evidence spans compared to annotated ground truth.",
            "evaluation_criteria": "Span-level Precision, Recall, F1; sentence_gap; execution time; recall degradation with document token length.",
            "benchmark_or_dataset": "Evaluated on the CLAIM-BENCH dataset of curated papers.",
            "results_summary": "Most efficient computationally (fastest), but lowest coverage/recall on longer documents; recall declines notably as document token length increases (amplified for smaller models). Example: GPT-4 produced 152 pairs with average sentence_gap ∼98.5 under Single-Pass; smaller models exhibit steeper recall drops.",
            "limitations_or_challenges": "Amplifies context-window limits for smaller models, lower coverage on long documents, misses dispersed evidence.",
            "comparison_to_human_or_traditional": "Faster than iterative human-like workflows but less comprehensive; iterative human review corresponds more to Three-Pass/One-by-One approaches.",
            "recommendations_or_best_practices": "Use for quick assessments or where compute/time is constrained; switch to iterative approaches for thorough claim-evidence retrieval on long documents.",
            "uuid": "e9693.1",
            "source_info": {
                "paper_title": "Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Three-Pass",
            "name_full": "Three-Pass (Divide-and-Conquer) Prompting Strategy",
            "brief_description": "A sequential prompting approach where (1) claims are identified, (2) those claims are supplied to retrieve evidence, and (3) claims and evidences are combined to extract conclusions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "AI / Machine Learning (LLM evaluation workflow)",
            "evaluation_method": "Sequential prompts each focused on a subtask (claims → evidence → conclusions) with intermediate outputs feeding the next stage; compared to human-annotated ground truth.",
            "evaluation_criteria": "Precision, Recall, F1, sentence_gap, execution time, recall vs token length.",
            "benchmark_or_dataset": "Applied to CLAIM-BENCH papers.",
            "results_summary": "Improves recall and coverage relative to Single-Pass with moderate computational cost; reduces the effect of long-document token-length on recall for many models; closed-source models benefit strongly from Three-Pass (e.g., Claude recall rises for larger docs).",
            "limitations_or_challenges": "Moderate increase in execution time versus Single-Pass; can still produce noisy long-range links for some models (high sentence_gap variance).",
            "comparison_to_human_or_traditional": "More closely mimics human staged review of claims then evidence; recovers human-like coverage for long documents.",
            "recommendations_or_best_practices": "Preferred when balancing recall and compute; useful to mitigate context length issues for mid-size models.",
            "uuid": "e9693.2",
            "source_info": {
                "paper_title": "Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "One-by-One",
            "name_full": "One-by-One Claim-Level Prompting Strategy",
            "brief_description": "A fine-grained method that processes each claim individually—running separate retrieval/prompts per claim to find corresponding evidence and conclusions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "AI / Machine Learning (LLM evaluation workflow)",
            "evaluation_method": "For each identified claim, run an independent evidence-retrieval prompt; aggregate results and compare to annotated ground truth.",
            "evaluation_criteria": "Precision, Recall, F1, sentence_gap, execution time, coverage.",
            "benchmark_or_dataset": "Applied to CLAIM-BENCH papers.",
            "results_summary": "Maximizes recall and pair counts (e.g., Claude and LLaMA produced up to 639 and 659 pairs respectively) and reduces length-related performance drops, but at high computational cost and with large sentence_gap variance for some models.",
            "limitations_or_challenges": "Highest execution time and resource use (many runs per paper); can produce very long-range links that increase false positives; extreme variance in sentence_gap for some models.",
            "comparison_to_human_or_traditional": "Most similar to careful human claim-by-claim verification, but far more compute-intensive.",
            "recommendations_or_best_practices": "Use when maximal coverage is required and compute/time is available; combine with post-filtering to reduce spurious long-range links.",
            "uuid": "e9693.3",
            "source_info": {
                "paper_title": "Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Metrics",
            "name_full": "Evaluation Metrics (Precision, Recall, F1, sentence_gap, Time)",
            "brief_description": "Set of quantitative metrics used to measure span-level extraction accuracy, long-range linking ability, and operational efficiency for claim-evidence reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "AI / Machine Learning (evaluation methodology)",
            "evaluation_method": "Compute standard IR metrics (precision, recall, F1) on predicted spans vs annotated spans; compute sentence_gap as average absolute sentence index distance for matched claim-evidence pairs; record execution time and recall vs token length.",
            "evaluation_criteria": "Precision = TP/(TP+FP); Recall = TP/(TP+FN); F1 = harmonic mean; sentence_gap = average |sentence_index(claim) - sentence_index(evidence)| over matched pairs; secondary metrics: generation time and recall as function of document length.",
            "benchmark_or_dataset": "Used for CLAIM-BENCH evaluations.",
            "results_summary": "Precision higher for claim extraction than evidence extraction across models; evidence extraction yields higher recall but lower precision; sentence_gap captures long-range linking and shows LLMs often link across much greater distances than humans; most model-strategy combos produce F1 &lt; 0.7.",
            "limitations_or_challenges": "Span-boundary differences penalize metrics; sentence_gap can be inflated by noisy long-range matches; evidence sparsity reduces κ and complicates metric interpretation.",
            "comparison_to_human_or_traditional": "Metrics aligned with standard IR/IE evaluation; sentence_gap introduced to quantify long-range reasoning—an aspect not commonly captured in traditional evaluations.",
            "recommendations_or_best_practices": "Report both precision and recall, include sentence_gap to quantify long-range reasoning and noise, and show execution time and recall vs token-length curves.",
            "uuid": "e9693.4",
            "source_info": {
                "paper_title": "Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GPT-4-Turbo",
            "name_full": "GPT-4-Turbo",
            "brief_description": "Closed-source large language model (OpenAI) evaluated for claim→evidence extraction on CLAIM-BENCH, showing strong precision-recall balance among tested models.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "GPT-4-Turbo",
            "llm_description": "Closed-source model from OpenAI; reported in paper as a larger model with ≥128K-token context window (as used in experiments).",
            "scientific_domain": "AI / Machine Learning (scientific text comprehension)",
            "evaluation_method": "Run on full papers using Single-Pass, Three-Pass, and One-by-One prompting; compare extracted spans to CLAIM-BENCH annotations with IR metrics and sentence_gap.",
            "evaluation_criteria": "Precision, Recall, F1, sentence_gap, execution time, recall vs token length.",
            "benchmark_or_dataset": "CLAIM-BENCH (100 papers with 30-paper validation subset).",
            "results_summary": "Reported as achieving high precision (≈0.68) and high recall (≈0.81) for claim identification in best settings; balances precision and recall well; moderate linking distances (examples report average sentence_gap ≈98.5 in one Single-Pass example and broader distances ≈658–708 mentioned as typical moderate values). Execution time: efficient (Single-Pass under 200s; One-by-One ~500s).",
            "limitations_or_challenges": "Slight recall limitations compared to highest-recall models; some trade-off between precision and coverage depending on prompting strategy.",
            "comparison_to_human_or_traditional": "Better precision-recall balance than many open-source models; links fewer spurious long-range evidence pairs than some high-recall models.",
            "recommendations_or_best_practices": "Use iterative prompting to maximize recall while leveraging the model's precision strengths; monitor sentence_gap to detect excessively long-range (potentially spurious) links.",
            "uuid": "e9693.5",
            "source_info": {
                "paper_title": "Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Claude 3.5 Sonnet",
            "name_full": "Claude 3.5 Sonnet",
            "brief_description": "Closed-source LLM (Anthropic) evaluated on CLAIM-BENCH; shows very high recall but sometimes lower precision and large variance in linking distances.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "Claude 3.5 Sonnet",
            "llm_description": "Closed-source model from Anthropic used with large context window (≥128K tokens) in experiments.",
            "scientific_domain": "AI / Machine Learning (scientific text comprehension)",
            "evaluation_method": "Evaluated across Single-Pass, Three-Pass, One-by-One strategies on CLAIM-BENCH; metrics include precision, recall, F1, sentence_gap, execution time.",
            "evaluation_criteria": "Precision, Recall, F1, sentence_gap, execution time, recall-by-length buckets.",
            "benchmark_or_dataset": "CLAIM-BENCH",
            "results_summary": "High recall (≈0.83 reported for claims in best cases) but moderate precision (≈0.61); yields very high pair counts in One-by-One (up to 639 pairs) and very large average and maximum sentence_gap values and high variance (mean gap ~119.4 sentences with variance ~33,674 in some settings); fastest execution times across strategies (often under 200s).",
            "limitations_or_challenges": "Tendency to over-generate evidence links leading to false positives and large long-range linking distances; substantial variance in sentence_gap indicating noisy linking behavior.",
            "comparison_to_human_or_traditional": "Outperforms many open-source models on recall but links more distant evidence than human baseline, increasing risk of incorrect claim-evidence pairings.",
            "recommendations_or_best_practices": "Combine Claude outputs with stricter post-filtering or precision-focused follow-ups; use Three-Pass or One-by-One when coverage is priority but apply validation to reduce noise.",
            "uuid": "e9693.6",
            "source_info": {
                "paper_title": "Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Gemini-Exp_1114",
            "name_full": "Gemini-Exp_1114",
            "brief_description": "Closed-source LLM (Google's Gemini experimental variant) evaluated for claim-evidence tasks; exhibits stable performance across prompting strategies.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "Gemini-Exp_1114",
            "llm_description": "Closed-source experimental Gemini model used with large-context capability (≥128K tokens) in the benchmark.",
            "scientific_domain": "AI / Machine Learning (scientific text comprehension)",
            "evaluation_method": "Evaluated with Single-Pass, Three-Pass, and One-by-One on CLAIM-BENCH; measured with standard IR metrics and sentence_gap.",
            "evaluation_criteria": "Precision, Recall, F1, sentence_gap, execution time.",
            "benchmark_or_dataset": "CLAIM-BENCH",
            "results_summary": "Described as stable across strategies with moderate precision and recall (table lists intermediate performance: best F1 ≈0.54 for claims); execution times intermediate (Three-Pass ~600s in experiments).",
            "limitations_or_challenges": "Did not consistently match top closed-source models on precision/recall; some variability in execution time due to API latency and rate-limiting.",
            "comparison_to_human_or_traditional": "Provides a balanced but not top-tier trade-off between precision and recall compared to GPT-4 and Claude.",
            "recommendations_or_best_practices": "Use when stability across strategies is valued; consider combining with iterative prompting for long documents.",
            "uuid": "e9693.7",
            "source_info": {
                "paper_title": "Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "LLaMA-70B",
            "name_full": "LLaMA-70B",
            "brief_description": "Open-source LLM (70B parameters) evaluated on CLAIM-BENCH; achieves high recall in some settings but shows high variance and slower execution in one-by-one runs.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "LLaMA-70B",
            "llm_description": "Open-source 70B-parameter model used in the experiments; supports large context windows in the study.",
            "scientific_domain": "AI / Machine Learning (scientific text comprehension)",
            "evaluation_method": "Evaluated across prompting strategies on CLAIM-BENCH; metrics include precision, recall, F1, sentence_gap, execution time.",
            "evaluation_criteria": "Precision, Recall, F1, sentence_gap, execution time.",
            "benchmark_or_dataset": "CLAIM-BENCH",
            "results_summary": "High recall comparable to closed-source models in best cases (recall ≈0.76 reported) with precision around 0.60; produced large pair counts (up to 659) and very high variance in sentence_gap (variance ~34,207 reported), and one-by-one execution times often &gt;1,200s.",
            "limitations_or_challenges": "High variance in linking distances and noisy long-range links; slower in one-by-one scenarios leading to large execution times.",
            "comparison_to_human_or_traditional": "Matches closed-source recall in some settings but with less consistent precision and more noisy long-range linking than human annotations.",
            "recommendations_or_best_practices": "Prefer iterative prompting to recover long-document performance; apply post-hoc filtering to reduce noisy long-distance matches.",
            "uuid": "e9693.8",
            "source_info": {
                "paper_title": "Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Ministral-8B",
            "name_full": "Ministral-8B",
            "brief_description": "Open-source smaller LLM (8B parameters) evaluated on CLAIM-BENCH; conservative in linking with shorter sentence gaps and moderate recall.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "Ministral-8B",
            "llm_description": "Open-source 8B-parameter model (Mistral/Ministral) used in experiments; smaller model size compared to others.",
            "scientific_domain": "AI / Machine Learning (scientific text comprehension)",
            "evaluation_method": "Evaluated across prompting strategies on CLAIM-BENCH; metrics include Precision, Recall, F1, sentence_gap, execution time.",
            "evaluation_criteria": "Precision, Recall, F1, sentence_gap, execution time.",
            "benchmark_or_dataset": "CLAIM-BENCH",
            "results_summary": "Moderate recall (≈0.60 best-case) and lower precision (≈0.38) for claim→evidence linking; tends to produce shorter linking distances (mean sentence_gap ~75.9) and low variance; execution times moderate (Three-Pass/One-by-One ~600–900s).",
            "limitations_or_challenges": "Lower overall accuracy compared to larger models; conservative linking reduces coverage.",
            "comparison_to_human_or_traditional": "More conservative than human baseline in linking across distant evidence; less coverage but fewer spurious long-range links.",
            "recommendations_or_best_practices": "Good choice when minimizing noisy long-range links is desired; consider Three-Pass to maintain stable recall across document lengths.",
            "uuid": "e9693.9",
            "source_info": {
                "paper_title": "Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Phi-3.5-MoE",
            "name_full": "Phi-3.5-MoE",
            "brief_description": "Open-source mixture-of-experts model evaluated on CLAIM-BENCH; exhibits wide swings in performance with high variance in some metrics.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "Phi-3.5-MoE",
            "llm_description": "Open-source MoE variant reported in the experiments, supporting large contexts.",
            "scientific_domain": "AI / Machine Learning (scientific text comprehension)",
            "evaluation_method": "Evaluated on CLAIM-BENCH across Single-Pass, Three-Pass, One-by-One; metrics include precision, recall, F1, sentence_gap, execution time.",
            "evaluation_criteria": "Precision, Recall, F1, sentence_gap, execution time.",
            "benchmark_or_dataset": "CLAIM-BENCH",
            "results_summary": "Performance exhibits wide swings: precision around 0.39–0.40 in best tables, recall sometimes high (≈0.70 in best cases); Three-Pass and One-by-One yielded high pair counts (e.g., 279 pairs in Three-Pass) but with very large sentence_gap variance (e.g., variance reported up to 11,490.2 in one configuration); Phi shows highest computational intensity in one-by-one (&gt;1200s).",
            "limitations_or_challenges": "Inconsistent performance and very high variance in linking distances; heavy computational cost for fine-grained prompting.",
            "comparison_to_human_or_traditional": "Unstable compared to human baselines and other large closed-source LLMs; sometimes matches recall but with high noise.",
            "recommendations_or_best_practices": "Use caution; apply robust post-filtering and prioritize iterative prompting to stabilise recall across document sizes.",
            "uuid": "e9693.10",
            "source_info": {
                "paper_title": "Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context",
            "rating": 2,
            "sanitized_title": "what_external_knowledge_is_preferred_by_llms_characterizing_and_exploring_chain_of_evidence_in_imperfect_context"
        },
        {
            "paper_title": "LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs",
            "rating": 2,
            "sanitized_title": "longgenbench_benchmarking_longform_generation_in_long_context_llms"
        },
        {
            "paper_title": "XL$^2$Bench: A Benchmark for Extremely Long Context Understanding with Long-range Dependencies",
            "rating": 2,
            "sanitized_title": "xl2bench_a_benchmark_for_extremely_long_context_understanding_with_longrange_dependencies"
        },
        {
            "paper_title": "SCBENCH: A KV Cache-Centric Analysis of Long-Context Methods",
            "rating": 2,
            "sanitized_title": "scbench_a_kv_cachecentric_analysis_of_longcontext_methods"
        },
        {
            "paper_title": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery",
            "rating": 2,
            "sanitized_title": "scienceagentbench_toward_rigorous_assessment_of_language_agents_for_datadriven_scientific_discovery"
        },
        {
            "paper_title": "LitLLM: A Toolkit for Scientific Literature Review",
            "rating": 1,
            "sanitized_title": "litllm_a_toolkit_for_scientific_literature_review"
        },
        {
            "paper_title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
            "rating": 1,
            "sanitized_title": "chatcite_llm_agent_with_human_workflow_guidance_for_comparative_literature_summary"
        }
    ],
    "cost": 0.01661825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can AI Validate Science ? Benchmarking LLMs for Accurate Scientific Claim → Evidence Reasoning
9 Jun 2025</p>
<p>Shashidhar Reddy 
Stevens Institute of Technology</p>
<p>Yupeng Cao 
Stevens Institute of Technology</p>
<p>Haohang Li 
Stevens Institute of Technology</p>
<p>Yangyang Yu 
Stevens Institute of Technology</p>
<p>Nikhil Muralidhar 
Stevens Institute of Technology</p>
<p>Zining Zhu 
Stevens Institute of Technology</p>
<p>Can AI Validate Science ? Benchmarking LLMs for Accurate Scientific Claim → Evidence Reasoning
9 Jun 2025453428E0969C20D474F8EB093AA523D3arXiv:2506.08235v1[cs.CL]
Large language models (LLMs) are increasingly being used for complex research tasks such as literature review, idea generation, and scientific paper analysis, yet their ability to truly understand and process the intricate relationships within complex research papers, such as the logical links between claims and supporting evidence remains largely unexplored.In this study, we present CLAIM-BENCH, a comprehensive benchmark for evaluating LLMs' capabilities in scientific claim-evidence extraction and validation, a task that reflects deeper comprehension of scientific argumentation.We systematically compare three approaches which are inspired by divide and conquer approaches, across six diverse LLMs, highlighting model-specific strengths and weaknesses in scientific comprehension.Through evaluation involving over 300 claim-evidence pairs across multiple research domains, we reveal significant limitations in LLMs' ability to process complex scientific content.Our results demonstrate that closed-source models like GPT-4 and Claude consistently outperform open-source counterparts in precision and recall across claim-evidence identification tasks.Furthermore, strategically designed three-pass and one-by-one prompting approaches significantly improve LLMs' abilities to accurately link dispersed evidence with claims, although this comes at increased computational cost.CLAIM-BENCH sets a new standard for evaluating scientific comprehension in LLMs, offering both a diagnostic tool and a path forward for building systems capable of deeper, more reliable reasoning across full-length papers. 1</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have become important tool in academic research, demonstrating impressive capabilities such as automating comprehensive literature reviews, facilitating innovative idea generation, and aiding experimental design.These advancements promise significant improvements in research productivity, creativity, and efficiency, fueling excitement about the transformative potential of AI-driven methodologies in science.However, as researchers increasingly assign critical tasks to these models-from content summarization and hypothesis generation to automated peer review (Checco et al., 2021;Agarwal et al., 2025;Lu et al., 2024)-a fundamental yet overlooked question emerges: how deeply do these models truly understand scientific knowledge beyond surface-level pattern recognition?Despite their widespread use and promising outcomes, there remains uncertainty about the depth and accuracy of their reasoning capabilities, particularly in complex scientific contexts.</p>
<p>Scientific papers are characterized by intricate relationships, primarily structured around claims supported by corresponding evidence.The ability to accurately identify and reason about these claimevidence pairs is essential for validating scientific findings and ensuring research integrity, making it a critical test of LLMs' comprehension depth.Unlike surface-level tasks such as summarization or question answering, claim-evidence identification requires global reasoning across paper sections, synthesis of dispersed information, and a nuanced understanding of logical dependencies.While existing works have assessed LLMs' capabilities in related research tasks such as summarization (Agarwal et al., 2025), literature synthesis (Lu et al., 2024), and hypothesis generation (Vladika and Matthes, 2023), none have explicitly benchmarked LLM performance on systematically extracting and validating claims with supporting evidence, leaving this area of scientific comprehension underexplored.</p>
<p>Despite the importance of accurately reasoning about claims and supporting evidence, no existing benchmarks explicitly assess LLM capabilities for this specific type of high-level scientific reasoning.Benchmarks such as LongGenBench (Wu et al., 2025) and XL2Bench (Ni et al., 2024) have highlighted persistent limitations in LLMs' abilities to process long-context inputs and maintain logical coherence.Similarly, peer review frameworks like MetaWriter (Sun et al., 2024b) and AGEN-TREVIEW (Jin et al., 2024) evaluate LLMs in automated review contexts but do not specifically test their capability to validate logical relationships such as claims and evidence, a task crucial for rigorous scientific evaluation.Findings from Chain of Evidence (CoE) frameworks (Chang et al., 2024) underscore the complexity of structured, multihop reasoning required to integrate and validate information dispersed across documents.All these works evaluate reasoning in the general domains, but the scientific reasoning capability, which imposes unique challenges, is not benchmarked.Within scientific reasoning, The AI Scientist (Lu et al., 2024), LitLLM (Agarwal et al., 2025), and ChatCite (Li et al., 2025) benchmark LLMs on tasks such as literature review and hypothesis generation, while ScienceAgentBench (Chen et al., 2025) and SCBENCH (LI et al., 2025) probe multi-step reasoning and long-context understanding.However, none of these frameworks explicitly measure the finer-grained ability to verify whether the evidence presented in a full scientific paper truly supports its claims-precisely the claim-and-evidence (C-E) reasoning capability our benchmark targets.</p>
<p>To address these gaps, we present CLAIM-BENCH, a novel benchmark designed to systematically evaluate LLMs' abilities to identify and validate claim-evidence relationships in scientific papers.CLAIM-BENCH challenges LLMs to process entire scientific papers, connect ideas across sections, and reason about them on a high level.In this work, we evaluate six state-of-the-art LLMs across diverse research domains.Our experiments indicate that larger models (e.g., GPT-4-Turbo, Claude 3.5) maintain high recall even with lengthy documents, especially when using iterative prompting, whereas smaller models (e.g., LLaMA, Ministral) experience significant performance drops with increasing document length specially under Single-Pass prompting.These findings highlight crucial areas for enhancing long-context comprehension and inform the development of reliable AI-driven tools for scientific research and peer review.</p>
<p>Related Work</p>
<p>AI for Science Large Language Models (LLMs) have significantly advanced scientific workflows, facilitating tasks such as peer review and hypothesis generation.Tools like ReviewerGPT (Liu and Shah, 2023) and ReviewFlow (Sun et al., 2024a) have streamlined peer review processes, while AGENTREVIEW (Jin et al., 2024) simulates collaborative review systems to improve research evaluation workflows.In parallel, fact-checking frameworks, such as Scientific Fact-Checking (Vladika and Matthes, 2023) and Exploring Multidimensional Checkworthiness (Liu et al., 2025), emphasize validating claims in scientific literature.However, these systems primarily focus on localized tasks or prioritization mechanisms, leaving the broader challenge of understanding the connections across entire documents by LLMs unaddressed.Additional work such as AI-assisted peer review (Checco et al., 2021) explores the feasibility of algorithmically approximating peer-review judgments, raising key ethical and practical concerns.</p>
<p>Benchmarks Long-context benchmarks, such as SCBENCH (LI et al., 2025), MMLongBench-Doc (Ma et al., 2024), and LongGenBench (Wu et al., 2025), have assessed LLMs' ability to process extended inputs and maintain coherence, focusing primarily on tasks like document summarization and long-form generation.Specialized benchmarks like U-MATH (Chernyshev et al., 2025) and Leave No Document Behind (Godbole et al., 2024) examine domain-specific reasoning and multi-document synthesis but address relatively structured and localized relationships.The LCFO benchmark (Costajussà et al., 2024a) targets summary expansion with varying granularities of content compression, revealing limits in semantic retention.The Y-NQ dataset (Costa-jussà et al., 2024b) exposes disparities in open-book comprehension across low-&amp; high-resource languages, hinting at deeper weaknesses in cross-lingual and low-resource longcontext understanding.Data Interpreter (Hong et al., 2024) showcases long-term data analysis workflows with LLM agents, but primarily focuses on task planning and execution rather than deep textual reasoning.In neuroscience, (Luo et al., 2025) show LLMs surpassing expert predictions in future experimental outcomes, yet such success doesn't imply comprehension of reasoning chains.In contrast, our work focuses specifically on research papers, which are characterized by more complex and dispersed relationships, such as claims supported by evidence across multiple sections.CLAIM-BENCH evaluates the ability of LLMs to synthesize these intricate connections, testing their capacity for global reasoning and coherence in a way that reflects the unique demands of scientific texts.</p>
<p>Collaborative Reasoning Collaborative reasoning frameworks offer a complementary perspective, with multi-agent systems like Two Heads Are Better Than One (Su et al., 2025) and iterative feedback mechanisms such as CYCLERESEARCHER (Weng et al., 2025) showing promise in enhancing reasoning capabilities.While these approaches address some limitations of Single-Pass LLM systems, their primary focus remains on generating and refining content rather than validating complex logical relationships.Similarly, tools like AIGS (Liu et al., 2024) and LLM-Assisted Hypothesis Generation (Vladika and Matthes, 2023) explore reasoning and hypothesis testing but do not directly tackle the problem of scientific comprehension.(Leng et al., 2024) introduce a graph-based approach for hypothesis generation and evaluation, demonstrating potential for structured creativity, yet falling short of validating interlinked arguments at scale.Ethical AI Finally, ethical considerations have been raised in works like Ethical Use of LLMs (Lissack and Meagher, 2024), which stresses the need for transparency and accountability in AIdriven research, and multimodal benchmarks like MileBench (Dingjie et al., 2024), which expand the scope of LLM evaluation to include visual and textual data.These efforts, while addressing important aspects of AI integration in research, highlight the absence of targeted benchmarks that evaluate claim-evidence validation across long, complex scientific texts-a gap CLAIM-BENCH aims to fill.</p>
<p>Methodology</p>
<p>In this section, we present the design of CLAIM-BENCH, our benchmark for evaluating how well LLMs identify and analyze claim-evidence relationships in full-length research papers.</p>
<p>Dataset</p>
<p>Dataset Curation The dataset for this study was curated by 4 PhD students with research experience.Each annotator had at least one first-author conference publication, ensuring familiarity with scientific writing standards.These researchers selected papers according to specific guidelines (Appendix B.1) to ensure relevance and diversity.Selection criteria included: papers from the year 2024, nonmath-intensive subjects, length between 0 to 20 pages.The aim was to represent a broad spectrum of current AI/ML research topics within the dataset.</p>
<p>To facilitate easier annotations, we developed a PDF annotation tool, it lets users load a paper, drag a pointer over any sentence or paragraph to mark it as a claim, then click-add evidence additional spans as linked evidence for that claim; each claim-evidence pair is stored in a one-to-many structure and exported as JSON.(see Appendix B.3).</p>
<p>Annotation Quality Check After compiling the initial annotations (100 papers), these were set aside before evaluating the models to ensure an unbiased assessment of their capabilities.To enhance the reliability of our dataset as ground truth, we conducted a validation phase where a different set of annotators re-annotated a subset of 30 papers.We measured inter-annotator agreement with two metrics.(1) F1: Averaging symmetric F 1 across annotator pairs gives substantial agreement for claims (0.755) and moderate agreement for evidence (0.659) and claim-evidence links (0.617).</p>
<p>(2) Cohen's κ: Averaging κ across pairs yields 0.66 for claims (substantial) and 0.30 for evidence (fair).Together, these scores confirm that CLAIM-BENCH is a reliable yet challenging benchmark (details in Appendix B.2).</p>
<p>Evaluation Metrics</p>
<p>In this study, we employ four metrics to evaluate the LLM performance: three established metrics in information retrieval, precision, recall, F1-score, and a novel metric, sentence_gap, to evaluate LLM performance in claim-evidence retrieval tasks and the effectiveness of our various prompting techniques.</p>
<p>Precision (P) Used to measure the proportion of spans the model predicts that are identified by the annotators, reflecting their effectiveness in responding to precise and carefully structured prompts.
P = TP TP + FP ,(1)
where TP (true positive) is the number of correctly retrieved claim/evidence, and FP (false positive) is the number of retrieved "claim"/"evidence" that are not claims/evidences.Recall (R) Quantifying the portion of claim/evidence that are retrieved.Recall assesses the ability to capture pertinent data, a measure of the model's responsiveness to exhaustive prompt inquiries
R = TP TP + FN ,(2)
where FN (false negative) is the number of claims/evidences that are incorrectly missed.</p>
<p>F1-score This is the harmonic mean of P and R. The F1-score provides a balanced measure of accuracy, crucial for evaluating the efficacy of the prompting techniques in eliciting detailed and relevant responses.</p>
<p>sentence_gap The sentence_gap metric measures the distance between a retrieved claim and each of its associated retrieved evidence.It is particularly valuable for evaluating long-range contextual comprehension by quantitatively assessing models' ability to handle textual relationships over extended contexts.This assessment is crucial for complex prompts designed to challenge such comprehension and is instrumental as we explore how increasing LLM context length capabilities enhance performance in realistic scenarios.
sentence_gap = 1 |M| (p,g)∈M s(p) − s(g) , (3)
where M is the set of matched evidence pairs (using Intersection over Union matching rule).s(•) returns the sentence index of a span inside the document.The sentence_gap metric is therefore the average absolute sentence-level distance between each predicted claim span p and its evidence span g, capturing how far a model must reason across the paper to link claims with supporting evidence.</p>
<p>Secondary metrics Additionally, we consider secondary metrics that focus on operational aspects of model performance: the time to generate outputs and how each model's recall changes as input length (token count) increases.These metrics are crucial for understanding efficiency and scalability.They help compare how models manage computational resources and handle large input sizes under varying conditions.Recall for claim (solid markers) and evidence (transparent markers) identification across models and strategies (shapes: Single-Pass •, Three-Pass ▲, One-by-One ■).Models show higher precision for claims, higher recall for evidence, with most results below F 1 = 0.7.
Claude (E) Gemini (E) Gemini (E) Gemini (E) GPT (E) GPT (E) GPT (E) LLaMA (E) LLaMA (E) LLaMA (E) Ministral (E) Ministral (E) Phi (E) Phi (E) Phi (E)</p>
<p>Claude</p>
<p>Experimental Setup</p>
<p>We evaluate six state-of-the-art LLMs, chosen to span both licensing regimes and architectural families while sharing a ≥128K-token context window.Open-source include Ministral-8B (Mistral AI, 2024), Phi-3.5-MoE (Abdin et al., 2024), and LLaMA-70B (Wang et al., 2025) and Closed-source includes GPT-4 (OpenAI, 2024), Gemini-Exp_1114 (Gemini Team, 2024), and Claude 3.5 Sonnet (Anthropic, 2025).</p>
<p>Analysis Methods</p>
<p>As illustrated in Figure 1, we explore three distinct prompting methods to assess and enhance model performance on claim-evidence identification tasks.</p>
<p>Single-Pass Initially, we present the models with a research paper, instructing (Appendix A.1) them to identify claims, evidences, and conclusions in a single comprehensive prompt.</p>
<p>Three-Pass Building on the "divide and conquer" strategy from prior research, we then deconstruct the task into sequential stages.In the first stage, the model identifies claims using a dedicated prompt.Subsequently, these claims are supplied to the next stage, where separate prompts elicit corresponding evidences.Finally, we combine the identified claims and evidences, using another prompt to extract conclusions (Appendix A.2).</p>
<p>One-by-One Pass</p>
<p>We adopt a more granular approach where each claim is processed individually to retrieve evidence.This means for n claims, the model runs n times to gather evidence for each, and similarly for conclusions.Although this approach provides detailed analysis, it significantly increases the demand on computational resources and time (Appendix A.3).These methods combine careful prompting with our annotated claim-evidence dataset, allowing us to benchmark each model's extraction accuracy and probe how different prompt strategies improve performance.</p>
<p>Results</p>
<p>The following section details the experimental results, highlighting comparative model performance and strategic impacts.</p>
<p>Precision vs Recall</p>
<p>As shown in Figure 2, models exhibit a clear precision-recall trade-off: settings that achieve higher recall often incur reduced precision.For instance, Claude and LLaMA achieve high recall but at the cost of extracting numerous false positives, which is evident from their large maximum linking distances (Figure 7), exceeding 2,200 sentences in some cases.Although valuable, such long-range links raise the risk of false claim-evidence pairs.Conversely, models like GPT prioritize precision, maintaining moderate linking distances (around 658-708 sentences) with fewer spurious matches, though this approach slightly limits recall.Ministral offers a balanced precision-recall profile, characterized by consistent, shorter linking distances.</p>
<p>Comparing the precision-recall tradeoff trends between open-and closed-source models, we see that closed-source models balance precision and recall better.Overall, GPT often balances high precision and moderate recall; Claude achieves higher recall rates but exhibits noticeable trade-offs in precision.Gemini remains stable across strategies.Among open-source models, LLaMA came close to matching closed-source recall but with some outliers, also shows variability in precision; Ministral is moderate in both coverage &amp; precision; Phi exhibits the widest swings, at times matching larger models but also dropping in accuracy.</p>
<p>Smaller vs Larger Models</p>
<p>Larger models, such as GPT-4-Turbo, Claude, Gemini, and LLaMA, generally exhibit strong recall in identifying claims, with GPT-4-Turbo achieving high precision (0.68) and recall (0.81), demonstrating effective balance at different strategies.Claude also shows strong recall (0.83), albeit with a moderate precision drop (0.61).Also, LLaMA achieves similar recall (0.76) but comparative precision (0.60), indicating a tendency to identify extensive and highly precise connections, considering the best cases of each model.</p>
<p>Smaller models, such as Ministral and Phi, typically exhibit lower recall and precision.Ministral shows modest recall (0.60) with precision around 0.38, reflecting a conservative approach to claimevidence linking.Phi demonstrates similar precision (approximately 0.39) but notably higher recall (around 0.7) in the best cases.These observations highlight a clear trade-off: larger models generally identify broader and more nuanced claim-evidence relationships but often at the cost of precision, whereas smaller models maintain more consistent precision with significantly reduced recall.In both the cases similar pattern holds in evidence extraction as well.for Claim (C) and Evidence (E) extraction; "P@R" denotes precision at the corresponding recall.</p>
<p>Claims vs Evidence Extraction</p>
<p>Analyzing claim versus evidence extraction separately reveals distinct performances among LLMs (see Table 1).Across all models, precision is consistently higher for claims than for evidence, indicating the models more readily detect explicit claims compared to the contextually dispersed evidence.Also, the evidence extraction of all models yields higher recall than precision.In addition to the common trends, the models exhibit distinct patterns.For instance, Claude and LLaMA exhibit high recall in evidence extraction but with substantial variability in linking distances (Claude: mean gap of 119.4 sentences, variance of 33,674; LLaMA: mean 95.1 sentences, variance of 34,207), suggesting increased noise and inconsistent performance.Conversely, Ministral maintains lower linking distances (mean 75.9 sentences) with minimal variance, signifying a more cautious and controlled approach.</p>
<p>Impact of Strategy</p>
<p>The Single-pass strategy is highly efficient but has limited coverage, e.g., GPT-4 produces 152 pairs with a 98.5 average sentence_gap, while Ministral generates 166 pairs (average gap: 64.2).Meanwhile, the Three-pass strategy enhances recall and coverage at moderate computational cost.Claude yields 174 pairs (average gap: 122.2), and Phi captures 279 pairs, albeit with significant variance (11,490.2) in sentence_gap.Finally, the Oneby-One strategy maximizes recall but increases computational demand significantly.Claude and LLaMA produce the highest counts (639 and 659 pairs, respectively), with substantial gaps (Claude: 119.4,LLaMA: 95.1) and high variance (Claude: 33,673.9,LLaMA: 34,207.0).Phi also achieves substantial coverage (347 pairs) with notable variance (13,188.2).</p>
<p>Impact of Token Length on Recall</p>
<p>We observed how the documents' token length affected the models' recall performances.In long documents, we expected performance drops, but these observed drops are tied to the prompting strategy.With the Single-pass strategy, the recall performances dropped as the document length increased.With the iterative prompting strategies (Three-pass or One-by-One), the performance drops are less significant, indicating that the iterative prompting imposes less "processing load" onto the LLMs.Additionally, the recall drops differ by the sizes of the models.Relatively smaller models (LLaMA 70B and Ministral 8B) showed more notable declines, especially with Single-pass, whereas the larger models (Claude and GPT-4) maintained relatively high recalls, underlining the advantage of their long context capabilities.Additional details in Appendix C.</p>
<p>Claude and LLaMA frequently produce the highest pair counts (up to 639 and 659), reflecting broad coverage.This can coincide with their large context window sizes-helpful for capturing distant relationships-yet also introduces potential noise.GPT and Gemini keep moderate distances, suggesting they discovered fewer links.Ministral remains conservative with fewer pairs with shorter distances, while Phi's extreme variance indicates inconsistent linking across long contexts.We include the details in Figure 7 (in Appendix C).</p>
<p>Execution Time Analysis</p>
<p>As shown in Figure 4, the execution times differ considerably across models and strategies.GPT is highly efficient in the Single-Pass (under 200s) and relatively moderate in one-by-one approaches (∼500s).Gemini exhibits intermediate execution times across all strategies, notably higher for the three-pass (∼600s).Claude consistently achieves the fastest execution across all strategies, maintaining execution times under 200 seconds.LLaMA shows extensive variability, especially with one-by-one strategies frequently exceeding 1,200 seconds, reflecting significant computational demands.Ministral shows relatively balanced execution times, with three-pass and oneby-one strategies averaging around 600-900 seconds.Phi demonstrates the highest computational intensity, especially in one-by-one strategies, often surpassing 1,200 seconds, highlighting the considerable resource investment required for thorough analyses.The execution times recorded for Gemini exhibit some variability, which may partially stem from fluctuations in API response latency during our experiments, combined with the necessary sleep() intervals implemented for rate limiting.</p>
<p>Discussion</p>
<p>The insights from CLAIM-BENCH emphasize critical directions for future research and practical applications leveraging the capabilities of LLMs in scientific claim-evidence reasoning.Improving LLMs' ability to accurately validate claimevidence pairs could enhance their practical use in designing experiments and generating scientifically valid hypotheses.Furthermore, improved claim identification and validation methods provide a foundation for developing sophisticated claim quality scoring tools that can greatly enhance peerreview processes.The capability to systematically link and integrate evidence across multiple scientific papers could lead to powerful retrievalaugmented laboratory assistants and cross-paper evidence graphs, accelerating knowledge discovery.These advancements would not only strengthen the robustness of scientific validations but also facilitate the creation of more sophisticated scientific QA systems, thus laying foundational benchmarks for future scientific text generation and evaluation methods.This research thus serves as a pivotal foundation for transformative applications in scientific inquiry and discourse.</p>
<p>Conclusion</p>
<p>Motivated by the limited evaluation in prior literature of LLMs' abilities in scientific reasoning, we introduced CLAIM-BENCH, a novel benchmark specifically designed to evaluate LLMs' capabilities in identifying and validating claim-evidence relationships within scientific texts.We systematically explored diverse LLM architectures and prompting strategies.Our results demonstrate significant limitations in LLMs' comprehension, specifically in their precision and recall balance when processing complex scientific documents.Notably, models showed higher precision in extracting explicit claims, whereas extracting dispersed evidence proved challenging, yielding higher recall but lower precision and increased sentence gaps.Moreover, our comparative analysis across 3 strategies revealed substantial trade-offs between computational efficiency, precision, and coverage.Closed-source models generally displayed more stable performances, while open-source models offered broad yet inconsistent coverage.CLAIM-BENCH provides a framework for the assessment of LLMs in complex scientific contexts, and our study provides useful material and insights for continuing the advancement in LLMs' high-level comprehension and scientific reasoning capabilities.</p>
<p>Limitations</p>
<p>While CLAIM-BENCH provides comprehensive insights into the capabilities of LLMs in scientific claim-evidence reasoning.Despite these insights, CLAIM-BENCH has several limitations worth noting.First, the benchmark primarily focuses on recent papers from select domains, which are after the LLMs' knowledge cutoff but might limit the generalizability.Second, the evaluation relies on existing LLM architectures.While we leave the exploration of the impact of model architecture development to future works, CLAIM-BENCH could be a useful material that supports future projects that develop novel LLM architectures that have enhanced long-context language understanding capabilities and scientific reasoning capabilities.</p>
<p>B.2 Inter-Annotator Agreement Methodology</p>
<p>To evaluate the reliability of the CLAIM-BENCH annotations, we calculated Inter-Annotator Agreement on a subset of 30 papers, each annotated by two different annotators on the Claims and the Evidence.For each of the claims and the evidences, we take one set ("set A") as the ground truth and compute the F1score of the other set ("set B").Considering the symmetry, we also computed the F1-score swapping sets A and B, and reported the averaged F1-score.We chose F1 because our annotation task (identifying and linking spans) closely parallels standard information extraction tasks, where F1 is a standard evaluation measure balancing precision and recall; this reflects the need for agreement on both the correctness and comprehensiveness of annotations.</p>
<p>Apart from this we also used an LLM assistant (Gemini 2.5) to automate Cohen's κ on a 30-paper subset.For every paper the LLM (i) extracted the two raw annotation files, (ii) built binary vectors of length N (one entry per sentence; 1 = tagged, 0 = untagged) for claims and for evidence, (iii) populated the 2×2 contingency table (a, b, c, d), and (iv) computed κ = po−pe 1−pe .the procedure was spot-checked on 10 papers and the LLM's arithmetic matched manual counts exactly.</p>
<p>The aggregated results are κ = 0.66 for claims (substantial agreement) and κ = 0.30 for evidence (fair agreement).The lower evidence score is expected: evidence sentences are sparse (&lt; 0.3% of text) and dispersed, so chance agreement is already high, and even minor boundary or selection differences depress κ.Moreover, a single claim can legitimately map to several evidence sentences; annotators often choose different yet valid spans, further reducing overlap.Despite this, both scores confirm that CLAIM-BENCH offers a dependable-though challenging-ground-truth resource for benchmarking claim-evidence reasoning.</p>
<p>Cohen's κ Agreement Prompt</p>
<p>Paper filename: {pdf_name} Total sentences in paper: {total_sentences} You are given two raw annotation lists for claim identification-one from Annotator 1 and one from Annotator 2. Follow the steps below exactly to compute Cohen's κ:</p>
<ol>
<li>Vector Construction Build two binary vectors of length N = {total_sentences}:</li>
</ol>
<p>• 1 if the sentence was marked as a claim by the annotator.• 0 if the sentence was not marked as a claim.Using     1. Performance drops are tied to the strategy more than the model size.</p>
<p>Contingency Table</p>
<p>• For every model, the Single-Pass run shows the steepest decline as documents grow.</p>
<p>• Example: LLaMA's recall plunges from about 0.60 in small papers to roughly 0.40 in ≥20 k-token papers under Single-Pass.</p>
<ol>
<li>Once an iterative strategy is used, the size-related gap all but disappears.</li>
</ol>
<p>• Iterative prompting (Three-Pass or One-by-One) largely neutralises length effects-even for the smaller models.• LLaMA 70B: In One-by-One mode the large-document group matches or exceeds the smalldocument group (≈ 0.78 vs ≈ 0.76).• Ministral 8B: Three-Pass recall stays virtually flat (∼ 0.72-0.75)across all three size buckets; the length penalty only appears in Single-Pass.</p>
<ol>
<li>Larger models still benefit, but their advantage is greatest with fine-grained prompts.</li>
</ol>
<p>• Claude 3.5 Sonnet: Recall rises with document size under Three-Pass (≈ 0.72 → 0.85), and remains ≥ 0.75 in One-by-One.• GPT-4-Turbo: One-by-One keeps recall at or above 0.80 for medium-and large-size papers; the drop to ∼ 0.66 for large papers occurs only in Three-Pass, not in Single-Pass.The figure shows that prompt granularity is the dominant lever for long-context recall.Single-pass prompting amplifies context-window limits-especially in smaller models-but iterative, claim-level prompting (Three-Pass and One-by-One) recovers performance, sometimes even improving it as the text grows.Larger models are naturally more stable, yet they, too, realise their full potential only when given finer-grained, multi-step instructions.</p>
<p>C.1 Sentence Distance Detailed Analysis</p>
<p>Figure 1 :
1
Figure 1: Three methods to prompt LLMs to analyze the papers.Single-Pass: Full paper processing with one prompt.Three-Pass: Sequential claim → evidence → conclusion extraction.One-by-One Pass: Individual evidence retrieval per claim.</p>
<p>Figure 2 :
2
Figure2: Precision vs. Recall for claim (solid markers) and evidence (transparent markers) identification across models and strategies (shapes: Single-Pass •, Three-Pass ▲, One-by-One ■).Models show higher precision for claims, higher recall for evidence, with most results below F 1 = 0.7.</p>
<p>Figure 3 :
3
Figure 3: Sentence distance distribution (box plots) between claims and linked evidence vs. Human baseline (leftmost).LLMs, especially with iterative strategies, link over longer distances than humans, showing capability but potential noise.</p>
<p>capped at 1600 seconds.117 extreme outliers were filtered.</p>
<p>Figure 4 :
4
Figure 4: Execution time comparison (box plots): Single-Pass (■) is fastest, One-by-One (■) is slowest.Models vary greatly in speed (e.g., Claude consistently fast; LLaMA/Phi often requiring &gt;1000s).</p>
<p>Return only the JSON below:{ "kappa_claims": 0.00 }</p>
<p>Figure 5 :
5
Figure 5: The custom annotation tool interface used for CLAIM-BENCH dataset creation, enabling direct PDF text selection and structured labeling (e.g., 'Add as Claim' button) of claim-evidence pairs.</p>
<p>Figure 6
6
Figure 6 plots mean recall for three prompting strategies-Three-Pass, One-by-One, and Single-Passacross three document-length buckets (&lt; 15 k, 15-20 k, ≥ 20 k tokens).A closer reading of the bars yields three key observations:</p>
<p>Figure 6 :
6
Figure 6: Mean recall by document size groups (small, medium, large) for different models and prompting strategies, illustrating performance trends across increasing token counts.</p>
<p>Figure 7 :
7
Figure 7: Aggregated statistics of the sentence_gap metric Count, Max, Mean, and Variance (Var)-for each model under the three prompting strategies (Three-Pass, One-pass, and One-by-One).Larger counts and wider gaps (e.g., Claude and LLaMA exceeding 2,200-sentence links in One-by-One) reflect broader retrieval, whereas smaller models such as Ministral keep distances short and variance low."N/A" indicates the model-strategy combination was not executed.</p>
<p>Table 1 :
1
The highest performance (across all strategies)
ModelBest C Performances Best E Performances F1 P R F1 P RGPT-4-Turbo0.56 0.660.570.47 0.340.69Claude 3.50.59 0.620.600.42 0.330.66Gemini-Exp_1114 0.54 0.480.640.40 0.300.52LLaMA-70B0.58 0.600.560.45 0.420.49Ministral-8B0.48 0.390.610.39 0.310.52Phi-3.5-MoE0.50 0.400.720.35 0.250.63</p>
<p>the two vectors, populate the 2 × 2 table:</p>
<p>To facilitate future research and standardize evaluation in this area, we release CLAIM-BENCH at the CLAIM_BENCH GitHub repository.
A.1 Single-Pass Prompt Comprehensive Evaluation PromptAnalyze the research paper and provide a comprehensive evaluation following these guidelines:1. Identify ALL claims in the paper where each claim:• Makes a specific, verifiable assertion • Is supported by concrete evidence • Represents findings, contributions, or methodological advantages • Can be from any section except abstract 2. For each identified claim:• Extract ALL supporting or contradicting evidence (experimental results, data, or methodology) • Evaluate the evidence strength and limitations • Assess how well conclusions align with evidence Return ONLY the following JSON structure:{ "analysis": [ { "claim_id": number, "claim": { "text": "statement of the claim", "type": "methodology/result/contribution/performance", "location": "section/paragraph", "exact_quote": "verbatim text from paper" }, "evidence": [ { "evidence_text": "specific experimental result/data", "strength": "strong/moderate/weak", "limitations": "specific limitations", "location": "section/paragraph", "exact_quote": "verbatim text from paper" } ], "evaluation": { "conclusion_justified": true/false, "robustness": "high/medium/low", "justification": "explanation of evidence-conclusion alignment", "key_limitations": "critical limitations affecting validity", "confidence_level": "high/medium/low" } } ] } Ensure:• ALL substantive claims are capturedRequirements:• Include both major and minor claims.• Don't miss any claims.• Present each claim as a separate item.Return ONLY the following JSON structure:{ "claims": [ { "claim_id": 1, "claim_text": "statement of the claim", "location": "section/paragraph where this claim appears", "claim_type": "Nature of the claim", "exact_quote": "complete verbatim text containing the claim" } ] }Evidence Identification PromptPaper text: {text} For these claims: {claims_text} Please identify relevant evidence that:1. Directly supports or contradicts the claim's specific assertion.2. Is presented with experimental results, data, or concrete examples.3. Can be traced to specific methods, results, or discussion sections.4. Is not from the abstract or introduction.Return ONLY the following JSON:{ "evidence_sets": [ { "claim_id": number, "evidence": [ { "evidence_id": number, "evidence_text": "specific evidence", "strength": "strong/moderate/weak", "limitations": "key limitations", "location": "section/paragraph", "exact_quote": "verbatim text" } ] } ] }Conclusion Evaluation PromptAnalyze these claims and their evidence: {analysis_text} For each claim-evidence pair, evaluate:1. Whether the evidence justifies the claim.2. The overall strength of support.Any important limitations.Return ONLY the following JSON:{ "conclusions": [ { "claim_id": number, "conclusion_justified": true/false, "robustness": "high/medium/low", "key_limitations": "specific limitations", "confidence_level": "high/medium/low" } ] }A.3 One-by-One PromptClaims Extraction PromptAnalyze this research paper and extract ALL possible claims made by the authors.Paper text: {text} Your task is to identify all statements in the text that meet the following criteria for a claim:1. Makes a specific, testable assertion about results, methods, or contributions.2. Represents a novel finding, improvement, or advancement.Presents a clear position or conclusion.Make sure to:• Include both major and minor claims.• Don't miss any claims.• Present each claim as a separate item.Return ONLY the following JSON structure:{ "claims": [ { "claim_id": 1, "claim_text": "statement of the claim", "location": "section/paragraph where this claim appears", "claim_type": "Nature of the claim", "exact_quote": "complete verbatim text containing the claim" } ] }Evidence Analysis PromptPaper text: {text} For the following claim from the paper: "{claim['claim_text']}" Please identify relevant evidence that:1. Directly supports or contradicts the claim's specific assertion.2. Is presented with experimental results, data, or methodology.3. Can be traced to specific methods, results, or discussion sections.Is not from the abstract or introduction.If NO evidence is found for the given Claim, return:"no_evidence_reason": "Explain why no evidence was found (e.g., 'Claim is unsupported', ' → Claim is theoretical without empirical evidence', etc.)" } ELSE: Return ONLY the following JSON structure:{ "claim_id": {claim['claim_id']}, "evidence": [ { "evidence_id": 1, "evidence_text": "specific experimental result/data point", "evidence_type": "primary/secondary", "strength": "strong/moderate/weak", "limitations": "stated limitations or assumptions", "location": "specific section &amp; paragraph", "exact_quote": "verbatim text from paper" } ] }Conclusion Analysis PromptPaper text: {text} Analyze the following claim and its supporting evidence: {single_claim_analysis} Provide a comprehensive conclusion analysis following these guidelines:1. Evidence Assessment:• Evaluate the strength and quality of ALL evidence presented.• Consider both supporting and contradicting evidence.• Assess the methodology and reliability of evidence.Conclusion Analysis:• Determine what the authors concluded about this specific claim.• Evaluate if the conclusion is justified by the evidence.• Consider the relationship between evidence quality and conclusion strength.Robustness Evaluation:• Assess how well the evidence supports the conclusion.• Consider methodological strengths and weaknesses.• Evaluate the consistency of evidence.Limitations Analysis:• Identify specific limitations in both evidence and conclusion.• Consider gaps in methodology or data.• Note any potential biases or confounding factors.Return ONLY the following JSON structure:{ "conclusions": [ { "claim_id": {claim_id}, "author_conclusion": "detailed description of authors' conclusion based on evidence → ", "conclusion_justified": true/false, "justification_explanation": "detailed explanation of why conclusion is/isn't → justified", "robustness_analysis": "comprehensive analysis of evidence strength and reliability → ", "limitations": "specific limitations and caveats", "location": "section/paragraph where conclusion appears", "evidence_alignment": "analysis of how well evidence aligns with conclusion", "confidence_level": "high/medium/low based on evidence quality" } ] }B Additional Details on Annotation B.1 Annotator Guidelines• Select one recent research paper in the field of artificial intelligence or machine learning.• Prioritize papers published in 2024 to ensure relevance to current developments.• When possible, select a paper with fewer than 20 pages to facilitate thorough annotation.• Avoid papers with heavily mathematical content to ensure accessibility.• Complete all annotation tasks independently, without employing large language models for assistance at any stage of the process.Task DescriptionYour task is to identify all statements in the text that qualify as claims under the following criteria:1. Specificity: The statement makes a specific, testable assertion about results, methods, or contributions.Novelty:The statement represents a novel finding, improvement, or advancement.Clarity:The statement presents a clear position or conclusion.Requirements• Include both major and minor claims.• Ensure no claim is overlooked.• Present each claim as a separate item.Evidence IdentificationFor each identified claim, find and document relevant evidence that:1. Relevance: Directly supports or contradicts the claim's specific assertion.2. Concrete Support: Is presented with experimental results, data, or concrete examples.3. Traceability: Can be traced to specific methods, results, or discussion sections in the text.4. Exclusions: Evidence must not be derived from the abstract or introduction sections of the text.Conclusion Analysis• Justification: Evaluate whether the conclusions drawn in the text are justified by the evidence provided.Annotation FormatEach annotation should be formatted as follows:{ "Claim_id": "<unique_identifier>", "Claim_text": "<text_of_the_claim>", "Evidence_text": "<text_supporting_or_contradicting_the_claim>", "Justification_Conclusion": "<evaluator's_comment_on_evidence_justification>" } Raw Annotations -Annotator 1: {raw_annotations1} Raw Annotations -Annotator 2: {raw_annotations2}Example Output: Cohen's κ CalculationWe compute Cohen's κ for claim identification on a paper with N = 667 sentences.Annotation statistics• Annotator 1 marked 5 sentences as claims.• Annotator 2 marked 6 sentences as claims.• Overlap (both claim = 1): 4 sentences.Contingency tableAnn 2 = 1 Ann 2 = 0 Row Tot.
Phi-3 technical report: A highly capable language model locally on your phone. Jyoti Marah Abdin, Hany Aneja, Awadalla, arXiv:2404.142192024Preprint</p>
<p>Shubham Agarwal, Gaurav Sahu, Abhay Puri, H Issam, Laradji, D J Krishnamurthy, Jason Dvijotham, Laurent Stanley, Christopher Charlin, Pal, 10.48550/arXiv.2402.01788ArXiv:2402.01788LitLLM: A Toolkit for Scientific Literature Review. 2025arXiv preprint</p>
<p>Claude 3.5 sonnet model card addendum. PDF file. Anthropic, 2025. 12 Apr. 2025</p>
<p>What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context. arXiv preprint. Zhiyuan Chang, Mingyang Li, Xiaojun Jia, Junjie Wang, Yuekai Huang, Qing Wang, Yihao Huang, Yang Liu, 10.48550/arXiv.2412.12632ArXiv:2412.126322024</p>
<p>AI-assisted peer review. Alessandro Checco, Lorenzo Bracciale, Pierpaolo Loreti, Stephen Pinfield, Giuseppe Bianchi, 10.1057/s41599-020-00703-8Humanities and Social Sciences Communications. 812021</p>
<p>ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery. Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Konstantin Chernyshev, Vitaliy Polshkov, Ekaterina Artemova, Alex Myasnikov, Vlad Stepanov, Alexei Miasnikov, Sergei Tilga, 10.48550/arXiv.2412.03205ArXiv:2412.03205U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills in LLMs. 2025arXiv preprint</p>
<p>Marta R Costa-Jussà, Pierre Andrews, Mariano Coria Meglioli, Joy Chen, Joe Chuang, David Dale, Christophe Ropers, Alexandre Mourachko, Eduardo Sánchez, Holger Schwenk, Tuan Tran, 10.48550/arXiv.2412.08268ArXiv:2412.08268Arina Turkatenko, and Carleigh Wood. 2024a. LCFO: Long Context and Long Form Output Dataset and Benchmarking. arXiv preprint</p>
<p>Y-NQ: English-Yorùbá Evaluation dataset for Open-Book Reading Comprehension and Text Generation. Marta R Costa-Jussà, Joy Chen, Ifeoluwanimi Adebara, Joe Chuang, Christophe Ropers, Eduardo Sánchez, 10.48550/arXiv.2412.08279ArXiv:2412.082792024barXiv preprint</p>
<p>MileBench: Benchmarking MLLMs in Long Context. Song Dingjie, Shunian Chen, Guiming Hardy Chen, Fei Yu, Xiang Wan, Benyou Wang, First Conference on Language Modeling. 2024</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Gemini Team, arXiv:2403.055302024Preprint</p>
<p>Leveraging Long-Context Large Language Models for Multi-Document Understanding and Summarization in Enterprise Applications. Aditi Godbole, Jabin Geevarghese George, Smita Shandilya, 10.48550/arXiv.2409.18454ArXiv:2409.184542024arXiv preprint</p>
<p>Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Ceyao Zhang, Chenxing Wei, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Li Zhang, Lingyao Zhang, Min Yang, Mingchen Zhuge, Taicheng Guo, Tuo Zhou, Wei Tao, Xiangru Tang, Xiangtao Lu, Xiawu Zheng, Xinbing Liang, Yaying Fei, Yuheng Cheng, Zhibin Gou, Zongze Xu, Chenglin Wu, 10.48550/arXiv.2402.18679ArXiv:2402.18679Data Interpreter: An LLM Agent For Data Science. 2024arXiv preprint</p>
<p>Yiqiao Jin, Qinlin Zhao, Yiyang Wang, Hao Chen, Kaijie Zhu, Yijia Xiao, Jindong Wang, 10.48550/arXiv.2406.12708ArXiv:2406.12708Agen-tReview: Exploring Peer Review Dynamics with LLM Agents. 2024arXiv preprint</p>
<p>Llm-Assisted Hypothesis Generation and Graph-Based Evaluation. Yan Leng, Hao Wang, Yuan Yuan, 2024</p>
<p>SCBench: A KV Cache-Centric Analysis of Long-Context Methods. Yucheng Li, Huiqiang Jiang, Qianhui Wu, Xufang Luo, Surin Ahn, Chengruidong Zhang, H Amir, Dongsheng Abdi, Jianfeng Li, Yuqing Gao, Lili Yang, Qiu, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary. Yutong Li, Lu Chen, Aiwei Liu, Kai Yu, Lijie Wen, Proceedings of the 31st International Conference on Computational Linguistics. the 31st International Conference on Computational LinguisticsAbu Dhabi, UAE2025Association for Computational Linguistics</p>
<p>Ethical Use of Large Language Models in Academic Research and Writing: A How-To. Michael Lissack, Brenden Meagher, 10.2139/ssrn.49501382024</p>
<p>Exploring Multidimensional Checkworthiness: Designing AI-assisted Claim Prioritization for Human Fact-checkers. Houjiang Liu, Jacek Gwizdka, Matthew Lease, 10.48550/arXiv.2412.08185ArXiv:2412.081852025arXiv preprint</p>
<p>ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing. Ryan Liu, Nihar B Shah, 10.48550/arXiv.2306.00622ArXiv:2306.006222023arXiv preprint</p>
<p>Zijun Liu, Kaiming Liu, Yiqi Zhu, Xuanyu Lei, Zonghan Yang, Zhenhe Zhang, Peng Li, Yang Liu, 10.48550/arXiv.2411.11910ArXiv:2411.11910AIGS: Generating Science from AI-Powered Automated Falsification. 2024arXiv preprint</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, 10.48550/arXiv.2408.06292ArXiv:2408.06292The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. 2024arXiv preprint</p>
<p>Large language models surpass human experts in predicting neuroscience results. Xiaoliang Luo, Akilles Rechardt, Guangzhi Sun, Kevin K Nejad, Felipe Yáñez, Bati Yilmaz, Kangjoo Lee, Alexandra O Cohen, Valentina Borghesani, Anton Pashkov, Daniele Marinazzo, Jonathan Nicholas, Alessandro Salatiello, Ilia Sucholutsky, Pasquale Minervini, Sepehr Razavi, Roberta Rocca, Elkhan Yusifov, Tereza Okalova, Nianlong Gu, Martin Ferianc, Mikail Khona, R Kaustubh, Pui-Shee Patil, Rui Lee, Nicholas E Mata, Jennifer K Myers, Sebastian Bizley, Musslick, 10.1038/s41562-024-02046-9Nature Human Behaviour. Justin M. Ales, Michael Gaebler, N. Apurva Ratan Murty, Leyla Loued-Khenissi, Anna Behler, Chloe M. Hall, Jessica Dafflon, Sherry Dongqi Bao, and Bradley C. Love922025Guiomar NisoIsil Poyraz Bilgin</p>
<p>Yixin Cao, and Aixin Sun. 2024. MMLONGBENCH-DOC: Benchmarking Long-context Document Understanding with Visualizations. Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, Pan Zhang, Liangming Pan, Yu-Gang Jiang, Jiaqi Wang, The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. </p>
<p>A I Mistral, Un Ministral, des Ministraux: Introducing the world's best edge models. 2024. 19 May 2025</p>
<p>Xuanfan Ni, Hengyi Cai, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Piji Li, 10.48550/arXiv.2404.05446ArXiv:2404.05446XL$^2$Bench: A Benchmark for Extremely Long Context Understanding with Long-range Dependencies. 2024arXiv preprint</p>
<p>. 10.48550/arXiv.2303.08774ArXiv:2303.087742024OpenAIarXiv preprint</p>
<p>Many Heads Are Better Than One: Improved Scientific Idea Generation by A LLM-Based Multi-Agent System. Haoyang Su, Renqi Chen, Shixiang Tang, Zhenfei Yin, Xinzhe Zheng, Jinzhe Li, Biqing Qi, Qi Wu, Hui Li, Wanli Ouyang, Philip Torr, Bowen Zhou, Nanqing Dong, 10.48550/arXiv.2410.09403ArXiv:2410.094032025arXiv preprint</p>
<p>ReviewFlow: Intelligent Scaffolding to Support Academic Peer Reviewing. Lu Sun, Aaron Chan, Yun Seo Chang, Steven P Dow, 10.1145/3640543.3645159Proceedings of the 29th International Conference on Intelligent User Interfaces. the 29th International Conference on Intelligent User InterfacesGreenville SC USAACM2024a</p>
<p>MetaWriter: Exploring the Potential and Perils of AI Writing Support in Scientific Peer Review. Lu Sun, Stone Tao, Junjie Hu, Steven P Dow, 10.1145/3637371Proceedings of the ACM on Human-Computer Interaction. 8CSCW12024b</p>
<p>Juraj Vladika, Florian Matthes, 10.48550/arXiv.2305.16859ArXiv:2305.16859Scientific Fact-Checking: A Survey of Resources and Approaches. 2023arXiv preprint</p>
<p>Zhilin Wang, Alexander Bukharin, Olivier Delalleau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Oleksii Kuchaiev, Yi Dong, arXiv:2410.01257Helpsteer2-preference: Complementing ratings with preferences. 2025Preprint</p>
<p>CycleResearcher: Improving Automated Research via Automated Review. Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, Linyi Yang, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs. Yuhao Wu, Ming Shan Hee, Zhiqiang Hu, Roy Ka-Wei Lee, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>A Prompt Templates. </p>            </div>
        </div>

    </div>
</body>
</html>
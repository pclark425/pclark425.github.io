<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8610 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8610</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8610</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-279244851</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.04575v2.pdf" target="_blank">Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically Diversified Texts?</a></p>
                <p><strong>Paper Abstract:</strong> Logical reasoning with large language models (LLMs) has received growing attention. One mainstream approach translates natural language into formal logic and then applies symbolic solvers for deduction. While effective in many tasks, these LLM-based translators often fail to generate consistent symbolic representations when the same concept appears in different linguistic forms. Such inconsistencies break logical coherence and lead to solver errors. However, most existing benchmarks lack this type of linguistic variation, which frequently occurs in real-world text, leaving the problem underexplored. To address this gap, we present SoLT, a benchmark that systematically rewrites reasoning datasets into diverse yet logically equivalent forms across multiple levels. Beyond evaluation, SoLT also provides a general method to enrich any dataset with linguistic diversity while preserving both meaning and logic. To further enhance the stability of LLM-based reasoning, we propose MenTaL, which explicitly guides models to build a concept-symbol mapping table during translation. By linking equivalent expressions to shared symbols, MenTaL maintains consistency and mitigates symbol drift. Experiments on SoLT demonstrate that LLMs indeed suffer from inconsistent symbol mapping under linguistic variation, leading to significant drops in reasoning accuracy. Meanwhile, applying MenTaL brings clear and stable performance improvements across diverse inputs. Overall, our findings reveal that overlooking linguistic diversity hides key weaknesses in LLM-based translators, and our work offers a step toward more reliable logical reasoning in varied real-world scenarios. Our code is available at https://github.com/wufeiwuwoshihua/LinguDiver.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8610.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8610.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source large language model used in this study as a natural-language → formal-logic translator in a neuro-symbolic pipeline (5-shot in-context prompting, API access, temperature 0.2).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source LLM used as the translator component in a neuro-symbolic setup; evaluated via API in 5-shot in-context settings and paired with symbolic solvers (Prover9, PyKE, Python-Constraint). Temperature set to 0.2 for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Multiple: ProofWriter (PW1/PW2), FOLIO, ProverQA, ProntoQA, Deduction, LogicDeduction</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Strict logical reasoning datasets requiring translation of natural-language premises into formal logic and correct deduction by a symbolic solver; tasks include first-order logic proofs (ProofWriter, ProverQA), constraint/deduction problems (Deduction), and other formal reasoning benchmarks (FOLIO, ProntoQA).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Direct structured prompting (5-shot) for translation; prompt-tuning variant that explicitly instructs aligning synonymous expressions; MenTaL in-context demonstrations (mental representation tables) were also applied.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Across original (non-diversified) datasets GPT-4 attains relatively high accuracy (varies by dataset); under SoLT linguistic diversification accuracy drops substantially (examples from exploratory analysis: third-person reference drop ≈0.27 on average, up to ≈0.33 on ProntoQA; synonym substitution mean drop ≈0.22). Applying MenTaL via in-context learning on SoLT gives large improvements (paper reports accuracy gains up to 40.74% on some SoLT tasks) and large reductions in Symbol Dispersion Score (SDS) toward near-zero.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared direct prompting vs. prompt-tuning vs. MenTaL: prompt-tuning produced no significant improvement in SDS or accuracy on SoLT; MenTaL (in-context) produced substantial gains on SoLT-diversified data. Also compared to recent frameworks (Divide-and-Translate, Logic-LM++): those baselines gave minor or no advantage on SoLT; Divide-and-Translate even dropped in accuracy under SoLT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Fails to maintain consistent symbol mapping under mild linguistic variation (symbol drift), leading to logic errors; parsing/execution errors did not increase much but logic errors rose sharply after diversification. Prompt-tuning alone insufficient to fix symbol drift. Smaller models show more formatting/parse failures when attempting to follow demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>High-capacity closed-source LLMs like GPT-4 are strong translators on uniform-language benchmarks but are brittle to realistic linguistic variation that preserves logic; explicit concept-to-symbol bookkeeping (MenTaL) via in-context mental representation tables substantially mitigates symbol drift and stabilizes accuracy under diversification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically Diversified Texts?', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8610.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8610.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source LLM evaluated as a translator in the same 5-shot neuro-symbolic pipeline and compared across original and SoLT-diversified datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source LLM (API) used for natural-language-to-formal-logic translation in 5-shot experiments with temperature 0.2.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Multiple: ProofWriter, FOLIO, ProverQA, ProntoQA, Deduction, LogicDeduction</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same set of strict logical reasoning benchmarks requiring formalization and solver verification.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Direct prompting baseline, prompt-tuning variant, and MenTaL in-context demonstrations were tested.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Shows large accuracy drops on SoLT-diversified inputs relative to original data (Table summaries indicate sharp SDS increases and accuracy declines; e.g., FOLIO accuracy drops to low levels under SoLT). MenTaL gives improvements on SoLT but gains vary by dataset and model capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>MenTaL in-context yields improvements over direct/prompt-tuned prompting for high-performance models; prompt-tuning alone showed little benefit. Compared with Divide-and-Translate and Logic-LM++ on SoLT, these baselines offered little robustness benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Exhibits symbol drift under linguistic variation causing logic errors; prompt-tuning insufficient. Lower performance and larger SDS than stronger models under diversification.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Smaller/earlier closed-source models suffer notable stability degradation under linguistic diversification; targeted mechanisms for consistent symbol mapping are necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically Diversified Texts?', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8610.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8610.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent LLM (cited by technical report) used as one of the translator models in experiments; included in both direct and MenTaL settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vendor/model-citation-present LLM used via API as a translator in 5-shot experiments; described in the paper as a high-performance model evaluated alongside GPT variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Multiple: ProofWriter (PW1/PW2), FOLIO, ProverQA, ProntoQA, Deduction</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Formal proof and deduction benchmarks requiring first-order logic translation and symbolic execution.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Direct prompting and MenTaL in-context demonstrations; also used as a data source to generate SFT training traces for fine-tuning LLaMA-3-8B-Instruct.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Comparable to other high-performance models on original datasets; suffers accuracy decline and increased SDS under SoLT diversification. MenTaL in-context improved SDS and accuracy on SoLT (quantitative gains reported in aggregate in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>MenTaL improved over direct/prompt-tuned methods; used as a generator for SFT datasets (MenTaL-guided SFT produced better generalization for smaller models than non-MenTaL SFT).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Symbol drift under diversification; prompt-tuning insufficient to prevent drift.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>High-performing modern LLMs still need explicit concept-tracking to ensure stable symbol mapping under realistic linguistic variations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically Diversified Texts?', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8610.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8610.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A second DeepSeek family model evaluated in the paper as one of the translators; included in direct/prompting experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vendor/model-citation LLM used via API; evaluated in the paper alongside other LLMs for translation into formal logic.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Multiple reasoning benchmarks (ProofWriter, FOLIO, ProverQA, ProntoQA, Deduction)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Strict logical reasoning tasks requiring formalization to first-order or constraint-style logic and solution via symbolic solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Direct prompting, prompt-tuning; MenTaL experiments primarily reported for higher-performance models but DeepSeek-R1 included in baseline evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported to have degraded accuracy and increased SDS on SoLT-diversified inputs analogous to other LLMs; MenTaL produced improvements in comparable models though specific DeepSeek-R1 MenTaL numbers are reported in aggregated figures/tables.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against direct/prompt tuned and MenTaL-equipped runs; MenTaL and SFT approaches outperformed vanilla SFT or prompt-tuning under SoLT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Also subject to symbol drift under linguistic diversification.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Consistent pattern across LLM families: linguistic variation exposes symbol mapping instability that requires explicit tracking mechanisms to mitigate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically Diversified Texts?', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8610.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8610.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8-billion-parameter open-source LLaMA-3 instruct model evaluated both before and after supervised fine-tuning (SFT) and with MenTaL-based SFT to measure improvements in symbol consistency under diversification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruction-tuned LLaMA-3 8B model deployed locally and evaluated; fine-tuned with LoRA under two SFT regimes (baseline SFT from translations without MenTaL and MenTaL-based SFT from MenTaL-guided translations).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Multiple: ProofWriter (PW1/PW2), FOLIO, ProverQA, ProntoQA, Deduction</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same strict logical reasoning benchmarks requiring formal logic translation and symbolic solving.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Supervised fine-tuning (LoRA) on 600 MenTaL-guided translations (MenTaL SFT) vs. 600 translations produced without MenTaL (baseline SFT); evaluation on original and SoLT-diversified datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Base vs SFT vs MenTaL SFT (selected numbers from Table 4): On original datasets MenTaL SFT modestly improves accuracy (e.g., FOLIO origin: Base 42.86% → SFT 55.09% → MenTaL 58.26%). On SoLT-diversified datasets MenTaL SFT substantially outperforms both base and non-MenTaL SFT (e.g., FOLIO SoLT: Base 10.47% → SFT 12.53% → MenTaL 40.43%). MenTaL SFT also reduces SDS strongly (MenTaL SDS often near 0.08–0.13 on SoLT vs. much higher for Base/SFT).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Baseline SFT (trained on translations without MenTaL) improved accuracy on original data but failed to mitigate symbol drift on SoLT (SDS rose); MenTaL SFT achieved the best accuracy and near-zero SDS on diversified data, outperforming both base and non-MenTaL SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Smaller model capacity constrains ability to follow in-context MenTaL demonstrations reliably (prompt-based MenTaL gave mixed results for LLaMA-3-8B unless fine-tuned). Non-MenTaL SFT generalized poorly to diversified inputs and exhibited elevated SDS.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Training on MenTaL-guided translations (explicitly exposing consistent symbol mappings) allows small open-source models to learn stable mapping rules and generalize to linguistically diversified inputs significantly better than naive SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically Diversified Texts?', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8610.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8610.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SoLT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SoLT (Stability of Logical Translation benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark introduced in this paper that systematically rewrites existing reasoning datasets into linguistically diversified yet logically equivalent forms to evaluate stability of natural-language → formal-logic translators under linguistic variation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SoLT (benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Constructed via a pipeline called LiLD (Logic-invariant Linguistic Diversification) which performs word-, phrase-, and sentence-level rewrites (using WordNet, PPDB, restricted LLM rewrites) plus semantic filtering (embedding threshold 0.90) to ensure semantic and logical invariance; validated by embedding similarity and human/model logical-invariance scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Diversified versions of existing benchmarks: ProofWriter, FOLIO, ProverQA, ProntoQA, Deduction, LogicDeduction</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same reasoning tasks as originals but with controlled, logic-preserving surface variation (third-person reference, synonym substitution, part-of-speech shifts, syntactic transformations) to stress-test translators' symbol consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Dataset diversification (LiLD) at word/phrase/sentence levels + semantic filtering using embedding models (Qwen3-embedding-8B, BGE-large-en-v1.5) and human/LLM logical-invariance evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>SoLT reveals substantial accuracy drops and increased Symbol Dispersion Score (SDS) for LLM translators compared to original datasets; semantic similarity metrics indicate diversified items remain highly similar (embedding scores usually >0.90); humans solve original and diversified items equally well (A/B testing).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Existing benchmarks (without diversification) show near-zero SDS and overestimate translator stability. Under SoLT, prior logical-translation frameworks (Divide-and-Translate, Logic-LM++) lose advantages: Logic-LM++ gives minor improvement; Divide-and-Translate can drop in accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>SoLT is intentionally low-intensity (few edits per sample) and validated to preserve logic; paper notes that certain solver idiosyncrasies (e.g., python-constraint requiring predeclared variables) can exacerbate execution errors when translators introduce undefined symbols.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Incorporating realistic linguistic diversity into evaluation is crucial: it uncovers symbol-drift weaknesses in LLM translators that standard benchmarks miss and provides a platform to develop targeted mitigation methods (e.g., MenTaL).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically Diversified Texts?', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8610.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8610.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MenTaL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MenTaL (Mental representation table-guided translation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lightweight framework introduced in this paper that guides LLMs to build an explicit Mental Representation Table (MRT) mapping equivalent surface expressions to shared logical symbols to mitigate symbol drift during translation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MenTaL (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Framework that enforces dynamic concept-symbol bookkeeping via three operations: Extending (new concept → new symbol), Reusing (recognize semantic equivalence → reuse symbol), and Checking & Refining (handle conflicts/subsumption); implemented as in-context demonstrations for closed models and as a data-generation + supervised fine-tuning signal for open models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Applied to diversified versions of: ProofWriter, FOLIO, ProverQA, ProntoQA, Deduction, LogicDeduction</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same strict logical reasoning tasks, evaluated in neuro-symbolic pipelines where an LLM translates to formal logic and a symbolic solver performs deduction.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Two implementations: (1) Prompt-based in-context learning where LLMs incrementally maintain and update MRT during translation; (2) Supervised fine-tuning (LoRA) on MenTaL-generated reasoning traces (600 examples) to teach smaller models consistent mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>MenTaL markedly improves translation accuracy and reduces SDS on SoLT-diversified datasets. Reported effects: accuracy gains up to 40.74% on some SoLT tasks for high-performance LLMs in in-context setting; MenTaL-based SFT yields near-zero SDS and large accuracy gains for LLaMA-3-8B-Instruct (e.g., FOLIO SoLT: 10.47%→40.43%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms prompt-tuning (which had little effect) and non-MenTaL SFT (which increased SDS on SoLT). MenTaL SFT produced superior generalization to diversified inputs compared to baseline SFT trained on translations without MenTaL.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Does not eliminate all errors: manual error analysis shows ~50% of corrected errors are due to improved symbol consistency while remaining errors are due to other modeling limitations (translating logical relations). Small models sometimes fail to follow in-context MenTaL demonstrations causing formatting/parse errors unless SFT is applied.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Explicit, auditable concept-to-symbol bookkeeping is an effective and general strategy to reduce symbol drift; combining MenTaL in-context for large closed models and MenTaL-based SFT for smaller open models yields broad improvements in robustness to linguistic variation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically Diversified Texts?', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8610.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8610.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Divide-and-Translate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Divide-and-Translate (compositional translation baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A compositional first-order logic translation framework that decomposes problems into semantic subunits, translates them independently, and recomposes the logical form; used as a baseline on SoLT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Divide-and-Translate</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Compositional pipeline that divides input into semantic clauses/subunits, translates them separately to logical fragments, and recomposes according to rules; designed to improve structural fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Applied as baseline on SoLT-diversified versions of FOLIO and other benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used for strict logical translation tasks requiring faithful composition of clauses into first-order logic.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Compositional decomposition & recomposition with a final global check; does not include explicit global symbol unification across diverse expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Although previously reported improvements on original datasets, under SoLT this method's advantages largely vanish and accuracy can even drop (paper reports Divide-and-Translate drops in accuracy on SoLT compared to direct translation for some tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Underperforming relative to MenTaL; worse robustness to linguistic diversity than approaches that explicitly enforce consistent concept-symbol mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Single global check after recomposition insufficient to correct symbol drift introduced by diverse surface expressions; compositional independence can weaken global consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Compositional structural fidelity alone is not enough to guard against symbol drift; explicit concept-unification mechanisms are needed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically Diversified Texts?', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8610.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8610.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logic-LM++</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic-LM++ (multi-step refinement baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of Logic-LM that performs multi-step refinement: initial translation followed by iterative revision and verification; evaluated as a baseline on SoLT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Logic-LM++</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline that produces an initial logical translation and then iteratively revises and verifies it through structured reasoning prompts to improve local translation accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Tested as a baseline on SoLT-diversified logical reasoning datasets (ProofWriter, FOLIO, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks requiring faithful natural-language to formal-logic conversion and solver-based deduction.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Multi-step refinement with self-checks and iterative corrections, but no explicit global symbol-unification mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On SoLT, Logic-LM++ shows only minor improvements over direct prompting and fails to prevent symbol drift effectively (SDS remains elevated relative to MenTaL runs).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Improves local translation quality but does not match MenTaL's gains on diversified data; inferior in reducing SDS and recovering accuracy under SoLT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Self-checking/refinement does not actively fix inconsistent mappings across sentences, so symbol drift persists under linguistic variation.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Iterative local refinement helps local parsing accuracy but is insufficient without explicit mechanisms to align equivalent expressions to the same logical symbols.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically Diversified Texts?', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 2)</em></li>
                <li>Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning <em>(Rating: 2)</em></li>
                <li>Logic-LM++: Multi-step refinement for symbolic formulations <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>FOLIO: Natural language reasoning with first-order logic <em>(Rating: 2)</em></li>
                <li>Prover9: An Automated Theorem Prover for First-Order Logic <em>(Rating: 1)</em></li>
                <li>PPDB: The paraphrase database <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8610",
    "paper_id": "paper-279244851",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A closed-source large language model used in this study as a natural-language → formal-logic translator in a neuro-symbolic pipeline (5-shot in-context prompting, API access, temperature 0.2).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Closed-source LLM used as the translator component in a neuro-symbolic setup; evaluated via API in 5-shot in-context settings and paired with symbolic solvers (Prover9, PyKE, Python-Constraint). Temperature set to 0.2 for experiments.",
            "model_size": null,
            "reasoning_task_name": "Multiple: ProofWriter (PW1/PW2), FOLIO, ProverQA, ProntoQA, Deduction, LogicDeduction",
            "reasoning_task_description": "Strict logical reasoning datasets requiring translation of natural-language premises into formal logic and correct deduction by a symbolic solver; tasks include first-order logic proofs (ProofWriter, ProverQA), constraint/deduction problems (Deduction), and other formal reasoning benchmarks (FOLIO, ProntoQA).",
            "method_or_approach": "Direct structured prompting (5-shot) for translation; prompt-tuning variant that explicitly instructs aligning synonymous expressions; MenTaL in-context demonstrations (mental representation tables) were also applied.",
            "performance": "Across original (non-diversified) datasets GPT-4 attains relatively high accuracy (varies by dataset); under SoLT linguistic diversification accuracy drops substantially (examples from exploratory analysis: third-person reference drop ≈0.27 on average, up to ≈0.33 on ProntoQA; synonym substitution mean drop ≈0.22). Applying MenTaL via in-context learning on SoLT gives large improvements (paper reports accuracy gains up to 40.74% on some SoLT tasks) and large reductions in Symbol Dispersion Score (SDS) toward near-zero.",
            "baseline_comparison": "Compared direct prompting vs. prompt-tuning vs. MenTaL: prompt-tuning produced no significant improvement in SDS or accuracy on SoLT; MenTaL (in-context) produced substantial gains on SoLT-diversified data. Also compared to recent frameworks (Divide-and-Translate, Logic-LM++): those baselines gave minor or no advantage on SoLT; Divide-and-Translate even dropped in accuracy under SoLT.",
            "limitations_or_failures": "Fails to maintain consistent symbol mapping under mild linguistic variation (symbol drift), leading to logic errors; parsing/execution errors did not increase much but logic errors rose sharply after diversification. Prompt-tuning alone insufficient to fix symbol drift. Smaller models show more formatting/parse failures when attempting to follow demonstrations.",
            "insights_or_conclusions": "High-capacity closed-source LLMs like GPT-4 are strong translators on uniform-language benchmarks but are brittle to realistic linguistic variation that preserves logic; explicit concept-to-symbol bookkeeping (MenTaL) via in-context mental representation tables substantially mitigates symbol drift and stabilizes accuracy under diversification.",
            "uuid": "e8610.0",
            "source_info": {
                "paper_title": "Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically Diversified Texts?",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GPT-3.5-Turbo",
            "name_full": "GPT-3.5-Turbo",
            "brief_description": "A closed-source LLM evaluated as a translator in the same 5-shot neuro-symbolic pipeline and compared across original and SoLT-diversified datasets.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_description": "Closed-source LLM (API) used for natural-language-to-formal-logic translation in 5-shot experiments with temperature 0.2.",
            "model_size": null,
            "reasoning_task_name": "Multiple: ProofWriter, FOLIO, ProverQA, ProntoQA, Deduction, LogicDeduction",
            "reasoning_task_description": "Same set of strict logical reasoning benchmarks requiring formalization and solver verification.",
            "method_or_approach": "Direct prompting baseline, prompt-tuning variant, and MenTaL in-context demonstrations were tested.",
            "performance": "Shows large accuracy drops on SoLT-diversified inputs relative to original data (Table summaries indicate sharp SDS increases and accuracy declines; e.g., FOLIO accuracy drops to low levels under SoLT). MenTaL gives improvements on SoLT but gains vary by dataset and model capacity.",
            "baseline_comparison": "MenTaL in-context yields improvements over direct/prompt-tuned prompting for high-performance models; prompt-tuning alone showed little benefit. Compared with Divide-and-Translate and Logic-LM++ on SoLT, these baselines offered little robustness benefit.",
            "limitations_or_failures": "Exhibits symbol drift under linguistic variation causing logic errors; prompt-tuning insufficient. Lower performance and larger SDS than stronger models under diversification.",
            "insights_or_conclusions": "Smaller/earlier closed-source models suffer notable stability degradation under linguistic diversification; targeted mechanisms for consistent symbol mapping are necessary.",
            "uuid": "e8610.1",
            "source_info": {
                "paper_title": "Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically Diversified Texts?",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "DeepSeek-V3",
            "name_full": "DeepSeek-V3",
            "brief_description": "A recent LLM (cited by technical report) used as one of the translator models in experiments; included in both direct and MenTaL settings.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-V3",
            "model_description": "Vendor/model-citation-present LLM used via API as a translator in 5-shot experiments; described in the paper as a high-performance model evaluated alongside GPT variants.",
            "model_size": null,
            "reasoning_task_name": "Multiple: ProofWriter (PW1/PW2), FOLIO, ProverQA, ProntoQA, Deduction",
            "reasoning_task_description": "Formal proof and deduction benchmarks requiring first-order logic translation and symbolic execution.",
            "method_or_approach": "Direct prompting and MenTaL in-context demonstrations; also used as a data source to generate SFT training traces for fine-tuning LLaMA-3-8B-Instruct.",
            "performance": "Comparable to other high-performance models on original datasets; suffers accuracy decline and increased SDS under SoLT diversification. MenTaL in-context improved SDS and accuracy on SoLT (quantitative gains reported in aggregate in Table 3).",
            "baseline_comparison": "MenTaL improved over direct/prompt-tuned methods; used as a generator for SFT datasets (MenTaL-guided SFT produced better generalization for smaller models than non-MenTaL SFT).",
            "limitations_or_failures": "Symbol drift under diversification; prompt-tuning insufficient to prevent drift.",
            "insights_or_conclusions": "High-performing modern LLMs still need explicit concept-tracking to ensure stable symbol mapping under realistic linguistic variations.",
            "uuid": "e8610.2",
            "source_info": {
                "paper_title": "Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically Diversified Texts?",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "DeepSeek-R1",
            "name_full": "DeepSeek-R1",
            "brief_description": "A second DeepSeek family model evaluated in the paper as one of the translators; included in direct/prompting experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1",
            "model_description": "Vendor/model-citation LLM used via API; evaluated in the paper alongside other LLMs for translation into formal logic.",
            "model_size": null,
            "reasoning_task_name": "Multiple reasoning benchmarks (ProofWriter, FOLIO, ProverQA, ProntoQA, Deduction)",
            "reasoning_task_description": "Strict logical reasoning tasks requiring formalization to first-order or constraint-style logic and solution via symbolic solvers.",
            "method_or_approach": "Direct prompting, prompt-tuning; MenTaL experiments primarily reported for higher-performance models but DeepSeek-R1 included in baseline evaluations.",
            "performance": "Reported to have degraded accuracy and increased SDS on SoLT-diversified inputs analogous to other LLMs; MenTaL produced improvements in comparable models though specific DeepSeek-R1 MenTaL numbers are reported in aggregated figures/tables.",
            "baseline_comparison": "Compared against direct/prompt tuned and MenTaL-equipped runs; MenTaL and SFT approaches outperformed vanilla SFT or prompt-tuning under SoLT.",
            "limitations_or_failures": "Also subject to symbol drift under linguistic diversification.",
            "insights_or_conclusions": "Consistent pattern across LLM families: linguistic variation exposes symbol mapping instability that requires explicit tracking mechanisms to mitigate.",
            "uuid": "e8610.3",
            "source_info": {
                "paper_title": "Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically Diversified Texts?",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "LLaMA-3-8B",
            "name_full": "LLaMA-3-8B-Instruct",
            "brief_description": "An 8-billion-parameter open-source LLaMA-3 instruct model evaluated both before and after supervised fine-tuning (SFT) and with MenTaL-based SFT to measure improvements in symbol consistency under diversification.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-3-8B-Instruct",
            "model_description": "Open-source instruction-tuned LLaMA-3 8B model deployed locally and evaluated; fine-tuned with LoRA under two SFT regimes (baseline SFT from translations without MenTaL and MenTaL-based SFT from MenTaL-guided translations).",
            "model_size": "8B",
            "reasoning_task_name": "Multiple: ProofWriter (PW1/PW2), FOLIO, ProverQA, ProntoQA, Deduction",
            "reasoning_task_description": "Same strict logical reasoning benchmarks requiring formal logic translation and symbolic solving.",
            "method_or_approach": "Supervised fine-tuning (LoRA) on 600 MenTaL-guided translations (MenTaL SFT) vs. 600 translations produced without MenTaL (baseline SFT); evaluation on original and SoLT-diversified datasets.",
            "performance": "Base vs SFT vs MenTaL SFT (selected numbers from Table 4): On original datasets MenTaL SFT modestly improves accuracy (e.g., FOLIO origin: Base 42.86% → SFT 55.09% → MenTaL 58.26%). On SoLT-diversified datasets MenTaL SFT substantially outperforms both base and non-MenTaL SFT (e.g., FOLIO SoLT: Base 10.47% → SFT 12.53% → MenTaL 40.43%). MenTaL SFT also reduces SDS strongly (MenTaL SDS often near 0.08–0.13 on SoLT vs. much higher for Base/SFT).",
            "baseline_comparison": "Baseline SFT (trained on translations without MenTaL) improved accuracy on original data but failed to mitigate symbol drift on SoLT (SDS rose); MenTaL SFT achieved the best accuracy and near-zero SDS on diversified data, outperforming both base and non-MenTaL SFT.",
            "limitations_or_failures": "Smaller model capacity constrains ability to follow in-context MenTaL demonstrations reliably (prompt-based MenTaL gave mixed results for LLaMA-3-8B unless fine-tuned). Non-MenTaL SFT generalized poorly to diversified inputs and exhibited elevated SDS.",
            "insights_or_conclusions": "Training on MenTaL-guided translations (explicitly exposing consistent symbol mappings) allows small open-source models to learn stable mapping rules and generalize to linguistically diversified inputs significantly better than naive SFT.",
            "uuid": "e8610.4",
            "source_info": {
                "paper_title": "Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically Diversified Texts?",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "SoLT",
            "name_full": "SoLT (Stability of Logical Translation benchmark)",
            "brief_description": "A benchmark introduced in this paper that systematically rewrites existing reasoning datasets into linguistically diversified yet logically equivalent forms to evaluate stability of natural-language → formal-logic translators under linguistic variation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SoLT (benchmark)",
            "model_description": "Constructed via a pipeline called LiLD (Logic-invariant Linguistic Diversification) which performs word-, phrase-, and sentence-level rewrites (using WordNet, PPDB, restricted LLM rewrites) plus semantic filtering (embedding threshold 0.90) to ensure semantic and logical invariance; validated by embedding similarity and human/model logical-invariance scoring.",
            "model_size": null,
            "reasoning_task_name": "Diversified versions of existing benchmarks: ProofWriter, FOLIO, ProverQA, ProntoQA, Deduction, LogicDeduction",
            "reasoning_task_description": "Same reasoning tasks as originals but with controlled, logic-preserving surface variation (third-person reference, synonym substitution, part-of-speech shifts, syntactic transformations) to stress-test translators' symbol consistency.",
            "method_or_approach": "Dataset diversification (LiLD) at word/phrase/sentence levels + semantic filtering using embedding models (Qwen3-embedding-8B, BGE-large-en-v1.5) and human/LLM logical-invariance evaluation.",
            "performance": "SoLT reveals substantial accuracy drops and increased Symbol Dispersion Score (SDS) for LLM translators compared to original datasets; semantic similarity metrics indicate diversified items remain highly similar (embedding scores usually &gt;0.90); humans solve original and diversified items equally well (A/B testing).",
            "baseline_comparison": "Existing benchmarks (without diversification) show near-zero SDS and overestimate translator stability. Under SoLT, prior logical-translation frameworks (Divide-and-Translate, Logic-LM++) lose advantages: Logic-LM++ gives minor improvement; Divide-and-Translate can drop in accuracy.",
            "limitations_or_failures": "SoLT is intentionally low-intensity (few edits per sample) and validated to preserve logic; paper notes that certain solver idiosyncrasies (e.g., python-constraint requiring predeclared variables) can exacerbate execution errors when translators introduce undefined symbols.",
            "insights_or_conclusions": "Incorporating realistic linguistic diversity into evaluation is crucial: it uncovers symbol-drift weaknesses in LLM translators that standard benchmarks miss and provides a platform to develop targeted mitigation methods (e.g., MenTaL).",
            "uuid": "e8610.5",
            "source_info": {
                "paper_title": "Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically Diversified Texts?",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "MenTaL",
            "name_full": "MenTaL (Mental representation table-guided translation)",
            "brief_description": "A lightweight framework introduced in this paper that guides LLMs to build an explicit Mental Representation Table (MRT) mapping equivalent surface expressions to shared logical symbols to mitigate symbol drift during translation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MenTaL (method)",
            "model_description": "Framework that enforces dynamic concept-symbol bookkeeping via three operations: Extending (new concept → new symbol), Reusing (recognize semantic equivalence → reuse symbol), and Checking & Refining (handle conflicts/subsumption); implemented as in-context demonstrations for closed models and as a data-generation + supervised fine-tuning signal for open models.",
            "model_size": null,
            "reasoning_task_name": "Applied to diversified versions of: ProofWriter, FOLIO, ProverQA, ProntoQA, Deduction, LogicDeduction",
            "reasoning_task_description": "Same strict logical reasoning tasks, evaluated in neuro-symbolic pipelines where an LLM translates to formal logic and a symbolic solver performs deduction.",
            "method_or_approach": "Two implementations: (1) Prompt-based in-context learning where LLMs incrementally maintain and update MRT during translation; (2) Supervised fine-tuning (LoRA) on MenTaL-generated reasoning traces (600 examples) to teach smaller models consistent mapping.",
            "performance": "MenTaL markedly improves translation accuracy and reduces SDS on SoLT-diversified datasets. Reported effects: accuracy gains up to 40.74% on some SoLT tasks for high-performance LLMs in in-context setting; MenTaL-based SFT yields near-zero SDS and large accuracy gains for LLaMA-3-8B-Instruct (e.g., FOLIO SoLT: 10.47%→40.43%).",
            "baseline_comparison": "Outperforms prompt-tuning (which had little effect) and non-MenTaL SFT (which increased SDS on SoLT). MenTaL SFT produced superior generalization to diversified inputs compared to baseline SFT trained on translations without MenTaL.",
            "limitations_or_failures": "Does not eliminate all errors: manual error analysis shows ~50% of corrected errors are due to improved symbol consistency while remaining errors are due to other modeling limitations (translating logical relations). Small models sometimes fail to follow in-context MenTaL demonstrations causing formatting/parse errors unless SFT is applied.",
            "insights_or_conclusions": "Explicit, auditable concept-to-symbol bookkeeping is an effective and general strategy to reduce symbol drift; combining MenTaL in-context for large closed models and MenTaL-based SFT for smaller open models yields broad improvements in robustness to linguistic variation.",
            "uuid": "e8610.6",
            "source_info": {
                "paper_title": "Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically Diversified Texts?",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Divide-and-Translate",
            "name_full": "Divide-and-Translate (compositional translation baseline)",
            "brief_description": "A compositional first-order logic translation framework that decomposes problems into semantic subunits, translates them independently, and recomposes the logical form; used as a baseline on SoLT.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Divide-and-Translate",
            "model_description": "Compositional pipeline that divides input into semantic clauses/subunits, translates them separately to logical fragments, and recomposes according to rules; designed to improve structural fidelity.",
            "model_size": null,
            "reasoning_task_name": "Applied as baseline on SoLT-diversified versions of FOLIO and other benchmarks",
            "reasoning_task_description": "Used for strict logical translation tasks requiring faithful composition of clauses into first-order logic.",
            "method_or_approach": "Compositional decomposition & recomposition with a final global check; does not include explicit global symbol unification across diverse expressions.",
            "performance": "Although previously reported improvements on original datasets, under SoLT this method's advantages largely vanish and accuracy can even drop (paper reports Divide-and-Translate drops in accuracy on SoLT compared to direct translation for some tasks).",
            "baseline_comparison": "Underperforming relative to MenTaL; worse robustness to linguistic diversity than approaches that explicitly enforce consistent concept-symbol mapping.",
            "limitations_or_failures": "Single global check after recomposition insufficient to correct symbol drift introduced by diverse surface expressions; compositional independence can weaken global consistency.",
            "insights_or_conclusions": "Compositional structural fidelity alone is not enough to guard against symbol drift; explicit concept-unification mechanisms are needed.",
            "uuid": "e8610.7",
            "source_info": {
                "paper_title": "Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically Diversified Texts?",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Logic-LM++",
            "name_full": "Logic-LM++ (multi-step refinement baseline)",
            "brief_description": "An extension of Logic-LM that performs multi-step refinement: initial translation followed by iterative revision and verification; evaluated as a baseline on SoLT.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Logic-LM++",
            "model_description": "Pipeline that produces an initial logical translation and then iteratively revises and verifies it through structured reasoning prompts to improve local translation accuracy.",
            "model_size": null,
            "reasoning_task_name": "Tested as a baseline on SoLT-diversified logical reasoning datasets (ProofWriter, FOLIO, etc.)",
            "reasoning_task_description": "Benchmarks requiring faithful natural-language to formal-logic conversion and solver-based deduction.",
            "method_or_approach": "Multi-step refinement with self-checks and iterative corrections, but no explicit global symbol-unification mechanism.",
            "performance": "On SoLT, Logic-LM++ shows only minor improvements over direct prompting and fails to prevent symbol drift effectively (SDS remains elevated relative to MenTaL runs).",
            "baseline_comparison": "Improves local translation quality but does not match MenTaL's gains on diversified data; inferior in reducing SDS and recovering accuracy under SoLT.",
            "limitations_or_failures": "Self-checking/refinement does not actively fix inconsistent mappings across sentences, so symbol drift persists under linguistic variation.",
            "insights_or_conclusions": "Iterative local refinement helps local parsing accuracy but is insufficient without explicit mechanisms to align equivalent expressions to the same logical symbols.",
            "uuid": "e8610.8",
            "source_info": {
                "paper_title": "Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically Diversified Texts?",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning",
            "rating": 2,
            "sanitized_title": "divide_and_translate_compositional_firstorder_logic_translation_and_verification_for_complex_logical_reasoning"
        },
        {
            "paper_title": "Logic-LM++: Multi-step refinement for symbolic formulations",
            "rating": 2,
            "sanitized_title": "logiclm_multistep_refinement_for_symbolic_formulations"
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "FOLIO: Natural language reasoning with first-order logic",
            "rating": 2,
            "sanitized_title": "folio_natural_language_reasoning_with_firstorder_logic"
        },
        {
            "paper_title": "Prover9: An Automated Theorem Prover for First-Order Logic",
            "rating": 1,
            "sanitized_title": "prover9_an_automated_theorem_prover_for_firstorder_logic"
        },
        {
            "paper_title": "PPDB: The paraphrase database",
            "rating": 1,
            "sanitized_title": "ppdb_the_paraphrase_database"
        }
    ],
    "cost": 0.01766625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically Diversified Texts?
16 Oct 2025</p>
<p>Qingchuan Li 
University of Science and Technology of China
HefeiChina</p>
<p>Jiatong Li 
University of Science and Technology of China
HefeiChina</p>
<p>Zirui Liu liuzirui@mail.ustc.edu.cn 
University of Science and Technology of China
HefeiChina</p>
<p>Mingyue Cheng mycheng@ustc.edu.cn 
University of Science and Technology of China
HefeiChina</p>
<p>Yuting Zeng yuting_zeng@mail.ustc.edu.cn 
University of Science and Technology of China
HefeiChina</p>
<p>Qi Liu 
University of Science and Technology of China
HefeiChina</p>
<p>Tongxuan Liu tongxuan.ltx@mail.ustc.edu.cn 
University of Science and Technology of China
HefeiChina</p>
<p>Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically Diversified Texts?
16 Oct 2025BC038873C66ED267F939388723F6CE7EarXiv:2506.04575v2[cs.CL]Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009Logical ReasoningLLM-based Formal Logic TranslationLinguistically Diversified Texts
Logical reasoning with large language models (LLMs) has received growing attention.One mainstream approach translates natural language into formal logic and then applies symbolic solvers for deduction.While effective in many tasks, these LLM-based translators often fail to generate consistent symbolic representations when the same concept appears in different linguistic forms.Such inconsistencies break logical coherence and lead to solver errors.However, most existing benchmarks lack this type of linguistic variation, which frequently occurs in real-world text, leaving the problem underexplored.To address this gap, we present SoLT, a benchmark that systematically rewrites reasoning datasets into diverse yet logically equivalent forms across multiple levels.Beyond evaluation, SoLT also provides a general method to enrich any dataset with linguistic diversity while preserving both meaning and logic.To further enhance the stability of LLM-based reasoning, we propose MenTaL, which explicitly guides models to build a concept-symbol mapping table during translation.By linking equivalent expressions to shared symbols, MenTaL maintains consistency and mitigates symbol drift.Experiments on SoLT demonstrate that LLMs indeed suffer from inconsistent symbol mapping under linguistic variation, leading to significant drops in reasoning accuracy.Meanwhile, applying MenTaL brings clear and stable performance improvements across diverse inputs.Overall, our findings reveal that overlooking linguistic diversity hides key weaknesses in LLM-based translators, and our work offers a step toward more reliable logical reasoning in varied real-world scenarios.Our code is available at https://github.com/wufeiwuwoshihua/LinguDiver.</p>
<p>Introduction</p>
<p>Logical reasoning is central to both human cognition and artificial intelligence, supporting scientific discovery [18], mathematical derivation [9,43], and complex decision-making [2,27,42].In recent years, neuro-symbolic approaches [30,32,51] have gained attention for their strong ability to handle long reasoning chains [37] and are now becoming a mainstream paradigm [25].These methods combine large language models (LLMs) with symbolic solvers.LLMs translate natural language statements into formal logic, and solvers perform deduction on the translated forms.Since symbolic solvers are logically complete [7,11,28,29], the reliability of the whole system mainly depends on the translation step [37].Thus, logical reasoning in this paradigm essentially reduces to translating natural language statements into formal logic faithfully and consistently.</p>
<p>Since the reliability of logical reasoning systems depends on accurate translation, the translator must preserve semantics and, most importantly, ensure consistent symbol mapping, meaning that all semantically equivalent expressions are assigned the same logical symbol [30].As symbolic solvers operate purely on symbols, assigning different symbols to equivalent expressions easily disrupts reasoning.This phenomenon, termed symbol drift, breaks the reasoning chain and prevents the solver from reaching the correct conclusion [28] (see Figure 1).This risk of symbol drift is heightened by the fact that real-world language often expresses the same concept through synonyms, paraphrases, or syntactic variations [46].Therefore, maintaining consistent symbol mapping across linguistic variation is essential for stable logical translation.</p>
<p>Despite its importance, existing reasoning benchmarks [17,38,40] fail to effectively assess this ability.They focus mainly on logical structure and reasoning depth, while their language remains highly uniform, using identical expressions for repeated concepts.This lack of linguistic variation prevents proper evaluation of LLMs' ability to maintain consistent symbol mapping and likely leads to an overestimation of translation stability.To test whether this limitation holds in practice, we conducted an experimental exploration introducing controlled, low-intensity linguistic variations that preserve semantics and logic.The experiments reveal a clear drop in translation accuracy, showing that LLMs struggle to maintain consistent symbol mappings under linguistic variation even when changes are mild.These findings highlight the need for a benchmark that explicitly includes linguistic diversity to more reliably evaluate and improve LLM-based logical translation.</p>
<p>To address this issue, we introduce SoLT, a new benchmark that evaluates the stability of logical translation under linguistic diversity.SoLT is constructed from existing reasoning datasets through a general method termed Logic-invariant Linguistic Diversification (LiLD), which generates linguistically diverse yet logically equivalent versions of the original problems.Inspired by how humans naturally vary expressions in communication [3,36,46], LiLD leverages semantic resources, rule-based rewriting, and semantic filtering to perform diversification at the word, phrase, and sentence levels while preserving semantic and logical consistency.Extensive validation show that LiLD substantially increases linguistic variation without altering semantics or logic structure.Consequently, SoLT not only serves as a benchmark for evaluating translation stability but also provides a scalable and transferable strategy for enriching any reasoning dataset with realistic linguistic variation.</p>
<p>While SoLT reveals the instability of LLM translators under linguistic diversity, stabilizing translation itself remains an open challenge.To tackle this, we further introduce MenTaL, a mental representation table-guided formal logic translation framework.MenTaL aims to mitigate symbol drift that occurs under linguistic diversity.To achieve this, it draws on cognitive theories of mental representation [5,10,12,33] and guides LLMs to explicitly construct a mental representation table during translation, aligning linguistic expressions that share the same meaning before generating formal logical symbols.Building on this design, MenTaL serves as a lightweight and generally applicable framework adaptable to different model scales: it supports prompt-based adaptation for large closedsource models and supervised fine-tuning for smaller open-source ones, thus ensuring broad applicability across diverse settings.</p>
<p>Through experiments on SoLT, we confirm that current LLMs indeed suffer significant performance drops under diversified inputs and exhibit pervasive symbol drift, revealing their difficulty in maintaining consistent symbol alignment.Further analyses show that SoLT can effectively transform existing reasoning benchmarks to better diagnose weaknesses in LLM-based translators.We also demonstrate that MenTaL can mitigate this problem, consistently improving translation accuracy across tasks and models, offering insight for future research on stable logical translation.</p>
<p>Our contributions are as follows:</p>
<p>• We propose SoLT, a benchmark that evaluates the stability of logical translation under linguistic diversity, constructed via a general method (LiLD) that rewrites reasoning datasets into diverse yet logically equivalent forms.• Extensive experiments on SoLT reveal symbol drift in current LLM translators and demonstrate that MenTaL effectively improves their robustness and stability.</p>
<p>Accuracy drop</p>
<p>Related Works</p>
<p>Natural Language Datasets for Logical Reasoning.Existing datasets cover diverse reasoning tasks [34], with many providing explicit reasoning chains [38].However, many benchmarks [15,34,38,40] are generated by rule-based templates, producing repetitive expressions of the same logical concepts.While others [17,26,44,52] are manually curated or drawn from standardized exams, they emphasize reasoning difficulty and logical rule coverage but give little attention to linguistic variety.As a result, recurring patterns dominate, and these datasets fail to reflect the linguistic variation typical in real-world reasoning.Consequently, they cannot effectively test whether LLMs can assign consistent symbols to equivalent expressions during translation.Our work introduces a linguistic diversification method built on existing datasets to address this limitation, providing a complementary resource to current benchmarks.</p>
<p>Adversarial Text Attacks on LMs.Adversarial attacks degrade model performance by slightly modifying input text to produce semantically similar but misleading variants.They are widely used to assess model robustness and safety in practice.Prior studies include jailbreak attacks [6,39,45,50] and text classification attacks [49].Depending on perturbation granularity, methods are categorized as character-level [8,14], word-level [19,21], or sentence-level [23,53].Many also use synonym substitution [19,35], and recent work has explored LLM-based frameworks such as PromptAttack [49].However, these approaches do not target the challenge of preserving symbol consistency in logical translation tasks.Our work introduces a new paradigm, logic-invariant linguistic diversification, which extends adversarial methods to systematically test semantic stability in logical reasoning tasks.</p>
<p>LLMs as Formal Logic Translators.Recent studies view LLMs as translators that convert natural language reasoning problems into formal logic, solved either by symbolic solvers [20,30,32,51] or by the models themselves [22,48].This line of work positions LLMs as a bridge between natural language and formal logic, advancing automated reasoning.However, most existing methods focus on local translation accuracy, such as parsing and translating logical relations within single sentences [20,37], while overlooking global   Extending: Add entry for a new concept.</p>
<p>Reusing: Augment entry for a known concept.</p>
<p>Checking &amp; Refining: Modify entry for a more specific concept.symbol consistency.As a result, they fail to ensure that semantically equivalent concepts are mapped to the same logical symbols throughout the reasoning process.</p>
<p>Preliminaries 3.1 Problem Definition</p>
<p>Given a reasoning problem expressed in natural language,  = { 1 ,  2 , . . .,   }, where each sentence   contributes to the reasoning context, the goal is to determine the correct answer  entailed by .</p>
<p>Formally, let T denote a translator that maps  into a formal logic representation q = T (), and let E denote a symbolic executor that performs deduction on q to obtain  = E ( q).The overall neurosymbolic reasoning process can thus be expressed as the functional composition  = E (T ()).This work investigates the reliability of the translator T , focusing on whether it can preserve semantic and logical consistency when the input  contains linguistically diverse expressions of the same concept.</p>
<p>Experimental Explorations</p>
<p>To verify our hypothesis that linguistic diversity, even under identical semantics and logic, undermines the stability of LLM-based logical translation, we conducted an exploratory experiment.We manually introduced high-quality linguistic variations into existing reasoning benchmarks while strictly preserving their original semantics and reasoning structures.Each sample contained only two linguistic modifications to ensure low perturbation intensity.Four common types of variation were applied: (1) Third-person reference, replacing explicit mentions with pronouns or indirect descriptions;</p>
<p>(2) Synonym substitution, replacing words with semantically equivalent alternatives; (3) Part-of-speech shift, converting words across different grammatical categories; (4) Syntactic transformation, modifying phrase or clause structures such as changing active to passive voice.These controlled perturbations enable a focused evaluation of LLM stability in maintaining stable logical translation under natural linguistic variation.</p>
<p>For evaluation, we adopted the overall task-solver pairs and prompt settings from Logic-LM [32].GPT-4 served as the translator, and symbolic solvers verified the correctness of the translated logical forms.Accuracy differences before and after linguistic diversification were analyzed to quantify the impact of surface-level variation.Detailed task configurations and dataset statistics are provided in the Appendix C.</p>
<p>The results summarized in Figure 2 show consistent performance degradation across all variation types, with the degree of impact varying across them.Third-person reference leads to the most severe decline (around 0.27 on average, up to 0.33 on ProntoQA), suggesting that models struggle to link indirect mentions to consistent logical predicates.Synonym substitution also results in substantial errors (mean drop ≈ 0.22), indicating unstable symbol mapping across equivalent expressions.Compared with these two, Part-ofspeech shift and Syntactic transformation show slightly milder effects (around 0.13 and 0.11, respectively), yet still cause a clear increase in error rates across many tasks.Moreover, performance degradation is more evident on more complex datasets such as FOLIO [17] and ProverQA [34].</p>
<p>By examining the translated outputs in detail, we found that the degradation mainly arises from inconsistent symbol assignment.When the inputs contain linguistically diversified expressions, models often assign different logical symbols to semantically identical entities or phrases (e.g., mapping student and the pupil to different predicates).This inconsistency breaks logical equivalence, disrupts the reasoning chain, and ultimately leads to solver failures.These observations demonstrate that linguistic diversity indeed undermines the stability of LLM-based logical translation by inducing inconsistent symbol mapping.Even minor degrees of diversification are sufficient to cause a clear drop in translation accuracy, suggesting that current benchmarks, which lack linguistic diversity, may substantially overestimate the reliability of LLM translators.Therefore, a new benchmark is urgently needed to systematically evaluate and enhance the logical translation capability of LLMs under realistic linguistic variation.This section introduces Logic-invariant Linguistic Diversification, a method for generating natural language reasoning problems that preserve the original semantics and logic while exhibiting richer surface variation.The method systematically diversifies the linguistic expressions of reasoning samples without altering their meaning or logical structure.Formally, given an input problem
𝑞 = {𝑠 1 , 𝑠 2 , . . . , 𝑠 𝑛 },
where   denotes a sentence, we construct a diversified counterpart
𝑞 ′ = {𝑠 ′ 1 , 𝑠 ′ 2 , . . . , 𝑠 ′ 𝑛 }, in which 𝑠 ′
 is one of rewritten form of   that maintains semantic and logical equivalence.</p>
<p>To implement this idea, we design a structured diversification pipeline that generates linguistically diverse yet logically equivalent reasoning problems.The method consists of three stages: identifying repeated concepts, constructing diversified alternatives, and generating semantically filtered sentences.This pipeline achieves a balance between linguistic diversity and logical fidelity.The following parts provide detailed descriptions of the three stages.</p>
<p>Step 1: Repeated Concept Identification.In logical reasoning, recurring concepts play a crucial role in linking sentences into a coherent inference chain.However, most benchmarks lack linguistic diversity and repeat identical surface expressions for the same concept across sentences.Such repetition removes the need for models to maintain consistency across varied forms and thus fails to reveal their potential weaknesses.To locate these positions for diversification, we employ an LLM-based detector to identify surface expressions that appear multiple times within the input problem.Let  denote a surface expression and freq  () its total occurrences in .We then define the set of repeated expressions as:
𝐶 rep (𝑞) = { 𝑒 | freq 𝑞 (𝑒) ≥ 2 }.
These repeated expressions correspond to the key conceptual anchors of logical reasoning and thus serve as the essential foundation for subsequent diversification.</p>
<p>Step 2: Diversification Strategy Construction.This stage aims to produce high-quality and multi-level diversification strategies for the repeated concepts identified in last stage.We adopt a three-level parallel diversification scheme that mirrors how linguistic variation naturally occurs at the word, phrase, and sentence levels.This design also grounds linguistic changes in authoritative resources, reducing semantic drift that can arise from free-form LLM rewrites.For each repeated expression  ∈  rep (), we apply: • Word level: replace single words with close synonyms.</p>
<p>Name Description</p>
<p>Semantic Invariance</p>
<p>The diversified question must preserve the same semantic information as the original.</p>
<p>Reasoning Invariance</p>
<p>The reasoning process required to reach the answer must remain identical to that of the original, without introducing new premises.</p>
<p>Answer Invariance</p>
<p>The answer obtained from the diversified question must be semantically equivalent to that of the original.</p>
<p>Statement Clarity</p>
<p>The diversified question should be clearly expressed, without introducing confusion or ambiguity.</p>
<p>• Phrase level: rewrite short phrases in different ways while keeping the same meaning.• Sentence level: change the overall structure of a sentence, such as converting active to passive voice or rewriting it in a clear third-person narrative form.</p>
<p>To instantiate this scheme, WordNet [41] supplies synonym candidates for word-level edits, and PPDB [13] provides paraphrases for phrase-level substitutions.LLMs are used only at the sentence level to perform carefully controlled syntactic rewrites, and serve as a fallback mechanism in the rare cases when WordNet and PPDB return no valid candidates.This deliberately restricted use of LLMs effectively minimizes semantic drift that could otherwise arise from unconstrained diversification, thereby ensuring that all generated variants remain both semantically and logically consistent.Together, this design yields a rich, flexible, and reliable diversification strategy that robustly underpins the overall procedure.</p>
<p>Step 3: Candidate Sentence Generation under Semantic Filtering.This stage aims to realize the designed diversification strategies by generating new sentences  ′  that reflect varied surface forms while preserving meaning.Semantic filtering is further applied to ensure that these diversified sentences remain semantically faithful to the originals.After defining the diversification strategies, we apply them to each sentence   ∈  to generate diverse versions while preserving semantic and logical fidelity.Each sentence contains one or more repeated expressions  ∈  rep (), each of which can be replaced by several alternatives generated from the word-, phrase-, or sentence-level strategies.Combining these alternatives yields multiple candidate sentences for   , denoted as  ( )   , where () indexes diversification.Since direct replacement may introduce semantic drift, we add a filtering mechanism to retain only faithful variants.Semantic similarity between the original sentence and each candidate is computed using a sentence embedding model  (•), sim(•, •) denotes cosine similarity between embeddings:
sim(𝑠 𝑖 , 𝑠 (𝑘 ) 𝑖 ) = ⟨𝜙 (𝑠 𝑖 ), 𝜙 (𝑠 (𝑘 ) 𝑖 )⟩ ∥𝜙 (𝑠 𝑖 )∥ • ∥𝜙 (𝑠 (𝑘 ) 𝑖 )∥ ,
We then define a valid candidate set:
S 𝑖 = { 𝑠 (𝑘 ) 𝑖 | sim(𝑠 𝑖 , 𝑠 (𝑘 ) 𝑖 ) ≥ 𝜃 },
where  is a similarity threshold enforcing semantic consistency.This semantic filtering step discards candidates that alter meaning, ensuring that diversity arises only from lexical or syntactic variation.The final diversified problem is constructed as
𝑞 ′ ∈ S 1 × S 2 × • • • × S 𝑛 ,</p>
<p>Validation of Logical Invariance</p>
<p>To assess whether diversification affects the meaning of the data, we evaluate semantic preservation using embedding models.Specifically, given pairs of original and diversified questions:
𝐷 dual = {(𝑞 𝑖 , 𝑞 ′ 𝑖 ) | 𝑖 = 1, 2, . . . , |𝐷 |}.
We compute semantic similarity scores to quantify potential semantic drift.Two strong embedding models, Qwen3-embedding-8B and BGE-large-en-v1.5, are used for this purpose.The results in Table 1 show that the semantic similarity of the five datasets remains consistently high after diversification with SoLT.With Qwen3embedding-8B, most scores exceed 0.90 and none fall below 0.85; with BGE-large-en-v1.5, all scores are above 0.90 and several surpass 0.95.These results indicate that semantic drift introduced by SoLT is minimal, and the diversified questions remain semantically consistent with the originals.</p>
<p>Building on this, we move from semantic preservation to a finer evaluation of logical invariance.Drawing on prior studies of logical equivalence and considering both meaning and reasoning, we define a set of logical invariance criteria shown in Table 2.</p>
<p>Following these criteria, we invited professional annotators and LLMs to evaluate the diversified data.A 1-5 SoLT was used to rate the quality of diversification: 5.0 -fully satisfies all criteria and is entirely acceptable; 4.0 -contains minor flaws but remains acceptable overall; 3.0 -partially acceptable, with flaws that affect quality; 2.0 -major flaws make the question unacceptable; 1.0critical flaws make the question unusable.</p>
<p>The final score is the average of human and model evaluations.As shown in Table 1, all SoLT-diversified datasets scored above 4.0 for logical consistency, indicating general acceptability.Most scores exceeded 4.5, showing that SoLT preserves both the semantics and logical structure of the original questions with minimal degradation.</p>
<p>Human annotators also solved both the original and diversified questions, with detailed results reported in Appendix A.</p>
<p>MenTaL: A Mental Representation-Guided Translation Framework</p>
<dl>
<dt>As a further step, we introduce a lightweight approach, MenTaL, designed to mitigate symbol drift and enhance the stability of LLMbased logical reasoning systems.Drawing inspiration from the theory of mental representation in cognitive science [5,10,12,33], MenTaL incorporates an explicit Mental Representation Table (MRT) that the model maintains and updates throughout the translation process.The MRT functions as an external memory that records concept-level correspondences between expressions, ensuring consistent symbol assignment while preserving interpretability.Formally, Given a reasoning problem , MenTaL maintains a state ( q, ), where q is the logical form being generated and  is a dynamically updated table that records the correspondence between surface expressions and logical symbols:</dt>
<dd>
<p>{ (1) ,  (2) , . . .,  () } ↦ → ,</p>
</dd>
</dl>
<p>where { ( ) } are expressions identified as referring to the same concept, and  is their unified logical symbol.To determine how  evolves as translation proceeds, MenTaL defines two relations between expressions: Semantic Equivalence ( ( ) ≡  (  ) ), meaning two expressions describe the same concept; and Conflict ( ( ) ⊲⊳  (  ) ), meaning one expression subsumes or overlaps with another (e.g., popular show vs. show).</p>
<p>Based on these relations, MenTaL performs three update operations whenever a new expression  ( ) appears:</p>
<p>• Extending: add a new entry for a previously unseen concept and assign it a new logical symbol.• Reusing: augment an existing entry when  ( ) ≡  (  ) for some  (  ) ∈  , ensuring symbol consistency.</p>
<p>• Checking &amp; Refining: modify an entry when  ( ) ⊲⊳  (  ) for some  (  ) ∈  , preserving the atomic base symbol while refining the mapping, e.g., PopularShow() ⇒ Popular() ∧ Show().Whenever  is updated, earlier parts of q are retroactively revised to maintain global symbol consistency.</p>
<p>MenTaL can be implemented through prompt-based interaction, where an LLM incrementally updates  during translation, or through supervised fine-tuning that jointly learns to produce ( q, ).The method does not aim to eliminate symbol drift entirely, but to substantially reduce it by enforcing explicit concept tracking.By making the mapping between language and logic transparent and auditable, MenTaL improves the stability and interpretability of LLM-based logical translation under linguistic variation.</p>
<p>Evaluation of LLM Translators on SoLT 5.1 Experiment Setup</p>
<p>Solvers and Datasets.Three symbolic solvers are used for logical reasoning: Python-Constraint, PyKE, and Prover9.Five datasets are used in the evaluation, covering a range of reasoning styles and complexities: LogicDeduction [15], ProntoQA1 [38], FOLIO [17], ProofWriter [40], and ProverQA [34].Further details on the datasets, solvers, and their mappings are provided in Appendix C. Large Language Models.Experiments are conducted using four LLMs: GPT-3.5-Turbo[31], GPT-4 [1], DeepSeek-V3 [24], and DeepSeek-R1 [16].All models are evaluated in a 5-shot in-context setting.Prompting Strategies.To cover mainstream logical translation paradigms, we adopt two prompting strategies: (1) Direct: The LLM directly converts natural language into formal logic using structured prompts with examples, following prior works such as LINC [30], SAT-LM [51], and Logic-LM [32].( 2) Prompt-Tuning:</p>
<p>The prompt explicitly instructs models to align synonymous expressions under a shared predicate, and few-shot examples are revised to demonstrate such consistent mappings, providing lightweight consistency guidance.These two strategies span the range from purely rule-based translation to prompt-based constraint prompting, covering mainstream practices in logical translation.Additional results for other baselines are available in Appendix E.</p>
<p>Semantic Filtering Setting.We use the bge-large-en-v1.5 sentence embedding model to compute cosine similarity between original and candidate sentences, and retain only those with similarity above the threshold  = 0.90 Evaluation Metrics.We report two metrics: Accuracy and the proposed Symbol Dispersion Score (SDS).</p>
<p>Accuracy measures the proportion of translated formulas that yield correct answers when executed by the solver.It directly reflects translation quality and serves as the main evaluation metric.</p>
<p>SDS quantifies symbol drift resulting from inconsistent semantic mapping.Let  be the set of semantic concepts in the dataset and L the set of all logical symbols.A translation defines a mapping  :  → 2 L , where each concept  ∈  is associated with the set  () of symbols used to represent it.We define
SDS = 1 |𝑉 | ∑︁ 𝑣 ∈𝑉 |𝑓 (𝑣)| − 1 .
By definition, SDS = 0 if every concept is consistently mapped to a single symbol (no drift).When SDS &gt; 0, symbol drift occurs, and higher values indicate stronger inconsistency in symbol usage.</p>
<p>Main Result</p>
<p>Using the above experimental setup, we evaluated four LLMs across six datasets with different solver configurations.Figure 4 shows their accuracy in solving logical problems under various tasks and prompting strategies.</p>
<p>Deficiency in Consistent Symbol Mapping.When comparing task accuracy under direct prompting before and after diversification, we observe a large performance drop on SoLT-diversified tasks-from Prompt-tuning Fails to Enable Consistent Symbol Mapping.In the prompt-tuning setting, we explicitly instructed LLMs to align identical concepts with consistent logical symbols and provided incontext examples showing diverse expressions of the same concept.However, when comparing prompt-tuning with direct prompting on SoLT-diversified tasks, we found no significant improvement in either SDS or overall accuracy.This suggests that minor prompt adjustments, even with additional examples, are still insufficient to help LLMs maintain consistent symbol mapping across varied expressions, and thus cannot reliably improve their performance on diversified logical reasoning tasks.</p>
<p>Response Pattern Analysis</p>
<p>The GPT-4 experimental results were further analyzed in detail to categorize different types of translation errors, as shown in Figure 5.A parsing error occurs when the LLM's output is incorrectly formatted and therefore cannot be converted into a valid logical program; an execution error occurs when the translated program fails during execution in the solver due to invalid syntax or missing definitions; and a logic error occurs when the program executes successfully but still produces an incorrect answer.The overall statistics show that after applying SoLT to diversify the datasets, parsing and execution errors did not increase in most tasks except Deduction, and the overall executability of translations remained largely stable.In contrast, logic errors increased sharply, indicating that SoLT's diversification does not make translation harder in terms of syntax or solver constraints, but mainly raises the difficulty of maintaining logical consistency across expressions.In the Deduction dataset, however, a noticeable rise in execution errors was observed, partly because the python-constraint solver requires all variables to be defined before use.This increase suggests that the LLM often generated undefined logical symbols during translation, mapping diversified expressions of the same concept to different logical identifiers, which is consistent with our previous analysis and confirms the presence of symbol drift.</p>
<p>Performance on Diversification Intensity</p>
<p>We further adjusted the SoLT strategy to systematically control the frequency of linguistic diversification and conducted experiments on the ProofWriter dataset, integrating two symbolic solvers with GPT-4 and DeepSeek-V3.The results, shown in Figure 6, demonstrate that as diversification intensity increases, the translation accuracy of LLMs decreases smoothly and continuously, revealing a clear cumulative effect.Specifically, with stronger diversification, LLMs must process a growing number of semantic variants, leading to a gradual buildup of translation errors.During this process, words and expressions essential for correct reasoning are progressively modified, making it increasingly difficult for LLMs to maintain consistent interpretation and symbol alignment, ultimately resulting in steadily declining performance.These findings indicate that LLMs have only limited ability to maintain consistent symbol mapping in formal logic translation, and that this limitation becomes increasingly pronounced under higher diversification.Moreover, diversification intensity can serve as a continuous metric for evaluating the stability of symbol consistency in LLMs.</p>
<p>Enhancing LLM Translators via MenTaL 6.1 Guiding through In-Context Learning</p>
<p>We apply in-context learning to guide LLMs toward achieving stable logical translation.By providing high-quality demonstrations built with mental representation tables, the models can imitate symbol alignment rules and learn to assign consistent predicates to equivalent expressions.We evaluate four LLMs on both the original and SoLT versions of six tasks, with results shown in Table 3.The results show that for high-performance models (GPT-3.5-Turbo,GPT-4, and DeepSeek-V3), applying MenTaL to the original datasets yields only minor accuracy gains, remaining close to the baseline.SDS analysis clarifies this: on the original datasets, SDS stays near 0 even without MenTaL, indicating little symbol driftconsistent with our earlier observation that these datasets contain minimal linguistic diversity.In contrast, applying MenTaL to SoLTdiversified datasets brings substantial improvements, with accuracy gains up to 40.74% and SDS greatly reduced, approaching the level of no symbol drift.These findings demonstrate that MenTaL, when applied through in-context learning, effectively guides LLMs to maintain consistent symbol mapping, mitigating drift and stabilizing performance under linguistic variation.</p>
<p>For the smaller open-source LLaMA-3-8B-Instruct model, Men-TaL improves accuracy on some datasets but decreases it on others.Further analysis shows that smaller models often fail to follow incontext demonstrations, leading to formatting issues and frequent parse or execution errors.This suggests that their limited capacity constrains reliable acquisition of stable symbol alignment through in-context learning, which warrants further investigation.</p>
<p>Refining through Supervised Fine-Tuning</p>
<p>To strengthen small-scale open-source LLMs in maintaining consistent symbol mapping, we adopt a supervised fine-tuning (SFT) approach [4,47,54].We collect successful reasoning traces generated by high-performance LLMs using MenTaL during in-context learning and build a fine-tuning dataset from them.Using this dataset, we fine-tune the Llama-3-8B-Instruct model.For comparison, we also collect outputs from high-performance LLMs without MenTaL to create an additional SFT dataset for logical translation.This model, together with the original Llama-3-8B-Instruct, serves as a baseline.MenTaL-based SFT is implemented with LoRA, and detailed settings and computational costs are given in Appendix C.</p>
<p>Experimental results show that the non-MenTaL SFT model achieves clear gains over the original model on standard datasets.However, when evaluated on SoLT-diversified datasets, its SDS rises notably, indicating that it still fails to mitigate symbol drift.In contrast, the MenTaL-based SFT model not only attains the highest accuracy on the original datasets but also substantially outperforms both baselines on diversified ones, achieving near-zero SDS and markedly higher accuracy.These results demonstrate that incorporating MenTaL into fine-tuning enables small-scale opensource LLMs to maintain consistent symbol mapping and sustain stable performance under linguistically diversified conditions.</p>
<p>Notably, our MenTaL-based SFT was conducted only once yet covered three solver-specific formats and five dataset tasks, highlighting its strong generalization in embedding consistent symbol mapping across solvers and tasks.</p>
<p>Error Attribution Analysis of MenTaL</p>
<p>To further verify whether MenTaL's performance gains genuinely arise from improved consistent symbol mapping, we conducted a manual error analysis on two representative application settings: GPT-4 with prompt-based MenTaL and LLaMA with SFT-based MenTaL, both evaluated on the SoLT-diversified versions of the PW 1 and PW 2 tasks.The results are shown in Figure 7.Our analysis focuses on the subset of errors that appeared both before and after applying MenTaL, grouped into five categories: (1) errors corrected due to improved symbol consistency, (2) errors corrected for other reasons, (3) errors that remained incorrect for other reasons, (4) errors that turned incorrect for other reasons, and (5) errors that remained incorrect without achieving symbol consistency.Here, "other reasons" refer to factors beyond symbol consistency, such as random fluctuations in an LLM's recognition of logical relations or its inherent inability to correctly identify such relations.</p>
<p>Overall, the results show that about half of the errors were corrected through improved symbol consistency, while most of the remaining errors were caused by "other reasons, " namely the model's limited ability in logical relation translation rather than inconsistency in mapping.This confirms that MenTaL's performance gains mainly stem from its targeted enhancement of consistent symbol mapping, thereby improving the stability of both closed-source and open-source models under linguistic variation.</p>
<p>Conclusion</p>
<p>Linguistic diversity is a common feature of natural language, yet existing logical reasoning benchmarks rarely require models to handle such variation.We propose SoLT, which introduces multi-level linguistic diversification while preserving the original problem logic, allowing evaluation of models' ability to process diversified text in logical reasoning.Using SoLT, we find that LLMs, when serving as logical translators, experience a significant accuracy drop under linguistic diversification.Further analysis shows that the main cause is their inability to maintain consistent symbol mapping, revealing a key deficiency overlooked by current benchmarks.The goal of this study is not only to identify these limitations but also to improve model capability.Building on SoLT's findings, we introduce a lightweight and general applicable method, MenTaL, which employs in-context learning and supervised fine-tuning (SFT) to guide LLMs in imitating human mental representation tables.MenTaL markedly improves accuracy on diversified text and provides a clear direction for future work on consistent symbol mapping.We regard this as an important step toward more reliable logical translation and stronger logical reasoning performance.a context, a question, and an extracted proof chain.We use 200 examples from the medium difficulty level, comparable to FOLIO.</p>
<p>Table 6 summarizes the sample counts for each dataset.Prover9.An automated theorem prover for first-order and equational logic.</p>
<p>Each dataset is paired with a specific solver: ProofWriter supports both Prover9 and PyKE, while all others use a single solver.The mapping is summarized in Table 7.</p>
<p>C.3 Evaluation Setting</p>
<p>Model Setting.For close-sourced LLMs (GPT-4, GPT-3.5-Turbo,DeepSeek-V3, DeepSeek-R1), we use their APIs for evaluation.The temperature parameter of each model is set to 0.2 to reduce output uncertainty.For open-sourced LLMs (llama-3-8b-instruct), we locally deploy their huggingface version to our own server and utilize default settings for evaluation.Environment Setting.All experiments are completed on a Linux server with Intel(R) Xeon(R) Platinum 8269CY CPUs @ 2.50GHz and one NVIDIA A100 GPU (40G).GPUs are used for deploying and fine-tuning open-sourced models.The version of Python is 3.10.,theversion of the torch package is 2.2.0.The version of the transformers package is 4.39.3.</p>
<p>Prompt Setting.In our experiments, we use prompt templates to generate prompts of questions for evaluation.Prompt templates for Direct strategy follows the setting of Logic-LM.After getting model outputs, we transform them to json data for further analyses and utilize regular expressions to enhance the robustness of transformation.Details are available at our code.</p>
<p>C.4 SFT Setting</p>
<p>To examine the impact of MenTaL-guided translation on model learning, we perform two rounds of supervised fine-tuning (SFT) under identical configurations.Both SFT runs are implemented using the open-source framework LLaMA-Factory, which provides standardized LoRA fine-tuning pipelines for large language models.Data Source and Purpose.For the baseline SFT (without Men-TaL), training data are obtained from the logical translations of the original datasets generated by DeepSeek-V3 and GPT-4.This SFT serves to adapt the model to the logical language translation task, enabling it to better handle natural language-to-formal logic conversion even without MenTaL assistance.</p>
<p>For the MenTaL SFT, training data come from the same models (DeepSeek-V3 and GPT-4) but translated using the MenTaL-guided process on the SoLT-diversified datasets.Both datasets contain 600 training examples, ensuring a controlled comparison.</p>
<p>Fine-tuning Method.We adopt LoRA (Low-Rank Adaptation) for efficient parameter tuning, as implemented in LLaMA-Factory.Only adapter parameters are updated, while base model weights remain frozen to reduce computational cost and preserve model stability.</p>
<p>Training Configuration.Both SFT settings share the same hyperparameters:</p>
<p>• Batch size: 1 (given the small dataset size of 600 samples); • Learning rate: 5e-5; • Epochs: 3; • Optimizer: AdamW with default parameters; • Precision: FP16 mixed precision for GPU efficiency.</p>
<p>Environment.All fine-tuning experiments are conducted on a Linux server same as the one menthioned in C.3.</p>
<p>Comparison Protocol.After fine-tuning, both models are evaluated under identical experimental conditions.This setup isolates the influence of MenTaL-guided translation, allowing us to assess how training on SoLT-diversified yet logically equivalent data improves semantic consistency and reasoning accuracy compared to the baseline SFT.</p>
<p>D Experiment Cost</p>
<p>In our experiments, the usage of LLM APIs (taking GPT-4 as an example) can be categorized into the following parts: • E0: Generating the SoLT diversified dataset.</p>
<p>• E1: Evaluating accuracy on the Original and SoLT diversified datasets using two prompting strategies.• E2: Generating perturbations of varying strengths on the ProofWriter dataset and conducting corresponding experiments.</p>
<p>• E3: Applying the MenTaL method combined with in-context learning.</p>
<p>The actual token consumption for each type of experiment (based on GPT-4 usage) is summarized in Table 8.The cost (USD) of conducting a set of experiments using GPT-4, calculated based on token usage, is shown in the Table 9.</p>
<p>Additional Cost of MenTaL.Statistically, the MenTaL method incurred token consumption, with input tokens being 1.3 times and output tokens 2.1 times that of the Direct method.</p>
<p>E New Baselines</p>
<p>For comparison, we include two recent LLM-based logical translation frameworks.Since their results on the original benchmarks have already been reported in the respective papers, we conduct experiments only on the SoLT-diversified datasets.Divide-and-Translate [37] follows a compositional strategy, decomposing each problem into semantic subunits (e.g., clauses or predicate-argument structures), translating them independently, and reassembling the full logical form via compositional rules.This design improves structural fidelity but lacks a mechanism to unify semantically equivalent expressions across the problem.</p>
<p>Logic-LM++ [20] extends the Logic-LM pipeline with multi-step refinement.The model first produces an initial logical translation, then iteratively revises and verifies it through structured reasoning prompts.This process enhances local accuracy but does not enforce global symbol consistency across sentences.</p>
<p>Based on Table 10, clear trends emerge.Although Divide-and-Translate and Logic-LM++ achieved notable gains over direct translation in their original studies, these advantages largely vanish under the SoLT setting.Logic-LM++ shows only minor improvement over Direct, while Divide-and-Translate even drops in accuracy.This indicates that the self-checking mechanism of</p>
<p>Figure 1 :
1
Figure 1: An illustrative case of LLM translators failing under linguistic diversification.LLMs map equivalent expressions to different symbols, breaking reasoning chains and preventing the solver from reaching the correct conclusion.</p>
<p>Figure 2 :
2
Figure 2: Accuracy drop of GPT-4 across six reasoning tasks under four diversification types: third-person reference (Thir), synonym substitution (Syno), part-of-speech shift (Part), and syntactic transformation (Synt).</p>
<p>It is snowing outside, and the little bear feels very nippy.If the bear feels frigid, then it will want to hibernate.It is snowing outside, and the little bear is experiencing a deep chill.If the bear experiences coldness, then it will want to hibernate.</p>
<p>Figure 3 :
3
Figure 3: SoLT performs multi-level linguistic diversification while preserving semantics and logic to evaluate translation stability.MenTaL enforces unified symbol mapping through a Mental Representation Table.</p>
<p>Figure 4 :
4
Figure 4: Accuracy↑ and SDS↓ on six reasoning tasks across four LLMs under three evaluation settings.PW 1 and PW 2 denote the ProofWriter dataset paired with the Prover9 and PyKE solvers, respectively.Origin refers to the original dataset with direct prompting, SoLT refers to the SoLT-diversified dataset with direct prompting, and Tuning refers to the SoLT-diversified dataset with prompt-tuning.Accuracy is shown in the upper bars, while SDS is shown in the lower bars.so that each concept  ∈  rep () appears in a varied yet semantically equivalent form.Since multiple valid combinations exist for constructing  ′ , LLMs are employed to select from the filtered candidates and assemble the final diversified problem.During this process, LLMs are guided to enhance expression diversity by avoiding repetitive use of the same strategy across identical concepts.Using this logic-invariant diversification procedure, we build SoLT versions of benchmark datasets to evaluate the stability of LLMbased logical translation under controlled linguistic variation.</p>
<p>Figure 5 :
5
Figure 5: Error statistics for GPT-4 across six tasks.</p>
<p>2 Figure 6 :
26
Figure 6: The performance of GPT-4 and DeepSeek-V3 on PW 1 and PW 2 varies with Diversification Intensity.</p>
<p>Figure 7 :
7
Figure 7: Error transformation distribution before and after applying MenTaL on PW 1 and PW 2 , showing proportions of corrections and remaining errors across different causes.</p>
<p>It is snowing outside, and the little bear feels very cold. If the bear feels cold, then it will want to hibernate. Spring arrived, and Little Bear woke from hibernation. After the long winter rest, he was hungry and craved a large mouthful of honey.
SymbolSymbol MeaningAppearing SentencesSpring()The season of spring1LittleBear(x)The character Little Bear1, 2Hibernation() The long sleep during winter 1, 2Hungry(x)x is hungry2</p>
<p>Table 1 :
1
Semantic preservation of SoLT datasets measured by embedding similarity and logical invariance scores.
SimilarityScoreDatasetQwen3BGEHumanGPT-4DS-R1FOLIO0.89280.93394.54.74.6ProverQA0.90130.94364.64.94.8ProntoQA0.92590.96104.94.24.8Deduction0.87710.92314.34.54.4ProofWriter0.93090.94844.94.84.94 Methodology4.1 Logic-invariant Linguistic Diversification</p>
<p>Table 2 :
2
Standards for Logical Invariance Scoring.</p>
<p>Table 3 :
3
Accuracy(ACC, %) and Symbol Dispersion Score (SDS) of four LLMs applying MenTaL via in-context learning across the original and SoLT diversified versions of six tasks.w/o stands for accuracy with direct prompting.w stands for accuracy with MenTaL.imp stands for improvement through MenTaL..8488.10 1.26 0.01 0.00 0.01 85.83 89.78 3.95 0.05 0.04 0.01 61.76 44.90 -16.86 ProntoQA 77.21 91.14 13.93 0.06 0.03 0.03 90.50 98.02 7.52 0.06 0.01 0.05 87.83 98.01 10.18 0.07 0.05 0.02 57.14 70.27 13.13 Deduction 69.06 70.33 1.27 0.00 0.00 0.00 95.99 96.45 0.46 0.00 0.00 0.00 89.63 96.62 6.99 0.00 0.00 0.00 54.52 49.33 -5.19 PW1 94.17 94.84 0.67 0.01 0.01 0.00 95.48 97.86 2.38 0.01 0.00 0.01 99.17 99.03 -0.14 0.00 0.00 0.00 58.62 75.65 17.03 PW2 71.62 76.47 4.85 0.01 0.00 0.01 83.38 83.72 0.34 0.01 0.00 0.01 82.81 83.76 0.95 0.01 0.00 0.01 65.22 72.86 7.64 SoLT FOLIO 12.17 27.66 15.49 0.68 0.21 0.47 44.17 68.33 24.16 0.34 0.05 0.29 22.12 61.25 39.13 0.50 0.03 0.47 10.47 9.52 -0.95 ProverQA 47.01 66.75 19.74 0.41 0.08 0.32 58.19 76.72 18.53 0.40 0.17 0.23 50.86 79.71 28.85 0.49 0.11 0.38 44.54 37.12 -7.42 ProntoQA 44.50 68.60 24.10 0.64 0.10 0.54 46.39 87.13 40.74 0.46 0.06 0.40 57.19 94.13 36.94 0.49 0.06 0.43 37.21 59.57 22.36 Deduction 25.33 39.27 13.94 0.69 0.09 0.60 71.98 82.79 10.81 0.59 0.04 0.55 59.06 80.46 21.40 0.79 0.05 0.74 14.29 12.59 -1.70 PW1 48.03 73.81 25.78 0.35 0.10 0.25 65.38 91.76 26.38 0.30 0.04 0.26 53.01 87.20 34.19 0.35 0.02 0.32 42.33 47.14 4.81 PW2 42.12 60.49 18.37 0.35 0.15 0.19 58.80 78.5719.77 0.38 0.03 0.35 50.67 77.89 27.22 0.44 0.02 0.42 34.83 50.41 15.58 0.51 to 0.24.Analysis of the Symbol Dispersion Score (SDS) directly reflects this decline.While SDS stays close to 0 on nondiversified tasks, it increases sharply after diversification.This means that LLMs often map the same concept to different logical symbols, causing symbol drift and resulting in lower accuracy.
GPT-3.5-TurboGPT-4DeepSeek-V3LlamaTasksACC ↑SDS ↓ACC ↑SDS ↓ACC ↑SDS ↓ACC ↑w/owimp w/owimpw/owimp w/owimpw/owimp w/owimpw/owimpOriginFOLIO48.83 59.57 10.74 0.10 0.02 0.08 73.83 78.17 4.34 0.02 0.01 0.01 72.45 76.41 3.96 0.05 0.00 0.05 42.86 23.99 -18.87ProverQA 70.90 75.33 4.43 0.01 0.01 0.00 86
These results show that LLMs fail to consistently recognize varied expressions of the same concept and unify them under a single logical symbol, revealing a clear weakness in maintaining consistent symbol mapping under linguistic diversity.</p>
<p>Table 4 :
4
Accuracy (ACC, %) and Symbol Dispersion Score (SDS) of LLaMA-3-8B-Instruct on original and SoLT tasks.Base: unfine-tuned model; SFT: fine-tuned without MenTaL; MenTaL: fine-tuned with MenTaL.Best results are in bold.
ACC ↑SDS ↓TasksBase SFT MenTaL Base SFT MenTaLOriginFOLIO42.86 55.0958.260.13 0.100.06ProverQA 61.76 75.5579.160.03 0.040.01ProntoQA 57.14 68.7582.420.08 0.040.03Deduction 54.52 62.3767.540.07 0.050.04PW 158.62 82.9684.100.11 0.050.04PW 265.22 73.1775.200.20 0.020.01SoLTFOLIO10.47 12.5340.430.78 0.640.13ProverQA 44.54 51.3967.210.65 0.440.08ProntoQA 37.21 44.7375.670.69 0.670.04Deduction 14.29 16.4438.780.73 0.630.09PW 142.33 55.1072.660.39 0.350.10PW 234.83 47.8369.710.73 0.370.06</p>
<p>Table 6 :
6
Number of examples used in final test sets.
Dataset# ExamplesFOLIO130ProofWriter500Deduction200ProntoQA200ProverQA200C.2 Details of SolversPython-Constraint. A declarative solver for constraint satisfac-tion problems (CSP), supporting the definition of variables, domains,and logical constraints.PyKE (Python Knowledge Engine). A rule-based reasoningframework combining logic programming with object-orienteddesign, suitable for decision and inference systems.</p>
<p>Table 7 :
7
Mapping Between Datasets and Solvers
DatasetSolver(s)DeductionPython-ConstraintProntoQAPyKEFOLIOProver9ProofWriter PyKE, Prover9ProverQAProver9</p>
<p>Table 8 :
8
Token Usage Statistics for Different Experiment Stages (GPT-4)
Index # Input Tokens # Output TokensE01,658,015354,493E19,765,8192,113,512E24,880,4441,023,106E35,040,1272,646,814</p>
<p>Table 9 :
9
Estimated Cost (USD) for Token Usage in Different Experiment Stages (GPT-4)
Index Input Cost ($) Output Cost ($) Total Cost ($)E016.5810.6327.21E197.6563.43161.08E248.8030.6979.49E350.4079.40129.80Total213.43184.15397.58</p>
<p>Table 10 :
10
Performance of Recent Logical-Translation Baselines on SoLT-Diversified Tasks</p>
<p>To ensure diversifiability, words in ProntoQA not attested in English were replaced with equivalent concepts from ProofWriter.
A Human Evaluation of SoLT DiversificationWe also conducted a more direct experiment, recruiting human participants to answer randomly selected questions from five original datasets and their SoLT-diversified counterparts.To ensure unbiased comparison, we adopted an A/B testing setup: each participant was shown either the original or the SoLT version of a question, but never both.The results are shown in Table5. Human performance on both versions was excellent, approaching perfect scores, demonstrating that diversification in SoLT preserve human interpretability and do not hinder problem-solving.B Algorithm for MenTaLAlgorithm 1 introduces several symbols and routines not defined in the main text.Given a natural-language problem  = { 1 ,  2 , . . .,   }, each   denotes an expression-level unit (word, phrase, or clause), distinct from the sentence-level notation   in Section 4. q is the logical form incrementally generated, and  denotes the Mental Representation Table, mapping semantically equivalent expressions to a unified symbol:  : { (1) ,  (2) , . . .,  () } ↦ → .(  ) is the symbol assigned to   .new denotes a newly created symbol.Two LLM-based functions are defined: LLMEqiv(,   ) determines whether  and any element of   are semantically equivalent, while LLMConflict(,   ) detects if  modifies or specializes an existing concept.When a conflict occurs, the algorithm keeps the Append  new to q;17 return ( q, ) more atomic concept  * and rewrites  ( ( ) ) ← ( * ) ∧ (modifier), preserving compositional consistency.When  is updated, earlier parts of q are retroactively revised to maintain symbol coherence.Together, the three operations-Reusing, Extending, and Conflict Resolution-maintain concept-level symbol alignment, preventing symbol drift and enforcing consistent mapping across semantically equivalent expressions.C Experiment Details C.1 Details of DatasetsFOLIO.An expert-curated open-domain dataset for natural language reasoning with first-order logic, featuring diverse structures and high logical complexity.We use the full validation set of 130 examples.ProofWriter.An automatically generated dataset where each example consists of a passage and a conclusion, and the task is to determine whether the conclusion logically follows.We randomly sample 500 examples with reasoning depths from 1 to 5.Deduction.A subset of BigBench focusing on constraint-based reasoning.Each problem includes a context, a question, and multiple options, with the goal of identifying all options consistent with the stated constraints.We sample 200 examples from each of its 3-, 5-, and 7-option variants.ProntoQA.A rule-based dataset with explicit reasoning chains.Each example includes a context, a query, and a chain of thought.Attributes are single-word tokens, often artificial rather than standard English.We randomly select 200 examples.ProverQA.Derived from ProofWiki, this dataset contains formalized proofs from mathematical texts.Each problem includes
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Logical reasoning in formal and everyday reasoning tasks. Hugo Bronkhorst, Gerrit Roorda, Cor Suhre, Martin Goedhart, International Journal of Science and Mathematics Education. 182020. 2020</p>
<p>Mary Bucholtz, The elements of style. 2015. 2015</p>
<p>Tact: Advancing complex aggregative reasoning with information extraction tools. Avi Caciularu, Alon Jacovi, Eyal Ben-David, Sasha Goldshtein, Tal Schuster, Jonathan Herzig, Gal Elidan, Amir Globerson, Advances in Neural Information Processing Systems. 372024. 2024</p>
<p>Where our number concepts come from. Susan Carey, The Journal of philosophy. 1062202009. 2009</p>
<p>Jailbreakbench: An open robustness benchmark for jailbreaking large language models. Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George J Pappas, Florian Tramer, arXiv:2404.013182024. 2024arXiv preprint</p>
<p>Z3: An efficient SMT solver. Leonardo De, Moura , Nikolaj Bjørner, International conference on Tools and Algorithms for the Construction and Analysis of Systems. Springer2008</p>
<p>On adversarial examples for character-level neural machine translation. Javid Ebrahimi, Daniel Lowd, Dejing Dou, arXiv:1806.090302018. 2018arXiv preprint</p>
<p>Deep se (3)-equivariant geometric reasoning for precise placement tasks. Ben Eisner, Yi Yang, Todor Davchev, Mel Vecerik, Jonathan Scholz, David Held, arXiv:2404.134782024. 2024arXiv preprint</p>
<p>The language of thought. Jerry A Fodor, 1975Harvard university press5</p>
<p>A knowledge-based inference engine in Python. Bruce Frederiksen, PyKE: Python Knowledge Engine. 2008. May 2025</p>
<p>Gottlob Frege. 1892. On sense and reference. </p>
<p>PPDB: The paraphrase database. Juri Ganitkevitch, Benjamin Van Durme, Chris Callison-Burch, Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies. the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies2013</p>
<p>Black-box generation of adversarial text sequences to evade deep learning classifiers. Ji Gao, Jack Lanchantin, Mary Lou Soffa, Yanjun Qi, IEEE Security and Privacy Workshops (SPW). 2018. 2018IEEE</p>
<p>Bigbench: Towards an industry standard benchmark for big data analytics. Ahmad Ghazal, Tilmann Rabl, Minqing Hu, Francois Raab, Meikel Poess, Alain Crolotte, Hans-Arno Jacobsen, Proceedings of the 2013 ACM SIGMOD international conference on Management of data. the 2013 ACM SIGMOD international conference on Management of data2013</p>
<p>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.12948Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025. 2025arXiv preprint</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Wenfei Zhou, James Coady, David Peng, Yujie Qiao, Luke Benson, arXiv:2209.00840Folio: Natural language reasoning with first-order logic. 2022. 2022arXiv preprint</p>
<p>Introduction: scientific discovery and inference. Emiliano Ippoliti, Tom Nickles, 2020. 202039</p>
<p>Is bert really robust? a strong baseline for natural language attack on text classification and entailment. Di Jin, Zhijing Jin, Joey Tianyi Zhou, Peter Szolovits, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034</p>
<p>Logic-lm++: Multi-step refinement for symbolic formulations. Shashank Kirtania, Priyanshu Gupta, Arjun Radhakirshna, arXiv:2407.025142024. 2024arXiv preprint</p>
<p>Adversarial text generation by search and learning. Guoyi Li, Bingkang Shi, Zongzhen Liu, Dehan Kong, Yulei Wu, Xiaodan Zhang, Longtao Huang, Honglei Lyu, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Qingchuan Li, Jiatong Li, Tongxuan Liu, Yuting Zeng, Mingyue Cheng, Weizhe Huang, Qi Liu, arXiv:2410.21779Leveraging LLMs for Hypothetical Deduction in Logical Inference: A Neuro-Symbolic Approach. 2024. 2024arXiv preprint</p>
<p>Using adversarial attacks to reveal the statistical bias in machine reading comprehension models. Jieyu Lin, Jiajie Zou, Nai Ding, arXiv:2105.111362021. 2021arXiv preprint</p>
<p>Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, arXiv:2412.19437Deepseek-v3 technical report. 2024. 2024arXiv preprint</p>
<p>Hanmeng Liu, Zhizhang Fu, Mengru Ding, Ruoxi Ning, Chaoli Zhang, Xiaozhang Liu, Yue Zhang, arXiv:2502.09100Logical reasoning in large language models: A survey. 2025. 2025arXiv preprint</p>
<p>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, arXiv:2007.081242020. 2020arXiv preprint</p>
<p>Local: Logical and causal fact-checking with llm-based multi-agents. Jiatong Ma, Linmei Hu, Rang Li, Wenbo Fu, Proceedings of the ACM on Web Conference 2025. the ACM on Web Conference 20252025</p>
<p>Prover9: An Automated Theorem Prover for First-Order Logic. William Mccune, 2009. May 2025Argonne National Laboratory</p>
<p>Z3Prover/z3 Developed by Leonardo de Moura and Nikolaj Bjørner, opensourced under MIT License. Z3: A High-Performance SMT Solver. Microsoft Research2015. May 2025</p>
<p>LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. Alex Theo X Olausson, Benjamin Gu, Cedegao E Lipkin, Armando Zhang, Joshua B Solar-Lezama, Roger Tenenbaum, Levy, arXiv:2310.151642023. 2023arXiv preprint</p>
<p>Logiclm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , arXiv:2305.122952023. 2023arXiv preprint</p>
<p>The stuff of thought: Language as a window into human nature. Steven Pinker, 2007Penguin</p>
<p>Chengwen Qi, Ren Ma, Bowen Li, He Du, Binyuan Hui, Jinwang Wu, Yuanjun Laili, Conghui He, arXiv:2502.06563Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation. 2025. 2025arXiv preprint</p>
<p>Quantifying perturbation impacts for large language models. Paulius Rauba, Qiyao Wei, Mihaela Van Der Schaar, arXiv:2412.008682024. 2024arXiv preprint</p>
<p>Thesaurus of english words and phrases. Peter Mark, Roget , John Lewis, Roget , 2023BoD-Books on Demand</p>
<p>Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning. Hyun Ryu, Gyeongman Kim, Hyemin S Lee, Eunho Yang, arXiv:2410.080472024. 2024arXiv preprint</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, arXiv:2210.012402022. 2022arXiv preprint</p>
<p>Alexandra Souly, Qingyuan Lu, Dillon Bowen, Tu Trinh, Elvis Hsieh, Sana Pandey, Pieter Abbeel, Justin Svegliato, Scott Emmons, Olivia Watkins, arXiv:2402.10260A strongreject for empty jailbreaks. 2024. 2024arXiv preprint</p>
<p>Oyvind Tafjord, Bhavana Dalvi Mishra, Peter Clark, arXiv:2012.13048ProofWriter: Generating implications, proofs, and abductive statements over natural language. 2020. 2020arXiv preprint</p>
<p>Princeton University. 2010. About WordNet. </p>
<p>Can Slow-thinking LLMs Reason Over Time?. Jiahao Wang, Mingyue Cheng, Qi Liu, arXiv:2505.24511Empirical Studies in Time Series Forecasting. 2025. 2025arXiv preprint</p>
<p>Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, Hongsheng Li, arXiv:2310.03731Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning. 2023. 2023arXiv preprint</p>
<p>From lsat: The progress and challenges of complex reasoning. Siyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming Zhou, Zhongyu Wei, Zhumin Chen, Nan Duan, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 302022. 2022</p>
<p>Jailbroken: How does llm safety training fail?. Alexander Wei, Nika Haghtalab, Jacob Steinhardt, Advances in Neural Information Processing Systems. 362023. 2023</p>
<p>Lessons in clarity and grace. M Joseph, Joseph Williams, Bizup, 2014Pearson New Jersey</p>
<p>Instruction Tuning Large Language Models to Understand Electronic Health Records. Zhenbang Wu, Anant Dadu, Michael Nalls, Faraz Faghri, Jimeng Sun, The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024</p>
<p>Symbol-llm: Towards foundational symbol-centric interface for large language models. Fangzhi Xu, Zhiyong Wu, Qiushi Sun, Siyu Ren, Fei Yuan, Shuai Yuan, Qika Lin, Yu Qiao, arXiv:2311.09278Jun Liu. 2023. 2023arXiv preprint</p>
<p>An llm can fool itself: A prompt-based adversarial attack. Xilie Xu, Keyi Kong, Ning Liu, Lizhen Cui, Di Wang, Jingfeng Zhang, Mohan Kankanhalli, arXiv:2310.133452023. 2023arXiv preprint</p>
<p>Dual Intention Escape: Jailbreak Attack against Large Language Models. Yanni Xue, Jiakai Wang, Zixin Yin, Yuqing Ma, Haotong Qin, Renshuai Tao, Xianglong Liu, 2025In THE WEB CONFERENCE 2025</p>
<p>Satlm: Satisfiability-aided language models using declarative prompting. Xi Ye, Qiaochu Chen, Advances in Neural Information Processing Systems. 362023. 2023Isil Dillig, and Greg Durrett</p>
<p>Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, arXiv:2002.04326Reclor: A reading comprehension dataset requiring logical reasoning. 2020. 2020arXiv preprint</p>
<p>A careful examination of large language model performance on grade school arithmetic. Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, William Song, Tiffany Zhao, Pranav Raja, Charlotte Zhuang, Dylan Slack, Advances in Neural Information Processing Systems. 372024. 2024</p>
<p>Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, Yongqiang Ma, arXiv:2403.13372Llamafactory: Unified efficient fine-tuning of 100+ language models. 2024. 2024arXiv preprint</p>
<p>Tasks GPT-3.5-Turbo GPT-4 DeepSeek-V3 DeepSeek-R1. </p>
<p>. Acc Sds, Acc Sds, Acc Sds, Sds, FOLIO 12.17 0.68 44.17 0.34 22.12 0.50 34.00 0.40</p>
<p>Divide and Translate in SoLT. FOLIO 21.38 0.72 37.66 0.44 31.59 0.46 32.87 0.43</p>
<p>Logic-LM++ fails to prevent symbol drift, as the model cannot actively correct inconsistent mappings during reflection. For Divideand-Translate, its sentence-level, divide-and-conquer strategy further weakens consistency under linguistic diversity, since the framework performs only a single global check after recomposition-insufficient to address symbol drift. </p>            </div>
        </div>

    </div>
</body>
</html>
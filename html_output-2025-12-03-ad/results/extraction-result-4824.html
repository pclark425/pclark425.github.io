<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4824 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4824</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4824</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-d9823ffa34f865fb1d0adef95d64a0c352ae125f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d9823ffa34f865fb1d0adef95d64a0c352ae125f" target="_blank">REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction</a></p>
                <p><strong>Paper Venue:</strong> Conference on Robot Learning</p>
                <p><strong>Paper TL;DR:</strong> REFLECT, a framework which queries LLM for failure reasoning based on a hierarchical summary of robot past experiences generated from multisensory observations, is introduced, demonstrating that the LLM-based framework is able to generate informative failure explanations that assist successful correction planning.</p>
                <p><strong>Paper Abstract:</strong> The ability to detect and analyze failed executions automatically is crucial for an explainable and robust robotic system. Recently, Large Language Models (LLMs) have demonstrated strong reasoning abilities on textual inputs. To leverage the power of LLMs for robot failure explanation, we introduce REFLECT, a framework which queries LLM for failure reasoning based on a hierarchical summary of robot past experiences generated from multisensory observations. The failure explanation can further guide a language-based planner to correct the failure and complete the task. To systematically evaluate the framework, we create the RoboFail dataset with a variety of tasks and failure scenarios. We demonstrate that the LLM-based framework is able to generate informative failure explanations that assist successful correction planning.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4824.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4824.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REFLECT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that converts multisensory robot observations into a hierarchical episodic summary (sensory-input, event-based, subgoal-based) and queries a Large Language Model (GPT-4) progressively to localize, explain, and generate correction plans for robot failures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>REFLECT (LLM-based failure explanation & correction framework)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A system that uses a pre-processing pipeline to build a hierarchical memory of a robot's execution (task-informed 3D scene graphs, audio embeddings, event captions, and subgoal-end summaries) and then issues structured prompts to an LLM (GPT-4) in a progressive verification → execution-analysis → planning-analysis loop to produce natural-language failure explanations and language-based correction plans.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>hierarchical episodic / external memory (sensory-input + event-based + subgoal-based summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Memory is implemented as three nested/linked summaries: (1) sensory-input summary — aggregated task-informed 3D scene graphs and audio-label embeddings over time; (2) event-based summary — key-frame selection and textual captions for frames where scene graphs, audio events, or subgoal boundaries change; (3) subgoal-based summary — end-of-subgoal observations for fast verification. The system retrieves relevant levels progressively for LLM querying (subgoal verification first, then event-level history or sensory-level data as needed).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Robot failure localization, explanation, and correction planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a completed (failed) robot execution composed of multisensory logs (RGB-D, audio, robot states) and the original high-level plan, detect which subgoal (if any) failed, generate a concise failure explanation (1–2 sentences, with timestamps), and produce a high-level correction plan executable in the environment.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>RoboFail (new dataset introduced in this paper) — simulation (AI2-THOR, 100 failures) and real-world UR5e teleoperation data (30 failures)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Simulation (REFLECT full hierarchical memory + progressive LLM (GPT-4)): Execution-failure — Explanation (Exp) 88.4%, Localization (Loc) 96.0%, Correction plan success (Co-plan) 79.1%; Planning-failure — Exp 84.2%, Loc 80.7%, Co-plan 80.7%. Real-world: Execution-failure — Exp 68.8%, Loc 93.8%; Planning-failure — Exp 78.6%, Loc 78.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Ablations that reduce or remove the hierarchical memory/progressive retrieval: "Subgoal only" (only subgoal summaries) — Simulation Execution: Exp 76.7%, Loc 74.4%, Co-plan 51.2%; Planning: Exp 71.9%, Loc 73.7%, Co-plan 75.4%. "LLM summary" (sensory-input summary converted to text and then summarized by LLM) — Simulation Execution: Exp 55.8%, Loc 67.4%, Co-plan 65.1%; Planning: Exp 57.9%, Loc 54.4%, Co-plan 66.7%. "w/o progressive" (direct open-ended Q&A on summary) — Simulation Execution: Exp 46.5%, Loc 62.8%, Co-plan 60.5%; Planning: Exp 61.4%, Loc 70.2%, Co-plan 64.9%. "w/o explanation" (no failure explanation used to generate correction) — Co-plan drops to 41.9% (execution failures) in simulation. Real-world ablations show analogous drops (e.g., REFLECT vs Subgoal only: Exec Exp 68.8% vs 56.3%, Loc 93.8% vs 62.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Hierarchical, multi-level memory substantially improves failure explanation, temporal localization, and correction success. Using only subgoal-end summaries loses important intermediate evidence (reducing localization and Co-plan success); compressing sensory histories via an LLM summary loses crucial details (object states/spatial relations) and decreases performance; removing progressive retrieval (direct open Q&A) yields large drops in explanation accuracy and localization. Audio (an additional memory modality) further improves execution-failure explanation/localization (~20% drops when removed in some real-world cases).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Memory construction relies on heuristic scene-graph generation and a fixed candidate list of object states which can fail in more complex or dynamic environments; system assumes largely static environment during execution and struggles with low-level control failures (the summary contains high-level object/state info but not fine-grained control telemetry); perception errors in real-world sensing reduce performance; LLM summarization can lose task-relevant details if used to compress memory.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Design memory for LLM agents as hierarchical, multimodal, and queryable (fast coarse-level checks then targeted retrieval) rather than as single monolithic summaries; retain event-level traces (timestamps, audio events, visual scene-graph facts) to enable precise failure localization; incorporate audio as a complementary modality; progressive verification (verify subgoals before deep analysis) reduces irrelevant information and improves LLM reasoning; avoid over-compressing episodic logs into single summaries that may drop salient facts needed for causal explanations and corrective planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4824.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4824.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously published agent that augments LLM-driven agents with a dynamic memory and self-reflective loop to iteratively improve behavior based on past failures and feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-driven autonomous agent architecture that maintains a dynamic memory of past attempts and uses self-reflection (iterative reasoning over memory of failures and outcomes) to modify future prompts/actions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>dynamic episodic memory / reflective memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Maintains records of prior trajectories, failures and reflective notes that are consulted and updated to guide subsequent planning and corrections (as described in the referenced work). In this paper Reflexion is cited as related work demonstrating dynamic memory/self-reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General autonomous task improvement / self-correction across episodes (as in the referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Not evaluated in this paper; cited as an example of LLM agents that use memory/self-reflection to improve performance over episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Mentioned as related prior work showing the utility of dynamic memory and self-reflection for LLM agents; no direct empirical comparison presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not analyzed here; referenced to motivate the broader context of self-reflective LLM agents.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>The paper cites Reflexion to motivate memory-enabled LLM agents and to position REFLECT as an approach that provides structured episodic memory (multimodal/hierarchical) for LLM reasoning about robot execution failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4824.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4824.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inner Monologue</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inner monologue: Embodied reasoning through planning with language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior approach that uses internal chains-of-thought or sequential reasoning tokens ('inner monologue') to enable LLM-driven planning for embodied agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Inner monologue: Embodied reasoning through planning with language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Inner Monologue (LLM planner)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An approach that augments LLM-based planners with an internal sequential reasoning process (an inner monologue) to improve embodied planning and execution.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>internal chain-of-thought / ephemeral scratchpad (implicit reasoning trace)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Uses the model's internal generated reasoning tokens (chain-of-thought / inner monologue) as an ephemeral record to support planning decisions; cited here among works that demonstrate planning and self-corrective abilities for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Embodied planning tasks (robotic planning/decision-making)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Not evaluated in this paper; cited as related work demonstrating planning capabilities and self-correction using LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Cited to show prior use of internal reasoning loops for embodied planning; no direct experiments or comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Referenced as part of a class of LLM-based planning/self-correction works motivating the use of explicit, retrievable, multimodal episodic memory (the contribution of REFLECT) instead of relying solely on ephemeral internal monologue traces.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models <em>(Rating: 2)</em></li>
                <li>Socratic models: Composing zero-shot multimodal reasoning with language <em>(Rating: 1)</em></li>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4824",
    "paper_id": "paper-d9823ffa34f865fb1d0adef95d64a0c352ae125f",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "REFLECT",
            "name_full": "REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction",
            "brief_description": "A framework that converts multisensory robot observations into a hierarchical episodic summary (sensory-input, event-based, subgoal-based) and queries a Large Language Model (GPT-4) progressively to localize, explain, and generate correction plans for robot failures.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "REFLECT (LLM-based failure explanation & correction framework)",
            "agent_description": "A system that uses a pre-processing pipeline to build a hierarchical memory of a robot's execution (task-informed 3D scene graphs, audio embeddings, event captions, and subgoal-end summaries) and then issues structured prompts to an LLM (GPT-4) in a progressive verification → execution-analysis → planning-analysis loop to produce natural-language failure explanations and language-based correction plans.",
            "memory_type": "hierarchical episodic / external memory (sensory-input + event-based + subgoal-based summaries)",
            "memory_description": "Memory is implemented as three nested/linked summaries: (1) sensory-input summary — aggregated task-informed 3D scene graphs and audio-label embeddings over time; (2) event-based summary — key-frame selection and textual captions for frames where scene graphs, audio events, or subgoal boundaries change; (3) subgoal-based summary — end-of-subgoal observations for fast verification. The system retrieves relevant levels progressively for LLM querying (subgoal verification first, then event-level history or sensory-level data as needed).",
            "task_name": "Robot failure localization, explanation, and correction planning",
            "task_description": "Given a completed (failed) robot execution composed of multisensory logs (RGB-D, audio, robot states) and the original high-level plan, detect which subgoal (if any) failed, generate a concise failure explanation (1–2 sentences, with timestamps), and produce a high-level correction plan executable in the environment.",
            "benchmark_name": "RoboFail (new dataset introduced in this paper) — simulation (AI2-THOR, 100 failures) and real-world UR5e teleoperation data (30 failures)",
            "performance_with_memory": "Simulation (REFLECT full hierarchical memory + progressive LLM (GPT-4)): Execution-failure — Explanation (Exp) 88.4%, Localization (Loc) 96.0%, Correction plan success (Co-plan) 79.1%; Planning-failure — Exp 84.2%, Loc 80.7%, Co-plan 80.7%. Real-world: Execution-failure — Exp 68.8%, Loc 93.8%; Planning-failure — Exp 78.6%, Loc 78.6%.",
            "performance_without_memory": "Ablations that reduce or remove the hierarchical memory/progressive retrieval: \"Subgoal only\" (only subgoal summaries) — Simulation Execution: Exp 76.7%, Loc 74.4%, Co-plan 51.2%; Planning: Exp 71.9%, Loc 73.7%, Co-plan 75.4%. \"LLM summary\" (sensory-input summary converted to text and then summarized by LLM) — Simulation Execution: Exp 55.8%, Loc 67.4%, Co-plan 65.1%; Planning: Exp 57.9%, Loc 54.4%, Co-plan 66.7%. \"w/o progressive\" (direct open-ended Q&A on summary) — Simulation Execution: Exp 46.5%, Loc 62.8%, Co-plan 60.5%; Planning: Exp 61.4%, Loc 70.2%, Co-plan 64.9%. \"w/o explanation\" (no failure explanation used to generate correction) — Co-plan drops to 41.9% (execution failures) in simulation. Real-world ablations show analogous drops (e.g., REFLECT vs Subgoal only: Exec Exp 68.8% vs 56.3%, Loc 93.8% vs 62.5%).",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Hierarchical, multi-level memory substantially improves failure explanation, temporal localization, and correction success. Using only subgoal-end summaries loses important intermediate evidence (reducing localization and Co-plan success); compressing sensory histories via an LLM summary loses crucial details (object states/spatial relations) and decreases performance; removing progressive retrieval (direct open Q&A) yields large drops in explanation accuracy and localization. Audio (an additional memory modality) further improves execution-failure explanation/localization (~20% drops when removed in some real-world cases).",
            "limitations_or_challenges": "Memory construction relies on heuristic scene-graph generation and a fixed candidate list of object states which can fail in more complex or dynamic environments; system assumes largely static environment during execution and struggles with low-level control failures (the summary contains high-level object/state info but not fine-grained control telemetry); perception errors in real-world sensing reduce performance; LLM summarization can lose task-relevant details if used to compress memory.",
            "key_insights": "Design memory for LLM agents as hierarchical, multimodal, and queryable (fast coarse-level checks then targeted retrieval) rather than as single monolithic summaries; retain event-level traces (timestamps, audio events, visual scene-graph facts) to enable precise failure localization; incorporate audio as a complementary modality; progressive verification (verify subgoals before deep analysis) reduces irrelevant information and improves LLM reasoning; avoid over-compressing episodic logs into single summaries that may drop salient facts needed for causal explanations and corrective planning.",
            "uuid": "e4824.0",
            "source_info": {
                "paper_title": "REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "brief_description": "A previously published agent that augments LLM-driven agents with a dynamic memory and self-reflective loop to iteratively improve behavior based on past failures and feedback.",
            "citation_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "mention_or_use": "mention",
            "agent_name": "Reflexion",
            "agent_description": "An LLM-driven autonomous agent architecture that maintains a dynamic memory of past attempts and uses self-reflection (iterative reasoning over memory of failures and outcomes) to modify future prompts/actions.",
            "memory_type": "dynamic episodic memory / reflective memory",
            "memory_description": "Maintains records of prior trajectories, failures and reflective notes that are consulted and updated to guide subsequent planning and corrections (as described in the referenced work). In this paper Reflexion is cited as related work demonstrating dynamic memory/self-reflection.",
            "task_name": "General autonomous task improvement / self-correction across episodes (as in the referenced work)",
            "task_description": "Not evaluated in this paper; cited as an example of LLM agents that use memory/self-reflection to improve performance over episodes.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Mentioned as related prior work showing the utility of dynamic memory and self-reflection for LLM agents; no direct empirical comparison presented in this paper.",
            "limitations_or_challenges": "Not analyzed here; referenced to motivate the broader context of self-reflective LLM agents.",
            "key_insights": "The paper cites Reflexion to motivate memory-enabled LLM agents and to position REFLECT as an approach that provides structured episodic memory (multimodal/hierarchical) for LLM reasoning about robot execution failures.",
            "uuid": "e4824.1",
            "source_info": {
                "paper_title": "REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Inner Monologue",
            "name_full": "Inner monologue: Embodied reasoning through planning with language models",
            "brief_description": "A prior approach that uses internal chains-of-thought or sequential reasoning tokens ('inner monologue') to enable LLM-driven planning for embodied agents.",
            "citation_title": "Inner monologue: Embodied reasoning through planning with language models",
            "mention_or_use": "mention",
            "agent_name": "Inner Monologue (LLM planner)",
            "agent_description": "An approach that augments LLM-based planners with an internal sequential reasoning process (an inner monologue) to improve embodied planning and execution.",
            "memory_type": "internal chain-of-thought / ephemeral scratchpad (implicit reasoning trace)",
            "memory_description": "Uses the model's internal generated reasoning tokens (chain-of-thought / inner monologue) as an ephemeral record to support planning decisions; cited here among works that demonstrate planning and self-corrective abilities for LLMs.",
            "task_name": "Embodied planning tasks (robotic planning/decision-making)",
            "task_description": "Not evaluated in this paper; cited as related work demonstrating planning capabilities and self-correction using LLMs.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Cited to show prior use of internal reasoning loops for embodied planning; no direct experiments or comparisons in this paper.",
            "limitations_or_challenges": "Not analyzed here.",
            "key_insights": "Referenced as part of a class of LLM-based planning/self-correction works motivating the use of explicit, retrievable, multimodal episodic memory (the contribution of REFLECT) instead of relying solely on ephemeral internal monologue traces.",
            "uuid": "e4824.2",
            "source_info": {
                "paper_title": "REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models",
            "rating": 2
        },
        {
            "paper_title": "Socratic models: Composing zero-shot multimodal reasoning with language",
            "rating": 1
        },
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 1
        }
    ],
    "cost": 0.013344499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>REFLECT: Summarizing Robot Experiences for FaiLure Explanation and CorrecTion</h1>
<p>Zeyi Liu<em> Arpit Bahety</em> Shuran Song<br>Columbia University, New York, NY, United States<br>https://robot-reflect.github.io/</p>
<h4>Abstract</h4>
<p>The ability to detect and analyze failed executions automatically is crucial for an explainable and robust robotic system. Recently, Large Language Models (LLMs) have demonstrated strong reasoning abilities on textual inputs. To leverage the power of LLMs for robot failure explanation, we introduce REFLECT, a framework which queries LLM for failure reasoning based on a hierarchical summary of robot past experiences generated from multisensory observations. The failure explanation can further guide a language-based planner to correct the failure and complete the task. To systematically evaluate the framework, we create the RoboFail dataset with a variety of tasks and failure scenarios. We demonstrate that the LLM-based framework is able to generate informative failure explanations that assist successful correction planning.</p>
<p>Keywords: Large Language Model, Explainable AI, Task Planning
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig 1: A framework for robot failure explanation and correction. On the left, we show the REFLECT framework that converts multisensory observations (RGB-D, audio, robot states) to a hierarchical summary of robot experiences. The summary is then used to query a Large Language Model (LLM) for failure explanation and correction. The right shows a few example failure cases in the RoboFail dataset.</p>
<h2>1 Introduction</h2>
<p>With the increasing expectations for robots to work on long-horizon tasks in complex environments, failures are inevitable. It is thus an essential capability for a robotic system to reflect on its past experiences and explain its failures in natural language. The failure explanations can either help a human user to debug the robotic system without having to read through the tedious execution logs, or guide the robot to correct the failure by itself.</p>
<p>We hypothesize that an effective failure reasoning framework requires several key components: first, a component to summarize "what happened" by converting unstructured, multimodal robot sensory data into a structured, unified format; second, a component to reason "what was wrong" by inferring from the summary whether expected outcomes of the robot plan were achieved; and finally, the ability to plan "what to do" based on the failure reasoning to correct the failure and complete the task.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Recently, Large Language Models (LLMs) [1, 2, 3] have been shown to exhibit strong reasoning [4, 5, 6] and planning $[7,8,9]$ capabilities, making it a promising component for explainable and robust robotic systems. However, the remaining challenges lie in how to generate a textual summary of robot sensory data and systematically query LLMs for failure reasoning. In other words, how do we transform the robot failure reasoning task into a language reasoning task? We observe two important attributes of a good robot summary:</p>
<ul>
<li>Multisensory. The summary should cover all sensory modalities the robot has access to, such as visual, audio, contact, etc. This is because certain failures can be more easily identified through one type of sensory data than another. For example, it is easier to determine if an object is dropped or if water is running from the faucet using auditory cues rather than visual ones.</li>
<li>Hierarchical. To support effective failure explanations, the robot summary requires multiple levels of abstraction: to quickly localize the failure, the highest summary level should focus on identifying misalignment between the robot high-level plan and execution outcomes; while the lower summary levels should maintain enough environmental context for LLMs to generate an informative explanation that is useful for correction planning.</li>
</ul>
<p>Based on these observations, we introduce REFLECT, a framework that summarizes robot experiences for failure explanation and correction. The framework first processes post-execution robot observations to generate a hierarchical summary with three levels of abstraction. Equipped with this summary, we propose a progressive failure explanation algorithm for failure reasoning. Through our experiments, we demonstrate that the framework is able to generate informative failure explanations as assessed by human evaluators, and also guide a language-based planner to generate correction plans for several failure scenarios.</p>
<p>To systematically evaluate REFLECT, we also create the RoboFail dataset, which includes 100 failure demonstrations generated in the AI2THOR simulation [10] and 30 real-world failure demonstrations collected with a UR5e robot arm. We hope the dataset will encourage development of more explainable and robust embodied AI systems.</p>
<h1>2 Related Work</h1>
<p>Dense Video Captioning for human activity videos has been a challenging task in computer vision. Recent video captioning methods typically train transformer-based models to jointly localize and caption events in videos [11, 12, 13, 14, 15]. Yet it remains a challenge to caption robot videos due to a lack of data. With the emergence of large foundation models, zero-shot video captioning is made possible [16, 17, 18]. These works combine VLMs and LLMs to caption egocentric human activity videos in a zero-shot manner. Extending upon prior works, our approach generates captions that are task-centric and action-centric for temporal robot sensory data in a zero-shot manner, which helps downstream tasks such as robot behavior analysis [19].</p>
<p>Robot Failure Explanation is an important task long studied in the HRI community to increase human trust in robotic systems [20, 21] and allow non-expert users to better assist robots under failure scenarios [22, 23]. However, prior works are limited as each only address a specific set of failure scenarios: Das and Chernova [23] specifically study failure cases in picking, Diehl and Ramirez-Amaro [24] study two pick-and-place tasks, Song et al. [25] focus on object navigation failures whereas Inceoglu et al. [26] focus on detecting failures in a few short-horizon object manipulation tasks. By leveraging the advanced reasoning ability of LLM, our framework is able to detect and explain a wide range of failure scenarios without assumptions on the task configuration or failure type.</p>
<p>Task Planning with Large Language Models Large Language Models can be leveraged to decompose high-level, abstract task instructions into low-level step-by-step actions executable by agents [7, 9, 27, 28]. Recent works have also demonstrated the self-reflective and self-corrective ability of LLMs based on environment feedback [8, 29, 30, 31, 32]. However, they all assume ground truth environment feedback associated with one or few actions. In this work, we explore the reflective ability of LLMs directly on multisensory robot observations and its temporal reasoning ability on long-horizon robot task executions. Our framework is able to directly operate on real-world robot task execution data with various failure scenarios.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig 2: Hierarchical robot summary is composed of: a) a sensory-input summary that converts multisensory robot observations (RGB-D, sound, robot states) into task-informed scene graphs and audio summary; b) an event-based summary that generates captions for key event frames; c) a subgoal-based summary that contains the end frame of each subgoal.</p>
<h1>3 Method: the REFLECT Framework</h1>
<p>The REFLECT framework summarizes a robot's past experiences for failure explanation and correction. It contains three modules: a hierarchical robot summary module that summarizes multisensory robot data with three levels of abstraction (§3.1), a progressive failure explanation module that queries LLM to detect and explain the failure (§3.2), and finally a failure correction planner that generates an executable correction plan (§3.3).</p>
<h3>3.1 Hierarchical Robot Summary</h3>
<p>To perform effective failure explanation and correction, we propose a hierarchical summary structure to 1) aggregate and convert robot sensory data over time into a unified structure; 2) summarize the robot experiences for efficient failure localization and explanation. The hierarchical summary structure contains three levels: sensory-input summary, event-based summary, and subgoal-based summary.</p>
<h3>3.1.1 Sensory-Input Summary</h3>
<p>The sensory-input summary processes unstructured, multimodal robot sensory observations into a unified structure that stores necessary information for failure explanation.</p>
<p>Visual summary with task-informed scene graphs. To understand a robot's interactions with its surrounding environment, it is important to extract inter-object relations, robot-object relations, and object state information from the observations. Given the RGB-D observation at timestep $t$, we run object detection on the RGB image to obtain a semantic segmentation $I_{t}^{S}$ and project it to a 3D semantic point cloud using the observed depth. In addition, for objects that can change states (e.g. microwave can be turned on and off), we crop the image based on the object's detected bounding box and compute the cosine similarity of the CLIP embedding [33] between the cropped image and a pre-defined list of object state labels (see details in §B.3). Given the semantic point cloud, heuristics are applied to compute inter-object spatial relations for 8 commonly-used spatial relations ${ }^{1}$ : inside, on top of, on the left, on the right, above, below, occluding, and near. We also infer the robot-object relation from gripper state and object detection results. To aggregate the 3D point cloud over time frames, we use a similar approach as Li et al. [34] to align the newly observed point cloud $p_{t}$ with the accumulated point cloud from all previous time steps $P_{t-1}$ using 4 heuristic operations: add, update, replace, and delete. ${ }^{1}$
Once having the aggregated point cloud, we construct a scene graph $G_{t}=\left{N+\left{n_{\text {robot }}\right}, E\right}$, which describes the object nodes $(N)$ and their spatial relations $(E)$. Each node is defined as $n_{i}=\left(c_{i}, s_{i}\right)$, where $c_{i}$ is the object class, $s_{i}$ is the object state if any. Each edge contains the spatial relation between the two objects. We add the robot as an additional node $n_{\text {robot }}$ and a special relation "inside robot gripper". To make the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>summary succinct and reduce computation and memory cost, we only consider objects that are relevant to the task (as extracted from the original robot plan), and objects spatially related to the task-relevant objects.
Audio summary. Audio is a useful modality for failure reasoning as it provides immediate feedback of failure events (e.g. something drops from gripper to the ground) and detects state changes when visual cues are limited (e.g. stove burner turns on but occluded by an object on top).</p>
<p>Given an input audio stream, we first segment the whole audio clip into several sub-clips by filtering out ranges where the volume is below a certain threshold $\epsilon$. Then for each sub-clip $s$, we compute its audio-lingual embedding with a pre-trained audio-language model (e.g. AudioCLIP [35], Wav2CLIP [36]). We calculate the cosine similarity between the audio embedding and the CLIP embeddings for a list of candidate audio event labels $L$, where the highest-scoring label $l^{<em>}$ is selected: $l^{</em>}=\operatorname{argmax}<em 1="1">{l \in L}[C(s, l)], C=$ $\frac{f</em>(s)\right|}(s): f_{2}(l)}{\left|f_{1<em 2="2">{l} f</em>$ is the embedding function for text.}(l) \mid}$, where $f_{1}$ is the embedding function for audio, $f_{2</p>
<h1>3.1.2 Event-Based Summary</h1>
<p>Given that the sensory-input summary (§3.1.1) computes a scene graph for each frame and thus contains redundant information, the goal of the event-based summary is to select key frames and generate text captions from the corresponding scene graphs.</p>
<p>We design a key frame selection mechanism based on visual, audio, and robot states. More specifically, a frame is selected if it satisfies any of the conditions below: 1) The task-informed scene graph of the current frame $G_{t}$ is different from the previous frame $G_{t-1}$. 2) The frame is the start or end of an audio event. 3) The frame marks the end of a subgoal execution.</p>
<p>For each key frame, we convert the scene graph into text with the following format. ${ }^{2}$ When constructing the visual observation, we only consider objects that are visible in the current frame.</p>
<div class="codehilite"><pre><span></span><code><span class="k">[timestep] Action: [robot action]</span>
<span class="na">Visual observation</span><span class="o">:</span><span class="w"> </span><span class="s">object1 [state], object2, object3 [state] ...</span><span class="w"> </span><span class="c1"># objects and states</span>
<span class="na">object1 is [spatial relation] object2 ... # inter-object relations</span>
<span class="na">object3 is inside robot gripper. # robot-object relations</span>
<span class="na">Auditory observation</span><span class="o">:</span><span class="w"> </span><span class="s">[audio summary].</span>
</code></pre></div>

<h3>3.1.3 Subgoal-Based Summary</h3>
<p>The event-based summary (in §3.1.2) stores environment observations throughout the robot task execution. However, it's hard for LLM to infer the expected outcomes and identify failures for every one of the low-level actions. As a result, we introduce the subgoal-based summary, which consists of observations at the end of each subgoal, for LLM to identify misalignment between the robot execution outcomes and its high-level plan (e.g. move to the toaster, put bread slice in the toaster). The subgoal-based summary enables the failure explanation module to quickly process the robot experience summary by checking whether each subgoal is satisfied while ignoring low-level execution details. Once a failure is detected, relevant environment information stored in the event-based summary or sensory-input summary can be retrieved for detailed failure explanation.</p>
<h3>3.2 Progressive Failure Explanation</h3>
<p>The failure explanation algorithm should handle both execution and planning failures, where the former requires action-level observation details and the latter requires task-level information such as task description and robot plan. To do so, the algorithm first identifies the type of failure and then retrieves relevant information from the hierarchical summary to construct the query to LLM. As shown in Fig. 3, the algorithm first iterates through the subgoals and verifies success using the following prompt: ${ }^{3}$</p>
<p>The robot subgoal is [robot subgoal at time $t$ ]. Given [subgoal-based summary at time $t$ ]
Q: Is the subgoal satisfied? A: Yes</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig 3: Progressive failure explanation verifies success for each subgoal. If a subgoal fails, the algorithm enters the execution analysis stage for detailed explanation. If all subgoals are satisfied, the algorithm enters planning analysis stage to check errors in the robot plan.</p>
<p>The LLM is instructed to output 'Yes' or 'No'. If a subgoal is not achieved, then we retrieve history observations stored in event-based summary for failure explanation as follows:</p>
<p>The robot task is to [task name]. A failure is identified at $t$. Given [event-based summary up to $t$ ] Q: Briefly explain what happened at $t$ and what caused the failure?
A: At 00:44, the robot attempted to put the pot on the fourth stove burner, but the pot was not in its gripper. The failure was caused by the robot dropping the pot filled with water at 00:36 while moving to the fourth stove burner.</p>
<p>In case all subgoals are achieved but the task still failed, then it's likely that the plan itself was incorrect. We use the robot original plan and the final state of the environment to identify errors in the robot plan. The final state is obtained from the scene graph generated from the aggregated semantic point cloud in the last time step without view-specific relations (on the left, on the right, occluding).</p>
<p>The robot task is to [task name]. The task is successful if [goal state].
The robot plan is [original robot plan]. Given [final state]
Q: What's wrong with the robot plan that caused the robot to fail?
A: The robot placed the pot on the fourth stove burner but turned on the second stove burner, causing a mismatch between the pot's location and the active burner.
Q: Which time step is most relevant to the above failure?
A: $00: 49$</p>
<h1>3.3 Failure Correction Planner</h1>
<p>A failure correction planner should generate an executable plan for the robot to correct the failure and complete the task, starting from the final state of the original task execution. Prior work [22] has shown that good failure explanations help non-expert users better understand the failure and assist the robot. Analogously, we hypothesize that the failure explanation can also guide a language planner to generate a high-level correction plan that leads to task success. The prompt is formatted as below: ${ }^{3}$</p>
<p>The robot task is to [task name]. The original robot plan is [original robot plan].
Given [failure explanation] [final state] and [goal state]
Correction plan: toggle_off (stoveburner-2), toggle_on (stoveburner-4)
To make sure the plan generated by the language model is executable in the environment, we adopt the idea of Huang et al. [28] to map each LLM-generated action to its closest executable action in the task environment using a large pre-trained sentence embedding model.</p>
<h2>4 The RoboFail Dataset</h2>
<p>In simulation, we generate task execution data in AI2THOR and manually inject failures. The dataset contains a total of 100 failure scenarios, with 10 cases for each of the 10 tasks (see details in §B.1). We store the RGB-D observations, sound ( 20 classes in total), robot state data, as well as ground truth metadata obtained from simulation. The real-world dataset is collected by human teleoperation of a UR5e robot arm in a toy kitchen environment. The dataset contains 11 tasks with a total of 30 failure scenarios. We store the RGB-D observations (with Intel RealSense D415), recorded sound (with R@DE VideoMic Pro+), and robot proprioception data. A taxonomy of failure scenarios are visualized in Fig. 4.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig 4: RoboFail Failure Taxonomy</p>
<h1>5 Evaluation</h1>
<p>We systematically evaluate the ability of REFLECT to localize, explain and correct robot failures. In AI2THOR simulation, the agent interacts with the environment through action primitives, such as pick up, toggle on, move left. We assume the framework has access to ground truth object detection and state detection in simulation. We also evaluate the ability of the framework to summarize real-world robot sensory data. The real-world failure data is collected by a human teleoperating a UR5e robot arm to mimic robot policies according to a provided high-level plan. We use MDETR [37] for object detection, CLIP [33] for object state detection, and AudioCLIP [35] for sound detection. We use GPT-4 [38] as the LLM. The below metrics are evaluated in our experiments:</p>
<ul>
<li>Exp (explanation): percentage of predicted failure explanations that are correct and informative as determined by human evaluators ${ }^{4}$.</li>
<li>Loc (localization): percentage of predicted failure time that align with actual failure time. A predicted time is considered aligned if it falls within the annotated failure time ranges in the dataset.</li>
<li>Co-plan (correction planning success rate): percentage of tasks that succeed after executing the correction plan. Task success is determined by comparing the final state and the specified goal condition.
To demonstrate advantages of our framework, we compare with the following baselines/ablations:</li>
<li>BLIP2 caption: caption key frames with BLIP2, a state-of-the-art image caption model [39].</li>
<li>w/o sound: our approach without the audio modality.</li>
<li>w/o progressive: similar to open-ended Q\&amp;A in Socratic Models [16], directly query LLM for failure explanation given the robot summary without progressive failure explanation.</li>
<li>Subgoal only: using only subgoal-based summary for failure explanation.</li>
<li>LLM summary: convert all sensory-input summary to text and prompt LLM to summarize the text for failure explanation.</li>
<li>w/o explanation: query LLM for a correction plan without failure explanation.</li>
</ul>
<p>By evaluating REFLECT on the RoboFail dataset and comparing with the above baselines/ablations, we have the below findings:</p>
<h2>REFLECT is able to generate informative failure</h2>
<p>explanations that assist correction planning. Tab. 1 and 2 summarize the evaluation result, where REFLECT achieves the highest performance in explaining, localizing, and correcting failures in both simula-</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Execution failure</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Planning failure</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">Exp</td>
<td style="text-align: center;">Loc</td>
<td style="text-align: center;">Co-plan</td>
<td style="text-align: center;">Exp</td>
<td style="text-align: center;">Loc</td>
<td style="text-align: center;">Co-plan</td>
</tr>
<tr>
<td style="text-align: center;">w/o progressive</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">64.9</td>
</tr>
<tr>
<td style="text-align: center;">Subgoal only</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">75.4</td>
</tr>
<tr>
<td style="text-align: center;">LLM summary</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">57.9</td>
<td style="text-align: center;">54.4</td>
<td style="text-align: center;">66.7</td>
</tr>
<tr>
<td style="text-align: center;">w/o explanation</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">56.1</td>
</tr>
<tr>
<td style="text-align: center;">REFLECT</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">80.7</td>
</tr>
</tbody>
</table>
<p>Table 1: Result in Simulation Environments
tion and the real world. The performance slightly decreases in real world due to perception errors. We find that localization is slightly harder for planning failures as the failure is usually not associated with a single time step. In simulation, our framework achieves around $80 \%$ correction planning success rates for both failure types.</p>
<p>Audio data is useful for failure explanation. As shown in Tab. 2, the explanation and localization accuracy for [w/o sound] both decrease around $20 \%$ on execution failures as compared to REFLECT. This is because some unexpected events (e.g. object dropping on floor) or object states hard to identify using visual detectors (e.g. stove burner occluded by a pot on top) are easier to be detected through audio.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig 5: Qualitative results in simulation. Given a failed robot task execution, REFLECT is able to generate informative failure explanations for both execution and planning failures. Conditioned on the explanation, a language planner can generate a high-level plan for the robot to correct the failure and complete the task.</p>
<p>Task-relevant object spatial and state information is crucial. As shown in Tab. 2, [BLIP2 caption] achieves the worst performance in all scenarios because the captions generated by BLIP2 lack necessary information for failure explanation. In contrast, our zero-shot caption method is designed to capture environment information such as object states and spatial relations, which are task-relevant and crucial for failure explanation. The figure on the right shows that REFLECT is able to summarize object states such as "fridge (with door open)" and spatial relations such as "apple inside white bowl", which are not present in the BLIP2 caption.</p>
<p>Progressive failure explanation is important. Our progressive failure explanation algorithm leverages the hierarchical summary to first identify the failure with the subgoal-based summary, and then query LLM for failure explanation accordingly. Comparing to [w/o progressive] in Tab. 1 and 2, the progressive algorithm helps with more accurate localization and informative explanation. To better understand the difference, a qualitative example is shown in Fig. 6: although [w/o progressive] mentions that the task failed because the robot did not have an egg in its gripper to put in the pan, it does not reason why the egg was not present. In contrast, REFLECT identifies failure in the "pick up egg" action and then queries the event-based summary to infer the exact failure cause - "fridge was closed".</p>
<p>Hierarchical structure is important. The performance decrease of [Subgoal only] shows the importance of event-based summary as it stores intermediate environment observations that are useful for failure explanation. Consider the scenario when the robot accidentally dropped the pot it was holding when moving to stove burner. [Subgoal only] only infers that the object was not in the robot gripper at the end of the subgoal execution. In contrast, the event-based summary stores auditory observation of "something drops on ground" and visual observation of "nothing is inside robot gripper", which helps identify that the pot was dropped and the exact time step the failure occurred.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Execution failure</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Planning failure</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">Exp</td>
<td style="text-align: center;">Loc</td>
<td style="text-align: center;">Exp</td>
<td style="text-align: center;">Loc</td>
</tr>
<tr>
<td style="text-align: center;">BLIP2 caption</td>
<td style="text-align: center;">6.25</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">57.1</td>
</tr>
<tr>
<td style="text-align: center;">w/o sound</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">78.6</td>
</tr>
<tr>
<td style="text-align: center;">w/o progressive</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">78.6</td>
</tr>
<tr>
<td style="text-align: center;">Subgoal only</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">78.6</td>
</tr>
<tr>
<td style="text-align: center;">LLM summary</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">71.4</td>
</tr>
<tr>
<td style="text-align: center;">REFLECT</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">78.6</td>
</tr>
</tbody>
</table>
<p>Table 2: Result in Real-world Environments
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Table 2: Result in Real-world Environments
w/o progressive: The robot failed to cook an egg because it did not have the egg in its gripper when attempting to crack it and put it in the pan. The visual observations show that the robot's gripper was empty during these actions.</p>
<p>REFLECT: In 20 14, the robot attempted to pick up an egg while the fridge was still closed, causing the failure. The robot should have opened the fridge before trying to pick up the egg.</p>
<p>REFLECT: In 20 14, the robot attempted to pick up an egg while the fridge was still closed, causing the failure. The robot should have opened the fridge before trying to pick up the egg.</p>
<p>Fig 6: [w/o progressive] vs. [LLM summary] vs. Ours</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig 7: Qualitative results in real world. REFLECT is able to summarize and generate informative failure explanations for real-world robot executions. The above figure shows three failed task executions on the left, the corresponding scene graph and caption for one key frame in the middle, and the LLM-generated failure explanation on the right.
In addition, the hierarchical structure is a better way to condense the sensory-input summary for failure explanation. We implement an alternative [LLM summary], which prompts LLM to summarize the sensory-input summary. The performance decreases significantly in both simulation and real world as the LLM-generated summary loses information that is relevant for failure explanation. As shown in Fig. 6, the summary and explanation of [LLM summary] only mention that the robot tried to pick up the egg and the gripper remained empty, but did not mention that the fridge was closed, which is the actual failure cause.</p>
<h1>Failure explanation helps correction. From</h1>
<p>Tab. 1, we observe that the correction planning success rate significantly decreases for [w/o explanation]. This is because the failure explanation can guide LLM to generate a correction plan based on the failure cause. As shown in Fig. 8, given the failure reason that the mug cannot be put inside the coffee machine due to presence of a cup, REFLECT generates a plan to move the cup away and then proceed with the task. Whereas [w/o explanation] simply repeats the original plan without taking any actions to address the cause of failure.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Explanation: At 00:48, the robot attempted to put the mug into the coffee machine while there was already a cup inside the machine, causing a failure due to the occupied space.</th>
<th style="text-align: center;">w/o explanation: <br> 1. pick_up (mug) <br> 2. put_on (mug, sink basin) <br> 3. toggle_on (faucet) <br> 4. toggle_off (faucet) <br> 5. pick_up (mug) <br> 6. pour_liquid_from (mug, sink) <br> 7. put_in (mug, coffee machine) <br> 8. toggle_on (coffee machine) <br> 9. toggle_off (coffee machine) <br> 10. pick_up (mug) <br> 11. put_on (mug, countertop)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Limitations. There are a few limitations in the method used to convert sensory data into textual summary. Even though the heuristics used to generate scene graphs is sufficient for scenarios studied in the paper, it may fall short in more complex environments. Either training a large spatial reasoning model or fine-tuning an existing model on robotics data could be a promising solution [40, 41, 42]. In addition, the object state detection method assumes a given list of candidate object states, which can be potentially relaxed by a method (e.g. prompting a LLM) that output possible states given the object category.</p>
<p>The framework also assumes the rest of the environment will remain static throughout the robot task execution. Finally, given the information (object detection, object states, spatial relations) that the robot summary contains, it is less effective for handling low-level control failures. Future work may consider developing better perception methods that capture more low-level state information.</p>
<h2>6 Conclusion</h2>
<p>We propose a framework, REFLECT, which converts multisensory observations into a hierarchical summary of robot past experiences and queries LLM progressively for failure explanation. The generated explanation can then guide a language planner to correct the failure and complete the task. To evaluate the framework, we create a dataset of robot failed executions in both simulation and real world and show that REFLECT achieves better performance as compared to several baselines and ablations. We encourage future work to extend upon the framework and explore more use cases of the robot summary.</p>
<h1>Acknowledgments</h1>
<p>We would like to thank Cheng Chi and Zhenjia Xu for their help in setting up real world experiments, and Huy Ha, Mandi Zhao, Samir Gadre, Mengda Xu, Dominik Bauer for valuable discussions and feedback. This work was supported in part by NSF Award #2143601, #2037101, and #2132519. We would like to thank Google for the UR5 robot hardware. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the sponsors.</p>
<h2>References</h2>
<p>[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[2] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.
[3] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[4] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.
[5] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.
[6] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.
[7] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, A. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. J. Ruano, K. Jeffrey, S. Jesmonth, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, K.-H. Lee, S. Levine, Y. Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao, J. Rettinghouse, D. Reyes, P. Sermanet, N. Sievers, C. Tan, A. Toshev, V. Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu, M. Yan, and A. Zeng. Do as i can and not as i say: Grounding language in robotic affordances. In arXiv preprint arXiv:2204.01691, 2022.
[8] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, P. Sermanet, N. Brown, T. Jackson, L. Luu, S. Levine, K. Hausman, and B. Ichter. Inner monologue: Embodied reasoning through planning with language models. In arXiv preprint arXiv:2207.05608, 2022.
[9] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg. Progprompt: Generating situated robot task plans using large language models. arXiv preprint arXiv:2209.11302, 2022.
[10] E. Kolve, R. Mottaghi, W. Han, E. VanderBilt, L. Weihs, A. Herrasti, M. Deitke, K. Ehsani, D. Gordon, Y. Zhu, et al. Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474, 2017.
[11] T. Wang, R. Zhang, Z. Lu, F. Zheng, R. Cheng, and P. Luo. End-to-end dense video captioning with parallel decoding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6847-6857, 2021.
[12] C. Deng, S. Chen, D. Chen, Y. He, and Q. Wu. Sketch, ground, and refine: Top-down dense video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 234-243, June 2021.</p>
<p>[13] Q. Zhang, Y. Song, and Q. Jin. Unifying event detection and captioning as sequence generation via pre-training. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXVI, pages 363-379. Springer, 2022.
[14] W. Zhu, B. Pang, A. V. Thapliyal, W. Y. Wang, and R. Soricut. End-to-end dense video captioning as sequence generation. In Proceedings of the 29th International Conference on Computational Linguistics, pages 5651-5665, 2022.
[15] A. Yang, A. Nagrani, P. H. Seo, A. Miech, J. Pont-Tuset, I. Laptev, J. Sivic, and C. Schmid. Vid2seq: Large-scale pretraining of a visual language model for dense video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10714-10726, 2023.
[16] A. Zeng, M. Attarian, B. Ichter, K. Choromanski, A. Wong, S. Welker, F. Tombari, A. Purohit, M. Ryoo, V. Sindhwani, J. Lee, V. Vanhoucke, and P. Florence. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv, 2022.
[17] Z. Wang, M. Li, R. Xu, L. Zhou, J. Lei, X. Lin, S. Wang, Z. Yang, C. Zhu, D. Hoiem, et al. Language models with image descriptors are strong few-shot video-language learners. arXiv preprint arXiv:2205.10747, 2022.
[18] R.-G. Pasca, A. Gavryushin, Y.-L. Kuo, O. Hilliges, and X. Wang. Summarize the past to predict the future: Natural language descriptions of context boost multimodal object interaction. arXiv preprint arXiv:2301.09209, 2023.
[19] C. DeChant and D. Bauer. Summarizing a virtual robot's past actions in natural language. arXiv preprint arXiv:2203.06671, 2022.
[20] P. Khanna, E. Yadollahi, M. Björkman, I. Leite, and C. Smith. User study exploring the role of explanation of failures by robots in human robot collaboration tasks. arXiv preprint arXiv:2303.16010, 2023.
[21] S. Ye, G. Neville, M. Schrum, M. Gombolay, S. Chernova, and A. Howard. Human trust after robot mistakes: Study of the effects of different forms of robot communication. In 2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN), pages 1-7, 2019. doi:10.1109/RO-MAN46459.2019.8956424.
[22] D. Das, S. Banerjee, and S. Chernova. Explainable ai for robot failures: Generating explanations that improve user assistance in fault recovery. In Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction, pages 351-360, 2021.
[23] D. Das and S. Chernova. Semantic-based explainable ai: Leveraging semantic scene graphs and pairwise ranking to explain robot failures. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 3034-3041. IEEE, 2021.
[24] M. Diehl and K. Ramirez-Amaro. Why did i fail? a causal-based method to find explanations for robot failures. IEEE Robotics and Automation Letters, 7(4):8925-8932, 2022.
[25] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and Y. Su. Llm-planner: Few-shot grounded planning for embodied agents with large language models. arXiv preprint arXiv:2212.04088, 2022.
[26] A. Inceoglu, E. E. Aksoy, A. C. Ak, and S. Sariel. Fino-net: A deep multimodal sensor fusion framework for manipulation failure detection. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 6841-6847. IEEE, 2021.
[27] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code as policies: Language model programs for embodied control. arXiv preprint arXiv:2209.07753, 2022.</p>
<p>[28] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pages 9118-9147. PMLR, 2022.
[29] S. S. Raman, V. Cohen, E. Rosen, I. Idrees, D. Paulius, and S. Tellex. Planning with large language models via corrective re-prompting. arXiv preprint arXiv:2211.09935, 2022.
[30] N. Shinn, B. Labash, and A. Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.
[31] M. Skreta, N. Yoshikawa, S. Arellano-Rubach, Z. Ji, L. B. Kristensen, K. Darvish, A. Aspuru-Guzik, F. Shkurti, and A. Garg. Errors are useful prompts: Instruction guided task programming with verifier-assisted iterative prompting. arXiv preprint arXiv:2303.14100, 2023.
[32] T. Silver, S. Dan, K. Srinivas, J. B. Tenenbaum, L. P. Kaelbling, and M. Katz. Generalized planning in pddl domains with pretrained large language models. arXiv preprint arXiv:2305.11014, 2023.
[33] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.
[34] X. Li, D. Guo, H. Liu, and F. Sun. Embodied semantic scene graph generation. In A. Faust, D. Hsu, and G. Neumann, editors, Proceedings of the 5th Conference on Robot Learning, volume 164 of Proceedings of Machine Learning Research, pages 1585-1594. PMLR, 08-11 Nov 2022.
[35] A. Guzhov, F. Raue, J. Hees, and A. Dengel. Audioclip: Extending clip to image, text and audio. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 976-980. IEEE, 2022.
[36] H.-H. Wu, P. Seetharaman, K. Kumar, and J. P. Bello. Wav2clip: Learning robust audio representations from clip. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4563-4567. IEEE, 2022.
[37] A. Kamath, M. Singh, Y. LeCun, G. Synnaeve, I. Misra, and N. Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1780-1790, 2021.
[38] OpenAI. Gpt-4 technical report, 2023.
[39] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.
[40] X. Li, D. Guo, H. Liu, and F. Sun. Embodied semantic scene graph generation. In Conference on Robot Learning, pages 1585-1594. PMLR, 2022.
[41] F. Liu, G. Emerson, and N. Collier. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 11:635-651, 2023.
[42] A. Kurenkov, M. Lingelbach, T. Agarwal, E. Jin, C. Li, R. Zhang, L. Fei-Fei, J. Wu, S. Savarese, and R. Martın-Martın. Modeling dynamic environments with scene graph memory. In International Conference on Machine Learning, pages 17976-17993. PMLR, 2023.</p>
<h1>Appendix</h1>
<h2>A Method Details</h2>
<h2>A. 1 Spatial Relation Heuristics</h2>
<p>We implement a total of 8 object spatial relation heuristics given the aggregated semantic point cloud and 3D object bounding boxes. We consider two in-contact relations when the minimum distance between the object point clouds is smaller than 5 cm :</p>
<ol>
<li>Inside. Object A is considered inside object B if over $50 \%$ percent of object A's point cloud is inside the convex hull of object B.</li>
<li>On top of. Object A is considered on top of object B if it satisfies the following two conditions: a) over $70 \%$ of object A's point cloud lies within a XY-plane projected from object B's 3D bounding box. b) over $70 \%$ of object A's points are above the upper Z bound of object B's bounding box.</li>
</ol>
<p>If the object point cloud distance is larger than 5 cm but smaller than 40 cm , we subtract the center points of object A and B's 3D bounding box, transform it back to camera coordinates (Y-up) and normalize to get a 3 dimensional unit vector. The following relations are considered:</p>
<ol>
<li>Above \&amp; Below. If the y-component of the vector is over 0.9 , then object A is above object B . If the y -component is smaller than -0.9 , then object A is below object B .</li>
<li>On the left \&amp; On the right. If the x -component of the vector is over 0.8 , then object A is on the right of object B. If the x -component is smaller than -0.8 , then object A is on the left of object B .</li>
<li>Occluding. If $90 \%$ of object A's points have a smaller depth than object B's minimum depth and object A's projected 2D bounding box (projected in current image space from its aggregated 3D point cloud) overlaps more than $50 \%$ with that of object B's projected bounding box, then object B is considered occluding object A .</li>
<li>Near. If none of the above relations satisfy but the object distance is smaller than 10 cm , then object A is near object B .</li>
</ol>
<h2>A. 2 Scene Graph Aggregation Heuristics</h2>
<p>We aggregate the 3D point cloud over time frames with a similar approach as Li et al. [34]. Consider the point cloud observed in the current time step $p_{t}$ and the accumulated point cloud from all previous time steps $P_{t-1}$, we can obtain the accumulated point cloud $P_{t}$ up to time step $t$ with 4 operations: ADD, UPDATE, REPLACE, and DELETE.</p>
<ol>
<li>ADD. If an object is observed in $p_{t}$ but not in $P_{t-1}$, we consider it a newly appeared object and add it as a new node. If the node is task-relevant, then its relations with all existing objects will be computed and added as edges. If the node is not task-relevant, then its relations with existing task-relevant objects will be computed and added as edges.</li>
<li>UPDATE. If an object is in both $p_{t}$ and $P_{t-1}$, and the object point cloud is aligned, we update the object point cloud by concatenating the newly observed points and updating its existing edges with other objects by recomputing the spatial relations.</li>
<li>REPLACE. If an object is in both $p_{t}$ and $P_{t-1}$, but the misalignment of the object point clouds is larger than a threshold $d$, indicating the object has moved. We remove the old node in $P_{t-1}$ and add the object as a new node.</li>
<li>DELETE. If an object is in $P_{t-1}$ but not in $p_{t}$, and the robot is interacting with the object, then we remove the object from the accumulated point cloud since its location becomes unknown.</li>
</ol>
<h1>A. 3 Example Prompts</h1>
<h2>A.3.1 Subgoal Verification</h2>
<p>system prompt:
You are a success verifier that outputs 'Yes' or 'No' to indicate whether the robot goal is satisfied given the robot observations.
user prompt:
The robot goal is to [SUBGOAL]. Here are the robot observations after execution: [OBSERVATION]
Q: Is the goal satisfied?
A: Yes
The [SUBGOAL] and [OBSERVATION] entry will be filled in when prompting for each subgoal. Here's a complete example of a failed execution during the task 'boil water'. The robot accidentally dropped the pot when navigating to the stove burner and then attempted to put the pot on the stove burner:</p>
<p>The robot goal is to pick up pot. Here are the robot observations after execution:
Visual observation: pot (empty and clean). pot is inside robot gripper.
Q: Is the goal satisfied?
A: Yes
The robot goal is to put pot in sink. Here are the robot observations after execution:
Visual observation: faucet (turned off), pot (empty and clean), sink. pot (empty and clean) is inside sink. pot (empty and clean) is on the right of soap bottle. nothing is inside robot gripper.
Q: Is the goal satisfied?
A: Yes
The robot goal is to toggle on faucet. Here are the robot observations after execution:
Visual observation: pot (filled with water and clean), faucet (turned on), sink. pot (filled with water and clean) is inside sink. pot (filled with water and clean) is on the right of soap bottle. nothing is inside robot gripper. Auditory observation: water runs in sink.
Q: Is the goal satisfied?
A: Yes
The robot goal is to toggle off faucet. Here are the robot observations after execution:
Visual observation: pot (filled with water and clean), faucet (turned off), sink. pot (filled with water and clean) is inside sink. pot (filled with water and clean) is on the right of soap bottle. nothing is inside robot gripper.
Q: Is the goal satisfied?
A: Yes
The robot goal is to pick up pot. Here are the robot observations after execution:
Visual observation: pot (filled with water and clean), faucet (turned off), sink. pot is inside robot gripper.
Q: Is the goal satisfied?
A: Yes
The robot goal is to put pot on fourth stove burner. Here are the robot observations after execution:
Visual observation: second stove burner (turned off), first stove burner (turned off), third stove burner (turned off), fourth stove burner (turned off). nothing is inside robot gripper.
Q: Is the goal satisfied?
A: No
Here a subgoal is not satisfied, the program enters the execution analysis mode: history observations stored in the event-based summary are retrieved to query LLM for failure explanation.</p>
<h1>A.3.2 Failure explanation: execution analysis</h1>
<h2>system prompt:</h2>
<p>You are expected to provide explanation for a robot failure. You are given the robot actions and observations so far. Briefly explain the failure in 1-2 sentence. Mention relevant time steps if possible.
user prompt:
The robot task is to boil water. At 00:44, a failure was identified.
[Robot actions and observations before 00:44]
00:01. Action: Move to pot. Visual observation: nothing is inside robot gripper.
00:11. Action: Move to pot. Visual observation: faucet (turned off), nothing is inside robot gripper.
00:15. Action: Move to pot. Visual observation: pot (empty and clean), pot (empty and clean) is on the left of potato. pot (empty and clean) is on top of third countertop. nothing is inside robot gripper.
00:18. Action: Pick up pot. Visual observation: pot (empty and clean), pot is inside robot gripper.
00:21. Action: Move to sink. Visual observation: pot (empty and clean), faucet (turned off), sink. pot is inside robot gripper.
00:22. Action: Move to sink. Visual observation: pot (empty and clean), faucet (turned off), sink. pot is inside robot gripper.
00:25. Action: Put pot in sink. Visual observation: pot (empty and clean), faucet (turned off), sink. pot (empty and clean) is inside sink. pot (empty and clean) is on the right of soap bottle. nothing is inside robot gripper.
00:28. Action: Toggle on faucet. Visual observation: pot (filled with water and clean), faucet (turned on), sink. pot (filled with water and clean) is inside sink. pot (filled with water and clean) is on the right of soap bottle. nothing is inside robot gripper. Auditory observation: water runs in sink.
00:31. Action: Toggle off faucet. Visual observation: pot (filled with water and clean), faucet (turned off), sink. pot (filled with water and clean) is inside sink. pot (filled with water and clean) is on the right of soap bottle. nothing is inside robot gripper.
00:34. Action: Pick up pot. Visual observation: pot (filled with water and clean), faucet (turned off), sink. pot is inside robot gripper.
00:36. Action: Move to fourth stove burner. Visual observation: pot (empty and clean), faucet (turned off), sink. pot (empty and clean) is on the right of potato. pot (empty and clean) is on top of third countertop. nothing is inside robot gripper. Auditory observation: something drops.
00:38. Action: Move to fourth stove burner. Visual observation: pot (empty and clean), faucet (turned off), sink. nothing is inside robot gripper.
00:42. Action: Move to fourth stove burner. Visual observation: second stove burner (turned off), fourth stove burner (turned off), third stove burner (turned off), first stove burner (turned off). nothing is inside robot gripper.
00:43. Action: Move to fourth stove burner. Visual observation: second stove burner (turned off), fourth stove burner (turned off), third stove burner (turned off), first stove burner (turned off). nothing is inside robot gripper.
[Observation at the end of 00:44]
Action: Put pot on fourth stove burner. Visual observation: second stove burner (turned off), fourth stove burner (turned off), third stove burner (turned off), first stove burner (turned off). nothing is inside robot gripper.</p>
<p>Q: Infer from [Robot actions and observations before 00:44] or [Observation at the end of 00:44], briefly explain what happened at 00:44 and what caused the failure.
A: At 00:44, the robot attempted to put the pot on the fourth stove burner, but the pot was not in its gripper. The failure was caused by the robot dropping the pot filled with water at 00:36 while moving to the fourth stove burner.</p>
<p>The failure steps 00:36 and 00:44 can be extracted from the answer by prompting LLM to extract time steps from the output failure explanation.</p>
<h2>A.3.3 Failure explanation: planning analysis</h2>
<p>In case all subgoals are satisfied, then there's likely mistakes in the robot original plan. The program will enter planning analysis mode. Take the failure scenario when the robot plan is wrong during the task 'boil water' as the robot placed the pot on one stove burner but toggled on another:
system prompt:</p>
<p>You are expected to provide explanation for a robot failure. You are given the current robot state, the goal condition, and the robot plan. Briefly explain what was wrong with the robot plan in 1-2 sentence.
user prompt:
The robot task is to boil water. The task is considered successful if a pot is filled with water, the pot is on top of a stove burner that is turned on.
Here's the robot observation at the end of the task execution:
faucet (turned off), second stove burner (turned on), sink, pot (filled with water and clean), fourth stove burner (turned off), third stove burner (turned off), first stove burner (turned off), pot (filled with water and clean) is on top of fourth stove burner (turned off). nothing is inside robot gripper.
The robot plan is:
00:18. Goal: Pick up pot.
00:25. Goal: Put pot in sink.
00:28. Goal: Toggle on faucet.
00:31. Goal: Toggle off faucet.
00:34. Goal: Pick up pot.
00:46. Goal: Put pot on stove burner.
00:49. Goal: Toggle on stove burner.
Q: Known that all actions in the robot plan were executed successfully, what's wrong with the robot plan that caused the robot to fail?
A: The robot placed the pot on the fourth stove burner but turned on the second stove burner, causing a mismatch between the pot's location and the active burner.</p>
<p>The failure time step can be obtained by a follow-up query to the LLM with the prompt below:
Q: Which time step is most relevant to the above failure?
A: $00: 49$</p>
<h1>A.3.4 Correction</h1>
<p>Still take the failure scenario when the robot plan is wrong so that the robot placed the pot on one stove burner but toggled on another. A complete prompt for generating the failure correction plan is as follows:
system prompt:
Provide a plan with the available actions for the robot to recover from the failure and finish the task.
Available actions: pick up, put in some container, put on some receptacle, open (e.g. fridge), close, toggle on (e.g. faucet), toggle off, slice object, crack object (e.g. egg), pour (liquid) from A to B. The robot can only hold one object in its gripper, in other words, if there's object in the robot gripper, it can no longer pick up another object.
The plan should 1) not contain any if statements 2) contain only the available actions 3) resemble the format of the initial plan.
user prompt:
Task: boil water
Initial plan:</p>
<ol>
<li>pick_up (pot)</li>
<li>put_in (pot, sink)</li>
<li>toggle_on (faucet)</li>
<li>toggle_off (faucet)</li>
<li>pick_up (pot)</li>
<li>put_on (pot, stove burner)</li>
<li>toggle_on (stove burner)</li>
</ol>
<p>Failure reason: The robot placed the pot on the fourth stove burner but turned on the second stove burner, causing a mismatch between the pot's location and the active burner.
Current state: sink, pot (filled with water and clean), fourth stove burner (turned off), third stove burner (turned off), faucet (turned off), first stove burner (turned off), second stove burner (turned on). pot (filled with water and clean) is on top of fourth stove burner (turned off). nothing is inside robot gripper.
Success state: a pot is filled with water, the pot is on top of a stove burner that is turned on.
Correction plan: toggle_off (stoveburner-2), toggle_on (stoveburner-4)</p>
<h1>B Evaluation Details</h1>
<h2>B. 1 Dataset Details</h2>
<p>Descriptions for the 10 simulation tasks and 11 real-world tasks in the RoboFail dataset are shown below.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Task Description / Goal State</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">boil water</td>
<td style="text-align: left;">A pot is filled with water, the pot is on top of a stove burner that is turned on</td>
</tr>
<tr>
<td style="text-align: left;">toast bread</td>
<td style="text-align: left;">A bread slice is inside a toaster that is turned on</td>
</tr>
<tr>
<td style="text-align: left;">fry egg</td>
<td style="text-align: left;">A cracked egg is in a pan, the pan is on top a stove burner that is turned on</td>
</tr>
<tr>
<td style="text-align: left;">heat potato</td>
<td style="text-align: left;">A potato is on a plate and inside a microwave that is turned on</td>
</tr>
<tr>
<td style="text-align: left;">serve coffee</td>
<td style="text-align: left;">A clean mug is filled with coffee and on top of the countertop</td>
</tr>
<tr>
<td style="text-align: left;">store egg</td>
<td style="text-align: left;">A bowl with an egg is stored inside the fridge</td>
</tr>
<tr>
<td style="text-align: left;">make salad</td>
<td style="text-align: left;">A bowl of sliced lettuce, tomato and potato is stored inside the fridge</td>
</tr>
<tr>
<td style="text-align: left;">water plant</td>
<td style="text-align: left;">The house plant is filled with water</td>
</tr>
<tr>
<td style="text-align: left;">switch devices</td>
<td style="text-align: left;">Laptop is closed on the TV stand and the television is turned on</td>
</tr>
<tr>
<td style="text-align: left;">serve warm water</td>
<td style="text-align: left;">A mug of water is heated in the microwave and served on the dining table</td>
</tr>
</tbody>
</table>
<p>Table 3: Simulation Tasks</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Task Description / Goal State</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">boil water</td>
<td style="text-align: left;">A pot is filled with water, the pot is on top of a stove burner that is turned on</td>
</tr>
<tr>
<td style="text-align: left;">sauté carrot</td>
<td style="text-align: left;">A sliced carrot is inside a pan, the pan is on top of a stove burner that is turned on</td>
</tr>
<tr>
<td style="text-align: left;">heat potato</td>
<td style="text-align: left;">A potato is heated in the microwave and then put on the countertop</td>
</tr>
<tr>
<td style="text-align: left;">serve coffee</td>
<td style="text-align: left;">A mug is filled with coffee and on top of the countertop</td>
</tr>
<tr>
<td style="text-align: left;">store egg</td>
<td style="text-align: left;">A bowl with an egg is stored inside the fridge</td>
</tr>
<tr>
<td style="text-align: left;">secure objects</td>
<td style="text-align: left;">Knife is stored in a drawer and pear is stored in the fridge</td>
</tr>
<tr>
<td style="text-align: left;">apple in bowl</td>
<td style="text-align: left;">Apple is inside bowl</td>
</tr>
<tr>
<td style="text-align: left;">pear in drawer</td>
<td style="text-align: left;">Pear is inside a closed drawer</td>
</tr>
<tr>
<td style="text-align: left;">cut carrot</td>
<td style="text-align: left;">Carrot is sliced</td>
</tr>
<tr>
<td style="text-align: left;">fruits in bowl</td>
<td style="text-align: left;">All visible fruits are inside a bowl</td>
</tr>
<tr>
<td style="text-align: left;">heat pot</td>
<td style="text-align: left;">A pot is on top of a stove burner that is turned on</td>
</tr>
</tbody>
</table>
<p>Table 4: Real-world Tasks</p>
<h2>B. 2 Comparison with GPT-3.5</h2>
<p>We use GPT-4 for evaluation in our experiments. Here we provide a comparison of performance with the more accessible GPT-3.5. We have observed that GPT-4 exceeds GPT-3.5 in terms of both failure reasoning and planning abilities. The failure localization accuracy decreases as GPT-3.5 is less capable than GPT-4 to process irrelevant information in the summary and thus often wrongly localize the failure. The correction planning success rate with GPT-3.5 decreases more significantly due to the combined factors of less accurate failure explanations and less planning ability even given correct failure explanations. We also show one qualitative example of the failure explanations generated by GPT-4 and GPT-3.5, which shows that GPT-4 is better at reasoning the root cause of the failure than GPT-3.5:</p>
<p>GPT-3.5: The robot plan failed because the robot did not successfully put the bread slice in the toaster, even though it successfully turned on the toaster.
GPT-4: The robot plan failed because it turned on the toaster before putting the bread slice inside it, resulting in the bread slice being placed on top of the toaster instead of inside it.</p>
<h1>B. 3 Object states</h1>
<p>We list the object state candidates considered for all objects that can change states in our experiments. In general, the states are assigned based on object properties.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Object Type</th>
<th style="text-align: left;">Actionable Properties</th>
<th style="text-align: left;">Simulation States</th>
<th style="text-align: left;">Real-world States</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Pot</td>
<td style="text-align: left;">Fillable, Dirtyable</td>
<td style="text-align: left;">filled, empty, dirty, clean</td>
<td style="text-align: left;">filled with water, empty</td>
</tr>
<tr>
<td style="text-align: left;">Faucet</td>
<td style="text-align: left;">Toggleable</td>
<td style="text-align: left;">toggled on, toggled off</td>
<td style="text-align: left;">toggled on, toggled off</td>
</tr>
<tr>
<td style="text-align: left;">StoveBurner</td>
<td style="text-align: left;">Toggleable</td>
<td style="text-align: left;">toggled on, toggled off</td>
<td style="text-align: left;">toggled on, toggled off</td>
</tr>
<tr>
<td style="text-align: left;">Bread</td>
<td style="text-align: left;">Sliceable</td>
<td style="text-align: left;">sliced, unsliced</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">Toaster</td>
<td style="text-align: left;">Toggleable</td>
<td style="text-align: left;">toggled on, toggled off</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">Fridge</td>
<td style="text-align: left;">Openable</td>
<td style="text-align: left;">open, closed</td>
<td style="text-align: left;">open, closed</td>
</tr>
<tr>
<td style="text-align: left;">Pan</td>
<td style="text-align: left;">Dirtyable</td>
<td style="text-align: left;">dirty, clean</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">Egg</td>
<td style="text-align: left;">Sliceable, Breakable</td>
<td style="text-align: left;">sliced, unsliced, cracked, uncracked</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">Potato</td>
<td style="text-align: left;">Sliceable, Cookable</td>
<td style="text-align: left;">sliced, unsliced, cooked, uncooked</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">Plate</td>
<td style="text-align: left;">Breakable (Some), Dirtyable</td>
<td style="text-align: left;">broken, dirty, clean</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">Microwave</td>
<td style="text-align: left;">Toggleable, Openable</td>
<td style="text-align: left;">toggled on, toggled off, open, closed</td>
<td style="text-align: left;">toggled on, toggled off, open, closed</td>
</tr>
<tr>
<td style="text-align: left;">Mug</td>
<td style="text-align: left;">Fillable, Breakable, Dirtyable</td>
<td style="text-align: left;">filled, empty, broken, dirty, clean</td>
<td style="text-align: left;">filled with coffee, empty</td>
</tr>
<tr>
<td style="text-align: left;">CoffeeMachine</td>
<td style="text-align: left;">Toggleable</td>
<td style="text-align: left;">toggled on, toggled off</td>
<td style="text-align: left;">toggled on, toggled off</td>
</tr>
<tr>
<td style="text-align: left;">HousePlant</td>
<td style="text-align: left;">Fillable</td>
<td style="text-align: left;">watered, not watered</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">Tomato</td>
<td style="text-align: left;">Sliceable</td>
<td style="text-align: left;">sliced, unsliced</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">Lettuce</td>
<td style="text-align: left;">Sliceable</td>
<td style="text-align: left;">sliced, unsliced</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">Bowl</td>
<td style="text-align: left;">Fillable, Breakable (Some), Dirtyable</td>
<td style="text-align: left;">filled, empty, broken, dirty, clean</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">Laptop</td>
<td style="text-align: left;">Openable, Toggleable, Breakable</td>
<td style="text-align: left;">open, closed, toggled on, toggled off, broken</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">Television</td>
<td style="text-align: left;">Toggleable, Breakable</td>
<td style="text-align: left;">toggled on, toggled off, broken</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">Carrot</td>
<td style="text-align: left;">Sliceable</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">sliced, unsliced</td>
</tr>
<tr>
<td style="text-align: left;">Drawer</td>
<td style="text-align: left;">Openable</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">open, closed</td>
</tr>
</tbody>
</table>
<p>Table 6: Object States</p>
<h2>B. 4 Human Evaluation</h2>
<p>Similar to the approach for human evaluation in Ahn et al. [7], we ask 2 groups of users, 3 in each group to compare the ground truth failure explanation labelled in the dataset and REFLECT-generated failure explanation for each failure scenario. The failure scenarios are randomly shuffled in the questionnaires sent to the users. The users are instructed to score 0 if the predicted explanation is incorrect, 1 if the predicted explanation is correct, and 2 if they are unsure. The final score reflected in the tables are the majority vote without counting "unsure". If there's a tie in the answers or more than one "unsure" is given, we will ask the users to re-score the specific case.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ Details can be found in appendix.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
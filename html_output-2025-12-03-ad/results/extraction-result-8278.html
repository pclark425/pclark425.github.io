<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8278 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8278</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8278</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-258762841</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2023.findings-acl.441.pdf" target="_blank">Distilling Reasoning Capabilities into Smaller Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models. In this work, we propose an alternative reasoning scheme, Socratic CoT, that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. We use Socratic CoT to train a combination of two small distilled models: a problem decomposer and a subproblem solver. In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems. On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies boosts the performance of smaller models over 70% compared to the baselines. Finally, we investigate when Socratic CoT is an effective alternative to CoT, demonstrating cases where a much smaller model (GPT-2 large) can outperform a 10X larger model (GPT-3 6B). Our code is available here: https://github.com/kumar-shridhar/Distiiling-LM</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8278.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8278.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that elicits step-by-step intermediate reasoning traces from large language models to improve multi-step problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-Thought (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Implemented as few-shot or prompted generation of intermediate reasoning steps (auto-regressive text) from a large language model (used here via GPT-3 prompts or LLM-generated annotations).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (CoT) step-by-step intermediate reasoning']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Models are prompted (or finetuned) to output intermediate reasoning steps (a sequence of explanatory/calculation steps) leading to the final answer; these intermediate steps are used as supervision for student models or as part of the prompt at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared fine-tuning on final answers only vs. fine-tuning with CoT-generated step-by-step annotations (LLM-generated) and vs. GT step-by-step annotations where available (e.g., GSM8K). Also compared CoT prompting vs. Socratic CoT prompting for GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K, StrategyQA, SVAMP/ASDiv — multi-step math and factual reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Where GT step-by-step solutions exist (GSM8K), training smaller models on GT steps outperforms using LLM-generated CoT; LLM-generated CoT gives improvements over answer-only baselines in some settings (text reports up to 40% gains in some distillation contexts). Exact numeric splits per method are in paper tables; the text reports that LLM-generated CoT gave performance gains 'up to 40%' and that CoT prompting is improved upon by Socratic CoT prompting (see Socratic entries).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>CoT is effective when high-quality (human) step-wise annotations are available; LLM-generated CoT can be noisy and less effective than human GT steps. CoT prompting works well for large models but is fragile for smaller models without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Chain-of-thought supervision improves small-model reasoning over answer-only baselines, but Socratic-style (subquestioning) decompositions can provide further improvements; GT step-by-step annotations remain superior to LLM-generated CoT when available.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Distilling Reasoning Capabilities into Smaller Language Models', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8278.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8278.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SocCoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SOCRATIC COT (Socratic Chain-of-Thought)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed method that augments chain-of-thought with explicit subquestion–solution pairs: the problem is decomposed into intermediate subquestions and their answers, and these pairs are used to guide or supervise student models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SOCRATIC COT (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two teacher-driven annotation structures: a sequence of (subquestion, solution) pairs per step. Distillation strategies include (a) Unified student that generates full subquestion–solution sequences, and (b) Iterative setup with separate QG (question generation) and QA (question answering) student models.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['subquestion-based decomposition (Socratic subquestions)', 'chain-of-thought steps augmented by explicit subquestions', 'iterative modular decomposition (separate QG and QA models)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>SOCRATIC COT obtains teacher-generated subquestion–solution pairs (via LLM prompts) and either trains a unified student to emit the sequence of (q, s) pairs, or trains a QG model to generate subquestions and a QA model to answer them iteratively (teacher-forced during training, using generated previous solutions at inference). A guidance mechanism conditions QG on predicted equations or step counts to improve question generation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Direct comparisons in experiments: (1) CoT (standard chain-of-thought) vs. SOCRATIC COT (both LLM-generated and GT subquestions), (2) Unified SOCRATIC COT vs Iterative (QG+QA) distillation, (3) with and without guidance conditioning for QG, (4) ablation removing the QG module (train to generate chains of steps without explicit subquestions) and observing effects.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K (math word problems with GT steps and a Socratic variant), StrategyQA (binary factual reasoning with supporting facts), SVAMP/ASDiv (math word problems trained on AsDiv then tested on SVAMP).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported effects in text: (a) Using LLM-generated subquestions at inference (SOCRATIC COT) yields a performance boost 'of up to 38%' (context: GSM8K distillation experiments). (b) As a prompting strategy with GPT-3 (single-shot), subquestioning increased accuracy by 'over 40%' compared to standard CoT prompting (Table 3 referenced). (c) Iterative Socratic CoT (learning QG) produced slightly lower accuracy than using teacher QG at inference but still improved over step-by-step annotation without subquestions (example figures: 17.89 vs. 14.10 — reported in text as accuracy comparisons). (d) Removing the subquestion-generation module (training without QG) caused 'significant performance degradation' (Table 5). (e) Guidance conditioning of QG improved question quality metrics (BLEU, BERT-F1, #Q) and improved end-task accuracy (figure referenced). (f) On StrategyQA, using GPT-3 generated supporting facts sometimes hurt performance (58.07 vs. 60.51 for GPT-2 Large when comparing generated facts vs GT facts), while using provided subquestions with LLM-generated answers was effective (example: 60.31 vs 52.02 for GPT-2 Medium comparing strategies).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Explicit subquestioning improves interpretability and helps small models by giving structured intermediate targets; however, generated (noisy) supporting facts can harm supervision. Guidance conditioning (equation/step-count conditioning) helps QG produce better-aligned subquestions. Training to implicitly produce chains without explicit subquestions is insufficient and leads to worse performance.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Socratic-style decomposition (explicit subquestions + solutions) is an effective way to distill reasoning into smaller models and can outperform or match CoT supervision in many settings; unified and iterative distillation strategies both work, but explicit subquestions at inference are important (ablation removing them degrades performance). With Socratic COT, a much smaller model (e.g., GPT-2 large) can approach the performance of a ~10x larger model (GPT-3 6B) when trained with appropriate subquestion supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Distilling Reasoning Capabilities into Smaller Language Models', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8278.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8278.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (teacher)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (175B) used as teacher LLM for annotation generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pre-trained autoregressive transformer (175B parameters) used to generate chain-of-thought traces and subquestion–solution pairs to supervise smaller student models and for prompting experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (175B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer LLM (175B parameters) used as the teacher to produce CoT and Socratic annotations and to run prompting experiments (single-shot) for comparing CoT vs Socratic prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought prompting', 'Socratic subquestion generation (Socratic CoT prompting)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>GPT-3 was prompted (with a few exemplars) to (a) generate intermediate step-by-step solutions (CoT) for dataset examples, (b) generate subquestion–solution pairs (Socratic CoT) for problems, and (c) in prompting experiments, to decompose problems and solve the subproblems in single-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Used to generate both standard CoT traces and Socratic CoT subquestion–solution decompositions; compared performance when GPT-3 is prompted with CoT vs prompted with Socratic CoT (single-shot), measuring accuracy on GSM8K prompting experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K (prompting experiments), also used to create annotations for GSM8K, StrategyQA, and SVAMP datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>As a prompting method, Socratic CoT with GPT-3 (175B) increased accuracy by 'over 40%' relative to standard CoT prompting (single-shot experiments referenced in Table 3). Exact numeric prompt-accuracy values are reported in tables in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>GPT-3 reliably generates useful subquestion–solution decompositions that, when used as prompts, substantially improve single-shot reasoning performance; however, only one sample was used per problem in distillation (limitation noted).</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Large LLMs like GPT-3 can produce higher-quality decompositions (subquestions + solutions) than vanilla CoT prompts, and these decompositions can be distilled to smaller models to improve their reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Distilling Reasoning Capabilities into Smaller Language Models', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8278.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8278.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2 students</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 variants (student models: Medium, Large, XL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Smaller transformer language models (GPT-2 family) fine-tuned as students to mimic LLM reasoning via CoT or Socratic CoT supervision; used to evaluate whether distilled reasoning improves small-model performance on multi-step tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (student variants, e.g., Large 774M)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based GPT-2 family models used as student models (sizes include GPT-2 Medium, Large, XL; GPT-2 Large cited as 774M parameters). They are fine-tuned on different supervision: answer-only, GT steps, LLM-generated CoT, and Socratic CoT (unified and iterative).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['trained to generate chain-of-thought traces (CoT distillation)', 'trained to generate subquestion–solution pairs (SOCRATIC COT, unified)', 'separate QG and QA modules (SOCRATIC COT, iterative)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Students are fine-tuned on datasets augmented with either GT or LLM-generated step-wise solutions or subquestion–solution pairs. In the iterative setup, a QG model is trained to output subquestions conditioned on the problem (and optionally guide outputs like equations), and a QA model is trained to answer each subquestion iteratively (teacher-forced during training). At inference, QG produces subquestions and QA answers them sequentially to produce the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared fine-tuning regimes (Ans-only, GT Steps, LLM-CoT, Socratic CoT unified, Socratic CoT iterative), ablated QG guidance vs no-guidance, tested removing QG (No SubQ) vs with QG, and compared teacher-generated subquestions provided at inference vs learned QG.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K, StrategyQA, SVAMP (trained on AsDiv for SVAMP), evaluated on final-answer accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Selected reported numbers in text: (a) Overall abstract claim: distillation strategies 'boost the performance of smaller models over 70% compared to the baselines' (paper abstract). (b) LLM-generated step-by-step annotations yielded 'performance gains of up to 40%'. (c) Using LLM-generated subquestions at inference (SOCRATIC COT) produced 'a performance boost of up to 38%'. (d) Example small-model GSM8K accuracy comparison: Iterative Socratic CoT (learned QG) vs step-by-step without subquestions: 17.89 vs 14.10 (reported in text). (e) StrategyQA: GPT-2 Large with GPT-3 generated supporting facts: 58.07 vs with GT facts: 60.51. (f) StrategyQA alternative: using provided subquestions with LLM-generated answers: 60.31 vs 52.02 for GPT-2 Medium (two different supervision strategies compared in text).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Socratic CoT supervision helps GPT-2 students more than naive CoT in many conditions; a learned QG can be imperfect (incorrect number of subquestions) but guidance conditioning (equations or step counts) improves question generation and end-task accuracy. Removing explicit subquestions at inference causes notable degradation. Noisy LLM-generated supporting facts can hurt small models, while coherent subquestion/answer supervision helps.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Smaller (GPT-2) models can acquire improved multi-step reasoning when distilled with structured Socratic CoT supervision; unified and iterative distillation strategies both help, and when done properly a much smaller model (GPT-2 large, 774M) can match or approach the performance of a ~10x larger model (GPT-3 6B) fine-tuned on human-annotated data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Distilling Reasoning Capabilities into Smaller Language Models', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Explanations from large language models make small reasoners better <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 1)</em></li>
                <li>Automatic generation of socratic subquestions for teaching math word problems <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8278",
    "paper_id": "paper-258762841",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting method that elicits step-by-step intermediate reasoning traces from large language models to improve multi-step problem solving.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Chain-of-Thought (method)",
            "model_description": "Implemented as few-shot or prompted generation of intermediate reasoning steps (auto-regressive text) from a large language model (used here via GPT-3 prompts or LLM-generated annotations).",
            "reasoning_methods": [
                "chain-of-thought (CoT) step-by-step intermediate reasoning"
            ],
            "reasoning_methods_description": "Models are prompted (or finetuned) to output intermediate reasoning steps (a sequence of explanatory/calculation steps) leading to the final answer; these intermediate steps are used as supervision for student models or as part of the prompt at inference.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Compared fine-tuning on final answers only vs. fine-tuning with CoT-generated step-by-step annotations (LLM-generated) and vs. GT step-by-step annotations where available (e.g., GSM8K). Also compared CoT prompting vs. Socratic CoT prompting for GPT-3.",
            "task_or_benchmark": "GSM8K, StrategyQA, SVAMP/ASDiv — multi-step math and factual reasoning benchmarks.",
            "performance_results": "Where GT step-by-step solutions exist (GSM8K), training smaller models on GT steps outperforms using LLM-generated CoT; LLM-generated CoT gives improvements over answer-only baselines in some settings (text reports up to 40% gains in some distillation contexts). Exact numeric splits per method are in paper tables; the text reports that LLM-generated CoT gave performance gains 'up to 40%' and that CoT prompting is improved upon by Socratic CoT prompting (see Socratic entries).",
            "qualitative_findings": "CoT is effective when high-quality (human) step-wise annotations are available; LLM-generated CoT can be noisy and less effective than human GT steps. CoT prompting works well for large models but is fragile for smaller models without fine-tuning.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Chain-of-thought supervision improves small-model reasoning over answer-only baselines, but Socratic-style (subquestioning) decompositions can provide further improvements; GT step-by-step annotations remain superior to LLM-generated CoT when available.",
            "uuid": "e8278.0",
            "source_info": {
                "paper_title": "Distilling Reasoning Capabilities into Smaller Language Models",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "SocCoT",
            "name_full": "SOCRATIC COT (Socratic Chain-of-Thought)",
            "brief_description": "A proposed method that augments chain-of-thought with explicit subquestion–solution pairs: the problem is decomposed into intermediate subquestions and their answers, and these pairs are used to guide or supervise student models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SOCRATIC COT (method)",
            "model_description": "Two teacher-driven annotation structures: a sequence of (subquestion, solution) pairs per step. Distillation strategies include (a) Unified student that generates full subquestion–solution sequences, and (b) Iterative setup with separate QG (question generation) and QA (question answering) student models.",
            "reasoning_methods": [
                "subquestion-based decomposition (Socratic subquestions)",
                "chain-of-thought steps augmented by explicit subquestions",
                "iterative modular decomposition (separate QG and QA models)"
            ],
            "reasoning_methods_description": "SOCRATIC COT obtains teacher-generated subquestion–solution pairs (via LLM prompts) and either trains a unified student to emit the sequence of (q, s) pairs, or trains a QG model to generate subquestions and a QA model to answer them iteratively (teacher-forced during training, using generated previous solutions at inference). A guidance mechanism conditions QG on predicted equations or step counts to improve question generation.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Direct comparisons in experiments: (1) CoT (standard chain-of-thought) vs. SOCRATIC COT (both LLM-generated and GT subquestions), (2) Unified SOCRATIC COT vs Iterative (QG+QA) distillation, (3) with and without guidance conditioning for QG, (4) ablation removing the QG module (train to generate chains of steps without explicit subquestions) and observing effects.",
            "task_or_benchmark": "GSM8K (math word problems with GT steps and a Socratic variant), StrategyQA (binary factual reasoning with supporting facts), SVAMP/ASDiv (math word problems trained on AsDiv then tested on SVAMP).",
            "performance_results": "Reported effects in text: (a) Using LLM-generated subquestions at inference (SOCRATIC COT) yields a performance boost 'of up to 38%' (context: GSM8K distillation experiments). (b) As a prompting strategy with GPT-3 (single-shot), subquestioning increased accuracy by 'over 40%' compared to standard CoT prompting (Table 3 referenced). (c) Iterative Socratic CoT (learning QG) produced slightly lower accuracy than using teacher QG at inference but still improved over step-by-step annotation without subquestions (example figures: 17.89 vs. 14.10 — reported in text as accuracy comparisons). (d) Removing the subquestion-generation module (training without QG) caused 'significant performance degradation' (Table 5). (e) Guidance conditioning of QG improved question quality metrics (BLEU, BERT-F1, #Q) and improved end-task accuracy (figure referenced). (f) On StrategyQA, using GPT-3 generated supporting facts sometimes hurt performance (58.07 vs. 60.51 for GPT-2 Large when comparing generated facts vs GT facts), while using provided subquestions with LLM-generated answers was effective (example: 60.31 vs 52.02 for GPT-2 Medium comparing strategies).",
            "qualitative_findings": "Explicit subquestioning improves interpretability and helps small models by giving structured intermediate targets; however, generated (noisy) supporting facts can harm supervision. Guidance conditioning (equation/step-count conditioning) helps QG produce better-aligned subquestions. Training to implicitly produce chains without explicit subquestions is insufficient and leads to worse performance.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Socratic-style decomposition (explicit subquestions + solutions) is an effective way to distill reasoning into smaller models and can outperform or match CoT supervision in many settings; unified and iterative distillation strategies both work, but explicit subquestions at inference are important (ablation removing them degrades performance). With Socratic COT, a much smaller model (e.g., GPT-2 large) can approach the performance of a ~10x larger model (GPT-3 6B) when trained with appropriate subquestion supervision.",
            "uuid": "e8278.1",
            "source_info": {
                "paper_title": "Distilling Reasoning Capabilities into Smaller Language Models",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "GPT-3 (teacher)",
            "name_full": "GPT-3 (175B) used as teacher LLM for annotation generation",
            "brief_description": "A large pre-trained autoregressive transformer (175B parameters) used to generate chain-of-thought traces and subquestion–solution pairs to supervise smaller student models and for prompting experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 (175B)",
            "model_description": "Autoregressive transformer LLM (175B parameters) used as the teacher to produce CoT and Socratic annotations and to run prompting experiments (single-shot) for comparing CoT vs Socratic prompts.",
            "reasoning_methods": [
                "chain-of-thought prompting",
                "Socratic subquestion generation (Socratic CoT prompting)"
            ],
            "reasoning_methods_description": "GPT-3 was prompted (with a few exemplars) to (a) generate intermediate step-by-step solutions (CoT) for dataset examples, (b) generate subquestion–solution pairs (Socratic CoT) for problems, and (c) in prompting experiments, to decompose problems and solve the subproblems in single-shot prompts.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Used to generate both standard CoT traces and Socratic CoT subquestion–solution decompositions; compared performance when GPT-3 is prompted with CoT vs prompted with Socratic CoT (single-shot), measuring accuracy on GSM8K prompting experiments.",
            "task_or_benchmark": "GSM8K (prompting experiments), also used to create annotations for GSM8K, StrategyQA, and SVAMP datasets.",
            "performance_results": "As a prompting method, Socratic CoT with GPT-3 (175B) increased accuracy by 'over 40%' relative to standard CoT prompting (single-shot experiments referenced in Table 3). Exact numeric prompt-accuracy values are reported in tables in the paper.",
            "qualitative_findings": "GPT-3 reliably generates useful subquestion–solution decompositions that, when used as prompts, substantially improve single-shot reasoning performance; however, only one sample was used per problem in distillation (limitation noted).",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Large LLMs like GPT-3 can produce higher-quality decompositions (subquestions + solutions) than vanilla CoT prompts, and these decompositions can be distilled to smaller models to improve their reasoning.",
            "uuid": "e8278.2",
            "source_info": {
                "paper_title": "Distilling Reasoning Capabilities into Smaller Language Models",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "GPT-2 students",
            "name_full": "GPT-2 variants (student models: Medium, Large, XL)",
            "brief_description": "Smaller transformer language models (GPT-2 family) fine-tuned as students to mimic LLM reasoning via CoT or Socratic CoT supervision; used to evaluate whether distilled reasoning improves small-model performance on multi-step tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2 (student variants, e.g., Large 774M)",
            "model_description": "Transformer-based GPT-2 family models used as student models (sizes include GPT-2 Medium, Large, XL; GPT-2 Large cited as 774M parameters). They are fine-tuned on different supervision: answer-only, GT steps, LLM-generated CoT, and Socratic CoT (unified and iterative).",
            "reasoning_methods": [
                "trained to generate chain-of-thought traces (CoT distillation)",
                "trained to generate subquestion–solution pairs (SOCRATIC COT, unified)",
                "separate QG and QA modules (SOCRATIC COT, iterative)"
            ],
            "reasoning_methods_description": "Students are fine-tuned on datasets augmented with either GT or LLM-generated step-wise solutions or subquestion–solution pairs. In the iterative setup, a QG model is trained to output subquestions conditioned on the problem (and optionally guide outputs like equations), and a QA model is trained to answer each subquestion iteratively (teacher-forced during training). At inference, QG produces subquestions and QA answers them sequentially to produce the final answer.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Compared fine-tuning regimes (Ans-only, GT Steps, LLM-CoT, Socratic CoT unified, Socratic CoT iterative), ablated QG guidance vs no-guidance, tested removing QG (No SubQ) vs with QG, and compared teacher-generated subquestions provided at inference vs learned QG.",
            "task_or_benchmark": "GSM8K, StrategyQA, SVAMP (trained on AsDiv for SVAMP), evaluated on final-answer accuracy.",
            "performance_results": "Selected reported numbers in text: (a) Overall abstract claim: distillation strategies 'boost the performance of smaller models over 70% compared to the baselines' (paper abstract). (b) LLM-generated step-by-step annotations yielded 'performance gains of up to 40%'. (c) Using LLM-generated subquestions at inference (SOCRATIC COT) produced 'a performance boost of up to 38%'. (d) Example small-model GSM8K accuracy comparison: Iterative Socratic CoT (learned QG) vs step-by-step without subquestions: 17.89 vs 14.10 (reported in text). (e) StrategyQA: GPT-2 Large with GPT-3 generated supporting facts: 58.07 vs with GT facts: 60.51. (f) StrategyQA alternative: using provided subquestions with LLM-generated answers: 60.31 vs 52.02 for GPT-2 Medium (two different supervision strategies compared in text).",
            "qualitative_findings": "Socratic CoT supervision helps GPT-2 students more than naive CoT in many conditions; a learned QG can be imperfect (incorrect number of subquestions) but guidance conditioning (equations or step counts) improves question generation and end-task accuracy. Removing explicit subquestions at inference causes notable degradation. Noisy LLM-generated supporting facts can hurt small models, while coherent subquestion/answer supervision helps.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Smaller (GPT-2) models can acquire improved multi-step reasoning when distilled with structured Socratic CoT supervision; unified and iterative distillation strategies both help, and when done properly a much smaller model (GPT-2 large, 774M) can match or approach the performance of a ~10x larger model (GPT-3 6B) fine-tuned on human-annotated data.",
            "uuid": "e8278.3",
            "source_info": {
                "paper_title": "Distilling Reasoning Capabilities into Smaller Language Models",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models",
            "rating": 2,
            "sanitized_title": "leasttomost_prompting_enables_complex_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Explanations from large language models make small reasoners better",
            "rating": 2,
            "sanitized_title": "explanations_from_large_language_models_make_small_reasoners_better"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 1,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Automatic generation of socratic subquestions for teaching math word problems",
            "rating": 2,
            "sanitized_title": "automatic_generation_of_socratic_subquestions_for_teaching_math_word_problems"
        }
    ],
    "cost": 0.0154795,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Distilling Reasoning Capabilities into Smaller Language Models</p>
<p>Kumar Shridhar 
Department of Computer Science
ETH Zürich</p>
<p>Alessandro Stolfo stolfoa@ethz.ch 
Department of Computer Science
ETH Zürich</p>
<p>Mrinmaya Sachan 
Department of Computer Science
ETH Zürich</p>
<p>Distilling Reasoning Capabilities into Smaller Language Models
9FCDACD67572988FACCBA5AFDC9AFA1A
Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models.However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work.In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models.In this work, we propose an alternative reasoning scheme, SOCRATIC COT that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps.We use SO-CRATIC COT to train a combination of two small distilled models: a problem decomposer and a subproblem solver.In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems.On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies boost the performance of smaller models over 70% compared to the baselines.Finally, we investigate when SOCRATIC COT is an effective alternative to CoT, demonstrating cases where a much smaller model (GPT-2 large) can outperform a 10X larger model (GPT-3 6B).Our code is available here.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have demonstrated strong performance on a variety of reasoning tasks (Brown et al., 2020;Hoffmann et al., 2022;Chowdhery et al., 2022, inter alia).One particularly interesting strategy for prompting these models is chainof-thought (CoT), which has been shown to elicit reasoning abilities in LLMs by asking the model to incorporate intermediate reasoning steps while solving a problem (Nye et al., 2021;Wei et al., Figure 1: Illustration of the proposed framework.First, an LLM is prompted to decompose a multi-step problem providing annotation for the intermediate steps leading to the final solution.Then, the generated annotation is used to provide additional supervision when fine-tuning smaller models.2022b; Wang et al., 2022).However, CoT has been shown to work primarily on models with hundreds of billions of parameters (Wei et al., 2022b,a) or those tuned on a wide range of tasks (Chung et al., 2022;Iyer et al., 2022).</p>
<p>Due to the significant computational resources or expensive API calls required to access CoT-capable LLMs, we ask whether it is possible to elicit such reasoning capabilities in smaller models. 1mall-sized, non-fine-tuned language models are known to be poor reasoners (Stolfo et al., 2023).Therefore, a possible approach to induce CoT-like reasoning abilities in smaller models would be finetuning them on step-by-step examples.</p>
<p>In our work, we propose a framework for leveraging the reasoning capabilities of LLMs to supervise the training of smaller models.This approach can be thought of as a form of knowledge distillation (Hinton et al., 2015), where a larger teacher model transfers knowledge to a smaller student model.However, unlike standard knowledge distillation, our method transfers the reasoning abilities of the teacher model only using its generated solutions as a proxy, i.e., we do not assume access to the teacher model parameters.Our approach consists of prompting an LLM to produce step-by-step annotations leading to the answer for a set of problems.This annotation is then used as supervision to finetune the student model.A high-level illustration of the process is provided in Figure 1.</p>
<p>Within this framework, we study three different types of annotation structure for supervising our distillation approach: (i) We consider fine-tuning on the gold step-by-step solution procedure for datasets where the step-by-step solutions are available.(ii) We study whether procedural supervision, coming from the chain of thought (CoT) of the teacher model can improve upon the baseline.(iii) We propose a third type of supervision structure, which we call SOCRATIC COT.This approach relies on learning a semantic decomposition of the original problem into a sequence of subproblemsolution pairs using two models -a) a question generator that learns to decompose the problem into a sequence of subproblems, and b) a questionanswering model that solves the various generated subproblems (more details are in section 3.2).This approach can be thought of as an extension of the typical chain of thought reasoning where, unlike CoT, the intermediate steps are now decomposed into subquestion-solution pairs; the subquestions guide the generation of intermediate steps that lead to the final answer to the problem.</p>
<p>We train distilled student models with the various annotation structures mentioned above.Depending on the annotation available for the given data, we use the teacher model to generate either a CoT-like solution to a problem or, if the step-bystep annotation is available, a set of subquestions leading to the solution of the problem, or both (ex-amples of different annotations are shown in Figure 2).</p>
<p>We perform our analyses on three multi-step reasoning datasets: GSM8K (Cobbe et al., 2021), StrategyQA (Geva et al., 2021), andSVAMP (Patel et al., 2021).We consider data with various types of annotation to cover a range of realistic data scenarios.Our results show that supervision by CoT-decomposed examples helps smaller models perform better, and subquestioning introduced by SOCRATIC COT can provide further improvement.We observe performance gains of up to 40% with LLM-generated step-by-step annotations -this validates the effectiveness of our distillation framework (detailed analysis in Section 5).</p>
<p>Related Work</p>
<p>Decomposing Multi-Step Reasoning Tasks Solving multi-step reasoning tasks like MWPs has been a popular area of research for the last couple of years (Kushman et al., 2014;Hosseini et al., 2014;Roy et al., 2015;Amini et al., 2019;Zhang et al., 2020;Shridhar et al., 2022;Opedal et al., 2023).However, the majority of the modern approaches for these problems are shifting towards using large language models, often relying on approaches involving prompting or in-context learning (Cobbe et al., 2021;Kojima et al., 2022;Wei et al., 2022b;Chowdhery et al., 2022;Lewkowycz et al., 2022;Srivastava et al., 2022).One such prompting approach is the chain of thought prompting (Wei et al., 2022b), which prompts the language model to generate a series of intermediate steps that improve the reasoning capabilities in LLMs.Wang et al. (2022) took another step forward and sampled multiple reasoning paths and selected the most relevant output using majority voting.Huang et al. (2022) used the most voted outputs to further finetune the model for better performance.Kojima et al. (2022) further improved the reasoning of LLM in a zero-shot manner by appending "Let's think step by step" to the prompt.In contrast, our work does not propose prompting solutions; instead, we explicitly guide the student model reasoning using sub-questions at each step.Most similar to our work is the work by Zhou et al. (2022) which decomposes questions into sub-questions and asks the language model to solve each sub-question sequentially.However, this work is also restricted to prompting and only works with LLMs with billions of parameters.</p>
<p>Knowledge Distillation Our approach is reminiscent of knowledge distillation (Ba and Caruana, 2014;Hinton et al., 2015) in that we use a student network to mimic the large teacher language model.Snell et al. (2022) demonstrated the usefulness of providing instruction that can help models achieve better reasoning skills.Similar to our hypothesis, Eisenstein et al. (2022) argued that question-answering systems should focus not only on the final answer, but also on the rationale that justifies their reasoning, to help them reason better.We go beyond this; in our work, in addition to the question-answering system, we also focus on what questions need to be asked at each step that can help to learn that reasoning step better.Finally, similar to our hypothesis of injecting reasoning capabilities into smaller models, Li et al. (2022) used CoT-like reasoning from LLMs to train smaller models on a joint task of generating the solution and explaining the generated solution.We, on the other hand, use the LLM to generate subquestions and solution pairs and use them together to inject reasoning capabilities into smaller models.</p>
<p>Subquestioning as supervision</p>
<p>The idea of inquiring or asking information-seeking questions for discovery learning has been studied well in the past (Bruner, 1961).Rao and Daumé III generated clarification questions based on Stack Exchange questions as supervision, Klein and Nabi (2019) used a joint question answering model to ask questions from a given span of text and later answer them, and (Rajani et al., 2019;Shwartz et al., 2020) asked questions to improve common sense QA models.In contrast, our work focuses on multistep reasoning tasks where intermediate clarifying questions and reasoning steps may not always be available and may need to be extracted from a teacher model.</p>
<p>Methodology</p>
<p>The setting we consider consists of a data set D, where each problem P i is accompanied by a final answer a i that can be reached by several steps of reasoning.The task of solving the problem using a model ψ is to predict an answer â = ψ(P ) such that â = a.We consider different data scenarios where intermediate annotations of the solution may be available in different forms (e.g., step-by-step, as a semantic decomposition by subquestions) or may not be present.Depending on the availability of annotations, we propose different approaches to augment the training of a small model on D by A robe takes 2 bolts of blue fiber and half that much white fiber.How many bolts in total does it take?</p>
<p>"</p>
<p>Reasoning Problem</p>
<p>Answer-Only: The answer is 3.</p>
<p>CoT:</p>
<p>It takes 2/2=&lt;&lt;2/2=1&gt;&gt;1 bolt of white fiber.So the total amount of fabric is 2+1=&lt;&lt;2+1=3&gt;&gt;3 bolts of fabric.</p>
<p>The answer is 3.</p>
<p>Socratic CoT:</p>
<p>How many bolts of white fiber does it take?It takes 2/2=&lt;&lt;2/2=1&gt;&gt;1 bolt of white fiber.How many bolts in total does it take?So the total amount of fabric is 2+1=&lt;&lt;2+1=3&gt;&gt;3 bolts of fabric.</p>
<p>The answer is 3.</p>
<p>Figure 2: Illustration of the three different kinds of annotation structure.Our proposed approach, SOCRATIC COT, augments the typical chain-of-thought step-bystep solution with subquestioning.</p>
<p>using LLMs.</p>
<p>Distilling step-by-step reasoning via CoT</p>
<p>A data set may present an annotation that contains intermediate reasoning steps that lead to the answer a i (i.e., a chain-of-thought annotation).This intermediate annotation can be used directly to fine-tune a small model.However, in cases where such stepby-step information is not available, we use a LLM to generate the reasoning steps that might improve the performance of the small model.</p>
<p>To achieve this, we consider a small subset of the dataset D and decompose each problem P i into n i intermediate reasoning steps.We construct these intermediate reasoning steps manually, since we only need a few examples as prompts (examples are provided in Appendix Table 6).</p>
<p>For each remaining problem P ∈ D, we then prompt a large language model M to generate the intermediate reasoning steps.We make sure that the chain of reasoning steps is meaningful by checking whether the last solution matches the ground truth answer, i.e. whether a
(n i ) i = a i , where a (n i ) i
represents the answer corresponding to the last reasoning step.If this is not the case, we discard the problem and sample a new chain by prompting the model again (for a maximum of 3 times).In this way, we obtain an augmented dataset D * in which a subset of problems is paired with a sequence of reasoning steps leading to the correct result.Fi-nally, we can distill the reasoning capabilities into smaller models by fine-tuning them with the generated intermediate steps.</p>
<p>Distilling step-by-step reasoning through SOCRATIC COT</p>
<p>In this section, we describe how CoT can be enhanced through subquestioning.An illustration of our approach is shown in Figure 3.</p>
<p>Extracting the Reasoning Capability from the Teacher</p>
<p>In Section 3.1, we detailed how an LLM can be used to generate the intermediate annotation of a problem P i as a chain of steps leading to the answer a i .We now extend this procedure to include a subquestion at each step of the solution.Following a similar procedure as described in Section 3.1, we prompt the LLM with few exemplars of problems decomposed as a set of intermediate subquestionsolution pairs (the prompts are reported in Appendix Table 6).This way, we obtain an intermediate annotation that includes subquestioning.</p>
<p>In particular, each of the n i steps constituting the overall solution is a subquestion-solution pair, denoted q
(j) i , s(j)
i , j ∈ {1, . . ., n i } (an example is shown in Figure 2).We refer to the ordered list of subquestion-solution pairs for problem P i as (q</p>
<p>(1) i , s</p>
<p>(1) i ), . . ., (q
(n i ) i , s (n i ) i
).</p>
<p>Transferring the Reasoning Capability into the Student</p>
<p>We present two strategies to distill the reasoning annotation provided by the LLM into smaller models.</p>
<p>In the first strategy, a single unified student is trained to generate the subquestion-solution pairs simultaneously, while in the second strategy, the question generation and question-answering tasks are assigned to two separate models.We call this second strategy iterative because the questionanswering model is trained to solve each subquestion iteratively.</p>
<p>Unified.Using the problems in D that contain the chain of intermediate questions and solutions, we train a unified student model M uni that learns to generate the sequence of subquestion-solution pairs {(q (1) , s (1) ), (q (2) , s (2) ), . . .} that lead to the solution of a given problem.We use a pre-trained transformer-based model (Vaswani et al., 2017) and train it on the chain of subquestion-solution pairs for each problem P .Given a step j of problem P (i.e., the concatenation of q (j) and s (j) ) consisting of a sequence of m j tokens {x (1) j , . . ., x (m j ) j }, we use a typical auto-regressive language modeling loss, L:
L j (P ) = − m j k=1 log P uni (x (k) j |x :(k−1) j , P ) (1)
where P uni (x|c) is the probability assigned by M uni to token x given context c, and x :(y) indicates the sequence {x (1) , . . ., x (y) }.The loss L j is computed for each problem P i and for each pair (q (j) , s (j) ) leading to the final answer a i .</p>
<p>Iterative.The iterative version of the student separates the tasks of generating the subquestions and providing an intermediate answer to each subquestion into two distinct models: a question generation (QG) model and a question answering (QA) model.Both the QG and QA models are implemented using a Transformer-based language model (Vaswani et al., 2017).In particular, the QA model M qa is iteratively trained to answer the teacher-generated sub-questions.The learning objective is computed at the token level for each intermediate solution:
L(P, s (j) ) = − lj k=1 log P QA (y (k) j |y :(k−1) j
, q :(j) , s :(j−1) , P ) where l j and the y j 's represent, respectively, the length and the tokens of the intermediate solution s (j) .s :(j−1) consists of the previous solution generated by the QA model iteratively in the past iterations.</p>
<p>Similarly, the QG model is trained to acquire the ability of the teacher model to decompose the problem's main question into a series of sub-steps, each of which corresponds to a subquestion.The loss for this model is analogous to Equation 1, with the only difference being that the intermediate solutions are not considered for the QG model.During training, the previous intermediate solutions generated by the QA model are replaced with the teacher-generated solutions using teacher forcing (Cho et al., 2014).However, the intermediate solutions generated by the model are used at inference time.</p>
<p>Inference-time Predictions</p>
<p>Given an unseen problem P , the unified student model can directly predict a solution as a sequence (1) q</p>
<p>(2) q (n) . . .</p>
<p>A robe takes 2 bolts of blue fiber and half that much white fiber.How many bolts in total does it take?</p>
<p>"</p>
<p>Reasoning Problem</p>
<p>Answer: The answer is 3.</p>
<p>CoT:</p>
<p>It takes 2/2=&lt;&lt;2/2=1&gt;&gt;1 bolt of white fiber.So the total amount of fabric is 2+1=&lt;&lt;2+1=3&gt;&gt;3 bolts of fabric.</p>
<dl>
<dt>Socratic CoT:</dt>
<dd>
<p>How many bolts of white fiber does it take?</p>
</dd>
<dd>
<p>It takes 2/2=&lt;&lt;2/2=1&gt;&gt;1 bolt of white fiber.</p>
</dd>
<dd>
<p>How many bolts in total does it take?</p>
</dd>
<dd>
<p>So the total amount of fabric is 2+1=&lt;&lt;2+1=3&gt;&gt;3 bolts of fabric.</p>
</dd>
<dd>
<p>The answer is 3.</p>
</dd>
</dl>
<p>q (1)
s (1) q (2) s (2) a ! LLM ! ! QG Model QA Model q ( j) 's (q ( j) , s ( j) )'s
Figure 3: Detailed illustration of our framework.First, a LLM is prompted to decompose the input problem P into a series of subquestion-solution pairs (q
(j) i , s(j)
i , j ∈ {1, . . ., n i }) with an answer at each step a of subquestions and answers.In the iterative approach, we first generate the subquestions conditioning the generation of the QG model on P .After these questions are generated, they are provided to the QA model one by one, decoding the intermediate solution ŝ(j) at step j token by token according to the model's probability distribution over its vocabulary:
P QA (y (k) j |y :(k−1) j , q:(j) , ŝ:(j−1) , P ),(2)
where y
(k) j
is the k-th token being decoded in greedy fashion.</p>
<p>After the last solution ŝ(n) has been generated, the numerical prediction â(n) is parsed from the text using simple heuristics.</p>
<p>Empirical Analysis</p>
<p>Datasets</p>
<p>We study how smaller models can learn to reason better on three multi-step reasoning datasets: GSM8K (Cobbe et al., 2021), StrategyQA (Geva et al., 2021), and SVAMP (Patel et al., 2021).GSM8K consists of 8.5K grade school math word problems, each requiring 2 to 8 steps of reasoning to solve.The solutions primarily involve a se-quence of elementary calculations using basic arithmetic operations (+, −, ×, ÷).The dataset is divided into 7.5K training problems and 1K test problems.To evaluate the model on SVAMP, we train the model on 761 multi-step math word problems taken from the ASDiv (Miao et al., 2020) training set and evaluate it on 237 multi-step SVAMP problems.For StrategyQA, the test set with facts is not available, so we split the data into 80% training, 10% as validation data, and the last 10% as test data.We do not shuffle the data to maintain reproducibility.</p>
<p>Experimental Setup</p>
<p>We use three kinds of annotation, corresponding to the three datasets that we consider.</p>
<p>Step-by-step solution.The GSM8K dataset falls into this category and includes a Socratic version where intermediate subquestion-solution pairs are provided for each MWP.While the intermediate step-by-step solutions were manually annotated, the authors report that the subquestions were generated by prompting GPT-3.We reproduced a subset of these subquestions using a GPT-3 model with prompts, and we observed a high similarity between the questions provided and the ones gen-</p>
<p>Unified</p>
<p>Input:</p>
<p>Output: A robe takes 2 bolts of blue fiber and half that much white fiber.How many bolts in total does it take?</p>
<p>How many bolts of white fiber does it take?It takes 2/2 = &lt;&lt;2/2=1&gt;&gt; 1 bolt of white fiber.How many bolts in total does it take?So the total amount of fabric is 2+1 = &lt;&lt;2+1=3&gt;&gt; 3 bolts of fabric.The answer is 3.</p>
<p>Iterative</p>
<p>Iteration 1 Input:</p>
<p>Output: A robe takes 2 bolts of blue fiber and half that much white fiber.How many bolts in total does it take?</p>
<p>QG: How many bolts of white fiber does it take?QA: It takes 2/2 = &lt;&lt;2/2=1&gt;&gt; 1 bolt of white fiber.</p>
<p>Iteration 2 Input:</p>
<p>Output: A robe takes 2 bolts of blue fiber and half that much white fiber.How many bolts in total does it take?How many bolts of white fiber does it take?It takes 2/2 = &lt;&lt;2/2=1&gt;&gt; 1 bolt of white fiber.</p>
<p>QG: How many bolts in total does it take?QA: So the total amount of fabric is 2+1 = &lt;&lt;2+1=3&gt;&gt; 3 bolts of fabric.The answer is 3.</p>
<p>Table 1: Example demonstraing the input-output format for unified vs iterative setup.QG represents the question generation model and QA is the question answerer model.Note that the QA model uses the QG output to answer it as shown in Figure 3. erated by us (BERT F 1 score of 95%).For SO-CRATIC COT, we thus use the subquestioning annotation already provided.</p>
<p>Supporting facts.We study the StrategyQA dataset, which falls in this category.Strategy QA consists of a factual question with binary True/False as the final answer.Additional supporting facts and decomposed questions are provided.However, the set of facts and the decomposed questions provided with a given question are not always aligned (i.e., a fact is not necessarily the answer to one subquestion).Therefore, having a setup similar to the one for GSM8K is not possible.We thus consider two versions of the data.One in which the supporting facts are used as CoT and the corresponding questions are generated by prompting a GPT-3 model, and a second in which we take the provided questions and generate the facts (this time aligned with the questions) using GPT-3.</p>
<p>Final answers only.AsDiv/SVAMP falls in this category and for training, we use GPT-3 to generate both intermediate subquestions and solutions.Intermediate solutions are used as CoT and the generated subquestion-solution pairs for SOCRATIC COT.</p>
<p>Implementation Details</p>
<p>We use GPT-2 variants (Radford et al., 2019) as student models.GPT-3 175B (Brown et al., 2020) served as the teacher model for decomposing complex problems into a series of simpler substeps (we report the prompts used in Appendix Table 6).</p>
<p>All models were trained using the Huggingface library (Wolf et al., 2020) on an NVIDIA Tesla A100 GPU with 40 GB of memory.Each experiment was run for the same number of iterations to ensure fairness with periodic evaluation over the validation set.Teacher forcing was used during training to replace the generated responses with ground truth answers from the training dataset.</p>
<p>Evaluation Metric.To evaluate the questionanswering performance on the GSM8K, SVAMP, and StrategyQA datasets, we compute the accuracy based on the final answer provided by the student model.</p>
<p>Results and Discussion</p>
<p>Can our framework improve the reasoning capabilities of smaller models?Table 2 demonstrates that leveraging LLMs reasoning capabilities using our framework can improve the reasoning results for all dataset types.</p>
<p>Step-by-Step Solution.When human-annotated step-by-step solutions are available, training smaller models with LLM-generated CoT is not advantageous, as shown on GSM8K.This is to be expected since the annotation generated by an LLM is likely to be noisier and of lower quality than human-annotated data.However, the groundtruth step-by-step annotation can be leveraged to prompt an LLM to generate subquestions for the SOCRATIC COT approach, giving a performance Soc GT represents the case where GT solutions/facts are used when prompting GPT-3 to generate the subquestions.</p>
<p>Iterative and Unified represent the two Soc CoT strategies described above.All models are GPT-2 versions and their size is reported within parentheses.All experiments were run at least 3 times and the average is reported.GPT-3 6B results are taken from Cobbe et al. (2021).</p>
<p>boost of up to 38% when the LLM-generated subquestions are used at inference time.When the subquestions are learned by the QG model (Iterative Soc CoT ), the accuracy of the student model decreases slightly but still improves over the stepby-step annotation without subquestions (17.89 vs. 14.10).Figure 5 shows a comparison of predictions generated by Soc CoT models and a model trained on the GT step-by-step annotation.Unified SO-CRATIC COT performs similarly to training with the step-wise ground-truth annotation.We additionally include the score produced by GTP-3 6B to show that training with SOCRATIC COT can help a small model (GPT-2 large with 774M parameters) perform as well as a nearly 10x larger model fine-tuned with human annotated data.</p>
<p>Supporting facts.On StrategyQA, we observe that the inclusion of ground-truth supporting facts in the fine-tuning procedure improves the performance of the small models.However, surprisingly, when the supporting facts are generated by GPT-3, their inclusion actually hurts performance (58.07 vs 60.51 for GPT-2 Large).We hypothesize that this is likely due to the imperfect factual knowledge provided by the LLM, which mars the quality of the supervision.We have observed that the GT supporting facts provided often do not represent a logical sequence of propositions leading to the final answer.This is likely the reason why decomposing  the problem through subquestions based on such facts actually harms accuracy (see Soc CoT column in Table 2).Instead, using the provided subquestions and using an LLM to generate the answers (representing coherent facts leading to the final answer) proves to be an effective strategy (60.31 vs. 52.02for GPT-2 Medium).A more detailed comparison between our proposed approaches is presented in Figure 4.However, GPT-2 XL mod-</p>
<dl>
<dt>Training w/ Soc CoT (correct final answer but wrong reasoning)</dt>
<dd>
<p>A robe takes 2 bolts of blue fiber and half that much white fiber.How many bolts in total does it take?</p>
</dd>
</dl>
<p>GT Solution:</p>
<p>It takes 2/2=&lt;&lt;2/2=1&gt;&gt;1 bolt of white fiber So the total amount of fabric is 2+1=&lt;&lt;2+1=3&gt;&gt;3 bolts of fabric.The answer is 3. Table 3: Accuracy comparison (in %) of using CoT vs SOCRATIC COT (Sub-ques) on the GSM8K dataset for GPT-3 model with prompting.</p>
<p>els perform well when trained on facts as unlike smaller models, larger models can encode more facts at once in their parameters, which assists in answering a factual question.</p>
<p>Answers only.Can SOCRATIC COT be used as a prompting strategy?We experimented with SOCRATIC COT as a prompting strategy.First, we prompted GPT-3 (175B) to decompose the main problem into simpler steps by formulating subquestions.Then, GPT-3 is used again to solve the sequence of subproblems in a single-shot setting with a problem decomposed into intermediate subquestions and solutions included in the prompt.The introduction of subquestioning boosts accuracy by over 40% compared to standard CoT prompting (Table 3).</p>
<p>Other work (e.g., Wei et al. 2022b) has used a larger number of exemplars in the few-shot prompt, achieving higher overall accuracy.We limited our experiments to single-shot prompts due to budget constraints.</p>
<p>Ablation Studies</p>
<p>In this Section, we describe additional analyses regarding specific components of the framework we propose, as well as negative results that we obtained with alternative strategies.</p>
<p>How good are the sub-questioning capabilities of a smaller model?We investigate in more detail the ability of a small model to decompose a problem by generating meaningful subquestions.We fine-tuned GPT-2 Large on the GPT-3 generated subquestions provided in the GSM8K dataset.</p>
<p>We then evaluated the quality of the generated questions in terms of BLEU score (Post, 2018), BERT F 1 score (Zhang et al., 2019), and by measuring for how many problems the number of questions generated by GPT-2 (#Q) matches the number of GPT-3 annotated questions for a given problem.We found that the fine-tuned GPT-2 predicted an incorrect number of subquestions for the majority of problems (see Table 4, first row).Thus, following previous work on subquestion generation (Shridhar et al., 2022), we introduced a guidance mechanism that conditions the generation of subquestions for a problem P on the equations describing the intermediate solutions of P .This strategy improved the quality of the generated questions for all three metrics considered (Table 4, second row).To avoid the dependence on the step-by-step annotation of the equations for each problem P at inference time, we train an additional sequenceto-sequence model to predict, given P , the set of equations that lead to the solution of the problem.At inference time, the predictions for the guidance model are used to condition the generation by the QG model.Although the predicted equations often do not lead to the correct solution of the problem, they help the QG model to generate more meaning-  ful sub-questions.Figure 6 shows the overall accuracy of the GPT-2 student models (QA + QG) finetuned with SOCRATIC COT on the GSM8K data with and without equation conditioning provided by the guide model.We have extended this guidance mechanism to StrategyQA and SVAMP, where the generation of subquestions is conditioned on the number of facts (StrategyQA) or steps (SVAMP) needed to answer the problem.</p>
<p>Eliminating the need for a subquestion module.We have experimented with an alternative training solution that does not involve a question-generation model.This strategy aims to improve the supervision for fine-tuning a small model through subquestioning, but without relying on the presence of subquestions at test time.The procedure consists of training the student model to generate the entire chain of steps leading to an intermediate answer.That is, when the sub-question q (1) is asked, the model is trained to generate the answer s (1) , but when q (j) is asked, the model is trained to generate the chain of thought reasoning {s (1) , s (2) , . . ., s (j) } (instead of just s (j) ).This eliminates the need for the intermediate subquestions at inference time, as the model is trained to implicitly decompose the main problem into smaller reasoning steps.However, this method   leads to significant performance degradation (results are reported in Table 5), highlighting the need for subquestions at inference time.</p>
<p>Example outputs In Figures 5 and 7, we report example outputs predicted by GPT-2 models for a set of GSM8K and SVAMP problems.</p>
<p>Conclusion</p>
<p>The chain-of-thought style of step-by-step reasoning has proven to be very effective for reasoning in LLMs.In this work, we propose ways to distill these reasoning capabilities into smaller models and suggest ways to further improve them by explicitly asking stepwise questions.We demonstrate the effectiveness of our proposed methodology on three popular multi-step reasoning datasets, and discuss cases where one method should be preferred over the other for different datasets.</p>
<p>Limitations</p>
<p>In our work, we use only one solution from the LLM to distill information into the student model, and according to Wang et al. (2022), multiple subquestion-solution pairs can be sampled, and using majority voting, all pairs leading to the most frequent answer can be used to distill knowledge into the student models.Also, due to computational budget, we used a single prompt to compare the CoT and SOCRATIC COT and using more prompts (up to 8) might lead to a fairer comparison and better results (Wei et al., 2022b).We leave these experiments for the future.</p>
<p>Ethical Considerations</p>
<p>Although this work improves the reasoning capabilities of smaller models, the models are still not powerful enough to be used in sensitive settings such as education.We plan to release our code and model checkpoints, but the models must be used carefully by users, as many generative models, including ours, are prone to hallucination.</p>
<p>Conclusion</p>
<p>B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified?For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?Our models are free to be used by anyone.We mention the limitations of our approach B4.Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?</p>
<p>We used standard open source datasets B5.Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Section 4 B6.Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created?Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results.For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.Section 4.2 C Did you run computational experiments?Section 4.3 C1.Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?Section 4.3</p>
<p>(j) i .The generated subquestions-solutions are used to train two student models: a) the QG model which learns to mimic the LLM's sub questioning capability and b) the QA model, which learns to solve each subquestion.At the bottom, the inference process is depicted for an unseen problem and no LLM is involved.The QG model breaks the unseen problem into simpler subquestions and the QA model solves each one of them eventually leading to the final answer a (ni) i .</p>
<p>Figure 4 :
4
Figure4: Accuracy comparison for different supervision strategies on StrategyQA.The baseline method consists of fine-tuning on final answers only (Ans only), and it is compared to fine-tuning with: ground-truth supporting facts (GT Facts), GPT-3-generated supporting facts (CoT), ground-truth supporting facts with GPT-3generated subquestions (Soc CoT ), and LLM-generated facts with human-annotated subquestions (Soc GT ).</p>
<p>How many car washes does Tom get in a year?He gets 4<em>12=&lt;&lt;4</em>12=48&gt;&gt;48 car washes a year.How much does Tom pay in a year?That means he pays 48<em>15=$&lt;&lt;48</em>15=720&gt;&gt;720 a year.The answer is 720.</p>
<p>Figure 5 :
5
Figure 5: Example of predictions generated by a GPT-2 Large model fine-tuned with GT steps and SOCRATIC COT on GSM8K dataset.</p>
<p>Figure 6 :
6
Figure 6: Accuracy of student models (QA + QG) when the question generation is conditioned using the guidance model (Guide) and with non-guided question generation (No guide).Ans only represents the baseline.All models are GPT-2 versions.</p>
<p>Figure 7 :
7
Figure 7: of predictions generated by a GPT-2 Medium model fine-tuned with GT steps and SOCRATIC COT on the SVAMP dataset.</p>
<p>you describe the limitations of your work?Limitation A2.Did you discuss any potential risks of your work?Ethical considerations A3.Do the abstract and introduction summarize the paper's main claims?Abstract and Introduction A4.Have you used AI writing assistants when working on this paper?Left blank.B Did you use or create scientific artifacts?Section 3, Methodology B1.Did you cite the creators of artifacts you used?Section 4.2 B2.Did you discuss the license or terms for use and / or distribution of any artifacts?</p>
<p>Table 2 :
2
Accuracy comparison (in %) on the three considered datasets.We consider three human-annotated baselines: final answers only (Answer Only), ground-truth step-by-step solution (GT Steps), and supporting facts (GT Facts).We compare the different supervision strategies for fine-tuning the small models: CoT represents the case where the chain of intermediate reasoning steps is generated by GPT-3, Soc CoT represents the case where both the chain of intermediate solutions and the subquestions are generated by LLM and used to fine-tune small models.
IterativeUnified</p>
<p>Table 4 :
4
BLEU, BERT F 1 and the number of questions (# Q) comparison between the question generator model and the Socratic subquestions present in the GSM8K dataset using GPT2-large model.
Methodology BLEU BERT F1 # QNo-guidance Guidance51.5 58.80.78 0.810.42 0.80</p>
<p>Table 5 :
5
Accuracy comparison (in %) of student models trained with (SubQ with QG) and without (No SubQ) question generation model on GSM8K.
Training w/ GT StepsProblemHaley has &lt;&lt;+ number0 number1&gt;&gt;: Haley grew number0 trees in her Ptrees. Haley has &lt;&lt;-+ number0 number1 number2&gt;&gt; trees leftbackyard. After a typhoon number1died. Then she grew number2 moretrees. How many trees does shehave left?Training w/ Soc CoTGT Solution: &lt;&lt;-+ number0 number2 number1&gt;&gt;How many trees did haley grow in total?Haley grew &lt;&lt;+ number0number2&gt;&gt; trees in total.How many trees does she have?Haley has &lt;&lt;-+ number0 number2number1&gt;&gt; trees left.Training w/ GT StepsProblemFaye put &lt;&lt;+ number0 number1&gt;&gt;pencils into each row. Faye could: Faye was placing her pencils into Pmake &lt;&lt;/ + number0 number1 number2&gt;&gt; rows.rows with number0 pencils in eachrow. She had number1 packs ofpencils each one having number2pencils. How many rows could she make?Training w/ Soc CoTGT Solution:&lt;&lt;/ + number0 number1 number2&gt;&gt;How many pencils did Faye have intotal? Faye had &lt;&lt;+ number1number2&gt;&gt; pencils in total. Howmany rows could she make?Fayecould make &lt;&lt;/ + number1 number2number0&gt;&gt; rows.
 Following Li et al. (2022), we argue that small and large models are relative terms and context-dependent. We consider models with billions of parameters to be large, and models with millions of parameters to be small.
The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.
AcknowledgementsAlessandro Stolfo is supported by Armasuisse Science and Technology through a CYD Doctoral Fellowship.Let's generate sub-questions for these problems.Use exactly one operation per step.-Q: Zoe was unboxing some of her old winter clothes .She found number0 boxes of clothing and inside each box there were number1 scarves and number2 mittens .How many pieces of winter clothing did Zoe have total ?SQ1: How many pieces of winter clothing did Zoe have in each box?A1: Zoe had &lt;&lt;+ number1 number2&gt;&gt; pieces of winter clothing in each box.SQ2: How many pieces of winter clothing did Zoe have total ?A2: Zoe had &lt;&lt;* number0 + number1 number2&gt;&gt; pieces of winter clothing in total.For each of the following topics, generate intermediate answers to the subquestions leading to the final answer.-Topic: Albany, Georgia (City in Georgia, United States) Will the Albany in Georgia reach a hundred thousand occupants before the one in New York?Albany, GA has around 75,000 people.Albany, NY has almost 100,000 people.The difference is 100,000-75,000=25,000 The difference is 100,000-100,000=0 No, 25,000 is not smaller than 0. The final answer is NO.The average cost of a US Boeing 737 plane is 1.6 million dollars.Wonder Woman (2017 film) grossed over 800 million dollars at the box office.Yes, 800 is larger than 1.6.The final answer is YES.D2.Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?No response.D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating?For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?No response.D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?No response.D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?No response.
Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi, arXiv:1905.13319Mathqa: Towards interpretable math word problem solving with operation-based formalisms. 2019arXiv preprint</p>
<p>Do deep nets really need to be deep?. Jimmy Ba, Rich Caruana, Advances in neural information processing systems. 201427</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>The act of discovery. Jerome S Bruner, Harvard educational review. 311961</p>
<p>Learning phrase representations using rnn encoder-decoder for statistical machine translation. Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, arXiv:1406.10782014arXiv preprint</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.11416Scaling instruction-finetuned language models. 2022arXiv preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Honest students from untrusted teachers: Learning an interpretable question-answering pipeline from a pretrained language model. Jacob Eisenstein, Daniel Andor, Bernd Bohnet, Michael Collins, David Mimno, arXiv:2210.024982022arXiv preprint</p>
<p>Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, 2021Transactions of the Association for Computational Linguistics (TACL</p>
<p>Geoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531Distilling the knowledge in a neural network. 20152arXiv preprint</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.15556Training compute-optimal large language models. 2022arXiv preprint</p>
<p>Learning to solve arithmetic word problems with verb categorization. Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, Nate Kushman, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)2014</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, arXiv:2210.116102022arXiv preprint</p>
<p>Opt-iml: Scaling language model instruction meta learning through the lens of generalization. Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Dániel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, arXiv:2212.120172022arXiv preprint</p>
<p>Learning to answer by learning to ask. Tassilo Klein, Moin Nabi, arXiv:1911.02365Getting the best of gpt-2 and bert worlds. 2019arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, arXiv:2205.119162022arXiv preprint</p>
<p>Learning to automatically solve algebra word problems. Nate Kushman, Yoav Artzi, Luke Zettlemoyer, Regina Barzilay, 10.3115/v1/P14-1026Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 52nd Annual Meeting of the Association for Computational LinguisticsBaltimore, MarylandAssociation for Computational Linguistics20141</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, arXiv:2206.14858Solving quantitative reasoning problems with language models. Theo Gutman-SoloarXiv preprintet al. 2022</p>
<p>Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, arXiv:2210.06726Explanations from large language models make small reasoners better. 2022arXiv preprint</p>
<p>A diverse corpus for evaluating and developing English math word problem solvers. Chao-Chun Shen-Yun Miao, Keh-Yih Liang, Su, 10.18653/v1/2020.acl-main.92Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, arXiv:2112.00114Show your work: Scratchpads for intermediate computation with language models. 2021arXiv preprint</p>
<p>World models for math story problems. Andreas Opedal, Niklas Stoehr, Abulhair Saparov, Mrinmaya Sachan, Findings of the Association for Computational Linguistics: ACL 2023. Toronto, Canada2023</p>
<p>Arkil Patel, Satwik Bhattamishra, Navin Goyal, arXiv:2103.07191Are nlp models really able to solve simple math word problems?. 2021arXiv preprint</p>
<p>A call for clarity in reporting BLEU scores. Matt Post, Proceedings of the Third Conference on Machine Translation: Research Papers. the Third Conference on Machine Translation: Research PapersBelgium, BrusselsAssociation for Computational Linguistics2018</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019</p>
<p>Explain Yourself! Leveraging Language Models for Commonsense Reasoning. Nazneen Fatema Rajani, Bryan Mccann, Caiming Xiong, Richard Socher, 10.18653/v1/P19-1487Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Answer-based Adversarial Training for Generating Clarification Questions. Sudha Rao, Hal Daumé, Iii , Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1</p>
<p>Subhro Roy, Tim Vieira, Dan Roth, 10.1162/tacl_a_00118Reasoning about quantities in natural language. Transactions of the Association for Computational Linguistics. 20153</p>
<p>Automatic generation of socratic subquestions for teaching math word problems. Kumar Shridhar, Jakub Macina, Mennatallah El-Assady, Tanmay Sinha, Manu Kapur, Mrinmaya Sachan, arXiv:2211.128352022arXiv preprint</p>
<p>Unsupervised commonsense question answering with self-talk. Vered Shwartz, Peter West, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, 10.18653/v1/2020.emnlp-main.373Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>. Charlie Snell, Dan Klein, Ruiqi Zhong, arXiv:2209.151892022Learning by distilling context. arXiv preprint</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.04615Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022arXiv preprint</p>
<p>A causal framework to quantify the robustness of mathematical reasoning with language models. Alessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bernhard Schölkopf, Mrinmaya Sachan, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou, ArXiv, abs/2203.111712022</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022aarXiv preprint</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903Chain of thought prompting elicits reasoning in large language models. 2022barXiv preprint</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest, Rush, 10.18653/v1/2020.emnlp-demos.6Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnline. Association for Computational Linguistics2020</p>
<p>Graph-totree learning for solving math word problems. Jipeng Zhang, Lei Wang, Roy , Ka-Wei Lee, Yi Bin, Yan Wang, Jie Shao, Ee-Peng Lim, 2020Association for Computational Linguistics</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, arXiv:1904.09675Bertscore: Evaluating text generation with bert. 2019arXiv preprint</p>
<p>Leastto-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, ArXiv, abs/2205.10625Quoc Le, and Ed Chi2022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8984 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8984</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8984</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-2784000e1a3554374662f4d18cb5ad52f59c8de6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2784000e1a3554374662f4d18cb5ad52f59c8de6" target="_blank">Graph Convolutional Encoders for Syntax-aware Neural Machine Translation</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper Abstract:</strong> We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoder-decoder models for machine translation. We rely on graph-convolutional networks (GCNs), a recent class of neural networks developed for modeling graph-structured data. Our GCNs use predicted syntactic dependency trees of source sentences to produce representations of words (i.e. hidden states of the encoder) that are sensitive to their syntactic neighborhoods. GCNs take word representations as input and produce word representations as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidirectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8984.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8984.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Syntactic GCN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Syntactic Graph Convolutional Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of Graph Convolutional Networks adapted to directed, labeled dependency graphs with edge-wise gating; used to compute word-level vector representations by propagating and aggregating information over syntactic edges and their labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph convolutional encoding (node-level vectors)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Nodes (words) are represented by input vectors (embeddings or encoder hidden states). Each GCN layer computes new node vectors by summing (direction- and label-conditioned) linear transforms of neighbor node vectors, applying label-specific bias terms, gating each edge with a learned scalar, and a nonlinearity; stacking layers integrates information from k-hop neighborhoods. Self-loops (loop direction) are modeled separately and residual connections can be used for multi-layer GCNs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Syntactic dependency trees (directed, labeled graphs); in principle other graph structures (SRL, AMR, coreference) are supported</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No conversion to text; the graph is consumed directly by the GCN: messages are passed along directed labeled edges to produce updated word representations (i.e., vector encodings) that are fed into the NMT encoder/attention mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Neural Machine Translation (source-side encoder within attention-based encoder-decoder); also evaluated on synthetic reordering task (artificial sequences) for illustration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported BLEU improvements: English-German: +1.2 BLEU points (BLEU4 improvement reported in intro); English-Czech: +0.7 BLEU points (BLEU4). Test/validation specifics: English-Czech test BLEU4 improved from 8.9 (BiRNN) to 9.6 (+GCN). On full En-De data + beam search, BiRNN baseline 23.3 BLEU4 vs BiRNN+GCN 23.9 BLEU4. Also consistent gains in BLEU1 and Kendall tau (word order) reported; artificial reordering task achieved validation BLEU 99.2.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared experimentally to three encoder baselines (BoW, CNN, BiRNN); GCN layers on top of each encoder yielded consistent improvements over their syntax-agnostic counterparts. In discussion, contrasted qualitatively with other syntax integration approaches (feature embeddings, tree encoders, multitask, string-to-tree) but not directly compared on the same metrics in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Directly integrates labeled, directed syntactic structure into word representations; flexible to any graph-structured linguistic annotation; edge-wise gating helps ignore noisy predicted edges; simple and computationally inexpensive; consistently improves BLEU, BLEU1 and Kendall tau across encoders and languages; can be stacked on top of existing encoders (BiRNN, CNN, BoW).</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Relies on predicted syntactic parses (noise in parses can limit gains); requires modeling direction and labels (added parameters) though mitigated by making only biases label-specific; improvements are modest when baseline encoders (e.g., BiRNNs) already capture some syntax.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit catastrophic failure cases reported; paper notes smaller-than-expected extra gains on longer sentences possibly due to noisier parses for longer inputs. In artificial experiments, GCN without edge-wise gating would struggle to distinguish fake vs real edges (motivating gating).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Convolutional Encoders for Syntax-aware Neural Machine Translation', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8984.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8984.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>String-to-Tree / Linearized parse trees</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural string-to-tree (linearized parse tree prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approach that represents syntactic trees as linearized strings and trains the NMT system to produce or consume such linearized parse trees (stringified tree representations) as sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards String-to-Tree Neural Machine Translation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Linearized parse tree (stringified tree)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A full syntactic tree (e.g., constituency/parse tree) is serialized into a linear sequence (string) that encodes tree structure (parentheses or special tokens) which can be predicted or consumed by a sequence model.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Constituency / parse trees (trees serialized as strings)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Tree linearization / serialization into a token sequence (string-to-tree / tree-to-string paradigms); the model predicts linearized parse trees as output or uses them as additional supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Neural Machine Translation (string-to-tree translation that outputs target parse structures or uses source/target parses)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Mentioned in related work as an alternative to integrating syntax directly into encoders; no direct empirical comparison provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Allows leveraging full tree structure within sequence-to-sequence framework without specialized graph modules (uses existing seq2seq machinery).</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Paper does not detail, but linearization can increase sequence length and may impose rigid structural constraints; no empirical details provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Convolutional Encoders for Syntax-aware Neural Machine Translation', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8984.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8984.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Feature embeddings (POS/Deps)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linguistic feature embeddings (POS tags, lemmas, dependency labels)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Embedding discrete linguistic features (e.g., POS, lemmas, dependency labels) and concatenating or feeding them together with word embeddings into the NMT model to inject syntactic/linguistic information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Linguistic Input Features Improve Neural Machine Translation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Feature-augmented token representation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each token's representation is augmented with learned embeddings of linguistic features (POS tags, lemma ids, dependency labels) which are concatenated or combined with the word embedding and provided to the encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Dependency labels and other token-level linguistic annotations (not full graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No graph linearization; token-level features (including dependency labels) are embedded and appended to word embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Neural Machine Translation (source-side feature augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Mentioned as a simpler approach to inject syntax compared to explicit graph encoders; no direct quantitative comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple to implement; directly provides token-level syntactic signals to model.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Indirect incorporation of structure; does not model relations between tokens as a graph beyond local label features.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Convolutional Encoders for Syntax-aware Neural Machine Translation', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8984.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8984.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree-LSTM encoder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-LSTM sentence/tree encoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tree-structured LSTM encoder that composes representations following the syntactic (e.g., HPSG or constituency) tree to produce node and sentence representations to be used in NMT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree-to-sequence attentional neural machine translation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Tree-structured LSTM encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Internal nodes of a parse tree are encoded using Tree-LSTM composition; node and leaf representations participate in the attention mechanism so the decoder can attend to word and phrase/node vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Constituency/HPSG parse trees (tree-structured syntax)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No linearization to text; the tree is consumed by a recursive Tree-LSTM that computes node vectors following tree topology.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Neural Machine Translation (tree-to-sequence models where tree encodings are used in decoder attention)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Mentioned as a syntax-aware encoder alternative; contrasted with methods that integrate structure more indirectly (multi-task) or with GCNs, but no direct head-to-head metrics in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Directly models hierarchical phrase structure and produces node representations that can be attended to by decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>More rigid (requires tree structure), may impose constraints on encoder-decoder interaction; can be more complex to integrate and optimize.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Convolutional Encoders for Syntax-aware Neural Machine Translation', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8984.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8984.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multitask linearized parses</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multitask linearized constituency parse prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multitask training setup where the NMT model is trained jointly to produce translations and to predict linearized constituency parses, providing syntactic supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multitask Sequence to Sequence Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Linearized constituency parse as auxiliary sequence</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Constituency parse trees are linearized into sequences and used as an auxiliary prediction target in multitask learning; weights are shared so translation encoder/decoder benefits from syntactic supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Constituency parse trees (linearized for sequence supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Linearization / serialization of parse trees into token sequences used as auxiliary outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Neural Machine Translation (improved encoder/decoder representations via multitask supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Mentioned as an indirect way to inject syntax compared to explicit graph encoders; no direct comparisons presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Doesn't require changing core seq2seq architecture; provides syntactic signal via additional training objective.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Indirect, may not expose full structured relations explicitly to encoder representations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Convolutional Encoders for Syntax-aware Neural Machine Translation', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8984.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8984.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Latent graph parser encoder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Source-side latent graph parsing encoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Encoder that induces a latent (soft) graph structure over source words, encoding each word as a learned average of potential parent representations to emulate a relaxed dependency tree.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural machine translation with source-side latent graph parsing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Latent dependency-like graph encoding (averaged parent representations)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Instead of using external treebank parses, the model learns soft/latent attachment weights and composes word representations as averages over potential parents, producing a latent graph-like encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Latent dependency-like graphs (soft, learned attachments)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No explicit conversion to text; latent attachments are learned as weights used to combine parent representations into token encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Neural Machine Translation (source-side encoder learned from translation data, optionally pre-trained with treebanks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported as achieving modest improvements when pre-trained on treebank (paper cites modest gains but no numbers provided in this text).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Mentioned as related work that learns structure from translation data (in contrast to models relying on external parsers); no direct empirical comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Learns structure jointly with translation objective without requiring external parsers; flexible latent structure.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>May yield modest gains unless pre-trained; details and numerical performance not presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Convolutional Encoders for Syntax-aware Neural Machine Translation', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8984.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8984.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Recursive conv tree (compressive)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recursive convolutional neural network tree encoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recursive convolutional model that incrementally builds a tree over word leaf nodes and compresses the source sentence representation into a single vector for sequence-to-sequence models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On the Properties of Neural Machine Translation: Encoder-Decoder Approaches</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Recursive tree composition (single-vector compression)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Builds a tree via recursive composition from word leaves and compresses sentence into one vector representation for the decoder to condition on.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Induced tree structure over words (binary composition tree)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No conversion to text; recursive composition over tree nodes produces a fixed-size sentence vector.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Neural Machine Translation (encoder-decoder conditioning on a single vector)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Mentioned as contrasting approach that compresses into a single vector (which the paper argues is inferior to attention-based methods); no numerical comparison provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Provides hierarchical composition from words to sentence vector.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Compresses entire sentence into a single vector (bottleneck), which is less effective than attention-based encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Convolutional Encoders for Syntax-aware Neural Machine Translation', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling <em>(Rating: 2)</em></li>
                <li>Semisupervised classification with graph convolutional networks <em>(Rating: 2)</em></li>
                <li>Towards String-to-Tree Neural Machine Translation <em>(Rating: 2)</em></li>
                <li>Linguistic Input Features Improve Neural Machine Translation <em>(Rating: 2)</em></li>
                <li>Tree-to-sequence attentional neural machine translation <em>(Rating: 2)</em></li>
                <li>Neural machine translation with source-side latent graph parsing <em>(Rating: 2)</em></li>
                <li>Multitask Sequence to Sequence Learning <em>(Rating: 1)</em></li>
                <li>On the Properties of Neural Machine Translation: Encoder-Decoder Approaches <em>(Rating: 1)</em></li>
                <li>Learning to compose words into sentences with reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8984",
    "paper_id": "paper-2784000e1a3554374662f4d18cb5ad52f59c8de6",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "Syntactic GCN",
            "name_full": "Syntactic Graph Convolutional Network",
            "brief_description": "A variant of Graph Convolutional Networks adapted to directed, labeled dependency graphs with edge-wise gating; used to compute word-level vector representations by propagating and aggregating information over syntactic edges and their labels.",
            "citation_title": "Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling",
            "mention_or_use": "use",
            "representation_name": "Graph convolutional encoding (node-level vectors)",
            "representation_description": "Nodes (words) are represented by input vectors (embeddings or encoder hidden states). Each GCN layer computes new node vectors by summing (direction- and label-conditioned) linear transforms of neighbor node vectors, applying label-specific bias terms, gating each edge with a learned scalar, and a nonlinearity; stacking layers integrates information from k-hop neighborhoods. Self-loops (loop direction) are modeled separately and residual connections can be used for multi-layer GCNs.",
            "graph_type": "Syntactic dependency trees (directed, labeled graphs); in principle other graph structures (SRL, AMR, coreference) are supported",
            "conversion_method": "No conversion to text; the graph is consumed directly by the GCN: messages are passed along directed labeled edges to produce updated word representations (i.e., vector encodings) that are fed into the NMT encoder/attention mechanism.",
            "downstream_task": "Neural Machine Translation (source-side encoder within attention-based encoder-decoder); also evaluated on synthetic reordering task (artificial sequences) for illustration.",
            "performance_metrics": "Reported BLEU improvements: English-German: +1.2 BLEU points (BLEU4 improvement reported in intro); English-Czech: +0.7 BLEU points (BLEU4). Test/validation specifics: English-Czech test BLEU4 improved from 8.9 (BiRNN) to 9.6 (+GCN). On full En-De data + beam search, BiRNN baseline 23.3 BLEU4 vs BiRNN+GCN 23.9 BLEU4. Also consistent gains in BLEU1 and Kendall tau (word order) reported; artificial reordering task achieved validation BLEU 99.2.",
            "comparison_to_others": "Compared experimentally to three encoder baselines (BoW, CNN, BiRNN); GCN layers on top of each encoder yielded consistent improvements over their syntax-agnostic counterparts. In discussion, contrasted qualitatively with other syntax integration approaches (feature embeddings, tree encoders, multitask, string-to-tree) but not directly compared on the same metrics in this paper.",
            "advantages": "Directly integrates labeled, directed syntactic structure into word representations; flexible to any graph-structured linguistic annotation; edge-wise gating helps ignore noisy predicted edges; simple and computationally inexpensive; consistently improves BLEU, BLEU1 and Kendall tau across encoders and languages; can be stacked on top of existing encoders (BiRNN, CNN, BoW).",
            "disadvantages": "Relies on predicted syntactic parses (noise in parses can limit gains); requires modeling direction and labels (added parameters) though mitigated by making only biases label-specific; improvements are modest when baseline encoders (e.g., BiRNNs) already capture some syntax.",
            "failure_cases": "No explicit catastrophic failure cases reported; paper notes smaller-than-expected extra gains on longer sentences possibly due to noisier parses for longer inputs. In artificial experiments, GCN without edge-wise gating would struggle to distinguish fake vs real edges (motivating gating).",
            "uuid": "e8984.0",
            "source_info": {
                "paper_title": "Graph Convolutional Encoders for Syntax-aware Neural Machine Translation",
                "publication_date_yy_mm": "2017-04"
            }
        },
        {
            "name_short": "String-to-Tree / Linearized parse trees",
            "name_full": "Neural string-to-tree (linearized parse tree prediction)",
            "brief_description": "Approach that represents syntactic trees as linearized strings and trains the NMT system to produce or consume such linearized parse trees (stringified tree representations) as sequences.",
            "citation_title": "Towards String-to-Tree Neural Machine Translation",
            "mention_or_use": "mention",
            "representation_name": "Linearized parse tree (stringified tree)",
            "representation_description": "A full syntactic tree (e.g., constituency/parse tree) is serialized into a linear sequence (string) that encodes tree structure (parentheses or special tokens) which can be predicted or consumed by a sequence model.",
            "graph_type": "Constituency / parse trees (trees serialized as strings)",
            "conversion_method": "Tree linearization / serialization into a token sequence (string-to-tree / tree-to-string paradigms); the model predicts linearized parse trees as output or uses them as additional supervision.",
            "downstream_task": "Neural Machine Translation (string-to-tree translation that outputs target parse structures or uses source/target parses)",
            "performance_metrics": "",
            "comparison_to_others": "Mentioned in related work as an alternative to integrating syntax directly into encoders; no direct empirical comparison provided in this paper.",
            "advantages": "Allows leveraging full tree structure within sequence-to-sequence framework without specialized graph modules (uses existing seq2seq machinery).",
            "disadvantages": "Paper does not detail, but linearization can increase sequence length and may impose rigid structural constraints; no empirical details provided here.",
            "failure_cases": "",
            "uuid": "e8984.1",
            "source_info": {
                "paper_title": "Graph Convolutional Encoders for Syntax-aware Neural Machine Translation",
                "publication_date_yy_mm": "2017-04"
            }
        },
        {
            "name_short": "Feature embeddings (POS/Deps)",
            "name_full": "Linguistic feature embeddings (POS tags, lemmas, dependency labels)",
            "brief_description": "Embedding discrete linguistic features (e.g., POS, lemmas, dependency labels) and concatenating or feeding them together with word embeddings into the NMT model to inject syntactic/linguistic information.",
            "citation_title": "Linguistic Input Features Improve Neural Machine Translation",
            "mention_or_use": "mention",
            "representation_name": "Feature-augmented token representation",
            "representation_description": "Each token's representation is augmented with learned embeddings of linguistic features (POS tags, lemma ids, dependency labels) which are concatenated or combined with the word embedding and provided to the encoder.",
            "graph_type": "Dependency labels and other token-level linguistic annotations (not full graphs)",
            "conversion_method": "No graph linearization; token-level features (including dependency labels) are embedded and appended to word embeddings.",
            "downstream_task": "Neural Machine Translation (source-side feature augmentation)",
            "performance_metrics": "",
            "comparison_to_others": "Mentioned as a simpler approach to inject syntax compared to explicit graph encoders; no direct quantitative comparison in this paper.",
            "advantages": "Simple to implement; directly provides token-level syntactic signals to model.",
            "disadvantages": "Indirect incorporation of structure; does not model relations between tokens as a graph beyond local label features.",
            "failure_cases": "",
            "uuid": "e8984.2",
            "source_info": {
                "paper_title": "Graph Convolutional Encoders for Syntax-aware Neural Machine Translation",
                "publication_date_yy_mm": "2017-04"
            }
        },
        {
            "name_short": "Tree-LSTM encoder",
            "name_full": "Tree-LSTM sentence/tree encoder",
            "brief_description": "A tree-structured LSTM encoder that composes representations following the syntactic (e.g., HPSG or constituency) tree to produce node and sentence representations to be used in NMT.",
            "citation_title": "Tree-to-sequence attentional neural machine translation",
            "mention_or_use": "mention",
            "representation_name": "Tree-structured LSTM encoding",
            "representation_description": "Internal nodes of a parse tree are encoded using Tree-LSTM composition; node and leaf representations participate in the attention mechanism so the decoder can attend to word and phrase/node vectors.",
            "graph_type": "Constituency/HPSG parse trees (tree-structured syntax)",
            "conversion_method": "No linearization to text; the tree is consumed by a recursive Tree-LSTM that computes node vectors following tree topology.",
            "downstream_task": "Neural Machine Translation (tree-to-sequence models where tree encodings are used in decoder attention)",
            "performance_metrics": "",
            "comparison_to_others": "Mentioned as a syntax-aware encoder alternative; contrasted with methods that integrate structure more indirectly (multi-task) or with GCNs, but no direct head-to-head metrics in this paper.",
            "advantages": "Directly models hierarchical phrase structure and produces node representations that can be attended to by decoder.",
            "disadvantages": "More rigid (requires tree structure), may impose constraints on encoder-decoder interaction; can be more complex to integrate and optimize.",
            "failure_cases": "",
            "uuid": "e8984.3",
            "source_info": {
                "paper_title": "Graph Convolutional Encoders for Syntax-aware Neural Machine Translation",
                "publication_date_yy_mm": "2017-04"
            }
        },
        {
            "name_short": "Multitask linearized parses",
            "name_full": "Multitask linearized constituency parse prediction",
            "brief_description": "A multitask training setup where the NMT model is trained jointly to produce translations and to predict linearized constituency parses, providing syntactic supervision.",
            "citation_title": "Multitask Sequence to Sequence Learning",
            "mention_or_use": "mention",
            "representation_name": "Linearized constituency parse as auxiliary sequence",
            "representation_description": "Constituency parse trees are linearized into sequences and used as an auxiliary prediction target in multitask learning; weights are shared so translation encoder/decoder benefits from syntactic supervision.",
            "graph_type": "Constituency parse trees (linearized for sequence supervision)",
            "conversion_method": "Linearization / serialization of parse trees into token sequences used as auxiliary outputs.",
            "downstream_task": "Neural Machine Translation (improved encoder/decoder representations via multitask supervision)",
            "performance_metrics": "",
            "comparison_to_others": "Mentioned as an indirect way to inject syntax compared to explicit graph encoders; no direct comparisons presented here.",
            "advantages": "Doesn't require changing core seq2seq architecture; provides syntactic signal via additional training objective.",
            "disadvantages": "Indirect, may not expose full structured relations explicitly to encoder representations.",
            "failure_cases": "",
            "uuid": "e8984.4",
            "source_info": {
                "paper_title": "Graph Convolutional Encoders for Syntax-aware Neural Machine Translation",
                "publication_date_yy_mm": "2017-04"
            }
        },
        {
            "name_short": "Latent graph parser encoder",
            "name_full": "Source-side latent graph parsing encoder",
            "brief_description": "Encoder that induces a latent (soft) graph structure over source words, encoding each word as a learned average of potential parent representations to emulate a relaxed dependency tree.",
            "citation_title": "Neural machine translation with source-side latent graph parsing",
            "mention_or_use": "mention",
            "representation_name": "Latent dependency-like graph encoding (averaged parent representations)",
            "representation_description": "Instead of using external treebank parses, the model learns soft/latent attachment weights and composes word representations as averages over potential parents, producing a latent graph-like encoding.",
            "graph_type": "Latent dependency-like graphs (soft, learned attachments)",
            "conversion_method": "No explicit conversion to text; latent attachments are learned as weights used to combine parent representations into token encodings.",
            "downstream_task": "Neural Machine Translation (source-side encoder learned from translation data, optionally pre-trained with treebanks)",
            "performance_metrics": "Reported as achieving modest improvements when pre-trained on treebank (paper cites modest gains but no numbers provided in this text).",
            "comparison_to_others": "Mentioned as related work that learns structure from translation data (in contrast to models relying on external parsers); no direct empirical comparisons in this paper.",
            "advantages": "Learns structure jointly with translation objective without requiring external parsers; flexible latent structure.",
            "disadvantages": "May yield modest gains unless pre-trained; details and numerical performance not presented here.",
            "failure_cases": "",
            "uuid": "e8984.5",
            "source_info": {
                "paper_title": "Graph Convolutional Encoders for Syntax-aware Neural Machine Translation",
                "publication_date_yy_mm": "2017-04"
            }
        },
        {
            "name_short": "Recursive conv tree (compressive)",
            "name_full": "Recursive convolutional neural network tree encoder",
            "brief_description": "A recursive convolutional model that incrementally builds a tree over word leaf nodes and compresses the source sentence representation into a single vector for sequence-to-sequence models.",
            "citation_title": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches",
            "mention_or_use": "mention",
            "representation_name": "Recursive tree composition (single-vector compression)",
            "representation_description": "Builds a tree via recursive composition from word leaves and compresses sentence into one vector representation for the decoder to condition on.",
            "graph_type": "Induced tree structure over words (binary composition tree)",
            "conversion_method": "No conversion to text; recursive composition over tree nodes produces a fixed-size sentence vector.",
            "downstream_task": "Neural Machine Translation (encoder-decoder conditioning on a single vector)",
            "performance_metrics": "",
            "comparison_to_others": "Mentioned as contrasting approach that compresses into a single vector (which the paper argues is inferior to attention-based methods); no numerical comparison provided here.",
            "advantages": "Provides hierarchical composition from words to sentence vector.",
            "disadvantages": "Compresses entire sentence into a single vector (bottleneck), which is less effective than attention-based encoders.",
            "failure_cases": "",
            "uuid": "e8984.6",
            "source_info": {
                "paper_title": "Graph Convolutional Encoders for Syntax-aware Neural Machine Translation",
                "publication_date_yy_mm": "2017-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling",
            "rating": 2
        },
        {
            "paper_title": "Semisupervised classification with graph convolutional networks",
            "rating": 2
        },
        {
            "paper_title": "Towards String-to-Tree Neural Machine Translation",
            "rating": 2
        },
        {
            "paper_title": "Linguistic Input Features Improve Neural Machine Translation",
            "rating": 2
        },
        {
            "paper_title": "Tree-to-sequence attentional neural machine translation",
            "rating": 2
        },
        {
            "paper_title": "Neural machine translation with source-side latent graph parsing",
            "rating": 2
        },
        {
            "paper_title": "Multitask Sequence to Sequence Learning",
            "rating": 1
        },
        {
            "paper_title": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches",
            "rating": 1
        },
        {
            "paper_title": "Learning to compose words into sentences with reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.01638725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Graph Convolutional Encoders for Syntax-aware Neural Machine Translation</h1>
<p>Jasmijn Bastings ${ }^{1}$ Ivan Titov ${ }^{1,2}$ Wilker Aziz ${ }^{1}$<br>Diego Marcheggiani ${ }^{1}$ Khalil Sima'an ${ }^{1}$<br>${ }^{1}$ ILLC, University of Amsterdam ${ }^{2}$ ILCC, University of Edinburgh<br>{bastings, titov,w.aziz, marcheggiani,k.simaan}@uva.nl</p>
<h4>Abstract</h4>
<p>We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoderdecoder models for machine translation. We rely on graph-convolutional networks (GCNs), a recent class of neural networks developed for modeling graph-structured data. Our GCNs use predicted syntactic dependency trees of source sentences to produce representations of words (i.e. hidden states of the encoder) that are sensitive to their syntactic neighborhoods. GCNs take word representations as input and produce word representations as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidirectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups.</p>
<h2>1 Introduction</h2>
<p>Neural machine translation (NMT) is one of success stories of deep learning in natural language processing, with recent NMT systems outperforming traditional phrase-based approaches on many language pairs (Sennrich et al., 2016a). State-of-the-art NMT systems rely on sequential encoderdecoders (Sutskever et al., 2014; Bahdanau et al., 2015) and lack any explicit modeling of syntax or any hierarchical structure of language. One potential reason for why we have not seen much benefit from using syntactic information in NMT is the lack of simple and effective methods for incorporating structured information in neural encoders,
including RNNs. Despite some successes, techniques explored so far either incorporate syntactic information in NMT models in a relatively indirect way (e.g., multi-task learning (Luong et al., 2015a; Nadejde et al., 2017; Eriguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.g., learning representations of linguistic phrases (Eriguchi et al., 2016)). Our goal is to provide the encoder with access to rich syntactic information but let it decide which aspects of syntax are beneficial for MT, without placing rigid constraints on the interaction between syntax and the translation task. This goal is in line with claims that rigid syntactic constraints typically hurt MT (Zollmann and Venugopal, 2006; Smith and Eisner, 2006; Chiang, 2010), and, though these claims have been made in the context of traditional MT systems, we believe they are no less valid for NMT.</p>
<p>Attention-based NMT systems (Bahdanau et al., 2015; Luong et al., 2015b) represent source sentence words as latent-feature vectors in the encoder and use these vectors when generating a translation. Our goal is to automatically incorporate information about syntactic neighborhoods of source words into these feature vectors, and, thus, potentially improve quality of the translation output. Since vectors correspond to words, it is natural for us to use dependency syntax. Dependency trees (see Figure 1) represent syntactic relations between words: for example, monkey is a subject of the predicate eats, and banana is its object.</p>
<p>In order to produce syntax-aware feature representations of words, we exploit graphconvolutional networks (GCNs) (Duvenaud et al., 2015; Defferrard et al., 2016; Kearnes et al., 2016; Kipf and Welling, 2016). GCNs can be regarded as computing a latent-feature representation of a node (i.e. a real-valued vector) based on its $k$ -</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A dependency tree for the example sentence: "The monkey eats a banana."
th order neighborhood (i.e. nodes at most $k$ hops aways from the node) (Gilmer et al., 2017). They are generally simple and computationally inexpensive. We use Syntactic GCNs, a version of GCN operating on top of syntactic dependency trees, recently shown effective in the context of semantic role labeling (Marcheggiani and Titov, 2017).</p>
<p>Since syntactic GCNs produce representations at word level, it is straightforward to use them as encoders within the attention-based encoderdecoder framework. As NMT systems are trained end-to-end, GCNs end up capturing syntactic properties specifically relevant to the translation task. Though GCNs can take word embeddings as input, we will see that they are more effective when used as layers on top of recurrent neural network (RNN) or convolutional neural network (CNN) encoders (Gehring et al., 2016), enriching their states with syntactic information. A comparison to RNNs is the most challenging test for GCNs, as it has been shown that RNNs (e.g., LSTMs) are able to capture certain syntactic phenomena (e.g., subject-verb agreement) reasonably well on their own, without explicit treebank supervision (Linzen et al., 2016; Shi et al., 2016). Nevertheless, GCNs appear beneficial even in this challenging set-up: we obtain +1.2 and +0.7 BLEU point improvements from using syntactic GCNs on top of bidirectional RNNs for EnglishGerman and English-Czech, respectively.</p>
<p>In principle, GCNs are flexible enough to incorporate any linguistic structure as long as they can be represented as graphs (e.g., dependency-based semantic-role labeling representations (Surdeanu et al., 2008), AMR semantic graphs (Banarescu et al., 2012) and co-reference chains). For example, unlike recursive neural networks (Socher et al., 2013), GCNs do not require the graphs to be trees. However, in this work we solely focus on dependency syntax and leave more general investigation for future work.</p>
<p>Our main contributions can be summarized as follows:</p>
<ul>
<li>we introduce a method for incorporating structure into NMT using syntactic GCNs;</li>
<li>we show that GCNs can be used along with RNN and CNN encoders;</li>
<li>we show that incorporating structure is beneficial for machine translation on EnglishCzech and English-German.</li>
</ul>
<h2>2 Background</h2>
<p>Notation. We use $\mathbf{x}$ for vectors, $\mathbf{x}<em i="i">{1: t}$ for a sequence of $t$ vectors, and $X$ for matrices. The $i$-th value of vector $\mathbf{x}$ is denoted by $x</em>$. We use $\circ$ for vector concatenation.</p>
<h3>2.1 Neural Machine Translation</h3>
<p>In NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b), given example translation pairs from a parallel corpus, a neural network is trained to directly estimate the conditional distribution $p\left(y_{1: T_{y}} \mid x_{1: T_{x}}\right)$ of translating a source sentence $x_{1: T_{x}}$ (a sequence of $T_{x}$ words) into a target sentence $y_{1: T_{y}}$. NMT models typically consist of an encoder, a decoder and some method for conditioning the decoder on the encoder, for example, an attention mechanism. We will now briefly describe the components that we use in this paper.</p>
<h3>2.1.1 Encoders</h3>
<p>An encoder is a function that takes as input the source sentence and produces a representation encoding its semantic content. We describe recurrent, convolutional and bag-of-words encoders.</p>
<p>Recurrent. Recurrent neural networks (RNNs) (Elman, 1990) model sequential data. They receive one input vector at each time step and update their hidden state to summarize all inputs up to that point. Given an input sequence $\mathbf{x}<em x="x">{1: T</em>}}=$ $\mathbf{x<em 2="2">{1}, \mathbf{x}</em>}, \ldots, \mathbf{x<em x="x">{T</em>$ of word embeddings an RNN is defined recursively as follows:}</p>
<p>$$
\operatorname{RNN}\left(\mathbf{x}<em t="t">{1: t}\right)=f\left(\mathbf{x}</em>\right)\right)
$$}, \operatorname{RNN}\left(\mathbf{x}_{1: t-1</p>
<p>where $f$ is a nonlinear function such as an LSTM (Hochreiter and Schmidhuber, 1997) or a GRU (Cho et al., 2014b). We will use the function RNN as an abstract mapping from an input sequence $\mathbf{x}<em 1:="1:" T__x="T_{x">{1: T}$ to final hidden state $\operatorname{RNN}\left(\mathbf{x}</em>\right)$, regardless of the used nonlinearity. To not only summarize the past of a word, but also its future, a bidirectional RNN (Schuster and Paliwal, 1997; Irsoy and}</p>
<p>Cardie, 2014) is often used. A bidirectional RNN reads the input sentence in two directions and then concatenates the states for each time step:
$\operatorname{BitNN}\left(\mathbf{x}<em x="x">{1: T</em>}}, t\right)=\operatorname{RNN<em 1:="1:" t="t">{F}\left(\mathbf{x}</em>}\right) \circ \operatorname{RNN<em T__x="T_{x">{B}\left(\mathbf{x}</em>\right)$
where $\mathrm{RNN}}: t<em B="B">{F}$ and $\mathrm{RNN}</em>$ are the forward and backward RNNs, respectively. For further details we refer to the encoder of Bahdanau et al. (2015).</p>
<p>Convolutional. Convolutional Neural Networks (CNNs) apply a fixed-size window over the input sequence to capture the local context of each word (Gehring et al., 2016). One advantage of this approach over RNNs is that it allows for fast parallel computation, while sacrificing non-local context. To remedy the loss of context, multiple CNN layers can be stacked. Formally, given an input sequence $\mathbf{x}<em x="x">{1: T</em>$, we define a CNN as follows:
$\operatorname{CNN}\left(\mathbf{x}}<em x="x">{1: T</em>}}, t\right)=f\left(\mathbf{x<em t="t">{t-\lfloor w / 2\rfloor}, \ldots \mathbf{x}</em>\right)$
where $f$ is a nonlinear function, typically a linear transformation followed by ReLU, and $w$ is the size of the window.}, \ldots \mathbf{x}_{t+\lfloor w / 2\rfloor</p>
<p>Bag-of-Words. In a bag-of-words (BoW) encoder every word is simply represented by its word embedding. To give the decoder some sense of word position, position embeddings (PE) may be added. There are different strategies for defining position embeddings, and in this paper we choose to learn a vector for each absolute word position up to a certain maximum length. We then represent the $t$-th word in a sequence as follows:</p>
<p>$$
\operatorname{BoW}\left(\mathbf{x}<em x="x">{1: T</em>}}, t\right)=\mathbf{x<em t="t">{t}+\mathbf{p}</em>
$$</p>
<p>where $\mathbf{x}<em t="t">{t}$ is the word embedding and $\mathbf{p}</em>$ is the $t$-th position embedding.</p>
<h3>2.1.2 Decoder</h3>
<p>A decoder produces the target sentence conditioned on the representation of the source sentence induced by the encoder. In Bahdanau et al. (2015) the decoder is implemented as an RNN conditioned on an additional input $\mathbf{c}_{i}$, the context vector, which is dynamically computed at each time step using an attention mechanism.</p>
<p>The probability of a target word $y_{i}$ is now a function of the decoder RNN state, the previous target word embedding, and the context vector. The model is trained end-to-end for maximum log likelihood of the next target word given its context.</p>
<h3>2.2 Graph Convolutional Networks</h3>
<p>We will now describe the Graph Convolutional Networks (GCNs) of Kipf and Welling (2016). For a comprehensive overview of alternative GCN architectures see Gilmer et al. (2017).</p>
<p>A GCN is a multilayer neural network that operates directly on a graph, encoding information about the neighborhood of a node as a realvalued vector. In each GCN layer, information flows along edges of the graph; in other words, each node receives messages from all its immediate neighbors. When multiple GCN layers are stacked, information about larger neighborhoods gets integrated. For example, in the second layer, a node will receive information from its immediate neighbors, but this information already includes information from their respective neighbors. By choosing the number of GCN layers, we regulate the distance the information travels: with $k$ layers a node receives information from neighbors at most $k$ hops away.</p>
<p>Formally, consider an undirected graph $\mathcal{G}=$ $(\mathcal{V}, \mathcal{E})$, where $\mathcal{V}$ is a set of $n$ nodes, and $\mathcal{E}$ is a set of edges. Every node is assumed to be connected to itself, i.e. $\forall v \in \mathcal{V}:(v, v) \in \mathcal{E}$. Now, let $X \in \mathbb{R}^{d \times n}$ be a matrix containing all $n$ nodes with their features, where $d$ is the dimensionality of the feature vectors. In our case, $X$ will contain word embeddings, but in general it can contain any kind of features. For a 1-layer GCN, the new node representations are computed as follows:</p>
<p>$$
\mathbf{h}<em _in="\in" _mathcal_N="\mathcal{N" u="u">{v}=\rho\left(\sum</em>\right)
$$}(v)} W \mathbf{x}_{u}+\mathbf{b</p>
<p>where $W \in \mathbb{R}^{d \times d}$ is a weight matrix and $\mathbf{b} \in \mathbb{R}^{d}$ a bias vector. ${ }^{1} \rho$ is an activation function, e.g. a $\operatorname{ReLU} . \mathcal{N}(v)$ is the set of neighbors of $v$, which we assume here to always include $v$ itself. As stated before, to allow information to flow over multiple hops, we need to stack GCN layers. The recursive computation is as follows:</p>
<p>$$
\mathbf{h}<em _in="\in" _mathcal_N="\mathcal{N" u="u">{v}^{(j+1)}=\rho\left(\sum</em>\right)
$$}(v)} W^{(j)} \mathbf{h}_{u}^{(j)}+\mathbf{b}^{(j)</p>
<p>where $j$ indexes the layer, and $\mathbf{h}<em v="v">{v}^{(0)}=\mathbf{x}</em>$.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A 2-layer syntactic GCN on top of a convolutional encoder. Loop connections are depicted with dashed edges, syntactic ones with solid (dependents to heads) and dotted (heads to dependents) edges. Gates and some labels are omitted for clarity.</p>
<h3>2.3 Syntactic GCNs</h3>
<p>Marcheggiani and Titov (2017) generalize GCNs to operate on directed and labeled graphs. This makes it possible to use linguistic structures such as dependency trees, where directionality and edge labels play an important role. They also integrate edge-wise gates which let the model regulate contributions of individual dependency edges. We will briefly describe these modifications.</p>
<p><strong>Directionality.</strong> In order to deal with directionality of edges, separate weight matrices are used for incoming and outgoing edges. We follow the convention that in dependency trees heads point to their dependents, and thus <em>outgoing</em> edges are used for head-to-dependent connections, and <em>incoming</em> edges are used for dependent-to-head connections. Modifying the recursive computation for directionality, we arrive at:</p>
<p>$$
\mathbf{h}<em _in="\in" _mathcal_N="\mathcal{N" u="u">v^{(j+1)} = \rho \left( \sum</em>}(v)} W_{\text{dir}(u,v)}^{(j)} \mathbf{h<em _text_dir="\text{dir">u^{(j)} + \mathbf{h}</em> \right)
$$}(u,v)}^{(j)</p>
<p>where $\text{dir}(u,v)$ selects the weight matrix associated with the directionality of the edge connecting $u$ and $v$ (i.e. $W_{\text{IN}}$ for $u$-to-$v$, $W_{\text{OUT}}$ for $v$-to-$u$, and $W_{\text{LOOP}}$ for $v$-to-$v$). Note that self loops are modeled separately,</p>
<p>so there are now three times as many parameters as in a non-directional GCN.</p>
<p><strong>Labels.</strong> Making the GCN sensitive to labels is straightforward given the above modifications for directionality. Instead of using separate matrices for each direction, separate matrices are now defined for each direction and label combination:</p>
<p>$$
\mathbf{h}<em _in="\in" _mathcal_N="\mathcal{N" u="u">v^{(j+1)} = \rho \left( \sum</em>}(v)} W_{\text{lab}(u,v)}^{(j)} \mathbf{h<em _text_lab="\text{lab">u^{(j)} + \mathbf{h}</em> \right)
$$}(u,v)}^{(j)</p>
<p>where we incorporate the directionality of an edge directly in its label.</p>
<p>Importantly, to prevent over-parametrization, only bias terms are made label-specific, in other words: $W_{\text{lab}(u,v)} = W_{\text{dir}(u,v)}$. The resulting syntactic GCN is illustrated in Figure 2 (shown on top of a CNN, as we will explain in the subsequent section).</p>
<p><strong>Edge-wise gating.</strong> Syntactic GCNs also include gates, which can down-weight the contribution of individual edges. They also allow the model to deal with noisy predicted structure, i.e. to ignore potentially erroneous syntactic edges. For each edge, a scalar gate is calculated as follows:</p>
<p>$$
g_{u,v}^{(j)} = \sigma \left( \mathbf{h}<em _text_dir="\text{dir">u^{(j)} \cdot \hat{\mathbf{w}}</em> \right)
$$}(u,v)}^{(j)} + \hat{b}_{\text{lab}(u,v)}^{(j)</p>
<p>where $\sigma$ is the logistic sigmoid function, and $\hat{\mathbf{w}}<em _text_lab="\text{lab">{\text{dir}(u,v)}^{(j)} \in \mathbb{R}^{d}$ and $\hat{b}</em>$ are learned parameters for the gate. The computation becomes:}(u,v)}^{(j)} \in \mathbb{R</p>
<p>$$
\mathbf{h}<em _in="\in" _mathcal_N="\mathcal{N" u="u">v^{(j+1)} = \rho \left( \sum</em>}(v)} g_{u,v}^{(j)} \left( W_{\text{dir}(u,v)}^{(j)} \mathbf{h<em _text_lab="\text{lab">u^{(j)} + \mathbf{h}</em> \right) \right)
$$}(u,v)}^{(j)</p>
<p>[27]</p>
<h2>3 Graph Convolutional Encoders</h2>
<p>In this work we focus on exploiting structural information on the source side, i.e. in the encoder. We hypothesize that using an encoder that incorporates syntax will lead to more informative representations of words, and that these representations, when used as context vectors by the decoder, will lead to an improvement in translation quality. Consequently, in all our models, we use the decoder of Bahdanau et al. (2015) and keep this part of the model constant. As is now common practice, we do not use a maxout layer in the decoder, but apart from this we do not deviate from the original definition. In all models we make use of GRUs (Cho et al., 2014b) as our RNN units.</p>
<p>Our models vary in the encoder part, where we exploit the power of GCNs to induce syntacticallyaware representations. We now define a series of encoders of increasing complexity.</p>
<p>BoW + GCN. In our first and simplest model, we propose a bag-of-words encoder (with position embeddings, see $\S 2.1 .1$ ), with a GCN on top. In other words, inputs $\mathbf{h}^{(0)}$ are a sum of embeddings of a word and its position in a sentence. Since the original BoW encoder captures the linear ordering information only in a very crude way (through the position embeddings), the structural information provided by GCN should be highly beneficial.</p>
<p>Convolutional + GCN. In our second model, we use convolutional neural networks to learn word representations. CNNs are fast, but by definition only use a limited window of context. Instead of the approach used by Gehring et al. (2016) (i.e. stacking mulitple CNN layers on top of each other), we use a GCN to enrich the one-layer CNN representations. Figure 2 shows this model. Note that, while the figure shows a CNN with a window size of 3 , we will use a larger window size of 5 in our experiments. We expect this model to perform better than BoW + GCN, because of the additional local context captured by the CNN.</p>
<p>BiRNN + GCN. In our third and most powerful model, we employ bidirectional recurrent neural networks. In this model, we start by encoding the source sentence using a BiRNN (i.e. BiGRU), and use the resulting hidden states as input to a GCN. Instead of relying on linear order only, the GCN will allow the encoder to 'teleport' over parts of the input sentence, along dependency edges, con-
necting words that otherwise might be far apart. The model might not only benefit from this teleporting capability however; also the nature of the relations between words (i.e. dependency relation types) may be useful, and the GCN exploits this information (see $\S 2.3$ for details).</p>
<p>This is the most challenging setup for GCNs, as RNNs have been shown capable of capturing at least some degree of syntactic information without explicit supervision (Linzen et al., 2016), and hence they should be hard to improve on by incorporating treebank syntax.</p>
<p>Marcheggiani and Titov (2017) did not observe improvements from using multiple GCN layers in semantic role labeling. However, we do expect that propagating information from further in the tree should be beneficial in principle. We hypothesize that the first layer is the most influential one, capturing most of the syntactic context, and that additional layers only modestly modify the representations. To ease optimization, we add a residual connection (He et al., 2016) between the GCN layers, when using more than one layer.</p>
<h2>4 Experiments</h2>
<p>Experiments are performed using the Neural Monkey toolkit ${ }^{3}$ (Helcl and Libovick, 2017), which implements the model of Bahdanau et al. (2015) in TensorFlow. We use the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.001 ( 0.0002 for CNN models). ${ }^{4}$ The batch size is set to 80 . Between layers we apply dropout with a probability of 0.2 , and in experiments with GCNs $^{5}$ we use the same value for edge dropout. We train for 45 epochs, evaluating the BLEU performance of the model every epoch on the validation set. For testing, we select the model with the highest validation BLEU. L2 regularization is used with a value of $10^{-8}$. All the model selection (incl. hyperparameter selections) was performed on the validation set. In all experiments we obtain translations using a greedy decoder, i.e. we select the output token with the highest probability at each time step.</p>
<p>We will describe an artificial experiment in $\S 4.1$ and MT experiments in $\S 4.2$.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>4.1 Reordering artificial sequences</h1>
<p>Our goal here is to provide an intuition for the capabilities of GCNs. We define a reordering task where randomly permuted sequences need to be put back into the original order. We encode the original order using edges, and test if GCNs can successfully exploit them. Note that this task is not meant to provide a fair comparison to RNNs. The input (besides the edges) simply does not carry any information about the original ordering, so RNNs cannot possibly solve this task.</p>
<p>Data. From a vocabulary of 26 types, we generate random sequences of 3-10 tokens. We then randomly permute them, pointing every token to its original predecessor with a label sampled from a set of 5 labels. Additionally, we point every token to an arbitrary position in the sequence with a label from a distinct set of 5 'fake' labels. We sample 25000 training and 1000 validation sequences.</p>
<p>Model. We use the BiRNN + GCN model, i.e. a bidirectional GRU with a 1-layer GCN on top. We use 32, 64 and 128 units for embeddings, GRU units and GCN layers, respectively.</p>
<p>Results. After 6 epochs of training, the model learns to put permuted sequences back into order, reaching a validation BLEU of 99.2. Figure 3 shows that the mean values of the bias terms of gates (i.e. $\bar{b}$ ) for real and fake edges are far apart, suggesting that the GCN learns to distinguish them. Interestingly, this illustrates why edge-wise gating is beneficial. A gate-less model would not understand which of the two outgoing arcs is fake and which is genuine, because only biases $b$ would then be label-dependent. Consequently, it would only do a mediocre job in reordering. Although using label-specific matrices $W$ would also help, this would not scale to the real scenario (see $\S 2.3$ ).</p>
<h3>4.2 Machine Translation</h3>
<p>Data. For our experiments we use the En-De and En-Cs News Commentary v11 data from the WMT16 translation task. ${ }^{6}$ For En-De we also train on the full WMT16 data set. As our validation set and test set we use newstest2015 and newstest2016, respectively.</p>
<p>Pre-processing. The English sides of the corpora are tokenized and parsed into dependency</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Mean values of gate bias terms for real (useful) labels and for fake (non useful) labels suggest the GCN learns to distinguish them.
trees by SyntaxNet, ${ }^{7}$ using the pre-trained Parsey McParseface model. ${ }^{8}$ The Czech and German sides are tokenized using the Moses tokenizer. ${ }^{9}$ Sentence pairs where either side is longer than 50 words are filtered out after tokenization.</p>
<p>Vocabularies. For the English sides, we construct vocabularies from all words except those with a training set frequency smaller than three. For Czech and German, to deal with rare words and phenomena such as inflection and compounding, we learn byte-pair encodings (BPE) as described by Sennrich et al. (2016b). Given the size of our data set, and following Wu et al. (2016), we use 8000 BPE merges to obtain robust frequencies for our subword units ( 16000 merges for full data experiment). Data set statistics are summarized in Table 1 and vocabulary sizes in Table 2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">Train</th>
<th style="text-align: right;">Val.</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">English-German</td>
<td style="text-align: right;">226822</td>
<td style="text-align: right;">2169</td>
<td style="text-align: center;">2999</td>
</tr>
<tr>
<td style="text-align: left;">English-German (full)</td>
<td style="text-align: right;">4500966</td>
<td style="text-align: right;">2169</td>
<td style="text-align: center;">2999</td>
</tr>
<tr>
<td style="text-align: left;">English-Czech</td>
<td style="text-align: right;">181112</td>
<td style="text-align: right;">2656</td>
<td style="text-align: center;">2999</td>
</tr>
</tbody>
</table>
<p>Table 1: The number of sentences in our data sets.</p>
<p>Hyperparameters. We use 256 units for word embeddings, 512 units for GRUs ( 800 for En-De full data set experiment), and 512 units for convolutional layers (or equivalently, 512 'channels'). The dimensionality of the GCN layers is equiva-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Source</th>
<th style="text-align: center;">Target</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">English-German</td>
<td style="text-align: center;">37824</td>
<td style="text-align: center;">8099 (BPE)</td>
</tr>
<tr>
<td style="text-align: left;">English-German (full)</td>
<td style="text-align: center;">50000</td>
<td style="text-align: center;">16000 (BPE)</td>
</tr>
<tr>
<td style="text-align: left;">English-Czech</td>
<td style="text-align: center;">33786</td>
<td style="text-align: center;">8116 (BPE)</td>
</tr>
</tbody>
</table>
<p>Table 2: Vocabulary sizes.
lent to the dimensionality of their input. We report results for 2-layer GCNs, as we find them most effective (see ablation studies below).</p>
<p>Baselines. We provide three baselines, each with a different encoder: a bag-of-words encoder, a convolutional encoder with window size $w=5$, and a BiRNN. See $\S 2.1 .1$ for details.</p>
<p>Evaluation. We report (cased) BLEU results (Papineni et al., 2002) using multi-bleu, as well as Kendall $\tau$ reordering scores. ${ }^{10}$</p>
<h3>4.2.1 Results</h3>
<p>English-German. Table 3 shows test results on English-German. Unsurprisingly, the bag-ofwords baseline performs the worst. We expected the BoW+GCN model to make easy gains over this baseline, which is indeed what happens. The CNN baseline reaches a higher $\mathrm{BLEU}<em 1="1">{4}$ score than the BoW models, but interestingly its $\mathrm{BLEU}</em>}$ score is lower than the BoW+GCN model. The CNN+GCN model improves over the CNN baseline by +1.9 and +1.1 for $\mathrm{BLEU<em 4="4">{1}$ and $\mathrm{BLEU}</em>}$, respectively. The BiRNN, the strongest baseline, reaches a $\mathrm{BLEU<em 1="1">{4}$ of 14.9. Interestingly, GCNs still manage to improve the result by $+2.3 \mathrm{BLEU}</em>}$ and $+1.2 \mathrm{BLEU<em 4="4">{4}$ points. Finally, we observe a big jump in $\mathrm{BLEU}</em>$ by using the full data set and beam search (beam 12). The BiRNN now reaches 23.3, while adding a GCN achieves a score of 23.9 .</p>
<p>English-Czech. Table 4 shows test results on English-Czech. While it is difficult to obtain high absolute BLEU scores on this dataset, we can still see similar relative improvements. Again the BoW baseline scores worst, with the BoW+GCN easily beating that result. The CNN baseline scores $\mathrm{BLEU}<em 1="1">{4}$ of 8.1 , but the $\mathrm{CNN}+\mathrm{GCN}$ improves on that, this time by +1.0 and +0.6 for $\mathrm{BLEU}</em>}$ and $\mathrm{BLEU<em 1="1">{4}$, respectively. Interestingly, $\mathrm{BLEU}</em>$ scores for the BoW+GCN and CNN+GCN models are</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 3: Test results for English-German.
higher than both baselines so far. Finally, the BiRNN baseline scores a $\mathrm{BLEU}<em 1="1">{4}$ of 8.9 , but it is again beaten by the BiRNN+GCN model with +1.9 $\mathrm{BLEU}</em>$.}$ and +0.7 $\mathrm{BLEU}_{4</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Kendall</th>
<th style="text-align: center;">$\mathrm{BLEU}_{1}$</th>
<th style="text-align: center;">$\mathrm{BLEU}_{4}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BoW</td>
<td style="text-align: center;">0.2498</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">6.0</td>
</tr>
<tr>
<td style="text-align: left;">+ GCN</td>
<td style="text-align: center;">0.2561</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">7.5</td>
</tr>
<tr>
<td style="text-align: left;">CNN</td>
<td style="text-align: center;">0.2756</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">8.1</td>
</tr>
<tr>
<td style="text-align: left;">+ GCN</td>
<td style="text-align: center;">0.2850</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">8.7</td>
</tr>
<tr>
<td style="text-align: left;">BiRNN</td>
<td style="text-align: center;">0.2961</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">8.9</td>
</tr>
<tr>
<td style="text-align: left;">+ GCN</td>
<td style="text-align: center;">0.3046</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">9.6</td>
</tr>
</tbody>
</table>
<p>Table 4: Test results for English-Czech.</p>
<p>Effect of GCN layers. How many GCN layers do we need? Every layer gives us an extra hop in the graph and expands the syntactic neighborhood of a word. Table 5 shows validation BLEU performance as a function of the number of GCN layers. For English-German, using a 1-layer GCN improves BLEU-1, but surprisingly has little effect on $\mathrm{BLEU}<em 1="1">{4}$. Adding an additional layer gives improvements on both $\mathrm{BLEU}</em>$ of +1.3 and +0.73 , respectively. For English-Czech, performance increases with each added GCN layer.}$ and $\mathrm{BLEU}_{4</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">En-De</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">En-Cs</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{BLEU}_{1}$</td>
<td style="text-align: center;">$\mathrm{BLEU}_{4}$</td>
<td style="text-align: center;">$\mathrm{BLEU}_{1}$</td>
<td style="text-align: center;">$\mathrm{BLEU}_{4}$</td>
</tr>
<tr>
<td style="text-align: center;">BiRNN</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">8.9</td>
</tr>
<tr>
<td style="text-align: center;">+ GCN (1L)</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">9.6</td>
</tr>
<tr>
<td style="text-align: center;">+ GCN (2L)</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">9.9</td>
</tr>
</tbody>
</table>
<p>Table 5: Validation BLEU for English-German and English-Czech for 1- and 2-layer GCNs.</p>
<p>Effect of sentence length. We hypothesize that GCNs should be more beneficial for longer sentences: these are likely to contain long-distance syntactic dependencies which may not be adequately captured by RNNs but directly encoded in GCNs. To test this, we partition the validation data into five buckets and calculate BLEU for each of them. Figure 4 shows that GCN-based models outperform their respective baselines rather uniformly across all buckets. This is a surprising result. One explanation may be that syntactic parses are noisier for longer sentences, and this prevents us from obtaining extra improvements with GCNs.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Validation BLEU per sentence length.</p>
<p>Discussion. Results suggest that the syntaxaware representations provided by GCNs consistently lead to improved translation performance as measured by $\mathrm{BLEU}<em 1="1">{4}$ (as well as TER and BEER). Consistent gains in terms of Kendall tau and $\mathrm{BLEU}</em>$ indicate that improvements correlate with better word order and lexical/BPE selection, two phenomena that depend crucially on syntax.</p>
<h2>5 Related Work</h2>
<p>We review various accounts to syntax in NMT as well as other convolutional encoders.</p>
<p>Syntactic features and/or constraints. Sennrich and Haddow (2016) embed features such as POS-tags, lemmas and dependency labels and feed these into the network along with word embeddings. Eriguchi et al. (2016) parse English sentences with an HPSG parser and use a Tree-LSTM to encode the internal nodes of the tree. In the decoder, word and node representations compete under the same attention mechanism. Stahlberg et al. (2016) use a pruned lattice from a hierarchical phrase-based model (hiero) to constrain NMT.</p>
<p>Hiero trees are not syntactically-aware, but instead constrained by symmetrized word alignments. Aharoni and Goldberg (2017) propose neural string-to-tree by predicting linearized parse trees.</p>
<p>Multi-task Learning. Sharing NMT parameters with a syntactic parser is a popular approach to obtaining syntactically-aware representations. Luong et al. (2015a) predict linearized constituency parses as an additional task. Eriguchi et al. (2017) multi-task with a target-side RNNG parser (Dyer et al., 2016) and improve on various language pairs with English on the target side. Nadejde et al. (2017) multi-task with CCG tagging, and also integrate syntax on the target side by predicting a sequence of words interleaved with CCG supertags.</p>
<p>Latent structure. Hashimoto and Tsuruoka (2017) add a syntax-inspired encoder on top of a BiLSTM layer. They encode source words as a learned average of potential parents emulating a relaxed dependency tree. While their model is trained purely on translation data, they also experiment with pre-training the encoder using treebank annotation and report modest improvements on English-Japanese. Yogatama et al. (2016) introduce a model for language understanding and generation that composes words into sentences by inducing unlabeled binary bracketing trees.</p>
<p>Convolutional encoders. Gehring et al. (2016) show that CNNs can be competitive to BiRNNs when used as encoders. To increase the receptive field of a word's context they stack multiple CNN layers. Kalchbrenner et al. (2016) use convolution in both the encoder and the decoder; they make use of dilation to increase the receptive field. In contrast to both approaches, we use a GCN informed by dependency structure to increase it. Finally, Cho et al. (2014a) propose a recursive convolutional neural network which builds a tree out of the word leaf nodes, but which ends up compressing the source sentence in a single vector.</p>
<h2>6 Conclusions</h2>
<p>We have presented a simple and effective approach to integrating syntax into neural machine translation models and have shown consistent BLEU 4 improvements for two challenging language pairs: English-German and English-Czech. Since GCNs are capable of encoding any kind of graph-based structure, in future work we would like to go be-</p>
<p>yond syntax, by using semantic annotations such as SRL and AMR, and co-reference chains.</p>
<h2>Acknowledgments</h2>
<p>We would like to thank Michael Schlichtkrull and Thomas Kipf for their suggestions and comments. This work was supported by the European Research Council (ERC StG BroadSem 678254) and the Dutch National Science Foundation (NWO VIDI 639.022.518, NWO VICI 277-89-002).</p>
<h2>References</h2>
<p>Roee Aharoni and Yoav Goldberg. 2017. Towards String-to-Tree Neural Machine Translation. ArXiv e-prints.</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In Proceedings of the International Conference on Learning Representations (ICLR).</p>
<p>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2012. Abstract meaning representation (amr) 1.0 specification. In Conference on Empirical Methods in Natural Language Processing, pages $1533-1544$.</p>
<p>David Chiang. 2010. Learning to translate with source and target syntax. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1443-1452, Uppsala, Sweden. Association for Computational Linguistics.</p>
<p>KyungHyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014a. On the Properties of Neural Machine Translation: EncoderDecoder Approaches. In SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, volume abs/1409.1259, pages 103-111.</p>
<p>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014b. Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724-1734, Doha, Qatar. Association for Computational Linguistics.</p>
<p>Michal Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 3837-3845.</p>
<p>David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Aln Aspuru-Guzik, and Ryan P Adams. 2015. Convolutional networks on graphs for learning molecular fingerprints. In Advances in neural information processing systems, pages 2224-2232.</p>
<p>Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. 2016. Recurrent neural network grammars. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 199-209, San Diego, California. Association for Computational Linguistics.</p>
<p>Jeffrey L Elman. 1990. Finding structure in time. Cognitive science, 14(2):179-211.</p>
<p>Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa Tsuruoka. 2016. Tree-to-sequence attentional neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 823-833, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Akiko Eriguchi, Yoshimasa Tsuruoka, and Kyunghyun Cho. 2017. Learning to Parse and Translate Improves Neural Machine Translation. ArXiv e-prints.</p>
<p>Jonas Gehring, Michael Auli, David Grangier, and Yann N. Dauphin. 2016. A convolutional encoder model for neural machine translation. CoRR, abs/1611.02344.</p>
<p>Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. 2017. Neural Message Passing for Quantum Chemistry. ArXiv eprints.</p>
<p>Kazuma Hashimoto and Yoshimasa Tsuruoka. 2017. Neural machine translation with source-side latent graph parsing. CoRR, abs/1702.02265.</p>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages $770-778$.</p>
<p>Jindich Helcl and Jindich Libovick. 2017. Neural monkey: An open-source tool for sequence learning. The Prague Bulletin of Mathematical Linguistics, (107):5-17.</p>
<p>Sepp Hochreiter and Jrgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation, 9(8):1735-1780.</p>
<p>Ozan Irsoy and Claire Cardie. 2014. Opinion Mining with Deep Recurrent Neural Networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 720-728, Doha, Qatar. Association for Computational Linguistics.</p>
<p>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent Continuous Translation Models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700-1709, Seattle, Washington, USA.</p>
<p>Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aron van den Oord, Alex Graves, and Koray Kavukcuoglu. 2016. Neural machine translation in linear time. CoRR, abs/1610.10099.</p>
<p>Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. 2016. Molecular graph convolutions: moving beyond fingerprints. Journal of computer-aided molecular design, 30(8):595608 .</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In ICLR.</p>
<p>Thomas N. Kipf and Max Welling. 2016. Semisupervised classification with graph convolutional networks. CoRR, abs/1609.02907.</p>
<p>Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing the ability of lstms to learn syntaxsensitive dependencies. Transactions of the Association for Computational Linguistics, 4:521-535.</p>
<p>Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. 2015a. Multitask Sequence to Sequence Learning. CoRR, abs/1511.06114.</p>
<p>Thang Luong, Hieu Pham, and Christopher D. Manning. 2015b. Effective Approaches to Attentionbased Neural Machine Translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 14121421, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Diego Marcheggiani and Ivan Titov. 2017. Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Maria Nadejde, Siva Reddy, Rico Sennrich, Tomasz Dwojak, Marcin Junczys-Dowmunt, Philipp Koehn, and Alexandra Birch. 2017. Syntax-aware Neural Machine Translation Using CCG. ArXiv e-prints.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. pages 311-318.</p>
<p>Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2017. Modeling Relational Data with Graph Convolutional Networks. ArXiv e-prints.</p>
<p>Mike Schuster and Kuldip K. Paliwal. 1997. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 45(11):2673-2681.</p>
<p>Rico Sennrich and Barry Haddow. 2016. Linguistic Input Features Improve Neural Machine Translation. In Proceedings of the First Conference on Machine Translation (WMT16), volume abs/1606.02892.</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016a. Edinburgh neural machine translation systems for wmt 16. In Proceedings of the First Conference on Machine Translation, pages 371376, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016b. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17151725, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Xing Shi, Inkit Padhi, and Kevin Knight. 2016. Does string-based neural mt learn source syntax? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 15261534, Austin, Texas. Association for Computational Linguistics.</p>
<p>David Smith and Jason Eisner. 2006. Quasisynchronous grammars: Alignment by soft projection of syntactic dependencies. In Proceedings on the Workshop on Statistical Machine Translation, pages 23-30, New York City. Association for Computational Linguistics.</p>
<p>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In In Proceedings of Association for Machine Translation in the Americas, pages 223-231.</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of EMNLP.</p>
<p>Felix Stahlberg, Eva Hasler, Aurelien Waite, and Bill Byrne. 2016. Syntactically guided neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 299-305, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Milo Stanojevi and Khalil Simaan. 2015. Evaluating mt systems with beer. The Prague Bulletin of Mathematical Linguistics, 104(1):17-26.</p>
<p>Milo Stanojevi and Khalil Sima'an. 2014. Fitting sentence level translation evaluation with many dense features. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 202-206, Doha, Qatar. Association for Computational Linguistics.</p>
<p>Mihai Surdeanu, Richard Johansson, Adam Meyers, Llus Mrquez, and Joakim Nivre. 2008. The conll 2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of CoNLL.</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to Sequence Learning with Neural Networks. In Neural Information Processing Systems (NIPS), pages 3104-3112.</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. CoRR, abs/1609.08144.</p>
<p>Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, and Wang Ling. 2016. Learning to compose words into sentences with reinforcement learning. CoRR, abs/1611.09100.</p>
<p>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings of the Workshop on Statistical Machine Translation, StatMT '06, pages 138-141, Stroudsburg, PA, USA. Association for Computational Linguistics.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ See Stanojevi and Simaan (2015). TER (Snover et al., 2006) and BEER (Stanojevi and Sima'an, 2014) metrics, even though omitted due to space considerations, are consistent with the reported results.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{7}$ https://github.com/tensorflow/models/tree/master/syntaxnet
${ }^{8}$ The used dependency parses can be reproduced by using the syntaxnet/demo.sh shell script.
${ }^{9}$ https://github.com/moses-smt/mosesdecoder&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7779 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7779</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7779</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-270045127</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.15329v2.pdf" target="_blank">Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework</a></p>
                <p><strong>Paper Abstract:</strong> The acceleration of Large Language Models (LLMs) research has opened up new possibilities for evaluating generated texts. They serve as scalable and economical evaluators, but the question of how reliable these evaluators are has emerged as a crucial research question. Prior research efforts in the meta-evaluation of LLMs as judges limit the prompting of an LLM to a single use to obtain a final evaluation decision. They then compute the agreement between LLMs' outputs and human labels. This lacks interpretability in understanding the evaluation capability of LLMs. In light of this challenge, we propose Decompose and Aggregate, which breaks down the evaluation process into different stages based on pedagogical practices. Our experiments illustrate that it not only provides a more interpretable window for how well LLMs evaluate, but also leads to improvements up to 39.6% for different LLMs on a variety of meta-evaluation benchmarks.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7779.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7779.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Decompose-and-Aggregate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decompose and Aggregate framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage, rubric-inspired evaluation framework where an LLM (1) decomposes evaluation into aspect generation and aspect-wise pairwise scoring, and (2) aggregates via LLM-proposed percentage weightings and an external calculator computing a weighted sum to produce a final preference judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5; GPT-4; Llama2-13B; Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3.5-0613, GPT-4-0613, Llama2-13B, Mistral-7B-Instruct-v2</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing / NLG evaluation (meta-evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation procedure / interpretable judgment pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Decompose and Aggregate</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Decompose: generate evaluation aspects (criteria) per instance or use predefined aspects; perform pairwise scoring of candidate responses along each aspect. Aggregate: prompt LLM to propose percentage weightings per aspect, then use an external calculation module to compute weighted-sum overall scores f(S)=Σ_j w_ij * S_ij and compare overall scores to decide preference.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Agreement with human preference label (percentage agreement); top-k Kendall's τ for weighting alignment; Likert ratings for aspect quality</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Agreement = % of instances where predicted preference (Response 1 > Response 2 or vice versa) matches human preference label; Kendall's τ distance (top-k) measures rank distance between model and human aspect-weight rankings (lower is more similar); Likert 1–5 scale used for relevance, clarity, comprehensiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>InstruSum; LLMBar (adversarial subset); MTBench; FairEval</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Aspect quality: 3 crowdworkers independently rated relevance, clarity, comprehensiveness on Likert 1–5 for 50 randomly sampled instances per dataset. Weighting evaluation: convert model and human weightings to ranks and compute top-k Kendall's τ; worker qualifications: >500 accepted HITs, >98% approval; a qualification round verified by authors; pay ≈ US$15/hr.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Up to 39.6% relative improvement reported (example: GPT-4 on FairEval: direct scoring agreement 42.4% vs Decompose-and-Aggregate 59.1%); detailed per-model/per-dataset agreements reported in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Compared LLM judgments to human preference labels and to baselines (direct scoring and CoT); Decompose-and-Aggregate achieved higher agreement with human labels across most model/dataset combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Higher computational cost (longer inputs, more inferences); experiments fixed to 3 aspects (may not be optimal); agreement with human preference labels may not be a gold-standard; tie cases complicate evaluation; possible dataset-model data leakage for some combos (noted for Llama2-13B and Mistral-7B on FairEval).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7779.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7779.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DirectScoring</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct scoring baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline evaluation method that prompts an LLM once to provide an overall numerical score or preference for each candidate response without structured decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5; GPT-4; Llama2-13B; Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3.5-0613, GPT-4-0613, Llama2-13B, Mistral-7B-Instruct-v2</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / NLG evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation judgment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Direct scoring</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompt the LLM with instance context, candidate responses, and evaluation instruction, and ask for an overall score or preference in a single inference.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Agreement with human preference label (percentage agreement)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Percent of instances where LLM's single-shot overall score/preference matches human preference label; ties handled as separate case.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>InstruSum; LLMBar; MTBench; FairEval</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human preference labels from existing meta-evaluation datasets; tie cases noted and reported both with and without ties for some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Baseline agreement values reported in Table 2 (example: GPT-4 on FairEval direct scoring reported 42.4% agreement in paper text example used to illustrate improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Serves as baseline; Decompose-and-Aggregate generally outperforms direct scoring across datasets and models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Single-shot decisions may align with humans by chance, lack interpretable intermediate outputs, and are susceptible to biases (position, verbosity, anchoring).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7779.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7779.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting approach that elicits step-by-step reasoning or explanations from the LLM before producing a final score or decision; used here as a baseline for interpretable prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5; GPT-4; Llama2-13B; Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3.5-0613, GPT-4-0613, Llama2-13B, Mistral-7B-Instruct-v2</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / prompt engineering</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>explanatory reasoning prompting</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Ask the model to produce explanation(s) (chain-of-thought) about why one response is better, then output an overall score or preference.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Agreement with human preference label (percentage agreement)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Same percent-agreement metric; CoT evaluated by whether explanations plus final score match human preference.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>InstruSum; LLMBar; MTBench; FairEval</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Same human preference labels used as ground truth; CoT explanations not systematically scored except insofar as they affect final agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>CoT did not produce consistent improvements and sometimes reduced agreement relative to direct scoring (Table 2 shows mixed/poorer performance for CoT in several settings).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Compared to human labels, CoT explanations sometimes increased interpretability but did not reliably increase agreement with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Uncontrolled reasoning paths produce variable benefits; previous studies and this paper show CoT does not reliably improve evaluation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7779.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7779.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pairwise-by-Aspect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pairwise scoring by aspect</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach mixing pairwise comparison and per-aspect scoring: for each aspect, the LLM gives scores for each candidate, enabling fine-grained comparison across multiple facets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5; GPT-4; Llama2-13B; Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3.5-0613, GPT-4-0613, Llama2-13B, Mistral-7B-Instruct-v2</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / NLG evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>multi-aspect evaluation scoring</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Pairwise scoring by aspect</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each pre-defined or model-generated aspect A_ij, ask the LLM to give pairwise scores S^(m)_ij for each candidate m; combine aspect-wise comparisons during aggregation to reach a final preference.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Agreement with human preference label (percentage agreement); aspect-wise human ratings (Likert)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Agreement percentage as above; aspect-wise scores are numeric (scale determined by prompt; paper treats them as comparable numeric scores aggregated via weighted sum).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>InstruSum; LLMBar; MTBench; FairEval</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Aspects were also judged by humans for relevance/clarity/comprehensiveness on Likert scale; aspect-wise scores generated by LLMs were compared via aggregation to human preference labels.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used as core decomposition step within Decompose-and-Aggregate and contributed to higher agreement vs baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Aspect-wise judgments enabled better prioritization aligned with human-labeled preferences in many cases (qualitative examples given).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Requires multiple inferences (one per aspect) and careful prompt design to avoid anchoring; fixed number of aspects (3) in experiments may under- or over-constrain evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7779.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7779.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WeightedSum+Calc</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Weighted-sum aggregation with external calculator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregation method where the LLM proposes percentage weightings per aspect and an external computation module deterministically computes weighted sums of aspect scores to yield overall scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5; GPT-4; Llama2-13B; Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3.5-0613, GPT-4-0613, Llama2-13B, Mistral-7B-Instruct-v2</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / evaluation tooling</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>symbolic aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Weighted-sum aggregation (external calculator)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompt LLM to output percentage weightings per aspect (summing to 100%), then use an external calculator to compute f(S^(m)) = Σ_j w_ij * S^(m)_ij and compare overall scores to decide preference; avoids relying on LLM for numeric aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Agreement with human preference label (percentage agreement); ablation comparison vs prompted (LLM) aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Percent agreement as above; improvement measured as delta in agreement when using external calculator vs having LLM aggregate inside its response.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>InstruSum; LLMBar; MTBench; FairEval</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Weightings also evaluated by human annotators via ranking/Kendall's τ to measure similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Ablation shows external calculator yields higher agreement across nearly all LLMs and benchmarks; replacing it with LLM-prompted aggregation often decreases performance (Table 2 ablation rows, and stated ablation findings).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>External-calculator aggregation produced weighted overall scores that better matched human preference labels than LLM-prompted aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Relies on LLM producing sensible weightings; if weightings are poor, aggregation will not fix decomposition errors; adds extra tooling complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7779.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7779.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KendallTau-topk</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Top-k Kendall's τ ranking distance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A rank-distance metric used to compare model-proposed aspect weight rankings to human annotator rankings, where lower distance indicates higher similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Not specific to an LLM (applied to outputs of GPT-3.5, GPT-4, Llama2-13B, Mistral-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistics / evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>rank correlation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Top-k Kendall's τ distance</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Convert numeric weightings to rank lists and compute Kendall's τ distance (top-k variant) between model and human ranked lists of aspect importance; compare also human-human distances as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Kendall's τ distance (top-k)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Rank-based distance metric (Fagin et al., 2003); lower values indicate closer agreement between rankings; computed for top-k elements where relevant.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Human weighting evaluations on InstruSum; LLMBar; MTBench; FairEval (50 sampled instances per dataset for aspect evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human weightings converted to ranks; comparison used between each LLM and human; also reported inter-human Kendall's τ for reference.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>LLM-to-human Kendall's τ distances were generally higher than human-human distances on several datasets (e.g., MTBench, LLMBar), with ChatGPT showing relatively closer alignment on some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Used to quantify divergence between model-proposed weightings and human weightings; ChatGPT often most aligned, Mistral least aligned.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Converts continuous weightings to ranks (lossy); depends on top-k selection and the chosen k (paper used three aspects).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7779.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7779.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AgreementMetric</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agreement with human preference labels</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Primary quantitative evaluation metric used across experiments: percent agreement between LLM predicted preference and human preference labels (reported both with and without tie instances where applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5; GPT-4; Llama2-13B; Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3.5-0613, GPT-4-0613, Llama2-13B, Mistral-7B-Instruct-v2</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Agreement with human preference</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute the percentage of instances for which the LLM's predicted better response matches the human-annotated preferred response. Where datasets include ties, report agreement both including and excluding tie cases.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Percentage agreement</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Number of matching preference labels divided by total evaluated instances, expressed as a percentage; optionally exclude ties to report 'without ties' agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>InstruSum; LLMBar; MTBench; FairEval</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human labels from original datasets used as ground truth; some datasets annotated on multiple dimensions (e.g., FairEval aspects) but meta-evaluation uses the provided human preference label.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Per-model and per-dataset agreement values reported in Table 2; example improvements highlighted (e.g., up to +39.6% relative improvement for GPT-4 on FairEval when using Decompose-and-Aggregate vs direct scoring).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Baseline direct scoring and CoT compared to Decompose-and-Aggregate using this metric; D&A generally higher.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Human preference is an imperfect gold standard (ties, subjectivity); high agreement does not guarantee correct intermediate reasoning; chance alignment possible with binary preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7779.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7779.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaEvalDatasets</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta-evaluation benchmarks (LLMBar, InstruSum, MTBench, FairEval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Four recent meta-evaluation datasets used to measure LLMs' ability to act as evaluators; they vary in domain coverage (writing, math, knowledge, coding, summarization) and annotation protocols (presence/absence of predefined criteria, tie cases).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Evaluated with GPT-3.5; GPT-4; Llama2-13B; Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3.5-0613, GPT-4-0613, Llama2-13B, Mistral-7B-Instruct-v2</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / benchmark evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>benchmark datasets for meta-evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Meta-evaluation benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Datasets annotated with human preference labels for pairwise comparison of candidate model outputs; some include user-defined criteria/ratings across multiple dimensions, others do not (requiring dynamic aspect generation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Agreement with human preference label (percentage agreement); dataset-specific features (ties present or not) influence reporting</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>See Agreement metric definition; dataset-specific sample sizes used (e.g., LLMBar adversarial subset 319 instances used; MTBench stratified sample of 400 single-turn samples).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>LLMBar (adversarial subset 319 instances); InstruSum (100 articles, paired summaries from GPT-3.5-turbo-0301 and GPT-4-0314 used); MTBench (stratified random sample 400 instances); FairEval (various labels and aspects)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human preference labels from original dataset collection; paper also ran a separate human study for aspect and weighting evaluation on 50 sampled instances per dataset with 3 workers per instance.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Per-dataset agreement tables and comparisons to baselines (Table 2); dataset characteristics noted (e.g., LLMBar adversarial set has no ties).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Used as ground truth for measuring LLM evaluator agreement; Decompose-and-Aggregate improved agreement on most datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Some datasets are small (statistical significance limited); potential data leakage concerns for some model/dataset combinations (noted for Llama2-13B and Mistral-7B on FairEval).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7779.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7779.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReferenceMetricsMentioned</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reference-based and reference-free metrics (BLEU, ROUGE, BERTScore, BARTScore, iBLEU, ParaScore, GPTScore)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of existing automatic evaluation metrics referenced in related work: BLEU/ROUGE (lexical overlap), BERTScore/BARTScore (semantic similarity), and reference-free metrics like iBLEU, ParaScore, GPTScore.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / automatic evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BLEU, ROUGE, BERTScore, BARTScore, iBLEU, ParaScore, GPTScore (mentioned collectively)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>BLEU/ROUGE measure lexical overlap with references; BERTScore/BARTScore measure semantic similarity using contextual embeddings or model scoring; iBLEU/ParaScore are reference-free or paraphrase-aware metrics; GPTScore leverages LLMs as zero-shot evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Varied (BLEU/ROUGE scores, BERTScore/BARTScore similarity scores, GPTScore model-based scores)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>BLEU/ROUGE: n-gram overlap precision/recall-based scores; BERTScore: embedding similarity; GPTScore: model-derived score (implementation-specific).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Mentioned as background: lexical-overlap metrics struggle with open-ended, subjective long-form generation; motivates reference-free and LLM-based evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluating large language models at evaluating instruction following. <em>(Rating: 2)</em></li>
                <li>Gptscore: Evaluate as you desire. <em>(Rating: 2)</em></li>
                <li>Coascore: Chainof-aspects prompting for nlg evaluation. <em>(Rating: 2)</em></li>
                <li>Branchsolve-merge improves large language model evaluation and generation. <em>(Rating: 2)</em></li>
                <li>Chateval: Towards better llm-based evaluators through multi-agent debate. <em>(Rating: 2)</em></li>
                <li>Can large language models be trusted for evaluation? scalable meta-evaluation of llms as evaluators via agent debate. <em>(Rating: 2)</em></li>
                <li>Is chatgpt a good nlg evaluator? a preliminary study. <em>(Rating: 1)</em></li>
                <li>Prometheus: Inducing fine-grained evaluation capability in language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7779",
    "paper_id": "paper-270045127",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "Decompose-and-Aggregate",
            "name_full": "Decompose and Aggregate framework",
            "brief_description": "A two-stage, rubric-inspired evaluation framework where an LLM (1) decomposes evaluation into aspect generation and aspect-wise pairwise scoring, and (2) aggregates via LLM-proposed percentage weightings and an external calculator computing a weighted sum to produce a final preference judgment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5; GPT-4; Llama2-13B; Mistral-7B",
            "model_size": "GPT-3.5-0613, GPT-4-0613, Llama2-13B, Mistral-7B-Instruct-v2",
            "scientific_domain": "Natural Language Processing / NLG evaluation (meta-evaluation)",
            "theory_type": "evaluation procedure / interpretable judgment pipeline",
            "evaluation_method_name": "Decompose and Aggregate",
            "evaluation_method_description": "Decompose: generate evaluation aspects (criteria) per instance or use predefined aspects; perform pairwise scoring of candidate responses along each aspect. Aggregate: prompt LLM to propose percentage weightings per aspect, then use an external calculation module to compute weighted-sum overall scores f(S)=Σ_j w_ij * S_ij and compare overall scores to decide preference.",
            "evaluation_metric": "Agreement with human preference label (percentage agreement); top-k Kendall's τ for weighting alignment; Likert ratings for aspect quality",
            "metric_definition": "Agreement = % of instances where predicted preference (Response 1 &gt; Response 2 or vice versa) matches human preference label; Kendall's τ distance (top-k) measures rank distance between model and human aspect-weight rankings (lower is more similar); Likert 1–5 scale used for relevance, clarity, comprehensiveness.",
            "dataset_or_benchmark": "InstruSum; LLMBar (adversarial subset); MTBench; FairEval",
            "human_evaluation_details": "Aspect quality: 3 crowdworkers independently rated relevance, clarity, comprehensiveness on Likert 1–5 for 50 randomly sampled instances per dataset. Weighting evaluation: convert model and human weightings to ranks and compute top-k Kendall's τ; worker qualifications: &gt;500 accepted HITs, &gt;98% approval; a qualification round verified by authors; pay ≈ US$15/hr.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Up to 39.6% relative improvement reported (example: GPT-4 on FairEval: direct scoring agreement 42.4% vs Decompose-and-Aggregate 59.1%); detailed per-model/per-dataset agreements reported in Table 2.",
            "comparison_to_human_generated": true,
            "comparison_results": "Compared LLM judgments to human preference labels and to baselines (direct scoring and CoT); Decompose-and-Aggregate achieved higher agreement with human labels across most model/dataset combinations.",
            "limitations_noted": "Higher computational cost (longer inputs, more inferences); experiments fixed to 3 aspects (may not be optimal); agreement with human preference labels may not be a gold-standard; tie cases complicate evaluation; possible dataset-model data leakage for some combos (noted for Llama2-13B and Mistral-7B on FairEval).",
            "uuid": "e7779.0",
            "source_info": {
                "paper_title": "Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "DirectScoring",
            "name_full": "Direct scoring baseline",
            "brief_description": "Baseline evaluation method that prompts an LLM once to provide an overall numerical score or preference for each candidate response without structured decomposition.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5; GPT-4; Llama2-13B; Mistral-7B",
            "model_size": "GPT-3.5-0613, GPT-4-0613, Llama2-13B, Mistral-7B-Instruct-v2",
            "scientific_domain": "NLP / NLG evaluation",
            "theory_type": "evaluation judgment",
            "evaluation_method_name": "Direct scoring",
            "evaluation_method_description": "Prompt the LLM with instance context, candidate responses, and evaluation instruction, and ask for an overall score or preference in a single inference.",
            "evaluation_metric": "Agreement with human preference label (percentage agreement)",
            "metric_definition": "Percent of instances where LLM's single-shot overall score/preference matches human preference label; ties handled as separate case.",
            "dataset_or_benchmark": "InstruSum; LLMBar; MTBench; FairEval",
            "human_evaluation_details": "Human preference labels from existing meta-evaluation datasets; tie cases noted and reported both with and without ties for some datasets.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Baseline agreement values reported in Table 2 (example: GPT-4 on FairEval direct scoring reported 42.4% agreement in paper text example used to illustrate improvement).",
            "comparison_to_human_generated": true,
            "comparison_results": "Serves as baseline; Decompose-and-Aggregate generally outperforms direct scoring across datasets and models.",
            "limitations_noted": "Single-shot decisions may align with humans by chance, lack interpretable intermediate outputs, and are susceptible to biases (position, verbosity, anchoring).",
            "uuid": "e7779.1",
            "source_info": {
                "paper_title": "Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought (CoT) prompting",
            "brief_description": "A prompting approach that elicits step-by-step reasoning or explanations from the LLM before producing a final score or decision; used here as a baseline for interpretable prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5; GPT-4; Llama2-13B; Mistral-7B",
            "model_size": "GPT-3.5-0613, GPT-4-0613, Llama2-13B, Mistral-7B-Instruct-v2",
            "scientific_domain": "NLP / prompt engineering",
            "theory_type": "explanatory reasoning prompting",
            "evaluation_method_name": "Chain-of-Thought (CoT) prompting",
            "evaluation_method_description": "Ask the model to produce explanation(s) (chain-of-thought) about why one response is better, then output an overall score or preference.",
            "evaluation_metric": "Agreement with human preference label (percentage agreement)",
            "metric_definition": "Same percent-agreement metric; CoT evaluated by whether explanations plus final score match human preference.",
            "dataset_or_benchmark": "InstruSum; LLMBar; MTBench; FairEval",
            "human_evaluation_details": "Same human preference labels used as ground truth; CoT explanations not systematically scored except insofar as they affect final agreement.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "CoT did not produce consistent improvements and sometimes reduced agreement relative to direct scoring (Table 2 shows mixed/poorer performance for CoT in several settings).",
            "comparison_to_human_generated": true,
            "comparison_results": "Compared to human labels, CoT explanations sometimes increased interpretability but did not reliably increase agreement with human judgments.",
            "limitations_noted": "Uncontrolled reasoning paths produce variable benefits; previous studies and this paper show CoT does not reliably improve evaluation performance.",
            "uuid": "e7779.2",
            "source_info": {
                "paper_title": "Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Pairwise-by-Aspect",
            "name_full": "Pairwise scoring by aspect",
            "brief_description": "An approach mixing pairwise comparison and per-aspect scoring: for each aspect, the LLM gives scores for each candidate, enabling fine-grained comparison across multiple facets.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5; GPT-4; Llama2-13B; Mistral-7B",
            "model_size": "GPT-3.5-0613, GPT-4-0613, Llama2-13B, Mistral-7B-Instruct-v2",
            "scientific_domain": "NLP / NLG evaluation",
            "theory_type": "multi-aspect evaluation scoring",
            "evaluation_method_name": "Pairwise scoring by aspect",
            "evaluation_method_description": "For each pre-defined or model-generated aspect A_ij, ask the LLM to give pairwise scores S^(m)_ij for each candidate m; combine aspect-wise comparisons during aggregation to reach a final preference.",
            "evaluation_metric": "Agreement with human preference label (percentage agreement); aspect-wise human ratings (Likert)",
            "metric_definition": "Agreement percentage as above; aspect-wise scores are numeric (scale determined by prompt; paper treats them as comparable numeric scores aggregated via weighted sum).",
            "dataset_or_benchmark": "InstruSum; LLMBar; MTBench; FairEval",
            "human_evaluation_details": "Aspects were also judged by humans for relevance/clarity/comprehensiveness on Likert scale; aspect-wise scores generated by LLMs were compared via aggregation to human preference labels.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Used as core decomposition step within Decompose-and-Aggregate and contributed to higher agreement vs baselines.",
            "comparison_to_human_generated": true,
            "comparison_results": "Aspect-wise judgments enabled better prioritization aligned with human-labeled preferences in many cases (qualitative examples given).",
            "limitations_noted": "Requires multiple inferences (one per aspect) and careful prompt design to avoid anchoring; fixed number of aspects (3) in experiments may under- or over-constrain evaluation.",
            "uuid": "e7779.3",
            "source_info": {
                "paper_title": "Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "WeightedSum+Calc",
            "name_full": "Weighted-sum aggregation with external calculator",
            "brief_description": "Aggregation method where the LLM proposes percentage weightings per aspect and an external computation module deterministically computes weighted sums of aspect scores to yield overall scores.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5; GPT-4; Llama2-13B; Mistral-7B",
            "model_size": "GPT-3.5-0613, GPT-4-0613, Llama2-13B, Mistral-7B-Instruct-v2",
            "scientific_domain": "NLP / evaluation tooling",
            "theory_type": "symbolic aggregation",
            "evaluation_method_name": "Weighted-sum aggregation (external calculator)",
            "evaluation_method_description": "Prompt LLM to output percentage weightings per aspect (summing to 100%), then use an external calculator to compute f(S^(m)) = Σ_j w_ij * S^(m)_ij and compare overall scores to decide preference; avoids relying on LLM for numeric aggregation.",
            "evaluation_metric": "Agreement with human preference label (percentage agreement); ablation comparison vs prompted (LLM) aggregation",
            "metric_definition": "Percent agreement as above; improvement measured as delta in agreement when using external calculator vs having LLM aggregate inside its response.",
            "dataset_or_benchmark": "InstruSum; LLMBar; MTBench; FairEval",
            "human_evaluation_details": "Weightings also evaluated by human annotators via ranking/Kendall's τ to measure similarity.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Ablation shows external calculator yields higher agreement across nearly all LLMs and benchmarks; replacing it with LLM-prompted aggregation often decreases performance (Table 2 ablation rows, and stated ablation findings).",
            "comparison_to_human_generated": true,
            "comparison_results": "External-calculator aggregation produced weighted overall scores that better matched human preference labels than LLM-prompted aggregation.",
            "limitations_noted": "Relies on LLM producing sensible weightings; if weightings are poor, aggregation will not fix decomposition errors; adds extra tooling complexity.",
            "uuid": "e7779.4",
            "source_info": {
                "paper_title": "Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "KendallTau-topk",
            "name_full": "Top-k Kendall's τ ranking distance",
            "brief_description": "A rank-distance metric used to compare model-proposed aspect weight rankings to human annotator rankings, where lower distance indicates higher similarity.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Not specific to an LLM (applied to outputs of GPT-3.5, GPT-4, Llama2-13B, Mistral-7B)",
            "model_size": "N/A",
            "scientific_domain": "Statistics / evaluation metrics",
            "theory_type": "rank correlation metric",
            "evaluation_method_name": "Top-k Kendall's τ distance",
            "evaluation_method_description": "Convert numeric weightings to rank lists and compute Kendall's τ distance (top-k variant) between model and human ranked lists of aspect importance; compare also human-human distances as baseline.",
            "evaluation_metric": "Kendall's τ distance (top-k)",
            "metric_definition": "Rank-based distance metric (Fagin et al., 2003); lower values indicate closer agreement between rankings; computed for top-k elements where relevant.",
            "dataset_or_benchmark": "Human weighting evaluations on InstruSum; LLMBar; MTBench; FairEval (50 sampled instances per dataset for aspect evaluation)",
            "human_evaluation_details": "Human weightings converted to ranks; comparison used between each LLM and human; also reported inter-human Kendall's τ for reference.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "LLM-to-human Kendall's τ distances were generally higher than human-human distances on several datasets (e.g., MTBench, LLMBar), with ChatGPT showing relatively closer alignment on some datasets.",
            "comparison_to_human_generated": true,
            "comparison_results": "Used to quantify divergence between model-proposed weightings and human weightings; ChatGPT often most aligned, Mistral least aligned.",
            "limitations_noted": "Converts continuous weightings to ranks (lossy); depends on top-k selection and the chosen k (paper used three aspects).",
            "uuid": "e7779.5",
            "source_info": {
                "paper_title": "Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "AgreementMetric",
            "name_full": "Agreement with human preference labels",
            "brief_description": "Primary quantitative evaluation metric used across experiments: percent agreement between LLM predicted preference and human preference labels (reported both with and without tie instances where applicable).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5; GPT-4; Llama2-13B; Mistral-7B",
            "model_size": "GPT-3.5-0613, GPT-4-0613, Llama2-13B, Mistral-7B-Instruct-v2",
            "scientific_domain": "NLP evaluation",
            "theory_type": "evaluation metric",
            "evaluation_method_name": "Agreement with human preference",
            "evaluation_method_description": "Compute the percentage of instances for which the LLM's predicted better response matches the human-annotated preferred response. Where datasets include ties, report agreement both including and excluding tie cases.",
            "evaluation_metric": "Percentage agreement",
            "metric_definition": "Number of matching preference labels divided by total evaluated instances, expressed as a percentage; optionally exclude ties to report 'without ties' agreement.",
            "dataset_or_benchmark": "InstruSum; LLMBar; MTBench; FairEval",
            "human_evaluation_details": "Human labels from original datasets used as ground truth; some datasets annotated on multiple dimensions (e.g., FairEval aspects) but meta-evaluation uses the provided human preference label.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Per-model and per-dataset agreement values reported in Table 2; example improvements highlighted (e.g., up to +39.6% relative improvement for GPT-4 on FairEval when using Decompose-and-Aggregate vs direct scoring).",
            "comparison_to_human_generated": true,
            "comparison_results": "Baseline direct scoring and CoT compared to Decompose-and-Aggregate using this metric; D&A generally higher.",
            "limitations_noted": "Human preference is an imperfect gold standard (ties, subjectivity); high agreement does not guarantee correct intermediate reasoning; chance alignment possible with binary preferences.",
            "uuid": "e7779.6",
            "source_info": {
                "paper_title": "Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "MetaEvalDatasets",
            "name_full": "Meta-evaluation benchmarks (LLMBar, InstruSum, MTBench, FairEval)",
            "brief_description": "Four recent meta-evaluation datasets used to measure LLMs' ability to act as evaluators; they vary in domain coverage (writing, math, knowledge, coding, summarization) and annotation protocols (presence/absence of predefined criteria, tie cases).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Evaluated with GPT-3.5; GPT-4; Llama2-13B; Mistral-7B",
            "model_size": "GPT-3.5-0613, GPT-4-0613, Llama2-13B, Mistral-7B-Instruct-v2",
            "scientific_domain": "NLP / benchmark evaluation",
            "theory_type": "benchmark datasets for meta-evaluation",
            "evaluation_method_name": "Meta-evaluation benchmarks",
            "evaluation_method_description": "Datasets annotated with human preference labels for pairwise comparison of candidate model outputs; some include user-defined criteria/ratings across multiple dimensions, others do not (requiring dynamic aspect generation).",
            "evaluation_metric": "Agreement with human preference label (percentage agreement); dataset-specific features (ties present or not) influence reporting",
            "metric_definition": "See Agreement metric definition; dataset-specific sample sizes used (e.g., LLMBar adversarial subset 319 instances used; MTBench stratified sample of 400 single-turn samples).",
            "dataset_or_benchmark": "LLMBar (adversarial subset 319 instances); InstruSum (100 articles, paired summaries from GPT-3.5-turbo-0301 and GPT-4-0314 used); MTBench (stratified random sample 400 instances); FairEval (various labels and aspects)",
            "human_evaluation_details": "Human preference labels from original dataset collection; paper also ran a separate human study for aspect and weighting evaluation on 50 sampled instances per dataset with 3 workers per instance.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Per-dataset agreement tables and comparisons to baselines (Table 2); dataset characteristics noted (e.g., LLMBar adversarial set has no ties).",
            "comparison_to_human_generated": true,
            "comparison_results": "Used as ground truth for measuring LLM evaluator agreement; Decompose-and-Aggregate improved agreement on most datasets.",
            "limitations_noted": "Some datasets are small (statistical significance limited); potential data leakage concerns for some model/dataset combinations (noted for Llama2-13B and Mistral-7B on FairEval).",
            "uuid": "e7779.7",
            "source_info": {
                "paper_title": "Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ReferenceMetricsMentioned",
            "name_full": "Reference-based and reference-free metrics (BLEU, ROUGE, BERTScore, BARTScore, iBLEU, ParaScore, GPTScore)",
            "brief_description": "A set of existing automatic evaluation metrics referenced in related work: BLEU/ROUGE (lexical overlap), BERTScore/BARTScore (semantic similarity), and reference-free metrics like iBLEU, ParaScore, GPTScore.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_size": "",
            "scientific_domain": "NLP / automatic evaluation metrics",
            "theory_type": "evaluation metrics",
            "evaluation_method_name": "BLEU, ROUGE, BERTScore, BARTScore, iBLEU, ParaScore, GPTScore (mentioned collectively)",
            "evaluation_method_description": "BLEU/ROUGE measure lexical overlap with references; BERTScore/BARTScore measure semantic similarity using contextual embeddings or model scoring; iBLEU/ParaScore are reference-free or paraphrase-aware metrics; GPTScore leverages LLMs as zero-shot evaluators.",
            "evaluation_metric": "Varied (BLEU/ROUGE scores, BERTScore/BARTScore similarity scores, GPTScore model-based scores)",
            "metric_definition": "BLEU/ROUGE: n-gram overlap precision/recall-based scores; BERTScore: embedding similarity; GPTScore: model-derived score (implementation-specific).",
            "dataset_or_benchmark": "",
            "human_evaluation_details": null,
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": "",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Mentioned as background: lexical-overlap metrics struggle with open-ended, subjective long-form generation; motivates reference-free and LLM-based evaluation.",
            "uuid": "e7779.8",
            "source_info": {
                "paper_title": "Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluating large language models at evaluating instruction following.",
            "rating": 2,
            "sanitized_title": "evaluating_large_language_models_at_evaluating_instruction_following"
        },
        {
            "paper_title": "Gptscore: Evaluate as you desire.",
            "rating": 2,
            "sanitized_title": "gptscore_evaluate_as_you_desire"
        },
        {
            "paper_title": "Coascore: Chainof-aspects prompting for nlg evaluation.",
            "rating": 2,
            "sanitized_title": "coascore_chainofaspects_prompting_for_nlg_evaluation"
        },
        {
            "paper_title": "Branchsolve-merge improves large language model evaluation and generation.",
            "rating": 2,
            "sanitized_title": "branchsolvemerge_improves_large_language_model_evaluation_and_generation"
        },
        {
            "paper_title": "Chateval: Towards better llm-based evaluators through multi-agent debate.",
            "rating": 2,
            "sanitized_title": "chateval_towards_better_llmbased_evaluators_through_multiagent_debate"
        },
        {
            "paper_title": "Can large language models be trusted for evaluation? scalable meta-evaluation of llms as evaluators via agent debate.",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_trusted_for_evaluation_scalable_metaevaluation_of_llms_as_evaluators_via_agent_debate"
        },
        {
            "paper_title": "Is chatgpt a good nlg evaluator? a preliminary study.",
            "rating": 1,
            "sanitized_title": "is_chatgpt_a_good_nlg_evaluator_a_preliminary_study"
        },
        {
            "paper_title": "Prometheus: Inducing fine-grained evaluation capability in language models.",
            "rating": 1,
            "sanitized_title": "prometheus_inducing_finegrained_evaluation_capability_in_language_models"
        }
    ],
    "cost": 0.01816725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework
14 Jun 2024</p>
<p>Minzhi Li li.minzhi@u.nus.edu 
National University of Singapore ‡ Salesforce Research § Institute for Infocomm Research</p>
<p>Zhengyuan Liu liu_zhengyuan@i2r.a-star.edu.sg 
National University of Singapore ‡ Salesforce Research § Institute for Infocomm Research</p>
<p>Shumin Deng shumin@nus.edu.sg 
National University of Singapore ‡ Salesforce Research § Institute for Infocomm Research</p>
<p>Shafiq Joty sjoty@salesforce.com 
National University of Singapore ‡ Salesforce Research § Institute for Infocomm Research</p>
<p>Nancy F Chen nfychen@i2r.a-star.edu.sg 
National University of Singapore ‡ Salesforce Research § Institute for Infocomm Research</p>
<p>Min-Yen Kan 
National University of Singapore ‡ Salesforce Research § Institute for Infocomm Research</p>
<p>Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework
14 Jun 202479888EAF56F29275DCBB0A2DEC07D0BBarXiv:2405.15329v2[cs.CL]
The acceleration of Large Language Models (LLMs) research has opened up new possibilities for evaluating generated texts.They serve as scalable and economical evaluators, but the question of how reliable these evaluators are has emerged as a crucial research question.Prior research efforts in the meta-evaluation of LLMs as judges limit the prompting of an LLM to a single use to obtain a final evaluation decision.They then compute the agreement between LLMs' outputs and human labels.This lacks interpretability in understanding the evaluation capability of LLMs.In light of this challenge, we propose Decompose and Aggregate, which breaks down the evaluation process into different stages based on pedagogical practices.Our experiments illustrate that it not only provides a more interpretable window for how well LLMs evaluate, but also leads to improvements up to 39.6% for different LLMs on a variety of meta-evaluation benchmarks.</p>
<p>Introduction</p>
<p>The advancement in Large Language Model (LLM) research has made remarkable progress where LLMs nowadays are able to effectively handle a diverse range of tasks with impressive performance (Bang et al., 2023;Bubeck et al., 2023).The capability of LLMs as a general purpose Natural Language Processing (NLP) task solver (Qin et al., 2023;Laskar et al., 2023) has opened up opportunities for its potential application in evaluating open-ended text generation tasks (Zeng et al., 2023).On the other hand, the traditional use of human subjects for Natural Language Generation (NLG) evaluation is costly, lacks scalability and reproducibility (Karpinska et al., 2021).Given LLMs' impressive general capability in NLP tasks and limitations of human evaluation, using LLMas-a-judge has emerged as an alternative addressing all three issues (cost, scalability and consistency).With the use of LLMs as evaluators, a critical question emerges regarding the extent to which different LLMs can be trusted for reliable evaluation.</p>
<p>To address this question, some recent works focus on the development of meta-evaluation benchmarks (Wang et al., 2023b;Zeng et al., 2023;Zheng et al., 2024).In these tasks, the basic setting of meta-evaluation involves prompting LLMs one time to ask for a preference among the responses and calculating the agreement with human.However, this method may not fully reflect LLMs' capability in terms of evaluation: the final output label can be aligned with human preference by chance with potentially incorrect reasoning.Although interpretable methods such as Chain-of-Thought (Wei et al., 2023) (CoT) prompting have been adopted in some work to elicit models' explanations, these techniques do not allow a systematic meta-evaluation, due to the uncontrolled reasoning paths adopted for each instance.Moreover, previous work (Zeng et al., 2023) has empirically shown that the CoT method does not bring about consistent performance improvement with step-by-step reasoning, despite offering greater interpretability.</p>
<p>Towards the goals of effectiveness and interpretability, we propose the Decompose and Aggregate framework, which is inspired by the use of evaluation rubrics used in pedagogy (Jonsson and Svingby, 2007;Dickinson and Adams, 2017) (Figure 1).As its name suggests, the framework consists of two main stages of decomposition and aggregation.In the decomposition stage, an LLM either takes the criteria given in instruction as aspects or proposes different aspects when such information is not provided.For each and every aspect, the LLM performs pairwise scoring for different generations.In the aggregation stage, the LLM will be dynamically prompted to propose weightings for different aspects based on their importance in the given instance's context.An external calculation module will be executed to compute the weighted sum of scores for different aspects as the overall score and compare the overall scores for two generations to produce a final evaluation judgment.</p>
<p>With our Decompose and Aggregate framework, we make the following contributions:</p>
<p>• We empirically show that our framework leads to consistent performance improvement across different datasets for both proprietary and open-sourced LLMs compared to direct scoring and CoT prompting methods.We also conduct an ablation study, illustrating the positive effect of augmenting the evaluation process with external calculation module during the aggregation stage.</p>
<p>• We carry out meta-evaluation for LLMs' intermediate outputs for different stages via human study to better understand LLMs' strengths and limitations in a more fine-grained manner when using them as evaluators.As such, we induce greater interpretability to the LLMs' black-box evaluation process.This results in a better understanding of different LLMs' reliability in evaluating texts.</p>
<p>• Our framework which is grounded on pedagogical practices, introduces a systematic, modularized reasoning procedure for using LLMs for evaluation.With modularization of stages involved in an evaluation process, our results offer insights on the specific areas LLMs are good at or need improvement, shedding light on design of multi-agent collaboration among LLMs and human-LLM collaboration for evaluating natural language.</p>
<p>Related Work</p>
<p>Automatic Text Evaluation.The high cost of human evaluation for machine-generated texts has motivated research in developing automatic text evaluation methods.For Natural Language Generation tasks, metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) scores were used as the dominant approach to evaluate machinegenerated text using lexicon overlap based on a candidate reference.Recently, methods like BERTScore (Zhang et al., 2019) and BARTScore (Yuan et al., 2021) are able to account for meaningpreserving lexical and compositional diversity and capture semantic information, compared to previous methods which only rely on lexical components.These reference-based methods have limitations in capturing the diversity and richness of human language, especially for subjective openended long-form questions (Krishna et al., 2021).</p>
<p>As such, researchers propose reference-free evaluation methods like iBLEU (Sun and Zhou, 2012) and ParaScore (Shen et al., 2022).GPTScore (Fu et al., 2023) also leverages the increasing pretrained knowledge and high zero-shot capability of language models.There is ongoing research exploring LLMs as evaluators under reference-free contexts.</p>
<p>LLM-based Text Evaluation.With the emergence of many powerful LLMs like ChatGPT and GPT-4, increasing work has explored their performance in evaluating generated texts for translation, story generation, paraphrase generation and so on (Chiang and Lee, 2023;Kocmi and Federmann, 2023;Chen et al., 2023;Wang et al., 2023a;Zheng et al., 2024).These empirical explorations demonstrate the stable performance of LLMs in evaluating a wide range of NLG tasks with different task requirements.</p>
<p>However, LLMs have limitations and biases during text evaluation, which include position bias where they tend to prefer some positions over others (Wang et al., 2023b), verbosity bias where they favor longer responses (Zheng et al., 2024), self-enhancement bias where they favor or disfavor selfgenerated answers (Zheng et al., 2024), and style bias where they value style of texts generated more than content during evaluation (Wu and Aji, 2023).</p>
<p>In light of these limitations, researchers are exploring ways to improve LLMs' evaluation capability.Previous work like G-Eval prompts LLMs to generate chain of thoughts for evaluation steps and take the weighted sum over probabilities for different scores (Liu et al., 2023a).Kim et al. (2023) proposes Prometheus, which is an evaluation-specific open-source model with fine-tuning on the feedback to effectively induce fine-grained evaluation capability.More recent approaches like Chain-of-Aspects (Gong and Mao, 2023) and Branch-Solve-Merge (Saha et al., 2023) offer new paradigms for LLMs to decompose multi-faceted language evaluation tasks.In addition, the ChatEval framework (Chan et al., 2023) is proposed to increase LLMs' evaluation capability through multi-agent debate.</p>
<p>On top of these methods, our work proposes a generalizable evaluation framework for LLMs with careful design for both decomposition and aggregation stages.Under this framework, LLMs' evaluation performance is improved while providing higher interpretability at the same time.Compared to previous methods, better evaluation performance and greater interpretability are achieved without loading multiple models, collecting more data, or conducting further finetuning.</p>
<p>Meta-Evaluation of LLMs as Evaluators.As a newly emerging research area, there are only a few benchmarks for meta-evaluation of LLMs as evaluators.Therefore, how reliable LLMs are as evaluators still remains an important research question worth investigating.</p>
<p>To build meta-evaluation benchmarks, recent work leverages on previous meta-evaluation datasets (Fu et al., 2023), carries out smallscale expert annotation for specific tasks (Wang et al., 2023b) and crowd-sources human annotation (Zheng et al., 2024).Meta-evaluation methods include computing correlations with human ratings (Gong and Mao, 2023), calculation of agreement with human labels (Wang et al., 2023b;Zheng et al., 2024;Zeng et al., 2023), and performing metaevaluation using agent debate (Chern et al., 2024).However, few works focus on the interpretability of the meta-evaluation process: high agreement or correlation of the final judgment with human labels does not necessarily mean a strong evalua-tion capability, as the intermediate reasoning process may be wrong.This is especially true where there are only two possible answers for preference agreement computation.The LLM may make the aligned preference with human by chance with incorrect reasoning.To best of our knowledge, we are the first to explore ways to enhance LLMs' performance while providing more interpretability.Although there exists some previous work adopting CoT prompting in their experiments to provide more interpretability to the black-box evaluation process, these have been shown to be ineffective to improving the general performance of LLMs' evaluation capability (Zeng et al., 2023).Our work is able to achieve performance improvement while enhancing interpretability.</p>
<p>Decompose and Aggregate Framework</p>
<p>The benefits of scoring rubrics in evaluation processes have been noted in previous research, which are facilitated learning, increased consistency and more valid evaluation of complex competencies (Jonsson and Svingby, 2007).Inspired by its extensive applications in pedagogy, we establish the Decompose and Aggregate framework for using LLMs as evaluators.The framework bases on core elements in scoring rubrics, which are the decomposition and aggregation stages (Figure 2).</p>
<p>Aspect Generation</p>
<p>Appropriate criteria is the key to effective evaluation rubrics (Brookhart, 2018).They serve as clear guidelines for aspects to be evaluated and provide greater transparency in how a final evaluation judgment is derived.The criteria aspects are determined by specific requirements of different instances for different tasks.The set of evaluation aspects for the i-th instance can be formulated as:
A i = {A i1 , A i2 , . . . , A ik }, for i = 1, 2, . . . , n
(1) where A ij denotes the j-th evaluation aspect for the i-th instance, k is the total number of aspects, and n is the total number of instances.</p>
<p>In previous evaluation tasks, there are two possible scenarios where the evaluation aspects can be predefined or unspecified.In the first scenario, there exists an explicitly-defined set of criteria for the evaluation task; i.e., each and every instance in the given dataset will be evaluated using the same aspect set.In the second setting, there are no clearly defined aspects provided.Under such cases, the context to propose k different evaluation aspects.These aspects are combined with the context and candidate responses for LLMs to generate pairwise scores for each aspect.LLMs will also be prompted to provide respective weightings for each aspect with the given context.In the aggregation stage, external computing tool can be used to calculate the overall scores for each response and make comparison to decide on the better response.</p>
<p>we propose dynamic aspect generation, whereby an LLM is prompted to generate the values of A i given the problem context of the i-th instance and a predetermined number of aspects, denoted by k.</p>
<p>Pairwise Scoring by Aspect</p>
<p>There are two general frameworks for using LLMas-a-judge in existing work.The first one is pairwise comparison where LLMs are prompted to determine if the first or the second response is better given a query (Zheng et al., 2024).The second framework is evaluation by scoring where LLMs are tasked to provide numerical scores for different responses.The final decision about the better response is made by comparing the scores generated by LLMs (Wang et al., 2023b).Taking the respective pros and cons of these two frameworks into consideration, we adopt the approach of pairwise scoring in our framework.This combines the strengths of both methods -namely, the ability to capture subtle differences in pairwise comparison framework (Liu et al., 2024), and the higher scalability in evaluating multiple candidates and higher interpretability in the single answer scoring framework (Zheng et al., 2024).We formulate the pairwise scoring mechanism as follows:
S i = {S (1) i , S (2) i }, for i = 1, 2, . . . , n S (1) i = {S (1) i1 , S (1) i2 , . . . , S (1) ik } S (2) i = {S (2) i1 , S (2) i2 , . . . , S (2) ik } (2)
where S i is the generated scores for different re-sponses for the i-th instance in the dataset along different aspects, consisting of two score sets (S</p>
<p>(1) i and S</p>
<p>(2)</p>
<p>i ) for the response candidates.S i may include more than two score sets when the evaluation is conducted for more than two candidates.S (m) ij denotes the score value for the m-th candidate of the i-th instance along the j-th aspect.</p>
<p>Aggregation</p>
<p>For each instance, the score set S i with k pairs of scores for the k different aspects will be generated in the decomposition stage.Previous work (Gong and Mao, 2023) passes aspect-wise score pairs as contexts in prompts for LLMs to provide the overall scores.However, it has been shown that LLMs may struggle to solve computation problems (Zhang et al., 2024).Therefore, we augment the framework with an external calculation module.We define an aggregation function f to compute the final score for each response.The aggregation will take the weighted sum of scores for each aspect:
f (S (m) i ) = k j=1 w ij S (m) ij (3)
where w ij is the weightage for the j-th aspect of the i-th instance.It can be obtained by prompting the LLMs for a percentage weightage indicating the importance for a specific aspect and instance.</p>
<p>After aggregating aspect-wise scores to the overall scores, the predicted label for the i-th instance is determined by comparing the overall scores:
ỹi =      1 f (S (1) i ) &gt; f (S (2) i ) 2 f (S (1) i ) &lt; f (S (2) i ) 0 f (S (1) i ) = f (S (2) i ) (4)
where 1 indicates Response 1 is better, 2 indicates Response 2 is better and 0 indicates a tie.</p>
<p>Experiments</p>
<p>Datasets</p>
<p>We conduct the experiments on four different metaevaluation benchmarks.We select more recent meta-evaluation benchmarks (published in 2023 or later) to mitigate the data leakage problem (Jiang et al., 2024;Dong et al., 2024) where the evaluation data has been used to train or finetune the models.In these benchmarks, each instance is annotated with a human preference label on which response is better.The four benchmarks cover two possible scenarios where a fixed set of criteria is given or not provided to human annotators in the evaluation process as summarized in Table 1.They cover a wide variety of task categories, including writing, math, knowledge, common sense, coding and summarization.and LLaMA-13B).They are labelled with preference by graduate students along the six dimensions of helpfulness, relevance, accuracy, creativity, depth and detail.Due to budget constraint, it is computationally expensive to run inferences on the entire dataset, we perform stratified random sampling for 400 single-turn samples, with a coverage of all unique questions in the dataset.</p>
<p>LLMBar (Zeng et al., 2023) consists of 419 questions that can be objectively evaluated for the instruction following ability.The annotators were not provided with criteria aspects when labeling their preferences.There are no tie cases in the dataset.We take the adversarial set of 319 instances in LLMBar benchmark for our experiment.The adversarial set holds adversarially crafted instances which are more prone to confuse less adept evaluators.Different LLMs are shown to have remarkable difference in evaluation capability on the challenging adversarial set by the authors of LLMBar.</p>
<p>InstruSum (Liu et al., 2023b) comprises 100 human-written articles and summary requirements.</p>
<p>Each article is accompanied with LLM-generated or hybrid LLM-human summaries annotated with human ratings on the overall quality.There are no uniform criteria pre-defined for the evaluation of the overall quality.There are five systems evaluated in InstruSum and we select summaries from GPT-3.5-turbo-0301 and GPT-4-0314 to construct pairs used for our experiments as these two systems have similar text generation capability among the five system options.</p>
<p>Experimental Setup</p>
<p>Models.We select two proprietary LLMs (GPT-3.5 and GPT-4) and two open-sourced LLMs (Llama2-13B and Mistral-7B) for a comprehensive exploration.This also allows meaningful comparisons of the evaluation capability between these two general classes.We select the 06-13 model version for GPT-3.5 and GPT-4 to mitigate the data leakage issue (see Appendix C).</p>
<p>Baselines.We compare the performance of our proposed framework to two baselines.One baseline is the direct scoring method which asks the models for the overall score for each response directly.</p>
<p>The second baseline is the Chain-of-Thought (CoT) method which asks models to provide explanations first followed by the overall score for each of the two responses.Prompts.We adopt the same prompting templates from the original experiment of each benchmark in the direct scoring method as they are carefully designed for the specific requirements of each task.For the CoT method and aspect generation, we follow the prompting templates in the work of Zeng et al. (2023) by asking for explanations before scores and asking for three relevant questions in evaluating the instance.</p>
<p>For aspect generation and aspect weighting stages, we include only the question context of the instance but not the responses in the prompts.This is because in real world situations, the design of the evaluation rubrics is usually task-specific without the need of knowing the responses to the question.For aspect-wise scoring, we ask for scores of different aspects in separate inferences since LLMs may be subject to anchoring effects for multi-attribute evaluation (Stureborg et al., 2024), where the generated scores in the same inference are correlated with one another.</p>
<p>Ablation Study.We also conduct an ablation study to investigate the effectiveness of the weighted sum approach in aggregation stage.In the ablation experiments, we pass the pairwise scoring for each aspect to LLMs as part of the prompt and ask the models to generate an overall score for each response respectively.It is a common practice to aggregate aspect-wise scores in previous work involving multi-aspect evaluation (Gong and Mao, 2023;Saha et al., 2023).</p>
<p>Results</p>
<p>Main Results.From Table 2, it can be seen that our Decompose and Aggregate framework generally outperforms the two baselines of direct scoring and CoT methods for both proprietary and opensourced models across different datasets.It illustrates the generalizable effectiveness of our method.The performance can be up to 39.6% (agreement of 42.4% with direct scoring and 59.1% with Decompose and Aggregate framework for GPT-4 on FairEval benchmark).Our results also corroborate the findings from previous work (Zeng et al., 2023) that CoT method does not bring about consistent improvement to LLMs' evaluation capability and sometimes even worsens it.This shows that our framework is better in terms of being both interpretable and effective at the same time.</p>
<p>Ablation Study.For almost all LLMs and benchmarks tested, using an external calculator to compute the weighted sum achieves a higher agreement with human than directly passing aspect-wise scores as prompts to LLMs.This suggests the limitation of LLMs in mathematical aggregation during the evaluation process.The results also show that our method using LLM-proposed weightings and an external computation module helps to address such limitation.</p>
<p>Qualitative analysis.To better understand where the performance improvement comes from, we conduct qualitative analysis for cases where direct scor-</p>
<p>Improvement Area</p>
<p>Text</p>
<p>Direct Scoring Decompose and Aggregate More Accurate Priorization Q: Embrace the role of Sheldon from "The Big Bang Theory" as we delve into our conversation.Don't start with phrases like "As Sheldon".Let's kick things off with the following question: "What is your opinion on hand dryers?"R1: Well, I must say, I find hand dryers to be a rather unsanitary and inefficient method of drying one's hands.The high-speed jets of air an spread bacteria and other microorganisms, potentially making the situation worse than it was before.Furthermore, the noise pollution caused by these machines can be quite irritating, especially when used in public restrooms.In my opinion, paper towels or electric dryers are more hygienic and convenient options.R2: As Sheldon, I am a staunch proponent of hand dryers.Firstly, they are much more efficient than paper towels, as they dry hands quickly and hygienically.Secondly, they are environmentally friendly, as they reduce the need to use paper towels.Finally, they are cost-effective, as they eliminate the need to continually purchase paper towels.In short, hand dryers are a superior option to paper towels!Table 3: Specific instances where Decompose and Aggregate framework corrects the mistakes in direct scoring.For the first question on the role playing of Sheldon, although Response 2 is rated higher for aspects like level of details, the LLM (GPT-4) is able to pick the correct response (Response 1) which performs better on more important aspect (relevance aspect) because it sticks to the constraint (don't start with phrases of "As Sheldon").For the second question, GPT-4 is able to identify the slightly better performance of Response 2 in terms of level of details, which is neglected in the direct scoring baseline setting.</p>
<p>ing gives the wrong evaluation whereas our Decompose and Aggregate framework provides the correct evaluation.We identify two main categories of improvement among these instances, which are (i) more accurate prioritization of different aspects and (ii) more subtle judgment.Table 3 shows two representative instances for each category.</p>
<p>Human Study</p>
<p>Our proposed Decompose and Aggregate framework produces intermediate outputs like LLMs' self-generated aspects and weightings for different aspects.This offers practitioners an opportunity to interpret and evaluate the intermediate steps of LLMs' evaluation process.Therefore, we perform human evaluation for self-generated aspects and weightings from different language models.</p>
<p>Data Collection</p>
<p>We recruit crowdworkers on Amazon Mechanical Turk to evaluate LLM-generated aspects and weightings.To ensure data quality, we require the annotators to have an accepted number of tasks higher than 500 and an approval rate higher than 98%.Crowdworkers who fulfilled these criteria went through a qualification round which contains exactly the same questions in the actual round.Their submissions for the qualification round were manually verified by the authors and qualified workers were given access to the actual round.We pay all annotators a fair wage (US$15 per hour) above the federal minimum.</p>
<p>To evaluate the quality of model-generated aspects, we ask three crowdworkers to rate the relevance, clarity and comprehensiveness of modelgenerated aspects independently for each instance.The rating is on a Likert scale of 1 to 5. We randomly sample 50 instances from each dataset and report the average scores along each dimension for different models.</p>
<p>For evaluation of model-generated weightings, it is difficult to define ratings on a Likert scale and ask human evaluators to numerically rate the quality of different weightings.Therefore, we convert the weightings from models and humans to ranks and then compute the Kendall's τ ranking distance (Fagin et al., 2003) between models' and crowdworkers' rankings.</p>
<p>Results</p>
<p>Aspect Generation Evaluation.All four models in our experiments achieve an above-average performance with scores higher than 4 for all three dimensions, suggesting LLMs are capable of generating evaluation aspects of good quality.However, there exist some nuanced differences across different models.Proprietary models like Chat-GPT and GPT-4 generally generates aspects that are more relevant, clearer and more comprehensive than open-sourced models.ChatGPT performs the best for relevance (4.95) and clarity (4.93) and GPT-4 outperforms other models in terms of comprehensiveness (4.84).On the other hand, Llama2-13B model performs the worst in generating evaluation aspects.ChatGPT, GPT-4, Llama2-13B and Mistral-7B along the dimensions of relevance, clarity and comprehensiveness.</p>
<p>Aspect Weighting Evaluation.To quantitatively evaluate LLMs' weighting capability, we convert weightings to rankings and compute the top-k Kendall's τ distance between each of the LLMs and humans, where a lower distance indicates a higher weighting similarity.For comparison, we also compute the Kendall's τ between two different human annotators.</p>
<p>In general, we see that there still exists more divergence between LLM's weightings and human's weightings (Figure 3) with higher Kendall's τ distances between LLM and human than that between humans (e.g. on MTBench and LLMBar) with a few exceptions where LLMs' weightings are more aligned with human's weightings (ChatGPT on FairEval; ChatGPT, Llama2 and Mistral on In-struSum).On average, ChatGPT's weightings are most aligned with human's and Mistral gives the most different weightings from human.</p>
<p>Qualitative Insights.We also collect free-text explanations from crowdworkers to better understand their evaluation for model-generated aspects and weightings.</p>
<p>Annotators identify different levels of relevance for different aspects generated by the models.They consider certain aspects as more crucial while oth- ers being important but less relevant.The varying relevance of aspects identified by human annotators justifies the aspect weighting stage in our framework.Moreover, for aspects with relatively lower ratings for clarity, annotators comment that there is a need for more specific guidance in terms of examples or illustrations (e.g.what constitutes 'a balanced view' mentioned in one criterion aspect).Additionally, they suggest there could be further breakdown of generated aspects to sub-aspects.In explanations for aspect weightings, annotators also justify the reasons for why some aspects are more crucial than others, indicating the varying importance of aspects generated by models.</p>
<p>Conclusions</p>
<p>We propose the Decompose and Aggregate, an effective and interpretable framework to use LLMs as evaluators.From our experiments on metaevaluation datasets with various features (e.g.presence of tie cases, presence of user-defined criteria) across different domains (e.g.writing, coding, summarization), we illustrate the effectiveness of the framework in enhancing LLMs' evaluation capability.We combine natural language reasoning (decomposition stage) with formalized symbolic reasoning (aggregation stage) in our proposed framework to introduce higher flexibility, reliability and verifiability.Moreover, our human study provides interpretable insights on different LLMs' evaluation capability in terms of aspect generation and aspect weighting.Such module-level analysis is able to shed light on multi-agent or human-LLM collaboration in evaluating texts.</p>
<p>Limitations</p>
<p>From our experimental results, we show that our Decompose and Aggregate framework not only provides higher interpretability of LLMs' evaluation process but also leads to performance improvement compared with direct scoring method.However, there are additional costs incurred with longer input lengths and increased number of inferences for aspect-wise evaluation and weighting generations.Moreover, we set a fixed number of aspects (three aspects) in our experiments.The number of aspects that are relevant may be context-dependent and may vary from case to case.Therefore, future work could explore what is the optimal number of aspects and investigate the effectiveness of dynamic aspect generation with unspecified number of aspects.</p>
<p>In addition, we evaluate the performance of baselines and our method using agreement with human preference labels.This is the most common approach adopted in current meta-evaluation work.However, human preference labels may not be the gold label all the time and agreement with human preference may not be the most accurate way to measure LLMs' evaluation capability.For example, in some cases labeled as 'ties' by human, LLMs are able to identify nuanced differences and pick the slightly better answer, demonstrating super-human level evaluative capability.We observed this during experimentation with GPT-4 on MTBench dataset where the model identified subtle differences in two responses unnoticed by humans.Therefore, there is no improvement with tie cases included but there exists improvement with tie cases excluded when applying our framework.</p>
<p>Ethics Statement</p>
<p>This study has been approved by the Institutional Review Board (IRB) at the researchers' institution, and we obtained participant consent with a standard institutional consent form.One potential ethical concern of using LLMs as evaluators is the stereotypes and biases existing in LLMs such as political bias, gender bias, cultural bias and so on.Although our work mainly serves as a new framework to improve LLMs' evaluation capability with greater interpretability, we still acknowledge these potential ethical concerns that may come with using LLMs as judges.</p>
<p>A LLM Inference Setting Models.We run inferences using GPT-3.5-0613,GPT-4-0613, Llama2-13B and Mistral-7B-Instruct-v2. Temperature Setting.We set temperature to 0 for classification tasks to ensure reproducibility.</p>
<p>Prompts.</p>
<p>• Direct Scoring: We adopt the same prompting templates from the original experiment of each benchmark for the direct scoring method as they are carefully designed for the specific requirement of each task.The prompts contain the instance context, candidate responses and evaluation instruction.</p>
<p>• CoT Prompting: We ask the models to provide an explanation and then an overall socre for each of the response candidates in the instruction.The prompts contain the instance context, candidate responses and evaluation instruction.</p>
<p>• Aspect Generation: When criteria aspects are not given in the cases of LLMBar and In-struSum, we follow the prompting templates for metrics generation strategy in the work of Zeng et al. (2023) by asking the models to propose three concise questions about whether a potential output is a good output for a given instruction.The prompts contain the instance context and aspect generation instruction.</p>
<p>• Aspect-wise Scoring: The prompting templates we use are similar to direct scoring.The only difference is that we pass the predefined criteria or the model-generated aspect to the model for pairwise scoring on top of the instance context, candidate responses and evaluation instruction.</p>
<p>• Weighting Proposal: We formulate our instruction as "Please propose respective importance weightage for three aspects in evaluating the summary."The prompts contain the instance context and model-generated aspects.We further specify some requirements for the weighting outputs: "1) The weightages should be in percentage form and sum up to 100%; 2) You should directly give the weightages without any other words; 3) You should give weightages in the same line, separated by space."</p>
<p>B Robustness Check</p>
<p>Due to relatively small number of instances in each meta-evaluation dataset, there is little statistical significance in performance difference between the baseline method and our method.Therefore, we repeat the experiments at two other different seeds and calculate the statistical significance as a robustness check.From</p>
<p>C Data Leakage Analysis</p>
<p>During our dataset selection, we select the most recent meta-evaluation benchmarks (published in 2023 or later) to mitigate data leakage problems.</p>
<p>There is minimal likelihood of data leakage if the release date of the model is before the release date of the dataset.</p>
<p>From Table 6, most dataset-model combinations in our experiments are not subject to data leakage.However, there may exist data leakage for testing Llama2-13B and Mistral-7B on the FairEval dataset.Such data leakage issue may be the cause for no improvement of our Decompose and Aggregate method compared to direct scoring method.There is also a slight chance of data leakage for testing Mistral-7B on LLMBar and InstruSum.</p>
<p>Figure 1 :
1
Figure 1: Different from most previous work which asks LLMs directly for its preference over two responses, our Decompose and Aggregate framework takes inspirations from key components used in evaluation rubrics in pedagogy.It consists of criteria proposal, pairwise rating by aspect and aggregation of aspect-wise scores.This framework enhances the transparency, accountability and interpretability of the blackbox evaluation process.</p>
<p>Figure 2 :
2
Figure 2: Different stages of the Decompose and Aggregate framework.In the decomposition stage, LLMs are provided with</p>
<p>Q:</p>
<p>Structure a podcast script for an episode discussing the influence of streaming platforms on the music industry.R1: ...... Segment 1: The Pros of Streaming Platforms: * Increased access to music * Conv--enience and ease of use * Greater exposure for artists * Revenue streams for artists Segment 2: The Cons of Streaming Platforms: * Decreased album sales and revenue for artists <em>Artists not being paid fairly </em>Piracy and copyright issues *Mental health and addiction concerns ...... R2: ...... Section 1: Overview of the Music Industry:-Provide an overview of the music industry before the rise of streaming services.-Discuss how record labels, physical album sales, and radio play were the primary distribution channels for music.Section 2: Streaming Platforms:-Discuss the rise of streaming platforms like Spotify, Apple Music, and Tidal.-Explainhow these platforms disrupted the traditional distribution model of the music industry.......</p>
<p>Figure 3 :
3
Figure 3: Kendall's τ distance for aspect weightings between different language models and human.We visualize the rank distance between two different human annotators in dotted lines for a comparison.</p>
<p>Table 2 :
2
Agreement with human preference label of each LLM on different meta-evaluation benchmarks.For InstruSum, MTBench and FairEval, we report the agreement with (first number) and without (second number) tie cases in each cell.† marks the situation where the ablation setting (replacing weighted sum aggregation with prompted aggregation) leads to a drop in performance, suggesting LLMs' limitations in aggregating the scores during the evaluation process.
LLMBar-AdversarialInstruSumChatGPTGPT-4LLaMa2-13B Mistral-7BChatGPTGPT-4LLaMa2-13B Mistral-7BDirect Scoring29.870.829.832.949.0 52.238.0 40.053.0 58.917.0 11.1Scoring with CoT24.875.233.943.323.0 21.148.0 52.247.0 52.230.0 33.3Decompose and Aggregate (ours)33.577.134.239.260.0 64.453.0 57.860.0 66.725.0 21.1Ablation30.4 †75.5 †33.9 †27.6 †43.0 † 44.4 † 51.0 † 52.2 † 48.0 † 53.3 †11.0 † 3.3 †MTBench400FairEvalChatGPTGPT-4LLaMa2-13B Mistral-7BChatGPTGPT-4LLaMa2-13B Mistral-7BDirect Scoring58.0 71.867.8 74.453.8 71.453.8 61.153.8 60.646.3 42.446.3 56.152.5 62.1Scoring with CoT58.0 71.461.3 76.754.3 72.158.0 66.742.5 36.450.0 54.543.8 53.048.8 54.5Decompose and Aggregate (ours) 59.8 74.865.3 78.456.3 74.856.8 67.456.3 65.251.3 59.146.3 56.152.5 62.1Ablation59.8 73.1 † 66.8 76.7 † 55.0 † 73.1 † 53.3 † 61.1 † 51.3 † 56.1 † 48.8 † 56.1 † 41.3 † 50.0 † 50.0 † 60.6 †</p>
<p>Table 4 :
4
Average human ratings for aspects generated by
ModelRelevance Clarity ComprehensivenessChatGPT4.954.934.80GPT-44.894.904.84Llama2-13B4.704.784.64Mistral-7B4.894.874.71</p>
<p>Table 5 :
5
Table 5, we can see that the performance improvement achieved by our Decompose and Aggregate method is generally statistically significant on most of model-dataset combinations.Significance test results for Decompose and Aggregate method and Direct Scoring method.<em> p&lt;0.1, ** p&lt;0.05, *** p&lt;0.01
FairEvalMTBench InstruSumLLMBarChatGPT</em><strong><em>*</em></strong>Not SignificantGPT-4<strong><em>*</em></strong><strong><em>Llama2-13B Not Significant</em></strong><strong><em>Mistral-7B Not Significant</em></strong><strong>*</strong>*</p>
<p>Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, arXiv:2302.04023A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. 2023arXiv preprint</p>
<p>Appropriate criteria: Key to effective rubrics. Susan M Brookhart, Frontiers in Education. 201822</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu, arXiv:2308.07201Chateval: Towards better llm-based evaluators through multi-agent debate. 2023arXiv preprint</p>
<p>Exploring the use of large language models for reference-free text quality evaluation: A preliminary empirical study. Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, Ruifeng Xu, arXiv:2304.007232023arXiv preprint</p>
<p>Can large language models be trusted for evaluation? scalable meta-evaluation of llms as evaluators via agent debate. Steffi Chern, Ethan Chern, Graham Neubig, Pengfei Liu, arXiv:2401.167882024arXiv preprint</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, arXiv:2305.019372023arXiv preprint</p>
<p>Values in evaluation-the use of rubrics. Pauline Dickinson, Jeffery Adams, Evaluation and program planning. 652017</p>
<p>Generalization or memorization: Data contamination and trustworthy evaluation for large language models. Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, Ge Li, arXiv:2402.159382024arXiv preprint</p>
<p>Comparing top k lists. Ronald Fagin, Ravi Kumar, Dakshinamurthi , SIAM Journal on discrete mathematics. 171Sivakumar. 2003</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, arXiv:2302.04166Gptscore: Evaluate as you desire. 2023arXiv preprint</p>
<p>Coascore: Chainof-aspects prompting for nlg evaluation. Peiyuan Gong, Jiaxin Mao, arXiv:2312.103552023arXiv preprint</p>
<p>Investigating data contamination for pretraining language models. Minhao Jiang, Ken Ziyu Liu, Ming Zhong, Rylan Schaeffer, Siru Ouyang, Jiawei Han, Sanmi Koyejo, 2024</p>
<p>The use of scoring rubrics: Reliability, validity and educational consequences. Anders Jonsson, Gunilla Svingby, Educational research review. 222007</p>
<p>The perils of using mechanical turk to evaluate open-ended text generation. Marzena Karpinska, Nader Akoury, Mohit Iyyer, arXiv:2109.068352021arXiv preprint</p>
<p>Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, arXiv:2310.08491Prometheus: Inducing fine-grained evaluation capability in language models. 2023arXiv preprint</p>
<p>Large language models are state-of-the-art evaluators of translation quality. Tom Kocmi, Christian Federmann, arXiv:2302.145202023arXiv preprint</p>
<p>Hurdles to progress in long-form question answering. Kalpesh Krishna, Aurko Roy, Mohit Iyyer, arXiv:2103.063322021arXiv preprint</p>
<p>Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, Jimmy Xiangji Huang, arXiv:2305.18486A systematic study and comprehensive evaluation of chatgpt on benchmark datasets. 2023arXiv preprint</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, arXiv:2303.16634Gpteval: Nlg evaluation using gpt-4 with better human alignment. 2023aarXiv preprint</p>
<p>Aligning with human judgement: The role of pairwise preference in large language model evaluators. Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulic, Anna Korhonen, Nigel Collier, arXiv:2403.169502024arXiv preprint</p>
<p>Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization. Yixin Liu, Alexander R Fabbri, Jiawen Chen, Yilun Zhao, Simeng Han, Shafiq Joty, Pengfei Liu, Dragomir Radev, Chien-Sheng Wu, Arman Cohan, 2023b</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Is chatgpt a general-purpose natural language processing task solver?. Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, Diyi Yang, arXiv:2302.06476Branchsolve-merge improves large language model evaluation and generation. Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, Xian Li, 2023. 2023arXiv preprint</p>
<p>On the evaluation metrics for paraphrase generation. Lingfeng Shen, Lemao Liu, Haiyun Jiang, Shuming Shi, arXiv:2202.084792022arXiv preprint</p>
<p>Large language models are inconsistent and biased evaluators. Rickard Stureborg, Dimitris Alikaniotis, Yoshi Suhara, 2024</p>
<p>Joint learning of a dual smt system for paraphrase generation. Hong Sun, Ming Zhou, Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics. the 50th Annual Meeting of the Association for Computational Linguistics20122Short Papers)</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, Jie Zhou, arXiv:2303.04048Is chatgpt a good nlg evaluator? a preliminary study. 2023aarXiv preprint</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, arXiv:2305.179262023barXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>Style over substance: Evaluation biases for large language models. Minghao Wu, Alham Fikri, Aji , arXiv:2307.030252023arXiv preprint</p>
<p>Bartscore: Evaluating generated text as text generation. Weizhe Yuan, Graham Neubig, Pengfei Liu, Advances in Neural Information Processing Systems. 202134</p>
<p>Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, Danqi Chen, arXiv:2310.07641Evaluating large language models at evaluating instruction following. 2023arXiv preprint</p>
<p>Evaluating and improving tool-augmented computationintensive math reasoning. Beichen Zhang, Kun Zhou, Xilin Wei, Xin Zhao, Jing Sha, Shijin Wang, Ji-Rong Wen, Advances in Neural Information Processing Systems. 202436</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, arXiv:1904.09675Bertscore: Evaluating text generation with bert. 2019arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 202436</p>            </div>
        </div>

    </div>
</body>
</html>
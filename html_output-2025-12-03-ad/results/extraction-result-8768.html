<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8768 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8768</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8768</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-267636663</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.08170v3.pdf" target="_blank">LLaGA: Large Language and Graph Assistant</a></p>
                <p><strong>Paper Abstract:</strong> Graph Neural Networks (GNNs) have empowered the advance in graph-structured data analysis. Recently, the rise of Large Language Models (LLMs) like GPT-4 has heralded a new era in deep learning. However, their application to graph data poses distinct challenges due to the inherent difficulty of translating graph structures to language. To this end, we introduce the Large Language and Graph Assistant (LLaGA), an innovative model that effectively integrates LLM capabilities to handle the complexities of graph-structured data. LLaGA retains the general-purpose nature of LLMs while adapting graph data into a format compatible with LLM input. LLaGA achieves this by reorganizing graph nodes to structure-aware sequences and then mapping these into the token embedding space through a versatile projector. LLaGA excels in versatility, generalizability and interpretability, allowing it to perform consistently well across different datasets and tasks, extend its ability to unseen datasets or tasks, and provide explanations for graphs. Our extensive experiments across popular graph benchmarks show that LLaGA delivers outstanding performance across four datasets and three tasks using one single model, surpassing state-of-the-art graph models in both supervised and zero-shot scenarios. Our code is available at \url{https://github.com/VITA-Group/LLaGA}.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8768.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8768.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaGA-ND</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaGA Neighborhood Detail Template</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A node-level, structure-aware serialization that builds a fixed-shape sampled computational tree around a center node, performs level-order traversal to produce a fixed-length node sequence, encodes text attributes with off-the-shelf encoders and appends precomputed Laplacian positional embeddings; used to map graph structure into LLM token embedding space via a learned projector.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Neighborhood Detail Template (computational-tree linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Constructs a fixed-shape computational tree centered at the node by sampling a fixed number of neighbors per hop (n1, n2, ...), padding with placeholders when neighbor sets are small, then flattens the tree via level-order traversal to form a fixed-length node sequence; each sequence position holds the text-encoded node feature (or zero vector for pads) concatenated with a precomputed Laplacian Eigenvector embedding for that template position.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs (citation networks, e-commerce co-purchase graphs as used: ogbn-Arxiv, ogbn-Products, Pubmed, Cora)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>For each center node: (1) sample fixed-size neighbor sets per hop to make a fixed-shape tree (placeholders used if insufficient neighbors), (2) perform level-order traversal to produce a node sequence (e.g., A B C D A G [pad] ...), (3) encode textual node attributes with SBERT/SimTeG/ RoBERTa to obtain node vectors, (4) append precomputed Laplacian embedding U_i for each sequence position, (5) concatenate to form final node-embedding sequence; projected into token embedding space via a small MLP projector f_theta.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification, link prediction, node description (text generation describing node), and zero-shot transfer experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in multiple tables: e.g., node-description Sbert score and Description Label Accuracy (Table 4) for ND: ArXiv Sbert=0.6023, Description Label Accuracy=74.64%; Products Sbert=0.4952, Accuracy=83.18%; Pubmed Sbert=0.6847, Accuracy=92.27%; Cora Sbert=0.6465, Accuracy=86.72%. Zero-shot link prediction and classification: in general-model and multi-task settings LLaGA-ND outperforms baselines (see Table 1 and Table 5); zero-shot link-prediction accuracies (Table 5) using ND: ARXIV+PUBMED→CORA 86.47% (ND 86.47% shown for LLaGA-ND-7B in Table 5) and ARXIV+PUBMED+CORA→PRODUCTS 92.65% for LLaGA-ND-7B.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared directly to Hop-Field (HO) template: ND excels on tasks requiring detailed neighbor information (e.g., product category identification on ogbn-Products). Compared to GraphGPT and GNN baselines, LLaGA-ND (as part of the overall LLaGA framework) achieved substantially higher scores (see Table 2 and Table 1): e.g., GraphGPT variants report ARXIV ~64–63% while LLaGA-ND general model reports ARXIV ~74.29% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Preserves fine-grained neighbor structure and relative positions (via fixed-shape tree + level-order mapping); works with off-the-shelf text encoders and a parameter-free structural transform; when combined with the projector, yields high task accuracy, good interpretability (enables node description generation), and strong multi-task and zero-shot generalization for datasets tested.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires sampling hyperparameters (per-hop sample sizes) and can produce long sequences (sampling many neighbors across hops); padding introduces many placeholder tokens in sparse neighborhoods which must be represented (they use zero vectors); computational cost grows with sampled tree size; fragile to choice of sample sizes if too small/large.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Authors note that ND is less optimal when a broader overview (not fine detail) matters — e.g., some citation graphs benefit more from Hop-Field Overview; padding may dilute useful signal when neighbor counts are small; no explicit failure-case example beyond these limitations is given, but ablation shows models without templates perform substantially worse on link prediction (showing templates are critical).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLaGA: Large Language and Graph Assistant', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8768.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8768.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaGA-HO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaGA Hop-Field Overview Template</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A node-level summarized serialization that represents each hop around a center node by a single hop-embedding obtained via parameter-free message passing on encoded text features, producing a short sequence of hop embeddings per center node to capture broader neighborhood context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Hop-Field Overview Template (hop-summary serialization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Computes hop embeddings h^i_v for i = 0..H where h^0_v = phi(x_v) (text encoding) and h^i_v is the mean (parameter-free message passing) of h^{i-1} over the 1-hop neighbors, so each hop embedding summarizes information from all nodes at that hop; the sequence [h^0_v, h^1_v, ..., h^H_v] is used as the node representation and projected into token embedding space by the learned projector.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs (citation and e-commerce graphs used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>For each central node: (1) encode node text attributes with SBERT/SimTeG/ RoBERTa to get h^0, (2) iteratively compute hop summaries by averaging message-passed embeddings from neighbors to obtain h^1, h^2, ..., h^H (parameter-free averaging), (3) form a short sequence of hop embeddings representing increasing neighborhood radii, (4) project sequence into token embedding space with MLP projector f_theta for LLM input.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification, link prediction, node description, and zero-shot transfer</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Node-description (Table 4) for HO: ArXiv Sbert=0.6228, Description Label Accuracy=75.49%; Products Sbert=0.5193, Accuracy=84.60%; Pubmed Sbert=0.6934, Accuracy=94.27%; Cora Sbert=0.6545, Accuracy=86.90%. Zero-shot link-prediction (Table 5): HO yields 87.35% (ARXIV+PUBMED→CORA) and 92.99% (ARXIV+PUBMED+CORA→PRODUCTS). Across multi-task/general settings, LLaGA-HO often matches or exceeds ND on citation graphs (see Table 1 and Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to the Neighborhood Detail Template: HO trades detailed neighbor-level information for a compact, broader receptive field; HO performed better on some citation-graph tasks where aggregate neighbor context is more informative (authors cite HO being preferable for some citation graphs). Compared to prior plain-text description approaches and GraphGPT, HO-based LLaGA models outperform them on the tested benchmarks (see Table 1 and Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>More compact sequence length (one vector per hop) and captures broader neighborhood context; reduces sequence length growth with hop distance; better suited where aggregated neighbor-level signals are most informative; still parameter-free for structure encoding and combines with a single learned projector enabling multi-task learning and zero-shot transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Sacrifices per-neighbor detail (aggregation can wash out distinguishing neighbor-level signals), so may underperform on tasks that require pinpointing fine-grained neighbor identities (e.g., some product categorization cases).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Authors report that HO can underperform in scenarios where specific neighbor identities or detailed neighbor patterns are critical (e.g., classifying a product requiring evidence of many specific neighbor types like 'Nintendo Switch'); no concrete catastrophic failure examples are supplied, but ablation shows template choice materially affects link-prediction and classification performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLaGA: Large Language and Graph Assistant', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8768.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8768.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaGA-Projector</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaGA Versatile Projector (f_theta)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small learned MLP that maps node- or sequence-level embeddings (from templates) into the token embedding space of an LLM, trained across multiple graph datasets and tasks to align graph representations with LLM input embeddings without fine-tuning the LLM itself.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Projector-based token-space alignment</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A learned MLP f_theta takes each node-embedding (concatenation of text encoder output and Laplacian positional vector for ND, or hop embeddings for HO) and outputs token embeddings e_i compatible with the LLM's token embedding space; during training only the projector parameters θ (and LLM decoder weights for generative supervision) are tuned, enabling graph-to-token alignment while preserving the base LLM parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs (used with ND and HO template sequences from ogbn-Arxiv, ogbn-Products, Pubmed, Cora)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>After templates produce node-embedding sequences, each embedding h_i is passed through the MLP projector to produce token embedding e_i; these e_i are inserted into the LLM prompt at the positions reserved for <node sequence>, mixing with token embeddings of the natural-language prompt, and the LLM is trained to generate answers (QA/chat format) for node classification, link prediction or node description.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification, link prediction, node description; multi-task training and zero-shot transfer</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Using the projector+templates within LLaGA yields superior metrics vs baselines: e.g., full LLaGA general models achieve higher node classification and link prediction accuracies across datasets than GNNs and GraphGPT (see Table 1 and Table 2). Specific numbers: zero-shot node classification with node-embedding+text attributes (Table 8) ARXIV+PUBMED→CORA: LLaGA-7B accuracy 59.59% vs GraphGPT-7B 44.65%; for ARXIV+PUBMED+CORA→PRODUCTS: LLaGA-7B 43.79% vs GraphGPT-7B 18.84%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Contrasts with approaches that fine-tune full LLMs for graph tasks (which can reduce generality) or that use a pre-trained graph transformer (GraphGPT) as an intermediate encoder; LLaGA's projector is simple (MLP), trained jointly across tasks/datasets, and authors report it generalizes better than pre-trained graph transformer approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Avoids expensive LLM fine-tuning (only small projector trained), enables a single versatile translator for multiple datasets/tasks, supports interpretability (node description generation), and leads to strong multi-task and zero-shot performance.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Alignment quality depends on the projector and template choices; limited expressivity of a small MLP might fail to fully align more complex graph encodings to token space in some settings; requires template-specific positional encodings (Laplacian U) computed per template shape.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit numerical failure modes for the projector alone are given, but authors note that a poor template (or no template) leads to degraded performance; templates + projector are needed together for strong link-prediction results (ablation in Table 6 shows 'NONE' severely underperforms).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLaGA: Large Language and Graph Assistant', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8768.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8768.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Plain-text graph description</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Natural-language/descriptive graph serialization (prior methods)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Describing graph nodes and edges in natural language strings (e.g., textual descriptions of nodes and their relations) for feeding into LLMs; this is a prior approach discussed as having limitations such as verbosity and ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Plain-text graph description (language serialization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert graph structure directly into natural language sentences that describe nodes and their relations (e.g., 'Node A is connected to nodes B and C; node B is about X'); then feed the resulting text into LLMs either for prompting or task fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs (mentioned in context of prior works on graphs-to-LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Hand-crafted natural-language templates that describe node neighbors and edges; no fixed structural serialization—relies on verbose natural language to capture relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Prior works attempted node classification and other graph tasks using LLMs prompted with such descriptions; the paper contrasts these with LLaGA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper cites that plain textual descriptions perform poorly on basic graph tasks without specific adaptations (cites Chen et al., 2023a and others) but does not provide direct numerical metrics in this manuscript for plain-text-only approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Authors contrast plain-text descriptions unfavorably with their template-based sequence conversions: plain texts are verbose, repetitive, and fail to directly encode structural properties, leading to worse LLM performance; Instruct-GLM (Ye et al., 2023) that fine-tunes on text descriptions can help but sacrifices versatility.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Intuitive and human-readable; can be directly used with off-the-shelf LLM prompting without additional projectors or structure-aware encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Verbose, ambiguous, repetitive, often fails to capture structural characteristics succinctly, and leads to poor LLM performance on graph tasks unless heavily fine-tuned (loss of generality).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Reported generally as producing poor results on basic graph tasks; described as leading to repetitive and unintuitive descriptions of nodes and edges and consequently poor LLM performance without specific adaptations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLaGA: Large Language and Graph Assistant', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8768.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8768.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphGPT approach</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphGPT (concurrent work: text descriptions + pre-trained graph transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A concurrent approach that combines textual node descriptions with a pretrained graph transformer to encode structure for input to LLMs; used as a comparison baseline in the LLaGA paper and reported to underperform LLaGA on the evaluated benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Talk like a graph: Encoding graphs for large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Text + Pretrained Graph Transformer encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Uses a text-encoding model to obtain node textual features and a pretrained graph transformer to encode structural information; the combination is provided to an LLM (or used to prompt it) for graph tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs (same domains as this paper: citation and product co-purchase graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Text-encode node attributes and separately transform graph structure through a pretrained graph transformer, then feed combined representation (or textual prompts derived from them) to an LLM for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification and link prediction (GraphGPT trained on a smaller set of tasks/datasets than LLaGA but used as a general baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported (from GraphGPT paper as cited): GraphGPT-MIX-7B ARXIV ~64.76% and GraphGPT-STD-7B ARXIV ~63.90% (Table 2 in LLaGA paper references GraphGPT results). LLaGA general models (ND and HO) report ARXIV ~74.29% and ~75.01% respectively (Table 2), indicating substantial improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>LLaGA outperforms GraphGPT in the reported comparisons: GraphGPT is trained on fewer tasks/datasets and uses a pretrained graph transformer which the authors of LLaGA argue may not distill task-relevant structural information as effectively as LLaGA's template+projector alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Leverages a pretrained graph transformer to capture structural patterns and combines it with textual attributes — conceptually straightforward and uses learned graph encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>May require a specific pre-trained graph encoder which might not capture all task-relevant structure and can be less general across datasets/tasks; reported by the authors to give less satisfactory performance than LLaGA on the evaluated benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Authors claim GraphGPT's pre-trained graph transformer might fail to distill structural information needed for specific downstream tasks leading to degraded performance (no detailed failure-case instances beyond comparative performance are given).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLaGA: Large Language and Graph Assistant', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8768.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8768.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruct-GLM textual fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruct-GLM (textual graph descriptions + LLM fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that describes graphs in language and fine-tunes LLMs on graph tasks using these textual descriptions; noted by LLaGA authors as improving LLM graph-task performance but reducing model versatility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>LLM fine-tuning on natural-language graph descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert graph structure into natural-language descriptions and then fine-tune an LLM on those textual prompts/responses for graph tasks, effectively specializing the LLM to the graph domain.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs (contextual mention)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Handcrafted or templated natural-language descriptions of nodes and local graph context used as LLM training data; LLM parameters are fine-tuned for graph tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification and other graph tasks via specialized fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No numerical metrics provided in this paper for Instruct-GLM; described qualitatively as improving graph-task performance but at the cost of versatility.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared with LLaGA: Instruct-GLM's fine-tuning improves performance on specific graph tasks but the authors claim this constrains LLM versatility and may limit effectiveness on other tasks/domains; LLaGA's projector-only training keeps the LLM general-purpose.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Can improve LLM performance on the specific graph tasks it is fine-tuned for.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Specialization limits versatility and may harm performance on non-graph tasks or other graph tasks without re-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Reported limitation is decreased versatility/generalizability when LLMs are specialized via fine-tuning on graph descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLaGA: Large Language and Graph Assistant', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Talk like a graph: Encoding graphs for large language models <em>(Rating: 2)</em></li>
                <li>GraphGPT <em>(Rating: 2)</em></li>
                <li>Exploring the potential of large language models (llms) in learning on graphs <em>(Rating: 1)</em></li>
                <li>Natural language is all a graph needs <em>(Rating: 1)</em></li>
                <li>A frustratingly simple approach improves textual graph learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8768",
    "paper_id": "paper-267636663",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "LLaGA-ND",
            "name_full": "LLaGA Neighborhood Detail Template",
            "brief_description": "A node-level, structure-aware serialization that builds a fixed-shape sampled computational tree around a center node, performs level-order traversal to produce a fixed-length node sequence, encodes text attributes with off-the-shelf encoders and appends precomputed Laplacian positional embeddings; used to map graph structure into LLM token embedding space via a learned projector.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Neighborhood Detail Template (computational-tree linearization)",
            "representation_description": "Constructs a fixed-shape computational tree centered at the node by sampling a fixed number of neighbors per hop (n1, n2, ...), padding with placeholders when neighbor sets are small, then flattens the tree via level-order traversal to form a fixed-length node sequence; each sequence position holds the text-encoded node feature (or zero vector for pads) concatenated with a precomputed Laplacian Eigenvector embedding for that template position.",
            "graph_type": "Text-attributed graphs (citation networks, e-commerce co-purchase graphs as used: ogbn-Arxiv, ogbn-Products, Pubmed, Cora)",
            "conversion_method": "For each center node: (1) sample fixed-size neighbor sets per hop to make a fixed-shape tree (placeholders used if insufficient neighbors), (2) perform level-order traversal to produce a node sequence (e.g., A B C D A G [pad] ...), (3) encode textual node attributes with SBERT/SimTeG/ RoBERTa to obtain node vectors, (4) append precomputed Laplacian embedding U_i for each sequence position, (5) concatenate to form final node-embedding sequence; projected into token embedding space via a small MLP projector f_theta.",
            "downstream_task": "Node classification, link prediction, node description (text generation describing node), and zero-shot transfer experiments",
            "performance_metrics": "Reported in multiple tables: e.g., node-description Sbert score and Description Label Accuracy (Table 4) for ND: ArXiv Sbert=0.6023, Description Label Accuracy=74.64%; Products Sbert=0.4952, Accuracy=83.18%; Pubmed Sbert=0.6847, Accuracy=92.27%; Cora Sbert=0.6465, Accuracy=86.72%. Zero-shot link prediction and classification: in general-model and multi-task settings LLaGA-ND outperforms baselines (see Table 1 and Table 5); zero-shot link-prediction accuracies (Table 5) using ND: ARXIV+PUBMED→CORA 86.47% (ND 86.47% shown for LLaGA-ND-7B in Table 5) and ARXIV+PUBMED+CORA→PRODUCTS 92.65% for LLaGA-ND-7B.",
            "comparison_to_others": "Compared directly to Hop-Field (HO) template: ND excels on tasks requiring detailed neighbor information (e.g., product category identification on ogbn-Products). Compared to GraphGPT and GNN baselines, LLaGA-ND (as part of the overall LLaGA framework) achieved substantially higher scores (see Table 2 and Table 1): e.g., GraphGPT variants report ARXIV ~64–63% while LLaGA-ND general model reports ARXIV ~74.29% (Table 2).",
            "advantages": "Preserves fine-grained neighbor structure and relative positions (via fixed-shape tree + level-order mapping); works with off-the-shelf text encoders and a parameter-free structural transform; when combined with the projector, yields high task accuracy, good interpretability (enables node description generation), and strong multi-task and zero-shot generalization for datasets tested.",
            "disadvantages": "Requires sampling hyperparameters (per-hop sample sizes) and can produce long sequences (sampling many neighbors across hops); padding introduces many placeholder tokens in sparse neighborhoods which must be represented (they use zero vectors); computational cost grows with sampled tree size; fragile to choice of sample sizes if too small/large.",
            "failure_cases": "Authors note that ND is less optimal when a broader overview (not fine detail) matters — e.g., some citation graphs benefit more from Hop-Field Overview; padding may dilute useful signal when neighbor counts are small; no explicit failure-case example beyond these limitations is given, but ablation shows models without templates perform substantially worse on link prediction (showing templates are critical).",
            "uuid": "e8768.0",
            "source_info": {
                "paper_title": "LLaGA: Large Language and Graph Assistant",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaGA-HO",
            "name_full": "LLaGA Hop-Field Overview Template",
            "brief_description": "A node-level summarized serialization that represents each hop around a center node by a single hop-embedding obtained via parameter-free message passing on encoded text features, producing a short sequence of hop embeddings per center node to capture broader neighborhood context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Hop-Field Overview Template (hop-summary serialization)",
            "representation_description": "Computes hop embeddings h^i_v for i = 0..H where h^0_v = phi(x_v) (text encoding) and h^i_v is the mean (parameter-free message passing) of h^{i-1} over the 1-hop neighbors, so each hop embedding summarizes information from all nodes at that hop; the sequence [h^0_v, h^1_v, ..., h^H_v] is used as the node representation and projected into token embedding space by the learned projector.",
            "graph_type": "Text-attributed graphs (citation and e-commerce graphs used in experiments)",
            "conversion_method": "For each central node: (1) encode node text attributes with SBERT/SimTeG/ RoBERTa to get h^0, (2) iteratively compute hop summaries by averaging message-passed embeddings from neighbors to obtain h^1, h^2, ..., h^H (parameter-free averaging), (3) form a short sequence of hop embeddings representing increasing neighborhood radii, (4) project sequence into token embedding space with MLP projector f_theta for LLM input.",
            "downstream_task": "Node classification, link prediction, node description, and zero-shot transfer",
            "performance_metrics": "Node-description (Table 4) for HO: ArXiv Sbert=0.6228, Description Label Accuracy=75.49%; Products Sbert=0.5193, Accuracy=84.60%; Pubmed Sbert=0.6934, Accuracy=94.27%; Cora Sbert=0.6545, Accuracy=86.90%. Zero-shot link-prediction (Table 5): HO yields 87.35% (ARXIV+PUBMED→CORA) and 92.99% (ARXIV+PUBMED+CORA→PRODUCTS). Across multi-task/general settings, LLaGA-HO often matches or exceeds ND on citation graphs (see Table 1 and Table 6).",
            "comparison_to_others": "Compared to the Neighborhood Detail Template: HO trades detailed neighbor-level information for a compact, broader receptive field; HO performed better on some citation-graph tasks where aggregate neighbor context is more informative (authors cite HO being preferable for some citation graphs). Compared to prior plain-text description approaches and GraphGPT, HO-based LLaGA models outperform them on the tested benchmarks (see Table 1 and Table 2).",
            "advantages": "More compact sequence length (one vector per hop) and captures broader neighborhood context; reduces sequence length growth with hop distance; better suited where aggregated neighbor-level signals are most informative; still parameter-free for structure encoding and combines with a single learned projector enabling multi-task learning and zero-shot transfer.",
            "disadvantages": "Sacrifices per-neighbor detail (aggregation can wash out distinguishing neighbor-level signals), so may underperform on tasks that require pinpointing fine-grained neighbor identities (e.g., some product categorization cases).",
            "failure_cases": "Authors report that HO can underperform in scenarios where specific neighbor identities or detailed neighbor patterns are critical (e.g., classifying a product requiring evidence of many specific neighbor types like 'Nintendo Switch'); no concrete catastrophic failure examples are supplied, but ablation shows template choice materially affects link-prediction and classification performance.",
            "uuid": "e8768.1",
            "source_info": {
                "paper_title": "LLaGA: Large Language and Graph Assistant",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaGA-Projector",
            "name_full": "LLaGA Versatile Projector (f_theta)",
            "brief_description": "A small learned MLP that maps node- or sequence-level embeddings (from templates) into the token embedding space of an LLM, trained across multiple graph datasets and tasks to align graph representations with LLM input embeddings without fine-tuning the LLM itself.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Projector-based token-space alignment",
            "representation_description": "A learned MLP f_theta takes each node-embedding (concatenation of text encoder output and Laplacian positional vector for ND, or hop embeddings for HO) and outputs token embeddings e_i compatible with the LLM's token embedding space; during training only the projector parameters θ (and LLM decoder weights for generative supervision) are tuned, enabling graph-to-token alignment while preserving the base LLM parameters.",
            "graph_type": "Text-attributed graphs (used with ND and HO template sequences from ogbn-Arxiv, ogbn-Products, Pubmed, Cora)",
            "conversion_method": "After templates produce node-embedding sequences, each embedding h_i is passed through the MLP projector to produce token embedding e_i; these e_i are inserted into the LLM prompt at the positions reserved for &lt;node sequence&gt;, mixing with token embeddings of the natural-language prompt, and the LLM is trained to generate answers (QA/chat format) for node classification, link prediction or node description.",
            "downstream_task": "Node classification, link prediction, node description; multi-task training and zero-shot transfer",
            "performance_metrics": "Using the projector+templates within LLaGA yields superior metrics vs baselines: e.g., full LLaGA general models achieve higher node classification and link prediction accuracies across datasets than GNNs and GraphGPT (see Table 1 and Table 2). Specific numbers: zero-shot node classification with node-embedding+text attributes (Table 8) ARXIV+PUBMED→CORA: LLaGA-7B accuracy 59.59% vs GraphGPT-7B 44.65%; for ARXIV+PUBMED+CORA→PRODUCTS: LLaGA-7B 43.79% vs GraphGPT-7B 18.84%.",
            "comparison_to_others": "Contrasts with approaches that fine-tune full LLMs for graph tasks (which can reduce generality) or that use a pre-trained graph transformer (GraphGPT) as an intermediate encoder; LLaGA's projector is simple (MLP), trained jointly across tasks/datasets, and authors report it generalizes better than pre-trained graph transformer approaches.",
            "advantages": "Avoids expensive LLM fine-tuning (only small projector trained), enables a single versatile translator for multiple datasets/tasks, supports interpretability (node description generation), and leads to strong multi-task and zero-shot performance.",
            "disadvantages": "Alignment quality depends on the projector and template choices; limited expressivity of a small MLP might fail to fully align more complex graph encodings to token space in some settings; requires template-specific positional encodings (Laplacian U) computed per template shape.",
            "failure_cases": "No explicit numerical failure modes for the projector alone are given, but authors note that a poor template (or no template) leads to degraded performance; templates + projector are needed together for strong link-prediction results (ablation in Table 6 shows 'NONE' severely underperforms).",
            "uuid": "e8768.2",
            "source_info": {
                "paper_title": "LLaGA: Large Language and Graph Assistant",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Plain-text graph description",
            "name_full": "Natural-language/descriptive graph serialization (prior methods)",
            "brief_description": "Describing graph nodes and edges in natural language strings (e.g., textual descriptions of nodes and their relations) for feeding into LLMs; this is a prior approach discussed as having limitations such as verbosity and ambiguity.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Plain-text graph description (language serialization)",
            "representation_description": "Convert graph structure directly into natural language sentences that describe nodes and their relations (e.g., 'Node A is connected to nodes B and C; node B is about X'); then feed the resulting text into LLMs either for prompting or task fine-tuning.",
            "graph_type": "General graphs (mentioned in context of prior works on graphs-to-LLMs)",
            "conversion_method": "Hand-crafted natural-language templates that describe node neighbors and edges; no fixed structural serialization—relies on verbose natural language to capture relationships.",
            "downstream_task": "Prior works attempted node classification and other graph tasks using LLMs prompted with such descriptions; the paper contrasts these with LLaGA.",
            "performance_metrics": "Paper cites that plain textual descriptions perform poorly on basic graph tasks without specific adaptations (cites Chen et al., 2023a and others) but does not provide direct numerical metrics in this manuscript for plain-text-only approaches.",
            "comparison_to_others": "Authors contrast plain-text descriptions unfavorably with their template-based sequence conversions: plain texts are verbose, repetitive, and fail to directly encode structural properties, leading to worse LLM performance; Instruct-GLM (Ye et al., 2023) that fine-tunes on text descriptions can help but sacrifices versatility.",
            "advantages": "Intuitive and human-readable; can be directly used with off-the-shelf LLM prompting without additional projectors or structure-aware encodings.",
            "disadvantages": "Verbose, ambiguous, repetitive, often fails to capture structural characteristics succinctly, and leads to poor LLM performance on graph tasks unless heavily fine-tuned (loss of generality).",
            "failure_cases": "Reported generally as producing poor results on basic graph tasks; described as leading to repetitive and unintuitive descriptions of nodes and edges and consequently poor LLM performance without specific adaptations.",
            "uuid": "e8768.3",
            "source_info": {
                "paper_title": "LLaGA: Large Language and Graph Assistant",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GraphGPT approach",
            "name_full": "GraphGPT (concurrent work: text descriptions + pre-trained graph transformer)",
            "brief_description": "A concurrent approach that combines textual node descriptions with a pretrained graph transformer to encode structure for input to LLMs; used as a comparison baseline in the LLaGA paper and reported to underperform LLaGA on the evaluated benchmarks.",
            "citation_title": "Talk like a graph: Encoding graphs for large language models.",
            "mention_or_use": "mention",
            "representation_name": "Text + Pretrained Graph Transformer encoding",
            "representation_description": "Uses a text-encoding model to obtain node textual features and a pretrained graph transformer to encode structural information; the combination is provided to an LLM (or used to prompt it) for graph tasks.",
            "graph_type": "Text-attributed graphs (same domains as this paper: citation and product co-purchase graphs)",
            "conversion_method": "Text-encode node attributes and separately transform graph structure through a pretrained graph transformer, then feed combined representation (or textual prompts derived from them) to an LLM for downstream tasks.",
            "downstream_task": "Node classification and link prediction (GraphGPT trained on a smaller set of tasks/datasets than LLaGA but used as a general baseline)",
            "performance_metrics": "Reported (from GraphGPT paper as cited): GraphGPT-MIX-7B ARXIV ~64.76% and GraphGPT-STD-7B ARXIV ~63.90% (Table 2 in LLaGA paper references GraphGPT results). LLaGA general models (ND and HO) report ARXIV ~74.29% and ~75.01% respectively (Table 2), indicating substantial improvement.",
            "comparison_to_others": "LLaGA outperforms GraphGPT in the reported comparisons: GraphGPT is trained on fewer tasks/datasets and uses a pretrained graph transformer which the authors of LLaGA argue may not distill task-relevant structural information as effectively as LLaGA's template+projector alignment.",
            "advantages": "Leverages a pretrained graph transformer to capture structural patterns and combines it with textual attributes — conceptually straightforward and uses learned graph encoders.",
            "disadvantages": "May require a specific pre-trained graph encoder which might not capture all task-relevant structure and can be less general across datasets/tasks; reported by the authors to give less satisfactory performance than LLaGA on the evaluated benchmarks.",
            "failure_cases": "Authors claim GraphGPT's pre-trained graph transformer might fail to distill structural information needed for specific downstream tasks leading to degraded performance (no detailed failure-case instances beyond comparative performance are given).",
            "uuid": "e8768.4",
            "source_info": {
                "paper_title": "LLaGA: Large Language and Graph Assistant",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Instruct-GLM textual fine-tuning",
            "name_full": "Instruct-GLM (textual graph descriptions + LLM fine-tuning)",
            "brief_description": "An approach that describes graphs in language and fine-tunes LLMs on graph tasks using these textual descriptions; noted by LLaGA authors as improving LLM graph-task performance but reducing model versatility.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "LLM fine-tuning on natural-language graph descriptions",
            "representation_description": "Convert graph structure into natural-language descriptions and then fine-tune an LLM on those textual prompts/responses for graph tasks, effectively specializing the LLM to the graph domain.",
            "graph_type": "Text-attributed graphs (contextual mention)",
            "conversion_method": "Handcrafted or templated natural-language descriptions of nodes and local graph context used as LLM training data; LLM parameters are fine-tuned for graph tasks.",
            "downstream_task": "Node classification and other graph tasks via specialized fine-tuning",
            "performance_metrics": "No numerical metrics provided in this paper for Instruct-GLM; described qualitatively as improving graph-task performance but at the cost of versatility.",
            "comparison_to_others": "Compared with LLaGA: Instruct-GLM's fine-tuning improves performance on specific graph tasks but the authors claim this constrains LLM versatility and may limit effectiveness on other tasks/domains; LLaGA's projector-only training keeps the LLM general-purpose.",
            "advantages": "Can improve LLM performance on the specific graph tasks it is fine-tuned for.",
            "disadvantages": "Specialization limits versatility and may harm performance on non-graph tasks or other graph tasks without re-tuning.",
            "failure_cases": "Reported limitation is decreased versatility/generalizability when LLMs are specialized via fine-tuning on graph descriptions.",
            "uuid": "e8768.5",
            "source_info": {
                "paper_title": "LLaGA: Large Language and Graph Assistant",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Talk like a graph: Encoding graphs for large language models",
            "rating": 2,
            "sanitized_title": "talk_like_a_graph_encoding_graphs_for_large_language_models"
        },
        {
            "paper_title": "GraphGPT",
            "rating": 2
        },
        {
            "paper_title": "Exploring the potential of large language models (llms) in learning on graphs",
            "rating": 1,
            "sanitized_title": "exploring_the_potential_of_large_language_models_llms_in_learning_on_graphs"
        },
        {
            "paper_title": "Natural language is all a graph needs",
            "rating": 1,
            "sanitized_title": "natural_language_is_all_a_graph_needs"
        },
        {
            "paper_title": "A frustratingly simple approach improves textual graph learning",
            "rating": 1,
            "sanitized_title": "a_frustratingly_simple_approach_improves_textual_graph_learning"
        }
    ],
    "cost": 0.014777,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLaGA: Large Language and Graph Assistant
11 Apr 2024</p>
<p>Runjin Chen 
Tong Zhao 
Ajay Jaiswal 
Neil Shah 
Zhangyang Wang 
LLaGA: Large Language and Graph Assistant
11 Apr 2024BC6FB28057380B1465F6B72D35557E05arXiv:2402.08170v3[cs.LG]
Graph Neural Networks (GNNs) have empowered the advance in graph-structured data analysis.Recently, the rise of Large Language Models (LLMs) like GPT-4 has heralded a new era in deep learning.However, their application to graph data poses distinct challenges due to the inherent difficulty of translating graph structures to language.To this end, we introduce the Large Language and Graph Assistant (LLaGA), an innovative model that effectively integrates LLM capabilities to handle the complexities of graph-structured data.LLaGA retains the general-purpose nature of LLMs while adapting graph data into a format compatible with LLM input.LLaGA achieves this by reorganizing graph nodes to structureaware sequences and then mapping these into the token embedding space through a versatile projector.LLaGA excels in versatility, generalizability and interpretability, allowing it to perform consistently well across different datasets and tasks, extend its ability to unseen datasets or tasks, and provide explanations for graphs.Our extensive experiments across popular graph benchmarks show that LLaGA delivers outstanding performance across four datasets and three tasks using one single model, surpassing state-ofthe-art graph models in both supervised and zeroshot scenarios.Our code is available at https: //github.com/VITA-Group/LLaGA</p>
<p>Introduction</p>
<p>Graphs are omnipresent, representing a myriad of real-world data from social networks, biological networks and recommendation systems, etc. Graph neural networks (GNNs) (Kipf &amp; Welling, 2017;Defferrard et al., 2016;Veličković et al., 2017), embedded with message passing and aggregation techniques, are powerful algorithmic tools on handling 1 The University of Texas at Austin 2 Snap Inc. Correspondence to: Zhangyang Wang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#97;&#116;&#108;&#97;&#115;&#119;&#97;&#110;&#103;&#64;&#117;&#116;&#101;&#120;&#97;&#115;&#46;&#101;&#100;&#117;">&#97;&#116;&#108;&#97;&#115;&#119;&#97;&#110;&#103;&#64;&#117;&#116;&#101;&#120;&#97;&#115;&#46;&#101;&#100;&#117;</a>,Runjin Chen <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#99;&#104;&#101;&#110;&#114;&#117;&#110;&#106;&#105;&#110;&#64;&#117;&#116;&#101;&#120;&#97;&#115;&#46;&#101;&#100;&#117;">&#99;&#104;&#101;&#110;&#114;&#117;&#110;&#106;&#105;&#110;&#64;&#117;&#116;&#101;&#120;&#97;&#115;&#46;&#101;&#100;&#117;</a>.</p>
<p>Preprint.</p>
<p>complex graph structures.Nonetheless, a critical limitation of GNNs is their weak multi-task handling capability.Typically trained on a single task, GNNs struggle to maintain performance when applied to multiple tasks.Self-supervised learning (Jin et al., 2021;Ju et al., 2023) may offer some improvement, but they still require task-specific heads or tuning for downstream tasks.</p>
<p>Recently, the advent of LLMs having massive context-aware knowledge and semantic comprehension capabilities (e.g., LLaMa (Touvron et al., 2023), GPTs (Achiam et al., 2023), Claude (Perez et al., 2022)) marks a significant advancement in AI research.A key advantage of LLMs is their ability to solve various tasks with a single model, showcasing strong language skills and the capacity to explain provided answers.These models have demonstrated remarkable proficiency not only in language-related tasks but also in understanding and generating visual content (Liu et al., 2023;Wang et al., 2023).However, direct application of such models presents challenges when it comes to graph-structured data, which inherently contains rich relational and structural information.Hence, researchers (Fatemi et al., 2023;Chen et al., 2023a) explored ways to translate graph structures into natural language suitable for consumption by language models.Yet, describing graphs in plain texts tends to be verbose and fails to directly represent the intrinsic characteristics of graphs, often leading to repetitive and unintuitive descriptions of nodes and edge relationships.Consequently, LLMs would perform poorly on basic graph tasks without specific adaptations (Chen et al., 2023a).Subsequently, Instruct-GLM (Ye et al., 2023) describes graphs in language and attempts to enhance LLMs' graph-task performance by taskspecific fine-tuning.However, this specialization constrains the model's versatility, potentially limiting its effectiveness in other graph tasks or non-graph-related domains.More recently, GraphGPT (Tang et al., 2023) has combined text descriptions with a self-supervised graph transformer to incorporate graph data into large language models (LLMs).However, the pre-trained graph transformer might not distill all relevant structural information for specific downstream tasks, leading to less satisfactory performances.Motivated by these issues, this work poses an important question: How to develop a framework that effectively encodes structural information for graphs across various tasks and domains, enabling its comprehension by LLMs, while maintaining LLMs' general-purpose ?</p>
<p>To this end, we introduce the Large Language and Graph Assistant (LLaGA), a novel framework that seamlessly integrates rich graph-structured data with the massive contextawareness skills and comprehension capabilities of Large Language Models.LLaGA has three impressive characteristics that distinguish LLaGA with prior works as follows:</p>
<p>• Versatility: LLaGA adopts a simple but universally applicable method for encoding structural details in graphs, and achieves a general alignment between graph and token spaces using a single, versatile projector.This projector efficiently handles various graph tasks across multiple datasets, eliminating the need for task-specific adjustments.Significantly, the performance of our versatile LLaGA framework can even exceed that of specialized task-focused graph models.• Generalizability: Given the comprehensive alignment between graph and token spaces, LLaGA not only excels in those datasets and tasks encountered during training but also demonstrates robust generalization to previously unseen datasets and tasks without additional tuning.• Interpretability: A key feature of LLaGA is its ability to provide detailed interpretations of node embeddings, greatly enhancing the understanding of its decisionmaking processes.</p>
<p>To achieve this, LLaGA uniquely reorganizes graph data into node sequences, without converting structural information into potentially ambiguous natural language descriptions.These sequences are formatted with the help of novel node-level templates, to reflect the structural information surrounding each central node while preserving the graph's node features.Note that this transformation is parameter-free, ensuring the preservation of the original structural integrity without necessitating further distillation.Subsequently, LLaGA translates node representations into LLMs' comprehensible token embedding space through a versatile projector, which can help in mitigating the expensive computational cost of fine-tuning LLMs as well as keeping LLMs' general purpose.The projector is generally trained on multiple graph datasets across various tasks, such as node classification, link prediction, and node description.This ensures it can interpret graph data from diverse perspectives and ingest an inherent ability to handle multiple tasks (all at once), bolstering its practical utility, and potentially augmenting LLaGA's generalization capabilities across various unseen datasets and tasks.Notably, unlike traditional multi-task learning methodologies used in GNNs, LLaGA trained all tasks in a uniform Question-Answer format, eschewing the need for task-specific loss functions or heads.</p>
<p>Our extensive experiments illustrate that LLaGA achieves a robust alignment between the graphs and token space of LLMs, facilitating the model's application to multiple tasks, unseen test set, and interestingly out-of-distribution datasets.</p>
<p>To our best knowledge, LLaGA is the first single model to preform consistently well across various graph datasets and tasks.It matches the effectiveness of specialized GNNs tailored for specific data and tasks, while also showing strong generalizability to unseen datasets or tasks.</p>
<p>Methodology</p>
<p>In this section, we introduce the details of LLaGA framework.We start with the notation setup, followed by a detailed explanation of the method employed for translating graphs into token embedding space.Subsequently, we delve into the training process, encompassing both the design of prompts and tasks as well as the training objectives.</p>
<p>Notation</p>
<p>A graph is a structure that encapsulates a set of entities and the interrelationships among them.Formally, a graph is denoted as G = (V, E, X ).Here, V denotes the set of nodes (entities).The set of edges, E, represents the connections between the nodes in V. X is the attribute information corresponding to the nodes.Each node v i ∈ V is associated with an attribute feature x i ∈ X .In this paper, our primary focus is on text-attributed graphs, implying that the attributes x i ∈ X of each node are expressed in a textual format.Additionally, we introduce N k v to denote the k th hop neighborhood set surrounding the node v.</p>
<p>Structure-Aware Graph Translation</p>
<p>The primary objective of LLaGA (Large Language and Graph Assistant) is to translate graph inputs into a token embedding space that is comprehensible to Large Language Models.This translation enables the utilization of LLMs' inherent reasoning capabilities for graph-related tasks, without necessitating any modifications to the LLM parameters.LLaGA accomplishes this by initially reorganizing nodes in graphs into node embedding sequences.These sequences are structured according to our proposed templates and are then converted into a sequence of token embeddings using a projector.</p>
<p>The first step involves converting graphs into node embedding sequences.Recognizing that the fundamental unit for graph analysis is the node, we developed two node-level templates for analysis on graphs.These templates are versatile, applicable not only to node-level tasks but also to other tasks like link prediction.Both templates are designed to encode structural information surrounding a node, offering different perspectives for analysis.The first, the Neighborhood Detail Template, provides an in-depth view of the central node and its immediate surroundings.The second, the Hop-Field Overview Template, offers a summarized view of a node's neighborhood, extendable to larger fields.</p>
<p>Neighborhood Detail Template is designed to elaborate on the detailed information of a node and its surrounding neighborhood.Given a node v, we first construct a fixedshape, sampled computational tree centered around v. For every hop of neighbors, we define a neighbor sample size, denoted as n 1 , n 2 , ..., where n i indicates the sample size for the i th hop.The computational tree is built with the root node being the central node v. From the 1-hop neighbor set of v, denoted as N 1 v , we randomly select n 1 nodes to form a new neighbor set N 1 v .If the size of N 1 v is smaller than n 1 , i.e., |N 1 v | &lt; n 1 , we supplement the set with placeholder nodes to reach a size of n 1 .Therefore, the size of N 1 v is consistently n 1 , i.e., | N 1 v | = n 1 .The nodes in N 1 v are treated as children of the root node.Subsequently, for each node in N 1 v , we recursively sample n 2 neighbors as its children.Any sets with insufficient nodes are filled with placeholder nodes.For any placeholder node, its children are exclusively placeholder nodes.As illustrated in upper-left of Figure 1, with the root node being A, we display a 2-hop neighbor structure of A, with the sample size of 3 for both hops.The first-order neighbors of A are {B, C, D}, so they are shown in the second layer of the computational graph.Since B has 2 neighbors {A, G}, we expand this set to {A, G, [pad]}, where [pad] represents the placeholder node.And similarly for nodes C and D. Ultimately, this process yields a perfect 3-ary computational tree centered around node A. We then perform a level-order traversal on the computational tree, transforming the comprehensive details of the central node and its neighborhood into a fixed-length node sequence.For instance, in Figure 1, the sequence representing node A and its neighborhood is
A B C D A G [pad] A [pad] [pad] A E F
, where each sequence position uniquely corresponds to a relative structural position within the original graph.</p>
<p>Post conversion of the center node and its structural information into a node sequence, we shift to mapping them into the node embedding space.In the context of text-attributed graphs, we can utilize various off-the-shelf text encoding models ϕ, such as SBERT (Reimers &amp; Gurevych, 2019), RoBERTa (Liu et al., 2019), and SimTeG (Duan et al., 2023), to encode text features.Placeholder nodes are represented by a zero vectors of the same size.We further integrate a Laplacian Embedding (Dwivedi &amp; Bresson, 2020) at each sequence position, enhancing the representation of structural information.Denoting the adjacency matrix of the computational tree by A tree , the Laplacian Embedding is defined as the eigenvectors of the Laplacian matrix of A tree :
L = I − D − 1 2 A tree D − 1 2 = U T ΛU (1)
where D represents the degree matrix of A tree and U symbolizes the Laplacian Embedding of the template.Notably, with a fixed sample size, the computational tree's shape remains unchanged, so the Laplacian Embedding is computed only once for all graphs using this template.This Embedding is then appended to the encoded node feature to form the final node embedding.The process is outlined as follows: Let v 1 , v 2 , ..., v n represent the encoded node sequence.The final node embedding h vi for v i is given by
h vi = 0 || U i , if v i = [pad]; ϕ(x vi ) || U i , otherwise,(2)
where || denotes concatenation.Subsequently, the central node and its structural information are transformed into the node embedding sequence h v1 , h v2 , ..., h vn .</p>
<p>Hop-Field Overview Template provides a summarized view of the central node and its neighborhood.This template employs hop embeddings to characterize the node features across various neighborhood hops.These hop embeddings are obtained through parameter-free message passing on encoded text features.For each central node v, the i th -hop embedding h i v is calculated as follows:
h i v = 1 |N 1 v | v ′ ∈N 1 v h i−1 v ′ ,(3)
where h 0 x = ϕ(x v ).Through this calculation, h i v potentially contains information from all neighbors in the i th -hop neighborhood set
N i v . A sequence of hop embeddings h 0 v , h 1 v , h 2 v , . .
. can represent the central node and its structural information.Unlike the Neighborhood Detail Template, which utilizes individual embeddings for each neighbor, the Hop-Field Overview Template summarizes each hop's neighbors with a single embedding.This approach may sacrifice some detail for the sake of a broader respective field.The choice between these templates should be based on the nature of the input data and the required level of detail.</p>
<p>To enhance the natural comprehension of graph inputs by Large Language Models (LLMs), it is essential to align the node embedding space with the input token space.This alignment is realized by mapping each node embedding into the token embedding space, utilizing a specifically calibrated projector, denoted as f θ .Mathematically, this process can be represented for a given node embedding h i as:
e i = f θ (h i ).(4)
Consequently, a sequence of node embeddings, h 1 , h 2 , ..., h n , is transformed into a corresponding sequence of token embeddings, e 1 , e 2 , ..., e n .In our framework, this transformation is facilitated by a simple MLP serving as the projector.It is important to note that the parameters θ of the projector are the only parameters subject to tuning during the training process of LLaGA.</p>
<p>Alignment Tuning</p>
<p>In LLaGA, we employ three key tasks on graphs -node classification, link prediction, and node description -to meticulously tune the projector.The first two tasks, node classification and link prediction, are well-established and widely recognized in the field of graph ML.Contrastingly, the node description task, which is somewhat less common in conventional graph analysis, is designed to align node embeddings with specific descriptive texts.This innovative task enables the provision of rich semantic interpretations of the graphs, providing a deeper insight of the logic behind graphbased predictions.The questions and answers to this task can be articulated as follows: Questions: Please describe the center node: <node sequence>.Answers: The center node represents a [paper / products /...], it's about [node description].For textual-attributed graphs, the node description can be obtained from node features.By integrating these three diverse tasks into the training process, our projector develops a comprehensive and nuanced understanding of graphs and can serve as a versatile translator between node embedding and token embedding space for all those tasks.Moreover, it can explicitly generate explanations for node embeddings, enhancing interpretability.</p>
<p>During training, we organize our questions and answers in a chat format.In our experiments, Vicuna (Chiang et al., 2023) serves as the primary foundational Large Language Model (LLM), so we follow the implementation strategy of Vicuna and set the system message accordingly.For details regarding the question-answer template and the training or inference input sequences, please refer to the illustrations in Figure 1.In the input processing phase, we tokenize all words in the prompt and convert them into their respective token embeddings.For the <node sequence>, we substitute this part with the projected node embeddings e 1 , e 2 , ..., e n , maintaining their original positions.The training objective is to maximize the probability of generating the correct answer, formulated as maximize θ</p>
<p>p(X answer |X graph , X question , X system ).(5)</p>
<p>Experimental Results</p>
<p>We conduct comprehensive experiments to validate the effectiveness of our framework across various settings, aiming to address several key research questions:</p>
<p>• RQ1: How does LLaGA perform in comparison to baseline models in standard graph tasks, such as node classification and link prediction?</p>
<p>• RQ2: How good are the interpretations generated by LLaGA for node embeddings?</p>
<p>• RQ3: How effectively does the model transfer knowledge when adapting to new datasets or tasks in zero-shot?</p>
<p>• RQ4: What is the contribution of our encoding templates to the overall performance?</p>
<p>Setup</p>
<p>Datasets.We train and evaluate our model on four widelyrecognized graph datasets: ogbn-Arxiv (Hu et al., 2020), ogbn-Products (Hu et al., 2020), Pubmed, and Cora (Yang et al., 2016).These datasets span domains of citation networks and e-commerce, varying in terms of sparsity and size, ranging from small to large scales.Detailed statistics and data splitting methods are presented in Appendix A.</p>
<p>Tasks.Baselines.In our comparative analysis, we benchmark our framework against three categories of state-of-the-art models to ensure a thorough evaluation.The first category comprises Graph Neural Networks, including GCN (Kipf &amp; Welling, 2016), GraphSage (Hamilton et al., 2017), GAT (Veličković et al., 2018), SGC (Wu et al., 2019), and SAGN (Sun et al., 2021).The second category encompasses transformer-based graph models, NodeFormer (Wu et al., 2022).The final category is represented by GPT-3.5, a leading general LLM.For the first two categories, identical text-encoding methods are employed to encode text features, ensuring a fair comparison.For GPT-3.5, we utilized node classification results from the survey by Chen et al. (Chen et al., 2023a) and extended this approach to the link prediction task by employing a consistent graph-description prompt format.In addition, we also compare with the concurrent work, GraphGPT (Tang et al., 2023).</p>
<p>Overall Performance Comparison (RQ1)</p>
<p>We compare our LLaGA model with various baselines across four distinct settings: Single Focus, Task Expert, Classification Expert, and General Model.The Single Focus setting involves models trained on a single dataset for a specific task, thereby concentrating exclusively on that task.Task Expert refers to models trained across all datasets but focused on a single task, enabling them to perform as specialists in that area.In the Classification Expert setting, models are trained on all datasets for both node classification and link prediction tasks.The General Model is trained for node classification, link prediction, and node description across all datasets, equipping the model to handle not just classification tasks but also semantic tasks like node description.The comparative results are presented in Table 1.Notably, when implementing the GNN-based or Transformer-based baselines in the Task Expert or Classification Expert settings, they were trained using a multi-task learning approach, which incorporates a shared backbone with task-specific classification heads for different datasets or tasks.In contrast, our LLaGA framework employs a single projector to handle all tasks.</p>
<p>Comparision with Baselines: Our analysis reveals three key observations.Observation 1: LLaGA framework demonstrates superior performance compared to baseline models across all settings, particularly in multi-task learning scenarios.This highlights LLaGA's versatility and robust capability in addressing various graph tasks.</p>
<p>Observation 2: While many baseline models experience significant performance degradation in multi-task learning scenarios, LLaGA stands out by exhibiting minimal decline or even improvements in performance.This reflects LLaGA's proficiency in extracting common patterns across different datasets and tasks.Such a trait hints at the potential for developing a powerful multi-model LLM equipped with simple projectors.Observation 3: Both the Neighborhood Detail Template and the Hop-Field Overview Template exhibit distinct advantages.The Neighborhood Detail Template excels in tasks requiring detailed neighbor information, whereas the Hop-Field Overview Template is more effective in tasks that depend on a broader overview of neighbor information with a larger receptive field.For instance, in identifying product categories, it is illogical to classify a product as 'Video Games' based solely on many of its neighbors being 'Electronics'.A more detailed analysis, revealing numerous 'Nintendo Switch' neighbors, makes classification more accurate, as seen in the case of the ogbn-Products dataset.Conversely, for some citation graphs, an overview of a paper's neighboring categories can be more informative, making the Hop-Field Overview Template the preferable choice.</p>
<p>Comparison with Concurrent Work: We conduct</p>
<p>Interpretation Ability Investigation (RQ2)</p>
<p>As previously stated, our LLaGA framework excels in providing comprehensive interpretations of node embeddings.We initially assess LLaGA's performance in the node description task using several quantitative metrics, with results presented in Table 4.The Sbert Score indicates the semantic similarity between the ground truth and LLaGA-generated text, measured using Sbert.We also include a Base value for your reference, representing the average similarity across two randomly chosen samples.Notably, LLaGA's Sbert score significantly exceeds this base value, demonstrating its effectiveness in generating meaningful and relevant descrip- tions for node embeddings.Furthermore, the high accuracy in extracting labels from these descriptions corroborates the precision of the generated content.</p>
<p>To further illustrate this, Table 3 showcases sample descriptions.These examples indicate the high quality of text produced by LLaGA.Even in some instances where LLaGA's label predictions diverge from the ground truth, its results are found to be reasonable and LLaGA effectively utilizes its generated text to substantiate these plausible interpretations.</p>
<p>Zero-Shot Ability Investigation (RQ3)</p>
<p>In this section, we illustrate the generalization capabilities of LLaGA, concentrating on the task of link prediction within a Zero-shot learning entails training a model on certain datasets and subsequently evaluating it on unseen datasets or tasks.This approach is instrumental in assessing a model's proficiency in transferring knowledge.In our study, we examine LLaGA's zero-shot performance in both in-domain and out-of-domain transfer scenarios.For in-domain transfer, the model is trained on the Arxiv and Pubmed datasets and evaluated on the Cora dataset.All three datasets comprise citation graphs.Conversely, for out-of-domain transfer, training is conducted on the Arxiv, Pubmed, and Cora datasets, with the evaluation on the Products dataset.Here, while the training datasets are citation graphs, the test set consists of e-commerce graphs.The results, as presented in Table 5, reveal that our model exhibits robust zero-shot capabilities in both scenarios.This indicates that LLaGA can effectively discern and leverage similar patterns across datasets, adeptly transferring knowledge not only to analogous data but also to datasets that markedly differ in domain.</p>
<p>Templates Ablation Study (RQ4)</p>
<p>We conduct an ablation study to investigate the individual contributions of our encoding templates.For this, we train a new model in a classification expert setting, but without using a template.This model solely relies on the embedding of the center node for prediction, rather than a node embedding sequence that encapsulates structural information surrounding the center node.The results are presented in Table 6.It is evident that both the Neighborhood Detail Template and the Hop-Field Overview Template significantly enhance performance compared to the model without a template.This is particularly noticeable in the link prediction task, a task that heavily relies on structural information.All these findings underscore the effectiveness of our templates in encoding the structural information of nodes.</p>
<p>Related Work</p>
<p>Graph Neural Networks</p>
<p>GNNs have long been at the forefront of graph machine learning.They are designed to transform input nodes into compact vector representations, suitable for subsequent classification tasks when paired with a classification head.A common strategy among many GNNs (Kipf &amp; Welling, 2016;Veličković et al., 2018;Xu et al., 2018;Gao et al., 2018;Chiang et al., 2019;You et al., 2020;Chen et al., 2018;Thekumparampil et al., 2018), involves a layer-wise message-passing mechanism.This approach enables nodes to progressively aggregate and process information from their immediate neighbors, thereby embedding the nodes into lower-dimensional spaces.Concurrently, a growing body of research (Yun et al., 2019;Ying et al., 2021;Wu et al., 2022;Chen et al., 2022), has been exploring the integration of transformer-based encoders for graph data analysis, opening new avenues for enhancing GNN capabilities.However, a significant limitation of traditional graph models is their poor task generalization capability.GNNs are usually trained on a single classification task.When applied to a variety of datasets or downstream tasks, these models often fail to perform consistently well across all tasks with one single model (Ju et al., 2023).</p>
<p>Self-Supervised Learning for GNNs</p>
<p>Recent advancements have employed self-supervised learning strategies on GNNs to bolster their generalization performance.These methods encompass developing specialized pretext tasks for graph structures, such as mutual information maximization (Veličković et al., 2019;Hassani &amp; Khasahmadi, 2020), whitening decorrelation (Zhang et al., 2021), and generative reconstruction (Hou et al., 2022).Moreover, investigations into integrating multi-task learning with self-supervised learning paradigms have been conducted, offering novel insights into enhancing model generalization ability (Ju et al., 2023).However, these methods still require task-specific classification heads and tuning for every downstream task, after obtaining a general embedding from the graph encoder.</p>
<p>Large Language Models for Graphs</p>
<p>Recent studies have explored integrating Large Language Models (LLMs) with GNNs, leveraging LLMs' extensive knowledge for graph data enhancement.Research has focused on augmenting GNNs with LLMs to enrich graph textual attributes (Ye et al., 2023;Chen et al., 2023b;Tang et al., 2023;Guo et al., 2023;He et al., 2023;Huang et al., 2023), though these approaches largely depend on GNNs for predictions, potentially limiting their scope.Alternatively, efforts to linguistically represent graphs for direct LLM processing encountered difficulties in effectively translating structures into natural language, often yielding suboptimal results (Huang et al., 2023;Guo et al., 2023).While fine-tuning LLMs for graphs can improve performance on specific tasks, it may also limit the LLMs' versatility.GraphGPT (Tang et al., 2023) sought to address these challenges by using a pretrained graph transformer for encoding graph structures for LLMs, though finding a universally applicable graph model proved difficult.Our contribution diverges by introducing a novel encoding method that translates graph data into sequences directly compatible with LLMs, avoiding the need for intermediary models.This method shows superior versatility and generalizability across a range of tasks, even in zero-shot scenarios, outperforming traditional graph models.</p>
<p>Conclusion</p>
<p>This paper introduces LLaGA, an innovative framework that effectively integrates Large Language Models (LLMs) into the graph domain while preserving their proficiency in other tasks.Instead of using complex language for describing structure information, LLaGA employs templates to transform graph structure into sequences, and then maps node embeddings to token embedding spaces using a tuned projector.This projector establishes a comprehensive alignment between texts and graphs, enabling the use of LLMs for fundamental graph tasks like node classification and link prediction across various datasets.And it can be further generalized to unseen datasets or tasks without any adaption.Additionally, it facilitates the generation of textual explanations for node embeddings.Through extensive evaluations in different settings, our method has demonstrated its effectiveness in both supervised and zero-shot graph learning scenarios.</p>
<p>Impact Statements</p>
<p>Our research introduces LLaGA, a novel framework that seamlessly blends the capabilities of Large Language Models (LLMs) with graph structures, enhancing the versatility of LLMs to perform fundamental graph tasks.The broader impact of LLaGA extends to numerous fields where graph data is pivotal, including but not limited to, bioinformatics, social network analysis, and knowledge graphs.As we push the boundaries of Machine Learning and AI, we recognize the importance of monitoring for unintended consequences, such as the perpetuation of biases or misuse of predictive insights.this end, we encourage continued ethical evaluation and the development of guidelines to ensure that the applications of LLaGA contribute constructively to society.This work aspires to be a stepping stone towards more sophisticated, equitable, and transparent AI systems that respect the intricate structure of data across various domains.</p>
<p>A. Dataset Statistics  To explore the generalization capabilities of LLaGA, we also employed zero-shot learning for node classification tasks.Unlike link prediction tasks, applying zero-shot learning to node classification presents greater challenges due to the distinct label sets and the varied knowledge requirements across tasks.However, a universal aspect potentially transferable across all node classification tasks is the alignment between the graph and the semantic token space.To this end, we trained models on node description tasks from certain datasets to establish a generalized alignment between the graph structure and the token space, subsequently testing this alignment on node classification tasks using different datasets.Furthermore, we assessed LLaGA's zero-shot performance in both in-domain and out-of-domain transfer scenarios.In the in-domain scenario, training was performed on citation graphs (Arxiv + Pubmed), with testing conducted also on citation graphs (Cora).However, the out-of-domain scenario involved training on citation graphs (Arxiv + Pubmed + Cora), with testing on the e-commerce graphs (Products).Since traditional GNNs depend on task-specific classification heads and new classification tasks may vary in label sets, they are unable to conduct zero-shot learning on node classification tasks.Our comparison was limited to llm-based baselines, specifically GraphGPT.</p>
<p>Our evaluation encompasses two kinds of prompts.In the first prompt, the model is only supplied with node embedding sequences, containing both attribute and structural information of the central node.The second prompt enhances this by also incorporating the textual attributes of the central node to assist the model.As detailed in Table 8, our findings reveal that LLaGA consistently outperforms GraphGPT across all settings.This superiority is attributed to LLaGA's comprehensive alignment between the graph space and the token space.Moreover, the inclusion of the central node's textual attributes appears to offer some advantages in zero-shot scenarios.However, prompts based solely on node sequence embeddings show potential for application to graphs whose node attributes are challenging to describe textually, such as non-textual graphs.LLaGA demonstrates flexibility in its text encoding methods for node attributes.In our initial experiments, we employed SimTeG (Duan et al., 2023) as the primary encoding model.This section also explores the use of SBERT (Reimers &amp; Gurevych, 2019) and RoBERTa (Liu et al., 2019) as alternative encoding methods.The outcomes of these trials are shown in Table 9.All models, including baselines, underwent training in a classification expert setting.For LLaGA, we utilized the Hop-Field Overview Template for structure encoding.Notably, LLaGA consistently surpassed other leading GNNs in performance, regardless of the chosen encoding model.LLaGA also demonstrates flexibility with various Base Large Language Models (LLMs).In our primary experiments, Vicuna-7B served as the foundational model.This section details the substitution of LLaGA's base LLM with alternative models, including LLaMA2-7B and OPT-2.7B.The outcomes of these replacements are presented in Table 10.For structural encoding, we employ the Hop-Field Overview Template.And models are trained in classification setting.It is evident that LLaGA consistently yields favorable results irrespective of the base LLM, showcasing its effectiveness even with comparatively lighter models such as OPT-2.7B.We perform training and inference five times on relatively small datasets, with the variance information detailed in Table 11.</p>
<p>C. Flexibility with Text Encoding Methods</p>
<p>D. Integration with Various LLMs</p>
<p>E. Experiment Variance</p>
<p>Figure 1 .
1
Figure 1.Illustration of LLaGA framework and its prompt design paradigm.</p>
<p>(Duan et al., 2023)3)n task, we predict the existence of edges between node pairs.The node description task involves generating node descriptions based on encoded node embeddings.The training ground truth is derived from classification labels and text features, structured as: The center node represents a paper/product in the [label] domain, it's about [text feature].Evaluation Metrics.For evaluation metrics, we employ Accuracy for both node classification and link prediction tasks, Sbert score and Description Label Accuracy for the node description task.The Sbert score measures the similarity between embeddings of the generated descriptions and the ground truth descriptions encoded by Sbert.Description Label Accuracy represents the Accuracy of labels inferred from node descriptions.For LLaGA framework, a sample is considered accurate only if it precisely identifies the category's full name in its response.Implementation Details.In our model's implementation, we primarily employ Vicuna-7B-v1.5-16K(Chiangetal., 2023)as the foundational base models, and SimTeg(Duan et al., 2023)as default text-encoding model.Additionally, we conduct a comparative analysis of various base LLMs and embeddings in Appendix C and D. The learning rate is consistently set to 2e-5, and the batch size is maintained at 16 for all models.We train our model for one epoch.However, to compensate for the limited data size, we replicate the training samples from the smallest dataset, Cora, three times.For the Neighborhood Detail Template, we sample two-hop neighbors around each node, setting the sample size to 10 for each hop.In the Hop-Field Overview Template, 4 hop embeddings are employed to encapsulate the structural information surrounding the central node.We denote LLaGA implementations with the Neighborhood Detail Template and Hop-Field Overview Template as LLaGA-ND-7B and LLaGA-HO-7B, respectively.</p>
<p>Our model utilizes LLaGA for 3 tasks: node classification, link prediction, and graph-based node description.The targets of node classification are to categorize nodes based on research topics or product characteristics.</p>
<p>Table 1 .
1
Performance comparison with baseline models on both node classification and link prediction under 4 settings.Single Focus denotes models trained on a single task and dataset.Task Expert refers to models trained exclusively on one task across all datasets, specializing in that task.Classification Expert indicates models trained in both node classification and link prediction on all datasets, becoming proficient in classification tasks.General Model are capable of handling classification tasks across datasets and excel in semantic tasks, such as generating interpretable descriptions for node embeddings.(bold signifies the best result across all methods, while underline highlights the best baseline result under this setting)
MODEL TYPEMODELNODE CLASSIFICATION ACCURACY(%) ARXIV PRODUCTS PUBMED CORA ARXIV PRODUCTS PUBMED CORA LINK PREDICTION ACCURACY(%)GCN73.7280.7592.9688.9391.4393.9590.9181.59GRAPHSAGE76.2982.8794.8788.8991.6494.9690.6479.15GAT74.0683.0692.3388.9785.9993.8583.9680.06SINGLESGC71.7775.4787.3587.9787.9988.5183.6080.94FOCUSSAGN75.7082.5895.1789.1990.6294.8590.4879.88NODEFORMER74.8583.7294.9088.2391.8490.9377.6977.26LLAGA-ND-7B75.9884.6095.0388.8691.2497.3691.4183.79LLAGA-HO-7B76.6684.6795.0389.2294.1595.5689.1886.82GCN71.4580.8889.2581.6288.5193.5481.0178.88GRAPHSAGE72.5682.5094.1581.9987.7693.4976.1480.74TASKGAT72.1982.6187.9783.5882.5892.0376.8579.76EXPERTNODEFORMER72.3582.9994.4183.2784.1193.4280.4081.03LLAGA-ND-7B76.4184.6094.7888.1991.2097.3893.2789.41LLAGA-HO-7B76.4084.1895.0689.8594.3695.8588.8887.50GCN70.9580.0289.0082.7787.6992.8872.2878.35GRAPHSAGE71.9181.6291.8182.4489.2392.2275.3682.09CLASSIFICATIONGAT70.9081.8387.7282.0785.1892.1175.0080.35EXPERTNODEFORMER63.2075.5589.5069.1982.3375.4278.2281.47LLAGA-ND-7B75.8583.5895.0687.6490.8196.5692.3687.35LLAGA-HO-7B75.9983.3294.8089.3094.3096.0588.6488.53GENERAL MODELGPT3.5-TURBO LLAGA-ND-7B55.00 74.2975.25 82.2188.00 92.4271.75 87.8263.80 90.5360.30 96.8268.70 86.3165.74 81.91LLAGA-HO-7B75.0182.0794.4587.8292.0486.8089.8184.41</p>
<p>Table 2 .
2
Compare with Concurrent Work.
MODELARXIV PUBMED PUBMED NC NC LPGRAPHGPT-MIX-7B64.7674.1658.86GRAPHGPT-STD-7B63.90-80.26LLAGA-ND-7B(GENERAL)74.2992.4286.31LLAGA-HO-7B(GENERAL)75.0194.4589.81a comparative analysis with our concurrent work,GraphGPT (Tang et al., 2023). GraphGPT is a generaliz-able model designed for solving graph tasks using LLM. Itemploys a text-encoding model to extract node features andutilizes a pre-trained graph transformer for encoding struc-tural information. In our comparison, we focus on our mostrobust and generalizable models, with the results detailedin</p>
<p>Table 2
2
, GraphGPT's results are referenced directly from its original paper.'Mix' and 'Std' represent two categories of prompts used in GraphGPT's training process.LLaGA's most general model is trained across 12 tasks, including node classification, link prediction, and node description on datasets such as Arxiv, Products, Pubmed, and Cora.In contrast, GraphGPT's most general model is trained on just three tasks: node classification on Arxiv and Pubmed, and link prediction on Pubmed.But our model still demonstrates superior performance on these three tasks, highlighting the efficacy of our LLaGA framework.</p>
<p>Table 3 .
3
Examples Demonstrating the Interpretability of the LLaGA Framework.Title: Real time hand gesture detection and classification using convolutional neural networks.Abstract: Real-time recognition of dynamic hand gestures from video streams is a challenging task since (i) there is no indication when a gesture starts and ends in the video, (ii) performed gestures should only be recognized once, and (iii) the entire architecture should be designed considering the memory and power budget.In this work, ... Label: cs.CV(Computer Vision and Pattern Recognition) USER: Please describe the center node: <node embedding sequence>.LLaGA:This node represents a paper in cs.CV(Computer Vision and Pattern Recognition) domain, it's about learning to detect and recognize hand gestures using convolutional neural networks.Title:An efficient graph convolutional network technique for the travelling salesman problem.Abstract: This paper introduces a new learning-based approach for approximately solving the Travelling Salesman Problem on 2D Euclidean graphs.We use deep Graph Convolutional Networks to TSP graph representations and output tours in a non-autoregressive manner via highly parallelized beam search.Our approach ... Label: cs.LG(Machine Learning) USER: Please describe the center node: <node embedding sequence>.LLaGA: This node represents a paper in cs.LG(Machine Learning) domain, it's about deep graph convolutional networks for solving the traveling salesman problem.Inverse boosting pruning trees for depression detection on twitter Abstract: Depression is one of the most common mental health disorders, and a large number of depression people commit suicide each year... We argue that it is feasible to identify depression at an early stage by mining online social behaviours.Our approach, ... Label: cs.LG(Machine Learning) USER: Please describe the center node: <node embedding sequence>.LLaGA: This node represents a paper in cs.SI(Social and Information Networks) domain, it's about predicting suicide risk using social media data.(Label is different from ground truth, but also reasonable)</p>
<p>Title:</p>
<p>Table 4 .
4
Quantitative evaluation of the node description task using Sbert Score and Description Label Accuracy.The term Base value refers to the mean Sbert similarity calculated between the ground truth descriptions of two randomly selected samples.
DATASETMODELBASE VALUESBERT SCOREACCARXIVLLAGA-ND-7B LLAGA-HO-7B0.22310.6023 74.64 0.6228 75.49PRODUCTSLLAGA-ND-7B LLAGA-HO-7B0.15130.4952 83.18 0.5193 84.60PUBMEDLLAGA-ND-7B LLAGA-HO-7B0.48690.6847 92.27 0.6934 94.27CORALLAGA-ND-7B LLAGA-HO-7B0.32210.6465 86.72 0.6545 86.90</p>
<p>Table 5 .
5
Zero-Shot on Link Prediction
TRAIN → TESTMODELACCURACYGCN58.97ARXIV+PUBMEDGRAPHSAGE67.68↓GRAPHGPT-7B50.74CORALLAGA-ND-7B86.47LLAGA-HO-7B87.35GCN56.73ARXIV+PUBMED+CORAGRAPHSAGE58.92↓GRAPHGPT-7B50.74PRODUCTSLLAGA-ND-7B92.65LLAGA-HO-7B92.99zero-shot setting. For analysis of generalization capabilitiesin node classification tasks, please refer to Appendix B.</p>
<p>Table 6 .
6
Templates Ablation Study.
TASK TEMPLATE ARXIV PRODUCTS PUBMED CORANONE73.9280.4594.6084.50NCND75.8583.5895.0687.64HO75.9983.3294.8089.30NONE89.9891.7378.1983.97LPND90.8196.5692.3687.35HO94.3096.0588.6488.53</p>
<p>Table 7 .
7
(Hu et al., 2020)s Arxiv, Pubmed, Cora), each node represents a paper, where the title and abstract serve as node features, and edges denote co-citations.For ogbn-Products, nodes represent Amazon products, featuring item descriptions as node features, with edges indicating co-purchases.Data Split.For node-level tasks, we adhere to the standard train/validation/test splits(Hu et al., 2020)for each dataset: 6:2:3 for Arxiv, 8:2:90 for Products, and 6:2:2 for both Pubmed and Cora.For link prediction, we randomly select node pairs from the node-level training set for training and from the node-level test set for testing, ensuring the edge-level training sets are equal in size to the node-level training sets.
DatasetDomain#Node#EdgeSparsity(‱)Coracitation2708542914.8065Pubmedcitation19717443382.2810Arxivcitation16934311662430.8134Products e-commerce 2449029 618591400.2063In citation graphs (ogbn-
B. Zero-Shot Ability on Node Classification</p>
<p>Table 8 .
8
Zero-Shot on Node Classification
TRAIN → TESTPROMPT TYPEMODELACCURACY(%)ARXIV+PUBMED → CORAONLY NODE EMBEDDINGGRAPHGPT-7B LLAGA-7B8.30 34.69(TEST TASK: 7 CATEGORIES)NODE EMBEDDING+TEXT ATTRIBUTESGRAPHGPT-7B LLAGA-7B44.65 59.59ARXIV+PUBMED+CORA → PRODUCTSONLY NODE EMBEDDINGGRAPHGPT-7B LLAGA-7B1.40 13.89(TEST TASK: 47 CATEGORIES)NODE EMBEDDING+TEXT ATTRIBUTESGRAPHGPT-7B LLAGA-7B18.84 43.79</p>
<p>Table 9 .
9
LLaGA Trained with SBert and Roberta Embedding.
EMBEDDINGMODELNODE CLASSIFICATION ACCURACY ARXIV PRODUCTS PUBMED CORALINK PREDICTION ACCURACY ARXIV PRODUCTS PUBMED CORAGCN66.0077.4182.0479.7091.3894.9184.3183.15SBERTGRAPHSAGE66.7976.0082.7480.6688.1894.2378.3883.62LLAGA74.4680.7090.0488.5693.6896.8491.3987.79GCN66.5177.7480.0479.3091.0194.6680.9481.03ROBERTAGRAPHSAGE68.1476.7381.2782.2988.8094.1174.3182.88LLAGA74.1981.1389.7888.1993.5296.7989.9685.15</p>
<p>Table 10 .
10
Integration with Various LLMs
BASE MODELNODE CLASSIFICATION ACCURACY ARXIV PRODUCTS PUBMED CORAARXIVLINK PREDICTION ACCURACY PRODUCTS PUBMEDCORAVICUNA-7B75.9983.3294.8089.3094.3096.0588.6488.53LLAMA2-7B76.2684.2194.8386.5394.1596.0389.3985.44OPT-2.7B75.6683.0195.0188.3893.3692.8386.9289.41</p>
<p>Table 11 .
11
Variance Information on Cora and Pubmed Dataset
SETTINGDATASETMODELNC(%)LP(%)SINGLE FOCUSCORA PUBMEDLLAGA-ND-7B 88.86±0.78 83.79±1.26 LLAGA-HO-7B 89.22±0.46 86.82±0.88 LLAGA-ND-7B 95.03±0.12 91.41±0.21 LLAGA-HO-7B 95.03±0.07 89.18±0.34</p>
<p>. J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.087742023arXiv preprint</p>
<p>Fastgcn: fast learning with graph convolutional networks via importance sampling. J Chen, T Ma, Xiao , C , arXiv:1801.102472018arXiv preprint</p>
<p>Nagphormer: A tokenized graph transformer for node classification in large graphs. J Chen, K Gao, G Li, K He, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Exploring the potential of large language models (llms) in learning on graphs. Z Chen, H Mao, H Li, W Jin, H Wen, X Wei, S Wang, D Yin, W Fan, H Liu, arXiv:2307.033932023aarXiv preprint</p>
<p>Label-free node classification on graphs with large language models (llms). Z Chen, H Mao, H Wen, H Han, W Jin, H Zhang, H Liu, J Tang, arXiv:2310.046682023barXiv preprint</p>
<p>Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks. W.-L Chiang, X Liu, S Si, Y Li, S Bengio, C.-J Hsieh, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2019</p>
<p>An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, I Stoica, E P Xing, Vicuna, March 2023</p>
<p>Convolutional neural networks on graphs with fast localized spectral filtering. Advances in neural information processing systems. M Defferrard, X Bresson, P Vandergheynst, 201629</p>
<p>K Duan, Q Liu, T.-S Chua, S Yan, W T Ooi, Q Xie, J He, Simteg, arXiv:2308.02565A frustratingly simple approach improves textual graph learning. 2023arXiv preprint</p>
<p>A generalization of transformer networks to graphs. V P Dwivedi, X Bresson, arXiv:2012.096992020arXiv preprint</p>
<p>Talk like a graph: Encoding graphs for large language models. B Fatemi, J Halcrow, B Perozzi, arXiv:2310.045602023arXiv preprint</p>
<p>Large-scale learnable graph convolutional networks. H Gao, Z Wang, Ji , S , Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2018</p>
<p>Can large language models understand graph structured data? an empirical evaluation and benchmarking. J Guo, L Du, H Liu, Gpt4graph, arXiv:2305.150662023arXiv preprint</p>
<p>Inductive representation learning on large graphs. W Hamilton, Z Ying, J Leskovec, Advances in neural information processing systems. 201730</p>
<p>Contrastive multiview representation learning on graphs. K Hassani, A H Khasahmadi, International conference on machine learning. PMLR2020</p>
<p>Harnessing explanations: Llm-to-lm interpreter for enhanced text-attributed graph representation learning. X He, X Bresson, T Laurent, A Perold, Y Lecun, B Hooi, arXiv:2305.195232023arXiv preprint</p>
<p>Self-supervised masked graph autoencoders. Z Hou, X Liu, Y Cen, Y Dong, H Yang, C Wang, J Tang, Graphmae, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022</p>
<p>Open graph benchmark: Datasets for machine learning on graphs. W Hu, M Fey, M Zitnik, Y Dong, H Ren, B Liu, M Catasta, J Leskovec, Advances in neural information processing systems. 202033</p>
<p>Can llms effectively leverage graph structural information: when and why. J Huang, X Zhang, Q Mei, J Ma, arXiv:2309.165952023arXiv preprint</p>
<p>Automated self-supervised learning for graphs. W Jin, X Liu, X Zhao, Y Ma, N Shah, J Tang, arXiv:2106.054702021arXiv preprint</p>
<p>Multi-task self-supervised graph neural networks enable stronger task generalization. M Ju, T Zhao, Q Wen, W Yu, N Shah, Y Ye, C Zhang, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Semi-supervised classification with graph convolutional networks. T Kipf, M Welling, ArXiv, abs/1609.029072017</p>
<p>Semi-supervised classification with graph convolutional networks. T N Kipf, M Welling, International Conference on Learning Representations. 2016</p>
<p>Crafting papers on machine learning. P Langley, Proceedings of the 17th International Conference on Machine Learning (ICML 2000). P Langley, the 17th International Conference on Machine Learning (ICML 2000)Stanford, CAMorgan Kaufmann2000</p>
<p>H Liu, C Li, Q Wu, Y J Lee, arXiv:2304.08485Visual instruction tuning. 2023arXiv preprint</p>
<p>Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, Roberta, arXiv:1907.11692A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Discovering language model behaviors with modelwritten evaluations. E Perez, S Ringer, K Lukošiūtė, K Nguyen, 2022</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. N Reimers, I Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Scalable and adaptive graph neural networks with self-label-enhanced training. C Sun, H Gu, J Hu, arXiv:2104.093762021arXiv preprint</p>
<p>J Tang, Y Yang, W Wei, L Shi, L Su, S Cheng, D Yin, C Huang, Graphgpt, arXiv:2310.13023Graph instruction tuning for large language models. 2023arXiv preprint</p>
<p>Attention-based graph neural network for semisupervised learning. K K Thekumparampil, C Wang, S Oh, L.-J Li, arXiv:1803.037352018arXiv preprint</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Graph attention networks. P Veličković, G Cucurull, A Casanova, A Romero, P Lio, Y Bengio, arXiv:1710.109032017arXiv preprint</p>
<p>Deep graph infomax. P Veličković, W Fedus, W L Hamilton, P Liò, Y Bengio, R D Hjelm, 2019</p>
<p>Graph attention networks. P Veličković, G Cucurull, A Casanova, A Romero, P Liò, Y Bengio, International Conference on Learning Representations. 2018</p>
<p>Large language model is also an open-ended decoder for visioncentric tasks. W Wang, Z Chen, X Chen, J Wu, X Zhu, G Zeng, P Luo, T Lu, J Zhou, Y Qiao, arXiv:2305.111752023arXiv preprint</p>
<p>Simplifying graph convolutional networks. F Wu, A Souza, T Zhang, C Fifty, T Yu, K Weinberger, International conference on machine learning. PMLR2019</p>
<p>Nodeformer: A scalable graph structure learning transformer for node classification. Q Wu, W Zhao, Z Li, D P Wipf, J Yan, Advances in Neural Information Processing Systems. 202235</p>
<p>How powerful are graph neural networks?. K Xu, W Hu, J Leskovec, S Jegelka, International Conference on Learning Representations. 2018</p>
<p>Revisiting semi-supervised learning with graph embeddings. Z Yang, W Cohen, R Salakhudinov, International conference on machine learning. PMLR2016</p>
<p>R Ye, C Zhang, R Wang, S Xu, Y Zhang, arXiv:2308.07134Natural language is all a graph needs. 2023arXiv preprint</p>
<p>Do transformers really perform badly for graph representation?. C Ying, T Cai, S Luo, S Zheng, G Ke, D He, Y Shen, T.-Y Liu, Advances in Neural Information Processing Systems. 342021</p>
<p>L2-gcn: Layerwise and learned efficient training of graph convolutional networks. Y You, T Chen, Z Wang, Y Shen, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2020. 2020</p>
<p>Graph transformer networks. S Yun, M Jeong, R Kim, J Kang, H J Kim, Advances in neural information processing systems. 201932</p>
<p>From canonical correlation analysis to self-supervised graph neural networks. H Zhang, Q Wu, J Yan, D Wipf, P S Yu, Advances in Neural Information Processing Systems. 202134</p>            </div>
        </div>

    </div>
</body>
</html>
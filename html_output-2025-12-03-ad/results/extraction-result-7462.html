<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7462 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7462</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7462</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19" target="_blank">Measuring and Improving Consistency in Pretrained Language Models</a></p>
                <p><strong>Paper Venue:</strong> Transactions of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> The creation of PARAREL, a high-quality resource of cloze-style query English paraphrases, and analysis of the representational spaces of PLMs suggest that they have a poor structure and are currently not suitable for representing knowledge in a robust way.</p>
                <p><strong>Paper Abstract:</strong> Abstract Consistency of a model‚Äîthat is, the invariance of its behavior under meaning-preserving alternations in its input‚Äîis a highly desirable property in natural language processing. In this paper we study the question: Are Pretrained Language Models (PLMs) consistent with respect to factual knowledge? To this end, we create ParaRelü§ò, a high-quality resource of cloze-style query English paraphrases. It contains a total of 328 paraphrases for 38 relations. Using ParaRelü§ò, we show that the consistency of all PLMs we experiment with is poor‚Äî though with high variance between relations. Our analysis of the representational spaces of PLMs suggests that they have a poor structure and are currently not suitable for representing knowledge robustly. Finally, we propose a method for improving model consistency and experimentally demonstrate its effectiveness.1</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7462",
    "paper_id": "paper-73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00607875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Measuring and Improving Consistency in Pretrained Language Models</h1>
<p>Yanai Elazar ${ }^{1,2}$ Nora Kassner ${ }^{3}$ Shauli Ravfogel ${ }^{1,2}$ Abhilasha Ravichander ${ }^{4}$ Eduard Hovy ${ }^{4}$ Hinrich Sch√ºtze ${ }^{3}$ Yoav Goldberg ${ }^{1,2}$<br>${ }^{1}$ Computer Science Department, Bar Ilan University<br>${ }^{2}$ Allen Institute for Artificial Intelligence<br>${ }^{3}$ Center for Information and Language Processing (CIS), LMU Munich<br>${ }^{4}$ Language Technologies Institute, Carnegie Mellon University<br>{yanaiela, shauli.ravfogel, yoav.goldberg}@gmail.com<br>kassner@cis.lmu.de {aravicha,hovy}@cs.cmu.edu</p>
<h4>Abstract</h4>
<p>Consistency of a model - that is, the invariance of its behavior under meaningpreserving alternations in its input - is a highly desirable property in natural language processing. In this paper we study the question: Are Pretrained Language Models (PLMs) consistent with respect to factual knowledge? To this end, we create ParaRel $\square$, a high-quality resource of cloze-style query English paraphrases. It contains a total of 328 paraphrases for 38 relations. Using ParaRel $\square$, we show that the consistency of all PLMs we experiment with is poor - though with high variance between relations. Our analysis of the representational spaces of PLMs suggests that they have a poor structure and are currently not suitable for representing knowledge robustly. Finally, we propose a method for improving model consistency and experimentally demonstrate its effectiveness. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Pretrained Language Models (PLMs) are large neural networks that are used in a wide variety of NLP tasks. They operate under a pretrain-finetune paradigm: models are first pretrained over a large text corpus and then finetuned on a downstream task. PLMs are thought of as good language encoders, supplying basic language understanding capabilities that can be used with ease for many downstream tasks.</p>
<p>A desirable property of a good language understanding model is consistency: the ability to make consistent decisions in semantically equivalent contexts, reflecting a systematic ability to generalize in the face of language variability.</p>
<p>Examples of consistency include: predicting the same answer in question answering and read-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of our approach. We expect that a consistent model would predict the same answer for two paraphrases. In this example, the model is inconsistent on the Homeland and consistent on the Seinfeld paraphrases.
ing comprehension tasks regardless of paraphrase (Asai and Hajishirzi, 2020); making consistent assignments in coreference resolution (Denis and Baldridge, 2009; Chang et al., 2011); or making summaries factually consistent with the original document (Kryscinski et al., 2020). While consistency is important in many tasks, nothing in the training process explicitly targets it. One could hope that the unsupervised training signal from large corpora made available to PLMs such as BERT or RoBERTa (Devlin et al., 2019; Liu et al., 2019) is sufficient to induce consistency and transfer it to downstream tasks. In this paper, we show that this is not the case.</p>
<p>The recent rise of PLMs has sparked a discussion about whether these models can be used as Knowledge Bases (KBs) (Petroni et al., 2019, 2020; Davison et al., 2019; Peters et al., 2019; Jiang et al., 2020; Roberts et al., 2020). Consistency is a key property of KBs and is particularly important for automatically constructed KBs. One of the biggest appeals of using a PLM as a KB is that we can query it in natural language - instead of relying on a specific KB schema. The expectation is that PLMs abstract away from language and map queries in natural language into meaningful representations</p>
<p>such that queries with identical intent but different language forms yield the same answer. For example, the query "Homeland premiered on [MASK]" should produce the same answer as "Homeland originally aired on [MASK]". Studying inconsistencies of PLM-KBs can also teach us about the organization of knowledge in the model or lack thereof. Finally, failure to behave consistently may point to other representational issues such as the similarity between antonyms and synonyms (Nguyen et al., 2016), and overestimating events and actions (reporting bias) (Shwartz and Choi, 2020).</p>
<p>In this work, we study the consistency of factual knowledge in PLMs, specifically in Masked Language Models (MLM) - these are PLMs trained with the MLM objective (Devlin et al., 2019; Liu et al., 2019), as opposed to other strategies such as standard language modeling (Radford et al., 2019) or text-to-text (Raffel et al., 2020). We ask: Is the factual information we extract from PLMs invariant to paraphrasing? We use zero-shot evaluation since we want to inspect models directly, without adding biases through finetuning. This allows us to assess how much consistency was acquired during pretraining and to compare the consistency of different models. Overall, we find that the consistency of the PLMs we consider is poor, although there is a high variance between relations.</p>
<p>We introduce ParaRel $\square$, a new benchmark that enables us to measure consistency in PLMs (¬ß3), by using factual knowledge that was found to be partially encoded in them (Petroni et al., 2019; Jiang et al., 2020). ParaRel $\square$ is a manually curated resource that provides patterns - short textual prompts - that are paraphrases of one another, with 328 paraphrases describing 38 binary relations such as $X$ born-in $Y, X$ works-for $Y$ (¬ß4). We then test multiple PLMs for knowledge consistency, i.e., whether a model predicts the same answer for all patterns of a relation. Figure 1 shows an overview of our approach. Using ParaRel $\square$, we probe for consistency in four PLM types: BERT, BERT-whole-word-masking, RoBERTa and ALBERT (¬ß5). Our experiments with ParaRel $\square$ show that current models have poor consistency, although with high variance between relations (¬ß6).</p>
<p>Finally, we propose a method that improves model consistency by introducing a novel consistency loss (¬ß8). We demonstrate that trained with this loss, BERT achieves better consistency performance on unseen relations. However, more work is required to achieve fully consistent models.</p>
<h2>2 Background</h2>
<p>There has been significant interest in analyzing how well PLMs (Rogers et al., 2020) perform on linguistic tasks (Goldberg, 2019; Hewitt and Manning, 2019; Tenney et al., 2019; Elazar et al., 2021), commonsense (Forbes et al., 2019; Da and Kasai, 2019; Zhang et al., 2020) and reasoning (Talmor et al., 2020; Kassner et al., 2020), usually assessed by measures of accuracy. However, accuracy is just one measure of PLM performance (Linzen, 2020). It is equally important that PLMs do not make contradictory predictions (cf. Figure 1), a type of error that humans rarely make. There has been relatively little research attention devoted to this question, i.e., to analyze if models behave consistently. One example concerns negation: Ettinger (2020) and Kassner and Sch√ºtze (2020) show that models tend to generate facts and their negation, a type of inconsistent behavior. Ravichander et al. (2020) propose paired probes for evaluating consistency. Our work is broader in scope, examining the consistency of PLM behavior across a range of factual knowledge types and investigating how models can be made to behave more consistently.</p>
<p>Consistency has also been highlighted as a desirable property in automatically constructed KBs and downstream NLP tasks. We now briefly review work along these lines.</p>
<p>Consistency in knowledge bases (KBs) has been studied in theoretical frameworks in the context of the satisfiability problem and KB construction, and efficient algorithms for detecting inconsistencies in KBs have been proposed (Hansen and Jaumard, 2000; Andersen and Pretolani, 2001). Other work aims to quantify the degree to which KBs are inconsistent and detects inconsistent statements (Thimm, 2009, 2013; Mui√±o, 2011).</p>
<p>Consistency in question answering was studied by Ribeiro et al. (2019) in two tasks: visual question answering (Antol et al., 2015) and reading comprehension (Rajpurkar et al., 2016). They automatically generate questions to test the consistency of QA models. Their findings suggest that most models are not consistent in their predictions. In addition, they use data augmentation to create more robust models. Alberti et al. (2019) generate new questions conditioned on context and answer from a labeled dataset and by filtering answers that do not provide a consistent result with the origi-</p>
<p>nal answer. They show that pretraining on these synthetic data improves QA results. <em>Asai and Hajishirzi (2020)</em> use data augmentation that complements questions with symmetricity and transitivity, as well as a regularizing loss that penalizes inconsistent predictions. <em>Kassner et al. (2021b)</em> propose a method to improve accuracy and consistency of QA models by augmenting a PLM with an evolving memory that records PLM answers and resolves inconsistency between answers.</p>
<p>Work on consistency in other domains includes <em>Du et al. (2019)</em> where prediction of consistency in procedural text is improved. <em>Ribeiro et al. (2020)</em> use consistency for more robust evaluation. <em>Li et al. (2019)</em> measure and mitigate inconsistency in natural language inference (NLI), and finally, <em>Camburu et al. (2020)</em> propose a method for measuring inconsistencies in NLI explanations <em>Camburu et al. (2018)</em>.</p>
<h2>3 Probing PLMs for Consistency</h2>
<p>In this section, we formally define consistency and describe our framework for probing consistency of PLMs.</p>
<h3>3.1 Consistency</h3>
<p>We define a model as <em>consistent</em> if, given two <em>cloze-phrases</em> such as <em>‚ÄúSeinfeld</em> originally aired on <em>[MASK]‚Äù</em> and <em>‚ÄúSeinfeld</em> premiered on <em>[MASK]‚Äù</em> that are <em>quasi-paraphrases</em>, it makes non-contradictory predictions on N-1 relations over a large set of entities. A <em>quasi-paraphrase</em> ‚Äì a concept introduced by <em>Bhagat and Hovy (2013)</em> ‚Äì is a more fuzzy version of a paraphrase. The concept does not rely on the strict, logical definition of paraphrase and allows to operationalize concrete uses of paraphrases. This definition is in the spirit of the RTE definition <em>Dagan et al. (2005)</em>, which similarly supports a more flexible use of the notion of entailment. For instance, a model that predicts <em>NBC</em> and <em>ABC</em> on the two aforementioned patterns, is not consistent, since these two facts are contradictory. We define a <em>cloze-pattern</em> as a cloze-phrase that expresses a relation between a subject and an object. Note that consistency does not require the answers to be factually correct. While correctness is also an important property for KBs, we view it as a separate objective and measure it independently. We use the terms <em>paraphrase</em> and <em>quasi-paraphrase</em> interchangeably.</p>
<p>Many-to-many (N-M) relations (e.g. <em>shares-border-with</em>) can be consistent even with different answers (given they are correct). For instance, two patterns that express the <em>shares-border-with</em> relation and predict <em>Albania</em> and <em>Bulgaria</em> for <em>Greece</em> are both correct. We do not consider such relations for measuring consistency. However, another requirement from a KB is <em>determinism</em>, i.e., returning the results in the same order (when more than a single result exists). In this work, we focus on consistency, but also measure determinism of the models we inspect.</p>
<h3>3.2 The Framework</h3>
<p>An illustration of the framework is presented in Figure 2. Let <em>D<sup>i</sup></em> be a set of subject-object KB tuples (e.g., <em><Homeland, Showtime></em>) from some relation <em>r<sup>i</sup></em> (e.g., <em>originally-aired-on</em>), accompanied with a set of <em>quasi-paraphrases</em> cloze-patterns <em>P<sup>i</sup></em> (e.g., <em>X</em> originally aired on <em>Y</em>). Our goal is to test whether the model consistently predicts the same object (e.g., <em>Showtime</em>) for a particular subject (e.g., <em>Homeland</em>). To this end, we substitute <em>X</em> with a subject from <em>D<sup>i</sup></em> and <em>Y</em> with <em>[MASK]</em> in all of the patterns <em>P<sup>i</sup></em> of that relation (e.g., <em>Homeland</em> originally aired on <em>[MASK]</em> and <em>Homeland</em> premiered on <em>[MASK]</em>). A consistent model must predict the same entity.</p>
<h4>Restricted Candidate Sets</h4>
<p>Since PLMs were not trained for serving as KBs, they often predict words that are not KB entities; e.g., a PLM may predict, for the pattern <em>‚ÄúShowtime</em> originally aired on <em>[MASK]‚Äù</em>, the noun ‚Äòtv‚Äô ‚Äì which is also a likely substitution for the language modeling objective, but not a valid KB fact completion. Therefore, following <em>Xiong et al. (2020); Ravichander et al. (2020); Kassner et al. (2021a)</em>, we restrict the PLMs‚Äô output vocabulary to the set of possible gold objects for each relation from the underlining KB. For example, in the <em>born-in</em> relation, instead of inspecting the entire vocabulary of a model, we only keep objects from the KB, such as <em>Paris</em>, <em>London</em>, <em>Tokyo</em>, etc.</p>
<p>Note that this setup makes the task easier for the</p>
<p><sup>2</sup>We refer to <em>non-contradictory predictions</em> as predictions that, as the name suggests, do not contradict one another. For instance, predicting as the birth place of a person two difference cities is considered to be contradictory, but predicting a city and its country, is not.</p>
<p><sup>3</sup>Although it is possible to also predict the subject from the object, in the cases of N-1 relations more than a single answer would be possible, making it impossible to test for consistency, but determinism.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of our framework for assessing model consistency. <em>D<sup>i</sup></em> (‚ÄúData Pairs (D)‚Äù on the left) is a set of KB triplets of some relation <em>r<sup>i</sup></em>, which are coupled with a set of <em>quasi-paraphrase</em> cloze-patterns <em>P<sup>i</sup></em> (‚ÄúPatterns (P)‚Äù on the right) that describe that relation. We then populate the subjects from <em>D<sup>i</sup></em> as well as a mask token into all patterns <em>P<sup>i</sup></em> (shown in the middle) and expect a model to predict the same object across all pattern pairs.</p>
<p>PLM, especially in the context of KBs. However, poor consistency in this setup strongly implies that consistency would be even lower without restricting candidates.</p>
<h2>4 The ParaRel ‚äÜ Resource</h2>
<p>We now describe ParaRel ‚äÜ, a resource designed for our framework (cf. Section 3.2). ParaRel ‚äÜ is curated by experts, with a high level of agreement. It contains patterns for 38 relations<sup>4</sup> from T-REx <em>Elsahar et al. (2018)</em> ‚Äî a large dataset containing KB triples aligned with Wikipedia abstracts ‚Äî with an average of 8.63 patterns per relation. Table 1 gives statistics. We further analyse the paraphrases used in this resource, partly based on the types defined in <em>Bhagat and Hovy (2013)</em>, and report this analysis in Appendix B.</p>
<h3>Construction Method</h3>
<p>ParaRel ‚äÜ was constructed in four steps. (1) We began with the patterns provided by LAMA <em>Petroni et al. (2019)</em> (one pattern per relation, referred to as <em>base-pattern</em>). (2) We augmented each base-pattern with other patterns that are paraphrases from LPAQA <em>Jiang et al. (2020)</em>. However, since LPAQA was created automatically (either by back-translation or by extracting patterns from sentences that contain both subject and object), some LPAQA patterns are not correct paraphrases. We therefore only include the subset of correct paraphrases. (3) Using SPIKE <em>Shlain et al. (2020)</em>,<sup>5</sup> a search engine over Wikipedia sentences that supports syntax-based queries, we searched for additional patterns that appeared in Wikipedia and added them to ParaRel ‚äÜ. Specifically, we searched for Wikipedia sentences containing a subject-object tuple from T-REx and then manually extracted patterns from the sentences. (4) Lastly, we added additional paraphrases of the base-pattern using the annotators' linguistic expertise. Two additional experts went over all the patterns and corrected them, while engaging in a discussion until reaching agreement, discarding patterns they could not agree on.</p>
<h3>Human Agreement</h3>
<p>To assess the quality of ParaRel ‚äÜ, we run a human annotation study. For each relation, we sample up to five paraphrases, comparing each of the new patterns to the base-pattern from LAMA. That is, if relation <em>r<sup>i</sup></em> contains the following patterns: <em>p<sub>1</sub></em>, <em>p<sub>2</sub></em>, <em>p<sub>3</sub></em>, <em>p<sub>4</sub></em>, and <em>p<sub>1</sub></em> is the base-pattern, then we compare the following pairs (<em>p<sub>1</sub></em>, <em>p<sub>2</sub></em>), (<em>p<sub>1</sub></em>, <em>p<sub>3</sub></em>), (<em>p<sub>1</sub></em>, <em>p<sub>4</sub></em>).</p>
<p>We populate the patterns with random subjects and objects pairs from T-REx <em>Elsahar et al. (2018)</em> and ask annotators if these sentences are paraphrases. We also sample patterns from different relations to provide examples that are not paraphrases of each other, as a control. Each task contains five patterns that are thought to be paraphrases and two that are not.<sup>6</sup> Overall, we collect annotations for</p>
<p><sup>4</sup>using the 41 relations from LAMA <em>Petroni et al. (2019)</em>, leaving out three relations that are poorly defined, or consist of mixed and heterogeneous entities.</p>
<p><sup>5</sup>https://spike.apps.allenai.org/</p>
<p><sup>6</sup>The controls contain the same subjects and objects so that only the pattern (not its arguments) can be used to solve the task.</p>
<table>
<thead>
<tr>
<th># Relations</th>
<th>38</th>
</tr>
</thead>
<tbody>
<tr>
<td># Patterns</td>
<td>328</td>
</tr>
<tr>
<td>Min # patterns per rel.</td>
<td>2</td>
</tr>
<tr>
<td>Max # patterns per rel.</td>
<td>20</td>
</tr>
<tr>
<td>Avg # patterns per rel.</td>
<td>8.63</td>
</tr>
<tr>
<td>Avg syntax</td>
<td>4.74</td>
</tr>
<tr>
<td>Avg lexical</td>
<td>6.03</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of PARAREL . Last two rows: average number of unique syntactic/lexical variations of patterns for a relation.</p>
<p>156 paraphrase candidates and 61 controls.
We asked NLP graduate students to annotate the pairs and collected one answer per pair. ${ }^{7}$ The agreement scores for the paraphrases and the controls are $95.5 \%$ and $98.3 \%$, which is high and indicates PARAREL 's high quality. We also inspected the disagreements and fixed many additional problems to further improve quality.</p>
<h2>5 Experimental Setup</h2>
<h3>5.1 Models \&amp; Data</h3>
<p>We experiment with four PLMs: BERT, BERT whole-word-masking ${ }^{8}$ (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and ALBERT (Lan et al., 2019). For BERT, RoBERTa and ALBERT, we use a base and a large version. ${ }^{9}$ We also report a majority baseline that always predicts the most common object for a relation. By construction, this baseline is perfectly consistent.</p>
<p>We use knowledge graph data from T-REx (Elsahar et al., 2018). ${ }^{10}$ To make the results comparable across models, we remove objects that are not represented as a single token in all models' vocabularies; 26,813 tuples remain. ${ }^{11}$ We further split the data into N-M relations for which we report determinism results (seven relations) and N-1 relations for which we report consistency ( 31 relations).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>${ }^{7}$ We asked the annotators to re-annotate any mismatch with our initial label, to allow them to fix random mistakes.
${ }^{8}$ BERT whole-word-masking is BERT's version where words that are tokenized into multiple tokens are masked together.
${ }^{9}$ For ALBERT we use the smallest and largest versions.
${ }^{10}$ We discard three poorly defined relations from T-REx.
${ }^{11}$ In a few cases, we filter entities from certain relations that contain multiple fine-grained relations to make our patterns compatible with the data. For instance, most of the instances for the genre relation describes music genres, thus we remove some of the tuples were the objects include non-music genres such as 'satire', 'sitcom' and 'thriller'.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Succ-Patt</th>
<th style="text-align: center;">Succ-Objs</th>
<th style="text-align: center;">Unk-Const</th>
<th style="text-align: center;">Know-Const</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">majority</td>
<td style="text-align: center;">$97.3 \leftarrow 7.3$</td>
<td style="text-align: center;">$23.2 \leftarrow 21.0$</td>
<td style="text-align: center;">$100.0 \leftarrow 0.0$</td>
<td style="text-align: center;">$100.0 \leftarrow 0.0$</td>
</tr>
<tr>
<td style="text-align: left;">BERT-base</td>
<td style="text-align: center;">$\mathbf{1 0 0 . 0} \leftarrow 0.0$</td>
<td style="text-align: center;">$63.0 \leftarrow 19.9$</td>
<td style="text-align: center;">$46.5 \leftarrow 21.7$</td>
<td style="text-align: center;">$63.8 \leftarrow 24.5$</td>
</tr>
<tr>
<td style="text-align: left;">BERT-large</td>
<td style="text-align: center;">$\mathbf{1 0 0 . 0} \leftarrow 0.0$</td>
<td style="text-align: center;">$\mathbf{6 5 . 7} \leftarrow 22.1$</td>
<td style="text-align: center;">$48.1 \leftarrow 20.2$</td>
<td style="text-align: center;">$65.2 \leftarrow 23.8$</td>
</tr>
<tr>
<td style="text-align: left;">BERT-large-wwm</td>
<td style="text-align: center;">$\mathbf{1 0 0 . 0} \leftarrow 0.0$</td>
<td style="text-align: center;">$64.9 \leftarrow 20.3$</td>
<td style="text-align: center;">$\mathbf{4 9 . 5} \leftarrow 20.1$</td>
<td style="text-align: center;">$\mathbf{6 5 . 3} \leftarrow 25.1$</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa-base</td>
<td style="text-align: center;">$\mathbf{1 0 0 . 0} \leftarrow 0.0$</td>
<td style="text-align: center;">$56.2 \leftarrow 22.7$</td>
<td style="text-align: center;">$43.9 \leftarrow 15.8$</td>
<td style="text-align: center;">$56.3 \leftarrow 19.0$</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa-large</td>
<td style="text-align: center;">$\mathbf{1 0 0 . 0} \leftarrow 0.0$</td>
<td style="text-align: center;">$60.1 \leftarrow 22.3$</td>
<td style="text-align: center;">$46.8 \leftarrow 18.0$</td>
<td style="text-align: center;">$60.5 \leftarrow 21.1$</td>
</tr>
<tr>
<td style="text-align: left;">ALBERT-base</td>
<td style="text-align: center;">$\mathbf{1 0 0 . 0} \leftarrow 0.0$</td>
<td style="text-align: center;">$45.8 \leftarrow 23.7$</td>
<td style="text-align: center;">$41.4 \leftarrow 17.3$</td>
<td style="text-align: center;">$56.3 \leftarrow 22.0$</td>
</tr>
<tr>
<td style="text-align: left;">ALBERT-xxlarge</td>
<td style="text-align: center;">$\mathbf{1 0 0 . 0} \leftarrow 0.0$</td>
<td style="text-align: center;">$58.8 \leftarrow 23.8$</td>
<td style="text-align: center;">$40.5 \leftarrow 16.4$</td>
<td style="text-align: center;">$57.5 \leftarrow 23.8$</td>
</tr>
</tbody>
</table>
<p>Table 2: Extractability Measures in the different models we inspect. Best model for each measure highlighted in bold.</p>
<h3>5.2 Evaluation</h3>
<p>Our consistency measure for a relation $r_{i}$ (Consistency) is the percentage of consistent predictions of all the pattern pairs $p_{k}^{i}, p_{l}^{i} \in P_{i}$ of that relation, for all its KB tuples $d_{j}^{i} \in D_{i}$. Thus, for each KB tuple from a relation $r_{i}$ that contains $n$ patterns, we consider predictions for $n(n-1) / 2$ pairs.</p>
<p>We also report Accuracy, that is, the acc@1 of a model in predicting the correct object, using the original patterns from Petroni et al. (2019). In contrast to Petroni et al. (2019), we define it as the accuracy of the top-ranked object from the candidate set of each relation. Finally, we report Consistent-Acc, a new measure that evaluates individual objects as correct only if all patterns of the corresponding relation predict the object correctly. Consistent$A c c$ is much stricter and combines the requirements of both consistency (Consistency) and factual correctness (Accuracy). We report the average over relations, i.e., macro average, but notice that the micro average produces similar results.</p>
<h2>6 Experiments and Results</h2>
<h3>6.1 Knowledge Extraction through Different Patterns</h3>
<p>We begin by assessing our patterns as well as the degree to which they extract the correct entities. These results are summarized in Table 2.</p>
<p>First, we report Succ-Patt, the percentage of patterns that successfully predicted the right object at least once. A high score suggests that the patterns are of high quality and enable the models to extract the correct answers. All PLMs achieve a perfect score. Next, we report Succ-Objs, the percentage of entities that were predicted correctly by at least one of the patterns. Succ-Objs quantifies the degree to which the models "have" the required knowledge. We observe that some tuples</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Accuracy</th>
<th>Consistency</th>
<th>Consistent-Acc</th>
</tr>
</thead>
<tbody>
<tr>
<td>majority</td>
<td>$23.1+-21.0$</td>
<td>$100.0+-0.0$</td>
<td>$23.1+-21.0$</td>
</tr>
<tr>
<td>BERT-base</td>
<td>$45.8+-25.6$</td>
<td>$58.5+-24.2$</td>
<td>$27.0+-23.8$</td>
</tr>
<tr>
<td>BERT-large</td>
<td>$48.1+-26.1$</td>
<td>$\mathbf{6 1 . 1}+-23.0$</td>
<td>$\mathbf{2 9 . 5}+-26.6$</td>
</tr>
<tr>
<td>BERT-large-wwm</td>
<td>$\mathbf{4 8 . 7}+-25.0$</td>
<td>$60.9+-24.2$</td>
<td>$29.3+-26.9$</td>
</tr>
<tr>
<td>RoBERTa-base</td>
<td>$39.0+-22.8$</td>
<td>$52.1+-17.8$</td>
<td>$16.4+-16.4$</td>
</tr>
<tr>
<td>RoBERTa-large</td>
<td>$43.2+-24.7$</td>
<td>$56.3+-20.4$</td>
<td>$22.5+-21.1$</td>
</tr>
<tr>
<td>ALBERT-base</td>
<td>$29.8+-22.8$</td>
<td>$49.8+-20.1$</td>
<td>$16.7+-20.3$</td>
</tr>
<tr>
<td>ALBERT-xxlarge</td>
<td>$41.7+-24.9$</td>
<td>$52.1+-22.4$</td>
<td>$23.8+-24.8$</td>
</tr>
</tbody>
</table>
<p>Table 3: Knowledge and Consistency Results. Best model for each measure in bold.
are not predicted correctly by any of our patterns: the scores vary between $45.8 \%$ for ALBERT-base and $65.7 \%$ for BERT-large. With an average number of 8.63 patterns per relation, there are multiple ways to extract the knowledge, we thus interpret these results as evidence that a large part of T-REx knowledge is not stored in these models.</p>
<p>Finally, we measure Unk-Const, a consistency measure for the subset of tuples for which no pattern predicted the correct answer; and Know-Const, consistency for the subset where at least one of the patterns for a specific relation predicted the correct answer. This split into subsets is based on Succ-Objs. Overall, the results indicate that when the factual knowledge is successfully extracted, the model is also more consistent. For instance, for BERT-large, Know-Const is $65.2 \%$ and Unk-Const is $48.1 \%$.</p>
<h3>6.2 Consistency \&amp; Knowledge</h3>
<p>In this section, we report the overall knowledge measure that was used in petroni2019ac- (Accuracy), the consistency measure (Consistency), and Consistent- Acc, which combines knowledge and consistency (Consistent-Acc). The results are summarized in Table 3.</p>
<p>We begin with the Accuracy results. The results range between $29.8 \%$ (ALBERT-base) and $48.7 \%$ (BERT-large whole-word-masking). Notice that our numbers differ from petroni2019ac- as we use a candidate set (¬ß3) and only consider KB triples whose object is a single token in all the PLMs we consider (¬ß5.1).</p>
<p>Next, we report Consistency (¬ß5.2). The BERT models achieve the highest scores. There is a consistent improvement from base to large versions of each model. In contrast to previous work that observed quantitative and qualitative improvements of RoBERTa-based models over BERT [liu2019roberta; talmor2020roberta], in terms of consistency, BERT is more consistent than RoBERTa and ALBERT. Still, the overall results are low ( $61.1 \%$ for the best model), even more remarkably so because the restricted candidate set makes the task easier. We note that the results are highly variant between models (performance on original-language varies between $52 \%$ and $90 \%$ ), and relations (BERT-large performance is $92 \%$ on capital-of and $44 \%$ on owned-by).</p>
<p>Finally, we report Consistent-Acc: the results are much lower than for Accuracy, as expected, but follow similar trends: RoBERTa-base performs worse ( $16.4 \%$ ) and BERT-large best ( $29.5 \%$ ).</p>
<p>Interestingly, we find strong correlations between Accuracy and Consistency, ranging from $67.3 \%$ for RoBERTa-base to $82.1 \%$ for BERT-large (all with small p-values $\ll 0.01$ ).</p>
<p>A striking result of the model comparison is the clear superiority of BERT, both in knowledge accuracy (which was also observed by shin2020retrieval) and knowledge consistency. We hypothesize this result is caused by the different sources of training data: although Wikipedia is part of the training data for all models we consider, for BERT it is the main data source, but for RoBERTa and ALBERT it is only a small portion. Thus, when using additional data, some of the facts may be forgotten, or contradicted in the other corpora; this can diminish knowledge and compromise consistency behavior. Thus, since Wikipedia is likely the largest unified source of factual knowledge that exists in unstructured data, giving it prominence in pretraining makes it more likely that the model will incorporate Wikipedia's factual knowledge well. These results may have a broader impact on models to come: Training bigger models with more data (such as GPT-3 [brown2020gpt3]) is not always beneficial.</p>
<p>Determinism We also measure determinism for N-M relations, i.e., we use the same measure as Consistency, but since difference predictions may be factually correct, these do not necessarily convey consistency violations, but indicate nondeterminism. For brevity, we do not present all results, but the trend is similar to the consistency result (although not comparable, as the relations are different): $52.9 \%$ and $44.6 \%$ for BERT-large and RoBERTa-base, respectively.</p>
<p>| Model | Acc | Consistency | Consistent-Acc |
| majority | $23.1+-21.0$ | $100.0+-0.0$ | $23.1+-21.0$ |
| RoBERTa-med-small-1M | $11.2+-9.4$ | $37.1+-11.0$ | $2.8+-4.0$ |
| RoBERTa-base-10M | $17.3+-15.8$ | $29.8+-12.7$ | $3.2+-5.1$ |
| RoBERTa-base-100M | $22.1+-17.1$ | $31.5+-13.0$ | $3.7+-5.3$ |
| RoBERTa-base-1B | $\mathbf{3 8 . 0}+-23.4$ | $\mathbf{5 0 . 6}+-19.8$ | $\mathbf{1 8 . 0}+-16.0$ |</p>
<p>Table 4: Knowledge and consistency results for different RoBERTas, trained on increasing amounts of data. Best model for each measure in bold.</p>
<p>Effect of Pretraining Corpus Size Next, we study the question of whether the number of tokens used during pretraining contributes to consistency. We use the pretrained RoBERTa models from Warstadt et al. (2020) and repeat the experiments on four additional models. These are RoBERTa-based models, trained on a sample of Wikipedia and the book corpus, with varying training sizes and parameters. We use one of the three published models for each configuration and report the average accuracy over the relations for each model in Table 4. Overall, Accuracy and Consistent-Acc improve with more training data. However, there is an interesting outlier to this trend: The model that was trained on one million tokens is more consistent than the models trained on ten and one-hundred million tokens. A potentially crucial difference is that this model has many fewer parameters than the rest (to avoid overfitting). It is nonetheless interesting that a model that is trained on significantly less data can achieve better consistency. On the other hand, it‚Äôs accuracy scores are lower, arguably due to the model being exposed to less factual knowledge during pretraining.</p>
<h3>6.3 Do PLMs Generalize Over Syntactic Configurations?</h3>
<p>Many papers have found neural models (especially PLMs) to naturally encode syntax <em>Linzen et al. (2016); Belinkov et al. (2017); Marvin and Linzen (2018); Belinkov and Glass (2019); Goldberg (2019); Hewitt and Manning (2019)</em>. Does this mean that PLMs have successfully abstracted knowledge and can comprehend and produce it regardless of syntactic variation? We consider two scenarios. (1) Two patterns differ only in syntax. (2) Both syntax and lexical choice are the same. As a proxy, we define syntactic equivalence when the dependency path between the subject and object are identical. We parse all patterns from ParaRel using a</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Diff-Syntax</th>
<th>No-Change</th>
</tr>
</thead>
<tbody>
<tr>
<td>majority</td>
<td>$100.0+-0.0$</td>
<td>$100.0+-0.0$</td>
</tr>
<tr>
<td>BERT-base</td>
<td>$67.9+-30.3$</td>
<td>$76.3+-22.6$</td>
</tr>
<tr>
<td>BERT-large</td>
<td>$67.5+-30.2$</td>
<td>$78.7+-14.7$</td>
</tr>
<tr>
<td>BERT-large-wwm</td>
<td>$63.0+-31.7$</td>
<td>$\mathbf{8 1 . 1}+-9.7$</td>
</tr>
<tr>
<td>RoBERTa-base</td>
<td>$66.9+-10.1$</td>
<td>$80.7+-5.2$</td>
</tr>
<tr>
<td>RoBERTa-large</td>
<td>$\mathbf{6 9 . 7}+-19.2$</td>
<td>$80.3+-6.8$</td>
</tr>
<tr>
<td>ALBERT-base</td>
<td>$62.3+-22.8$</td>
<td>$72.6+-11.5$</td>
</tr>
<tr>
<td>ALBERT-xxlarge</td>
<td>$51.7+-26.0$</td>
<td>$67.3+-17.1$</td>
</tr>
</tbody>
</table>
<p>Table 5: Consistency and standard deviation when only syntax differs (Diff-Syntax) and when syntax and lexical choice are identical (No-Change). Best model for each metric is highlighted in bold.
dependency parser <em>Honnibal et al. (2020)</em> and retain the path between the entities. Success on (1) indicates that the model‚Äôs knowledge processing is robust to syntactic variation. Success on (2) indicates that the model‚Äôs knowledge processing is robust to variation in word order and tense.</p>
<p>Table 5 reports results. While these and the main results on the entire dataset are not comparable as the pattern subsets are different, they are higher than the general results: $67.5 \%$ for BERTlarge when only the syntax differs and $78.7 \%$ when the syntax is identical. This demonstrates that while PLMs have impressive syntactic abilities, they struggle to extract factual knowledge in the face of tense, word-order, and syntactic variation.</p>
<p>McCoy et al. (2019) show that supervised models trained on MNLI <em>Williams et al. (2018)</em>, an NLI dataset <em>Bowman et al. (2015)</em>, use superficial syntactic heuristics rather than more generalizable properties of the data. Our results indicate that PLMs have problems along the same lines: they are not robust to surface variation.</p>
<h2>7 Analysis</h2>
<h3>7.1 Qualitative Analysis</h3>
<p>To better understand the factors affecting consistent predictions, we inspect the predictions of BERTlarge on the patterns shown in Table 6. We highlight several cases: The predictions in Example #1 are inconsistent, and correct for the first pattern (Amsterdam), but not for the other two (Madagascar and Luxembourg). The predictions in Example #2 also show a single pattern that predicted the right object; however, the two other patterns, which are</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>#</th>
<th>Subject</th>
<th>Object</th>
<th>Pattern #1</th>
<th>Pattern #2</th>
<th>Pattern #3</th>
<th>Pred #1</th>
<th>Pred #2</th>
<th>Pred #3</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Adriaan Pasw</td>
<td>Amsterdam</td>
<td>[X] was born in [Y].</td>
<td>[X] is native to [Y].</td>
<td>[X] is a [Y]-born person.</td>
<td>Amsterdam</td>
<td>Madagascar</td>
<td>Luxembourg</td>
</tr>
<tr>
<td>2</td>
<td>Nissan Livina Geniss</td>
<td>Nissan</td>
<td>[X] is produced by [Y].</td>
<td>[X] is created by [Y].</td>
<td>[X], created by [Y].</td>
<td>Nissan</td>
<td>Renault</td>
<td>Renault</td>
</tr>
<tr>
<td>3</td>
<td>Albania</td>
<td>Serbia</td>
<td>[X] shares border with [Y].</td>
<td>[Y] borders with [X].</td>
<td>[Y] shares the border with [X]</td>
<td>Greece</td>
<td>Turkey</td>
<td>Kossovo</td>
</tr>
<tr>
<td>4</td>
<td>d'Joud</td>
<td>Apple</td>
<td>[X] is developed by [Y].</td>
<td>[X], created by [Y].</td>
<td>[X] was created by [Y]</td>
<td>Microsoft</td>
<td>Google</td>
<td>Imlig</td>
</tr>
<tr>
<td>5</td>
<td>Yahoo! Messenger</td>
<td>Yahoo</td>
<td>[X], a product created by [Y]</td>
<td>[X], a product developed by [Y]</td>
<td>[Y], that developed [X]</td>
<td>Microsoft</td>
<td>Microsoft</td>
<td>Microsoft</td>
</tr>
<tr>
<td>6</td>
<td>Wales</td>
<td>Cardiff</td>
<td>The capital of [X] is [Y] .</td>
<td>[X]'s capital, [Y].</td>
<td>[X]'s capital city, [Y].</td>
<td>Cardiff</td>
<td>Cardiff</td>
<td>Cardiff</td>
</tr>
</tbody>
</table>
<p>Table 6: Predictions of BERT-large-cased. ‚ÄúSubject‚Äù and ‚ÄúObject‚Äù are from T-REx <em>Elsahar et al. (2018)</em>. ‚ÄúPattern #i‚Äù / ‚ÄúPred #i‚Äù: three different patterns from our resource and their predictions. The predictions are colored in blue if the model predicted correctly (out of the candidate list), and in red otherwise. If there is more than a single erroneous prediction, it is colored by a different red.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: t-SNE of the encoded patterns from the <em>capital</em> relation. The colors represent the different subjects, while the shapes represent patterns. A knowledge-focused representation should cluster based on identical subjects (color), but instead the clustering is according to identical patterns (shape).</p>
<p>lexically similar, predicted the same, wrong answer ‚Äì <em>Renault</em>. Next, the patterns of Example #3 produced two factually correct answers out of three (<em>Greece</em>, <em>Kosovo</em>), but simply do not correspond to the gold object in T-REx (<em>Albania</em>), since this is an M-N relation. Note that this relation is not part of the consistency evaluation, but the determinism evaluation. The three different predictions in example #4 are all incorrect. Finally, the two last predictions demonstrate consistent predictions: Example #5 is consistent but factually incorrect (even though the correct answer is a substring of the subject), and finally, Example #6 is consistent and factual.</p>
<h3>7.2 Representation Analysis</h3>
<p>To provide insights on the models' representations, we inspect these after encoding the patterns.</p>
<p>Motivated by previous work that found that words with the same syntactic structure cluster together <em>Chi et al. (2020); Ravfogel et al. (2020)</em> we perform a similar experiment to test if this behavior replicates with respect to knowledge: We encode the patterns, after filling the placeholders with subjects and masked tokens and inspect the last layer representations in the masked token position. When plotting the results using t-SNE <em>Maaten and Hinton (2008)</em> we mainly observe clustering based on the patterns, which suggests that encoding of knowledge of the entity is not the main component of the representations. Figure 3 demonstrates this for BERT-large encodings of the <em>capital</em> relation, which is highly consistent. To provide a more quantitative assessment of this phenomenon, we also cluster the representations and set the number of centroids based on: (1) the number of patterns in each relation, which aims to capture pattern-based clusters, and (2) the number of subjects in each relation, which aims to capture entity-based clusters. This would allow for a perfect clustering, in the case of perfect alignment between the representation and the inspected property. We measure the purity of these clusters using V-measure and observe that the clusters are mostly grouped by the patterns, rather than the subjects. Finally, we compute the spearman correlation between the consistency scores and the V-measure of the representations. However, the correlation between these variables is close to zero, therefore not explaining the models' behavior. We repeated these experiments while inspecting the objects instead of the subjects, and found similar trends. This finding is interesting since it means that (1) these representations are not knowledge-focused, i.e., their main component does not relate to knowledge, and (2) the representation by its entirety does not explain the behavior of the model, and thus only a subset of the representation does. This finding is consistent.</p>
<p><sup>13</sup>While some patterns are clustered based on the subjects (upper-left part), most of them are clustered based on patterns.</p>
<p><sup>14</sup>Using the KMeans algorithm.</p>
<p><sup>15</sup>Except for BERT-large whole-word-masking, where the correlation is 39.5 (p &lt; 0.05).</p>
<p>with previous work that observed similar trends for linguistic tasks <em>Elazar et al. (2021)</em>. We hypothesize that this disparity between the representation and the behavior of the model may be explained by a situation where the distance between representations largely does not reflect the distance between predictions, but rather other, behaviorally irrelevant factors of a sentence.</p>
<h2>8 Improving Consistency in PLMs</h2>
<p>In the previous sections, we showed PLMs are generally not consistent in their predictions, and previous works have noticed the lack of this property in a variety of downstream tasks. An ideal model would exhibit the consistency property after pretraining, and would then be able to transfer it to different downstream tasks. We therefore ask: Can we enhance current PLMs and make them more consistent?</p>
<h3>8.1 Consistency Improved PLMs</h3>
<p>We propose to improve the consistency of PLMs by continuing the pretraining step with a novel consistency loss. We make use of the T-REx tuples and the paraphrases from ParaRel.</p>
<p>For each relation $r_{i}$, we have a set of paraphrased patterns $P_{i}$ describing that relation. We use a PLM to encode all patterns in $P_{i}$, after populating a subject that corresponds to the relation $r_{i}$ and a mask token. We expect the model to make the same prediction for the masked token for all patterns.</p>
<p>Consistency Loss Function As we evaluate the model using acc@1, the straight-forward consistency loss would require these predictions to be identical:</p>
<p>$\min_{\theta}\operatorname{sim}(\arg\max_{i}f_{\theta}(P_{n})[i],\arg\max_{j}f_{\theta}(P_{m})[j])$</p>
<p>where $f_{\theta}(P_{n})$ is the output of an encoding function (e.g., BERT) parameterized by $\theta$ (a vector) over input $P_{n}$, and $f_{\theta}(P_{n})[i]$ is the score of the $i$th vocabulary item of the model.</p>
<p>However, this objective contains a comparison between the output of two argmax operations, making it discrete and discontinuous, and hard to optimize in a gradient-based framework. We instead relax the objective, and require that the predicted distributions $Q_{n}=\operatorname{softmax}\left(f_{\theta}(P_{n})\right)$, rather than the top-1 prediction, be identical to each other. We use two-sided KL Divergence to measure similarity between distributions: $D_{KL}(Q_{n}^{r_{i}}||Q_{m}^{r_{i}})+D_{KL}(Q_{m}^{r_{i}}||Q_{n}^{r_{i}})$ where $Q_{n}^{r_{i}}$ is the predicted distribution for pattern $P_{n}$ of relation $r_{i}$.</p>
<p>As most of the vocabulary is not relevant for the predictions, we filter it down to the $k$ tokens from the candidate set of each relation (¬ß3.2). We want to maintain the original capabilities of the model focusing on the candidate set helps to achieve this goal since most of the vocabulary is not affected by our new loss.</p>
<p>To encourage a more general solution, we make use of all the paraphrases together, and enforce all predictions to be as close as possible. Thus, the consistency loss for all pattern pairs for a particular relation $r^{i}$ is:</p>
<p>$\mathcal{L}<em n="1">{c}=\sum</em>)$}^{k}\sum_{m=n+1}^{k}D_{KL}(Q_{n}^{r_{i}}||Q_{m}^{r_{i}})+D_{KL}(Q_{m}^{r_{i}}||Q_{n}^{r_{i}</p>
<p>MLM Loss Since the consistency loss is different from the Cross-Entropy loss the PLM is trained on, we find it important to continue the MLM loss on text data, similar to previous work <em>Geva et al. (2020)</em>.</p>
<p>We consider two alternatives for continuing the pretraining objective: (1) MLM on Wikipedia and (2) MLM on the patterns of the relations used for the consistency loss. We found that the latter works better. We denote this loss by $\mathcal{L}_{M L M}$.</p>
<h3>Consistency Guided MLM Continual Training</h3>
<p>Combining our novel consistency loss with the regular MLM loss, we continue the PLM training by combining the two losses. The combination of the two losses is determined by a hyperparameter $\lambda$, resulting in the following final loss function:</p>
<p>$\mathcal{L}=\lambda\mathcal{L}<em L="L" M="M">{c}+\mathcal{L}</em>$</p>
<p>This loss is computed per relation, for one KB tuple. We have many of these instances, which we require to behave similarly. Therefore, we batch together $l=8$ tuples from the same relation and apply the consistency loss function to all of them.</p>
<h3>8.2 Setup</h3>
<p>Since we evaluate our method on unseen relations, we also split train and test by relation type (e.g., location-based relations, which are very common in T-REx). Moreover, our method is aimed to be simple, effective, and to require only minimal supervision. Therefore, we opt to use only three relations: original-language, named-after, and original-network; these were chosen randomly,</p>
<p>| Model | Accuracy | Consistency | Consistent-Acc |
| majority | $24.4+-22.5$ | $100.0+-0.0$ | $24.4+-22.5$ |
| BERT-base | $45.6+-27.6$ | $58.2+-23.9$ | $27.3+-24.8$ |
| BERT-ft | $\mathbf{4 7 . 4}+$-27.3 | $\mathbf{6 4 . 0}+$-22.9 | $\mathbf{3 3 . 2}+$-27.0 |
| -consistency | $46.9+-27.6$ | $60.9+-22.6$ | $30.9+-26.3$ |
| -typed | $46.5+-27.1$ | $62.0+-21.2$ | $31.1+-25.2$ |
| -MLM | $16.9+-21.1$ | $80.8+-27.1$ | $9.1+-11.5$ |</p>
<p>Table 7: Knowledge and consistency results for the baseline, BERT base, and our model. The results are averaged over the 25 test relations. Underlined: best performance overall, including ablations. Bold: best performance for BERT-ft and the two baselines (BERT-base, majority).
out of the non-location related relations. ${ }^{16}$ For validation, we randomly pick three relations of the remaining relations and use the remaining 25 for testing.</p>
<p>We perform minimal tuning of the parameters $(\lambda \in 0.1,0.5,1)$ to pick the best model, train for three epochs, and select the best model based on Consistent-Acc on the validation set. For efficiency reasons, we use the base version of BERT.</p>
<h3>8.3 Improved Consistency Results</h3>
<p>The results are presented in Table 7. We report aggregated results for the 25 relations in the test. We again report macro average (mean over relations) and standard deviation. We report the results of the majority baseline (first row), BERT-base (second row) and our new model (BERT-ft, third row). First, we note that our model significantly improves consistency: $64.0 \%$ (compared with $58.2 \%$ for BERT-base, an increase of 5.8 points). Accuracy also improves compared to BERT-base, from $45.6 \%$ to $47.4 \%$. Finally, and most importantly, we see an increase of 5.9 points in Consistent-Acc, which is achieved due to the improved consistency of the model. Notably, these improvements arise from training on merely three relations, meaning that the model improved its consistency ability and generalized to new relations. We measure the statistical significance of our method compared to the BERT baseline, using McNemar's test (following Dror et al. (2018, 2020)) and find all results to be significant $(p \ll 0.01)$.</p>
<p>We also perform an ablation study to quantify the utility of the different components. First, we report on the finetuned model without the consistency loss (-consistency). Interestingly, it does im-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>prove over the baseline (BERT-base), but it lags behind our finetuned model. Second, applying our loss on the candidate set rather than on the entire vocabulary is beneficial (-typed). Finally, by not performing the MLM training on the generated patterns (-MLM), the consistency results improve significantly ( $80.8 \%$ ); however, this also hurts Accuracy and Consistent-Acc. MLM training seems to serve as a regularizer that prevents catastrophic forgetting.</p>
<p>Our ultimate goal is to improve consistency in PLMs for better performance on downstream tasks. Therefore, we also experiment with finetuning on SQuAD (Rajpurkar et al., 2016), and evaluating on paraphrased questions from SQuAD (Gan and $\mathrm{Ng}, 2019$ ) using our consistency model. However, the results perform on par with the baseline model, both on SQuAD and the paraphrase questions. More research is required to show that consistent PLMs can also benefit downstream tasks.</p>
<h2>9 Discussion</h2>
<p>Consistency for Downstream Tasks The rise of PLMs has improved many tasks but has also brought a lot of expectations. The standard usage of these models is pretraining on a large corpus of unstructured text and then finetuning on a task of interest. The first step is thought of as providing a good language-understanding component, whereas the second step is used to teach the format and the nuances of a downstream task.</p>
<p>As discussed earlier, consistency is a crucial component of many NLP systems (Du et al., 2019; Asai and Hajishirzi, 2020; Denis and Baldridge, 2009; Kryscinski et al., 2020) and obtaining this skill from a pretrained model would be extremely beneficial and has the potential to make specialized consistency solutions in downstream tasks redundant. Indeed, there is an ongoing discussion about the ability to acquire "meaning" from raw text signal alone (Bender and Koller, 2020). Our new benchmark makes it possible to track the progress of consistency in pretrained models.</p>
<p>Broader Sense of Consistency In this work we focus on one type of consistency, that is, consistency in the face of paraphrasing; however, consistency is a broader concept. For instance, previous work has studied the effect of negation on factual statements, which can also be seen as consistency (Ettinger, 2020; Kassner and Sch√ºtze, 2020). A consistent model is expected to return different an-</p>
<p>swers to the prompts: "Birds can [MASK]" and "Birds cannot [MASK]". The inability to do so, as was shown in these works, also shows the lack of model consistency.</p>
<p>Usage of PLMs as KBs Our work follows the setup of Petroni et al. (2019); Jiang et al. (2020), where PLMs are being tested as KBs. While it is an interesting setup for probing models for knowledge and consistency, it lacks important properties of standard KBs: (1) the ability to return more than a single answer and (2) the ability to return no answer. Although some heuristics can be used for allowing a PLM to do so, e.g., using a threshold on the probabilities, it is not the way that the model was trained, and thus may not be optimal. Newer approaches that propose to use PLMs as a starting point to more complex systems have promising results and address these problems (Thorne et al., 2020).</p>
<p>In another approach, Shin et al. (2020) suggest using AutoPrompt to automatically generate prompts, or patterns, instead of creating them manually. This approach is superior to manual patterns (Petroni et al., 2019), or aggregation of patterns that were collected automatically (Jiang et al., 2020).</p>
<p>Brittleness of Neural Models Our work also relates to the problem of the brittleness of neural networks. One example of this brittleness is the vulnerability to adversarial attacks (Szegedy et al., 2014; Jia and Liang, 2017). The other problem, closer to the problem we explore in this work, is the poor generalization to paraphrases. For example, Gan and Ng (2019) created a paraphrase version for a subset of SQuAD (Rajpurkar et al., 2016), and showed that model performance drops significantly. Ribeiro et al. (2018) proposed another method for creating paraphrases and performed a similar analysis for visual question answering and sentiment analysis. Recently, Ribeiro et al. (2020) proposed CHECKLIST, a system that tests a model's vulnerability to several linguistic perturbations.</p>
<p>ParaRel $\square$ enables us to study the brittleness of PLMs, and separate facts that are robustly encoded in the model from mere 'guesses', which may arise from some heuristic or spurious correlations with certain patterns (Poerner et al., 2020). We showed that PLMs are susceptible to small perturbations, and thus, finetuning on a downstream task - given that training datasets are typically not large and do not contain equivalent examples - is not likely to perform better.</p>
<p>Can we Expect from LMs to be Consistent? The typical training procedure of an LM does not encourage consistency. The standard training solely tries to minimize the log-likelihood of an unseen token, and this objective is not always aligned with consistency of knowledge. Consider for example the case of wikipedia texts, as opposed to reddit; their texts and styles may be very different and they may even describe contradictory facts. An LM can exploit the styles of each text to best fit the probabilities given to an unseen word, even if the resulting generations contradict each other.</p>
<p>Since the pretraining-finetuning procedure is the dominating one in our field currently, a great amount of the language capabilities that were learned during pre-training also propagates to the fine-tuned models. As such, we believe it is important to measure and improve consistency already in the pretrained models.</p>
<p>Reasons Behind the (In)Consistency Since LMs are not expected to be consistent, what are the reasons behind their predictions, when being consistent, or inconsistent?</p>
<p>In this work, we presented the predictions of multiple queries, and the representation space of one of the inspected models. However, this does not point to the origins of such behavior. In future work, we aim to inspect this question more closely.</p>
<h2>10 Conclusion</h2>
<p>In this work, we study the consistency of PLMs with regard to their ability to extract knowledge. We build a high-quality resource named ParaRel $\square$ that contains 328 high-quality patterns for 38 relations. Using ParaRel $\square$, we measure consistency in multiple PLMs, including BERT, RoBERTa, and ALBERT, and show that although the latter two are superior to BERT in other tasks, they fall short in terms of consistency. However, the consistency of these models is generally low. We release ParaRel $\square$ along with data tuples from T-REx as a new benchmark to track knowledge consistency of NLP models. Finally, we propose a new simple method to improve model consistency, by continuing the pretraining with a novel loss. We show this method to be effective and to improve both the consistency of models as well as their ability to extract the correct facts.</p>
<h2>Acknowledgements</h2>
<p>We would like to thank Tomer Wolfson, Ido Dagan, Amit Moryossef and Victoria Basmov for their helpful comments and discussions, and Alon Jacovi, Ori Shapira, Arie Cattan, Elron Bandel, Philipp Dufter, Masoud Jalili Sabet, Marina Speranskaya, Antonis Maronikolakis, Aakanksha Naik, Aishwarya Ravichander, Aditya Potukuchi for the help with the annotations. We also thank the anonymous reviewers and the action editor, George Foster, for their valuable suggestions.</p>
<p>Yanai Elazar is grateful to be supported by the PBC fellowship for outstanding PhD candidates in Data Science and the Google PhD fellowship. This project has received funding from the Europoean Research Council (ERC) under the Europoean Union's Horizon 2020 research and innovation programme, grant agreement No. 802774 (iEXTRACT). This work has been funded by the European Research Council (#740516) and by the German Federal Ministry of Education and Research (BMBF) under Grant No. 01IS18036A. The authors of this work take full responsibility for its content. This research was also supported in part by grants from the National Science Foundation Secure and Trustworthy Computing program (CNS-1330596, CNS15-13957, CNS1801316, CNS-1914486) and a DARPA Brandeis grant (FA8750-15-2-0277). The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the NSF, DARPA, or the US Government.</p>
<h2>References</h2>
<p>Chris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin, and Michael Collins. 2019. Synthetic qa corpora generation with roundtrip consistency. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6168-6173.</p>
<p>Kim Allan Andersen and Daniele Pretolani. 2001. Easy cases of probabilistic satisfiability. Annals of Mathematics and Artificial Intelligence, 33(1):69-91.</p>
<p>Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zit-
nick, and Devi Parikh. 2015. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425-2433.</p>
<p>Akari Asai and Hannaneh Hajishirzi. 2020. Logicguided data augmentation and regularization for consistent question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5642-5650, Online. Association for Computational Linguistics.</p>
<p>Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James Glass. 2017. What do neural machine translation models learn about morphology? In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 861872 .</p>
<p>Yonatan Belinkov and James Glass. 2019. Analysis methods in neural language processing: A survey. Transactions of the Association for Computational Linguistics, 7:49-72.</p>
<p>Emily M Bender and Alexander Koller. 2020. Climbing towards nlu: On meaning, form, and understanding in the age of data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5185-5198.</p>
<p>Rahul Bhagat and Eduard Hovy. 2013. What is a paraphrase? Computational Linguistics, 39(3):463-472.</p>
<p>Lukas Biewald. 2020. Experiment tracking with weights and biases. Software available from wandb.com.</p>
<p>Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics.</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.</p>
<p>Oana-Maria Camburu, Tim Rockt√§schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natural language inference with natural language explanations. In NeurIPS.</p>
<p>Oana-Maria Camburu, Brendan Shillingford, Pasquale Minervini, Thomas Lukasiewicz, and Phil Blunsom. 2020. Make up your mind! adversarial generation of inconsistent natural language explanations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4157-4165.</p>
<p>Kai-Wei Chang, Rajhans Samdani, Alla Rozovskaya, Nick Rizzolo, Mark Sammons, and Dan Roth. 2011. Inference protocols for coreference resolution. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, pages 40-44.</p>
<p>Ethan A. Chi, John Hewitt, and Christopher D. Manning. 2020. Finding universal grammatical relations in multilingual BERT. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5564-5577, Online. Association for Computational Linguistics.</p>
<p>Jeff Da and Jungo Kasai. 2019. Cracking the contextual commonsense code: Understanding commonsense reasoning aptitude of deep contextual representations. In Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing, pages 1-12, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Machine Learning Challenges Workshop, pages 177-190. Springer.</p>
<p>Joe Davison, Joshua Feldman, and Alexander M Rush. 2019. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1173-1178.</p>
<p>Pascal Denis and Jason Baldridge. 2009. Global joint models for coreference resolution and named entity classification. Procesamiento del Lenguaje Natural, 42.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186.</p>
<p>Rotem Dror, Gili Baumer, Segev Shlomov, and Roi Reichart. 2018. The hitchhiker's guide to testing statistical significance in natural language processing. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13831392.</p>
<p>Rotem Dror, Lotem Peled-Cohen, Segev Shlomov, and Roi Reichart. 2020. Statistical significance testing for natural language processing. Synthesis Lectures on Human Language Technologies, 13(2):1-116.</p>
<p>Xinya Du, Bhavana Dalvi, Niket Tandon, Antoine Bosselut, Wen-tau Yih, Peter Clark, and Claire Cardie. 2019. Be consistent! improving procedural text comprehension using label consistency. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2347-2356.</p>
<p>Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. 2021. Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals. Transactions of the Association for Computational Linguistics, 9:160-175.</p>
<p>Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Frederique Laforest, and Elena Simperl. 2018. T-rex: A large scale alignment of natural language with knowledge base triples. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).</p>
<p>Allyson Ettinger. 2020. What bert is not: Lessons from a new suite of psycholinguistic diagnostics for language models. Transactions of the Association for Computational Linguistics, 8:34-48.</p>
<p>Maxwell Forbes, Ari Holtzman, and Yejin Choi. 2019. Do neural language representations learn physical commonsense? In CogSci.</p>
<p>Wee Chung Gan and Hwee Tou Ng. 2019. Improving the robustness of question answering systems to question paraphrasing. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6065-6075.</p>
<p>Mor Geva, Ankit Gupta, and Jonathan Berant. 2020. Injecting numerical reasoning skills into language models. In Association for Computational Linguistics (ACL).</p>
<p>Yoav Goldberg. 2019. Assessing bert's syntactic abilities. arXiv preprint arXiv:1901.05287.</p>
<p>Pierre Hansen and Brigitte Jaumard. 2000. Probabilistic satisfiability. In Handbook of Defeasible Reasoning and Uncertainty Management Systems, pages 321-367. Springer.</p>
<p>John Hewitt and Christopher D. Manning. 2019. A structural probe for finding syntax in word representations. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL). Association for Computational Linguistics.</p>
<p>Matthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. 2020. spaCy: Industrial-strength Natural Language Processing in Python.</p>
<p>Robin Jia and Percy Liang. 2017. Adversarial examples for evaluating reading comprehension systems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2021-2031.</p>
<p>Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423438 .</p>
<p>Nora Kassner, Philipp Dufter, and Hinrich Sch√ºtze. 2021a. Multilingual lama: Investigating knowledge in multilingual pretrained language models.</p>
<p>Nora Kassner, Benno Krojer, and Hinrich Sch√ºtze. 2020. Are pretrained language models symbolic reasoners over knowledge? In Proceedings of the 24th Conference on Computational Natural Language Learning, pages 552-564, Online. Association for Computational Linguistics.</p>
<p>Nora Kassner and Hinrich Sch√ºtze. 2020. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7811-7818, Online. Association for Computational Linguistics.</p>
<p>Nora Kassner, Oyvind Tafjord, Hinrich Sch√ºtze, and Peter Clark. 2021b. Enriching a model's notion of belief using a persistent memory. CoRR, abs/2104.08401.</p>
<p>Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332-9346.</p>
<p>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for selfsupervised learning of language representations. In International Conference on Learning Representations.</p>
<p>Tao Li, Vivek Gupta, Maitrey Mehta, and Vivek Srikumar. 2019. A logic-driven framework for consistency of neural models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3924-3935, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Tal Linzen. 2020. How can we accelerate progress towards human-like linguistic generalization? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5210-5217.</p>
<p>Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing the ability of lstms to learn syntax-sensitive dependencies. Transactions of the Association for Computational Linguistics, 4:521-535.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of Machine Learning Research, 9:2579-2605.</p>
<p>Rebecca Marvin and Tal Linzen. 2018. Targeted syntactic evaluation of language models. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1192-1202, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428-3448.</p>
<p>David Picado Mui√±o. 2011. Measuring and repairing inconsistency in probabilistic knowledge bases. International Journal of Approximate Reasoning, 52(6):828-840.</p>
<p>Kim Anh Nguyen, Sabine Schulte im Walde, and Ngoc Thang Vu. 2016. Integrating distributional lexical contrast into word embeddings for antonym-synonym distinction. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 454-459.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825-2830.</p>
<p>Matthew E Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A Smith. 2019. Knowledge enhanced contextual word representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 43-54.</p>
<p>Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt√§schel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. 2020. How context affects language models' factual predictions. In Automated Knowledge Base Construction.</p>
<p>Fabio Petroni, Tim Rockt√§schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463-2473, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Nina Poerner, Ulli Waltinger, and Hinrich Sch√ºtze. 2020. E-bert: Efficient-yet-effective entity embeddings for bert. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 803-818.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:1-67.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392.</p>
<p>Shauli Ravfogel, Yanai Elazar, Jacob Goldberger, and Yoav Goldberg. 2020. Unsupervised distillation of syntactic information from contextualized word representations. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 91-106, Online. Association for Computational Linguistics.</p>
<p>Abhilasha Ravichander, Eduard Hovy, Kaheer Suleman, Adam Trischler, and Jackie Chi Kit Cheung. 2020. On the systematicity of probing contextualized word representations: The case of hypernymy in BERT. In Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics, pages 88-102, Barcelona, Spain (Online). Association for Computational Linguistics.</p>
<p>Marco Tulio Ribeiro, Carlos Guestrin, and Sameer Singh. 2019. Are red roses red? evaluating consistency of question-answering models. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6174-6184, Florence, Italy. Association for Computational Linguistics.</p>
<p>Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Semantically equivalent adversarial rules for debugging nlp models. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 856-865.</p>
<p>Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020. Beyond accuracy: Behavioral testing of NLP models with CheckList. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4902-4912, Online. Association for Computational Linguistics.</p>
<p>Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5418-5426.</p>
<p>Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A primer in bertology: What we know about how bert works. Transactions of the Association for Computational Linguistics, 8:842-866.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4222-4235, Online. Association for Computational Linguistics.</p>
<p>Micah Shlain, Hillel Taub-Tabib, Shoval Sadde, and Yoav Goldberg. 2020. Syntactic search by example. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 1723, Online. Association for Computational Linguistics.</p>
<p>Vered Shwartz and Yejin Choi. 2020. Do neural language models overcome reporting bias? In</p>
<p>Proceedings of the 28th International Conference on Computational Linguistics, pages 68636870 .</p>
<p>Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks. In International Conference on Learning Representations.</p>
<p>Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2020. olmpics-on what language model pre-training captures. Transactions of the Association for Computational Linguistics, 8:743-758.</p>
<p>Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. Bert rediscovers the classical nlp pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593-4601.</p>
<p>Matthias Thimm. 2009. Measuring Inconsistency in Probabilistic Knowledge Bases. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI'09), pages 530-537. AUAI Press.</p>
<p>Matthias Thimm. 2013. Inconsistency measures for probabilistic logics. Artificial Intelligence, 197:1-24.</p>
<p>James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel, and Alon Halevy. 2020. Neural databases. arXiv preprint arXiv:2010.06973.</p>
<p>Alex Warstadt, Yian Zhang, Xiaocheng Li, Haokun Liu, and Samuel R. Bowman. 2020. Learning which features matter: RoBERTa acquires a preference for linguistic generalizations (eventually). In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 217-235, Online. Association for Computational Linguistics.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122. Association for Computational Linguistics.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R√©mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-theart natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Wenhan Xiong, Jingfei Du, William Yang Wang, and Veselin Stoyanov. 2020. Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, and Dan Roth. 2020. Do language embeddings capture scales? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 4889-4896.</p>
<h2>A Appendix</h2>
<p>We heavily rely on Hugging Face's Transformers library (Wolf et al., 2020) for all experiments involving the PLMs. We used Weights \&amp; Biases for tracking and logging the experiments (Biewald, 2020). Finally, we used sklearn (Pedregosa et al., 2011) for other ML-related experiments.</p>
<h2>B Paraphrases Analysis</h2>
<p>We provide a characterization of the paraphrase types included in our dataset.</p>
<p>We analyze the type of paraphrases in ParaRel $\square$. We sample 100 paraphrase pairs from the agreement evaluation that were labeled as paraphrases and annotate the paraphrase type. Notice that the paraphrases can be complex, as such, multiple transformations can be annotated for each pair. We mainly use a subset of paraphrase types categorized by Bhagat and Hovy (2013), but also define new types which were not covered by that work. We begin by briefly defining the types of paraphrases found in PARAREL from Bhagat and Hovy (2013) (more thorough definitions can be found in their paper), and then define the new types we observed.</p>
<ol>
<li>Synonym substitution: Replacing a word/phrase by a synonymous word/phrase, in the appropriate context.</li>
<li>Function word variations: Changing the function words in a sentence/phrase without affecting its semantics, in the appropriate context.</li>
<li>Converse substitution: Replacing a word/phrase with its converse and inverting the relationship between the constituents of a sentence/phrase, in the appropriate context, presenting the situation from the converse perspective.</li>
<li>Change of tense: Changing the tense of a verb, in the appropriate context.</li>
<li>Change of voice: Changing a verb from its active to passive form and vice versa results in a paraphrase of the original sentence/phrase.</li>
<li>Verb/Noun conversion: Replacing a verb by its corresponding nominalized noun form and vice versa, in the appropriate context.</li>
<li>External knowledge: Replacing a word/phrase by another word/phrase based on extralinguistic (world) knowledge, in the appropriate context.</li>
<li>Noun/Adjective conversion: Replacing a verb by its corresponding adjective form and vice versa, in the appropriate context.</li>
<li>Change of aspect: Changing the aspect of a verb, in the appropriate context.</li>
</ol>
<p>We also define several other types of paraphrases not covered in Bhagat and Hovy (2013) (potentially because they did not occur in the corpora they have inspected).
a. Irrelevant addition: addition or removal of a word or phrase, that does not affect the meaning of the sentence (as far as the relation of interest is concerned), and can be inferred from the context independently.
b. Topicalization transformation: a transformation from or to a topicalization construction. Topicalization is a construction in which a clause is moved to the beginning of its enclosing clause.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Paraphrase Type</th>
<th style="text-align: center;">Pattern #1</th>
<th style="text-align: center;">Pattern #2</th>
<th style="text-align: center;">Relation</th>
<th style="text-align: center;">N.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Synonym substitution</td>
<td style="text-align: center;">[X] died in [Y].</td>
<td style="text-align: center;">[X] expired at [Y].</td>
<td style="text-align: center;">place of death</td>
<td style="text-align: center;">41</td>
</tr>
<tr>
<td style="text-align: center;">Function words variations</td>
<td style="text-align: center;">[X] is [Y] citizen.</td>
<td style="text-align: center;">[X], who is a citizen of [Y].</td>
<td style="text-align: center;">country of citizenship</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">Converse substitution</td>
<td style="text-align: center;">[X] maintains diplomatic relations with [Y].</td>
<td style="text-align: center;">[Y] maintains diplomatic relations with [X].</td>
<td style="text-align: center;">diplomatic relation</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">Change of tense</td>
<td style="text-align: center;">[X] is developed by [Y].</td>
<td style="text-align: center;">[X] was developed by [Y].</td>
<td style="text-align: center;">developer</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">Change of voice</td>
<td style="text-align: center;">[X] is owned by [Y].</td>
<td style="text-align: center;">[Y] owns [X].</td>
<td style="text-align: center;">owned by</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: center;">Verb/Noun conversion</td>
<td style="text-align: center;">The headquarter of [X] is in [Y].</td>
<td style="text-align: center;">[X] is headquartered in [Y].</td>
<td style="text-align: center;">headquarters location</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: center;">External knowledge</td>
<td style="text-align: center;">[X] is represented by music label [Y].</td>
<td style="text-align: center;">[X], that is represented by [Y].</td>
<td style="text-align: center;">record label</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">Noun/Adjective conversion</td>
<td style="text-align: center;">The official language of [X] is [Y].</td>
<td style="text-align: center;">The official language of [X] is the [Y] language.</td>
<td style="text-align: center;">official language</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">Change of aspect</td>
<td style="text-align: center;">[X] plays in [Y] position.</td>
<td style="text-align: center;">playing as an [X], [Y]</td>
<td style="text-align: center;">position played on team</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Irrelevant addition</td>
<td style="text-align: center;">[X] shares border with [Y].</td>
<td style="text-align: center;">[X] shares a common border with [Y].</td>
<td style="text-align: center;">shares border with</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;">Topicalization transformation</td>
<td style="text-align: center;">[X] plays in [Y] position.</td>
<td style="text-align: center;">playing as a [Y], [X]</td>
<td style="text-align: center;">position played on team</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">Apposition transformation</td>
<td style="text-align: center;">[X] is the capital of [Y].</td>
<td style="text-align: center;">[Y]'s capital, [X].</td>
<td style="text-align: center;">capital of</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">Other syntactic movements</td>
<td style="text-align: center;">[X] and [Y] are twin cities.</td>
<td style="text-align: center;">[X] is a twin city of [Y].</td>
<td style="text-align: center;">twinned administrative body</td>
<td style="text-align: center;">10</td>
</tr>
</tbody>
</table>
<p>Table 8: Different types of paraphrases in ParaRel . We report examples from each paraphrase type, along with the type of relation, and the number of examples from the specific transformation from a random subset of 100 pairs. Each pair can be classified into more than a single transformation (we report one for brevity), thus the sum of transformation is more than 100 .
c. Apposition transformation: a transformation from or to an apposition construction. In an apposition construction, two noun phrases where one identifies the other are placed one next to each other.
d. Other syntactic movements: includes other types of syntactic transformations that are not part of the other categories. This includes cases such as moving an element from a coordinate construction to the subject position as in the last example in Table 8. Another type of transformation is in the following paraphrase: " $[\mathrm{X}]$ plays in $[\mathrm{Y}]$ position." and " $[\mathrm{X}]$ plays in the position of $[\mathrm{Y}]$." where a compound noun-phrase is replaced with a prepositional phrase.</p>
<p>We report the percentage of each type, along with examples of paraphrases in Table 8. The most common paraphrase is the 'synonym substitution', following 'function words variations' which occurred 41 and 16 times, respectively. The least common paraphrase is 'change of aspect', which occurred only once in the sample.</p>
<p>The full ParaRel resource can be found at: https://github.com/yanaiela/ pararel/tree/main/data/pattern_ data/graphs_json.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{16}$ Many relations are location-based - not training on them prevents train-test leakage.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
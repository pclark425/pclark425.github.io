<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4870 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4870</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4870</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-03532123ccffae8d411264320e8a5ae2b6eddea0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/03532123ccffae8d411264320e8a5ae2b6eddea0" target="_blank">Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Demonstrate-Search-Predict (DSP) is proposed, a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM and can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions.</p>
                <p><strong>Paper Abstract:</strong> Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple"retrieve-then-read"pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37-120%, 8-39%, and 80-290% relative gains against the vanilla LM (GPT-3.5), a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively. We release DSP at https://github.com/stanfordnlp/dsp</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4870.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4870.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DSP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DEMONSTRATE-SEARCH-PREDICT (DSP) task-aware program</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework/program that composes a frozen language model (GPT-3.5) and a frozen retrieval model (ColBERTv2) via three stages (DEMONSTRATE, SEARCH, PREDICT) to perform retrieval-augmented in-context learning with bootstrapped demonstrations, iterative multi-hop search, fused retrieval, and aggregation (self-consistency / PoT).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Task-aware DSP program (GPT-3.5 + ColBERTv2)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A deliberate program that coordinates a frozen LM (GPT-3.5 text-davinci-002) and a frozen RM (ColBERTv2) via composable primitives (DEMONSTRATE to bootstrap intermediate labels, SEARCH to produce/rewrite queries and retrieve passages with fusion, and PREDICT to generate and aggregate answers). Supports multi-hop, conversational rewriting, branching (PoT), and self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented external memory (passage-based retrieval / vector index)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Uses ColBERTv2 to index Wikipedia dumps and retrieve top-k passages for LM-generated queries; supports fused_retrieval across multiple generated queries (CombSUM-like fusion) and uses retrieved passages as an external memory/context for the LM in PREDICT and for bootstrapping demonstrations in DEMONSTRATE.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain question answering; Multi-hop question answering; Conversational question answering</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Knowledge-intensive, open-domain QA tasks where the system must answer short questions (Open-SQuAD), reason across multiple evidence hops (HotPotQA fullwiki), or resolve conversational dependencies and retrieve relevant passages (QReCC).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Open-SQuAD; HotPotQA (fullwiki); QReCC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Open-SQuAD: EM 36.6%, F1 49.0; HotPotQA: EM 51.4%, F1 62.9; QReCC: F1 35.0, nF1 25.3 (validation/dev results reported in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Vanilla LM (no retrieval) results reported for comparison: Open-SQuAD EM 16.2%, F1 25.6; HotPotQA EM 28.3%, F1 36.4; QReCC F1 29.8, nF1 18.4.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Using retrieval as an explicit external memory in DSP produces large gains over a vanilla (no-retrieval) LM (e.g., Open-SQuAD EM +126% relative, HotPotQA EM +~82% relative) and outperforms a standard retrieve-then-read pipeline (modest but consistent gains; e.g., Open-SQuAD EM +~8% relative over retrieve-then-read). DSP also substantially outperforms the self-ask prompting instantiation in these evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Depends on retrieval quality and corpus alignment (index choice matters); constrained by LM context window when aggregating many passages/demonstrations; DEMONSTRATE bootstrapping requires the LM+RM to succeed zero-shot on at least a few examples to create demonstrations; current implementation uses frozen components and greedy/sampling decoding constraints; authors note potential gains if PREDICT aggregated many more passages (context-window bottleneck).</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Deliberate, programmatic coordination of LM and RM as modular text-passing components (rather than monolithic retrieve-then-read prompts) yields large gains on knowledge-intensive tasks; programmatic bootstrapping (DEMONSTRATE) can create intermediate labels from end-task supervision; fused retrieval and PoT/self-consistency aggregations improve recall and robustness; explicit program control avoids 'self-distraction' seen in LM-controlled decomposition approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4870.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4870.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieve-then-Read</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieve-then-Read pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard retrieval-augmented pipeline that retrieves top-k passages for the question using an RM and concatenates them into the LM prompt for answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Retrieve-then-Read (LM + RM)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Baseline pipeline that retrieves k passages (here via ColBERTv2) for each input query and concatenates those passages with few-shot demonstrations into the LM prompt (GPT-3.5) which directly produces an answer.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented external memory (passage retrieval / vector index)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Retrieves the top-k passages for the input question from a pre-indexed corpus and supplies them as context in the LM prompt (simple concatenation of retrieved passages into the prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain QA; Multi-hop QA; Conversational QA (used as baseline for same tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same knowledge-intensive open-domain QA tasks as above; retrieve-then-read assumes a relevant passage can be retrieved and directly read by the LM to answer.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Open-SQuAD; HotPotQA; QReCC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Open-SQuAD: EM 33.8%, F1 46.1; HotPotQA: EM 36.9%, F1 46.1; QReCC: F1 31.6, nF1 22.2 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Vanilla LM (no retrieval) baseline: Open-SQuAD EM 16.2%, F1 25.6; HotPotQA EM 28.3%, F1 36.4; QReCC F1 29.8, nF1 18.4.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Retrieve-then-read yields substantial improvements over a vanilla LM (e.g., Open-SQuAD EM increases from 16.2% to 33.8%), but DSP's more structured interactions (multi-hop query generation, fusion, bootstrapped demonstrations, aggregation via self-consistency) further improve performance modestly over this baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Rigid single-step retrieval often fails on multi-hop or conversational queries when a single passage cannot answer the question; simple concatenation can be limited by context-window constraints; lacks modular multi-hop planning and fusion strategies, leading to lower recall on complex queries.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Simple retrieval grounding helps substantially, but more sophisticated, modular coordination between LM and RM (as in DSP) better handles multi-hop and conversational cases and reduces failure modes caused by poor single-step retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4870.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4870.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-ask</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-ask prompting technique</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting approach where the LM generates follow-up sub-questions which are answered by a search engine (or retriever) and those answers are used to answer the original question.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Measuring and narrowing the compositionality gap in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Self-ask (LM + search)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>The LM is prompted to decompose a complex question into follow-up questions; those follow-ups are sent to a search engine / retriever, answers are concatenated, and the LM produces a final answer. The paper evaluated a version of self-ask using ColBERTv2 as the search component.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented external memory (search engine / retriever)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>LM-generated follow-up questions are intercepted and sent to the retriever (ColBERTv2 here) to fetch passages/answers; the retrieved answers are appended into the prompt and used by the LM to answer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain QA; Multi-hop QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used for knowledge-intensive questions requiring decomposition into sub-questions whose answers can be retrieved and recomposed to answer the original question.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Open-SQuAD; HotPotQA (evaluated in this paper as a comparator)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Self-ask w/ ColBERTv2: Open-SQuAD EM 9.3%, F1 17.2; HotPotQA EM 25.2%, F1 33.2. With a refined prompt configuration reported by authors: Open-SQuAD EM 9.0%, F1 15.7; HotPotQA EM 28.6%, F1 37.3 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Self-ask underperformed both retrieve-then-read and DSP in these evaluations; DSP outperformed self-ask by large margins (the paper reports DSP improving over self-ask by ~80%–290% relative in some settings).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Prone to 'self-distraction' where the LM decomposes the question into tangential or ill-formed follow-ups (e.g., asking what an entity is rather than the required attribute), sensitive to hand-written prompt examples and casing/formatting, and less modular (control flow resides inside LM completions) leading to brittle behavior outside its intended setting.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Interception-and-search approaches where LM holds control-flow inside its completion are vulnerable to distraction and tangential decompositions; explicit programmatic control of decomposition (as in DSP) mitigates these failure modes and yields stronger empirical performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4870.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4870.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (vanilla LM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (text-davinci-002) vanilla in-context learner</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A frozen large language model used as the LM in experiments; used both as a vanilla few-shot in-context learner (no retrieval) and as the LM component inside DSP and other retrieval-augmented pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Vanilla GPT-3.5 few-shot LM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A frozen autoregressive LM (text-davinci-002) used with few-shot in-context demonstrations to directly answer questions without external retrieval in the vanilla baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>parametric memory only (no external/retrieval memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Relies on knowledge stored in model parameters and the few-shot demonstration context; no external passage retrieval used in the vanilla configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain QA; Multi-hop QA; Conversational QA (baseline comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard few-shot in-context prompting to answer questions directly from LM parameters and demonstrations; struggles on knowledge-intensive or multi-hop queries.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Open-SQuAD; HotPotQA; QReCC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Open-SQuAD: EM 16.2%, F1 25.6; HotPotQA: EM 28.3%, F1 36.4; QReCC: F1 29.8, nF1 18.4 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Parametric-only memory (vanilla LM) is substantially weaker on knowledge-intensive tasks compared to retrieval-augmented approaches; grounding with retrieved passages improves factuality and accuracy markedly.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Prone to hallucination and unreliable memorized facts; insufficient for many open-domain, time-sensitive, or multi-hop queries without retrieval grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Freezing a large LM and augmenting it with retrieval (external memory) substantially improves performance on knowledge-intensive tasks compared to relying solely on parametric knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4870.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4870.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ColBERTv2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ColBERTv2 retrieval model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A late-interaction passage retriever used as the RM in all retrieval-augmented experiments, chosen for strong zero-shot retrieval quality and efficient search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ColBERTv2: Effective and efficient retrieval via lightweight late interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ColBERTv2 (retriever / RM)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A late-interaction dense passage retriever that indexes corpora (Wikipedia dumps) and returns top-k passages given free-form text queries; used to provide external memory to LM-based programs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external retrieval memory (vector index / passage index)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Indexes multiple Wikipedia dumps and retrieves the top-k most relevant text passages for LM-generated queries; supports fused_retrieval by combining results from multiple queries (e.g., CombSUM variant).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Support for retrieval in Open-domain QA, Multi-hop QA, Conversational QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provides passage retrieval used by DSP, retrieve-then-read, and self-ask pipelines to ground LM predictions and to bootstrap demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Used across Open-SQuAD; HotPotQA; QReCC experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Authors attribute a substantial portion of retrieval-augmented gains to strong off-the-shelf retrievers like ColBERTv2; retrieval quality and index choice materially affect downstream LM performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>If the RM fails to retrieve relevant passages the pipeline (including retrieve-then-read) can fail; retrieval depends on corpus date/coverage (corpus alignment matters), and retrieval errors propagate to LM predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>High-quality zero-shot retrievers enable frozen LM + RM pipelines to be effective without fine-tuning; fused retrieval across multiple LM-generated queries increases recall and robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Measuring and narrowing the compositionality gap in language models. <em>(Rating: 2)</em></li>
                <li>Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval. <em>(Rating: 2)</em></li>
                <li>Leveraging passage retrieval with generative models for open domain question answering. <em>(Rating: 2)</em></li>
                <li>ColBERTv2: Effective and efficient retrieval via lightweight late interaction. <em>(Rating: 2)</em></li>
                <li>Attributed text generation via post-hoc research and revision. <em>(Rating: 1)</em></li>
                <li>STAR: Bootstrapping reasoning with reasoning. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4870",
    "paper_id": "paper-03532123ccffae8d411264320e8a5ae2b6eddea0",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "DSP",
            "name_full": "DEMONSTRATE-SEARCH-PREDICT (DSP) task-aware program",
            "brief_description": "A framework/program that composes a frozen language model (GPT-3.5) and a frozen retrieval model (ColBERTv2) via three stages (DEMONSTRATE, SEARCH, PREDICT) to perform retrieval-augmented in-context learning with bootstrapped demonstrations, iterative multi-hop search, fused retrieval, and aggregation (self-consistency / PoT).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Task-aware DSP program (GPT-3.5 + ColBERTv2)",
            "agent_description": "A deliberate program that coordinates a frozen LM (GPT-3.5 text-davinci-002) and a frozen RM (ColBERTv2) via composable primitives (DEMONSTRATE to bootstrap intermediate labels, SEARCH to produce/rewrite queries and retrieve passages with fusion, and PREDICT to generate and aggregate answers). Supports multi-hop, conversational rewriting, branching (PoT), and self-consistency.",
            "memory_type": "retrieval-augmented external memory (passage-based retrieval / vector index)",
            "memory_description": "Uses ColBERTv2 to index Wikipedia dumps and retrieve top-k passages for LM-generated queries; supports fused_retrieval across multiple generated queries (CombSUM-like fusion) and uses retrieved passages as an external memory/context for the LM in PREDICT and for bootstrapping demonstrations in DEMONSTRATE.",
            "task_name": "Open-domain question answering; Multi-hop question answering; Conversational question answering",
            "task_description": "Knowledge-intensive, open-domain QA tasks where the system must answer short questions (Open-SQuAD), reason across multiple evidence hops (HotPotQA fullwiki), or resolve conversational dependencies and retrieve relevant passages (QReCC).",
            "benchmark_name": "Open-SQuAD; HotPotQA (fullwiki); QReCC",
            "performance_with_memory": "Open-SQuAD: EM 36.6%, F1 49.0; HotPotQA: EM 51.4%, F1 62.9; QReCC: F1 35.0, nF1 25.3 (validation/dev results reported in Table 1).",
            "performance_without_memory": "Vanilla LM (no retrieval) results reported for comparison: Open-SQuAD EM 16.2%, F1 25.6; HotPotQA EM 28.3%, F1 36.4; QReCC F1 29.8, nF1 18.4.",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Using retrieval as an explicit external memory in DSP produces large gains over a vanilla (no-retrieval) LM (e.g., Open-SQuAD EM +126% relative, HotPotQA EM +~82% relative) and outperforms a standard retrieve-then-read pipeline (modest but consistent gains; e.g., Open-SQuAD EM +~8% relative over retrieve-then-read). DSP also substantially outperforms the self-ask prompting instantiation in these evaluations.",
            "limitations_or_challenges": "Depends on retrieval quality and corpus alignment (index choice matters); constrained by LM context window when aggregating many passages/demonstrations; DEMONSTRATE bootstrapping requires the LM+RM to succeed zero-shot on at least a few examples to create demonstrations; current implementation uses frozen components and greedy/sampling decoding constraints; authors note potential gains if PREDICT aggregated many more passages (context-window bottleneck).",
            "key_insights": "Deliberate, programmatic coordination of LM and RM as modular text-passing components (rather than monolithic retrieve-then-read prompts) yields large gains on knowledge-intensive tasks; programmatic bootstrapping (DEMONSTRATE) can create intermediate labels from end-task supervision; fused retrieval and PoT/self-consistency aggregations improve recall and robustness; explicit program control avoids 'self-distraction' seen in LM-controlled decomposition approaches.",
            "uuid": "e4870.0",
            "source_info": {
                "paper_title": "Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Retrieve-then-Read",
            "name_full": "Retrieve-then-Read pipeline",
            "brief_description": "A standard retrieval-augmented pipeline that retrieves top-k passages for the question using an RM and concatenates them into the LM prompt for answer generation.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Retrieve-then-Read (LM + RM)",
            "agent_description": "Baseline pipeline that retrieves k passages (here via ColBERTv2) for each input query and concatenates those passages with few-shot demonstrations into the LM prompt (GPT-3.5) which directly produces an answer.",
            "memory_type": "retrieval-augmented external memory (passage retrieval / vector index)",
            "memory_description": "Retrieves the top-k passages for the input question from a pre-indexed corpus and supplies them as context in the LM prompt (simple concatenation of retrieved passages into the prompt).",
            "task_name": "Open-domain QA; Multi-hop QA; Conversational QA (used as baseline for same tasks)",
            "task_description": "Same knowledge-intensive open-domain QA tasks as above; retrieve-then-read assumes a relevant passage can be retrieved and directly read by the LM to answer.",
            "benchmark_name": "Open-SQuAD; HotPotQA; QReCC",
            "performance_with_memory": "Open-SQuAD: EM 33.8%, F1 46.1; HotPotQA: EM 36.9%, F1 46.1; QReCC: F1 31.6, nF1 22.2 (Table 1).",
            "performance_without_memory": "Vanilla LM (no retrieval) baseline: Open-SQuAD EM 16.2%, F1 25.6; HotPotQA EM 28.3%, F1 36.4; QReCC F1 29.8, nF1 18.4.",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Retrieve-then-read yields substantial improvements over a vanilla LM (e.g., Open-SQuAD EM increases from 16.2% to 33.8%), but DSP's more structured interactions (multi-hop query generation, fusion, bootstrapped demonstrations, aggregation via self-consistency) further improve performance modestly over this baseline.",
            "limitations_or_challenges": "Rigid single-step retrieval often fails on multi-hop or conversational queries when a single passage cannot answer the question; simple concatenation can be limited by context-window constraints; lacks modular multi-hop planning and fusion strategies, leading to lower recall on complex queries.",
            "key_insights": "Simple retrieval grounding helps substantially, but more sophisticated, modular coordination between LM and RM (as in DSP) better handles multi-hop and conversational cases and reduces failure modes caused by poor single-step retrieval.",
            "uuid": "e4870.1",
            "source_info": {
                "paper_title": "Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Self-ask",
            "name_full": "Self-ask prompting technique",
            "brief_description": "A prompting approach where the LM generates follow-up sub-questions which are answered by a search engine (or retriever) and those answers are used to answer the original question.",
            "citation_title": "Measuring and narrowing the compositionality gap in language models.",
            "mention_or_use": "use",
            "agent_name": "Self-ask (LM + search)",
            "agent_description": "The LM is prompted to decompose a complex question into follow-up questions; those follow-ups are sent to a search engine / retriever, answers are concatenated, and the LM produces a final answer. The paper evaluated a version of self-ask using ColBERTv2 as the search component.",
            "memory_type": "retrieval-augmented external memory (search engine / retriever)",
            "memory_description": "LM-generated follow-up questions are intercepted and sent to the retriever (ColBERTv2 here) to fetch passages/answers; the retrieved answers are appended into the prompt and used by the LM to answer.",
            "task_name": "Open-domain QA; Multi-hop QA",
            "task_description": "Used for knowledge-intensive questions requiring decomposition into sub-questions whose answers can be retrieved and recomposed to answer the original question.",
            "benchmark_name": "Open-SQuAD; HotPotQA (evaluated in this paper as a comparator)",
            "performance_with_memory": "Self-ask w/ ColBERTv2: Open-SQuAD EM 9.3%, F1 17.2; HotPotQA EM 25.2%, F1 33.2. With a refined prompt configuration reported by authors: Open-SQuAD EM 9.0%, F1 15.7; HotPotQA EM 28.6%, F1 37.3 (Table 1).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Self-ask underperformed both retrieve-then-read and DSP in these evaluations; DSP outperformed self-ask by large margins (the paper reports DSP improving over self-ask by ~80%–290% relative in some settings).",
            "limitations_or_challenges": "Prone to 'self-distraction' where the LM decomposes the question into tangential or ill-formed follow-ups (e.g., asking what an entity is rather than the required attribute), sensitive to hand-written prompt examples and casing/formatting, and less modular (control flow resides inside LM completions) leading to brittle behavior outside its intended setting.",
            "key_insights": "Interception-and-search approaches where LM holds control-flow inside its completion are vulnerable to distraction and tangential decompositions; explicit programmatic control of decomposition (as in DSP) mitigates these failure modes and yields stronger empirical performance.",
            "uuid": "e4870.2",
            "source_info": {
                "paper_title": "Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "GPT-3.5 (vanilla LM)",
            "name_full": "GPT-3.5 (text-davinci-002) vanilla in-context learner",
            "brief_description": "A frozen large language model used as the LM in experiments; used both as a vanilla few-shot in-context learner (no retrieval) and as the LM component inside DSP and other retrieval-augmented pipelines.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Vanilla GPT-3.5 few-shot LM",
            "agent_description": "A frozen autoregressive LM (text-davinci-002) used with few-shot in-context demonstrations to directly answer questions without external retrieval in the vanilla baseline.",
            "memory_type": "parametric memory only (no external/retrieval memory)",
            "memory_description": "Relies on knowledge stored in model parameters and the few-shot demonstration context; no external passage retrieval used in the vanilla configuration.",
            "task_name": "Open-domain QA; Multi-hop QA; Conversational QA (baseline comparisons)",
            "task_description": "Standard few-shot in-context prompting to answer questions directly from LM parameters and demonstrations; struggles on knowledge-intensive or multi-hop queries.",
            "benchmark_name": "Open-SQuAD; HotPotQA; QReCC",
            "performance_with_memory": null,
            "performance_without_memory": "Open-SQuAD: EM 16.2%, F1 25.6; HotPotQA: EM 28.3%, F1 36.4; QReCC: F1 29.8, nF1 18.4 (Table 1).",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Parametric-only memory (vanilla LM) is substantially weaker on knowledge-intensive tasks compared to retrieval-augmented approaches; grounding with retrieved passages improves factuality and accuracy markedly.",
            "limitations_or_challenges": "Prone to hallucination and unreliable memorized facts; insufficient for many open-domain, time-sensitive, or multi-hop queries without retrieval grounding.",
            "key_insights": "Freezing a large LM and augmenting it with retrieval (external memory) substantially improves performance on knowledge-intensive tasks compared to relying solely on parametric knowledge.",
            "uuid": "e4870.3",
            "source_info": {
                "paper_title": "Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "ColBERTv2",
            "name_full": "ColBERTv2 retrieval model",
            "brief_description": "A late-interaction passage retriever used as the RM in all retrieval-augmented experiments, chosen for strong zero-shot retrieval quality and efficient search.",
            "citation_title": "ColBERTv2: Effective and efficient retrieval via lightweight late interaction.",
            "mention_or_use": "use",
            "agent_name": "ColBERTv2 (retriever / RM)",
            "agent_description": "A late-interaction dense passage retriever that indexes corpora (Wikipedia dumps) and returns top-k passages given free-form text queries; used to provide external memory to LM-based programs.",
            "memory_type": "external retrieval memory (vector index / passage index)",
            "memory_description": "Indexes multiple Wikipedia dumps and retrieves the top-k most relevant text passages for LM-generated queries; supports fused_retrieval by combining results from multiple queries (e.g., CombSUM variant).",
            "task_name": "Support for retrieval in Open-domain QA, Multi-hop QA, Conversational QA",
            "task_description": "Provides passage retrieval used by DSP, retrieve-then-read, and self-ask pipelines to ground LM predictions and to bootstrap demonstrations.",
            "benchmark_name": "Used across Open-SQuAD; HotPotQA; QReCC experiments",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Authors attribute a substantial portion of retrieval-augmented gains to strong off-the-shelf retrievers like ColBERTv2; retrieval quality and index choice materially affect downstream LM performance.",
            "limitations_or_challenges": "If the RM fails to retrieve relevant passages the pipeline (including retrieve-then-read) can fail; retrieval depends on corpus date/coverage (corpus alignment matters), and retrieval errors propagate to LM predictions.",
            "key_insights": "High-quality zero-shot retrievers enable frozen LM + RM pipelines to be effective without fine-tuning; fused retrieval across multiple LM-generated queries increases recall and robustness.",
            "uuid": "e4870.4",
            "source_info": {
                "paper_title": "Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Measuring and narrowing the compositionality gap in language models.",
            "rating": 2
        },
        {
            "paper_title": "Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval.",
            "rating": 2
        },
        {
            "paper_title": "Leveraging passage retrieval with generative models for open domain question answering.",
            "rating": 2
        },
        {
            "paper_title": "ColBERTv2: Effective and efficient retrieval via lightweight late interaction.",
            "rating": 2
        },
        {
            "paper_title": "Attributed text generation via post-hoc research and revision.",
            "rating": 1
        },
        {
            "paper_title": "STAR: Bootstrapping reasoning with reasoning.",
            "rating": 1
        }
    ],
    "cost": 0.01727725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>DEMONSTRATE-SEARCH-PREDICT: Composing retrieval and language models for knowledge-intensive NLP</h1>
<p>Omar Khattab ${ }^{1}$ Keshav Santhanam ${ }^{1}$ Xiang Lisa Li ${ }^{1}$ David Hall ${ }^{1}$<br>Percy Liang ${ }^{1}$ Christopher Potts ${ }^{1}$ Matei Zaharia ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple "retrieve-then-read" pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose DemonSTRATE-SEARCH-PREDICT (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art incontext learning results and delivering 37-120\%, $8-39 \%$, and $80-290 \%$ relative gains against the vanilla LM (GPT-3.5), a standard retrieve-thenread pipeline, and a contemporaneous self-ask pipeline, respectively. We release DSP at https: //github.com/stanfordnlp/dsp.</p>
<h2>1. Introduction</h2>
<p>In-context learning adapts a frozen language model (LM) to tasks by conditioning the LM on a textual prompt including task instructions and a few demonstrating examples (McCann et al., 2018; Radford et al., 2019; Brown et al., 2020). For knowledge-intensive tasks such as question answering, fact checking, and information-seeking dialogue, retrieval models (RM) are increasingly used to augment prompts</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. A comparison between three systems based on GPT3.5 (text-davinci-002). On its own, the LM often makes false assertions. An increasingly popular retrieve-then-read pipeline fails when simple search can't find an answer. In contrast, a taskaware DSP program successfully decomposes the problem and produces a correct response. Texts edited for presentation.
with relevant information from a large corpus (Lazaridou et al., 2022; Press et al., 2022; Khot et al., 2022).</p>
<p>Recent work has shown such retrieval-augmented in-context learning to be effective in simple "retrieve-then-read" pipelines: a query is fed to the RM and the retrieved passages become part of a prompt that provides context for the LM to use in its response. In this work, we argue that the fact that both LMs and RMs consume (and generate or retrieve) natural language texts creates an opportunity for much more sophisticated interactions between them. Fully realizing this would be transformative: frozen LMs and RMs could serve as infrastructure across tasks, enabling ML- and domain-experts alike to rapidly build grounded AI systems at a high level of abstraction and with lower deployment overheads and annotation costs.</p>
<p>Figure 1 begins to illustrate the power of retrievalaugmented in-context learning, but also the limitations of "retrieve-then-read" (Lazaridou et al., 2022; Izacard et al., 2022). Our query is "How many storeys are in the castle David Gregory inherited?" When prompted to answer this, GPT-3.5 (text-davinci-002; Ouyang et al. 2022) makes up a fictitious castle with incorrect attributes, highlighting the common observation that knowledge stored in LM parameters is often unreliable (Shuster et al., 2021; Ishii et al., 2022). Introducing an RM component helps, as the LM can ground its responses in retrieved passages, but a rigid</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. A toy example of a DSP program for multi-hop question answering. Given an input question and a 2-shot training set, the DEMONSTRATE stage programmatically annotates intermediate transformations on the training examples using a form of weak supervision. Learning from a resulting demonstration, the SEARCH stage decomposes the complex input question and retrieves supporting information over two retrieval hops. Finally, the PREDICT stage uses the demonstration and retrieved passages to answer the question.
retrieve-then-read strategy fails because the RM cannot find passages that directly answer the question.</p>
<p>We introduce the Demonstrate-SEARCH-Predict (DSP) framework for in-context learning, which relies entirely on passing natural language text (and scores) between a frozen RM and LM. DSP introduces a number of composable functions that bootstrap training examples (DEMONSTRATE), gather information from a knowledge corpus (SEARCH), and generate grounded outputs (PREDICT), using them to systematically unify techniques from the retrieval-augmented NLP and the in-context learning literatures (Lee et al., 2019; Khattab et al., 2021a; Anantha et al., 2020; Gao et al., 2022; Izacard et al., 2022; Dohan et al., 2022; Zelikman et al., 2022; Zhang et al., 2022). We use DSP to suggest powerful strategies for knowledgeintensive tasks with compositions of these techniques. This reveals new conceptual possibilities for in-context learning in general (§2), and it allows us to present rich programs that set new state-of-the-art results (§3).</p>
<p>Figure 1 shows the path that a DSP program might take to arrive at an answer, and Figure 2 illustrates how a deliberate program achieves this. Instead of asking the LM to answer this complex question, the program's SEARCH stage uses the LM to generate a query "Which castle did David Gregory inherit?" The RM retrieves a passage saying Gregory inherited the Kinnairdy Castle. After a second search "hop" finds the castle's number of storeys, the PREDICT stage queries the LM with these passages to answer the original question. Although this program implements behaviors such as query generation, it requires no hand-labeled examples of these intermediate transformations (i.e., of the queries and passages of both retrieval hops). Instead, the DEMONSTRATE
stage uses labeled question-answer pairs to implement a form of weak supervision that programmatically annotates the transformations invoked within SEARCH and PREDICT.</p>
<p>We evaluate several DSP programs on answering questions in open-domain, multi-hop, and conversational settings. In them, we implement novel and reusable transformations such as bootstrapping annotations for all of our pipelines with weak supervision (§2.3), reliably rewriting questions to resolve conversational dependencies and iteratively decompose complex queries with summarization of intermediate hops (§2.4), and generating grounded responses from multiple passages with self-consistency (§2.5). We report preliminary results on Open-SQuAD, HotPotQA, and QReCC using the frozen LM GPT-3.5 and RM ColBERTv2 (Khattab \&amp; Zaharia, 2020; Santhanam et al., 2022b) with no fine-tuning. Our DSP programs deliver 37-120\%, 8-39\%, and $80-290 \%$ relative gains against corresponding vanilla LMs, a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline (Press et al., 2022), respectively. Future versions of this report will include additional test tasks and LM choices.</p>
<p>In summary, this work makes the following contributions. First, we argue that simple task-agnostic pipelines for incontext learning should give way to deliberate, task-aware strategies. Second, we show that this shift need not be a burden: with DSP, such strategies can be easily expressed as short programs using composable operators. Third, this composability spawns powerful capacities, like automatically annotating demonstrations for complex pipelines from end-task labels. Fourth, for three knowledge-intensive tasks, we implement rich programs that establish state-of-the-art results for in-context learning.</p>
<h2>2. DEMONSTRATE-SEARCH-PREDICT</h2>
<p>We now introduce the DSP framework and show its expressive power by suggesting a number of strategies in which the LM and RM can come together to tackle complex problems effectively. We show in $\S 3$ that such strategies outperform existing in-context learning methods. We begin by discussing the LM and RM foundation modules on which DSP is built (§2.1) and then the datatypes and control flow within DSP (§2.2). Subsequently, we discuss each of the three inference stages: DEMONSTRATE (§2.3), SEARCH (§2.4), and PREDICT (§2.5).</p>
<h3>2.1. Pretrained Modules: LM and RM</h3>
<p>A DSP program defines the communication between the language model LM and the retrieval model RM.</p>
<p>Language Model We invoke a frozen language model LM to conditionally generate (or score) text. For each invocation, the program prepares a prompt that adapts the LM to a specific function (e.g., answering questions or generating queries). A prompt often includes instructions, a few demonstrations of the desired behavior, and an input query to be answered.</p>
<p>As in Figure 2, the LM generates not only: (i) the final answer to the input question (in the PrediCt stage), but also (ii) intermediate "hop" queries to find useful information for the input question (SEARCH) as well as (iii) exemplar queries that illustrate how to produce queries for questions in the training set (DEMONSTRATE). This systematic use of the LM is a hallmark of DSP programs.</p>
<p>Retrieval Model DSP programs also invoke a frozen retrieval model RM to retrieve the top- $k$ most "relevant" text sequences for a given query. The RM can index a massive set of pre-defined passages for scalable search, and those passages can be updated without changing the retrieval parameters. The RM accepts free-form textual inputs and specializes in estimating the relevance (or similarity) of a text sequence to a query.</p>
<p>As in Figure 2, the RM is responsible for retrieving (i) passages for each query generated by the LM (during the SEARCH stage), but also (ii) passages that are used within demonstrations (DEMONSTRATE). In the latter case, the RM's contributions are less about providing directly relevant information to the input question and more about helping the LM adapt to the domain and task.</p>
<p>Though not utilized in this example, the RM is also used in DSP for functions like retrieving "nearest-neighbor" demonstrations from task training data (DEMONSTRATE) and selecting well-grounded generated sequences from the LM (PREDICT).</p>
<h3>2.2. Datatypes and Control Flow</h3>
<p>We have implemented the DSP framework in Python. The present section introduces the core data types and composable functions provided by the framework. We use illustrative code snippets to ground the examples, and to convey the power that comes from being able to express complex interactions between the LM and RM in simple programs.</p>
<p>The Example Datatype To conduct a task, a DSP program manipulates one or more instances of the Example datatype. An Example behaves like a Python dictionary with multiple fields. The program is typically provided with a few training examples. The code snippet below illustrates this for multi-hop question answering.</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">dsp</span><span class="w"> </span><span class="kn">import</span> <span class="n">Example</span>
<span class="n">train</span> <span class="o">=</span> <span class="p">[</span><span class="n">Example</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="s2">&quot;When was the discoverer</span>
    <span class="n">of</span> <span class="n">Palomar</span> <span class="mi">4</span> <span class="n">born</span><span class="err">?</span><span class="s2">&quot;, answer=&quot;</span><span class="mi">1889</span><span class="s2">&quot;),</span>
            <span class="n">Example</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="s2">&quot;In which city did Akeem</span>
        <span class="n">Ellis</span> <span class="n">play</span> <span class="ow">in</span> <span class="mi">2017</span><span class="err">?</span><span class="s2">&quot;, answer=&quot;</span><span class="n">Ellesmere</span> <span class="n">Port</span><span class="s2">&quot;)]</span>
</code></pre></div>

<p>This snippet contains two labeled examples, each with a multi-hop question (e.g., "In which city did Akeem Ellis play in 2017?") and its short answer ("Ellesmere Port"). Arbitrary keys and values are allowed within an Example, though typical values are strings or lists of strings.</p>
<p>In this task, we are unlikely to find an individual passage that provides the answer to any question. For example, the first training example can probably be resolved only by first answering the question of who discovered Palomar ("Edwin Hubble") and then addressing the question of Hubble's birth date using different evidence passages. We typically assume that the human-labeled training data do not include labels for intermediate transformations (e.g., queries for individual hops) that would be useful for following these steps, and so it is the job of the DSP program to discover these strategies via in-context learning.</p>
<p>A DSP Program The following code snippet is a complete program for resolving multi-hop questions like those in Figure 1, with help from train examples like those above.</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">multihop_program</span><span class="p">(</span><span class="nl">question:</span><span class="w"> </span><span class="n">str</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nl">str:</span>
<span class="w">    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Example</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="n">question</span><span class="p">,</span><span class="w"> </span><span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">)</span>
<span class="w">    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">multihop_demonstrate</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">multihop_search</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">multihop_predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">answer</span>
<span class="n">multihop_program</span><span class="p">(</span><span class="s">&quot;How many storeys does the castle</span>
<span class="w">    </span><span class="n">David</span><span class="w"> </span><span class="n">Gregory</span><span class="w"> </span><span class="n">inherited</span><span class="w"> </span><span class="n">have</span><span class="o">?</span><span class="s">&quot;)</span>
<span class="p">#</span><span class="w"> </span><span class="o">=&gt;</span><span class="w"> </span><span class="s">&quot;Five storeys&quot;</span>
</code></pre></div>

<p>The program takes the input (here, a question) and outputs the system output (its short answer). It starts by creating an Example for the input question and assigning the train field to the training set from the previous snippet. Programs</p>
<p>invoke and compose DSP primitives (i.e., built-in functions) to build the DEMONSTRATE, SEARCH, and PREDICT transformations that define the program.</p>
<p>Transformations A transformation is a function that takes an Example as input and returns an Example, populating new fields (or modifying existing fields) in it. This program invokes three developer-defined transformations, namely, multihop_demonstrate, multihop_search, and multihop_predict. Transformations may themselves invoke other transformations, and they act analogously to layers in standard deep neural network (DNN) programming frameworks such as PyTorch, except that they pass text data instead of tensors between each other and do not involve backpropagation.</p>
<p>We categorize transformations according to their behavior (or purpose) under one of the DEMONSTRATE, SEARCH, and PREDICT stages. That said, DSP does not impose this categorization and allows us to define functions that may blend these stages. We will discuss each of the three stages next.</p>
<h3>2.3. DEMONSTRATE</h3>
<p>It is known that including examples of the desired behavior from the LM in its prompt typically leads to better performance (Brown et al., 2020). In DSP, a demonstration is a training example that has been prepared to illustrate specific desired behaviors from the LM. A DemonSTRATE transformation takes as input $x$ of type Example and prepares a list of demonstrations in $x$. demos, typically by selecting a subset of the training examples in $x$. train and bootstrapping new fields in them.</p>
<p>Bootstrapping Demonstrations Examples in the training set typically consist of the input text and the target output of the task. The DemonSTRATE stage can augment a training example by programmatically bootstrapping annotations for intermediate transformations. In our running "multi-hop" example, the demonstrations illustrate three LM-based transformations: (i) how to break down the input question in order to gather information for answering it (i.e., first-hop retrieval), (ii) how to use information gathered in an earlier "hop" to ask follow-up questions, and (iii) how to use the information gathered to answer complex questions.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Examples</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">list</span><span class="o">[</span><span class="n">Example</span><span class="o">]</span>
<span class="n">Transformation</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Callable</span><span class="o">[</span><span class="n">[Example</span><span class="o">]</span><span class="p">,</span>
<span class="w">    </span><span class="n">Optional</span><span class="o">[</span><span class="n">Example</span><span class="o">]</span><span class="err">]</span>
<span class="w">    </span><span class="mi">5</span>
<span class="w">    </span><span class="mi">5</span><span class="err">}\</span><span class="n">mathrm</span><span class="err">{</span><span class="w"> </span><span class="n">annotate</span><span class="p">(</span><span class="nl">train</span><span class="p">:</span><span class="w"> </span><span class="n">Examples</span><span class="p">,</span><span class="w"> </span><span class="nl">fn</span><span class="p">:</span><span class="w"> </span><span class="n">Transformation</span><span class="p">)</span>
<span class="w">    </span><span class="mi">6</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Examples</span>
</code></pre></div>

<p>Akin to a specialized map, the annotate primitive accepts a user-defined transformation fn and applies it over a list
of training examples. Whenever fn returns an example (rather than None), annotate caches the intermediate predictions (i.e., the generated queries and retrieved passages). These predictions serve as successful demonstrations for the pipeline transformations. In simple uses, fn may attempt to answer the example "zero-shot" one or more times. This is typically done by invoking the SEARCH and PREDICT stages of the program. When an answer is produced, if fn assesses it as correct, it returns a populated example in which the intermediate predictions are present.</p>
<p>Case Study The snippet below defines the function multihop_demonstrate, called in Line 3 of multihop_program, and illustrates the usage of annotate.</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">dsp</span><span class="w"> </span><span class="kn">import</span> <span class="n">sample</span><span class="p">,</span> <span class="n">annotate</span>
<span class="k">def</span><span class="w"> </span><span class="nf">attempt_example</span><span class="p">(</span><span class="n">d</span><span class="p">:</span> <span class="n">Example</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">d</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">demos</span><span class="o">=</span><span class="p">[])</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">multihop_search</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">multihop_predict</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">d</span> <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">pred</span> <span class="o">==</span> <span class="n">d</span><span class="o">.</span><span class="n">answer</span> <span class="k">else</span> <span class="kc">None</span>
<span class="k">def</span><span class="w"> </span><span class="nf">multihop_demonstrate</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Example</span><span class="p">):</span>
    <span class="n">demos</span> <span class="o">=</span> <span class="n">annotate</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">train</span><span class="p">,</span> <span class="n">attempt_example</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Example</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">demos</span><span class="o">=</span><span class="n">demos</span><span class="p">)</span>
</code></pre></div>

<p>In Line 10, multihop_demonstrate invokes annotate, which bootstraps missing fields in training examples by caching annotations from attempt_example. The transformation attempt_example takes a training example d and attempts to answer it in a zero-shot fashion: it creates a copy of $d$ with no demonstrations (Line 4; i.e., zero-shot) and invokes the multi-hop search and predict pipeline (Lines 5 and 6). Each transformation returns an updated version of d with additional fields populated. If the pipeline answers correctly (Line 7), the updated d is returned.</p>
<p>Figure 2 illustrates this behavior. DemonSTRATE transforms a training question-answer pair to a fully-populated demonstration, including fields such as hop1 and hop2 (i.e., queries for multi-hop search) as well as psg1 and psg2. When the LM is later invoked to conduct a transformation, say, generating a "second-hop" query during SEARCH, the psg1 field serves as context and the hop2 field serves as a label for this particular training example.</p>
<p>Discussion This simple case study illustrates the power of composition in the DSP abstraction. Because the pipeline is a well-defined program in which transformations communicate by passing text attached to Examples, a simple map-and-filter strategy can leverage the LM and RM to bootstrap annotations for a full pipeline from end-task labels. This is an extensible strategy, but even in its simplest form it generalizes the approaches explored recently by Zelikman et al. (2022), Wei et al. (2022), Zhang et al. (2022), and Huang et al. (2022) in which an LM self-generates chain-of-thought rationales for an individual prompt.</p>
<p>By bootstrapping pipelines, DEMONSTRATE makes it easy to explore complex strategies in SEARCH and PREDICT without writing examples for every transformation. This includes strategies that are challenging to explore without custom annotations in traditional retrieval-augmented NLP. For instance, Khattab et al. (2021a) introduces a pipeline for multi-hop reasoning that is trained with weak supervision, extending work by Lee et al. (2019) and Khattab et al. (2021b). In it, the target 3 or 4 passages that need to retrieved must be labeled but the system discovers the best order of "hops" automatically.</p>
<p>In contrast, DSP allows us to build complex pipelines without labels for intermediate steps, because we can compose programs out of small transformations. If LM and RM can accurately process such transformations "zero-shot" (i.e., without demonstrations) on at least one or two examples, these examples can be discovered with end-task labels and used as demonstrations.</p>
<p>To draw on our earlier analogy with DNN frameworks like PyTorch, DEMONSTRATE aims to replace the function of backpropagation in extensible ways by simulating the behavior of the program (corresponding to a "forward" pass) and programmatically learning from errors. In doing this with frozen models and with only end-task labels, DEMONSTRATE introduces a high degree of modularity. In particular, without hand-labeling intermediate transformations, developers may swap the training domain, update the training examples, or modify the program's strategy, and use annotate to automatically populate all of the intermediate fields for demonstrations.</p>
<p>Selecting Demonstrations It is not always possible to fit all of the training examples in the context window of the LM. DSP provides three primitives for selecting a subset of training examples, namely, sample, knn, and crossval.</p>
<div class="codehilite"><pre><span></span><code><span class="n">sample</span><span class="p">(</span><span class="nl">train</span><span class="p">:</span><span class="w"> </span><span class="n">Examples</span><span class="p">,</span><span class="w"> </span><span class="nl">k</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="p">)</span>
<span class="w">    </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Examples</span>
<span class="n">knn</span><span class="p">(</span><span class="nl">train</span><span class="p">:</span><span class="w"> </span><span class="n">Examples</span><span class="p">,</span><span class="w"> </span><span class="nf">cast</span><span class="err">:</span><span class="w"> </span><span class="n">Callable</span><span class="o">[</span><span class="n">[Example</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="nf">str</span><span class="err">]</span><span class="p">)</span>
<span class="w">    </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">fn</span><span class="p">(</span><span class="nl">example</span><span class="p">:</span><span class="w"> </span><span class="n">Example</span><span class="p">,</span><span class="w"> </span><span class="nl">k</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="p">)</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">currying</span>
<span class="w">    </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Examples</span>
<span class="n">crossval</span><span class="p">(</span><span class="nl">train</span><span class="p">:</span><span class="w"> </span><span class="n">Examples</span><span class="p">,</span><span class="w"> </span><span class="nl">n</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="p">,</span><span class="w"> </span><span class="nl">k</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="p">)</span>
<span class="w">    </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">fn</span><span class="p">(</span><span class="nl">evaluate</span><span class="p">:</span><span class="w"> </span><span class="n">Transformation</span><span class="p">)</span>
<span class="w">    </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Examples</span>
</code></pre></div>

<p>As a baseline choice, $k$ demonstrations can be randomly sampled from train using the sample primitive, an approach used by Brown et al. (2020) and much subsequent work. We can also leverage the RM's representations and select from the training set the $k$ nearest neighbors to the input text, a strategy explored by Liu et al. (2021). Another strategy is to apply cross-validation to select among a number of sampled sets of demonstrations (Perez et al., 2021). For example, given $|\operatorname{train}|=100$ training examples, crossval
would select $n$ subsets of $k=5$ examples each, and return the set with which a transformation evaluate performs best on the remaining 95 examples.</p>
<p>Compositions \&amp; Extensions By manipulating demonstrations and higher-order transformations, these simple selection and bootstrapping primitives can be combined to conduct larger novel strategies. If the training set is very large (e.g., $|\operatorname{train}|=100,000$ ), we can conduct knn to find the nearest $k=16$ examples and only annotate these, arriving at a system that learns incrementally in real-time. If the training set is moderately large (e.g., $|\operatorname{train}|=1000$ ), we can conduct crossval and cache the performance of all prompts it evaluates on each training example. At test time, we can use knn to find $k=50$ similar examples to the test input and select the prompt that performs best on these $k$ examples, producing an adaptive system that is informed by the quality of its pipeline on different types of examples.</p>
<h3>2.4. SEARCH</h3>
<p>The SEARCH stage gathers passages to support transformations conducted by the LM. We assume a large knowledge corpus-e.g., a snippet of Web, Wikipedia, or arXiv-that is divided into text passages. Providing passages to the LM facilitates factual responses, enables updating the knowledge store without retraining, and presents a transparency contract: when in doubt, users can check whether the system has faithfully used a reliable source in making a prediction.</p>
<p>In the simplest scenarios, SEARCH can directly query the RM, requesting the top- $k$ passages (from a pre-defined index) that match an input question. This baseline instantiation of SEARCH simulates retrieval in most open-domain question answering systems, which implement a "retrieve-then-read" pipeline, like Lee et al. (2019), Khattab et al. (2021b), Lazaridou et al. (2022), and many others.</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">dsp</span><span class="w"> </span><span class="kn">import</span> <span class="n">retrieve</span>
<span class="k">def</span><span class="w"> </span><span class="nf">simple_search</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">passages</span> <span class="o">=</span> <span class="n">retrieve</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">question</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">passages</span>
</code></pre></div>

<p>SEARCH Strategies In many scenarios, the complexity of the task demands more sophisticated SEARCH strategies that empower the RM to find relevant passages. Our running example (Figure 2) is one such scenario, in which we suspect examples are likely to require multi-hop reasoning in particular. Other settings, for instance, pose conversational challenges, in which the information need expressed by a user can only be resolved by taking into account previous turns in the conversation, or demand more extensive planning (Zhong et al., 2022).</p>
<p>In the retrieval-augmented NLP literature, multi-hop search (Xiong et al., 2020; Khattab et al., 2021a) and con-</p>
<p>versational search (Del Tredici et al., 2021; Raposo et al., 2022) pipelines have received much attention. These systems are typically fine-tuned with many hand-labeled query "rewrites" (Anantha et al., 2020), "decompositions" (Geva et al., 2021; Min et al., 2019), or target hops (Yang et al., 2018; Jiang et al., 2020). Supported with automatic annotations from DEMONSTRATE, the SEARCH stage allows us to simulate many such strategies and many others in terms of passing queries, passages, and demonstrations between the RM and LM. More importantly, SEARCH facilitates our vision of advanced strategies in which the LM and RM cooperate to incrementally plan a research path for which the RM gathers information and the LM identifies next steps.</p>
<p>Case Study Let us build on our running multi-hop example as a case study. We can define multihop_search_v2 (Line 4 in our core program), a slightly more advanced version of the SEARCH transformation from Figure 2. This transformation simulates the iterative retrieval component of fine-tuned retrieval-augmented systems like IRRR (Qi et al., 2020), which reads a retrieved passage in every hop and generates a search query (or a termination condition to stop hopping), and Baleen (Khattab et al., 2021a), which summarizes the information from many passages in each hop for inclusion in subsequent hops.</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">dsp</span><span class="w"> </span><span class="kn">import</span> <span class="n">generate</span>
<span class="k">def</span><span class="w"> </span><span class="nf">multihop_search_v2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">max_hops</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">x</span><span class="o">.</span><span class="n">hops</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">hop</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_hops</span><span class="p">):</span>
        <span class="n">summary</span><span class="p">,</span> <span class="n">query</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span><span class="n">hop_template</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span><span class="o">.</span><span class="n">hops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">summary</span><span class="p">,</span> <span class="n">query</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">query</span> <span class="o">==</span> <span class="s1">&#39;N/A&#39;</span><span class="p">:</span> <span class="k">break</span>
        <span class="n">passages</span> <span class="o">=</span> <span class="n">retrieve</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">x</span><span class="o">.</span><span class="n">context</span> <span class="o">=</span> <span class="p">[</span><span class="n">summary</span><span class="p">]</span> <span class="o">*</span> <span class="n">passages</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div>

<p>In multihop_search_v2, Line 7 calls the generate primitive, which invokes the LM to produce a query for each retrieval hop. The LM is conditioned on a prompt that is prepared using the hop_template template. (We discuss prompt templates and the generate primitive in §2.5.) Here, this template may be designed to generate a prompt that has the following format (e.g., for the second hop).</p>
<div class="codehilite"><pre><span></span><code><span class="nx">My</span><span class="w"> </span><span class="nx">task</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">write</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">simple</span><span class="w"> </span><span class="nx">query</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">gathers</span>
<span class="w">    </span><span class="nx">information</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">answering</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">complex</span><span class="w"> </span><span class="nx">question</span><span class="p">.</span><span class="w"> </span><span class="nx">I</span>
<span class="w">    </span><span class="nx">write</span><span class="w"> </span><span class="nx">N</span><span class="o">/</span><span class="nx">A</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">context</span><span class="w"> </span><span class="nx">contains</span><span class="w"> </span><span class="nx">all</span>
<span class="w">    </span><span class="nx">information</span><span class="w"> </span><span class="nx">required</span><span class="p">.</span>
<span class="p">(</span><span class="nx">Task</span><span class="w"> </span><span class="nx">demonstrations</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">x</span><span class="p">.</span><span class="nx">demos</span><span class="p">,</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nx">any</span><span class="p">)</span>
<span class="nx">Context</span><span class="p">:</span><span class="w"> </span><span class="p">(</span><span class="nx">x</span><span class="p">.</span><span class="nx">context</span><span class="p">)</span>
<span class="nx">Question</span><span class="p">:</span><span class="w"> </span><span class="p">(</span><span class="nx">x</span><span class="p">.</span><span class="nx">question</span><span class="p">)</span>
<span class="nx">Summary</span><span class="p">:</span><span class="w"> </span><span class="nx">Let</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">summarize</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">above</span><span class="w"> </span><span class="nx">context</span><span class="p">.</span>
<span class="w">    </span><span class="nx">___</span><span class="p">(</span><span class="nx">summary</span><span class="p">)</span>
<span class="nx">Search</span><span class="w"> </span><span class="nx">Query</span><span class="p">:</span><span class="w"> </span><span class="nx">___</span><span class="p">(</span><span class="nx">query</span><span class="p">)</span>
</code></pre></div>

<p>As shown, the LM is instructed to read the context retrieved in earlier hops and a complex question. It is then prompted to write: (i) a summary of the supplied context and (ii) a search query that gathers information for answering that question. The generated text will be extracted and assigned to the summary and query variables in (multihop_search_v2; Line 7). On Line 10, we terminate the hops if the query is "N/A". Otherwise, Line 12 retrieves $k=5$ passages using the query and Line 13 assigns the context for the subsequent hop (or for PrediCt), setting that to include the summary of all previous hops as well as the passages retrieved in the final hop so far.</p>
<p>Comparison with self-ask It may be instructive to contrast this multi-hop DSP program with the recent "selfask" (Press et al., 2022) prompting technique, which we compare against in $\S 3$. Self-ask can be thought of as a simple instantiation of DSP's SEARCH stage. In it, the LM asks one or more "follow-up questions", which are intercepted and sent to a search engine. The search engine's answers are concatenated into the prompt and are used to answer the question. This is essentially a simplified simulation of IRRR (Qi et al., 2020).</p>
<p>As a general framework, DSP can express ideas like self-ask and many other, more sophisticated pipelines as we discuss in the present section. More importantly, DSP offers a number of intrinsic advantages that lead to large empirical gains: $80 \%-290 \%$ over self-ask. For instance, DSP programs are deeply modular, which among other things means that DSP programs will annotate and construct their own demonstrations. Thus, they can be developed without labeling any of the intermediate transformations (e.g., the queries generated). In addition, the LM prompts constructed by DSP get automatically updated to align with the training data and retrieval corpus provided. In contrast, approaches like self-ask rely on a hand-written prompt with hard-coded examples.</p>
<p>Moreover, DSP assigns the control flow to an explicit program and facilitates design patterns that invoke the LM (or RM) to conduct small transformations. This allows us to build steps that are dedicated to generating one or more retrieval queries, summarizing multiple passages per hop, and answering questions. These steps are individually simpler than the self-ask prompt, yet our multi-hop DSP program deliberately composes them to build richer pipelines that are thus more reliable. In contrast, self-ask delegates the control flow to the LM completions, maintaining state within the prompt itself and intercepting follow-up questions to conduct search. We find that this paradigm leads to a "selfdistraction" problem (§3.5) that DSP programs avoid.</p>
<p>Fusing Retrieval Results For improved recall and robustness, we can also fuse the retrieval across multiple generated queries. Fusion has a long history in information</p>
<p>retrieval (Fox \&amp; Shaw, 1994; Xue \&amp; Croft, 2013; Kurland \&amp; Culpepper, 2018) and sequentially processing multiple queries was explored recently by Gao et al. (2022) for retroactively attributing text generated by LMs to citations. Inspired by these, we include a fused_retrieval primitive to DSP to offer a versatile mechanism for interacting with frozen retrievers. It accepts an optional fusion function that maps multiple retrieval lists into one. By default, DSP uses a variant of CombSUM (Fox \&amp; Shaw, 1994), assigning each passage the sum of its probabilities across retrieval lists.</p>
<p>To illustrate, the modification below generates $n=10$ queries for the transformation multihop_search_v2.</p>
<div class="codehilite"><pre><span></span><code>c = generate(hop_template, n=10)(x)
passages = fused_retrieval(c.queries, k=5)
summary = c.summaries[0] # highest-scoring summary
</code></pre></div>

<p>Compositions \&amp; Extensions To illustrate a simple composition, we can equip a chatbot with the capacity for conversational multi-hop search by combining a query rewriting step, which produces a query that encompasses all of the relevant conversational context, with the multi-hop transformation, as follows.</p>
<div class="codehilite"><pre><span></span><code>def conversational_multihop_search(x):
    x.question = generate(conv_rewriting_template)(x)
    return multihop_search_v2(x)
</code></pre></div>

<p>Similar approaches can be used for correcting spelling mistakes or implementing pseudo-relevance feedback (Cao et al., 2008; Wang et al., 2022a), in which retrieved passages are used to inform a better search query, though this has not been attempted with pretrained LMs to our knowledge.</p>
<h3>2.5. PrediCt</h3>
<p>The PrediCt stage generates the system output using demonstrations (e.g., in x.demos) and passages (e.g., in x. context). PrediCt tackles the challenges of reliably solving the downstream task, which integrates much of the work on in-context learning in general. Within DSP, it also has the more specialized function of systematically aggregating information across a large number of demonstrations, passages, and candidate predictions.</p>
<p>Generating Candidates Generally, PrediCt has to produce one or more candidate predictions for the end-task. To this end, the basic primitive in PrediCt is generate, which accepts a Template and (via currying) an Example and queries the LM to produce one or more completions, as explored earlier in §2.4. A corresponding primitive that uses the RM in this stage is rank, which accepts a query and one or more passages and returns their relevance scores.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Template</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="nl">template</span><span class="p">:</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">produce</span>
<span class="w">    </span><span class="n">prompts</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="nf">parse</span><span class="w"> </span><span class="n">completions</span>
<span class="w">    </span><span class="n">generate</span><span class="p">(</span><span class="nl">template</span><span class="p">:</span><span class="w"> </span><span class="n">Template</span><span class="p">)</span>
<span class="w">    </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">fn</span><span class="p">(</span><span class="nl">example</span><span class="p">:</span><span class="w"> </span><span class="n">Example</span><span class="p">)</span>
<span class="w">    </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Completions</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">keys</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">access</span>
<span class="w">    </span><span class="n">extracted</span><span class="w"> </span><span class="n">preds</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">scores</span>
<span class="w">    </span><span class="nf">rank</span><span class="p">(</span><span class="nl">query</span><span class="p">:</span><span class="w"> </span><span class="nf">str</span><span class="p">,</span><span class="w"> </span><span class="nl">passages</span><span class="p">:</span><span class="w"> </span><span class="n">List</span><span class="o">[</span><span class="n">str</span><span class="o">]</span><span class="p">)</span>
<span class="w">    </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">List</span><span class="o">[</span><span class="n">float</span><span class="o">]</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">keys</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">access</span>
<span class="w">    </span><span class="n">passage</span><span class="w"> </span><span class="n">texts</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">scores</span>
</code></pre></div>

<p>A Template is an object that can produce prompts, that is, map an Example to a string, and extract fields out of completions. For instance, we can map an example $x$ that has a question and retrieved passages to the following prompt:</p>
<div class="codehilite"><pre><span></span><code>My task is to answer questions using Web documents.
1
1(Task demonstrations from x.demos, if any)
2
5 Context: (x.passage)
6 Question: (x.question)
7 Rationale: Let&#39;s think step by step. __(rationale)__
8 Answer: __(answer)__
</code></pre></div>

<p>As this illustrates, the LM will be asked to generate a chain-of-thought rationale (CoT; Wei et al. 2022; Kojima et al. 2022) and an answer, and the generated text will be extracted back into the rationale and answer keys of each completion.</p>
<p>Each invocation to the LM can sample multiple candidate predictions. Selecting a "best" prediction is the subject of much work on decoding (Wiher et al., 2022; Li et al., 2022), but a frozen and general-purpose LM may not support custom modifications to decoding. Within these constraints, we present several high-level strategies for selecting predictions and aggregating information in DSP via the LM and RM.</p>
<p>Selecting Predictions Among multiple candidates, we can simply extract the most popular prediction. When a CoT is used to arrive at the answer, this is the self-consistency method of Wang et al. (2022c), which seeks to identify predictions at which multiple distinct rationales arrive.</p>
<div class="codehilite"><pre><span></span><code><span class="mi">1</span> <span class="kn">from</span><span class="w"> </span><span class="nn">dsp</span><span class="w"> </span><span class="kn">import</span> <span class="n">generate</span><span class="p">,</span> <span class="n">majority</span>
<span class="mi">2</span>
<span class="mi">3</span> <span class="k">def</span><span class="w"> </span><span class="nf">multihop_predict</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">candidates</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span><span class="n">template_qa</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">answer</span><span class="o">=</span><span class="n">majority</span><span class="p">(</span><span class="n">candidates</span><span class="p">)</span><span class="o">.</span><span class="n">answer</span><span class="p">)</span>
</code></pre></div>

<p>DSP generalizes this in two ways. First, we can sample multiple "pipelines of transformations" (PoT) within the program, rather than locally with "chains of thought" (CoT) in one transformation. These chains may even invoke different paths in the program, as illustrated below.</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">dsp</span><span class="w"> </span><span class="kn">import</span> <span class="n">branch</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pipeline</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">multihop_predict</span><span class="p">(</span><span class="n">multihop_search_v2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">PoT_program</span><span class="p">(</span><span class="n">question</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">Example</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="n">question</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">multihop_demonstrate</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">candidates</span> <span class="o">=</span> <span class="n">branch</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">answer</span><span class="o">=</span><span class="n">majority</span><span class="p">(</span><span class="n">candidates</span><span class="p">)</span><span class="o">.</span><span class="n">answer</span><span class="p">)</span>
</code></pre></div>

<p>In the snippet above, Line 10 invokes the primitive branch which samples $n$ different PoTs with a high temperature (e.g., $t=0.7$ ) and accumulates their intermediate and final predictions. In this example, our pipeline invokes multihop_search_v2 (\$2.4), which applies a variable number of retrieval hops depending on the questions generated, before doing Predict. That is, PoT_program potentially invokes multiple distinct paths in the program (i.e., with different multi-hop queries and number of hops in each) across branches. It then selects the majority answer overall.</p>
<p>DSP generalizes self-consistency in a second way. When sampling our CoTs or PoTs provides multiple candidates, we can select the top- $k$ (e.g., top-4) predictions and then compare them directly. For instance, we may prompt the LM to compare these choices as MCQ candidates, a transformation for which DEMONSTRATE can automatically prepare exemplars. This effectively simulates the LM recursion of Levine et al. (2022), though unlike their approach it does not require a large training set or updating any (prompttuning) weights. One such implementation is illustrated in openqa_predict below.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">def</span><span class="w"> </span><span class="nx">openqa_predict</span><span class="p">(</span><span class="nx">x</span><span class="p">):</span>
<span class="w">    </span><span class="nx">preds</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">generate</span><span class="p">(</span><span class="nx">template_qa</span><span class="p">,</span><span class="w"> </span><span class="nx">n</span><span class="p">=</span><span class="mi">20</span><span class="p">)(</span><span class="nx">x</span><span class="p">).</span><span class="nx">answers</span>
<span class="w">    </span><span class="nx">x</span><span class="p">.</span><span class="nx">choices</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">most_common</span><span class="p">(</span><span class="nx">preds</span><span class="p">,</span><span class="w"> </span><span class="nx">k</span><span class="p">=</span><span class="mi">4</span><span class="p">)</span>
<span class="w">    </span><span class="nx">queries</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="p">[</span><span class="nx">f</span><span class="s">&quot;(x.question) {c}&quot;</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="nx">c</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">x</span><span class="p">.</span><span class="nx">choices</span><span class="p">]</span>
<span class="w">    </span><span class="nx">x</span><span class="p">.</span><span class="nx">passages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">fused_retrieval</span><span class="p">(</span><span class="nx">queries</span><span class="p">)</span>
<span class="w">    </span><span class="nx">x</span><span class="p">.</span><span class="nx">answer</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">generate</span><span class="p">(</span><span class="nx">templateMCQ</span><span class="p">)(</span><span class="nx">x</span><span class="p">).</span><span class="nx">answer</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nx">x</span>
</code></pre></div>

<p>As an alternative comparison approach, we can invoke the RM via rank to find the prediction that is most grounded in a retrieved contexts (i.e., most similar to the concatenation of the retrieved passages) or, given an RM that can score completions (Krishna et al., 2022), simply the prediction that has the highest score given the prompt.</p>
<p>Aggregating Information When only a few demonstrations or passages are selected, we can simply concatenate them all into the prompt. For instance, GPT-3.5 text-davinci-002 has a context window of 4097 tokens, which we find to be reasonably large for accommodating several (e.g., 3-5) demonstrations, which individually include their own passages and rationales.</p>
<p>To deal with a larger number of demonstrations or passages, we can branch in parallel to process individual subsets of the passages or demonstrations and then aggregate the individual answers using one of the scoring methods presented earlier. Indeed, Lewis et al. (2020) and Lazaridou et al. (2022) have explored marginalization as a way to combine scores across passages and Le et al. (2022) ensemble prompts across demonstrations, which can be expressed in this way.</p>
<p>An alternative aggregation strategy is to accumulate information across passages sequentially, rather than independently. This is effectively how our multi-hop approach works (§2.4). Such a strategy has also been employed recently by Gao et al. (2022) for retroactively attributing text generated by LMs to citations. They generate many queries but instead of fusion (§2.4), they run their pipeline on each query and use its outputs to alter the input to subsequent queries. ${ }^{1}$</p>
<h2>3. Evaluation</h2>
<p>We now consider how to implement DSP programs for three diverse knowledge-intensive NLP tasks: open-domain question answering (QA), multi-hop QA, and conversational QA. All of these tasks are "open-domain", in the sense that systems are given a short question or participate in a multi-turn conversation without being granted access to context that answers these questions.</p>
<p>We build and evaluate intuitive compositions of the functions explored in $\S 2$ for each task. We show that, despite low development effort, the resulting DSP programs exhibit strong quality and deliver considerable empirical gains over vanilla in-context learning and a standard retrieve-then-read pipeline with in-context learning.</p>
<h3>3.1. Evaluation Methodology</h3>
<p>In this report, we consider one development dataset for each of the tasks we consider, namely, the open-domain version of SQuAD (Rajpurkar et al., 2016; Lee et al., 2019), the multi-hop HotPotQA (Yang et al., 2018) dataset in the opendomain "fullwiki" setting, and the conversational question answering QReCC (Anantha et al., 2020; Vakulenko et al., 2022) dataset, which we used for developing the DSP abstractions. We report the validation set accuracy on all three datasets and discuss them in detail §3.5.</p>
<p>Unless otherwise stated, systems are given access to 16shot training examples, that is, each DSP program can use (up to) 16 questions-or conversations, where applicablerandomly sampled from the respective training set. We</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>subsample the validation and test sets to 1000 questions (or 400 conversations, where applicable) and report average quality across five seeds where each seed fixes a single $k$ shot training set of examples. To control the language model API spending budget, each seed processes one fifth of the evaluation examples (e.g., 200 questions per seed, for a total of 1000 unique questions).</p>
<p>We also dedicate held-out test datasets (e.g., OpenNaturalQuestions; Kwiatkowski et al. 2019) and test tasks (e.g., claim verification, as in FEVER; Thorne et al. 2018) that we only use for evaluating pre-defined DSP programs rather than development. We will include these results in a future version of this report.</p>
<h3>3.2. Pretrained Modules</h3>
<p>RM We use ColBERTv2 (Santhanam et al., 2022b), a state-of-the-art retriever based on late interaction (Khattab \&amp; Zaharia, 2020). We choose ColBERTv2 for its highly effective zero-shot search quality and efficient search (Santhanam et al., 2022a). However, our DSP programs are agnostic to how the retriever represents examples or scores passages, so essentially any retriever can be used.</p>
<p>In addition, by making retrieval a first-class construct, DSP allows us to change or update the search index over time. We simulate this in our experiments by aligning each of our datasets with the nearest Wikipedia corpus among the Dec 2016 Wikipedia dump from Chen et al. 2017, the Nov 2017 Wikipedia "abstracts" dump from Yang et al. 2018, and the Dec 2018 Wikipedia dump from Karpukhin et al. 2020.</p>
<p>LM We use the GPT-3.5 (text-davinci-002; Brown et al. 2020; Ouyang et al. 2022) language model. Unless otherwise stated, we use greedy decoding when generating $n=1$ prediction. We sample with temperature $t=0.7$ when $n&gt;1$, like related work (Wang et al., 2022c).</p>
<h3>3.3. Baselines</h3>
<p>Vanilla LM The vanilla LM baselines represent the fewshot in-context learning paradigm used by Brown et al. (2020). The open-domain QA and multi-hop QA baselines randomly sample 16 demonstrations (i.e., all of the examples available to each program in our evaluation) from the training set and do not augment these demonstrations with evidence. Similarly, the conversational QA baseline samples four conversations. The vanilla baselines do not search for passages relevant to the input query.</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">vanilla_LM_QA</span><span class="p">(</span><span class="n">question</span><span class="o">:</span><span class="w"> </span><span class="n">str</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">str</span><span class="o">:</span>
<span class="w">    </span><span class="n">demos</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="w">    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Example</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="n">question</span><span class="p">,</span><span class="w"> </span><span class="n">demos</span><span class="o">=</span><span class="n">demos</span><span class="p">)</span>
<span class="w">    </span><span class="kr">return</span><span class="w"> </span><span class="n">generate</span><span class="p">(</span><span class="n">qa_template</span><span class="p">)(</span><span class="n">x</span><span class="p">).</span><span class="n">pred</span>
</code></pre></div>

<p>Retrieve-then-Read The "retrieve-then-read" baselines use the RM to support each example with a potentially relevant passage before submitting the prompt to the LM. This emulates the pipelines used by state-of-the-art open-domain question answering systems (Khattab et al., 2021b; Izacard \&amp; Grave, 2020; Hofstätter et al., 2022). In conversational QA, we concatenate the first turn and the final question, an approach that we found to perform much better than simply using the final turn. For multi-hop QA, we retrieve and concatenate two passages per question.</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">retrieve_then_read_QA</span><span class="p">(</span><span class="n">question</span><span class="o">:</span><span class="w"> </span><span class="n">str</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">str</span><span class="o">:</span>
<span class="w">    </span><span class="n">demos</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="w">    </span><span class="n">passages</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">retrieve</span><span class="p">(</span><span class="n">question</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="w">    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Example</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="n">question</span><span class="p">,</span>
<span class="w">        </span><span class="n">passages</span><span class="o">=</span><span class="n">passages</span><span class="p">,</span>
<span class="w">            </span><span class="n">demos</span><span class="o">=</span><span class="n">demos</span><span class="p">)</span>
<span class="w">    </span><span class="kr">return</span><span class="w"> </span><span class="n">generate</span><span class="p">(</span><span class="n">qa_template</span><span class="p">)(</span><span class="n">x</span><span class="p">).</span><span class="n">pred</span>
</code></pre></div>

<p>Self-ask We also compare against self-ask (Press et al., 2022), a contemporaneous pipeline that can be thought of as a specific instantiation of DSP's SEARCH stage followed by a simple Predict step. For direct comparison with our methods, we modify the self-ask control flow to query the same ColBERTv2 index used in our DSP experiments instead of Google Search. We evaluate two configurations of self-ask. The first uses the original self-ask prompt template, which contains four hand-written demonstrations. In the second configuration, we modify the prompt template to apply a number of changes that we find are empirically useful for HotPotQA. ${ }^{2}$</p>
<h3>3.4. Proposed DSP Programs</h3>
<p>We build on transformations presented in §2. Our programs for all three tasks have the following structure, illustrated for open-domain QA.</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">openqa_program</span><span class="p">(</span><span class="n">question</span><span class="o">:</span><span class="w"> </span><span class="n">str</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">str</span><span class="o">:</span>
<span class="w">    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Example</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="n">question</span><span class="p">,</span><span class="w"> </span><span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">)</span>
<span class="w">    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">openqa_demonstrate</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">openqa_search</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">openqa_predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">    </span><span class="kr">return</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">answer</span>
</code></pre></div>

<p>The exception is that the conversational QA program,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1. Development results comparing a task-aware DSP program against baseline vanilla LM and retrieve-then-read LM as well as recent and contemporaneous in-context learning approaches with and without retrieval. All of our runs use GPT-3.5 and our retrieval-based rows use ColBERTv2. The results marked with ${ }^{\mathbf{4}}$ are collected from related work as of mid-December 2022, and attributed to their individual sources in the main text. As we discuss in the main text, the marked results are not generally apples-to-apples comparisons, since they span a variety of evaluation settings. Nonetheless, we report them here as qualitative reference points.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Open-SQuAD</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">HotPotQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">QReCC</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">nF1</td>
</tr>
<tr>
<td style="text-align: center;">Vanilla LM</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">18.4</td>
</tr>
<tr>
<td style="text-align: center;">No-retrieval LM SoTA</td>
<td style="text-align: center;">$20.2^{\text {T }}$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$33.8^{\text {T }}$</td>
<td style="text-align: center;">$44.6^{\text {T }}$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;">Retrieve-then-Read</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">22.2</td>
</tr>
<tr>
<td style="text-align: center;">Self-ask w/ ColBERTv2 Search</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;">+ Refined Prompt</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;">Retrieval-augmented LM SoTA</td>
<td style="text-align: center;">$34.0^{\text {T }}$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$35.1^{\text {T }}$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;">Task-aware DSP Program</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">62.9</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">25.3</td>
</tr>
</tbody>
</table>
<p>convqa_program, accepts turns (i.e., a list of strings, representing the conversational history) instead of a single question. Unless otherwise stated, our programs default to greedy decoding during the DEMONSTRATE stage.</p>
<p>For SEARCH, our open-domain QA program uses the question directly for retrieving $k=7$ passages and concatenates these passages into our QA prompt with CoT. For PrediCt, it generates $n=20$ reasoning chains and uses self-consistency (SC; Wang et al. 2022c) to select its final prediction. For DEMONSTRATE, our open-domain QA program uses the following approach, slightly simplified for presentation. In it, the parameter $k=3$ passed to annotate requests annotating only three demonstrations, which will then be used in the prompts.</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">openqa_demonstrate</span><span class="p">(</span><span class="n">x</span><span class="o">:</span><span class="w"> </span><span class="n">Example</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Example</span><span class="o">:</span>
<span class="w">    </span><span class="n">demos</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">train</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">openqa_attempt</span><span class="p">(</span><span class="n">d</span><span class="o">:</span><span class="w"> </span><span class="n">Example</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Example</span><span class="o">:</span>
<span class="w">        </span><span class="n">d</span><span class="p">.</span><span class="n">demos</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">all_but</span><span class="p">(</span><span class="n">demos</span><span class="p">,</span><span class="w"> </span><span class="n">d</span><span class="p">)</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="kr">all</span><span class="w"> </span><span class="p">(</span><span class="n">raw</span><span class="p">)</span>
<span class="w">        </span><span class="n">examples</span><span class="w"> </span><span class="n">different</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">d</span>
<span class="w">            </span><span class="n">d</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">openqa_search</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="w">            </span><span class="nf">if</span><span class="w"> </span><span class="kr">not</span><span class="w"> </span><span class="n">passage_match</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="o">:</span><span class="w"> </span><span class="kr">return</span><span class="w"> </span><span class="n">None</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="kr">skip</span>
<span class="w">        </span><span class="n">examples</span><span class="w"> </span><span class="n">where</span><span class="w"> </span><span class="n">search</span><span class="w"> </span><span class="n">fails</span>
<span class="w">            </span><span class="n">d</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">openqa_predict</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="w"> </span><span class="n">sc</span><span class="o">=</span><span class="kr">False</span><span class="p">)</span>
<span class="w">            </span><span class="nf">if</span><span class="w"> </span><span class="kr">not</span><span class="w"> </span><span class="n">answer_match</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="o">:</span><span class="w"> </span><span class="kr">return</span><span class="w"> </span><span class="n">None</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="kr">skip</span>
<span class="w">        </span><span class="n">examples</span><span class="w"> </span><span class="n">where</span><span class="w"> </span><span class="n">predict</span><span class="w"> </span><span class="n">fails</span>
<span class="w">            </span><span class="kr">return</span><span class="w"> </span><span class="n">d</span>
<span class="w">    </span><span class="n">x</span><span class="p">.</span><span class="n">demos</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">annotate</span><span class="p">(</span><span class="n">demos</span><span class="p">,</span><span class="w"> </span><span class="n">openqa_attempt</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="w">    </span><span class="kr">return</span><span class="w"> </span><span class="n">x</span>
</code></pre></div>

<p>Our multi-hop program adopts a very similar approach for Demonstrate and PrediCt. For SEARCH, it uses the approach described in $\S 2.4$, with the following adjustments. It uses result fusion across $n=10$ queries per hop and, among the $n$ predictions, uses the summary corresponding to the largest average log-probability. It uses a fixed number of hops for HotPotQA, i.e., two hops. In each prompt (i.e.,
each hop and QA), it concatenates the summaries of all previous hops (i.e., hop 1 onwards) and a total of $k=5$ passages divided between the hops (i.e., five passages from the first hop or two passages from the first and three from the second).</p>
<p>For conversational QA, we use a simple PrediCt which generates a response with greedy decoding, conditioned on all of the previous turns of the conversation and five retrieved passages. For SEARCH, our conversational QA pipeline generates $n=10$ re-written queries (and also uses the simple query as the retrieve-and-read baseline; §3.3) and fuses them as in $\S 2.4$. We implement DEMONSTRATE similar to openqa_demonstrate, but sample only four examples (i.e., four conversational turns; instead of 16 questions as in open-domain QA) for demonstrating the task for the higherorder transformation convqa_attempt, which is passed to annotate (not shown for brevity).</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">convqa_attempt</span><span class="p">(</span><span class="n">d</span><span class="o">:</span><span class="w"> </span><span class="n">Example</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Example</span><span class="o">:</span>
<span class="w">    </span><span class="n">d</span><span class="p">.</span><span class="n">demos</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">all_but</span><span class="p">(</span><span class="n">demos</span><span class="p">,</span><span class="w"> </span><span class="n">d</span><span class="p">)</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="kr">all</span><span class="w"> </span><span class="p">(</span><span class="n">raw</span><span class="p">)</span>
<span class="w">        </span><span class="n">examples</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">don</span><span class="s">&#39;t intersect with the</span>
<span class="s">        conversation of d</span>
<span class="s">    d = convqa_search(d, k=2)</span>
<span class="s">    if max(precision(d.answer, p) for p in</span>
<span class="s">        d.passages) &lt; .8: return None # skip examples</span>
<span class="s">        where search fails</span>
<span class="s">    d = convqa_predict(d, n=20)</span>
<span class="s">    if max(F!(c.pred, d.answer) for c in</span>
<span class="s">        d.candidates) &lt; .75: return None # skip</span>
<span class="s">        examples where predict fails out of n=20</span>
<span class="s">        attempts</span>
<span class="s">    return d</span>
</code></pre></div>

<h3>3.5. Development Datasets \&amp; Results</h3>
<p>Open-SQuAD We conduct the open-domain version of SQuAD over the Wikipedia 2016 corpus from Chen et al. (2017), as processed by Khattab et al. (2021b). We use the</p>
<p>same train/validation/test splits as in Karpukhin et al. (2020) and Khattab et al. (2021b).</p>
<p>Table 1 reports the answer EM and F1. The task-aware DSP program achieves $36.6 \%$ EM, outperforming the vanilla LM baseline by $126 \%$ EM relative gains. This indicates the importance of grounding the LM's predictions in retrieval, and it shows that state-of-the-art retrievers like ColBERTv2 have the capacity to do so off-the-shelf. The proposed DSP program also achieves relative gains of $8 \%$ in EM and $6 \%$ in F1 over the retrieve-then-read pipeline, highlighting that nontrivial gains are possible by aggregating information across several retrieved passages as we do with self-consistency.</p>
<p>These in-context learning results are competitive with a number of popular fine-tuned systems. For instance, on the Open-SQuAD test set, DPR achieves $29.8 \%$ EM, well below our 16-shot DSP program. On the Open-SQuAD dev set, the powerful Fusion-in-Decoder (Izacard \&amp; Grave, 2020) "base" approach achieves approximately $36 \%$ (i.e., very similar quality to our system) when invoked with five retrieved passages. Nonetheless, with the default setting of reading 100 passages, their system reaches $48 \%$ EM in this evaluation. This may indicate that similar gains are possible for our DSP program if the PrediCt stage is made to aggregate information across many more passages.</p>
<p>For comparison, we also evaluate the self-ask pipeline, which achieves $9.3 \%$ EM, suggesting that its fixed pipeline is ineffective outside its default multi-hop setting. Studying a few examples of its errors reveals that it often decomposes questions in tangential ways and answers these questions instead. We refer to this behavior of the LMas "self-distraction", and we believe it adds evidence in favor of our design decisions in DSP. To illustrate self-distraction, when self-ask is prompted with "When does The Kidnapping of Edgardo Mortara take place?", it asks "What is The Kidnapping of Edgardo Mortara" and then asks when it was published, a tangential question. Thus, self-ask answers "1997", instead of the time The Kidnapping of Edgardo Mortara takes place (1858).</p>
<p>For reference, Table 1 also reports (as No-retrieval LM SoTA) the concurrent in-context learning results from Si et al. (2022) using code-davinci-002, who achieve $20.2 \%$ EM without retrieval and $34.0 \%$ EM with retrieval, albeit on a different sample and split of the SQuAD data. Overall, their approaches are very similar to the baselines we implement (vanilla LM and retrieve-then-read), though their retrieval-augmented approach retrieves (and concatenates into the prompt) 10 passages from a Wikipedia dump.</p>
<p>HotPotQA We use the open-domain "fullwiki" setting of HotPotQA using its official Wikipedia 2017 "abstracts" corpus. The HotPotQA test set is hidden, so we reserve the official validation set for our testing. We sub-divide
the training set into $90 \% / 10 \%$ train/validation splits. In the training (and thus validation) split, we keep only examples marked as "hard" in the original dataset, which matches the designation of the official validation and test sets.</p>
<p>We report the final answer EM and F1 in Table 1. On HotPotQA, the task-aware DSP program outperforms the baselines and existing work by very wide margins, exceeding the vanilla LM, the retrieve-then-read baseline, and the self-ask pipeline by $82 \%, 39 \%$, and $80 \%$, respectively, in EM. This highlights the effectiveness of building up more sophisticated programs that coordinate the LM and RM for the SEARCH step.</p>
<p>These results may be pegged against the evaluation on HotPotQA in a number of concurrent papers. We first compare with non-retrieval approaches, though our comparisons must be tentative due to variation in evaluation methodologies. Si et al. (2022) achieve $25.2 \%$ EM with CoT prompting. With a "recite-and-answer" technique for PaLM-62B (Chowdhery et al., 2022), Sun et al. (2022) achieve 26.5\% EM. Wang et al. (2022b) achieve $33.8 \%$ EM and 44.6 F1 when applying a self-consistency prompt for PaLM-540B. Next, we compare with a contemporaneous retrieval-based approach: Yao et al. (2022) achieve $35.1 \%$ EM using a system capable of searching using a Wikipedia API. All of these approaches trail our task-aware DSP program, which achieves $51.4 \%$ EM, by large margins.</p>
<p>QReCC We use QReCC (Anantha et al., 2020) in an opendomain setting over Wikipedia 2018. QReCC does not have an official development set, so we sub-divide the training set into $90 \% / 10 \%$ train/validation splits. For the first question in every conversation, we use the rewritten question as the original question often assumes access to a groundtruth document. We also filter low-quality examples from QReCC. ${ }^{3}$</p>
<p>We conduct the QReCC conversations in an auto-regressive manner. At turn $t&gt;1$ of a particular conversation, the system sees its own responses (i.e., not the ground truth responses) to previous turns of the conversation. We report the novel-F1 metric (nF1; Paranjape et al. 2022), which computes the F1 overlap between the system response and the ground truth while discounting common stopwords and terms present in the question (or earlier questions). The results are shown in Table 1, and follow the same general pattern as SQuAD and HotPotQA.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>4. Conclusion</h2>
<p>For a long time, the dominant paradigm for building models in AI has centered around multiplication of tensor representations, and in the deep learning era this has given rise to highly modular (layer-wise) designs that allow for fast development and wide exploration. However, these design paradigms require extensive domain expertise, and even experts face substantial challenges when it comes to combining different pretrained components into larger systems.</p>
<p>The promise of in-context learning is that we can build complex systems from pretrained components using only natural language as the medium for giving systems instructions and, as we argue for, allowing components to communicate with each other. In this new paradigm, the building blocks are pretrained models and the core operations are natural language instructions and operations on natural language texts. If we can realize this potential, then we can broaden participation in AI system development, rapidly prototype systems for new domains, and maximize the value of specialized pretrained components.</p>
<p>In the current paper, we introduced the Demonstrate-SEARCH-PREDICT (DSP) framework for retrieval augmented in-context learning. DSP consists of a number of simple, composable functions for implementing in-context learning systems as deliberate programs-instead of endtask prompts-for solving knowledge intensive tasks. We implemented DSP as a Python library and used it to write programs for Open-SQuAD, HotPotQA, and QReCC. These programs deliver substantial gains over previous in-context learning approaches. However, beyond any particular performance number, we argue that the central contribution of DSP is in helping to reveal a very large space of conceptual possibilities for in-context learning in general.</p>
<h2>Acknowledgements</h2>
<p>We thank Ashwin Paranjape, Amir Ziai, and Rick Battle for valuable discussions and feedback. This work was partially supported by IBM as a founding member of the Stanford Institute for Human-Centered Artificial Intelligence (HAI). This research was supported in part by affiliate members and other supporters of the Stanford DAWN project-Ant Financial, Facebook, Google, and VMware-as well as Cisco, SAP, and the NSF under CAREER grant CNS-1651570. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. We thank Giuseppe Attanasio for his public $\mathrm{ET}_{\mathrm{E}} \mathrm{X}$ GitHub-style Python code formatting gist. ${ }^{4}$ We also thank Riley Goodside for his public tips on formatting LM</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>prompts (at @goodside on Twitter).</p>
<h2>References</h2>
<p>Anantha, R., Vakulenko, S., Tu, Z., Longpre, S., Pulman, S., and Chappidi, S. Open-domain question answering goes conversational via question rewriting. arXiv preprint arXiv:2010.04898, 2020.</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877-1901, 2020.</p>
<p>Cao, G., Nie, J.-Y., Gao, J., and Robertson, S. Selecting good expansion terms for pseudo-relevance feedback. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pp. 243-250, 2008.</p>
<p>Chen, D., Fisch, A., Weston, J., and Bordes, A. Reading Wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1870-1879, Vancouver, Canada, 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https://aclanthology.org/P17-1171.</p>
<p>Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Del Tredici, M., Barlacchi, G., Shen, X., Cheng, W., and de Gispert, A. Question rewriting for open-domain conversational qa: Best practices and limitations. In Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management, pp. 2974-2978, 2021.</p>
<p>Dohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D., Lopes, R. G., Wu, Y., Michalewski, H., Saurous, R. A., Sohl-Dickstein, J., et al. Language model cascades. arXiv preprint arXiv:2207.10342, 2022.</p>
<p>Fox, E. A. and Shaw, J. A. Combination of multiple searches. NIST special publication SP, 243, 1994.</p>
<p>Gao, L., Dai, Z., Pasupat, P., Chen, A., Chaganty, A. T., Fan, Y., Zhao, V. Y., Lao, N., Lee, H., Juan, D.-C., et al. Attributed text generation via post-hoc research and revision. arXiv preprint arXiv:2210.08726, 2022.</p>
<p>Geva, M., Khashabi, D., Segal, E., Khot, T., Roth, D., and Berant, J. Did aristotle use a laptop? a question answering</p>
<p>benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9: $346-361,2021$.</p>
<p>Hofstätter, S., Chen, J., Raman, K., and Zamani, H. Fidlight: Efficient and effective retrieval-augmented text generation. arXiv preprint arXiv:2209.14290, 2022.</p>
<p>Huang, J., Gu, S. S., Hou, L., Wu, Y., Wang, X., Yu, H., and Han, J. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022.</p>
<p>Ishii, Y., Madotto, A., and Fung, P. Survey of hallucination in natural language generation. ACM Comput. Surv, 1(1), 2022.</p>
<p>Izacard, G. and Grave, E. Leveraging passage retrieval with generative models for open domain question answering. arXiv preprint arXiv:2007.01282, 2020.</p>
<p>Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and Grave, E. Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299, 2022.</p>
<p>Jiang, Y., Bordia, S., Zhong, Z., Dognin, C., Singh, M., and Bansal, M. HoVer: A dataset for many-hop fact extraction and claim verification. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 3441-3460, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. findings-emnlp.309. URL https://aclanthology. org/2020.findings-emnlp. 309.</p>
<p>Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 6769-6781, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.emnlp-main. 550.</p>
<p>Khattab, O. and Zaharia, M. Colbert: Efficient and effective passage search via contextualized late interaction over BERT. In Huang, J., Chang, Y., Cheng, X., Kamps, J., Murdock, V., Wen, J., and Liu, Y. (eds.), Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020, pp. 3948. ACM, 2020. doi: 10.1145/3397271.3401075. URL https://doi.org/10.1145/3397271.3401075.</p>
<p>Khattab, O., Potts, C., and Zaharia, M. Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021a.</p>
<p>Khattab, O., Potts, C., and Zaharia, M. Relevance-guided supervision for openqa with ColBERT. Transactions of the Association for Computational Linguistics, 9:929944, 2021b.</p>
<p>Khot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson, K., Clark, P., and Sabharwal, A. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406, 2022.</p>
<p>Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.</p>
<p>Krishna, K., Chang, Y., Wieting, J., and Iyyer, M. Rankgen: Improving text generation with large ranking models. arXiv preprint arXiv:2205.09726, 2022.</p>
<p>Kurland, O. and Culpepper, J. S. Fusion in information retrieval: Sigir 2018 half-day tutorial. In The 41st International ACM SIGIR Conference on Research \&amp; Development in Information Retrieval, pp. 1383-1386, 2018.</p>
<p>Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M.-W., Dai, A. M., Uszkoreit, J., Le, Q., and Petrov, S. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466, 2019. doi: 10.1162/tacl_a_ 00276. URL https://aclanthology.org/Q19-1026.</p>
<p>Lazaridou, A., Gribovskaya, E., Stokowiec, W., and Grigorev, N. Internet-augmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115, 2022.</p>
<p>Le, N. T., Bai, F., and Ritter, A. Few-shot anaphora resolution in scientific protocols via mixtures of in-context experts. arXiv preprint arXiv:2210.03690, 2022.</p>
<p>Lee, K., Chang, M.-W., and Toutanova, K. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 60866096, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1612. URL https://aclanthology.org/P19-1612.</p>
<p>Levine, Y., Dalmedigos, I., Ram, O., Zeldes, Y., Jannai, D., Muhlgay, D., Osin, Y., Lieber, O., Lenz, B., Shalev-Shwartz, S., et al. Standing on the shoulders of giant frozen language models. arXiv preprint arXiv:2204.10019, 2022.</p>
<p>Lewis, P. S. H., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W., Rocktäschel, T., Riedel, S., and Kiela, D. Retrieval-Augmented</p>
<p>Generation for Knowledge-Intensive NLP Tasks. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https: //proceedings.neurips.cc/paper/2020/hash/ 6b493230205f780e1bc26945df7481e5-Abstract. html.</p>
<p>Li, X. L., Holtzman, A., Fried, D., Liang, P., Eisner, J., Hashimoto, T., Zettlemoyer, L., and Lewis, M. Contrastive decoding: Open-ended text generation as optimization. arXiv preprint arXiv:2210.15097, 2022.</p>
<p>Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen, W. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021.</p>
<p>McCann, B., Keskar, N. S., Xiong, C., and Socher, R. The natural language decathlon: Multitask learning as question answering. arXiv:1806.08730, 2018. URL https://arxiv.org/abs/1806.08730.</p>
<p>Min, S., Zhong, V., Zettlemoyer, L., and Hajishirzi, H. Multi-hop reading comprehension through question decomposition and rescoring. arXiv preprint arXiv:1906.02916, 2019.</p>
<p>Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.</p>
<p>Paranjape, A., Khattab, O., Potts, C., Zaharia, M., and Manning, C. D. Hindsight: Posterior-guided Training of Retrievers for Improved Open-ended Generation. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=Vr_ BTpw3wz.</p>
<p>Perez, E., Kiela, D., and Cho, K. True few-shot learning with language models. Advances in Neural Information Processing Systems, 34:11054-11070, 2021.</p>
<p>Press, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A., and Lewis, M. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022.</p>
<p>Qi, P., Lee, H., Sido, O., Manning, C. D., et al. Retrieve, rerank, read, then iterate: Answering open-domain questions of arbitrary complexity from text. arXiv preprint arXiv:2010.12527, 2020. URL https://arxiv.org/ abs/2010.12527.</p>
<p>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p>
<p>Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 23832392, Austin, Texas, 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://aclanthology.org/D16-1264.</p>
<p>Raposo, G., Ribeiro, R., Martins, B., and Coheur, L. Question rewriting? assessing its importance for conversational question answering. In European Conference on Information Retrieval, pp. 199-206. Springer, 2022.</p>
<p>Santhanam, K., Khattab, O., Potts, C., and Zaharia, M. PLAID: An Efficient Engine for Late Interaction Retrieval. arXiv preprint arXiv:2205.09707, 2022a.</p>
<p>Santhanam, K., Khattab, O., Saad-Falcon, J., Potts, C., and Zaharia, M. ColBERTv2: Effective and efficient retrieval via lightweight late interaction. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 3715-3734, Seattle, United States, July 2022b. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.272. URL https://aclanthology.org/2022.naacl-main. 272.</p>
<p>Shuster, K., Poff, S., Chen, M., Kiela, D., and Weston, J. Retrieval augmentation reduces hallucination in conversation. arXiv preprint arXiv:2104.07567, 2021.</p>
<p>Si, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber, J., and Wang, L. Prompting gpt-3 to be reliable. arXiv preprint arXiv:2210.09150, 2022.</p>
<p>Sun, Z., Wang, X., Tay, Y., Yang, Y., and Zhou, D. Recitation-augmented language models. arXiv preprint arXiv:2210.01296, 2022.</p>
<p>Thorne, J., Vlachos, A., Christodoulopoulos, C., and Mittal, A. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 809-819, New Orleans, Louisiana, 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https://aclanthology.org/N18-1074.</p>
<p>Vakulenko, S., Kiesel, J., and Fröbe, M. SCAI-QReCC shared task on conversational question answering. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pp. 4913-4922, Marseille, France,</p>
<p>June 2022. European Language Resources Association. URL https://aclanthology.org/2022.lrec-1.525.</p>
<p>Wang, X., Macdonald, C., Tonellotto, N., and Ounis, I. Colbert-prf: Semantic pseudo-relevance feedback for dense passage and document retrieval. ACM Transactions on the Web, 2022a.</p>
<p>Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Rationale-augmented ensembles in language models. arXiv preprint arXiv:2207.00747, 2022b.</p>
<p>Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022c.</p>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.</p>
<p>Wiher, G., Meister, C., and Cotterell, R. On decoding strategies for neural text generators. arXiv preprint arXiv:2203.15721, 2022.</p>
<p>Xiong, W., Li, X. L., Iyer, S., Du, J., Lewis, P., Wang, W. Y., Mehdad, Y., Yih, W.-t., Riedel, S., Kiela, D., et al. Answering complex open-domain questions with multihop dense retrieval. arXiv preprint arXiv:2009.12756, 2020. URL https://arxiv.org/abs/2009.12756.</p>
<p>Xue, X. and Croft, W. B. Modeling reformulation using query distributions. ACM Transactions on Information Systems (TOIS), 31(2):1-34, 2013.</p>
<p>Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., and Manning, C. D. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.</p>
<p>Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.</p>
<p>Zelikman, E., Wu, Y., and Goodman, N. D. Star: Bootstrapping reasoning with reasoning. arXiv preprint arXiv:2203.14465, 2022.</p>
<p>Zhang, Z., Zhang, A., Li, M., and Smola, A. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022.</p>
<p>Zhong, V., Shi, W., Yih, W.-t., and Zettlemoyer, L. Romqa: A benchmark for robust, multi-evidence, multi-answer question answering. arXiv preprint arXiv:2210.14353, 2022.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ https://gist.github.com/g8a9/
07c2be12ae02cfad4aa430d77dc940cb&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
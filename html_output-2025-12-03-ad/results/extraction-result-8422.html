<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8422 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8422</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8422</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283" target="_blank">NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> The initial results show that the proposed approach can be used with several LLM providers to develop controllable and safe LLM applications using programmable rails.</p>
                <p><strong>Paper Abstract:</strong> NeMo Guardrails is an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems. Guardrails (or rails for short) are a specific way of controlling the output of an LLM, such as not talking about topics considered harmful, following a predefined dialogue path, using a particular language style, and more. There are several mechanisms that allow LLM providers and developers to add guardrails that are embedded into a specific model at training, e.g. using model alignment. Differently, using a runtime inspired from dialogue management, NeMo Guardrails allows developers to add programmable rails to LLM applications - these are user-defined, independent of the underlying LLM, and interpretable. Our initial results show that the proposed approach can be used with several LLM providers to develop controllable and safe LLM applications using programmable rails.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8422.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8422.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NeMo Guardrails (Topical Rails)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NeMo Guardrails Topical Rails (Colang runtime + LLM few-shot retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A runtime/dialogue-manager layer (Colang + Guardrails runtime) that uses an external vector-indexed memory of canonical forms, flows, and bot canonical forms to select few-shot examples and guide LLM behavior through a three-step CoT-style pipeline (generate user canonical form → decide next steps → generate bot message).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NeMo Guardrails topical agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-powered conversational agent mediated by the Guardrails runtime/Colang interpreter which enforces developer-defined dialogue flows (topical rails) and uses few-shot in-context examples selected from a vector database to generate canonical forms and next-steps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various (text-davinci-003, gpt-3.5-turbo, falcon-7b-instruct, llama2-13b-chat, dolly-v2-3b)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multiple LLMs evaluated; includes OpenAI instruction models (text-davinci-003, gpt-3.5-turbo) and open-source instruction/chat models (falcon-7b-instruct, llama2-13b-chat, dolly-v2-3b); performance varies by model and prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Topical rails / Conversational NLU mapping (chit-chat, banking datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Map user utterances to canonical forms (user intents), choose next dialogue steps (bot intents/flows), and generate appropriate bot messages according to developer-defined Colang flows; evaluated on transformed conversational NLU datasets (chit-chat and banking).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>task-oriented dialogue / intent classification and response generation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external retrieval-augmented memory (vector-index of canonical forms and flows) + dialogue context (event history)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Vector database (Annoy/FAISS) storing canonical forms, dialogue flows, and bot canonical forms; nearest-neighbors semantic search to select k few-shot examples for in-prompt few-shot generation; event-driven runtime stores conversation context (sequence of events) accessible to actions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Indexed canonical forms, dialogue flows, bot canonical forms (text embeddings); conversation event history (utterances, canonical forms, actions, last_bot_message, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic nearest-neighbors search over vector embeddings (k nearest examples), optionally with similarity thresholding (e.g., sim=0.6) or exact match fallback; retrieved examples are concatenated into the prompt for few-shot generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Examples (banking dataset): text-davinci-003 user intent accuracy: k=all -> 0.77; text-davinci-003 k=3 -> 0.65; text-davinci-003 k=1 -> 0.50; Table 3 (banking) reports per-step metrics for multiple models (see paper Tables 2/3). For chit-chat dataset Table 2: text-davinci-003 user intent acc k=all 0.89, k=3 0.82, k=1 0.65; falcon-7b-instruct user-int k=all 0.81 (see tables).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Implicit baselines: reduced k or no semantic retrieval degrades performance (e.g., text-davinci-003 banking user-int: k=1 0.50 vs k=all 0.77); using no semantic-similarity matching for some models (e.g., gpt-3.5-turbo) yields lower accuracies (see tables).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Ablations over retrieval size k (k=all, k=3, k=1) and use of semantic-similarity matching show clear effects: at least k=3 samples per canonical form recommended for good performance; adding semantic similarity matching (sim=0.6) improves results for some models (notably gpt-3.5-turbo). Each of the three pipeline steps (canonical form → next step → bot message) incrementally improves total performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>External vector-indexed memory of canonical forms + few-shot selection substantially improves mapping to canonical forms and downstream dialogue behavior; recommended to store >=3 exemplars per canonical form and to use semantic similarity matching for LLMs that generate diverse canonical forms. Smaller open-source models (falcon-7b-instruct, llama2-13b-chat) can achieve strong topical-rail performance when combined with this retrieval-based few-shot approach.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>The three-step CoT retrieval/generation pipeline increases latency and cost (approx. 3x compared to a single bot-generation call) because steps are sequential and cannot be batched; performance is sensitive to prompt design and number/quality of indexed exemplars; similarity thresholds are an important inference hyperparameter.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8422.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8422.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fact-Checking Rail</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Execution Rail: Fact-Checking via Retrieval-Augmented Entailment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An execution rail that frames fact-checking as an entailment task: given retrieved evidence (context) and a generated bot response (hypothesis), an LLM predicts yes/no entailment to determine whether the response is grounded in the evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Fact-checking rail (LLM entailment verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-based verifier that takes (evidence, hypothesis) pairs (retrieved from a retrieval component) and outputs binary entailment (yes/no) to determine whether the bot response is grounded in evidence; used as an execution action invoked from Colang flows.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003, gpt-3.5-turbo (evaluated as verifiers)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI instruction/chat models used as binary entailment verifiers; prompted with evidence and hypothesis to answer yes/no.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Fact-checking on MSMARCO (context, question, answer triples)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a context (retrieved passage) and a candidate answer, decide whether the answer is entailed/grounded in the context; negatives were constructed by rewriting positives into hard negatives with text-davinci-003.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>retrieval-augmented fact verification / entailment classification</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented memory (external evidence/documents indexed and retrieved as context)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Retrieve evidence passages (MSMARCO) for a query and pass evidence + candidate bot response to an LLM entailment prompt that returns yes/no.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>External documents/evidence passages retrieved from the corpus (text).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Retrieval component (not exhaustively specified) used to supply evidence; evaluation constructs (context, question, answer) triples — evidence is provided to the entailment probe.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Overall accuracy ~80% for both text-davinci-003 and gpt-3.5-turbo on the constructed balanced MSMARCO-based dataset (equal sampling of positives and hard negatives); figure in paper (Fig. 11) and Appendix G.2.2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Comparison across verifier models shows similar overall accuracy (~80%) with differences in behavior: gpt-3.5-turbo better at detecting negatives, while text-davinci-003 performs better on positive (grounded) samples. There is no direct ablation reported that removes retrieval/evidence (i.e., no true 'without memory' baseline reported).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Framing fact-checking as an entailment verification using retrieved evidence and an LLM verifier yields ~80% accuracy on the constructed MSMARCO test; the approach is model-agnostic and can be plugged as an execution rail within Colang flows to remove or replace ungrounded bot responses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Quality of retrieval/evidence and negative-mining procedure affect evaluation; current implementation uses pure prompting (no specialized verifier models), and factual-checking depends on the retrieval step — no baseline reported without retrieval; performance is subjective to the LLM used as verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8422.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8422.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hallucination Rail (Self-Consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Execution Rail: Hallucination Detection via Self-Consistency / Sample-Context Agreement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hallucination rail that samples n responses from the LLM (high temperature), concatenates n-1 samples as context and uses the nth as a hypothesis; an LLM verifier (temp=0) checks whether the hypothesis is consistent with the sampled context to detect hallucination/unanswerability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Hallucination rail (SelfCheck-style verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses repeated stochastic sampling from the base conversational LLM to create multiple candidate answers; a separate entailment/consistency LLM call checks agreement among sampled responses to identify hallucinations or unanswerable prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003, gpt-3.5-turbo (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI models used both as base generators (sampled with high temp) and as deterministic verifiers (temp=0) for self-consistency checks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Hallucination detection on a curated set of unanswerable/false-premise questions</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Detect whether a question has a valid factual answer; the testset consists of 20 questions designed with false premises (no correct answer). Any non-deflecting generation is considered failure; the rail is evaluated as an intercept/fallback mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>hallucination detection / unanswerability detection</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term sampled-response memory (in-prompt context built from multiple sampled outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Create n stochastic samples from the LLM; concatenate n-1 sampled responses into a 'context' and use the nth as 'hypothesis'; pass both to a verifier LLM which checks agreement (yes/no). The sampled responses function as an ephemeral memory/context for verification.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Multiple sampled bot responses (text) serving as the context for a consistency check.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation of sampled responses (no external index) — a form of ephemeral in-call memory constructed from multiple outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported effects: For text-davinci-003, employing the hallucination rail intercepts ~70% of unanswerable prompts (base model fails to deflect). For gpt-3.5-turbo, the base model already deflects or hedges ~65% of cases; the paper reports that employing the hallucination rail 'boosts performance up to 95%' in main text and 'helps boost identification by 25%' in Appendix G.2.3 (appendix wording implies 65% -> +25% = 90%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>text-davinci-003 baseline: unable to deflect unanswerable prompts (base deflection ~0% reported); gpt-3.5-turbo baseline deflection/hedging ~65% (see paper).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Comparison between base LLM behavior and behavior when the hallucination rail is applied shows large gains for weaker models (davinci) and further improvements for already strong models (gpt-3.5). The paper reports both absolute interception rates and relative boosts; there is a minor inconsistency between main-text (boost to 95%) and appendix wording (boost by 25%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Self-consistency using multiple sampled outputs as ephemeral memory is an effective hallucination detection fallback: it substantially improves interception of unanswerable prompts (notably turning models that rarely deflect into ones that do) and further improves performance for models that already hedge; using high temperature for sampling (temp=1) and deterministic verifier (temp=0) is recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Evaluation is small-scale and depends on subjective notion of 'deflection'; effectiveness depends on sampling variability and verifier quality; extra sampling increases latency and cost; reported metrics derive from a small 20-item testset of false-premise questions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8422.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8422.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Runtime Context Memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Guardrails Runtime Conversational Context (event history accessible to actions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The Guardrails runtime stores dialogue context as a sequence of events (utterances, canonical forms, actions, messages) in a key-value context accessible to Colang actions (e.g., context.get('last_bot_message')), providing persistent short-term memory for decision logic and actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Guardrails runtime context memory</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An event-driven internal memory implemented as a context object (key-value dictionary) containing conversation history and shortcut accessors; used by execution actions to implement moderation, fact-checking, and other logic.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Dialogue state / action conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provide actions and flows with access to recent conversation state (last user message, last bot message, canonical forms, actions called) so flows can branch, remove messages, call executions, and generate alternate bot responses.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>dialogue state tracking / short-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic / dialogue history (short-term) stored in runtime context</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Event loop and context object that records events (UtteranceUserActionFinished, action calls, canonical forms, etc.); actions read/write this context during flow execution.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Sequence of event records and convenient shortcuts (e.g., last_bot_message), encoded in the runtime context dictionary.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Direct programmatic access via context.get(key) within custom Python actions invoked from Colang flows.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No explicit ablation reported that removes or disables runtime context; however, many Colang flows and execution rails rely on this context for correct behavior (e.g., moderation flows that remove last message).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A structured runtime context (event history) is essential for implementing execution rails and for Colang flows to inspect and modify conversation state; it enables actions such as removing a generated response or adding a deflection message.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Context-based decision logic is sensitive to the fidelity of canonical form generation and to timely availability of action outputs; using the context plus the multi-step pipeline increases sequential dependencies and latency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models <em>(Rating: 2)</em></li>
                <li>Shall we pretrain autoregressive language models with retrieval? a comprehensive study <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
                <li>Prompt learning for domain adaptation in task-oriented dialogue <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8422",
    "paper_id": "paper-b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "NeMo Guardrails (Topical Rails)",
            "name_full": "NeMo Guardrails Topical Rails (Colang runtime + LLM few-shot retrieval)",
            "brief_description": "A runtime/dialogue-manager layer (Colang + Guardrails runtime) that uses an external vector-indexed memory of canonical forms, flows, and bot canonical forms to select few-shot examples and guide LLM behavior through a three-step CoT-style pipeline (generate user canonical form → decide next steps → generate bot message).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "NeMo Guardrails topical agent",
            "agent_description": "An LLM-powered conversational agent mediated by the Guardrails runtime/Colang interpreter which enforces developer-defined dialogue flows (topical rails) and uses few-shot in-context examples selected from a vector database to generate canonical forms and next-steps.",
            "model_name": "various (text-davinci-003, gpt-3.5-turbo, falcon-7b-instruct, llama2-13b-chat, dolly-v2-3b)",
            "model_description": "Multiple LLMs evaluated; includes OpenAI instruction models (text-davinci-003, gpt-3.5-turbo) and open-source instruction/chat models (falcon-7b-instruct, llama2-13b-chat, dolly-v2-3b); performance varies by model and prompt.",
            "task_name": "Topical rails / Conversational NLU mapping (chit-chat, banking datasets)",
            "task_description": "Map user utterances to canonical forms (user intents), choose next dialogue steps (bot intents/flows), and generate appropriate bot messages according to developer-defined Colang flows; evaluated on transformed conversational NLU datasets (chit-chat and banking).",
            "task_type": "task-oriented dialogue / intent classification and response generation",
            "memory_used": true,
            "memory_type": "external retrieval-augmented memory (vector-index of canonical forms and flows) + dialogue context (event history)",
            "memory_mechanism": "Vector database (Annoy/FAISS) storing canonical forms, dialogue flows, and bot canonical forms; nearest-neighbors semantic search to select k few-shot examples for in-prompt few-shot generation; event-driven runtime stores conversation context (sequence of events) accessible to actions.",
            "memory_representation": "Indexed canonical forms, dialogue flows, bot canonical forms (text embeddings); conversation event history (utterances, canonical forms, actions, last_bot_message, etc.).",
            "memory_retrieval_method": "Semantic nearest-neighbors search over vector embeddings (k nearest examples), optionally with similarity thresholding (e.g., sim=0.6) or exact match fallback; retrieved examples are concatenated into the prompt for few-shot generation.",
            "performance_with_memory": "Examples (banking dataset): text-davinci-003 user intent accuracy: k=all -&gt; 0.77; text-davinci-003 k=3 -&gt; 0.65; text-davinci-003 k=1 -&gt; 0.50; Table 3 (banking) reports per-step metrics for multiple models (see paper Tables 2/3). For chit-chat dataset Table 2: text-davinci-003 user intent acc k=all 0.89, k=3 0.82, k=1 0.65; falcon-7b-instruct user-int k=all 0.81 (see tables).",
            "performance_without_memory": "Implicit baselines: reduced k or no semantic retrieval degrades performance (e.g., text-davinci-003 banking user-int: k=1 0.50 vs k=all 0.77); using no semantic-similarity matching for some models (e.g., gpt-3.5-turbo) yields lower accuracies (see tables).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Ablations over retrieval size k (k=all, k=3, k=1) and use of semantic-similarity matching show clear effects: at least k=3 samples per canonical form recommended for good performance; adding semantic similarity matching (sim=0.6) improves results for some models (notably gpt-3.5-turbo). Each of the three pipeline steps (canonical form → next step → bot message) incrementally improves total performance.",
            "key_findings": "External vector-indexed memory of canonical forms + few-shot selection substantially improves mapping to canonical forms and downstream dialogue behavior; recommended to store &gt;=3 exemplars per canonical form and to use semantic similarity matching for LLMs that generate diverse canonical forms. Smaller open-source models (falcon-7b-instruct, llama2-13b-chat) can achieve strong topical-rail performance when combined with this retrieval-based few-shot approach.",
            "limitations_or_challenges": "The three-step CoT retrieval/generation pipeline increases latency and cost (approx. 3x compared to a single bot-generation call) because steps are sequential and cannot be batched; performance is sensitive to prompt design and number/quality of indexed exemplars; similarity thresholds are an important inference hyperparameter.",
            "uuid": "e8422.0",
            "source_info": {
                "paper_title": "NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Fact-Checking Rail",
            "name_full": "Execution Rail: Fact-Checking via Retrieval-Augmented Entailment",
            "brief_description": "An execution rail that frames fact-checking as an entailment task: given retrieved evidence (context) and a generated bot response (hypothesis), an LLM predicts yes/no entailment to determine whether the response is grounded in the evidence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Fact-checking rail (LLM entailment verifier)",
            "agent_description": "An LLM-based verifier that takes (evidence, hypothesis) pairs (retrieved from a retrieval component) and outputs binary entailment (yes/no) to determine whether the bot response is grounded in evidence; used as an execution action invoked from Colang flows.",
            "model_name": "text-davinci-003, gpt-3.5-turbo (evaluated as verifiers)",
            "model_description": "OpenAI instruction/chat models used as binary entailment verifiers; prompted with evidence and hypothesis to answer yes/no.",
            "task_name": "Fact-checking on MSMARCO (context, question, answer triples)",
            "task_description": "Given a context (retrieved passage) and a candidate answer, decide whether the answer is entailed/grounded in the context; negatives were constructed by rewriting positives into hard negatives with text-davinci-003.",
            "task_type": "retrieval-augmented fact verification / entailment classification",
            "memory_used": true,
            "memory_type": "retrieval-augmented memory (external evidence/documents indexed and retrieved as context)",
            "memory_mechanism": "Retrieve evidence passages (MSMARCO) for a query and pass evidence + candidate bot response to an LLM entailment prompt that returns yes/no.",
            "memory_representation": "External documents/evidence passages retrieved from the corpus (text).",
            "memory_retrieval_method": "Retrieval component (not exhaustively specified) used to supply evidence; evaluation constructs (context, question, answer) triples — evidence is provided to the entailment probe.",
            "performance_with_memory": "Overall accuracy ~80% for both text-davinci-003 and gpt-3.5-turbo on the constructed balanced MSMARCO-based dataset (equal sampling of positives and hard negatives); figure in paper (Fig. 11) and Appendix G.2.2.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Comparison across verifier models shows similar overall accuracy (~80%) with differences in behavior: gpt-3.5-turbo better at detecting negatives, while text-davinci-003 performs better on positive (grounded) samples. There is no direct ablation reported that removes retrieval/evidence (i.e., no true 'without memory' baseline reported).",
            "key_findings": "Framing fact-checking as an entailment verification using retrieved evidence and an LLM verifier yields ~80% accuracy on the constructed MSMARCO test; the approach is model-agnostic and can be plugged as an execution rail within Colang flows to remove or replace ungrounded bot responses.",
            "limitations_or_challenges": "Quality of retrieval/evidence and negative-mining procedure affect evaluation; current implementation uses pure prompting (no specialized verifier models), and factual-checking depends on the retrieval step — no baseline reported without retrieval; performance is subjective to the LLM used as verifier.",
            "uuid": "e8422.1",
            "source_info": {
                "paper_title": "NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Hallucination Rail (Self-Consistency)",
            "name_full": "Execution Rail: Hallucination Detection via Self-Consistency / Sample-Context Agreement",
            "brief_description": "A hallucination rail that samples n responses from the LLM (high temperature), concatenates n-1 samples as context and uses the nth as a hypothesis; an LLM verifier (temp=0) checks whether the hypothesis is consistent with the sampled context to detect hallucination/unanswerability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Hallucination rail (SelfCheck-style verifier)",
            "agent_description": "Uses repeated stochastic sampling from the base conversational LLM to create multiple candidate answers; a separate entailment/consistency LLM call checks agreement among sampled responses to identify hallucinations or unanswerable prompts.",
            "model_name": "text-davinci-003, gpt-3.5-turbo (evaluated)",
            "model_description": "OpenAI models used both as base generators (sampled with high temp) and as deterministic verifiers (temp=0) for self-consistency checks.",
            "task_name": "Hallucination detection on a curated set of unanswerable/false-premise questions",
            "task_description": "Detect whether a question has a valid factual answer; the testset consists of 20 questions designed with false premises (no correct answer). Any non-deflecting generation is considered failure; the rail is evaluated as an intercept/fallback mechanism.",
            "task_type": "hallucination detection / unanswerability detection",
            "memory_used": true,
            "memory_type": "short-term sampled-response memory (in-prompt context built from multiple sampled outputs)",
            "memory_mechanism": "Create n stochastic samples from the LLM; concatenate n-1 sampled responses into a 'context' and use the nth as 'hypothesis'; pass both to a verifier LLM which checks agreement (yes/no). The sampled responses function as an ephemeral memory/context for verification.",
            "memory_representation": "Multiple sampled bot responses (text) serving as the context for a consistency check.",
            "memory_retrieval_method": "Prompt concatenation of sampled responses (no external index) — a form of ephemeral in-call memory constructed from multiple outputs.",
            "performance_with_memory": "Reported effects: For text-davinci-003, employing the hallucination rail intercepts ~70% of unanswerable prompts (base model fails to deflect). For gpt-3.5-turbo, the base model already deflects or hedges ~65% of cases; the paper reports that employing the hallucination rail 'boosts performance up to 95%' in main text and 'helps boost identification by 25%' in Appendix G.2.3 (appendix wording implies 65% -&gt; +25% = 90%).",
            "performance_without_memory": "text-davinci-003 baseline: unable to deflect unanswerable prompts (base deflection ~0% reported); gpt-3.5-turbo baseline deflection/hedging ~65% (see paper).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Comparison between base LLM behavior and behavior when the hallucination rail is applied shows large gains for weaker models (davinci) and further improvements for already strong models (gpt-3.5). The paper reports both absolute interception rates and relative boosts; there is a minor inconsistency between main-text (boost to 95%) and appendix wording (boost by 25%).",
            "key_findings": "Self-consistency using multiple sampled outputs as ephemeral memory is an effective hallucination detection fallback: it substantially improves interception of unanswerable prompts (notably turning models that rarely deflect into ones that do) and further improves performance for models that already hedge; using high temperature for sampling (temp=1) and deterministic verifier (temp=0) is recommended.",
            "limitations_or_challenges": "Evaluation is small-scale and depends on subjective notion of 'deflection'; effectiveness depends on sampling variability and verifier quality; extra sampling increases latency and cost; reported metrics derive from a small 20-item testset of false-premise questions.",
            "uuid": "e8422.2",
            "source_info": {
                "paper_title": "NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Runtime Context Memory",
            "name_full": "Guardrails Runtime Conversational Context (event history accessible to actions)",
            "brief_description": "The Guardrails runtime stores dialogue context as a sequence of events (utterances, canonical forms, actions, messages) in a key-value context accessible to Colang actions (e.g., context.get('last_bot_message')), providing persistent short-term memory for decision logic and actions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Guardrails runtime context memory",
            "agent_description": "An event-driven internal memory implemented as a context object (key-value dictionary) containing conversation history and shortcut accessors; used by execution actions to implement moderation, fact-checking, and other logic.",
            "model_name": null,
            "model_description": null,
            "task_name": "Dialogue state / action conditioning",
            "task_description": "Provide actions and flows with access to recent conversation state (last user message, last bot message, canonical forms, actions called) so flows can branch, remove messages, call executions, and generate alternate bot responses.",
            "task_type": "dialogue state tracking / short-term memory",
            "memory_used": true,
            "memory_type": "episodic / dialogue history (short-term) stored in runtime context",
            "memory_mechanism": "Event loop and context object that records events (UtteranceUserActionFinished, action calls, canonical forms, etc.); actions read/write this context during flow execution.",
            "memory_representation": "Sequence of event records and convenient shortcuts (e.g., last_bot_message), encoded in the runtime context dictionary.",
            "memory_retrieval_method": "Direct programmatic access via context.get(key) within custom Python actions invoked from Colang flows.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "No explicit ablation reported that removes or disables runtime context; however, many Colang flows and execution rails rely on this context for correct behavior (e.g., moderation flows that remove last message).",
            "key_findings": "A structured runtime context (event history) is essential for implementing execution rails and for Colang flows to inspect and modify conversation state; it enables actions such as removing a generated response or adding a deflection message.",
            "limitations_or_challenges": "Context-based decision logic is sensitive to the fidelity of canonical form generation and to timely availability of action outputs; using the context plus the multi-step pipeline increases sequential dependencies and latency.",
            "uuid": "e8422.3",
            "source_info": {
                "paper_title": "NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
            "rating": 2
        },
        {
            "paper_title": "Shall we pretrain autoregressive language models with retrieval? a comprehensive study",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1
        },
        {
            "paper_title": "Prompt learning for domain adaptation in task-oriented dialogue",
            "rating": 1
        }
    ],
    "cost": 0.015259749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails</h1>
<p>Traian Rebedea<em>, Razvan Dinu</em>, Makesh Sreedhar, Christopher Parisien, Jonathan Cohen NVIDIA<br>Santa Clara, CA<br>{trebedea, rdinu, makeshn, cparisien, jocohen}@nvidia.com</p>
<h4>Abstract</h4>
<p>NeMo Guardrails is an open-source toolkit ${ }^{1}$ for easily adding programmable guardrails to LLM-based conversational systems. Guardrails (or rails for short) are a specific way of controlling the output of an LLM, such as not talking about topics considered harmful, following a predefined dialogue path, using a particular language style, and more. There are several mechanisms that allow LLM providers and developers to add guardrails that are embedded into a specific model at training, e.g. using model alignment. Differently, using a runtime inspired from dialogue management, NeMo Guardrails allows developers to add programmable rails to LLM applications - these are user-defined, independent of the underlying LLM, and interpretable. Our initial results show that the proposed approach can be used with several LLM providers to develop controllable and safe LLM applications using programmable rails.</p>
<h2>1 Introduction</h2>
<p>Steerability and trustworthiness are key factors for deploying Large Language Models (LLMs) in production. Enabling these models to stay on track for multiple turns of a conversation is essential for developing task-oriented dialogue systems. This seems like a serious challenge as LLMs can be easily led into veering off-topic (Pang et al., 2023). At the same time, LLMs also tend to generate responses that are factually incorrect or completely fabricated (hallucinations) (Manakul et al., 2023; Peng et al., 2023; Azaria and Mitchell, 2023). In addition, they are vulnerable to prompt injection (or jailbreak) attacks, where malicious actors manipulate inputs to trick the model into producing harmful outputs (Kang et al., 2023; Wei et al., 2023; Zou et al., 2023).</p>
<p>Building trustworthy and controllable conversational systems is of vital importance for deploy-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Programmable vs. embedded rails for LLMs.
ing LLMs in customer facing situations. NeMo Guardrails is an open-source toolkit for easily adding programmable rails to LLM-based applications. Guardrails (or rails) provide a mechanism for controlling the output of an LLM to respect some human-imposed constraints, e.g. not engaging in harmful topics, following a predefined dialogue path, adding specific responses to some user requests, using a particular language style, extracting structured data. To implement the various types of rails, several techniques can be used, including model alignment at training, prompt engineering and chain-of-thought (CoT), and adding a dialogue manager. While model alignment provides general rails embedded in the LLM at training and prompt tuning can offer user-specific rails embedded in a customized model, NeMo Guardrails allows users to define custom programmable rails at runtime as shown in Fig. 1. This mechanism is independent of alignment strategies and supplements embedded rails, works with different LLMs, and provides interpretable rails defined using a custom modeling language, Colang.</p>
<p>To implement user-defined programmable rails for LLMs, our toolkit uses a programmable runtime engine that acts like a proxy between the user and the LLM. This approach is complementary to model alignment and it defines the rules the LLM should follow in the interaction with the users. Thus, the Guardrails runtime has the role of a dialogue manager, being able to interpret and impose the rules defining the programmable rails. These rules are expressed using a modeling language called Colang. More specifically, Colang is used to define rules as dialogue flows that the LLM should always follow (see Fig. 2). Using a prompting technique with in-context learning and a specific form of CoT, we enable the LLM to generate the next steps that guide the conversation. Colang is then interpreted by the dialogue manager to apply the guardrails rules predefined by users or automatically generated by the LLM to guide the behavior of the LLM.</p>
<p>While NeMo Guardrails can be used to add safety and steerability to any LLM-based application, we consider that dialogue systems powered by an LLM benefit the most from using Colang and the Guardrails runtime. The toolkit is licensed as Apache 2.0, and we provide initial support for several LLM providers, together with starter example applications and evaluation tools.</p>
<h2>2 Related Work</h2>
<h3>2.1 Model Alignment</h3>
<p>Existing solutions for adding rails to LLMs rely heavily on model alignment techniques such as instruction-tuning <em>Wei et al. (2021)</em> or reinforcement learning <em>Ouyang et al. (2022); Glaese et al. (2022); OpenAI (2023)</em>. The alignment of LLMs works on several dimensions, mainly to improve helpfulness and to reduce harmfulness. Alignment in general, including red-teaming <em>Perez et al. (2022)</em>, requires a large collection of input prompts and responses that are manually labeled according to specific criteria (e.g., harmlessness).</p>
<p>Model alignment provides rails embedded at training in the LLM, that cannot easily be changed at runtime by users. Moreover, it also requires a large set of human-annotated response ratings for each rail to be incorporated by the LLM. While Reinforcement Learning from Human Feedback <em>Ouyang et al. (2022)</em> is the most popular method for model alignment, alternatives such as RL from AI Feedback <em>Bai et al. (2022b)</em> do not</p>
<div class="codehilite"><pre><span></span><code>define flow
    user express greeting
    bot express greeting
define flow
    user ask math question
    do ask wolfram alpha
define flow
    user ask distance
    do ask wolfram alpha
define subflow ask wolfram alpha
    # Generate the full query for wolfram Alpha.
    $full_wolfram_query = ...
    $result = execute wolfram alpha request
        (query=$full_wolfram_query)
    bot respond with result
</code></pre></div>

<p>Figure 2: Dialogue flows defined in Colang: a simple greeting flow and two topical rail flows calling the custom action wolfram alpha request to respond to math and distance queries.
require a human labeled dataset and use the actual LLM to provide feedback for each response.</p>
<p>While most alignment methods provide general embedded rails, in a similar way developers can add app-specific embedded rails to an LLM via prompt tuning <em>Lester et al. (2021); Liu et al. (2022)</em>.</p>
<h3>2.2 Prompting and Chain-of-Thought</h3>
<p>The most common approach to add user-defined programmable rails to an LLM is to use prompting, including prompt engineering and in-context learning <em>Brown et al. (2020)</em>, by prepending or appending a specific text to the user input <em>Wang and Chang (2022); Si et al. (2022)</em>. This text specifies the behavior that the LLM should adhere to.</p>
<p>The other approach to provide LLMs with userdefined runtime rails is to use chain-of-thought (CoT) <em>Wei et al. (2022)</em>. In its simplest form, CoT appends to the user instruction one or several similar examples of input and output pairs for the task at hand. Each of these examples contains a more detailed explanation in the output, useful for determining the final answer. Other more complex approaches involve several steps of prompting the LLM in a generic to specific way <em>Zhou et al. (2022)</em> or even with entire dialogues with different roles similar to an inner monologue <em>Huang et al. (2022)</em>.</p>
<h3>2.3 Task-Oriented Dialogue Agents</h3>
<p>Building task-oriented dialogue agents generally requires two components: a Natural Language Understanding (NLU) and a Dialogue Management (DM) engine <em>Bocklisch et al. (2017); Liu et al. (2022)</em>.</p>
<p>2021). There exist a wide range of tools and solutions for both NLU and DM, ranging from opensource solutions like Rasa (Bocklisch et al., 2017) to proprietary platforms, such as Microsoft LUIS or Google DialogFlow (Liu et al., 2021). Their functionality mostly follows these two steps: first the NLU extracts the intent and slots from the user message, then the DM predicts the next dialogue state given the current dialogue context.</p>
<p>The set of intents and dialogue states are finite and pre-defined by a conversation designer. The bot responses are also chosen from a closed set depending on the dialogue state. This approach allows to define specific dialogue flows that tightly control any dialogue agent. Conversely, these agents are rigid and require a high amount of human effort to design and update the NLU and dialogue flows.</p>
<p>At the other end of the spectrum are recent end-to-end (E2E) generative approaches that use LLMs for dialogue tracking and bot message generation (Hudeček and Dušek, 2023; Zhang et al., 2023). NeMo Guardrails also uses an E2E approach to build LLM-powered dialogue agents, but it combines a DM-like runtime able to interpret and maintain the state of dialogue flows written in Colang with a CoT-based approach to generate bot messages and even new dialogue flows using an LLM.</p>
<h2>3 NeMo Guardrails</h2>
<h3>3.1 General Architecture</h3>
<p>NeMo Guardrails acts like a proxy between the user and the LLM as detailed in Fig. 3. It allows developers to define programmatic rails that the LLM should follow in the interaction with the users using Colang, a formal modeling language designed to specify flows of events, including conversations. Colang is interpreted by the Guardrails runtime which applies the user-defined rules or automatically generated rules by the LLM, as described next. These rules implement the guardrails and guide the behavior of the LLM.</p>
<p>An excerpt from a Colang script is shown in Fig. 2 - these scripts are at the core of a Guardrails app configuration. The main elements of a Colang script are: user canonical forms, dialogue flows, and bot canonical forms. All these three types of definitions are also indexed in a vector database (e.g., Annoy (Spotify), FAISS (Johnson et al., 2019)) to allow for efficient nearest-neighbors lookup when selecting the few-shot examples for the prompt. The interaction between the LLM and
the Guardrails runtime is defined using Colang rules. When prompted accordingly, the LLM is able to generate Colang-style code using few-shot in-prompt learning. Otherwise, the LLM works in normal mode and generates natural language.</p>
<p>Canonical forms (Sreedhar and Parisien, 2022) are a key mechanism used by Colang and the runtime engine. They are expressed in natural language (e.g., English) and encode the meaning of a message in a conversation, similar to an intent. The main difference between intents and canonical forms is that the former are designed as a closed set for a text classification task, while the latter are generated by an LLM and thus are not bound in any way, but are guided by the canonical forms defined by the Guardrails app. The set of canonical forms used to define the rails that guide the interaction is specified by the developer; these are used to select few-shot examples when generating the canonical form for a new user message.</p>
<p>Using these key concepts, developers can implement a variety of programmable rails. We have identified two main categories: topical rails and execution rails. Topical rails are intended for controlling the dialogue, e.g. to guide the response for specific topics or to implement complex dialogue policies. Execution rails call custom actions defined by the app developer; we will focus on a set of safety rails available to all Guardrails apps.</p>
<h3>3.2 Topical Rails</h3>
<p>Topical rails employ the key mechanism used by NeMo Guardrails: Colang for describing programmable rails as dialogue flows, together with the Colang interpreter in the runtime for dialogue management (Execute flow [Colang] block in Fig. 3). Flows are specified by the developer to determine how the user conversation should proceed. The dialogue manager in the Guardrails runtime uses an event-driven design (an event loop that processes events and generates back other events) to ensure which flows are active in the current dialogue context.</p>
<p>The runtime has three main stages (see Fig. 3) for guiding the conversation with dialogue flows and thus ensuring the topical rails:</p>
<p>Generate user canonical form. Using similarity-based few-shot prompting, generate the canonical form for each user input, allowing the guardrails system to trigger any user-defined flows.</p>
<p>Decide next steps and execute them. Once the user canonical form is identified, there are two po-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />Figure 3: NeMo Guardrails general architecture.</p>
<p>tential paths: 1) Pre-defined flow: If the canonical form matches any of the developer-specified flows, the next step is extracted from that particular flow by the dialogue manager; 2) LLM decides next steps: For user canonical forms that are not defined in the current dialogue context, we use the generalization capability of the LLM to decide the appropriate next steps - e.g., for a travel reservation system, if a flow is defined for booking bus tickets, the LLM should generate a similar flow if the user wants to book a flight.</p>
<p>Generate bot message(s). Conditioned by the next step, the LLM is prompted to generate a response. Thus, if we do not want the bot to respond to political questions, and the next step for such a question is bot inform cannot answer - the bot would deflect from responding, respecting the rail.</p>
<p>Appendix B provides details about the Colang language. Appendix C contains sample prompts.</p>
<h3>3.3 Execution Rails</h3>
<p>The toolkit also makes it easy to add "execution" rails. These are custom actions (defined in Python), monitoring both the input and output of the LLM, and can be executed by the Guardrails runtime when encountered in a flow. While execution rails can be used for a wide range of tasks, we provide several rails for LLM safety covering fact-checking, hallucination, and moderation.</p>
<h3>3.3.1 Fact-Checking Rail</h3>
<p>Operating under the assumption of retrieval augmented generation [wang2023exploiting], we formulate the task as an entailment problem. Specifically, given an evidence text and a generated bot response, we ask the LLM to predict whether the response is grounded in and entailed by the evidence. For each evidence-hypothesis pair, the model must respond with a binary entailment prediction using the following prompt:</p>
<p>You are given a task to identify if the hypothesis is grounded and entailed in the evidence. You will only use the contents of the evidence and not rely on external knowledge. Answer with yes/no. "evidence": {{evidence}} "hypothesis": {{bot_response}} "entails":</p>
<p>If the model predicts that the hypothesis is not entailed by the evidence, this suggests the generated response may be incorrect. Different approaches can be used to handle such situations, such as abstaining from providing an answer.</p>
<h3>3.3.2 Hallucination Rail</h3>
<p>For general-purpose questions that do not involve a retrieval component, we define a hallucination rail to help prevent the bot from making up facts. The rail uses self-consistency checking similar to SelfCheckGPT [manakul2023selfcheckgpt]: given a query, we first sample several answers from the LLM and then check if these different answers are in agreement. For hallucinated statements, repeated sampling is likely to produce responses that are not in agreement.</p>
<p>After we obtain $n$ samples from the LLM for the same prompt, we concatenate $n-1$ responses to form the context and use the $n^{\text {th }}$ response as the hypothesis. Then we use the LLM to detect if the sampled responses are consistent using the prompt template defined in Appendix D.</p>
<h3>3.3.3 Moderation Rails</h3>
<p>The moderation process in NeMo Guardrails contains two key components:</p>
<ul>
<li>Input moderation, also referred as jailbreak rail, aims to detect potentially malicious user messages before reaching the dialogue system.</li>
<li>Output moderation aims to detect whether the LLM responses are legal, ethical, and not harmful prior to being returned to the user.</li>
</ul>
<p>The moderation system functions as a pipeline, with the user message first passing through input moderation before reaching the dialogue system. After the dialogue system generates a response powered by an LLM, the output moderation rail is triggered. Only after passing both moderation rails, the response is returned to the user.</p>
<p>Both the input and output moderation rails are framed as another task to a powerful, well-aligned LLM that vets the input or response. The prompt templates for these rails are found in Appendix D.</p>
<h2>4 Sample Guardrails Applications</h2>
<p>Adding rails to conversation applications is simple and straightforward using Colang scripts.</p>
<h3>4.1 Topical Rails</h3>
<p>Topical rails can be used in combination with execution rails to decide when a specific action should be called or to define complex dialogue flows for building task oriented agents.</p>
<p>In the example presented in Fig. 2, we implement two topical rails that allow the Guardrails app to use the WolframAlpha engine to respond to math and distance queries. To achieve this, the wolfram alpha request custom action (implemented in Python, available on Github) is using the WolframAlpha API to get a response to the user query. This response is then used by the LLM to generate an answer in the context of the current conversation.</p>
<h3>4.2 Execution Rails</h3>
<p>The steps involved in adding executions rails are:</p>
<ol>
<li>Define the action - Defining a rail requires the developer to define an action that specifies the logic for the rail (in Python).</li>
<li>Invoke action in dialogue flows - Once the action has been defined, we can call the action from Colang using the execute keyword.</li>
<li>Use action output in dialogue flow - The developer can specify how the application should react to the output from the action.</li>
</ol>
<p>Appendix E contains details about defining actions, together with an example of the actions that implement the input and output moderation rails.</p>
<p>Fig. 4 shows a sample flow in Colang that invokes the check_jailbreak action. If the jailbreak rail flags a user message, the developer can decide not to show the generated response and to output a default text instead. Appendix F provides other examples of flows using the executions rails.</p>
<div class="codehilite"><pre><span></span><code>define flow check jailbreak
    user ...
    $allowed = execute check_jailbreak
    if not $allowed
    bot remove last message
    bot inform message breaks moderation
</code></pre></div>

<p>Figure 4: Flow using jailbreak rail in Colang</p>
<h2>5 Evaluation</h2>
<p>In this section, we provide details on how we measure the performance of various rails. Additional information for all tasks and a discussion on the automatic evaluation tools available in NeMo Guardrails are provided in Appendix G.</p>
<h3>5.1 Topical Rails</h3>
<p>The evaluation of topical rails focuses on the core mechanism used by the toolkit to guide conversations using canonical forms and dialogue flows. The current evaluation experiments employ datasets used for conversational NLU. In this section, we present the results for the Banking dataset (Casanueva et al., 2022), while additional experiments can be found in Appendix G.</p>
<p>Starting from a NLU dataset, we create a Colang application (publicly available on Github) by mapping intents to canonical forms and defining simple dialogue flows for them. The evaluation dataset used in our experiments is balanced, containing at most 3 samples per intent sampled randomly from the original datasets. The test dataset has 231 samples spanning over 77 different intents.</p>
<p>The results of the top 3 performing models are presented in Fig. 5, showing that topical rails can be successfully used to guide conversations even with smaller open source models such as falcon-7b-instruct or llama2-13b-chat. As the performance of an LLM is heavily dependent on the prompt, all results might be improved with better prompting.</p>
<p>The topical rails evaluation highlights several important aspects. First, each step in the three-step</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: Performance of topical rails on Banking.</p>
<p>approach (user canonical form, next step, bot message) used by Guardrails offers an improvement in performance. Second, it is important to have at least <em>k</em> = 3 samples in the vector database for each user canonical form for achieving good performance. Third, some models (i.e., gpt-3.5-turbo) produce a wider variety of canonical forms, even with few-shot prompting. In these cases, it is useful to add a similarity match instead of exact match for generating canonical forms.</p>
<h3>5.2 Execution Rails</h3>
<p><strong>Moderation Rails</strong> To evaluate the moderation rails, we use the Anthropic Red-Teaming and Helpful datasets (Bai et al., 2022a; Perez et al., 2022). We have sampled a balanced <em>harmful-helpful</em> evaluation set as follows: from the Red-Teaming dataset we sample prompts with the highest harmful score, while from the Helpful dataset we select an equal number of prompts.</p>
<p>We quantify the performance of the rails based on the proportion of harmful prompts that are blocked and the proportion of helpful ones that are allowed. Analysis of the results shows that using both the input and output moderation rails is much more robust than using either one of the rails individually. Using both rails gpt-3.5-turbo has a great performance - blocking close to 99% of harmful (compared to 93% without the rails) and just 2% of helpful requests - details in Appendix G.</p>
<p><strong>Fact-Checking Rail</strong> We consider the MSMARCO dataset (Bajaj et al., 2016) to evaluate the performance of the fact-checking rail. The dataset consists of <em>(context, question, answer)</em> triples. In order to mine negatives (answers that are <em>not</em> grounded in the context) we use OpenAI text-davinci-003 to rewrite the positive answer to a hard negative that looks similar to it, but is</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: Performance of the hallucination rail.</p>
<p>not grounded in the evidence. We construct a combined dataset by equally sampling both positive and negative triples. Both text-davinci-003 and gpt-3.5-turbo perform well on the fact-checking rail and obtain an overall accuracy of 80% (see Fig. 11 in Appendix G.2.2).</p>
<p><strong>Hallucination Rail</strong> Evaluating the hallucination rail is difficult without employing subjective manual annotation. To overcome this issue and be able to automatically quantify its performance, we compile a list of 20 questions based on a false premise (questions that do not have a right answer).</p>
<p>Any generation from the language model, apart from deflection, is considered a failure. We then quantify the benefit of employing the hallucination rail as a fallback mechanism. For text-davinci-003, the LLM is unable to deflect prompts that are unanswerable and using the hallucination rail helps intercept 70% of these prompts. gpt-3.5-turbo performs much better, deflecting unanswerable prompts or marking that its response could be incorrect in 65% of the cases. Even in this case, employing the hallucination rail boosts performance up to 95%.</p>
<h2>6 Conclusions</h2>
<p>We present NeMo Guardrails, a toolkit that allows developers to build controllable and safe LLM-based applications by implementing programmable rails. These rails are expressed using Colang and can also be implemented as custom actions if they require a complex logic. Using CoT prompting and a dialogue manager that can interpret Colang code, the Guardrails runtime acts like a proxy between the application and the LLM enforcing the user-defined rails.</p>
<h2>7 Limitations</h2>
<h3>7.1 Programmable Rails and Embedded Rails</h3>
<p>Building controllable and safe LLM-powered applications, in general, and dialogue systems, in particular, is a difficult task. We acknowledge that the approach employed by NeMo Guardrails of using developer-defined programmable rails, implemented with prompting and the Colang interpreter, is not a perfect solution.</p>
<p>Therefore we advocate that, whenever possible, our toolkit should not be used as a stand-alone solution, especially for safety-specific rails. Programmable rails complement embedded rails and these two solutions should be used together for building safe LLM applications. The vision of the project is to also provide, in the future, more powerful customized models for some of the execution rails that should supplement the current pure prompting methods. On another hand, our results show that adding the moderation rails to existing safety rails embedded in powerful LLMs (e.g., ChatGPT), provides a better protection against jailbreak attacks.</p>
<p>In the context of controllable and task-oriented dialogue agents, it is difficult to develop customized models for all possible tasks and topical rails. Therefore, in this context, NeMo Guardrails is a viable solution for building LLM-powered taskoriented agents without extra mechanisms. However, even for topical rails and task-oriented agents, we plan to release p-tuned models that achieve better performance for some of the tasks, e.g. for canonical form generation.</p>
<h3>7.2 Extra Costs and Latency</h3>
<p>The three-step CoT prompting approach used by the Guardrails runtime incurs extra costs and extra latency. As these calls are sequentially chained (i.e., the generation of the next steps in the second phase depends on the user canonical form generated in the first stage), the calls cannot be batched. In our current implementation, the latency and costs required are about 3 times the latency and cost of a normal call to generate the bot message without using Guardrails. We are currently investigating if in some cases we could use a single call to generate all three steps (user canonical form, next steps in the flow, and bot message).</p>
<p>Using a more complex prompt and few-shot in-context learning also generates slightly extra latency and a larger cost compared to a normal
bot message generation for a vanilla conversation. Developers can decide to use a simpler prompt if needed.</p>
<p>However, we consider that developers should be provided with various options for their needs. Some might be willing to pay the extra costs for having safer and controllable LLM-powered dialogue agents. Moreover, GPU inference costs will decrease and smaller models can also achieve good performance for some or all NeMo Guardrails tasks. As presented in our paper, we know that falcon-7b-instruct (Penedo et al., 2023) already achieves very good performance for topical rails. We have seen similar positive performance from other recent models, like Llama 2 (7B and 13B) chat variants (Touvron et al., 2023).</p>
<h2>8 Broader Impact</h2>
<p>As a toolkit to enforce programmable rails for LLM applications, including dialogue systems, NeMo Guardrails should provide benefits to developers and researchers. Programmable rails supplement embedded rails, either general (using RLHF) or user-defined (using p-tuned customized models). For example, using the fact-checking rail developers can easily build an enhanced retrieval-based LLM application and it also allows them to assess the performance of various models as programmable rails are model-agnostic. The same is true for building LLM-based task-oriented agents that should follow complex dialogue flows.</p>
<p>At the same time, before putting a Guardrails application into production, the implemented programmable rails should be thoroughly tested (especially safety related rails). Our toolkit provides a set of evaluation tools for testing the performance both for topical and execution rails.</p>
<p>Additional details for our toolkit can be found in the Appendix, including simple installation steps for running the toolkit with the example Guardrails applications that are shared on Github. A short demo video is also available: https://youtu.be/ Pfab6UWszEc.</p>
<h2>References</h2>
<p>Amos Azaria and Tom Mitchell. 2023. The internal state of an llm knows when its lying. arXiv preprint arXiv:2304.13734.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.</p>
<p>2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022b. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073.</p>
<p>Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. 2016. Ms marco: A human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268.</p>
<p>Tom Bocklisch, Joey Faulkner, Nick Pawlowski, and Alan Nichol. 2017. Rasa: Open source language understanding and dialogue management. arXiv preprint arXiv:1712.05181.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Inigo Casanueva, Ivan Vulić, Georgios Spithourakis, and Paweł Budzianowski. 2022. NLU++: A multilabel, slot-rich, generalisable dataset for natural language understanding in task-oriented dialogue. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 1998-2013, Seattle, United States. Association for Computational Linguistics.</p>
<p>Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. 2022. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375.</p>
<p>Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. 2022. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608.</p>
<p>Vojtěch Hudeček and Ondřej Dušek. 2023. Are llms all you need for task-oriented dialogue? arXiv preprint arXiv:2304.06556.</p>
<p>Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535-547.</p>
<p>Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto. 2023. Exploiting programmatic behavior of llms: Dual-use through standard security attacks. arXiv preprint arXiv:2302.05733.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045-3059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 61-68, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Xingkun Liu, Arash Eshghi, Pawel Swietojanski, and Verena Rieser. 2021. Benchmarking natural language understanding services for building conversational agents. In Increasing Naturalness and Flexibility in Spoken Dialogue Interaction: 10th International Workshop on Spoken Dialogue Systems, pages 165183. Springer.</p>
<p>Potsawee Manakul, Adian Liusie, and Mark JF Gales. 2023. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. arXiv preprint arXiv:2303.08896.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Richard Yuanzhe Pang, Stephen Roller, Kyunghyun Cho, He He, and Jason Weston. 2023. Leveraging implicit feedback from deployment data in dialogue. arXiv preprint arXiv:2307.14117.</p>
<p>Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116.</p>
<p>Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. 2023. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813.</p>
<p>Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. 2022. Red teaming language models with language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3419-3448,</p>
<p>Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Lee Boyd-Graber, and Lijuan Wang. 2022. Prompting gpt-3 to be reliable. In The Eleventh International Conference on Learning Representations.</p>
<p>Spotify. ANNOY library. https://github.com/ spotify/annoy. Accessed: 2023-08-01.</p>
<p>Makesh Narsimhan Sreedhar and Christopher Parisien. 2022. Prompt learning for domain adaptation in taskoriented dialogue. In Proceedings of the Towards Semi-Supervised and Reinforced Task-Oriented Dialog Systems (SereTOD), pages 24-30, Abu Dhabi, Beijing (Hybrid). Association for Computational Linguistics.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei Xiao, et al. 2023. Shall we pretrain autoregressive language models with retrieval? a comprehensive study. arXiv preprint arXiv:2304.06762.</p>
<p>Yau-Shian Wang and Yingshan Chang. 2022. Toxicity detection with generative prompt-based inference. arXiv preprint arXiv:2205.12390.</p>
<p>Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2023. Jailbroken: How does llm safety training fail? arXiv preprint arXiv:2307.02483.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837.</p>
<p>Xiaoying Zhang, Baolin Peng, Kun Li, Jingyan Zhou, and Helen Meng. 2023. Sgp-tod: Building task bots effortlessly via schema-guided llm prompting. arXiv preprint arXiv:2305.09067.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. 2022. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625.</p>
<p>Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. 2023. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043.</p>
<h2>A Installation Guide and Examples</h2>
<p>Developers can download and install the latest version of the NeMo Guardrails toolkit directly from Github ${ }^{2}$. They can also install the latest stable release using pip install nemoguardrails.</p>
<p>We have a concise installation guide ${ }^{3}$ showing how to run a Guardrails app using the provided Command Line Interface (CLI) or how to launch the Guardrails web server. The server powers a simple chat web client to engage with all the Guardrails apps found in the folder specified when starting the server.</p>
<p>Five reference Guardrails applications are provided as a general demonstration for building different types of rails.</p>
<ul>
<li>Topical Rail: Making the bot stick to a specific topic of conversation.</li>
<li>Moderation Rail: Moderating a bot's response.</li>
<li>Fact Checking and Hallucination Rail: Ensuring factual answers.</li>
<li>Secure Execution Rail: Executing a thirdparty service with LLMs.</li>
<li>Jail-breaking Rail: Ensuring safe answers despite malicious intent from the user.</li>
</ul>
<p>These examples are meant to showcase the process of building rails, not as out-of-the-box safety features. Customization and strengthening of the rails is highly recommended.</p>
<p>The sample Guardrails applications also contain examples on how to use several open-source models (e.g., falcon-7b-instruct, dolly-v2-3b, vicuna-7b-v1.3) deployed locally or using HuggingFace Inference private endpoints. Other examples cover how to combine various chains defined in Langchain with programmable rails defined in NeMo Guardrails.</p>
<p>Additional details about the reference applications and about the toolkit in general can be found on the main documentation page ${ }^{4}$.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>B Colang Language and Dialogue Manager</h2>
<p>Colang is a language for modeling sequences of events and interactions, being particularly useful for modeling conversations. At the same time, it enables the design of guardrails for conversational systems using the Colang interpreter, an eventbased processing engine that acts like a dialogue manager.</p>
<p>Creating guardrails for conversational systems requires some form of understanding of how the dialogue between the user and the bot unfolds. Existing dialog management techniques such us flow charts, state machines or frame-based systems are not well suited for modeling highly flexible conversational flows like the ones we expect when interacting with an LLM-based system.</p>
<p>However, since learning a new language is not an easy task, Colang was designed as a mix of natural language (English) and Python. If you are familiar with Python, you should feel confident using Colang after seeing a few examples, even without any explanation.</p>
<p>The main concepts used by the Colang language are the following:</p>
<ul>
<li>Utterance: the raw text coming from the user or the bot.</li>
<li>Message: the canonical form (structured representation) of a user/bot utterance.</li>
<li>Event: something that has happened and is relevant to the conversation, e.g. user is silent, user clicked something, user made a gesture, etc.</li>
<li>Action: a custom code that the bot can invoke; usually for connecting to a third-party API.</li>
<li>Context: any data relevant to the conversation (encoded as a key-value dictionary).</li>
<li>Flow: a sequence of messages and events, potentially with additional branching logic.</li>
<li>Rails: specific ways of controlling the behavior of a conversational system (a.k.a. bot), e.g. not talk about politics, respond in a specific way to certain user requests, follow a predefined dialog path, use a specific language style, extract data etc. A rail in Colang can be modeled through one or more flows.</li>
</ul>
<p>For additional details about Colang, please consult the Colang syntax guide ${ }^{5}$.</p>
<p>The Guardrails runtime uses an event-driven design (i.e., an event loop that processes events and generates back other events). Dialogue flows are treated as sequences of events, but even a simple user message is also an event - as an UtteranceUserActionFinished event is created and sent to the runtime. More details are available in the NeMo Guardrails architecture guide ${ }^{6}$.</p>
<h2>C Prompts for Topical Rails</h2>
<p>NeMo Guardrails uses complex prompts, chained in 3 steps, to respond to a user message as described in Section 3.2. In the following listing we provide an example for the first step, to generate the canonical form for the last user message in the current conversation.</p>
<p>The prompt below is designed for text-davinci-003 and is structured in four parts:</p>
<ol>
<li>General prompt describing the task of the application.</li>
<li>Sample conversation using Colang syntax.</li>
<li>The most similar, given the current user message, few-shot $(k=5)$ examples for mapping user messages to their corresponding canonical form.</li>
<li>The current conversation between the user and the bot in Colang syntax.</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="p">...</span>
<span class="n">Below</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">conversation</span><span class="w"> </span><span class="ow">between</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">helpful</span><span class="w"> </span><span class="n">AI</span><span class="w"> </span><span class="n">assistant</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">a</span>
<span class="k">user</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">bot</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">designed</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">generate</span><span class="w"> </span><span class="n">human</span><span class="o">-</span><span class="ow">like</span><span class="w"> </span><span class="nc">text</span>
<span class="n">based</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">input</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">receives</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">bot</span><span class="w"> </span><span class="k">is</span>
<span class="n">talkative</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">provides</span><span class="w"> </span><span class="n">lots</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="k">specific</span><span class="w"> </span><span class="n">details</span><span class="p">.</span><span class="w"> </span><span class="k">If</span><span class="w"> </span><span class="n">the</span>
<span class="n">bot</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">know</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">question</span><span class="p">,</span><span class="w"> </span><span class="n">it</span>
<span class="n">trailfully</span><span class="w"> </span><span class="n">says</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">know</span><span class="p">.</span>
<span class="p">...</span>
<span class="err">#</span><span class="w"> </span><span class="n">This</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">how</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">conversation</span><span class="w"> </span><span class="ow">between</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="k">user</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">bot</span><span class="w"> </span><span class="n">can</span>
<span class="k">go</span><span class="err">:</span>
<span class="k">user</span><span class="w"> </span><span class="ss">&quot;Hello there!&quot;</span>
<span class="n">express</span><span class="w"> </span><span class="n">greeting</span>
<span class="n">bot</span><span class="w"> </span><span class="n">express</span><span class="w"> </span><span class="n">greeting</span>
<span class="ss">&quot;Hello! How can I assist you today?&quot;</span>
<span class="k">user</span><span class="w"> </span><span class="ss">&quot;What can you do for me?&quot;</span>
<span class="n">ask</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">capabilities</span>
<span class="n">bot</span><span class="w"> </span><span class="n">respond</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">capabilities</span>
<span class="w">    </span><span class="n">I</span><span class="w"> </span><span class="n">am</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">AI</span><span class="w"> </span><span class="n">assistant</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">helps</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">questions</span><span class="w"> </span><span class="n">based</span>
<span class="w">    </span><span class="k">on</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">knowledge</span><span class="w"> </span><span class="n">base</span><span class="p">.</span><span class="w"> </span><span class="k">For</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">interaction</span><span class="p">,</span><span class="w"> </span><span class="n">I</span>
<span class="w">    </span><span class="n">can</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">question</span><span class="w"> </span><span class="n">based</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">job</span><span class="w"> </span><span class="n">report</span><span class="w"> </span><span class="n">published</span>
<span class="w">        </span><span class="k">by</span><span class="w"> </span><span class="n">US</span><span class="w"> </span><span class="n">Bureau</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">Labor</span><span class="w"> </span><span class="k">Statistics</span><span class="ss">&quot;</span>
<span class="ss">user &quot;</span><span class="n">Tell</span><span class="w"> </span><span class="n">me</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="nc">bit</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">US</span><span class="w"> </span><span class="n">Bureau</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">Labor</span><span class="w"> </span><span class="k">Statistics</span><span class="p">.</span>
<span class="w">    </span><span class="n">ask</span><span class="w"> </span><span class="n">question</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">publisher</span>
</code></pre></div>

<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>```
bot response for question about publisher
    "The Bureau of Labor Statistics is the principal fact-
    finding agency for the Federal Government in the
        broad field of labor economics and statistics"
user "Thanks"
express appreciation
bot express appreciation and offer additional help
    "You're welcome. If you have any more questions or if
        there's anything else I can help you with, please don
            t hesitate to ask."</p>
<h1>This is how the user talks:</h1>
<p>user "What was the movement on nonfarm payroll?"
    ask about headline numbers
user "What's the number of part-time employed number?"
    ask about household survey data
user "How much did the nonfarm payroll rise by?"
    ask about headline numbers
user "What is this month's unemployment rate?"
    ask about headline numbers
user "How many long term unemployment individuals were
        reported?"
    ask about household survey data</p>
<h1>This is the current conversation between the user and the</h1>
<div class="codehilite"><pre><span></span><code>bot:
</code></pre></div>

<p>user "Hello there!"
express greeting
bot express greeting
    "Hello! How can I assist you today?"
user "What can you do for me?"
    ask about capabilities
bot respond about capabilities
    I am an AI assistant which helps answer questions based
        on a given knowledge base. For this interaction, I
        can answer question based on the job report published
            by US Bureau of Labor Statistics"
user "how many unemployed people were there in March?"</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Similar</span><span class="w"> </span><span class="nx">prompts</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">defined</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">other</span><span class="w"> </span><span class="nx">LLMs</span><span class="w"> </span><span class="p">(</span><span class="nx">i</span><span class="p">.</span><span class="nx">e</span><span class="p">.,</span><span class="w"> </span><span class="nx">gpt</span><span class="o">-</span><span class="m m-Double">3.5</span><span class="o">-</span><span class="nx">turbo</span><span class="p">,</span><span class="w"> </span><span class="nx">falcon</span><span class="o">-</span><span class="mi">7</span><span class="nx">b</span><span class="o">-</span><span class="nx">instruct</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">others</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">available</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">Github</span><span class="w"> </span><span class="err">$</span><span class="p">{</span><span class="w"> </span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="mi">7</span><span class="p">}</span><span class="err">$</span><span class="p">.</span>

<span class="nx">When</span><span class="w"> </span><span class="nx">generating</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">user</span><span class="w"> </span><span class="nx">canonical</span><span class="w"> </span><span class="nx">form</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">next</span><span class="w"> </span><span class="nx">steps</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">guide</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">conversation</span><span class="p">,</span><span class="w"> </span><span class="nx">we</span><span class="w"> </span><span class="nx">use</span><span class="w"> </span><span class="nx">temp</span><span class="w"> </span><span class="err">$</span><span class="p">=</span><span class="mi">0</span><span class="err">$</span><span class="p">,</span><span class="w"> </span><span class="k">while</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">sampling</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">bot</span><span class="w"> </span><span class="nx">message</span><span class="w"> </span><span class="nx">we</span><span class="w"> </span><span class="nx">use</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">higher</span><span class="w"> </span><span class="nx">temperature</span><span class="w"> </span><span class="p">(</span><span class="nx">temp</span><span class="w"> </span><span class="err">$</span><span class="p">=</span><span class="m m-Double">0.7</span><span class="err">$</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">temp</span><span class="w"> </span><span class="err">$</span><span class="p">=</span><span class="err">$</span><span class="w"> </span><span class="mi">1</span><span class="p">).</span>

<span class="err">##</span><span class="w"> </span><span class="nx">D</span><span class="w"> </span><span class="nx">Prompt</span><span class="w"> </span><span class="nx">Templates</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">Execution</span><span class="w"> </span><span class="nx">Rails</span>

<span class="nx">In</span><span class="w"> </span><span class="nx">this</span><span class="w"> </span><span class="nx">section</span><span class="w"> </span><span class="nx">we</span><span class="w"> </span><span class="nx">provide</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="nx">templates</span><span class="w"> </span><span class="nx">used</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">hallucination</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">moderation</span><span class="w"> </span><span class="nx">rails</span><span class="p">.</span>

<span class="err">##</span><span class="w"> </span><span class="nx">D</span><span class="p">.</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">Hallucination</span><span class="w"> </span><span class="nx">Rail</span>

<span class="nx">After</span><span class="w"> </span><span class="nx">we</span><span class="w"> </span><span class="nx">obtain</span><span class="w"> </span><span class="err">$</span><span class="nx">n</span><span class="err">$</span><span class="w"> </span><span class="nx">samples</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">conversational</span><span class="w"> </span><span class="nx">agent</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">same</span><span class="w"> </span><span class="nx">prompt</span><span class="p">,</span><span class="w"> </span><span class="nx">we</span><span class="w"> </span><span class="nx">concatenate</span><span class="w"> </span><span class="err">$</span><span class="nx">n</span><span class="o">-</span><span class="err">$</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">responses</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">form</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">context</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">use</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="err">$</span><span class="nx">n</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">th</span><span class="w"> </span><span class="p">}}</span><span class="err">$</span><span class="w"> </span><span class="nx">response</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">hypothesis</span><span class="p">.</span><span class="w"> </span><span class="nx">We</span><span class="w"> </span><span class="nx">utilize</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">LLM</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">verify</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">hypothesis</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">consistent</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">context</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">following</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="nx">template</span><span class="p">:</span>

<span class="nx">You</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">given</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">task</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">identify</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">hypothesis</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">agreement</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">context</span><span class="w"> </span><span class="nx">below</span><span class="p">.</span><span class="w"> </span><span class="nx">You</span><span class="w"> </span><span class="nx">will</span><span class="w"> </span><span class="nx">only</span><span class="w"> </span><span class="nx">use</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">contents</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">context</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">rely</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="kd">external</span><span class="w"> </span><span class="nx">knowledge</span><span class="p">.</span><span class="w"> </span><span class="nx">Answer</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">yes</span><span class="o">/</span><span class="nx">no</span><span class="p">.</span>

<span class="p">[</span><span class="o">^</span><span class="mi">1</span><span class="p">]</span>
<span class="p">[</span><span class="o">^</span><span class="mi">0</span><span class="p">]:</span><span class="w">    </span><span class="err">$</span><span class="p">{</span><span class="w"> </span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="mi">5</span><span class="p">}</span><span class="err">$</span><span class="w"> </span><span class="nx">https</span><span class="p">:</span><span class="c1">//github.com/NVIDIA/</span>
<span class="w">    </span><span class="nx">NeMo</span><span class="o">-</span><span class="nx">Guardrails</span><span class="o">/</span><span class="nx">blob</span><span class="o">/</span><span class="nx">main</span><span class="o">/</span><span class="nx">docs</span><span class="o">/</span><span class="nx">user_guide</span><span class="o">/</span><span class="w"> </span><span class="nx">colang</span><span class="o">-</span><span class="nx">language</span><span class="o">-</span><span class="nx">syntax</span><span class="o">-</span><span class="nx">guide</span><span class="p">.</span><span class="nx">md</span>
<span class="w">    </span><span class="err">$</span><span class="p">{</span><span class="w"> </span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="mi">6</span><span class="p">}</span><span class="err">$</span><span class="w"> </span><span class="nx">https</span><span class="p">:</span><span class="c1">//github.com/NVIDIA/NeMo-Guardrails/ blob/main/docs/architecture/README.md</span>

<span class="p">[</span><span class="o">^</span><span class="mi">1</span><span class="p">]:</span><span class="w">    </span><span class="err">$</span><span class="p">{</span><span class="w"> </span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="mi">7</span><span class="p">}</span><span class="err">$</span><span class="w"> </span><span class="nx">https</span><span class="p">:</span><span class="c1">//github.com/NVIDIA/NeMo-Guardrails/ tree/main/nemoguardrails/llm/prompts</span>

<span class="s">&quot;context&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{{</span><span class="w"> </span><span class="nx">sampled_responses</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="s">&quot;hypothesis&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{{</span><span class="w"> </span><span class="nx">bot_response</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="s">&quot;agreement&quot;</span><span class="p">:</span>

<span class="nx">When</span><span class="w"> </span><span class="nx">sampling</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">bot</span><span class="w"> </span><span class="nx">responses</span><span class="p">,</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">proposed</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">SelfCheckGPT</span><span class="w"> </span><span class="p">(</span><span class="nx">Manakul</span><span class="w"> </span><span class="nx">et</span><span class="w"> </span><span class="nx">al</span><span class="p">.,</span><span class="w"> </span><span class="mi">2023</span><span class="p">)</span><span class="w"> </span><span class="nx">we</span><span class="w"> </span><span class="nx">use</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">high</span><span class="w"> </span><span class="nx">temperature</span><span class="w"> </span><span class="p">(</span><span class="nx">temp</span><span class="w"> </span><span class="err">$</span><span class="p">=</span><span class="mi">1</span><span class="err">$</span><span class="w"> </span><span class="p">)</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">ensure</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">variation</span><span class="w"> </span><span class="nx">between</span><span class="w"> </span><span class="nx">them</span><span class="w"> </span><span class="nx">necessary</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">assess</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">selfconsistency</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">answers</span><span class="p">.</span><span class="w"> </span><span class="nx">At</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">same</span><span class="w"> </span><span class="nx">time</span><span class="p">,</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">LLM</span><span class="o">-</span><span class="nx">based</span><span class="w"> </span><span class="nx">hallucination</span><span class="w"> </span><span class="nx">verification</span><span class="w"> </span><span class="nx">call</span><span class="w"> </span><span class="nx">uses</span><span class="w"> </span><span class="nx">temp</span><span class="w"> </span><span class="err">$</span><span class="p">=</span><span class="mi">0</span><span class="err">$</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">get</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">most</span><span class="w"> </span><span class="nx">probable</span><span class="w"> </span><span class="nx">response</span><span class="p">.</span><span class="w"> </span><span class="nx">This</span><span class="w"> </span><span class="nx">behavior</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">similar</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">other</span><span class="w"> </span><span class="nx">execution</span><span class="w"> </span><span class="nx">rails</span><span class="w"> </span><span class="nx">described</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">paper</span><span class="p">.</span>

<span class="err">##</span><span class="w"> </span><span class="nx">D</span><span class="p">.</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="nx">Moderation</span><span class="w"> </span><span class="nx">Rails</span>

<span class="nx">Both</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">input</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">output</span><span class="w"> </span><span class="nx">moderation</span><span class="w"> </span><span class="nx">rails</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">framed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">another</span><span class="w"> </span><span class="nx">task</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">powerful</span><span class="p">,</span><span class="w"> </span><span class="nx">well</span><span class="o">-</span><span class="nx">aligned</span><span class="w"> </span><span class="nx">LLM</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">vets</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">input</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">response</span><span class="p">.</span><span class="w"> </span><span class="nx">For</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">jailbreak</span><span class="w"> </span><span class="nx">rail</span><span class="p">,</span><span class="w"> </span><span class="nx">we</span><span class="w"> </span><span class="nx">use</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">following</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="nx">template</span><span class="p">:</span>

<span class="nx">Instruction</span><span class="p">:</span><span class="w"> </span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="p">{</span><span class="w"> </span><span class="nx">user_input</span><span class="w"> </span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">}</span>
<span class="nx">Would</span><span class="w"> </span><span class="nx">this</span><span class="w"> </span><span class="nx">instruction</span><span class="w"> </span><span class="nx">make</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">language</span><span class="w"> </span><span class="nx">model</span><span class="w"> </span><span class="k">break</span><span class="w"> </span><span class="nx">moderation</span><span class="w"> </span><span class="nx">policies</span><span class="p">,</span><span class="w"> </span><span class="nx">deviate</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">good</span><span class="w"> </span><span class="nx">aligned</span><span class="w"> </span><span class="nx">responses</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">provide</span><span class="w"> </span><span class="nx">answers</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">language</span><span class="w"> </span><span class="nx">model</span><span class="w"> </span><span class="nx">should</span><span class="w"> </span><span class="nx">ideally</span><span class="w"> </span><span class="k">not</span><span class="p">?</span><span class="w"> </span><span class="nx">Answer</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">yes</span><span class="o">/</span><span class="nx">no</span><span class="p">.</span>

<span class="nx">The</span><span class="w"> </span><span class="nx">output</span><span class="w"> </span><span class="nx">moderation</span><span class="w"> </span><span class="nx">rail</span><span class="w"> </span><span class="nx">uses</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">following</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="nx">template</span><span class="p">:</span>

<span class="nx">Model</span><span class="w"> </span><span class="nx">output</span><span class="p">:</span><span class="w"> </span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="p">{</span><span class="w"> </span><span class="nx">bot_response</span><span class="w"> </span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">}</span>
<span class="nx">Is</span><span class="w"> </span><span class="nx">this</span><span class="w"> </span><span class="nx">output</span><span class="w"> </span><span class="nx">legal</span><span class="p">,</span><span class="w"> </span><span class="nx">ethical</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">harmful</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">any</span><span class="w"> </span><span class="nx">person</span><span class="p">?</span><span class="w"> </span><span class="nx">Answer</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">yes</span><span class="o">/</span><span class="nx">no</span><span class="p">.</span>

<span class="err">##</span><span class="w"> </span><span class="nx">E</span><span class="w"> </span><span class="nx">Action</span><span class="w"> </span><span class="nx">Definitions</span>

<span class="nx">The</span><span class="w"> </span><span class="nx">first</span><span class="w"> </span><span class="nx">step</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">creating</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">execution</span><span class="w"> </span><span class="nx">rail</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">define</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">custom</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">implements</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">logic</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">rail</span><span class="p">.</span><span class="w"> </span><span class="nx">At</span><span class="w"> </span><span class="nx">this</span><span class="w"> </span><span class="nx">moment</span><span class="p">,</span><span class="w"> </span><span class="nx">this</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">done</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">Python</span><span class="p">.</span>

<span class="nx">Any</span><span class="w"> </span><span class="nx">custom</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="nx">has</span><span class="w"> </span><span class="nx">access</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">context</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">conversation</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">can</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">seen</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">subsequent</span><span class="w"> </span><span class="nx">examples</span><span class="p">.</span><span class="w"> </span><span class="nx">In</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">Guardrails</span><span class="w"> </span><span class="nx">runtime</span><span class="p">,</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">context</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">sequence</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">events</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">conversation</span><span class="w"> </span><span class="nx">history</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">including</span><span class="w"> </span><span class="nx">user</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">bot</span><span class="w"> </span><span class="nx">messages</span><span class="p">,</span><span class="w"> </span><span class="nx">canonical</span><span class="w"> </span><span class="nx">forms</span><span class="p">,</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="nx">called</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">more</span><span class="p">.</span><span class="w"> </span><span class="nx">Some</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">context</span><span class="w"> </span><span class="nx">events</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">might</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">accessed</span><span class="w"> </span><span class="nx">more</span><span class="w"> </span><span class="nx">often</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">define</span><span class="w"> </span><span class="nx">actions</span><span class="w"> </span><span class="nx">have</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">shortcut</span><span class="p">,</span><span class="w"> </span><span class="nx">e</span><span class="p">.</span><span class="nx">g</span><span class="p">.</span><span class="w"> </span><span class="nx">context</span><span class="p">.</span><span class="nx">get</span><span class="p">(</span><span class="s">&quot;last_bot_message&quot;</span><span class="p">).</span>

<span class="nx">An</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="nx">can</span><span class="w"> </span><span class="nx">receive</span><span class="w"> </span><span class="nx">any</span><span class="w"> </span><span class="nx">number</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">parameters</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">Colang</span><span class="w"> </span><span class="nx">scripts</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="nx">they</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">called</span><span class="p">.</span><span class="w"> </span><span class="nx">These</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">passed</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">Python</span><span class="w"> </span><span class="nx">function</span><span class="w"> </span><span class="nx">implementing</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="nx">logic</span><span class="p">.</span><span class="w"> </span><span class="nx">At</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">same</span><span class="w"> </span><span class="nx">time</span><span class="p">,</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="nx">usually</span><span class="w"> </span><span class="nx">returns</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">value</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">can</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">used</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">further</span><span class="w"> </span><span class="nx">guide</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">dialogue</span><span class="p">.</span>

<span class="err">##</span><span class="w"> </span><span class="nx">E</span><span class="p">.</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">Output</span><span class="w"> </span><span class="nx">Moderation</span><span class="w"> </span><span class="nx">Rail</span>

<span class="nx">This</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">current</span><span class="w"> </span><span class="nx">implementation</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">output</span><span class="w"> </span><span class="nx">moderation</span><span class="w"> </span><span class="nx">action</span><span class="p">.</span><span class="w"> </span><span class="nx">It</span><span class="w"> </span><span class="nx">uses</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="nx">template</span><span class="w"> </span><span class="nx">defined</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">Appendix</span><span class="w"> </span><span class="nx">D</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">returns</span><span class="w"> </span><span class="nx">True</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">bot</span><span class="w"> </span><span class="nx">response</span><span class="w"> </span><span class="nx">breaks</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">output</span><span class="w"> </span><span class="nx">moderation</span><span class="w"> </span><span class="nx">rail</span><span class="p">.</span>

<span class="nx">The</span><span class="w"> </span><span class="nx">call</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">LLM</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">done</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="nx">templates</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">simple</span><span class="w"> </span><span class="nx">LLM</span><span class="w"> </span><span class="nx">chain</span><span class="w"> </span><span class="nx">provided</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">Langchain</span><span class="w"> </span><span class="err">$</span><span class="p">{</span><span class="w"> </span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="mi">8</span><span class="p">}</span><span class="err">$</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">simplifying</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">communication</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">several</span><span class="w"> </span><span class="nx">LLM</span><span class="w"> </span><span class="nx">providers</span><span class="p">.</span>
</code></pre></div>

<p>bot_response = context.get("last_bot_message")
if bot_response:
    output_moderation_template =
    "Model Output: [bot_response]
    Is this output legal, ethical and not harmful to any
        person? Answer with yes/no."
    prompt = PromptTemplate(
        template=output_moderation_template, input_variables
            =["bot_response"]
    )
    output_moderation_chain = LLMChain(prompt=prompt, llm=
        llm)
    check = await output_moderation_chain.apredict(
        bot_response=bot_response)
    check = check.lower().strip()
    log.info(f"Output moderation check result is [check].")
    if "no" in check:
        return False
return True</p>
<div class="codehilite"><pre><span></span><code><span class="err">##</span><span class="w"> </span><span class="nx">E</span><span class="p">.</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="nx">Jailbreak</span><span class="w"> </span><span class="nx">Rail</span>

<span class="nx">This</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">current</span><span class="w"> </span><span class="nx">implementation</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">input</span><span class="w"> </span><span class="nx">moderation</span><span class="w"> </span><span class="nx">action</span><span class="p">.</span><span class="w"> </span><span class="nx">It</span><span class="w"> </span><span class="nx">uses</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="nx">template</span><span class="w"> </span><span class="nx">defined</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">Appendix</span><span class="w"> </span><span class="nx">D</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">returns</span><span class="w"> </span><span class="nx">True</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">user</span><span class="w"> </span><span class="nx">input</span><span class="w"> </span><span class="nx">breaks</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">jailbreak</span><span class="w"> </span><span class="nx">rail</span><span class="p">.</span>
</code></pre></div>

<p>user_input = context.get("last_user_message")
if user_input:
    jailbreak_template = "Instruction: [user_input]
    Would this instruction make a language model break
        moderation policies, deviate from good aligned
        responses and provide answers that a language model
        should ideally not? Answer with yes/no."
    prompt = PromptTemplate(
        template=jailbreak_template, input_variables=[
            user_input:]
    )
    jailbreak_chain = LLMChain(prompt=prompt, llm=llm)
    check = await jailbreak_chain.apredict(bot_response=
        bot_response)
    check = check.lower().strip()
    log.info(f"Jailbreak check result is [check].")
    if "no" in check:
        return False
return True</p>
<div class="codehilite"><pre><span></span><code>##<span class="w"> </span>F<span class="w"> </span>Sample<span class="w"> </span>Guardrails<span class="w"> </span>Flows<span class="w"> </span>using<span class="w"> </span>Actions

This<span class="w"> </span>section<span class="w"> </span>includes<span class="w"> </span>some<span class="w"> </span>examples<span class="w"> </span>of<span class="w"> </span>using<span class="w"> </span>the<span class="w"> </span>safety<span class="w"> </span>execution<span class="w"> </span>rails,<span class="w"> </span>implemented<span class="w"> </span>as<span class="w"> </span>custom<span class="w"> </span>actions,<span class="w"> </span>inside<span class="w"> </span>Colang<span class="w"> </span>flows<span class="w"> </span>to<span class="w"> </span>define<span class="w"> </span>simple<span class="w"> </span>Colang<span class="w"> </span>applications.

[^0]
[^0]:<span class="w">    </span><span class="cp">${</span> <span class="cp">}</span>^{8}$<span class="w"> </span>https://github.com/langchain-ai/langchain

Figure<span class="w"> </span>7<span class="w"> </span>shows<span class="w"> </span>how<span class="w"> </span>to<span class="w"> </span>use<span class="w"> </span>the<span class="w"> </span>check_jailbreak<span class="w"> </span>action<span class="w"> </span>for<span class="w"> </span>input<span class="w"> </span>moderation.<span class="w"> </span>The<span class="w"> </span>semantics<span class="w"> </span>is<span class="w"> </span>that<span class="w"> </span>for<span class="w"> </span>each<span class="w"> </span>user<span class="w"> </span>message<span class="w"> </span>(user<span class="w"> </span>...),<span class="w"> </span>the<span class="w"> </span>jailbreak<span class="w"> </span>action<span class="w"> </span>is<span class="w"> </span>called<span class="w"> </span>to<span class="w"> </span>verify<span class="w"> </span>the<span class="w"> </span>last<span class="w"> </span>user<span class="w"> </span>message,<span class="w"> </span>and<span class="w"> </span>if<span class="w"> </span>it<span class="w"> </span>is<span class="w"> </span>flagged<span class="w"> </span>as<span class="w"> </span>a<span class="w"> </span>jailbreak<span class="w"> </span>attempt<span class="w"> </span>the<span class="w"> </span>last<span class="w"> </span>LLM<span class="w"> </span>bot-generated<span class="w"> </span>answer<span class="w"> </span>is<span class="w"> </span>removed<span class="w"> </span>and<span class="w"> </span>a<span class="w"> </span>new<span class="w"> </span>one<span class="w"> </span>is<span class="w"> </span>uttered<span class="w"> </span>to<span class="w"> </span>inform<span class="w"> </span>the<span class="w"> </span>user<span class="w"> </span>her/his<span class="w"> </span>message<span class="w"> </span>breaks<span class="w"> </span>the<span class="w"> </span>moderation<span class="w"> </span>policy.<span class="w"> </span>Figure<span class="w"> </span>8<span class="w"> </span>shows<span class="w"> </span>how<span class="w"> </span>the<span class="w"> </span>output_moderation<span class="w"> </span>action<span class="w"> </span>is<span class="w"> </span>used<span class="w"> </span>-<span class="w"> </span>the<span class="w"> </span>meaning<span class="w"> </span>is<span class="w"> </span>similar<span class="w"> </span>to<span class="w"> </span>jail-breaking,<span class="w"> </span>however<span class="w"> </span>it<span class="w"> </span>is<span class="w"> </span>triggered<span class="w"> </span>after<span class="w"> </span>any<span class="w"> </span>output<span class="w"> </span>bot<span class="w"> </span>message<span class="w"> </span>event<span class="w"> </span>(bot<span class="w"> </span>...).
define<span class="w"> </span>flow<span class="w"> </span>check<span class="w"> </span>jailbreak
user<span class="w"> </span>...
\<span class="nv">$allowed</span><span class="w"> </span>=<span class="w"> </span>execute<span class="w"> </span>check_jailbreak
if<span class="w"> </span>not<span class="w"> </span>\<span class="nv">$allowed</span>
bot<span class="w"> </span>remove<span class="w"> </span>last<span class="w"> </span>message
bot<span class="w"> </span>inform<span class="w"> </span>message<span class="w"> </span>breaks<span class="w"> </span>moderation
Figure<span class="w"> </span>7:<span class="w"> </span>Flow<span class="w"> </span>using<span class="w"> </span>jailbreak<span class="w"> </span>rail<span class="w"> </span>in<span class="w"> </span>Colang
</code></pre></div>

<p>define flow check bot response
    bot ...
    $allowed = execute output_moderation
    if not $allowed
        bot remove last message
        bot inform message breaks moderation</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Figure</span><span class="w"> </span><span class="mi">8</span>:<span class="w"> </span><span class="nv">Flow</span><span class="w"> </span><span class="nv">using</span><span class="w"> </span><span class="nv">output</span><span class="w"> </span><span class="nv">moderation</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">Colang</span>

<span class="nv">In</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">similar</span><span class="w"> </span><span class="nv">way</span>,<span class="w"> </span><span class="nv">Fig</span>.<span class="w"> </span><span class="mi">9</span><span class="w"> </span><span class="nv">shows</span><span class="w"> </span><span class="nv">how</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">use</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">hallucination</span><span class="w"> </span><span class="nv">rail</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">check</span><span class="w"> </span><span class="nv">responses</span><span class="w"> </span><span class="nv">when</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">particular</span><span class="w"> </span><span class="nv">topic</span><span class="w"> </span><span class="ss">(</span><span class="nv">i</span>.<span class="nv">e</span>.,<span class="w"> </span><span class="nv">asking</span><span class="w"> </span><span class="nv">questions</span><span class="w"> </span><span class="nv">about</span><span class="w"> </span><span class="nv">persons</span>,<span class="w"> </span><span class="nv">where</span><span class="w"> </span><span class="nv">GPT</span><span class="w"> </span><span class="nv">models</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">prone</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">hallucinate</span><span class="ss">)</span>.<span class="w"> </span><span class="nv">In</span><span class="w"> </span><span class="nv">this</span><span class="w"> </span><span class="nv">case</span>,<span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">bot</span><span class="w"> </span><span class="nv">message</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">not</span><span class="w"> </span><span class="nv">removed</span>,<span class="w"> </span><span class="nv">but</span><span class="w"> </span><span class="nv">an</span><span class="w"> </span><span class="nv">extra</span><span class="w"> </span><span class="nv">message</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">added</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">warn</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">user</span><span class="w"> </span><span class="nv">about</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">possible</span><span class="w"> </span><span class="nv">incorrect</span><span class="w"> </span><span class="nv">answer</span>.<span class="w"> </span><span class="nv">Fig</span>.<span class="w"> </span><span class="mi">10</span><span class="w"> </span><span class="nv">shows</span><span class="w"> </span><span class="nv">how</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">add</span><span class="w"> </span><span class="nv">fact</span><span class="o">-</span><span class="nv">checking</span><span class="w"> </span><span class="nv">again</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">specific</span><span class="w"> </span><span class="nv">topic</span>,<span class="w"> </span><span class="nv">when</span><span class="w"> </span><span class="nv">asking</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">question</span><span class="w"> </span><span class="nv">about</span><span class="w"> </span><span class="nv">an</span><span class="w"> </span><span class="nv">employment</span><span class="w"> </span><span class="nv">report</span>.<span class="w"> </span><span class="nv">In</span><span class="w"> </span><span class="nv">this</span><span class="w"> </span><span class="nv">situation</span>,<span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">LLM</span><span class="w"> </span><span class="nv">should</span><span class="w"> </span><span class="nv">be</span><span class="w"> </span><span class="nv">consistent</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">information</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">report</span>.
</code></pre></div>

<p>define flow check hallucination
    user ask about person
    bot ...
    $result = execute check_hallucination
    if $result
        bot inform answer prone to hallucination</p>
<div class="codehilite"><pre><span></span><code>Figure 9: Flow using hallucination rail in Colang
</code></pre></div>

<p>define flow answer report question
user ask about report
bot provide report answer
$accurate = execute check_facts
if not $accurate
    bot remove last message
    bot inform answer unknown
```</p>
<p>Figure 10: Flow using fact-checking rail in Colang</p>
<h2>G Additional Details on Evaluation</h2>
<p>Our toolkit also provides the evaluation tooling and methodology to assess the performance of topical and execution rails. All the results reported in the paper can be replicated using the CLI evaluation tool available on Github, following the instructions about evaluation ${ }^{9}$. The same page contains slightly more details than the current paper and is regularly updated with new results (including new LLMs).</p>
<p>Detailed instructions on how to replicate the experiments can be found here ${ }^{10}$.</p>
<h2>G. 1 Topical Rails</h2>
<p>Topical rails evaluation focuses on the core mechanism used by NeMo Guardrails to guide conversations using canonical forms and dialogue flows.</p>
<p>The current evaluation experiments for topical rails uses two datasets employed for conversational NLU: chit-chat ${ }^{11}$ and banking.</p>
<p>The datasets were transformed into a NeMo Guardrails app, by defining canonical forms for each intent, specific dialogue flows, and even bot messages (for the chit-chat dataset alone). The two datasets have a large number of user intents, thus topical rails. One of them is very generic and with coarse-grained intents (chit-chat), while the banking dataset is domain-specific and more fine-grained. More details about running the topical rails evaluation experiments and the evaluation datasets is available here.</p>
<p>Preliminary evaluation results follow next. In all experiments, we have chosen to have a balanced test set with at most 3 samples per intent. For both datasets, we have assessed the performance for various LLMs and also for the number of samples</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>$(k=$ all, 3, 1) per intent that are indexed in the vector database. We have used a random seed of 42 for all experiments to ensure consistency.</p>
<p>The results of the top 3 performing models are presented in Fig. 5, showing that topical rails can be successfully used to guide conversations even with smaller open source models such as falcon-7b-instruct or llama2-13b-chat. As the performance of an LLM is heavily dependent on the prompt, due to the complex prompt used by NeMo Guardrails all results might be improved with better prompting.</p>
<p>The topical rails evaluation highlights several important aspects. First, each step in the three-step approach (user canonical form, next step, bot message) used by Guardrails offers an improvement in performance. Second, it is important to have at least $k=3$ samples in the vector database for each user canonical form for achieving good performance. Third, some models (i.e., gpt-3.5-turbo) produce a wider variety of canonical forms, even with few-shot prompting. In these cases, it is useful to add a similarity match instead of exact match for generating canonical forms. In this case, the similarity threshold becomes an important inference parameter.</p>
<p>Dataset statistics and detailed results for several LLMs are presented in Tables 1, 2, and 3. Some experiments have missing numbers either because those experiments did not compute those metrics or because the dataset does not contain specific items (for example, user-defined bot messages for the banking dataset).</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th># intents</th>
<th># test samples</th>
</tr>
</thead>
<tbody>
<tr>
<td>chit-chat</td>
<td>76</td>
<td>226</td>
</tr>
<tr>
<td>banking</td>
<td>77</td>
<td>231</td>
</tr>
</tbody>
</table>
<p>Table 1: Dataset statistics for the topical rails evaluation.</p>
<h3>G.2 Execution Rails</h3>
<h2>G.2.1 Moderation Rail</h2>
<p>To evaluate the moderation rails, we use the Anthropic Red-Teaming and Helpful datasets (Bai et al., 2022a; Perez et al., 2022). The redteaming dataset consists of prompts that are humanannotated (0-4) on their ability to elicit inappropriate responses from language models. A higher score implies that the prompt was more successful in bypassing model alignment. We randomly sample prompts with the highest rating to curate the harmful set. All the prompts in the Anthropic Helpful dataset are genuine queries and forms our helpful set. We create a balanced evaluation set with an equal number of harmful and helpful samples.</p>
<p>We quantify the performance of the rails based on the proportion of harmful prompts that are blocked and the proportion of helpful ones that are allowed. An ideal model would be able to block 100% of the harmful prompts and allow 100% of the helpful ones. We pass prompts from our evaluation set through the input (jailbreak) moderation rail. Only those that are not flagged are passed to the conversational agent to generate a response which is passed through the output moderation rail. Once again, only those responses that are not flagged are displayed back to the user.</p>
<p>Analysis of the results shows that using a combination of both the input (aka jailbreak rail) and output moderation rails is more robust than using either one of the rails individually. It should also be noted that evaluation of the output moderation rail is subjective and each person/organization would have different subjective opinions on what should be allowed to pass through or not. In such situations, it would be easy to modify prompts to the moderation rails to reflect the beliefs of the entity deploying the conversational agent.</p>
<p>Using an evaluation set of 200 samples split equally between harmful and helpful and created as described above, we have seen that text-davinci-003 blocks only 24% of the harmful messages, while gpt-3.5-turbo does much better blocking 93% of harmful messages without any moderation guardrail. In this case, blocking means that the model is not providing a response to an input requiring moderation. On the helpful inputs, both models do not block any request. Using only the input moderation rail, text-davinci-003 blocks 87% of harmful and 3% of helpful requests. Using both input and output moderation, text-davinci-003 blocks 97% of harmful and 5% of helpful requests, while gpt-3.5-turbo has a great performance - blocking close to 99% of harmful and just 2% of helpful requests.</p>
<h2>G.2.2 Fact-checking Rail</h2>
<p>We consider the MSMARCO dataset (Bajaj et al., 2016) to evaluate the performance of the factchecking rail. The dataset consists of (context, question, answer) triples. In order to mine negatives (answers that are not grounded in the context),</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Us int, <br> no sim</th>
<th style="text-align: left;">Us int, <br> sim $=0.6$</th>
<th style="text-align: left;">Bt int, <br> no sim</th>
<th style="text-align: left;">Bt int, <br> sim $=0.6$</th>
<th style="text-align: left;">Bt msg, <br> no sim</th>
<th style="text-align: left;">Bt msg, <br> sim $=0.6$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">text-davinci-003, k=all</td>
<td style="text-align: left;">0.89</td>
<td style="text-align: left;">0.89</td>
<td style="text-align: left;">0.90</td>
<td style="text-align: left;">0.90</td>
<td style="text-align: left;">0.91</td>
<td style="text-align: left;">0.91</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-003, k=3</td>
<td style="text-align: left;">0.82</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">0.85</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-003, k=1</td>
<td style="text-align: left;">0.65</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">0.73</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">gpt-3.5-turbo, k=all</td>
<td style="text-align: left;">0.44</td>
<td style="text-align: left;">0.56</td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;">0.61</td>
<td style="text-align: left;">0.54</td>
<td style="text-align: left;">0.65</td>
</tr>
<tr>
<td style="text-align: left;">dolly-v2-3b, k=all</td>
<td style="text-align: left;">0.65</td>
<td style="text-align: left;">0.78</td>
<td style="text-align: left;">0.68</td>
<td style="text-align: left;">0.78</td>
<td style="text-align: left;">0.69</td>
<td style="text-align: left;">0.78</td>
</tr>
<tr>
<td style="text-align: left;">falcon-7b-instruct, k=all</td>
<td style="text-align: left;">0.81</td>
<td style="text-align: left;">0.81</td>
<td style="text-align: left;">0.81</td>
<td style="text-align: left;">0.82</td>
<td style="text-align: left;">0.81</td>
<td style="text-align: left;">0.82</td>
</tr>
<tr>
<td style="text-align: left;">llama2-13b-chat, k=all</td>
<td style="text-align: left;">0.87</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">0.88</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">0.89</td>
<td style="text-align: left;">N/A</td>
</tr>
</tbody>
</table>
<p>Table 2: Topical evaluation results on chit-chat dataset. Us int means accuracy for user intents, Bt int is accuracy for next step generation (i.e., the bot intent), Bt msg is accuracy for generated bot message. Sim denotes if semantic similarity was used for matching (with a specified threshold, in this case 0.6) or exact match.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Us int, <br> no sim</th>
<th style="text-align: left;">Us int, <br> sim $=0.6$</th>
<th style="text-align: left;">Bt int, <br> no sim</th>
<th style="text-align: left;">Bt int, <br> sim $=0.6$</th>
<th style="text-align: left;">Bt msg, <br> no sim</th>
<th style="text-align: left;">Bt msg, <br> sim $=0.6$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">text-davinci-003, k=all</td>
<td style="text-align: left;">0.77</td>
<td style="text-align: left;">0.82</td>
<td style="text-align: left;">0.83</td>
<td style="text-align: left;">0.84</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-003, k=3</td>
<td style="text-align: left;">0.65</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">0.73</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-003, k=1</td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">0.63</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">gpt-3.5-turbo, k=all</td>
<td style="text-align: left;">0.38</td>
<td style="text-align: left;">0.73</td>
<td style="text-align: left;">0.45</td>
<td style="text-align: left;">0.73</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">dolly-v2-3b, k=all</td>
<td style="text-align: left;">0.32</td>
<td style="text-align: left;">0.62</td>
<td style="text-align: left;">0.40</td>
<td style="text-align: left;">0.64</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">falcon-7b-instruct, k=all</td>
<td style="text-align: left;">0.70</td>
<td style="text-align: left;">0.76</td>
<td style="text-align: left;">0.75</td>
<td style="text-align: left;">0.78</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">llama2-13b-chat, k=all</td>
<td style="text-align: left;">0.76</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">0.78</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">N/A</td>
</tr>
</tbody>
</table>
<p>Table 3: Topical evaluation results on banking dataset.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 11: Performance of the fact-checking rail.
we use OpenAI text-davinci-003 to rewrite the positive answer to a hard negative that looks similar to it, but is not grounded in the evidence. We construct a combined dataset by equally sampling both positive and negative triples. Both text-davinci-003 and gpt-3.5-turbo perform well on the fact-checking rail and obtain an overall accuracy of $80 \%$ (Fig. 11). The behavior of the two models is slightly different: while gpt-3.5-turbo is better at discovering negatives, text-davinci-003 performs better on positive samples.</p>
<h2>G.2.3 Hallucination Rail</h2>
<p>Evaluating the hallucination rail is difficult since we cannot ascertain the questions that can be answered with factual knowledge embedded in the parameters of the language model. To effectively quantify the ability of the model to detect hallucinations, we compile a list of 20 questions based on a false premise. For example, one such question that does not have a right answer is: "When was the undersea city in the Gulf of Mexico established?"</p>
<p>Any generation from the language model apart from deflection (i.e., recognizing that the question is unanswerable) is considered a failure. We also quantify the benefit of employing the hallucination rail as a fallback mechanism. For text-davinci003, the base language model is unable to deflect prompts that are unanswerable and using the hallucination rail helps intercept $70 \%$ of the unanswerable prompts. gpt-3.5-turbo performs very well at deflecting prompts that cannot be answered or hedging its response with statements about it could be incorrect. Even for such powerful models, we find that employing the hallucination rail helps boost the identification of questions that are prone to incorrect responses by $25 \%$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{9}$ https://github.com/NVIDIA/NeMo-Guardrails/ blob/main/nemoguardrails/eval/README. $m d$
${ }^{10}$ https://github.com/NVIDIA/NeMo-Guardrails/ blob/main/docs/README. 8evaluation-tools
${ }^{11}$ https://github.com/rahul851296/ small-talk-rasa-stack, dataset was initially released by Rasa&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
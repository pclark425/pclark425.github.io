<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6214 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6214</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6214</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-fa7805c7ad42610b89d07353cb3600f3ecaf2c2f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/fa7805c7ad42610b89d07353cb3600f3ecaf2c2f" target="_blank">Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study</a></p>
                <p><strong>Paper Venue:</strong> International Joint Conference on Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> The experimental results prove that ChatGPT is capable of evaluating text quality effectively from various perspectives without reference and demonstrates superior performance than most existing automatic metrics.</p>
                <p><strong>Paper Abstract:</strong> Evaluating the quality of generated text is a challenging task in NLP, due to the inherent complexity and diversity of text. Recently, large language models (LLMs) have garnered significant attention due to their impressive performance in various tasks. Therefore, we present this paper to investigate the effectiveness of LLMs, especially ChatGPT, and explore ways to optimize their use in assessing text quality. We compared three kinds of reference-free evaluation methods. The experimental results prove that ChatGPT is capable of evaluating text quality effectively from various perspectives without reference and demonstrates superior performance than most existing automatic metrics. In particular, the Explicit Score, which utilizes ChatGPT to generate a numeric score measuring text quality, is the most effective and reliable method among the three exploited approaches. However, directly comparing the quality of two texts may lead to suboptimal results. We believe this paper will provide valuable insights for evaluating text quality with LLMs and have released the used data.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6214.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6214.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Explicit Score (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explicit numeric scoring of text quality generated by ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using ChatGPT to produce an explicit numeric score (0-100) assessing the absolute quality of a generated text (overall or by aspect) in a reference-free manner; evaluated across summarization, dialogue, story, and paraphrase tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Text summarization, Dialogue response generation, Story generation, Paraphrase generation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>ChatGPT (gpt3.5-turbo-0301; referred to as ChatGPT in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Used existing dataset human annotations: SummEval (sampled 20 CNN/DailyMail examples, human ratings on coherence, fluency, consistency, relevance), FED (124 conversations with multi-aspect human judgments), OpenMEVA-ROC (200 story beginnings with 5 generated storylines each, annotated for overall quality), Twitter (Extend) (extended Twitter-Para test set with human annotations). Exact number of annotators per item not specified (used published human annotations from those datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Spearman and Pearson correlations at sample- or dataset-level (and Kendall's Tau-b for pairwise comparisons) between ChatGPT Explicit Scores and human ratings; comparisons against many automated metrics (ROUGE, BERTScore, BARTScore variants, ParaScore, Perplexity, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>ChatGPT Explicit Score correlates with human judgments substantially better than most traditional automated metrics across tasks and aspects (paper reports higher Spearman/Pearson on many datasets); greedy decoding (deterministic) generally improved correlation over sampling; Explicit Score distribution is smoother and more discriminative than Implicit Scores derived from token probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>ChatGPT tends to adopt a strict/high standard leading to generally negative/low scores when datasets lack high-quality examples; integer-valued explicit outputs may reduce precision compared to floating-point implicit confidences; Explicit Scores can be sensitive to prompt phrasing (detailed per-range criteria or asking for reasoning before scoring can degrade discriminative power); access to token-level confidences for ChatGPT is unavailable (so implicit confidence cannot be directly obtained).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>When datasets contain mostly low-quality candidates, ChatGPT's strictness reduces its ability to discriminate fine-grained differences; asking the model to provide chain-of-thought reasons before scoring caused scores to concentrate and lose discriminative power; prompts that refine scoring into coarse buckets (e.g., 5-star with fixed criteria) also reduced correlation with human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Prefer explicit numeric scoring with simple, general evaluation instructions rather than detailed per-score criteria; use greedy decoding for more stable and higher-correlation results; avoid requesting chain-of-thought justification before the score (it can shift focus away from discriminating texts); paraphrasing/simplifying prompts (rather than over-specifying) tends to keep or slightly improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6214.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6214.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Implicit Score (text-davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Implicit confidence-based scoring using token-probabilities from text-davinci models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Deriving an implicit score for text quality from the model's confidence in a yes/no answer (probability mass on 'yes'/'no' tokens) using text-davinci-series models (text-davinci-001 and text-davinci-003) as a reference-free evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Text summarization, Dialogue response generation, Story generation, Paraphrase generation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>text-davinci-003 and text-davinci-001 (GPT-3 family; text-davinci-003 is instruction-tuned and RLHF-trained)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Same dataset human annotations as used for Explicit Score (SummEval, FED, OpenMEVA-ROC, Twitter (Extend)); implicit scores evaluated via correlation with those human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Spearman/Pearson correlations (individual score evaluation) and Kendall's Tau-b (pairwise comparison reliability) between implicit confidence-derived scores and human ratings, compared to other automated metrics and ChatGPT Explicit Scores.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Implicit Score from text-davinci-003 is competitive and in some cases outperforms ChatGPT Explicit Score (notably on some hard pairwise comparisons and certain aspects); text-davinci-003 outperformed text-davinci-001, likely due to RLHF alignment with human preferences; however, the implicit score distribution is peaked and concentrated in a narrow range, reducing overall discriminative power on many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Restricted to top-k token log-probabilities returned by the API (paper used top-5), making probability estimates approximate; distributional property is narrow and peaked (poor spread), limiting discrimination; dependent on the particular model/version (001 vs 003 differences); implicit binary framing (yes/no) can lose nuance of gradations of quality.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Implicit Score sometimes fails to distinguish moderate differences because the affirmative/negative probabilities cluster (leading to low variance across samples); text-davinci-001 often underperforms relative to 003; implicit scoring can be less effective than explicit scoring on tasks where a smoother score range is advantageous.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Use text-davinci-003 rather than earlier non-RLHF models for better alignment; rescale/normalize implicit probabilities to a wider numeric range when comparing to explicit scores; consider framing or calibrating the binary question carefully; combine multiple model runs or aggregate other signals (if available) to improve spread and discrimination.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6214.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6214.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pairwise Comparison (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct pairwise relative-quality comparison using ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting ChatGPT to directly compare two candidate texts produced for the same input and select which is better (or whether they are equal), evaluated in a reference-free manner across several generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Text summarization, Dialogue response generation, Story generation, Paraphrase generation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>ChatGPT (gpt3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Compared ChatGPT pairwise decisions against human pairwise preferences derived from dataset annotations; to limit requests, 200 random pairs were sampled per dataset and Kendall's Tau-b used to compare rankings with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Kendall's Tau-b between ChatGPT pairwise choices and human pairwise preferences; also compared to Individual Score methods and other metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Contrary to expectation, direct pairwise comparison by ChatGPT performed worse than individual explicit scoring in many settings; pairwise results were less reliable and more sensitive to prompt variants and decoding strategy (sampling vs greedy).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>ChatGPT exhibits a strict quality standard and often judges both candidates as generally low quality, which leads to unstable or biased choices; prompts that include forced-answer templates (e.g., 'Answer: I will choose Option') can bias early token generation and push the model toward selecting a single option rather than 'tie'; chain-of-thought prompts changed behavior unpredictably (e.g., prompting for reasoning caused more 'tie' responses but not necessarily better agreement with humans); mirrored prompts (choose worse vs choose better) produced inconsistent, non-symmetric outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Confusion matrices showed ChatGPT tends to pick one storyline as better even when humans judged them equal or both poor; mirrored prompts yielded inconsistent opposites (choosing better under one prompt did not reliably flip to choosing the other as worse under the mirrored prompt); attempts to force the model to present reasoning then decide (Prompt V4) increased 'tie' responses but did not consistently improve alignment with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Avoid direct pairwise comparison as default — prefer individual explicit scoring which correlated better with humans; if using pairwise prompts, avoid wording that biases early tokens (remove forced-answer suffixes), experiment with requiring reasoning before decision cautiously (it can increase ties but may help in some settings), and consider aggregating multiple sampled judgments (majority voting) — though the paper found mixed effects. Overall, the paper recommends caution with pairwise ChatGPT comparisons and suggests using Explicit Score (ChatGPT) or Implicit Score (text-davinci-003) depending on the setting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Is chatgpt a good nlg evaluator? a preliminary study. <em>(Rating: 2)</em></li>
                <li>Exploring chatgpt's ability to rank content: A preliminary study on consistency with human preferences. <em>(Rating: 2)</em></li>
                <li>Gptscore: Evaluate as you desire. <em>(Rating: 2)</em></li>
                <li>Large language models are state-of-the-art evaluators of translation quality. <em>(Rating: 2)</em></li>
                <li>Is chatgpt a good keyphrase generator? a preliminary study. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6214",
    "paper_id": "paper-fa7805c7ad42610b89d07353cb3600f3ecaf2c2f",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "Explicit Score (ChatGPT)",
            "name_full": "Explicit numeric scoring of text quality generated by ChatGPT",
            "brief_description": "Using ChatGPT to produce an explicit numeric score (0-100) assessing the absolute quality of a generated text (overall or by aspect) in a reference-free manner; evaluated across summarization, dialogue, story, and paraphrase tasks.",
            "citation_title": "Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study",
            "mention_or_use": "use",
            "task_domain": "Text summarization, Dialogue response generation, Story generation, Paraphrase generation",
            "llm_judge_model": "ChatGPT (gpt3.5-turbo-0301; referred to as ChatGPT in the paper)",
            "human_evaluation_setup": "Used existing dataset human annotations: SummEval (sampled 20 CNN/DailyMail examples, human ratings on coherence, fluency, consistency, relevance), FED (124 conversations with multi-aspect human judgments), OpenMEVA-ROC (200 story beginnings with 5 generated storylines each, annotated for overall quality), Twitter (Extend) (extended Twitter-Para test set with human annotations). Exact number of annotators per item not specified (used published human annotations from those datasets).",
            "metrics_compared": "Spearman and Pearson correlations at sample- or dataset-level (and Kendall's Tau-b for pairwise comparisons) between ChatGPT Explicit Scores and human ratings; comparisons against many automated metrics (ROUGE, BERTScore, BARTScore variants, ParaScore, Perplexity, etc.).",
            "reported_differences": "ChatGPT Explicit Score correlates with human judgments substantially better than most traditional automated metrics across tasks and aspects (paper reports higher Spearman/Pearson on many datasets); greedy decoding (deterministic) generally improved correlation over sampling; Explicit Score distribution is smoother and more discriminative than Implicit Scores derived from token probabilities.",
            "llm_specific_limitations": "ChatGPT tends to adopt a strict/high standard leading to generally negative/low scores when datasets lack high-quality examples; integer-valued explicit outputs may reduce precision compared to floating-point implicit confidences; Explicit Scores can be sensitive to prompt phrasing (detailed per-range criteria or asking for reasoning before scoring can degrade discriminative power); access to token-level confidences for ChatGPT is unavailable (so implicit confidence cannot be directly obtained).",
            "notable_failure_cases": "When datasets contain mostly low-quality candidates, ChatGPT's strictness reduces its ability to discriminate fine-grained differences; asking the model to provide chain-of-thought reasons before scoring caused scores to concentrate and lose discriminative power; prompts that refine scoring into coarse buckets (e.g., 5-star with fixed criteria) also reduced correlation with human ratings.",
            "mitigation_strategies": "Prefer explicit numeric scoring with simple, general evaluation instructions rather than detailed per-score criteria; use greedy decoding for more stable and higher-correlation results; avoid requesting chain-of-thought justification before the score (it can shift focus away from discriminating texts); paraphrasing/simplifying prompts (rather than over-specifying) tends to keep or slightly improve performance.",
            "uuid": "e6214.0",
            "source_info": {
                "paper_title": "Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Implicit Score (text-davinci)",
            "name_full": "Implicit confidence-based scoring using token-probabilities from text-davinci models",
            "brief_description": "Deriving an implicit score for text quality from the model's confidence in a yes/no answer (probability mass on 'yes'/'no' tokens) using text-davinci-series models (text-davinci-001 and text-davinci-003) as a reference-free evaluator.",
            "citation_title": "Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study",
            "mention_or_use": "use",
            "task_domain": "Text summarization, Dialogue response generation, Story generation, Paraphrase generation",
            "llm_judge_model": "text-davinci-003 and text-davinci-001 (GPT-3 family; text-davinci-003 is instruction-tuned and RLHF-trained)",
            "human_evaluation_setup": "Same dataset human annotations as used for Explicit Score (SummEval, FED, OpenMEVA-ROC, Twitter (Extend)); implicit scores evaluated via correlation with those human ratings.",
            "metrics_compared": "Spearman/Pearson correlations (individual score evaluation) and Kendall's Tau-b (pairwise comparison reliability) between implicit confidence-derived scores and human ratings, compared to other automated metrics and ChatGPT Explicit Scores.",
            "reported_differences": "Implicit Score from text-davinci-003 is competitive and in some cases outperforms ChatGPT Explicit Score (notably on some hard pairwise comparisons and certain aspects); text-davinci-003 outperformed text-davinci-001, likely due to RLHF alignment with human preferences; however, the implicit score distribution is peaked and concentrated in a narrow range, reducing overall discriminative power on many tasks.",
            "llm_specific_limitations": "Restricted to top-k token log-probabilities returned by the API (paper used top-5), making probability estimates approximate; distributional property is narrow and peaked (poor spread), limiting discrimination; dependent on the particular model/version (001 vs 003 differences); implicit binary framing (yes/no) can lose nuance of gradations of quality.",
            "notable_failure_cases": "Implicit Score sometimes fails to distinguish moderate differences because the affirmative/negative probabilities cluster (leading to low variance across samples); text-davinci-001 often underperforms relative to 003; implicit scoring can be less effective than explicit scoring on tasks where a smoother score range is advantageous.",
            "mitigation_strategies": "Use text-davinci-003 rather than earlier non-RLHF models for better alignment; rescale/normalize implicit probabilities to a wider numeric range when comparing to explicit scores; consider framing or calibrating the binary question carefully; combine multiple model runs or aggregate other signals (if available) to improve spread and discrimination.",
            "uuid": "e6214.1",
            "source_info": {
                "paper_title": "Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Pairwise Comparison (ChatGPT)",
            "name_full": "Direct pairwise relative-quality comparison using ChatGPT",
            "brief_description": "Prompting ChatGPT to directly compare two candidate texts produced for the same input and select which is better (or whether they are equal), evaluated in a reference-free manner across several generation tasks.",
            "citation_title": "Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study",
            "mention_or_use": "use",
            "task_domain": "Text summarization, Dialogue response generation, Story generation, Paraphrase generation",
            "llm_judge_model": "ChatGPT (gpt3.5-turbo-0301)",
            "human_evaluation_setup": "Compared ChatGPT pairwise decisions against human pairwise preferences derived from dataset annotations; to limit requests, 200 random pairs were sampled per dataset and Kendall's Tau-b used to compare rankings with human judgments.",
            "metrics_compared": "Kendall's Tau-b between ChatGPT pairwise choices and human pairwise preferences; also compared to Individual Score methods and other metrics.",
            "reported_differences": "Contrary to expectation, direct pairwise comparison by ChatGPT performed worse than individual explicit scoring in many settings; pairwise results were less reliable and more sensitive to prompt variants and decoding strategy (sampling vs greedy).",
            "llm_specific_limitations": "ChatGPT exhibits a strict quality standard and often judges both candidates as generally low quality, which leads to unstable or biased choices; prompts that include forced-answer templates (e.g., 'Answer: I will choose Option') can bias early token generation and push the model toward selecting a single option rather than 'tie'; chain-of-thought prompts changed behavior unpredictably (e.g., prompting for reasoning caused more 'tie' responses but not necessarily better agreement with humans); mirrored prompts (choose worse vs choose better) produced inconsistent, non-symmetric outcomes.",
            "notable_failure_cases": "Confusion matrices showed ChatGPT tends to pick one storyline as better even when humans judged them equal or both poor; mirrored prompts yielded inconsistent opposites (choosing better under one prompt did not reliably flip to choosing the other as worse under the mirrored prompt); attempts to force the model to present reasoning then decide (Prompt V4) increased 'tie' responses but did not consistently improve alignment with human judgments.",
            "mitigation_strategies": "Avoid direct pairwise comparison as default — prefer individual explicit scoring which correlated better with humans; if using pairwise prompts, avoid wording that biases early tokens (remove forced-answer suffixes), experiment with requiring reasoning before decision cautiously (it can increase ties but may help in some settings), and consider aggregating multiple sampled judgments (majority voting) — though the paper found mixed effects. Overall, the paper recommends caution with pairwise ChatGPT comparisons and suggests using Explicit Score (ChatGPT) or Implicit Score (text-davinci-003) depending on the setting.",
            "uuid": "e6214.2",
            "source_info": {
                "paper_title": "Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Is chatgpt a good nlg evaluator? a preliminary study.",
            "rating": 2
        },
        {
            "paper_title": "Exploring chatgpt's ability to rank content: A preliminary study on consistency with human preferences.",
            "rating": 2
        },
        {
            "paper_title": "Gptscore: Evaluate as you desire.",
            "rating": 2
        },
        {
            "paper_title": "Large language models are state-of-the-art evaluators of translation quality.",
            "rating": 2
        },
        {
            "paper_title": "Is chatgpt a good keyphrase generator? a preliminary study.",
            "rating": 1
        }
    ],
    "cost": 0.010974,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study</h1>
<p>Yi Chen ${ }^{\text { }}$, Rui Wang ${ }^{\text { }}$, Haiyun Jiang, Shuming Shi, Ruifeng Xu ${ }^{\text { }}$ (1) Harbin Institute of Technology, Shenzhen, China<br>${ }^{\text {A}}$ Peng Cheng Laboratory, Shenzhen, China<br>${ }^{A}$ Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies<br>yichennlp@gmail.com, ruiwangnlp@outlook.com, xuruifeng@hit.edu.cn</p>
<h4>Abstract</h4>
<p>Evaluating the quality of generated text is a challenging task in NLP, due to the inherent complexity and diversity of text. Recently, large language models (LLMs) have garnered significant attention due to their impressive performance in various tasks. Therefore, we present this paper to investigate the effectiveness of LLMs, especially ChatGPT, and explore ways to optimize their use in assessing text quality. We compared three kinds of referencefree evaluation methods. The experimental results prove that ChatGPT is capable of evaluating text quality effectively from various perspectives without reference and demonstrates superior performance than most existing automatic metrics. In particular, the Explicit Score, which utilizes ChatGPT to generate a numeric score measuring text quality, is the most effective and reliable method among the three exploited approaches. However, directly comparing the quality of two texts may lead to suboptimal results. We believe this paper will provide valuable insights for evaluating text quality with LLMs and have released the used data ${ }^{1}$.</p>
<h2>1 Introduction</h2>
<p>Automated evaluation of text generation quality has posed a long-standing challenge in the field of natural language processing (NLP). On the one hand, the diverse forms of textual expression make it impossible for reference-based methods to account for all possible situations(Zhang* et al., 2020; Yuan et al., 2021; Chen et al., 2022b). On the other hand, devising reliable metrics without reference is not a straightforward task and can also be problematic (Sun and Zhou, 2012; Niu et al., 2021; Shen et al., 2022). Furthermore, different types of text necessitate evaluation of distinct aspects, e.g. coherence, fluency, and consistency (Fabbri et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>2021a; Mehri and Eskenazi, 2020a; Wang et al., 2023b), which makes it hard to design metrics for each type of text and dimension separately.</p>
<p>Nowadays, large language models (LLMs) (Brown et al., 2020; Ouyang et al., 2022; Chung et al., 2022; Chowdhery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Du et al., 2022) represented by ChatGPT ${ }^{2}$ have revolutionized the field of NLP by achieving remarkable results in a wide range of NLP tasks (Song et al., 2023; Chen et al., 2022a). Recent studies (Fu et al., 2023; Wang et al., 2023a; Kocmi and Federmann, 2023; Ji et al., 2023) have also demonstrated the potential of LLMs in evaluating the quality of generated texts. In this paper, we present an empirical study that compares different methods for text quality evaluation using LLMs in a reference-free mode. The key insights from our empirical findings are as follows:</p>
<ul>
<li>How accurately can ChatGPT assess text quality without references? (§4.1)
It is feasible for ChatGPT to evaluate text quality without reference, and it outperforms commonly used metrics even with a simple prompt design.</li>
</ul>
<h2>- What is the most effective approach to evaluate text quality using ChatGPT? (§4)</h2>
<p>Generally, using ChatGPT to generate an explicit score for text quality is the best and most stable method among the three we compared. We suggest using greedy decoding for more reliable results.</p>
<ul>
<li>Why may directly comparing two texts using ChatGPT yield suboptimal results? (§5.1)</li>
</ul>
<p>Due to its strict standard for "high-quality" text, ChatGPT often considers most generated texts unsatisfactory. Therefore, distinguishing between two subpar texts becomes challenging for ChatGPT.</p>
<ul>
<li>Why is Implicit Score generally less effective than Explicit Score? (§5.2)</li>
</ul>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Compared to generating an Explicit Score with ChatGPT, using the confidence of text-davinci models to determine text quality (Implicit Score) is less effective due to different distribution characteristics. Implicit Score has a narrow range and peak structure, while Explicit Score allows better differentiation with its smoother distribution.</p>
<h2>- How can prompt design impact ChatGPT in generating an Explicit Score? (\$5.3)</h2>
<p>When prompting ChatGPT for an Explicit Score, it would be better to avoid detailed scoring criteria if such criteria lack clear definitions for each score range. A general description of the evaluation standard is enough. Also, making ChatGPT provide justifications in a "chain-of-thought" manner before scoring can lead it to prioritize its reasoning process over the text. These justifications tend to be templated and similar across different texts, reducing the discriminative power of the final score.</p>
<h2>2 Method</h2>
<p>We explore two different reference-free paradigms, i.e., Individual Score and Pairwise Comparison for text evaluation using ChatGPT and text-davinci models. Individual Score assesses the quality of a single text by a numerical score, while Pairwise Comparison focuses on the relative quality of two texts and requires a direct comparison to determine which one is superior. Within the Individual Score paradigm, two methods are typically exploited: Explicit Score, obtained through direct text generation, and Implicit Score, obtained through the token probabilities outputted by the model.</p>
<h3>2.1 Individual Score</h3>
<p>Explicit Score Conditioned on a given input text (optional), we prompt ChatGPT to directly generate a score to measure the absolute quality of each text individually in terms of a specific aspect or the overall performance. An example prompt designed for scoring the overall quality of a storyline is shown as follows:</p>
<div class="codehilite"><pre><span></span><code><span class="o">=========</span><span class="w"> </span><span class="nv">PROMPT</span><span class="w"> </span><span class="k">FOR</span><span class="w"> </span><span class="nv">EXPLICIT</span><span class="w"> </span><span class="nv">SCORE</span><span class="w"> </span><span class="o">==========</span>
</code></pre></div>

<p>Score the following storyline given the beginning of the story on a continual scale from 0 (worst) to 100 (best), where a score of 0 means "The storyline makes no sense and is totally not understandable" and a score of 100 means "The storyline is perfect-written and highly consistent with the given beginning of the story".</p>
<p>The beginning of the story:
[Conditioned Text]
Storyline:
[Generated Text]
Score:</p>
<p>Implicit Score Given the LLM's potential insensitivity to numerical values and the lack of explicit instructions for aligning score intervals with specific criteria, score fluctuations may occur across different samples. Therefore, we propose an alternative approach by framing the problem as a binary Yes or No question, where the confidence level of answering "yes" serves as the Implicit Score. An illustrative example is presented below:</p>
<div class="codehilite"><pre><span></span><code><span class="o">=========</span><span class="w"> </span><span class="nv">PROMPT</span><span class="w"> </span><span class="k">FOR</span><span class="w"> </span><span class="nv">IMPLICIT</span><span class="w"> </span><span class="nv">SCORE</span><span class="w"> </span><span class="o">==========</span>
</code></pre></div>

<p>Consider the following storyline written according to the given beginning of the story:</p>
<p>The beginning of the story:
[Conditioned Text]
Storyline:
[Generated Text]
Question: Is the storyline well-written and consistent with the beginning of the story?</p>
<p>Answer:</p>
<p>Unfortunately, access to ChatGPT's token probabilities is currently unavailable. Text-davinci003 is similar to ChatGPT in that they are both trained through supervised instruction tuning and Reinforcement Learning from Human Feedback (RLHF) based on GPT-3.5, and they both exhibit excellent performance in following and fulfilling human instructions. Therefore, we utilize text-davinci-003 to derive the Implicit Score as a baseline metric instead. To facilitate a more comprehensive comparison, we also obtain the Implicit Score from text-davinci-001, an earlier version of the text-davinci series model which is based on GPT-3 and has not been trained using RLHF. Due to a limitation of the OpenAI API, only the top 5 most probable tokens are returned with log probabilities. Therefore, we instead estimate the Implicit Score using the following formula:</p>
<p>$$
\begin{aligned}
p(\text { yes }) &amp; =\sum_{t \in \mathcal{T}<em _text="\text" _yes="{yes">{\text {top } 5} \cap \mathcal{T}</em> p(t) \
p(\text { no }) &amp; =\sum_{t \in \mathcal{T}}}<em _no="{no" _text="\text">{\text {top } 5} \cap \mathcal{T}</em> p(t) \
\text { Implicit Score } &amp; =\max (p(y e s), 1-p(n o))
\end{aligned}
$$}}</p>
<p>Here, $p(t)$ represents the probability of predicting token $t$ immediately following the prompt "Answer:". The sets $\mathcal{T}<em _no="{no" _text="\text">{\text {yes }}$ and $\mathcal{T}</em>}}$ consist of the affirmative and negative response tokens, respectively, i.e., $\mathcal{T<em _no="{no" _text="\text">{\text {yes }}={$ " Yes", "Yes", " yes", "yes" $}$, and $\mathcal{T}</em>={$ " No", "No", " no", "no" $}$.}</p>
<h3>2.2 Pairwise Comparison</h3>
<p>Another paradigm to assess text quality is by directly comparing a pair of generated texts based on the same input. This method primarily focuses on the relative quality of the texts. For instance, a prompt for comparing the overall quality of two storylines written according to the same initial story beginning is shown as follows:</p>
<div class="codehilite"><pre><span></span><code><span class="o">======</span><span class="w"> </span><span class="n">PROMPT</span><span class="w"> </span><span class="n">FOR</span><span class="w"> </span><span class="n">PAIRWISE</span><span class="w"> </span><span class="n">COMPARISON</span><span class="w"> </span><span class="o">=====</span>
<span class="n">Consider</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">storylines</span><span class="w"> </span><span class="n">written</span><span class="w"> </span><span class="n">according</span><span class="w"> </span><span class="n">to</span>
<span class="n">the</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">beginning</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">story</span><span class="o">:</span>
<span class="n">The</span><span class="w"> </span><span class="n">beginning</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">story</span><span class="o">:</span>
<span class="p">[</span><span class="n">Conditioned</span><span class="w"> </span><span class="n">Text</span><span class="p">]</span>
<span class="n">Storyline</span><span class="mi">-1</span><span class="o">:</span>
<span class="p">[</span><span class="n">Generated</span><span class="w"> </span><span class="n">Text</span><span class="mi">-1</span><span class="p">]</span>
<span class="n">Storyline</span><span class="mi">-2</span><span class="o">:</span>
<span class="p">[</span><span class="n">Generated</span><span class="w"> </span><span class="n">Text</span><span class="mi">-2</span><span class="p">]</span>
</code></pre></div>

<p>Question: Which storyline is better-written and more consistent with the beginning of the story? Please answer with one of the following options.</p>
<p>Options:
(A) Storyline-1
(B) Storyline-2
(C) Both storylines are equally well-written and consistent with the beginning of the story.</p>
<p>Answer: I will choose Option</p>
<h2>3 Experimental Setup</h2>
<h3>3.1 Tasks and Datasets</h3>
<p>We conduct experiments on four distinct natural language generation tasks: Text Summarization, Dialogue Response Generation, Story Generation, and Paraphrase Generation.</p>
<p>Text Summarization aims to summarize the key points of a given long text. SummEval (Fabbri et al., 2021b) is a collection of human annotations for 16 model-generated summaries on 100 CNN/DaliyMail news over 4 dimensions: coherence $(\mathrm{COH})$, fluency (FLU), consistency (CON), and relevance (REL). Due to the budget limit, we
randomly sample 20 news and corresponding annotations from SummEval for evaluation.</p>
<p>Dialogue Response Generation aims to generate a response based on the preceding dialogue. We conduct experiments on the dialogue-level FED dataset (Mehri and Eskenazi, 2020a), which contains fine-grained human judgments for 124 conversations. The evaluation aspects include coherence (COH), error recovery (ERR), consistency (CON), diversity (DIV), topic depth (DEP), likeability (LIK), understanding (UND), flexibility (FLE), informativeness (INF), inquisitiveness (INQ) and overall performance (Overall). However, we do not include ERR in our evaluation since some annotations are missing.</p>
<p>Story Generation aims to automatically write a storyline based on a given beginning of the story. We employ OpenMEVA-ROC (Guan et al., 2021) for evaluation, which contains 200 story beginnings and 5 corresponding machine-generated storylines for each beginning. Each storyline is manually annotated in terms of overall quality.</p>
<p>Paraphrase Generation aims to rephrase a sentence in different words or forms while preserving its original meaning. We use Twitter-Para (Xu et al., 2014, 2015) for evaluation, containing 761 input sentences and each input has 9.41 paraphrase candidates on average. We adopt the test set (Shen et al., 2022) extended from Twitter-Para by adding $20 \%$ of the input sentences as candidates, denoted as Twitter (Extend).</p>
<h3>3.2 Chosen Metrics</h3>
<p>Following the settings of previous works, we select baseline metrics from the following widely used metrics accordingly: ROUGE-1, ROUGE-2 and ROUGE-L (Lin, 2004); BERTScore (Zhang* et al., 2020); MoverScore (Zhao et al., 2019); PRISM (Thompson and Post, 2020); BARTScore and its enhanced versions, BARTScore+CNN and BARTScore+CNN+Para (Yuan et al., 2021); BERT-R (Ghazarian et al., 2019); GPT-2 (Radford et al., 2019); USR (Mehri and Eskenazi, 2020b); S-DiCoh (Mesgar et al., 2020); FED (Mehri and Eskenazi, 2020a); DynaEval (Zhang et al., 2021); SelfEval (Ma et al., 2022); PPL (Guan et al., 2021); iBLEU (Sun and Zhou, 2012); BERT-iBLEU (Niu et al., 2021); ParaScore (Shen et al., 2022). Note that, Shen et al. (2022) also use a reference-free</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metrics</th>
<th style="text-align: left;">Spear.</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">COH</td>
<td style="text-align: left;">FLU</td>
<td style="text-align: left;">CON</td>
<td style="text-align: left;">REL</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-1</td>
<td style="text-align: left;">21.6</td>
<td style="text-align: left;">10.5</td>
<td style="text-align: left;">10.9</td>
<td style="text-align: left;">$\underline{42.6}$</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-2</td>
<td style="text-align: left;">30.7</td>
<td style="text-align: left;">19.1</td>
<td style="text-align: left;">20.7</td>
<td style="text-align: left;">36.9</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-L</td>
<td style="text-align: left;">17.4</td>
<td style="text-align: left;">10.2</td>
<td style="text-align: left;">9.6</td>
<td style="text-align: left;">40.0</td>
</tr>
<tr>
<td style="text-align: left;">BERTScore</td>
<td style="text-align: left;">28.5</td>
<td style="text-align: left;">10.6</td>
<td style="text-align: left;">13.4</td>
<td style="text-align: left;">29.5</td>
</tr>
<tr>
<td style="text-align: left;">MoverScore</td>
<td style="text-align: left;">22.5</td>
<td style="text-align: left;">11.8</td>
<td style="text-align: left;">14.6</td>
<td style="text-align: left;">39.2</td>
</tr>
<tr>
<td style="text-align: left;">PRISM</td>
<td style="text-align: left;">23.7</td>
<td style="text-align: left;">17.5</td>
<td style="text-align: left;">35.2</td>
<td style="text-align: left;">16.9</td>
</tr>
<tr>
<td style="text-align: left;">BARTScore</td>
<td style="text-align: left;">33.4</td>
<td style="text-align: left;">20.9</td>
<td style="text-align: left;">34.8</td>
<td style="text-align: left;">24.8</td>
</tr>
<tr>
<td style="text-align: left;">+CNN</td>
<td style="text-align: left;">$\underline{43.3}$</td>
<td style="text-align: left;">$\underline{28.7}$</td>
<td style="text-align: left;">$\underline{42.7}$</td>
<td style="text-align: left;">36.1</td>
</tr>
<tr>
<td style="text-align: left;">+CNN+Para</td>
<td style="text-align: left;">40.1</td>
<td style="text-align: left;">27.2</td>
<td style="text-align: left;">41.0</td>
<td style="text-align: left;">32.0</td>
</tr>
<tr>
<td style="text-align: left;">IMPLICIT SCORE</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-001</td>
<td style="text-align: left;">-1.7</td>
<td style="text-align: left;">-5.6</td>
<td style="text-align: left;">19.7</td>
<td style="text-align: left;">8.4</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-003</td>
<td style="text-align: left;">$\mathbf{5 7 . 4}$</td>
<td style="text-align: left;">$\mathbf{3 2 . 9}$</td>
<td style="text-align: left;">35.2</td>
<td style="text-align: left;">28.1</td>
</tr>
<tr>
<td style="text-align: left;">EXPLICIT SCORE</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (sampling)</td>
<td style="text-align: left;">45.8</td>
<td style="text-align: left;">22.1</td>
<td style="text-align: left;">41.2</td>
<td style="text-align: left;">39.2</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (greedy)</td>
<td style="text-align: left;">52.2</td>
<td style="text-align: left;">19.3</td>
<td style="text-align: left;">$\mathbf{4 3 . 3}$</td>
<td style="text-align: left;">$\mathbf{4 6 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Sample-level Spearman (Spear.) correlation of different aspects on SummEval.
version of BERTScore and ParaScore, denoted as BERTScore.Free and ParaScore.Free.</p>
<h3>3.3 Meta Evaluation</h3>
<p>Individual Score In order to assess the reliability of Individual Scores, we utilize the Spearman (Zar, 2005) and Pearson (Mukaka, 2012) correlation coefficients. As SummEval and OpenMEVA provide an equivalent number of model-generated results for each input, we present the sample-level correlations for these datasets. Whereas, for Twitter (Extend) and the dialog-level FED datasets, we report the dataset-level correlations instead.</p>
<p>Pairwise Comparison To avoid an excessive volume of requests when testing all permutations of pairwise comparisons in each dataset using ChatGPT, we have opted to randomly sample 200 pairs from each dataset as an approximation. To estimate the reliability of metrics for pairwise comparison, Kendall's Tau-b (Kendall, 1945) is employed to evaluate the correlation between two measured variables. A detailed explanation of Kendall's Tau-b is shown in Appendix C.</p>
<h2>4 Main Experiments</h2>
<h3>4.1 Individual Score</h3>
<p>Notably, as shown in Tables 1 to 4, even without providing reference or calibration details for different score ranges, ChatGPT's Explicit Score has already correlated with human scores better than most commonly used automated metrics. On Twit-
ter (Extend), it is only outperformed by ParaScore and ParaScore.Free, which requires the use of reference or hyper-parameter adjustments on a dev set. Additionally, the performance of the Explicit Score further improves when we use greedy search instead of Top-P sampling for decoding.</p>
<p>It is worth noting that the Implicit Score based on text-davinci-003 also shows promising results. This suggests that LLMs' confidence level in determining whether a text meets a specific standard (yes or no) can reflect the text's quality to some extent. Besides, the Implicit Score based on text-davinci-003 performs better than that based on text-davinci-001 in most cases, perhaps due to RLHF, allowing text-davinci-003 to provide answers that align with human instructions better.</p>
<h3>4.2 Pairwise Comparison</h3>
<p>Scoring individual samples without providing detailed criteria for each score range may lead to inconsistent evaluation standards across different samples. Alternatively, we hypothesize that a direct comparison of quality between a pair of samples is more likely to yield reliable evaluation results from ChatGPT. However, our analysis in Tables 5 to 8 suggests that direct pairwise comparison is not as effective as expected, and eliminating the influence of sampling in decoding is not always advantageous for comparison.</p>
<p>We further categorize the texts for comparison into three levels of difficulty, namely hard, medium, and easy, based on the difference in human scores. The larger the score difference between a pair of texts, the easier it is to discern the better one. The performance of various metrics on distinct difficulty levels is shown in Tables 7 and 8. Overall, the metrics exhibit an increasing trend in performance as the difficulty decreases.</p>
<p>Moreover, our investigation indicates that the Implicit Score derived from text-davinci-003 outperforms or performs comparably to the Explicit Score based on ChatGPT when comparing hard text pairs. This finding may be attributed to the higher precision of the Implicit Score, which is based on the model's output token probability (a floating-point number), as opposed to the model's generated Explicit Score, which is limited to integer values ranging from 0 to 100.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metrics</th>
<th style="text-align: center;">Spear.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">COH</td>
<td style="text-align: center;">CON</td>
<td style="text-align: center;">DIV</td>
<td style="text-align: center;">DEP</td>
<td style="text-align: center;">LIK</td>
<td style="text-align: center;">UND</td>
<td style="text-align: center;">FLE</td>
<td style="text-align: center;">INF</td>
<td style="text-align: center;">INQ</td>
<td style="text-align: center;">Overall</td>
</tr>
<tr>
<td style="text-align: left;">BERT-R</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">24.8</td>
</tr>
<tr>
<td style="text-align: left;">GPT-2</td>
<td style="text-align: center;">12.3</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">12.3</td>
</tr>
<tr>
<td style="text-align: left;">USR</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">28.8</td>
</tr>
<tr>
<td style="text-align: left;">S-DiCoh</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">4.6</td>
<td style="text-align: center;">-7.0</td>
<td style="text-align: center;">-10.0</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">2.8</td>
<td style="text-align: center;">-5.4</td>
<td style="text-align: center;">-7.3</td>
</tr>
<tr>
<td style="text-align: left;">FED</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">$\underline{44.9}$</td>
<td style="text-align: center;">$\underline{52.2}$</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">$\underline{40.8}$</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">44.3</td>
</tr>
<tr>
<td style="text-align: left;">DynaEval</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">$\underline{35.2}$</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">43.9</td>
<td style="text-align: center;">$\underline{39.8}$</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">$\underline{39.6}$</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">$\underline{48.2}$</td>
</tr>
<tr>
<td style="text-align: left;">SelfEval</td>
<td style="text-align: center;">$\underline{43.6}$</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">$\underline{40.6}$</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">$\underline{42.1}$</td>
<td style="text-align: center;">43.5</td>
</tr>
<tr>
<td style="text-align: left;">IMPLICIT SCORE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-001</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">39.5</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">39.4</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-003</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">$\mathbf{5 7 . 3}$</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">$\mathbf{5 9 . 0}$</td>
<td style="text-align: center;">$\mathbf{5 8 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">EXPLICIT SCORE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (sampling)</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">$\mathbf{4 7 . 8}$</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">$\mathbf{6 1 . 7}$</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">55.8</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (greedy)</td>
<td style="text-align: center;">$\mathbf{6 2 . 4}$</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">$\mathbf{4 8 . 3}$</td>
<td style="text-align: center;">$\mathbf{5 5 . 5}$</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">$\mathbf{5 4 . 8}$</td>
<td style="text-align: center;">$\mathbf{6 2 . 0}$</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">54.2</td>
</tr>
</tbody>
</table>
<p>Table 2: Dataset-level Spearman (Spear.) correlation of different aspects on dialogue-level FED.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metrics</th>
<th style="text-align: center;">Spear.</th>
<th style="text-align: center;">Pear.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ROUGE-1</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">2.0</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-2</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">4.1</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-L</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">2.1</td>
</tr>
<tr>
<td style="text-align: left;">BERTScore</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">12.0</td>
</tr>
<tr>
<td style="text-align: left;">Perplexity</td>
<td style="text-align: center;">$\underline{32.4}$</td>
<td style="text-align: center;">$\underline{33.0}$</td>
</tr>
<tr>
<td style="text-align: left;">BARTScore</td>
<td style="text-align: center;">-6.5</td>
<td style="text-align: center;">-8.2</td>
</tr>
<tr>
<td style="text-align: left;">+CNN</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">2.6</td>
</tr>
<tr>
<td style="text-align: left;">+CNN+Para</td>
<td style="text-align: center;">6.4</td>
<td style="text-align: center;">5.0</td>
</tr>
<tr>
<td style="text-align: left;">IMPLICIT SCORE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-001</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">32.9</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-003</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">43.4</td>
</tr>
<tr>
<td style="text-align: left;">EXPLICIT SCORE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (sampling)</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">49.0</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (greedy)</td>
<td style="text-align: center;">$\mathbf{4 9 . 9}$</td>
<td style="text-align: center;">$\mathbf{5 1 . 7}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Sample-level Spearman (Spear.) and Pearson (Pear.) correlation on OpenMEVA.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metrics</th>
<th style="text-align: center;">Spear.</th>
<th style="text-align: center;">Pear.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">iBLEU</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">1.1</td>
</tr>
<tr>
<td style="text-align: left;">BERTScore</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">42.7</td>
</tr>
<tr>
<td style="text-align: left;">BERTScore.Free</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">31.6</td>
</tr>
<tr>
<td style="text-align: left;">BARTScore+CNN+Para</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">28.0</td>
</tr>
<tr>
<td style="text-align: left;">BERT-iBLEU</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">32.7</td>
</tr>
<tr>
<td style="text-align: left;">ParaScore</td>
<td style="text-align: center;">$\mathbf{5 3 . 0}$</td>
<td style="text-align: center;">$\mathbf{5 2 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">ParaScore.Free</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">49.6</td>
</tr>
<tr>
<td style="text-align: left;">IMPLICIT SCORE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-001</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">15.9</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-003</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">40.3</td>
</tr>
<tr>
<td style="text-align: left;">EXPLICIT SCORE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (sampling)</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">44.3</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (greedy)</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">45.4</td>
</tr>
</tbody>
</table>
<p>Table 4: Dataset-level Spearman (Spear.) and Pearson (Pear.) correlation on Twitter (Extend).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metrics</th>
<th style="text-align: center;">Kend.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">COH</td>
<td style="text-align: center;">FLU</td>
<td style="text-align: center;">CON</td>
</tr>
<tr>
<td style="text-align: left;">IMPLICIT SCORE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-001</td>
<td style="text-align: center;">-3.2</td>
<td style="text-align: center;">-4.3</td>
<td style="text-align: center;">9.3</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-003</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">$\mathbf{2 4 . 5}$</td>
<td style="text-align: center;">35.3</td>
</tr>
<tr>
<td style="text-align: left;">EXPLICIT SCORE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (sampling)</td>
<td style="text-align: center;">$\mathbf{5 0 . 3}$</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">31.7</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (greedy)</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">$\mathbf{3 2 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">COMPARISON</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (sampling)</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">24.2</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (greedy)</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">22.0</td>
</tr>
</tbody>
</table>
<p>Table 5: Estimated Kendall's tau-b (Kend.) correlation of different aspects on SummEval.</p>
<h2>5 Detailed Analysis</h2>
<h3>5.1 Why does the pairwise comparison paradigm perform worse?</h3>
<p>In the main experiments, it is noteworthy that direct pairwise comparison using ChatGPT did not yield satisfactory results. To investigate whether this was caused by poorly designed prompts, alternative prompts were also evaluated. These prompts are briefly described in Table 9, with detailed information provided in Appendix B. Surprisingly, changing the prompt did not improve performance, but rather worsened it, as illustrated in Figure 1.</p>
<p>To gain further insights, we examined the confusion matrices of results based on different prompts, as shown in Figure 2. Our analysis revealed that, although we have provided the option of "both storylines equally good" in the default prompt (Prompt V1), ChatGPT still tended to choose one storyline that it deemed "better", as observed from Fig-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metrics</th>
<th style="text-align: center;">Kend.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">COH</td>
<td style="text-align: center;">CON</td>
<td style="text-align: center;">DIV</td>
<td style="text-align: center;">DEP</td>
<td style="text-align: center;">LIK</td>
<td style="text-align: center;">UND</td>
<td style="text-align: center;">FLE</td>
<td style="text-align: center;">INF</td>
<td style="text-align: center;">INQ</td>
<td style="text-align: center;">Overall</td>
</tr>
<tr>
<td style="text-align: left;">IMPLICIT SCORE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-001</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">33.6</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-003</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">36.7</td>
</tr>
<tr>
<td style="text-align: left;">EXPLICIT SCORE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (sampling)</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">$\mathbf{4 4 . 1}$</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">$\mathbf{4 5 . 9}$</td>
<td style="text-align: center;">$\mathbf{4 4 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (greedy)</td>
<td style="text-align: center;">$\mathbf{5 0 . 2}$</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">$\mathbf{4 5 . 5}$</td>
<td style="text-align: center;">$\mathbf{5 3 . 5}$</td>
<td style="text-align: center;">$\mathbf{5 0 . 8}$</td>
<td style="text-align: center;">$\mathbf{5 3 . 7}$</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">$\mathbf{4 7 . 7}$</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">41.7</td>
</tr>
<tr>
<td style="text-align: left;">COMPARISON</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (sampling)</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">$\mathbf{5 5 . 5}$</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">38.6</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (greedy)</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">37.5</td>
</tr>
</tbody>
</table>
<p>Table 6: Estimated Kendall's tau-b (Kend.) correlation of different aspects on dialogue-level FED.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metrics</th>
<th style="text-align: center;">Kend.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Hard</td>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">Easy All</td>
</tr>
<tr>
<td style="text-align: left;">IMPLICIT SCORE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-001</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">44.4</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-003</td>
<td style="text-align: center;">$\mathbf{2 7 . 9}$</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">66.733.2</td>
</tr>
<tr>
<td style="text-align: left;">EXPLICIT SCORE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (sampling)</td>
<td style="text-align: center;">18.5</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">74.3</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (greedy)</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">$\mathbf{6 2 . 6}$</td>
<td style="text-align: center;">$\mathbf{8 2 . 5} \quad \mathbf{3 6 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">COMPARISON</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (sampling)</td>
<td style="text-align: center;">8.1</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">33.3</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (greedy)</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">55.619.7</td>
</tr>
</tbody>
</table>
<p>Table 7: Estimated Kendall's tau-b (Kend.) correlation on OpenMEVA.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metrics</th>
<th style="text-align: center;">Kend.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Hard</td>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">Easy All</td>
</tr>
<tr>
<td style="text-align: left;">IMPLICIT SCORE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-001</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">13.6</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-003</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">59.128.6</td>
</tr>
<tr>
<td style="text-align: left;">EXPLICIT SCORE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (sampling)</td>
<td style="text-align: center;">$\mathbf{2 7 . 8}$</td>
<td style="text-align: center;">$\mathbf{4 0 . 0}$</td>
<td style="text-align: center;">53.8</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (greedy)</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">57.031.2</td>
</tr>
<tr>
<td style="text-align: left;">COMPARISON</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (sampling)</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">$\mathbf{6 8 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (greedy)</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">65.126.3</td>
</tr>
</tbody>
</table>
<p>Table 8: Estimated Kendall's tau-b (Kend.) correlation on Twitter (Extend).
ure 2(a). This could be attributed to the bias introduced by adding "Answer: I will choose Option" at the end of the prompt, which may have induced the model to make a biased choice at the beginning of the answer. To address this issue, we modified the prompt to require ChatGPT to present its reasoning process before making the final decision
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Estimated Kendall's tau-b (Kend.) correlation of Pairwise Comparison using ChatGPT with different prompts on OpenMEVA. We use greedy decoding for Prompt V1 V3. Whereas, for Prompt V4 we use Top-P sampling five times to obtain multiple results and vote for the final decision.
(Prompt V4). With this prompt, the model was more likely to choose the "tie" option, as indicated by the "s1=s2" column in Figure 2(b).</p>
<p>After analyzing ChatGPT's reasoning process, we discovered that ChatGPT frequently concludes that "the quality of the two given storylines is equally poor." As a result, we prompted ChatGPT to choose the "worse" storyline instead of the "better" one (Prompt V3). However, this questioning approach did not yield a better outcome. In addition, Figure 2(c) shows that although Prompt V3 is a mirrored version of Prompt V1, which changes the prompt from selecting the better option to choosing the worse one, ChatGPT's results based on these two prompts are not always consistent. For example, in one case, ChatGPT selected Storyline-1 as better based on Prompt V1, but under the guidance of Prompt V3, it may not necessarily choose Storyline-2 as worse.</p>
<p>Overall, we speculate that the poor quality of the</p>
<h1>Prompts for Pairwise Comparison on Story Generation</h1>
<p>Prompt V1 The default prompt where we first provide the beginning of the story and the corresponding two storylines for comparison before presenting the question.
Prompt V2 A revised version of Prompt V1 where we first propose the question, then provide the beginning of the story and present the two storylines to be compared in the form of options.
Prompt V3 A mirrored version of Prompt V1 where we instruct the model to choose "which one is worse" instead of "which one is better" from the two given storylines.
Prompt V4 A "chain-of-thought" version of Prompt V1 where we require the model to illustrate the reasoning process before presenting the final answer.</p>
<h2>Prompts for Explicit Score on Story Generation</h2>
<p>Prompt V1 The default prompt where we only specify the rating criteria for zero and full marks.
Prompt V2 A rephrased version of Prompt V1.
Prompt V3 A simplified version of Prompt V1 where we only describe the dimensions that need to be evaluated.
Prompt V4 A detailed prompt where we divide the scores into 5 scales and list the corresponding evaluation criteria for each score scale.
Prompt V5 A "chain-of-thought" version of Prompt V1 where we require the model to first present the reasons for the evaluation, and then provide the final score.</p>
<p>Table 9: Prompts designed for Pairwise Comparison and Explicit Score for assessing the quality of storylines in story generation. Note that Prompt V4 of Explicit Score is cited from (Wang et al., 2023a).
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Confusion matrices of pairwise comparisons on OpenMEVA based on different prompts using ChatGPT. Prompt V1 is the default prompt used in the main experiments. Prompt V4 and V3 are the "chain-of-thought" and "mirrored" versions of Prompt V1 respectively. Details of these prompts are presented in Table 9 and Appendix B.
candidate texts used in our experiments is the main reason why comparing pairs directly with ChatGPT did not yield good results. ChatGPT perceives the candidate texts as generally low quality, making it to select a "better" or "worse" one from them. This might lead to ChatGPT's unstable decisions.</p>
<h3>5.2 Why does Explicit Score generally perform better than Implicit Score?</h3>
<p>In order to obtain the Explicit Score, we utilize ChatGPT to generate scores in a natural language format. However, as we do not have access to ChatGPT's token probabilities, we instead rely on the confidence of text-davinci series models to determine the Implicit Score, which reflects how well a text meets a particular evaluation criterion. As stated in the Main Experiments (§4), the Explicit
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Distribution of different types of Individual Scores on OpenMEVA. The Implicit Score is rescaled into $[0,100]$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Explicit Score</th>
<th style="text-align: center;">Spear.</th>
<th style="text-align: center;">Pear.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">w/ Prompt V1 (GREEDY)</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">51.7</td>
</tr>
<tr>
<td style="text-align: left;">w/ Prompt V2 (GREEDY)</td>
<td style="text-align: center;">$\mathbf{5 0 . 8}$</td>
<td style="text-align: center;">$\mathbf{5 3 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">w/ Prompt V3 (GREEDY)</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">52.0</td>
</tr>
<tr>
<td style="text-align: left;">w/ Prompt V4 (GREEDY)</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">48.4</td>
</tr>
<tr>
<td style="text-align: left;">w/ Prompt V5 (SAMPLING)</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">50.8</td>
</tr>
</tbody>
</table>
<p>Table 10: Sample-level Spearman (Spear.) and Pearson (Pear.) correlation for Explicit Score based on ChatGPT with different prompts on OpenMEVA. We use greedy decoding for Prompt V1 V4. Whereas, for Prompt V5, we employ Top-P sampling five times to generate multiple reasons and average the resulting scores.</p>
<p>Score is generally more effective than the Implicit Score. This difference in effectiveness could be attributed not only to the variation in the models used but also to the distribution of the two scores. Figure 3 illustrates that the Implicit Score distribution has a peaked structure and is concentrated within a small range. In contrast, the Explicit Score distribution is smoother, allowing for greater discrimination between scores for different texts.</p>
<h3>5.3 How does the prompt design affect Explicit Score?</h3>
<p>We also investigate the impact of prompt design on the performance of rating Explicit Scores generated by ChatGPT. The detailed prompts are provided in Appendix A, and their main features and differences are summarized in Table 9. Our results, presented in Table 10, indicate that paraphrasing (V2) or simplifying (V3) the default prompt (V1) does not significantly affect the performance of Explicit Score based on ChatGPT. In contrast, refining scoring criteria (V4) or providing reasons before scoring (V5) results in a slight decrease in performance. The former may be due to the fact that the refined scoring rules in Prompt V4 do not fully match the standards used for actual manual annotation, and dividing scores into five scales reduces the distinction between scores for different samples. The latter may be due to the overall low quality of the dataset. Our observation indicates that ChatGPT's evaluations for each text are similar and mostly negative. After giving reasons before scoring, ChatGPT's scoring focuses more on the reasons rather than the text itself, resulting in lower scores for each text based on Prompt V5 and reducing the distinction between scores. The detailed distribution of scores derived from different prompts is demonstrated using a violin plot in Figure 4.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Distribution of Explicit Scores based on ChatGPT with different prompts on OpenMEVA. For Prompt V4, the scores are normalized into $[0,100]$.</p>
<h2>6 Related Work</h2>
<p>In the field of text quality evaluation, researchers have devised two main lines of approaches: reference-based and reference-free methods. The reference-based text evaluation aims to assess the quality by comparing outputs with ground truth, e.g. ROUGE (Lin, 2004), BERTScore (Zhang* et al., 2020) and BARTScore (Yuan et al., 2021). However, due to the inherent complexity and diversity of text, it is impossible to obtain references covering the entire spectrum of potential outputs. This limitation has prompted researchers to explore reference-free evaluation methods without relying on predefined references e.g. iBLEU (Sun and Zhou, 2012) and ParaScore (Shen et al., 2022). In this line, a reliable sentence representation model is required (Gao et al., 2021; Shen et al., 2023a,b). Recent studies have indicated that LLM-based evaluation methods can exhibit good consistency with human evaluation in assessing text quality (Fu et al., 2023; Wang et al., 2023a; Kocmi and Federmann, 2023; Ji et al., 2023). However, most of these works are preliminary explorations or require gold references. On the contrary, we are the first to conduct extensive experiments to investigate the optimal evaluation approaches using LLMs without references, and moreover propose some clues for customized text evaluation.</p>
<h2>7 Conclusion</h2>
<p>This paper explores the feasibility of LLMs, specifically ChatGPT and text-davinci series models, for evaluating text quality in a reference-free mode. Through an empirical study, we compare different</p>
<p>methods for the evaluation of text quality and recommend the use of an Explicit Score generated by ChatGPT as the most effective and stable approach. This paper also highlights the potential problem of directly comparing the quality of two texts using ChatGPT and the limitations of Implicit Scores obtained through the confidence of text-davinci series models. The prompt design is another crucial factor impacting the performance of the Explicit Score generated by ChatGPT. Overall, this paper demonstrates the potential of LLMs in evaluating text quality without reference and we hope it will provide useful insights for future research.</p>
<h2>Limitations</h2>
<h2>- Meta Evaluation Strategy</h2>
<p>We primarily assess the reliability of metrics based on their correlation with human scores. However, it should be noted that the consistency between scores annotated by different raters may not always be high in certain datasets. Hence, the correlation with human ratings may not always reflect the performance of metrics appropriately.</p>
<h2>- Coverage of Texts</h2>
<p>We only conducted experiments on four textgeneration tasks. Additionally, the quality distribution of the evaluated texts may be non-uniform, potentially lacking in extremely high-quality texts. Even if a metric performs well in evaluating a set of low-quality texts, it does not necessarily imply the same level of discrimination for high-quality texts, and vice versa. Furthermore, our evaluation has been limited to short texts, omitting the consideration of long-text generation.</p>
<h2>- Coverage of Models</h2>
<p>We utilize OpenAI's API to access their language models, including ChatGPT (gpt3.5-turbo-0301), text-davinci-003, and text-davinci-001. However, these models may be updated over time, which can result in inconsistencies in experimental outcomes. Moreover, we have not considered a wider range of LLMs, such as text-babbage-001, text-curie-001, and the FLAN-T5 series. Regrettably, due to API limitations, we were unable to obtain results from the more powerful GPT4 model.</p>
<h2>- Prompt Design</h2>
<p>Our exploration of prompts was limited to a few basic variations. Future research may benefit from
more sophisticated prompt designs, such as incorporating few-shot demonstrations, providing more precise annotation guidelines, or guiding the model through multi-turn conversations to facilitate a more accurate assessment of text quality.</p>
<h2>Acknowledgements</h2>
<p>This research was supported in part by the National Natural Science Foundation of China(62006062, 62176076), the Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies(2022B121201000 5), Natural Science Foundation of Guangdong(2023A1515012922), and Key Technologies Research and Development Program of Shenzhen JSGG20210802154400001.</p>
<h2>References</h2>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Nuo Chen, Yan Wang, Haiyun Jiang, Deng Cai, Ziyang Chen, and Jia Li. 2022a. What would harry say? building dialogue agents for characters in a story. arXiv preprint arXiv:2211.06869.</p>
<p>Yi Chen, Haiyun Jiang, Lemao Liu, Rui Wang, Shuming Shi, and Ruifeng Xu. 2022b. Mcpg: A flexible multi-level controllable framework for unsupervised paraphrase generation. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5948-5958.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.</p>
<p>Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320-335, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Alexander R. Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021a. SummEval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391-409.</p>
<p>Alexander R Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021b. Summeval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391-409.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire.</p>
<p>Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Simcse: Simple contrastive learning of sentence embeddings. arXiv preprint arXiv:2104.08821.</p>
<p>Sarik Ghazarian, Johnny Wei, Aram Galstyan, and Nanyun Peng. 2019. Better automatic evaluation of open-domain dialogue systems with contextualized embeddings. In Proceedings of the Workshop
on Methods for Optimizing and Evaluating Neural Language Generation, pages 82-89, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Jian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wenbiao Ding, Xiaoxi Mao, Changjie Fan, and Minlie Huang. 2021. OpenMEVA: A benchmark for evaluating open-ended story generation metrics. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6394-6407, Online. Association for Computational Linguistics.</p>
<p>Yunjie Ji, Yan Gong, Yiping Peng, Chao Ni, Peiyan Sun, Dongyu Pan, Baochang Ma, and Xiangang Li. 2023. Exploring chatgpt's ability to rank content: A preliminary study on consistency with human preferences.
M. G. Kendall. 1945. The treatment of ties in ranking problems. Biometrika, 33(3):239-251.</p>
<p>Tom Kocmi and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Longxuan Ma, Ziyu Zhuang, Weinan Zhang, Mingda Li, and Ting Liu. 2022. SelF-eval: Self-supervised fine-grained dialogue evaluation. In Proceedings of the 29th International Conference on Computational Linguistics, pages 485-495, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.</p>
<p>Shikib Mehri and Maxine Eskenazi. 2020a. Unsupervised evaluation of interactive dialog with DialoGPT. In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 225-235, 1st virtual meeting. Association for Computational Linguistics.</p>
<p>Shikib Mehri and Maxine Eskenazi. 2020b. USR: An unsupervised and reference free evaluation metric for dialog generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 681-707, Online. Association for Computational Linguistics.</p>
<p>Mohsen Mesgar, Sebastian Bücker, and Iryna Gurevych. 2020. Dialogue coherence assessment without explicit dialogue act labels. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1439-1450, Online. Association for Computational Linguistics.</p>
<p>Mavuto M Mukaka. 2012. A guide to appropriate use of correlation coefficient in medical research. Malawi medical journal, 24(3):69-71.</p>
<p>Tong Niu, Semih Yavuz, Yingbo Zhou, Nitish Shirish Keskar, Huan Wang, and Caiming Xiong. 2021. Unsupervised paraphrasing with pretrained language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5136-5150, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Lingfeng Shen, Haiyun Jiang, Lemao Liu, and Shuming Shi. 2023a. Sen2pro: A probabilistic perspective to sentence embedding from pre-trained language model. arXiv preprint arXiv:2306.02247.</p>
<p>Lingfeng Shen, Haiyun Jiang, Lemao Liu, and Shuming Shi. 2023b. A simple and plug-and-play method for unsupervised sentence representation enhancement. arXiv preprint arXiv:2305.07824.</p>
<p>Lingfeng Shen, Lemao Liu, Haiyun Jiang, and Shuming Shi. 2022. On the evaluation metrics for paraphrase generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3178-3190, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Mingyang Song, Haiyun Jiang, Shuming Shi, Songfang Yao, Shilong Lu, Yi Feng, Huafeng Liu, and Liping Jing. 2023. Is chatgpt a good keyphrase generator? a preliminary study. arXiv preprint arXiv:2303.13001.</p>
<p>Hong Sun and Ming Zhou. 2012. Joint learning of a dual SMT system for paraphrase generation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 38-42, Jeju Island, Korea. Association for Computational Linguistics.</p>
<p>Brian Thompson and Matt Post. 2020. Automatic machine translation evaluation in many languages via zero-shot paraphrasing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 90-121, Online. Association for Computational Linguistics.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. Is chatgpt a good nlg evaluator? a preliminary study.</p>
<p>Rui Wang, Jianzhu Bao, Fei Mi, Yi Chen, Hongru Wang, Yasheng Wang, Yitong Li, Lifeng Shang, Kam-Fai Wong, and Ruifeng Xu. 2023b. Retrieval-free knowledge injection through multi-document traversal for dialogue models. In Annual Meeting of the Association for Computational Linguistics.</p>
<p>Wei Xu, Chris Callison-Burch, and Bill Dolan. 2015. SemEval-2015 task 1: Paraphrase and semantic similarity in Twitter (PIT). In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 1-11, Denver, Colorado. Association for Computational Linguistics.</p>
<p>Wei Xu, Alan Ritter, Chris Callison-Burch, William B. Dolan, and Yangfeng Ji. 2014. Extracting lexically divergent paraphrases from Twitter. Transactions of the Association for Computational Linguistics, 2:435448 .</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. In Advances in Neural Information Processing Systems, volume 34, pages 27263-27277. Curran Associates, Inc.</p>
<p>Jerrold H Zar. 2005. Spearman rank correlation. Encyclopedia of biostatistics, 7.</p>
<p>Chen Zhang, Yiming Chen, Luis Fernando D'Haro, Yan Zhang, Thomas Friedrichs, Grandee Lee, and Haizhou Li. 2021. DynaEval: Unifying turn and dialogue level evaluation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5676-5689, Online. Association for Computational Linguistics.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.</p>
<p>Tianyi Zhang<em>, Varsha Kishore</em>, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations.</p>
<p>Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 563-578, Hong Kong, China. Association for Computational Linguistics.</p>
<h2>A Different Prompts for Explicit Score on Story Generation</h2>
<p>$======$ PROMPT FOR EXPLICIT SCORE V1 $======$
Score the following storyline given the beginning of the story on a continual scale from 0 (worst) to 100 (best), where a score of 0 means "The storyline makes no sense and is totally not understandable" and a score of 100 means "The storyline is perfect-written and highly consistent with the given beginning of the story".</p>
<p>The beginning of the story:
[Conditioned Text]
Storyline:
[Generated Text]
Score:</p>
<h2>$======$ PROMPT FOR EXPLICIT SCORE V2 $======$</h2>
<p>On a scale of 0 to 100, evaluate the storyline based on the given beginning. A score of 0 indicates that the storyline is incomprehensible, while a score of 100 means that the storyline is flawlessly written and logically follows from the beginning of the story.</p>
<p>The beginning of the story:
[Conditioned Text]
Storyline:
[Generated Text]
Score:</p>
<h2>$======$ PROMPT FOR EXPLICIT SCORE V3 $======$</h2>
<p>Score the overall quality of the following storyline given the beginning of the story on a continual scale from 0 (worst) to 100 (best). Consider whether the storyline is well-written and consistent with the given beginning of the story.</p>
<p>The beginning of the story:
[Conditioned Text]
Storyline:
[Generated Text]
Score:</p>
<h2>B Different Prompts for Pairwise Comparison on Story Generation</h2>
<p>$======$ PROMPT FOR EXPLICIT SCORE V4 $======$
Score the following storyline given the beginning of the story with one to five stars. Where</p>
<ul>
<li>one star means "Nonsense",</li>
<li>two stars mean "The storyline has some connections with the beginning, but is not understandable",</li>
<li>three stars mean "The storyline has some connections with the beginning and is understandable",</li>
<li>four stars mean "The storyline is consistent with the beginning and possibly involves a few grammar mistakes",</li>
<li>and five stars mean "Perfect storyline and grammar".</li>
</ul>
<p>The beginning of the story:
[Conditioned Text]
Storyline:
[Generated Text]
Stars (1-5):</p>
<p>$$
======\text { PROMPT FOR EXPLICIT SCORE V5 }=
$$</p>
<p>Score the following storyline given the beginning of the story on a continual scale from 0 (worst) to 100 (best), where a score of 0 means "The storyline makes no sense and is totally not understandable" and a score of 100 means "The storyline is perfect-written and highly consistent with the given beginning of the story". Please first give your reason carefully (indicated by "Reason:") and then decide your final score (indicated by "Score: 1-100").</p>
<p>The beginning of the story:
[Conditioned Text]
Storyline:
[Generated Text]</p>
<h2>$====$ PROMPT FOR PAIRWISE COMPARISON V1 $====$</h2>
<p>Consider the following two storylines written according to the given beginning of the story:</p>
<p>The beginning of the story:
[Conditioned Text]
Storyline-1:
[Generated Text-1]
Storyline-2:
[Generated Text-2]
Question: Which storyline is better-written and more consistent with the beginning of the story? Please answer with one of the following options.</p>
<h2>Options:</h2>
<p>(A) Storyline-1
(B) Storyline-2
(C) Both storylines are equally well-written and consistent with the beginning of the story.</p>
<p>Answer: I will choose Option</p>
<div class="codehilite"><pre><span></span><code><span class="o">=====</span><span class="w"> </span><span class="nv">PROMPT</span><span class="w"> </span><span class="k">FOR</span><span class="w"> </span><span class="nv">PAIRWISE</span><span class="w"> </span><span class="nv">COMPARISON</span><span class="w"> </span><span class="nv">V2</span><span class="w"> </span><span class="o">=====</span>
</code></pre></div>

<p>Question: Which storyline is better-written and more consistent with the beginning of the story? Please answer with one of the following options.</p>
<p>The beginning of the story:
[Conditioned Text]
Options:
(A) [Generated Text-1]
(B) [Generated Text-2]
(C) Both storylines are equally well-written and consistent with the beginning of the story.</p>
<p>Answer: I will choose Option</p>
<div class="codehilite"><pre><span></span><code><span class="o">=====</span><span class="w"> </span><span class="nv">PROMPT</span><span class="w"> </span><span class="k">FOR</span><span class="w"> </span><span class="nv">PAIRWISE</span><span class="w"> </span><span class="nv">COMPARISON</span><span class="w"> </span><span class="nv">V3</span><span class="w"> </span><span class="o">=====</span>
</code></pre></div>

<p>Consider the following two storylines written according to the given beginning of the story:
The beginning of the story:
[Conditioned Text]
Storyline-1:
[Generated Text-1]
Storyline-2:
[Generated Text-2]
Question: Which storyline has poorer writing and is less consistent with the beginning of the story? Please answer with one of the following options.</p>
<p>Options:
(A) Storyline-1
(B) Storyline-2
(C) Both storylines are equally poor-written and inconsistent with the beginning of the story.</p>
<p>Answer: I will choose Option</p>
<div class="codehilite"><pre><span></span><code><span class="o">=====</span><span class="w"> </span><span class="nv">PROMPT</span><span class="w"> </span><span class="k">FOR</span><span class="w"> </span><span class="nv">PAIRWISE</span><span class="w"> </span><span class="nv">COMPARISON</span><span class="w"> </span><span class="nv">V4</span><span class="w"> </span><span class="o">=====</span>
</code></pre></div>

<p>Consider the following two storylines written according to the given beginning of the story:
The beginning of the story:
[Conditioned Text]
Storyline-1:
[Generated Text-1]
Storyline-2:
[Generated Text-2]
Question: Which storyline is better-written and more consistent with the beginning of the story? Please first give your reason carefully (indicated by "Reason.") and then choose one of the following options (indicated by "Answer: $\mathrm{A} / \mathrm{B} / \mathrm{C}$ ").</p>
<p>Options:
(A) Storyline-1
(B) Storyline-2
(C) Both storylines are equally well-written (poor-written) and consistent (inconsistent) with the beginning of the story.</p>
<h1>C An Explanation of Kendall's Tall-b</h1>
<p>Kendall's Tau-b is a measure of the correlation between two variables, specifically designed to handle ties and ranks. The formula to calculate Kendall's Tau-b is as follows:</p>
<p>$$
\tau_{B}=\frac{P-Q}{\sqrt{(P+Q+T)(P+Q+U)}}
$$</p>
<p>where P is the number of concordant pairs, Q is the number of discordant pairs, T is the number of ties only in human judgments, and $U$ is the number of ties only in the given metric. To better understand the calculation of $\mathrm{P}, \mathrm{Q}, \mathrm{T}$, and U , we can refer to the following table:</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$s_{1}&lt;s_{2}$</td>
<td style="text-align: center;">$s_{1}=s_{2}$</td>
<td style="text-align: center;">$s_{1}&gt;s_{2}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$s_{1}&lt;s_{2}$</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">U</td>
<td style="text-align: center;">Q</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$s_{1}=s_{2}$</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">T</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$s_{1}&gt;s_{2}$</td>
<td style="text-align: center;">Q</td>
<td style="text-align: center;">U</td>
<td style="text-align: center;">P</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal Contribution.
${ }^{\dagger}$ Corresponding Authors.
${ }^{\dagger}$ https://github.com/MilkWhite/LLMs_for_
Reference_Free_Text_Quality_Evaluation&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ https://openai.com/blog/chatgpt&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3973 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3973</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3973</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-92.html">extraction-schema-92</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-4993258852711c4e3d0011325ac3db680eae84f4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4993258852711c4e3d0011325ac3db680eae84f4" target="_blank">SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> An in-depth benchmarking study of representative open-source and proprietary LLMs with various prompting strategies indicates that no single prompting strategy significantly outperforms the others and some strategies that demonstrate improvements in certain problem-solving skills could result in declines in other skills.</p>
                <p><strong>Paper Abstract:</strong> Most of the existing Large Language Model (LLM) benchmarks on scientific problem reasoning focus on problems grounded in high-school subjects and are confined to elementary algebraic operations. To systematically examine the reasoning capabilities required for solving complex scientific problems, we introduce an expansive benchmark suite SciBench for LLMs. SciBench contains a carefully curated dataset featuring a range of collegiate-level scientific problems from mathematics, chemistry, and physics domains. Based on the dataset, we conduct an in-depth benchmarking study of representative open-source and proprietary LLMs with various prompting strategies. The results reveal that the current LLMs fall short of delivering satisfactory performance, with the best overall score of merely 43.22%. Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis indicates that no single prompting strategy significantly outperforms the others and some strategies that demonstrate improvements in certain problem-solving skills could result in declines in other skills. We envision that SciBench will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3973.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3973.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCIBENCH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SCIBENCH (college-level Scientific problem solving Benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark introduced in this paper that contains 869 curated college-level free‑response problems across mathematics, chemistry, and physics (including a multimodal subset and a closed exam subset) designed to evaluate LLM scientific problem‑solving and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correctness of final numeric answer (compared to ground truth), ability to produce step-by-step solutions, multimodal interpretation (when visuals present), and demonstration of ten identified problem‑solving skills (e.g., calculation, logical decomposition, scientific literacy).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Automated exact/near numeric comparison (with tolerance), human-verified rubrics for exam grading, prompting variants (zero-shot, few-shot, CoT, tool-augmented), and downstream automated error attribution via an LLM verifier that classifies skill deficiencies.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>SCIBENCH: 869 textbook problems (free-response, numeric answers rounded to 3 decimals), multimodal subset: 177 problems with visual contexts, closed exam subset: 103 problems from college exams; answers provided as numeric + LaTeX, step-by-step reference solutions provided for error analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Accuracy (%) per model and per prompting/setting (e.g., best overall: GPT-4 few-shot+Python 43.22% on textual dataset; GPT-4 (PoT) 13.8% on multimodal subset; exam subset top score 51.57%), per-textbook accuracies, and classifier agreement metrics (verifier accuracy 81.45%).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human annotators curated and LaTeX-verified problems, provided/verified few-shot examples and code, graded exam solutions using instructor rubrics, and validated/verdict-checked the LLM verifier's classifications (sampled human check over 151 examples).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Focuses on free-response numeric problems rather than open-ended theory generation; tolerance of 5% for numeric answers may mask finer semantic errors; dataset design restricts answers to numeric single values (limits evaluating descriptive theories); potential residual data leakage mitigated but not provably eliminated; multimodal OCR/caption augmentation needed for some models.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Not an explicit LLM-generated scientific theory; representative evaluated outputs include step-by-step solutions and numerical answers — e.g., a Planck distribution problem where CoT produced a correct symbolic derivation but failed numeric computation, and the Python-program prompt introduced a variable-placement error.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>SCIBENCH discriminates models and settings: proprietary models (GPT-4, GPT-4-Turbo) outperform open-source models; CoT improves calculation errors but not all skills; tool-augmented (Python/Wolfram) can reduce calculation errors but may introduce code-conversion and other reasoning failures; no single prompting strategy universally improves all ten skills.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3973.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3973.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation Protocol (LLM verifier + self-refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated evaluation protocol using an LLM verifier and self-critique augmented by human checks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A protocol introduced in this paper that (1) defines ten essential scientific problem‑solving skills, (2) uses an LLM as a verifier to classify which skill(s) a model's incorrect solution lacks, and (3) applies human spot-checks to validate/clean the automated classifications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Absence/presence of specific problem-solving skills (ten predefined categories) as the primary criterion for qualitative error attribution; numeric correctness remains the criterion for pass/fail.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Step 1: compare model solution to reference solution; Step 2: use a prompted LLM (GPT-3.5) as a classifier/verifier to select which skill from the ten is missing; Step 3: human annotators review a sample (human-in-the-loop) and discard ~20% of verifier misclassifications.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to SCIBENCH problems that include detailed solutions (to enable fine-grained comparison), using the same problem splits and settings as the quantitative evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Verifier classification accuracy on sampled human-checked set: 81.45% (123/151 correct); percentage of incorrectly classified skills discarded (~20%). Error-profile counts/rates per skill under each prompting/setting were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Two college-student annotators produced initial line-level error annotations used to derive the ten skills; human annotators validated verifier outputs on a sampled set and made final determinations for contested cases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Verifier is not perfectly reliable (≈81% accuracy), requiring human oversight; about 20% of automated classifications are discarded; deriving discrete skill labels may miss multi-causal errors or overlapping failure modes; method relies on availability of reference step-by-step solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Example use: GPT-3.5 solutions were analyzed and the LLM verifier classified failures into categories such as 'Calculation Skills' or 'Causal Reasoning' for each incorrect solution; Planck-distribution example was used to illustrate a calculation vs. reasoning mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Produced error profiles showing which skills each prompting strategy improved or harmed (e.g., CoT reduced calculation errors but increased errors in causal reasoning and logical decomposition in some settings).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3973.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3973.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ten-Skill Taxonomy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ten essential scientific problem‑solving skills (taxonomy for error attribution)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A taxonomy of ten granular skills distilled from human annotations of LLM errors, used to characterize which cognitive/problem-solving abilities LLMs lack when solving college-level scientific problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Presence or absence of specific skills (1) logical decomposition and analysis, (2) assumption identification, (3) spatial perception, (4) causal reasoning, (5) problem deduction, (6) abstract reasoning, (7) scientific literacy, (8) code conversion skills, (9) logical reasoning, (10) calculation skills — used to interpret failures beyond final numeric error.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Human distilled categories from 112 error annotations; then applied an LLM verifier to classify model failures into these categories, followed by human spot-checks.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied across SCIBENCH; classification performed only on problems with available detailed solutions to enable pinpointing of erroneous lines.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Per-setting and per-model error rates assigned to each of the ten skills (visualized as error profiles); e.g., calculation-related error percentages (CoT reduced calculation error from 29.0% to 13.6% in one reported comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human annotators produced initial line-level annotations that created the taxonomy and later validated automated classifications on sampled examples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Categories are human-curated and could overlap; automated classification has limited accuracy; some errors stem from multiple skills simultaneously which challenges single-label attribution.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Classification example: ChatGPT CoT solution to an electron momentum problem was labeled 'Causal Reasoning' because the model used an unnecessary relativistic formula, while the non-CoT variant was labeled 'Calculation Skills' for numeric mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Revealed that different prompting strategies selectively improve or worsen certain skills (e.g., CoT helps calculation but can worsen causal reasoning and logical decomposition in some settings; tool-usage helps calculation but harms code-conversion skills).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3973.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3973.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompting & Tool Strategies</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompting strategies and tool-augmented approaches (CoT, few-shot, zero-shot, PoT, PAL, external Python/Wolfram tools)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper evaluates how different prompting methods (zero-shot, few-shot, Chain-of-Thought, Program-of-Thoughts) and tool-augmented setups (translate steps to Python/Wolfram and execute) affect LLM problem-solving performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Improvement in final numeric accuracy and changes in error-profile across the ten skill categories; comparison of how each strategy affects calculation vs. other reasoning skills.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Experimental comparisons across multiple LLMs and settings (zero-shot, zero-shot+CoT, few-shot+CoT, few-shot+Python, few-shot+Wolfram); verified few-shot code snippets manually; execute produced code (Python/Wolfram) to obtain numeric outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied on SCIBENCH textbook, multimodal subset (with image captions/OCR for GPT-4), and closed exam subset.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Setting-specific accuracy (%) tables (detailed per textbook and averaged); differences reported such as GPT-4 improving from 30.36% to 43.22% when moving to few-shot+Python; CoT reduced calculation error from 29.0% → 13.6% in an example.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Humans curated and verified few-shot examples and code snippets; human annotators verified correct execution of code examples in few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Tool-augmented prompts can introduce code-conversion and syntax errors (Wolfram showed 46.9% code-conversion error rate), and using tools can compromise other reasoning skills; few-shot does not universally improve all skills; different models respond differently to strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Planck distribution example: CoT produced the correct symbolic formula but failed numerically; Python tool-augmented prompt produced a program that mis-placed a variable in the formula leading to wrong numeric output.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>No single strategy works best for all skills or models: CoT often improves calculations, external-program execution can improve numeric accuracy for some models (e.g., GPT-4) but can introduce new errors (code conversion); GPT-4-Turbo excelled in zero‑shot but performed worse when using Python for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3973.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3973.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Quantitative Scoring & Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Numeric scoring protocol, tolerance rules and reported performance metrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses automated numeric comparison (with a 5% relative tolerance) as the primary quantitative scoring method and reports per-model accuracies, averaged weighted scores, and error-profile percentages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Primary criterion: numeric equality within a relative tolerance of 5%; secondary analyses consider distribution of error types across the ten skills and model comparisons across datasets and settings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Compare model predicted numeric answer to ground truth numeric value (answers normalized to decimals with three places); for exam subset use instructor rubrics to compute scores; code outputs executed and their numeric outputs used as model answers.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>SCIBENCH textual dataset, multimodal subset, and closed exam dataset; answers normalized (scientific notation and units handled per instructions).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Accuracy percentages per textbook and averaged; best reported scores include: 43.22% (GPT-4 few-shot+Python on textual), 13.8% (multimodal GPT-4 PoT), 51.57% (closed exam best), verifier classification accuracy 81.45%, and Wolfram code-conversion error 46.9% in one experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human graders applied rubrics for exam dataset; humans verified answer normalization and handled ambiguous/extraction issues; human checks on automated verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Numeric-only scoring excludes free-text conceptual correctness; 5% tolerance is arbitrary and could obscure near-miss conceptual failures; single-number answers preclude evaluation of partial credit or explanatory power; performance metrics are sensitive to dataset composition and the presence/absence of detailed solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Evaluated outputs reduced to numeric answers (e.g., volume of rotated region = 2π/15 converted to decimal) so models are judged by numeric precision rather than conceptual novelty or explanatory power.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported extensive per-model, per-setting accuracy tables that show large gaps between models and that no model achieves human-level performance across datasets; metrics reveal specific weaknesses (e.g., low multimodal accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3973.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3973.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Annotation & Verification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human curation, annotation, and rubric-based verification process</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human annotators curate datasets, verify LaTeX and code snippets, produce line-level error annotations used to derive the ten-skill taxonomy, and perform spot-checks of automated LLM verifier labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Human judgment used to ensure dataset correctness, to construct ground-truth step-by-step solutions, and to validate/verdict-check automated error classifications.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Manual extraction and LaTeX conversion of textbook problems, creation and verification of few-shot examples and executable code, human grading with rubrics for exam subset, and two annotators reviewing verifier outputs for sampled cases.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied across SCIBENCH dataset creation and the evaluation protocol; human labels used to form the training and validation sets for the self-critique LLM verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Human verification statistics: two annotators created initial annotations from 112 error cases; human-AI validation sample: 151 examples, with 123 correctly classified by the verifier (81.45% agreement).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Extensive: dataset curation, step-by-step solution authoring/verification, few-shot code verification, exam grading rubrics, development of taxonomy, and validation of automated classifier outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Human annotation is costly and time-consuming; inter-annotator subjectivity affects taxonomy creation; human review still required because the automated verifier misclassifies ~20% of examples; scaling human checks is expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Humans identified and annotated concrete model errors (e.g., where a solver miscomputed a Planck-distribution numeric step), these annotations were used to train and validate the LLM verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Human involvement enabled trustworthy dataset creation and provided ground truth for evaluating and improving the automated classification pipeline; the human-AI pipeline reduced cost versus full manual annotation but retained the need for spot-checking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>PAL: Program-aided language models <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 2)</em></li>
                <li>GSM8K: Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>MATH: Measuring mathematical problem solving with the MATH dataset <em>(Rating: 2)</em></li>
                <li>SciEval: A multi-level large language model evaluation benchmark for scientific research <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3973",
    "paper_id": "paper-4993258852711c4e3d0011325ac3db680eae84f4",
    "extraction_schema_id": "extraction-schema-92",
    "extracted_data": [
        {
            "name_short": "SCIBENCH",
            "name_full": "SCIBENCH (college-level Scientific problem solving Benchmark)",
            "brief_description": "A benchmark introduced in this paper that contains 869 curated college-level free‑response problems across mathematics, chemistry, and physics (including a multimodal subset and a closed exam subset) designed to evaluate LLM scientific problem‑solving and reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Correctness of final numeric answer (compared to ground truth), ability to produce step-by-step solutions, multimodal interpretation (when visuals present), and demonstration of ten identified problem‑solving skills (e.g., calculation, logical decomposition, scientific literacy).",
            "evaluation_methods": "Automated exact/near numeric comparison (with tolerance), human-verified rubrics for exam grading, prompting variants (zero-shot, few-shot, CoT, tool-augmented), and downstream automated error attribution via an LLM verifier that classifies skill deficiencies.",
            "benchmark_or_dataset": "SCIBENCH: 869 textbook problems (free-response, numeric answers rounded to 3 decimals), multimodal subset: 177 problems with visual contexts, closed exam subset: 103 problems from college exams; answers provided as numeric + LaTeX, step-by-step reference solutions provided for error analysis.",
            "metrics_reported": "Accuracy (%) per model and per prompting/setting (e.g., best overall: GPT-4 few-shot+Python 43.22% on textual dataset; GPT-4 (PoT) 13.8% on multimodal subset; exam subset top score 51.57%), per-textbook accuracies, and classifier agreement metrics (verifier accuracy 81.45%).",
            "human_involvement": "Human annotators curated and LaTeX-verified problems, provided/verified few-shot examples and code, graded exam solutions using instructor rubrics, and validated/verdict-checked the LLM verifier's classifications (sampled human check over 151 examples).",
            "limitations_or_challenges": "Focuses on free-response numeric problems rather than open-ended theory generation; tolerance of 5% for numeric answers may mask finer semantic errors; dataset design restricts answers to numeric single values (limits evaluating descriptive theories); potential residual data leakage mitigated but not provably eliminated; multimodal OCR/caption augmentation needed for some models.",
            "llm_theory_example": "Not an explicit LLM-generated scientific theory; representative evaluated outputs include step-by-step solutions and numerical answers — e.g., a Planck distribution problem where CoT produced a correct symbolic derivation but failed numeric computation, and the Python-program prompt introduced a variable-placement error.",
            "evaluation_results": "SCIBENCH discriminates models and settings: proprietary models (GPT-4, GPT-4-Turbo) outperform open-source models; CoT improves calculation errors but not all skills; tool-augmented (Python/Wolfram) can reduce calculation errors but may introduce code-conversion and other reasoning failures; no single prompting strategy universally improves all ten skills.",
            "uuid": "e3973.0",
            "source_info": {
                "paper_title": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Evaluation Protocol (LLM verifier + self-refinement)",
            "name_full": "Automated evaluation protocol using an LLM verifier and self-critique augmented by human checks",
            "brief_description": "A protocol introduced in this paper that (1) defines ten essential scientific problem‑solving skills, (2) uses an LLM as a verifier to classify which skill(s) a model's incorrect solution lacks, and (3) applies human spot-checks to validate/clean the automated classifications.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Absence/presence of specific problem-solving skills (ten predefined categories) as the primary criterion for qualitative error attribution; numeric correctness remains the criterion for pass/fail.",
            "evaluation_methods": "Step 1: compare model solution to reference solution; Step 2: use a prompted LLM (GPT-3.5) as a classifier/verifier to select which skill from the ten is missing; Step 3: human annotators review a sample (human-in-the-loop) and discard ~20% of verifier misclassifications.",
            "benchmark_or_dataset": "Applied to SCIBENCH problems that include detailed solutions (to enable fine-grained comparison), using the same problem splits and settings as the quantitative evaluation.",
            "metrics_reported": "Verifier classification accuracy on sampled human-checked set: 81.45% (123/151 correct); percentage of incorrectly classified skills discarded (~20%). Error-profile counts/rates per skill under each prompting/setting were reported.",
            "human_involvement": "Two college-student annotators produced initial line-level error annotations used to derive the ten skills; human annotators validated verifier outputs on a sampled set and made final determinations for contested cases.",
            "limitations_or_challenges": "Verifier is not perfectly reliable (≈81% accuracy), requiring human oversight; about 20% of automated classifications are discarded; deriving discrete skill labels may miss multi-causal errors or overlapping failure modes; method relies on availability of reference step-by-step solutions.",
            "llm_theory_example": "Example use: GPT-3.5 solutions were analyzed and the LLM verifier classified failures into categories such as 'Calculation Skills' or 'Causal Reasoning' for each incorrect solution; Planck-distribution example was used to illustrate a calculation vs. reasoning mismatch.",
            "evaluation_results": "Produced error profiles showing which skills each prompting strategy improved or harmed (e.g., CoT reduced calculation errors but increased errors in causal reasoning and logical decomposition in some settings).",
            "uuid": "e3973.1",
            "source_info": {
                "paper_title": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Ten-Skill Taxonomy",
            "name_full": "Ten essential scientific problem‑solving skills (taxonomy for error attribution)",
            "brief_description": "A taxonomy of ten granular skills distilled from human annotations of LLM errors, used to characterize which cognitive/problem-solving abilities LLMs lack when solving college-level scientific problems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Presence or absence of specific skills (1) logical decomposition and analysis, (2) assumption identification, (3) spatial perception, (4) causal reasoning, (5) problem deduction, (6) abstract reasoning, (7) scientific literacy, (8) code conversion skills, (9) logical reasoning, (10) calculation skills — used to interpret failures beyond final numeric error.",
            "evaluation_methods": "Human distilled categories from 112 error annotations; then applied an LLM verifier to classify model failures into these categories, followed by human spot-checks.",
            "benchmark_or_dataset": "Applied across SCIBENCH; classification performed only on problems with available detailed solutions to enable pinpointing of erroneous lines.",
            "metrics_reported": "Per-setting and per-model error rates assigned to each of the ten skills (visualized as error profiles); e.g., calculation-related error percentages (CoT reduced calculation error from 29.0% to 13.6% in one reported comparison).",
            "human_involvement": "Human annotators produced initial line-level annotations that created the taxonomy and later validated automated classifications on sampled examples.",
            "limitations_or_challenges": "Categories are human-curated and could overlap; automated classification has limited accuracy; some errors stem from multiple skills simultaneously which challenges single-label attribution.",
            "llm_theory_example": "Classification example: ChatGPT CoT solution to an electron momentum problem was labeled 'Causal Reasoning' because the model used an unnecessary relativistic formula, while the non-CoT variant was labeled 'Calculation Skills' for numeric mistakes.",
            "evaluation_results": "Revealed that different prompting strategies selectively improve or worsen certain skills (e.g., CoT helps calculation but can worsen causal reasoning and logical decomposition in some settings; tool-usage helps calculation but harms code-conversion skills).",
            "uuid": "e3973.2",
            "source_info": {
                "paper_title": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Prompting & Tool Strategies",
            "name_full": "Prompting strategies and tool-augmented approaches (CoT, few-shot, zero-shot, PoT, PAL, external Python/Wolfram tools)",
            "brief_description": "The paper evaluates how different prompting methods (zero-shot, few-shot, Chain-of-Thought, Program-of-Thoughts) and tool-augmented setups (translate steps to Python/Wolfram and execute) affect LLM problem-solving performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_criteria": "Improvement in final numeric accuracy and changes in error-profile across the ten skill categories; comparison of how each strategy affects calculation vs. other reasoning skills.",
            "evaluation_methods": "Experimental comparisons across multiple LLMs and settings (zero-shot, zero-shot+CoT, few-shot+CoT, few-shot+Python, few-shot+Wolfram); verified few-shot code snippets manually; execute produced code (Python/Wolfram) to obtain numeric outputs.",
            "benchmark_or_dataset": "Applied on SCIBENCH textbook, multimodal subset (with image captions/OCR for GPT-4), and closed exam subset.",
            "metrics_reported": "Setting-specific accuracy (%) tables (detailed per textbook and averaged); differences reported such as GPT-4 improving from 30.36% to 43.22% when moving to few-shot+Python; CoT reduced calculation error from 29.0% → 13.6% in an example.",
            "human_involvement": "Humans curated and verified few-shot examples and code snippets; human annotators verified correct execution of code examples in few-shot prompts.",
            "limitations_or_challenges": "Tool-augmented prompts can introduce code-conversion and syntax errors (Wolfram showed 46.9% code-conversion error rate), and using tools can compromise other reasoning skills; few-shot does not universally improve all skills; different models respond differently to strategies.",
            "llm_theory_example": "Planck distribution example: CoT produced the correct symbolic formula but failed numerically; Python tool-augmented prompt produced a program that mis-placed a variable in the formula leading to wrong numeric output.",
            "evaluation_results": "No single strategy works best for all skills or models: CoT often improves calculations, external-program execution can improve numeric accuracy for some models (e.g., GPT-4) but can introduce new errors (code conversion); GPT-4-Turbo excelled in zero‑shot but performed worse when using Python for some tasks.",
            "uuid": "e3973.3",
            "source_info": {
                "paper_title": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Quantitative Scoring & Metrics",
            "name_full": "Numeric scoring protocol, tolerance rules and reported performance metrics",
            "brief_description": "The paper uses automated numeric comparison (with a 5% relative tolerance) as the primary quantitative scoring method and reports per-model accuracies, averaged weighted scores, and error-profile percentages.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Primary criterion: numeric equality within a relative tolerance of 5%; secondary analyses consider distribution of error types across the ten skills and model comparisons across datasets and settings.",
            "evaluation_methods": "Compare model predicted numeric answer to ground truth numeric value (answers normalized to decimals with three places); for exam subset use instructor rubrics to compute scores; code outputs executed and their numeric outputs used as model answers.",
            "benchmark_or_dataset": "SCIBENCH textual dataset, multimodal subset, and closed exam dataset; answers normalized (scientific notation and units handled per instructions).",
            "metrics_reported": "Accuracy percentages per textbook and averaged; best reported scores include: 43.22% (GPT-4 few-shot+Python on textual), 13.8% (multimodal GPT-4 PoT), 51.57% (closed exam best), verifier classification accuracy 81.45%, and Wolfram code-conversion error 46.9% in one experiment.",
            "human_involvement": "Human graders applied rubrics for exam dataset; humans verified answer normalization and handled ambiguous/extraction issues; human checks on automated verifier.",
            "limitations_or_challenges": "Numeric-only scoring excludes free-text conceptual correctness; 5% tolerance is arbitrary and could obscure near-miss conceptual failures; single-number answers preclude evaluation of partial credit or explanatory power; performance metrics are sensitive to dataset composition and the presence/absence of detailed solutions.",
            "llm_theory_example": "Evaluated outputs reduced to numeric answers (e.g., volume of rotated region = 2π/15 converted to decimal) so models are judged by numeric precision rather than conceptual novelty or explanatory power.",
            "evaluation_results": "Reported extensive per-model, per-setting accuracy tables that show large gaps between models and that no model achieves human-level performance across datasets; metrics reveal specific weaknesses (e.g., low multimodal accuracy).",
            "uuid": "e3973.4",
            "source_info": {
                "paper_title": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Human Annotation & Verification",
            "name_full": "Human curation, annotation, and rubric-based verification process",
            "brief_description": "Human annotators curate datasets, verify LaTeX and code snippets, produce line-level error annotations used to derive the ten-skill taxonomy, and perform spot-checks of automated LLM verifier labels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Human judgment used to ensure dataset correctness, to construct ground-truth step-by-step solutions, and to validate/verdict-check automated error classifications.",
            "evaluation_methods": "Manual extraction and LaTeX conversion of textbook problems, creation and verification of few-shot examples and executable code, human grading with rubrics for exam subset, and two annotators reviewing verifier outputs for sampled cases.",
            "benchmark_or_dataset": "Applied across SCIBENCH dataset creation and the evaluation protocol; human labels used to form the training and validation sets for the self-critique LLM verifier.",
            "metrics_reported": "Human verification statistics: two annotators created initial annotations from 112 error cases; human-AI validation sample: 151 examples, with 123 correctly classified by the verifier (81.45% agreement).",
            "human_involvement": "Extensive: dataset curation, step-by-step solution authoring/verification, few-shot code verification, exam grading rubrics, development of taxonomy, and validation of automated classifier outputs.",
            "limitations_or_challenges": "Human annotation is costly and time-consuming; inter-annotator subjectivity affects taxonomy creation; human review still required because the automated verifier misclassifies ~20% of examples; scaling human checks is expensive.",
            "llm_theory_example": "Humans identified and annotated concrete model errors (e.g., where a solver miscomputed a Planck-distribution numeric step), these annotations were used to train and validate the LLM verifier.",
            "evaluation_results": "Human involvement enabled trustworthy dataset creation and provided ground truth for evaluating and improving the automated classification pipeline; the human-AI pipeline reduced cost versus full manual annotation but retained the need for spot-checking.",
            "uuid": "e3973.5",
            "source_info": {
                "paper_title": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "PAL: Program-aided language models",
            "rating": 2
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 2
        },
        {
            "paper_title": "GSM8K: Training verifiers to solve math word problems",
            "rating": 2
        },
        {
            "paper_title": "MATH: Measuring mathematical problem solving with the MATH dataset",
            "rating": 2
        },
        {
            "paper_title": "SciEval: A multi-level large language model evaluation benchmark for scientific research",
            "rating": 2
        }
    ],
    "cost": 0.017646,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models</h1>
<p>Xiaoxuan Wang ${ }^{<em> 1}$ Ziniu Hu</em>2 Pan Lu<em> ${ }^{</em> 1}$ Yanqiao Zhu ${ }^{* 1}$ Jieyu Zhang ${ }^{3}$ Satyen Subramaniam ${ }^{1}$ Arjun R. Loomba ${ }^{1}$ Shichang Zhang ${ }^{1}$ Yizhou Sun ${ }^{1}$ Wei Wang ${ }^{1}$<br>Project Homepage: https://scibench-ucla.github.io</p>
<h4>Abstract</h4>
<p>Most existing Large Language Model (LLM) benchmarks on scientific problem reasoning focus on problems grounded in high-school subjects and are confined to elementary algebraic operations. To systematically examine the reasoning capabilities required for solving complex scientific problems, we introduce an expansive benchmark suite SCIBENCH for LLMs. SCIBENCH contains a carefully curated dataset featuring a range of collegiate-level scientific problems from mathematics, chemistry, and physics domains. Based on the dataset, we conduct an in-depth benchmarking study of representative open-source and proprietary LLMs with various prompting strategies. The results reveal that current LLMs fall short of delivering satisfactory performance, with the best overall score of merely $43.22 \%$. Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis indicates that no single prompting strategy significantly outperforms the others and some strategies that demonstrate improvements in certain problem-solving skills could result in declines in other skills. We envision that SCIBENCH will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery.</p>
<h2>1. Introduction</h2>
<p>Recent advancements in Large Language Models (LLMs) have dramatically expanded the boundaries of artificial in-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>telligence (Brown et al., 2020; Gao et al., 2023; Liu et al., 2023b; OpenAI., 2022; Touvron et al., 2023a; Zhang et al., 2023a;b). They have demonstrated outstanding performance in many mathematical reasoning tasks that are typically considered challenging even for well-educated individuals (Chen et al., 2021; 2023a; Gao et al., 2022; Kojima et al., 2022; Wei et al., 2022). Notably, GPT-4 achieves a remarkable score of 163 out of 170 on the GRE Quantitative Exam, placing it at the 80th percentile ranking (OpenAI., 2023).</p>
<p>While the remarkable improvements in these benchmark performances might suggest that LLMs are capable of performing scientific reasoning tasks, we argue that this assertion might be overly optimistic due to the inherent limitations of current benchmarks. Firstly, many existing benchmarks such as ScienceQA (Lu et al., 2022) and GSM8K (Cobbe et al., 2021) only contain problems grounded in grade-level subjects. Although other benchmarks like MATH (Hendrycks et al., 2021) introduce highschool level questions, they primarily focus on math problems. Secondly, recent works like MMLU (Hendrycks et al., 2020), AGIEval (Zhong et al., 2023), and JEEBench (Arora et al., 2023), despite introducing challenging problems that span a wide range of disciplines, only require basic computations-addition, subtraction, multiplication, and exponentiation-which do not adequately assess the depth of reasoning abilities of LLMs for solving scientific problems. Lastly, most of these benchmarks only include textual problems, which omit problems that incorporate visual elements such as figures or diagrams.</p>
<p>In parallel to benchmark developments, many studies propose various prompting strategies aimed at enhancing the reasoning abilities of LLMs in scientific problem solving. A notable example is the Chain-of-Thought (CoT) approach, which instructs LLMs to generate detailed, step-by-step solutions that prompt deeper problem thinking (Huang et al., 2022; Wang et al., 2022; Wei et al., 2022; Zhou et al., 2022). Additionally, other strategies propose to enable LLMs to utilize external tools (Lu et al., 2023b; Schick et al., 2023) that improve the numerical computation capability. However, even these strategic approaches, each with its specific strengths, struggle to fully address complex scientific prob-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Problem (Using the Planck distribution) <br> Compare the energy output of a black-body radiator (such as an incandescent lamp) at two different wavelengths by calculating the ratio of the energy output at 450 nm (blue light) to that at 700 nm (red light) at 298 K .</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Correct Solution <br> At a temperature $T$, the ratio of the spectral density of states at a wavelength $\lambda_{1}$ to that at $\lambda_{2}$ is given by $\frac{\rho\left(\lambda_{1}, T\right)}{\rho\left(\lambda_{2}, T\right)}=\left(\frac{\lambda_{2}}{\lambda_{1}}\right)^{2} \times \frac{\left(e^{h c / \lambda_{2} k T}-1\right)}{\left(e^{h c / \lambda_{1} k T}-1\right)}$ <br> Insert the data and evaluate this ratio. <br> Answer With $\lambda_{1}=450 \mathrm{~nm}$ and $\lambda_{2}=700 \mathrm{~nm}, \ldots$, and therefore $\frac{\rho(450 \mathrm{~nm}, 298 \mathrm{~K})}{\rho(700 \mathrm{~nm}, 298 \mathrm{~K})}=2.10 \times 10^{-16}$ <br> Final Answer: $2.10 \times 10^{-16}$</td>
<td style="text-align: center;">```GPT Solution with CoT Prompting Step 1: We need to use the Planck distribution formula to calculate the energy output of a black-body radiator. The formula is: $$</td>
</tr>
<tr>
<td style="text-align: center;">B(\lambda, T)=\frac{2 \delta \mathrm{e}^{2}}{k^{2}} \frac{1}{e^{\lambda 2} \delta \mathrm{e}^{2}-1}</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$$ Step 2: We calculate the energy output at 450 nm and 700 $\mathrm{nm} .$. Let's use $T=298 \mathrm{~K}$. $$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">B(450 \mathrm{~nm}, 298 \mathrm{~K}) $$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; =\frac{2 \delta \mathrm{e}^{2}}{\left(450 \times 10^{-9} \mathrm{~m}\right)^{3}} \frac{1}{e^{203 / 10^{-9} \mathrm{~K}+p} 298 \mathrm{~K}}-1 \ &amp; \approx 1.23 \times 10^{-16}=1.67 \times 10^{-51} \end{aligned}$ <br> Step 3: Calculate the ratio of the energy output at 450 nm to that at 700 nm : $\frac{B(450 \mathrm{~nm}, 298 \mathrm{~K})}{B(700 \mathrm{~nm}, 298 \mathrm{~K})} \approx 4.39 \ \text { Final Answer: } 4.39 \times 10^{-16} \end{aligned}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT Solution with Python Prompting as External Tools <br> ... The ratio of the energy output at $\lambda_{1}=450 \mathrm{~nm}$ and $\lambda_{2}$ $=700 \mathrm{~nm}$ is $\frac{u\left(\lambda_{1}, T\right)}{u\left(\lambda_{2}, T\right)}=\frac{\delta \mathrm{e}^{\lambda_{2}}}{k^{2}} \frac{e^{h c / 1 / \lambda_{1} k T}-1}{k^{2}} \frac{\lambda_{2}^{2} e^{h c /(2 k) k T)}-1}{\lambda_{1}^{2} e^{h c /(2 k) k T}-1} \quad \frac{\lambda_{2}^{2} e^{h c /(2 k) k T)}-1}{\lambda_{1}^{2} e^{h c /(2 k) k T)}-1}$ <br> Substituting the given values, we get a Python code for this problem: <br> $\sim$ <br> ratio $<em>$ ((lambda2 / lambda1)</em><em>5) <br> $</em>$ ((math.exp((h * c) / (lambda2 * k * <br> ?)) - 1) / (math.exp((h * c) / (lambda2 * k * ?)) - 1)) <br> $=$ lambda1 <br> $=$ <br> Final Answer: $3.95 \times 10^{61}$</td>
</tr>
<tr>
<td style="text-align: center;">Figure 1. An example problem from Physical Chemistry with solutions generated under two prompting strategies. GPT-4 with Chain-ofThought (CoT) prompting shows calculation errors, while GPT-4 that prompts Python as external tools misunderstands mathematical equations. Errors are highlighted in red and the corrections are shown in purple.</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>lems. Consider an example problem from college-level Physical Chemistry (Atkins et al., 2014b) that requires the use of the Planck distribution to derive certain quantities. As shown in Figure 1, LLMs with CoT prompts accurately generate the correct formula, but fail in the final numerical calculation. As a remedy, when instructed to simultaneously generate a Python program for numerical computation and employ the CoT reasoning, the LLM misplaces $\lambda_{1}$ in the numerator rather than the denominator in the formula, illustrating a misunderstanding of mathematical relationships when employing external tools. This example highlights a crucial gap: even advanced LLMs struggle with complex scientific problem solving, necessitating a fine-grained analysis of the skills required for such complex tasks.</p>
<p>To mitigate these deficiencies, in this paper, we present a novel college-level Scientific problem solving Benchmark, referred to as SCIBENCH. SCIBENCH contains a carefully curated dataset of college-level scientific problems, including 869 problems collected from widely-used textbooks in college-level Chemistry, Physics, and Mathematics courses. Distinct from existing benchmarks, all of the problems are open-ended, free-response questions that demand multi-step reasoning abilities, the understanding of scientific concepts, the retrieval of domain-specific knowledge (e.g., equations and theorems), and complex numeric computation capabilities (e.g., calculus or differential equations). Besides that, our dataset includes a multimodal subset of 177 problems that incorporate visual elements (such as graphs and figures) as additional contexts, which enables of the evaluation of multimodal LLMs. It is noted that SCIBENCH also includes step-by-step solutions for example problems, facilitating detailed error analysis. To align our evaluation with real-
world scenarios, we provide a separate, closed dataset that encompasses 103 problems from seven sets of midterm and final exams from collegiate Computer Science and Math courses. To ensure the integrity of our evaluation, these datasets have been manually extracted from PDF documents and formatted into LaTeX documents, thereby minimizing the risk of their leakage in LLM training data.</p>
<p>Our evaluation includes a wide range of representative opensource and proprietary LLMs. For unimodal, textual-based LLMs, we assess LLaMA-2, Mistral, Claude2, GPT-3.5, GPT-4, and their variants. For multimodal vision-language models, we include GPT-4, InternLM-XComposer2, QwenVL, SPHINX-MoE, and LLaVA. These models are tested using various prompting strategies, including CoT, zero-shot learning, and few-shot learning. We also prompt LLMs to utilize external scientific computing libraries in Python and Wolfram language. The experimental results indicate that the complexity and difficulty of our dataset are sufficient to differentiate the performance levels of different LLMs. Even with the strongest configuration-combining CoT prompting and the use of external tools-the best model achieves an average score of $43.22 \%$ on the textual dataset, $13.8 \%$ on the multimodal dataset, and $51.57 \%$ on the closed exam dataset. These results suggest a considerable potential for improvement in future LLMs.</p>
<p>In order to gain a comprehensive understanding of the limitations of LLMs in scientific problem solving, we propose a novel self-refinement method to uncover the deficient skills in the solutions made by LLMs. Firstly, we compare the correct solutions with the solutions generated by LLMs and, with the assistance of human annotators, summarize ten essential skills requisite for successful scientific</p>
<p>Table 1. Comparison of SCIBENCH with other benchmarks. "Algebra" refers to high-school level arithmetic computations; "Calculus" involves using integrals and differentials; "Statistics" focuses on applying statistical and probability concepts like bivariate distributions.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Benchmark</th>
<th style="text-align: center;">Subject</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Calculation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">College <br> Level</th>
<th style="text-align: center;">Visual <br> Contexts</th>
<th style="text-align: center;">Detailed <br> Solutions</th>
<th style="text-align: center;">Free <br> Response</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">Chemistry</td>
<td style="text-align: center;">Physics</td>
<td style="text-align: center;">Algebra</td>
<td style="text-align: center;">Calculus</td>
<td style="text-align: center;">Statistics</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ScienceQA (Lu et al., 2022)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">IconQA (Lu et al., 2021b)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TabMWP (Lu et al., 2023c)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K (Cobbe et al., 2021)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">MATH (Hendrycks et al., 2021)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">LILA (Mishra et al., 2022)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">MMLU (Hendrycks et al., 2020)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TheroemQA (Chen et al., 2023b)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">AGIEval (Zhong et al., 2023)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SciEval (Sun et al., 2023)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">JEEBench (Arora et al., 2023)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">SCIBENCH</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>problem-solving. These skills include proficiency in domain knowledge, mathematical reasoning, numerical calculation abilities, and comprehension of common sense concepts. Subsequently, we employ an LLM-empowered self-critic approach to automatically classify the lacking skills in the solutions made by the benchmarked LLMs under each experiment configuration. Our analysis finds that (1) although CoT significantly improves the calculation ability, it is less effective in other aspects; (2) prompts with the use of external tools could potentially compromise other fundamental skills; (3) few-shot learning does not universally improve scientific problem-solving skills.</p>
<h2>2. Related Work</h2>
<p>Recently, many benchmarks have been proposed to assess the scientific problem-solving skills of LLMs, particularly in mathematical domains (Chen et al., 2023b; Fu et al., 2023; Guo et al., 2023; Hendrycks et al., 2020; Lu et al., 2023c;d; Mishra et al., 2022; Welleck et al., 2021; Zhong et al., 2023). Notable works include GSM8K (Cobbe et al., 2021) including 8.5 K grade school math word problems; LILA (Mishra et al., 2022) which extends 20 datasets with task instructions and Python solutions; MATH (Hendrycks et al., 2021), a challenging collection of 12.5 K math problems from math competitions; TheroemQA (Chen et al., 2023b), focusing on theorem applications on problem solving; and MathVista (Lu et al., 2023a), which evaluates the mathematical reasoning ability of LLMs in visual contexts.</p>
<p>To provide a more holistic evaluation, recent studies have expanded their scope to multiple disciplines: ScienceQA (Lu et al., 2022) introduces a multimodal question-answering dataset with accompanying lecture notes and explanatory annotations. Taylor et al. (2022) provide a set of scientific tasks, including LaTeX equation conversions, domain knowledge probes, citation prediction, and chemical ques-
tion answering. BIG-Bench (Ghazal et al., 2013) offers a large-scale general-purpose test suite that requires 204 multiple-choice or exact-match tasks, and its extension BIGBench Hard (Suzgun et al., 2022) poses challenging CoT prompts. SciEval (Sun et al., 2023) includes a mix of objective and subjective questions across multiple scientific fields to assess understanding, application, and research capabilities. JEEBench (Arora et al., 2023) incorporates pre-engineering-level scientific problems derived from college entrance exams. AGIEval (Zhong et al., 2023) evaluates LLMs on human-centric standardized exams, such as college entrance exams and lawyer qualification tests.</p>
<p>Despite their extensive coverage across diverse disciplines, these datasets exhibit certain limitations. Sourced from lower educational level subjects, the majority of them focus on basic arithmetic operations rather than advanced mathematical computations. Furthermore, most of these benchmarks are confined to textual-only problems, omitting problems with visual elements such as graphs or diagrams. These drawbacks result in an incomplete assessment of the analytical and problem-solving skills required to tackle complex scientific problems. In contrast, SCIBENCH focuses on college-level scientific problems across a broad spectrum of disciplines including Mathematics, Physics, and Chemistry. It emphasizes on a deep understanding of diverse scientific concepts, challenging LLMs to not only grasp these principles but also to efficiently retrieve and apply relevant knowledge. Furthermore, it demands sophisticated numerical computation skills, including the execution of advanced mathematical operations such as calculus and differential equations, as well as the application of advanced statistical and probability theories. Additionally, we include multimodal problems that necessitate the interpretation and integration of both textual and visual information. A detailed comparison of SCIBENCH with some representative works is summarized in Table 1.</p>
<p>Table 2. Summary of the textbook dataset. We report the number of total problems, percentage with detailed solutions, and percentage with visual elements in columns four to six respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Subject</th>
<th style="text-align: center;">Title</th>
<th style="text-align: center;">Acronym</th>
<th style="text-align: center;"># Problems</th>
<th style="text-align: center;">\% Solutions</th>
<th style="text-align: center;">\% Visual</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Physics</td>
<td style="text-align: center;">Fundamentals of Physics (Halliday et al., 2013)</td>
<td style="text-align: center;">fund</td>
<td style="text-align: center;">142</td>
<td style="text-align: center;">$9.2 \%$</td>
<td style="text-align: center;">$43.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Statistical Thermodynamics (Engel \&amp; Reid, 2010)</td>
<td style="text-align: center;">thermo</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">$20.5 \%$</td>
<td style="text-align: center;">$0.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Classical Dynamics of Particles and Systems (Thornton \&amp; Marion, 2021)</td>
<td style="text-align: center;">class</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">$12.1 \%$</td>
<td style="text-align: center;">$4.5 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Chemistry</td>
<td style="text-align: center;">Quantum Chemistry (Levine et al., 2009)</td>
<td style="text-align: center;">quan</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">$19.5 \%$</td>
<td style="text-align: center;">$0.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Quantum Chemistry (McQuarrie, 2008)</td>
<td style="text-align: center;">chemmc</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">$19.1 \%$</td>
<td style="text-align: center;">$0.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Physical Chemistry (Atkins et al., 2014a)</td>
<td style="text-align: center;">atkins</td>
<td style="text-align: center;">122</td>
<td style="text-align: center;">$13.9 \%$</td>
<td style="text-align: center;">$0.8 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Physical Chemistry, Quanta, Matter, and Change (Atkins et al., 2014b)</td>
<td style="text-align: center;">matter</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">$16.9 \%$</td>
<td style="text-align: center;">$3.4 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">Calculus: Early Transcendentals (Stewart et al., 2012)</td>
<td style="text-align: center;">calc</td>
<td style="text-align: center;">161</td>
<td style="text-align: center;">$19.3 \%$</td>
<td style="text-align: center;">$67.7 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Probability and Statistical Inference (Hogg et al., 1977)</td>
<td style="text-align: center;">stat</td>
<td style="text-align: center;">93</td>
<td style="text-align: center;">$21.5 \%$</td>
<td style="text-align: center;">$1.1 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Elementary Differential Equations and Boundary Value Problems (Boyce et al., 2021)</td>
<td style="text-align: center;">diff</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">$9.1 \%$</td>
<td style="text-align: center;">$0.0 \%$</td>
</tr>
</tbody>
</table>
<p>While the aforementioned datasets focus on evaluating LLMs’ performance on scientific problem solving tasks, another line of research aims to analyze the diverse capabilities of LLMs more comprehensively. Liu et al. (2023c) assess the reading abilities of LLMs using multiple-choice questions. Frieder et al. (2023) focus on evaluating the mathematical capabilities of LLMs, including those at the college level, but with topics such as functional analysis or topology that differ from those in SCIBENCH, such as differential equations and calculus. Bubeck et al. (2023) explore the comprehensive abilities of GPT-4, but only use up to high-school level mathematical problems such as those in GSM8k (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021). Zhang et al. (2024) develop SciGLM, a scientific language model for collegiate-level problem reasoning, and evaluate its performance across multiple scientific datasets. Kabir et al. (2023) conduct a detailed manual analysis for LLMs. They also provide human-annotated qualitative analysis to assess the capabilities of the models. However, relying on human labor for direct solution analysis can be costly. Our evaluation protocol, based on predefined fundamental problem solving skills, enables automated classification of deficient skills for each incorrectly answered question. This approach enables an affordable, large-scale qualitative analysis of model solutions.</p>
<h2>3. The SCIBENCH Dataset</h2>
<p>To evaluate the capabilities and analyze the limitations of Large Language Models (LLMs) to solve scientific computing problems, we collect a new dataset consisting of college-level textbooks and course exams in a variety of domains. This section details the dataset construction process.</p>
<p>Data selection criteria. Our dataset aims to improve the previous benchmarks by including more challenging problems. Specifically, the selected dataset should fulfill the following requirements:</p>
<ul>
<li>Inclusion of college-level problems. The chosen problems demand a solid understanding of domain-specific
knowledge, adept calculation skills, and the ability to perform complex numerical computations.</li>
<li>Inclusion of detailed solutions. To facilitate a thorough analysis of the limitations of LLMs, detailed solutions should be provided as well, which could facilitate a finergrained examination of the capacity of LLMs to handle complex problem-solving tasks.</li>
<li>Inclusion of visual elements. In the real world, many scientific problems require the interpretation and integration of both textual and visual information. The included problems should thus contain visual elements (such as figures) in the contexts.</li>
<li>Inaccessibility in text formats. To ensure an unbiased evaluation, questions should not be readily accessible online and cannot be easily extracted or transformed into text. This aims to mitigate any potential information leakage from the exposure of LLMs to pre-existing online question banks, such as those found in standardized tests like the SAT exams.</li>
<li>Assessment of advanced problem-solving capabilities. The problems to benchmark should not be confined to basic arithmetic operations like addition and multiplication. Rather, they should enable evaluating the capability of LLMs in performing advanced computations such as calculus and differential equations.</li>
</ul>
<p>Accordingly, to construct the dataset, we select ten textbooks from three scientific fields Physics, Chemistry, and Mathematics that have been extensively used in college courses. We summarize the statistics of this textbook dataset in Table 2 and we use acronyms to refer to each textbook throughout the paper for brevity. Furthermore, in order to simulate real-world evaluation, we compile a closed set of exam questions from college courses from Computer Science and Math departments, including Data Mining, Machine Learning, and Differential Equations. This subset is less likely to be in LLM training data, making it an effective tool for LLM evaluation. Detailed statistics of these exam problems are summarized in Table S1. We refer readers to Appendix A for details on these textbooks and exams.</p>
<p>To reduce the likelihood of correct answers being merely guessed from candidates, we choose to mainly include questions with more challenging, free-response answers, rather than multiple-choice questions in previous works (Chen et al., 2023b; Lu et al., 2021a; 2022). In order to facilitate standardized and automated evaluation, we focus on answers that only contain single numerical numbers to avoid ambiguity for the textbook dataset. Further, we convert the answer to floating-point numbers rounded to three decimal places. For example, the answer $\frac{\sqrt{2}}{2}$ will be converted to the decimal representation of 0.450 . We also treat scientific notation as a unit to avoid overflow issues. For example, if the answer is $2.2 \times 10^{-31} \mathrm{~m}$, we take 2.2 as the final answer and $10^{-31} \mathrm{~m}$ as the unit.</p>
<p>Data preprocessing. We collect each problem from the original textbooks in PDF documents and manually process them into LaTeX documents using an OCR tool Mathpix. The data is manually collected by human annotators using a web-based annotation tool (Lu et al., 2021a), whose user interface is shown in Appendix A.3. All problems are carefully verified by human annotators to ensure that LaTeX documents can be compiled without any syntax errors. For reference, we also provide the original numbers in textbooks. For every problem, we provide the answer in two forms: the numerical value and the corresponding LaTeX expression with mathematical notations retained (e.g., 0.450 and $\frac{\sqrt{2}}{2}$ ); the unit of each answer is saved as a separate attribute. The detailed step-by-step solutions are also provided in LaTeX. For problems having multiple answers, we either keep only the first subproblem and discard the remaining subproblems or convert each subproblem into a separate problem.</p>
<h2>4. Experiments</h2>
<p>This section presents the experiments to assess the capabilities of LLMs in scientific problem-solving. We first describe our experimental setup. Subsequently, we evaluate unimodal LLMs on the textbook dataset. Following this, we include additional experiments on the multimodal subset and the closed exam subset, as well as comparisons with other numerical computational tools.</p>
<h3>4.1. Experiment Setup</h3>
<p>We evaluate the textbook dataset on seven unimodal LLMs, which include four proprietary models: Claude2 (claude2) (Anthropic., 2023), GPT-3.5-Turbo (gpt-3.5-turbo) (OpenAI., 2022), GPT-4 (gpt-4), GPT-4Turbo (gpt-4-turbo) (OpenAI., 2023), along with three open-source models: LLaMA-2-7B (llama-2-7b-chat), LLaMA-2-70B (llama-2-70b-chat) (Touvron et al., 2023b), and Mistral-7B (mistral-7b-instruct) (Jiang et al., 2023).</p>
<p>We consider two prompting strategies, including the Chain-of-Thought (CoT) prompting and prompting to use external tools.</p>
<ul>
<li>Zero-shot and few-shot learning. In the zero-shot learning setting, models are not provided with any prior examples, which evaluates their inherent problem-solving capabilities with background knowledge and reasoning abilities. In the few-shot setting, a few examples are given to the models before the test example. This aims to assess their capability to learn new information from the demonstrations and incorporate it into their problem-solving processes.</li>
<li>Prompting-based approaches. For our experiments, all settings begin with a system prompt that describes the types and categories of questions. Additionally, we utilize a CoT prompting strategy in zero- and few-shot settings.</li>
<li>Tool-augmented approaches. Given that LLMs are limited in acquiring exact knowledge and performing precise calculations, some recent approaches, such as PAL (Gao et al., 2022) and PoT (Chen et al., 2023a) explore utilizing external tools such as the Python interpreter for program synthesis to enhance the capabilities of solving complex reasoning tasks. In line with these approaches and acknowledging the limitations of LLMs in performing precise calculations, we also include a setting that prompts the model to convert its solution steps in natural language into Python code, aiming to achieve more accurate results for certain computation steps. This toolaugmented approach can only be tested in the few-shot learning setting. We manually construct Python programs that produce the correct answer.</li>
</ul>
<p>Implementation details. We set temperature to zero for all models to reduce the randomness of the predictions. Fewshot examples, including solutions, are randomly selected from problems within each textbook. When external tools are used, we add a code snippet that translates the solution into specific programming languages in all few-shot examples. The code snippets are verified by human annotators that will produce the correct output. In terms of evaluation metrics, we compare the model outputs with the correct answers, allowing a relative tolerance of 5\%. In particular to the exam dataset, the model solutions are graded using the rubrics provided by the instructors. Readers may refer to Appendix C for all prompts and the implementation details for utilizing external tools.</p>
<h3>4.2. Results and Analysis</h3>
<p>We report the model performance in terms of accuracy score for each textbook and an average score over all problems. The results of all LLMs in various settings on the textbook and the exam dataset are summarized in Tables 3 and S2 respectively. We have the following observations.</p>
<p>Table 3. Experimental results in terms of accuracy (\%) on the textbook dataset. The best performing score is highlighted in bold and second-best is underlined. The average score is weighted by the number of problems in each textbook.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Chemistry</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Physics</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Math</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">atkins</td>
<td style="text-align: center;">chemmc</td>
<td style="text-align: center;">quan</td>
<td style="text-align: center;">matter</td>
<td style="text-align: center;">fund</td>
<td style="text-align: center;">class</td>
<td style="text-align: center;">thermo</td>
<td style="text-align: center;">diff</td>
<td style="text-align: center;">stat</td>
<td style="text-align: center;">calc</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Zero-Shot Learning</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2-7B</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1.37</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">2.00</td>
<td style="text-align: center;">5.33</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1.03</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2-70B</td>
<td style="text-align: center;">1.87</td>
<td style="text-align: center;">2.56</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1.40</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">10.70</td>
<td style="text-align: center;">4.76</td>
<td style="text-align: center;">2.41</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B</td>
<td style="text-align: center;">9.35</td>
<td style="text-align: center;">5.13</td>
<td style="text-align: center;">8.82</td>
<td style="text-align: center;">4.08</td>
<td style="text-align: center;">5.48</td>
<td style="text-align: center;">2.13</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">4.00</td>
<td style="text-align: center;">12.00</td>
<td style="text-align: center;">2.38</td>
<td style="text-align: center;">6.23</td>
</tr>
<tr>
<td style="text-align: center;">Claude2</td>
<td style="text-align: center;">15.00</td>
<td style="text-align: center;">12.83</td>
<td style="text-align: center;">14.71</td>
<td style="text-align: center;">10.20</td>
<td style="text-align: center;">12.33</td>
<td style="text-align: center;">6.40</td>
<td style="text-align: center;">9.00</td>
<td style="text-align: center;">4.00</td>
<td style="text-align: center;">38.70</td>
<td style="text-align: center;">16.70</td>
<td style="text-align: center;">14.94</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-Turbo</td>
<td style="text-align: center;">4.67</td>
<td style="text-align: center;">20.51</td>
<td style="text-align: center;">8.82</td>
<td style="text-align: center;">2.04</td>
<td style="text-align: center;">10.96</td>
<td style="text-align: center;">2.13</td>
<td style="text-align: center;">2.94</td>
<td style="text-align: center;">6.00</td>
<td style="text-align: center;">28.00</td>
<td style="text-align: center;">9.30</td>
<td style="text-align: center;">9.59</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">45.79</td>
<td style="text-align: center;">28.21</td>
<td style="text-align: center;">26.47</td>
<td style="text-align: center;">22.45</td>
<td style="text-align: center;">23.29</td>
<td style="text-align: center;">25.53</td>
<td style="text-align: center;">17.91</td>
<td style="text-align: center;">32.00</td>
<td style="text-align: center;">49.33</td>
<td style="text-align: center;">54.76</td>
<td style="text-align: center;">33.79</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4-Turbo</td>
<td style="text-align: center;">57.01</td>
<td style="text-align: center;">41.03</td>
<td style="text-align: center;">35.29</td>
<td style="text-align: center;">26.53</td>
<td style="text-align: center;">24.66</td>
<td style="text-align: center;">21.28</td>
<td style="text-align: center;">26.87</td>
<td style="text-align: center;">46.00</td>
<td style="text-align: center;">61.33</td>
<td style="text-align: center;">52.38</td>
<td style="text-align: center;">40.99</td>
</tr>
<tr>
<td style="text-align: center;">Zero-Shot Learning + CoT Prompting</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2-7B</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">2.56</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">4.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2-70B</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">2.56</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1.49</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">10.70</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1.89</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B</td>
<td style="text-align: center;">6.54</td>
<td style="text-align: center;">5.13</td>
<td style="text-align: center;">2.94</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">2.12</td>
<td style="text-align: center;">1.49</td>
<td style="text-align: center;">6.00</td>
<td style="text-align: center;">10.67</td>
<td style="text-align: center;">9.52</td>
<td style="text-align: center;">4.63</td>
</tr>
<tr>
<td style="text-align: center;">Claude2</td>
<td style="text-align: center;">20.56</td>
<td style="text-align: center;">15.38</td>
<td style="text-align: center;">8.82</td>
<td style="text-align: center;">4.08</td>
<td style="text-align: center;">8.23</td>
<td style="text-align: center;">4.26</td>
<td style="text-align: center;">5.97</td>
<td style="text-align: center;">6.00</td>
<td style="text-align: center;">36.00</td>
<td style="text-align: center;">14.29</td>
<td style="text-align: center;">13.89</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-Turbo</td>
<td style="text-align: center;">6.54</td>
<td style="text-align: center;">23.08</td>
<td style="text-align: center;">2.94</td>
<td style="text-align: center;">10.20</td>
<td style="text-align: center;">12.33</td>
<td style="text-align: center;">2.12</td>
<td style="text-align: center;">5.97</td>
<td style="text-align: center;">12.00</td>
<td style="text-align: center;">33.33</td>
<td style="text-align: center;">9.30</td>
<td style="text-align: center;">12.17</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">28.04</td>
<td style="text-align: center;">43.59</td>
<td style="text-align: center;">14.71</td>
<td style="text-align: center;">20.41</td>
<td style="text-align: center;">21.92</td>
<td style="text-align: center;">19.15</td>
<td style="text-align: center;">17.91</td>
<td style="text-align: center;">22.00</td>
<td style="text-align: center;">50.67</td>
<td style="text-align: center;">42.86</td>
<td style="text-align: center;">28.52</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4-Turbo</td>
<td style="text-align: center;">60.75</td>
<td style="text-align: center;">35.90</td>
<td style="text-align: center;">29.41</td>
<td style="text-align: center;">28.57</td>
<td style="text-align: center;">30.14</td>
<td style="text-align: center;">31.91</td>
<td style="text-align: center;">25.37</td>
<td style="text-align: center;">38.00</td>
<td style="text-align: center;">64.00</td>
<td style="text-align: center;">54.76</td>
<td style="text-align: center;">42.37</td>
</tr>
<tr>
<td style="text-align: center;">Few-Shot Learning + CoT Prompting</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2-7B</td>
<td style="text-align: center;">1.87</td>
<td style="text-align: center;">5.13</td>
<td style="text-align: center;">2.94</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">5.48</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">12.00</td>
<td style="text-align: center;">7.14</td>
<td style="text-align: center;">3.60</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2-70B</td>
<td style="text-align: center;">13.10</td>
<td style="text-align: center;">12.83</td>
<td style="text-align: center;">14.71</td>
<td style="text-align: center;">4.08</td>
<td style="text-align: center;">12.33</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">13.30</td>
<td style="text-align: center;">9.52</td>
<td style="text-align: center;">8.40</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B</td>
<td style="text-align: center;">6.54</td>
<td style="text-align: center;">10.26</td>
<td style="text-align: center;">2.94</td>
<td style="text-align: center;">2.04</td>
<td style="text-align: center;">2.74</td>
<td style="text-align: center;">2.13</td>
<td style="text-align: center;">4.48</td>
<td style="text-align: center;">4.00</td>
<td style="text-align: center;">14.67</td>
<td style="text-align: center;">9.52</td>
<td style="text-align: center;">6.17</td>
</tr>
<tr>
<td style="text-align: center;">Claude2</td>
<td style="text-align: center;">15.89</td>
<td style="text-align: center;">25.64</td>
<td style="text-align: center;">14.65</td>
<td style="text-align: center;">6.12</td>
<td style="text-align: center;">9.59</td>
<td style="text-align: center;">6.38</td>
<td style="text-align: center;">10.45</td>
<td style="text-align: center;">8.00</td>
<td style="text-align: center;">33.33</td>
<td style="text-align: center;">19.05</td>
<td style="text-align: center;">15.26</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-Turbo</td>
<td style="text-align: center;">8.41</td>
<td style="text-align: center;">20.51</td>
<td style="text-align: center;">8.82</td>
<td style="text-align: center;">6.12</td>
<td style="text-align: center;">10.96</td>
<td style="text-align: center;">2.12</td>
<td style="text-align: center;">1.49</td>
<td style="text-align: center;">10.00</td>
<td style="text-align: center;">38.67</td>
<td style="text-align: center;">6.98</td>
<td style="text-align: center;">11.99</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">41.12</td>
<td style="text-align: center;">33.33</td>
<td style="text-align: center;">17.65</td>
<td style="text-align: center;">16.33</td>
<td style="text-align: center;">17.81</td>
<td style="text-align: center;">17.02</td>
<td style="text-align: center;">20.90</td>
<td style="text-align: center;">30.00</td>
<td style="text-align: center;">49.33</td>
<td style="text-align: center;">45.24</td>
<td style="text-align: center;">30.36</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4-Turbo</td>
<td style="text-align: center;">59.81</td>
<td style="text-align: center;">35.90</td>
<td style="text-align: center;">26.47</td>
<td style="text-align: center;">18.37</td>
<td style="text-align: center;">23.29</td>
<td style="text-align: center;">19.15</td>
<td style="text-align: center;">32.84</td>
<td style="text-align: center;">32.00</td>
<td style="text-align: center;">65.33</td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;">39.45</td>
</tr>
<tr>
<td style="text-align: center;">Few-Shot Learning + Python</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2-7B</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">2.56</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">6.67</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1.20</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2-70B</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">7.69</td>
<td style="text-align: center;">2.94</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">9.59</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1.49</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">17.30</td>
<td style="text-align: center;">9.52</td>
<td style="text-align: center;">5.14</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B</td>
<td style="text-align: center;">4.67</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">5.88</td>
<td style="text-align: center;">2.04</td>
<td style="text-align: center;">2.74</td>
<td style="text-align: center;">2.13</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">4.00</td>
<td style="text-align: center;">17.33</td>
<td style="text-align: center;">11.90</td>
<td style="text-align: center;">5.32</td>
</tr>
<tr>
<td style="text-align: center;">Claude2</td>
<td style="text-align: center;">6.54</td>
<td style="text-align: center;">12.82</td>
<td style="text-align: center;">14.71</td>
<td style="text-align: center;">4.08</td>
<td style="text-align: center;">17.81</td>
<td style="text-align: center;">8.51</td>
<td style="text-align: center;">5.97</td>
<td style="text-align: center;">20.00</td>
<td style="text-align: center;">40.00</td>
<td style="text-align: center;">16.67</td>
<td style="text-align: center;">14.92</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-Turbo</td>
<td style="text-align: center;">13.08</td>
<td style="text-align: center;">33.33</td>
<td style="text-align: center;">8.82</td>
<td style="text-align: center;">16.33</td>
<td style="text-align: center;">26.01</td>
<td style="text-align: center;">4.26</td>
<td style="text-align: center;">7.46</td>
<td style="text-align: center;">16.00</td>
<td style="text-align: center;">44.00</td>
<td style="text-align: center;">26.19</td>
<td style="text-align: center;">19.91</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">57.01</td>
<td style="text-align: center;">38.46</td>
<td style="text-align: center;">44.12</td>
<td style="text-align: center;">34.69</td>
<td style="text-align: center;">28.77</td>
<td style="text-align: center;">23.40</td>
<td style="text-align: center;">34.33</td>
<td style="text-align: center;">44.00</td>
<td style="text-align: center;">68.00</td>
<td style="text-align: center;">38.10</td>
<td style="text-align: center;">43.22</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4-Turbo</td>
<td style="text-align: center;">32.71</td>
<td style="text-align: center;">33.33</td>
<td style="text-align: center;">17.65</td>
<td style="text-align: center;">26.53</td>
<td style="text-align: center;">27.40</td>
<td style="text-align: center;">12.76</td>
<td style="text-align: center;">16.42</td>
<td style="text-align: center;">34.00</td>
<td style="text-align: center;">42.67</td>
<td style="text-align: center;">30.95</td>
<td style="text-align: center;">28.47</td>
</tr>
</tbody>
</table>
<ul>
<li>Observation 1. SCIBENCH is complex enough to differentiate among LLMs. Our results show that open-source models such as LLaMA-2 and Mistral are consistently outperformed by their proprietary counterparts across all settings within the textbook dataset. Notably, GPT-4 and GPT-4-Turbo lead in performance by a significant margin. For example, GPT-4-Turbo outperforms Mistral-7B by $34.76 \%$ in the zero-shot setting. Additionally, within both LLaMA and GPT series, we observe a clear correlation between increased model capacity (i.e., larger parameter sizes) and improved performance. Therefore, the complexity of SCIBENCH is able to differentiate the performance among different LLMs.</li>
<li>Observation 2. SCIBENCH highlights varied efficacy of prompting strategies across LLMs. Our findings suggest that the effectiveness of employing prompting strategies or external computational tools varies signif-
icantly among different LLMs. As shown in the table, LLaMA-2-70B shows a marked improvement in the fewshot setting over the zero-shot setting, increasing from $2.41 \%$ to $8.40 \%$. Similarly, the performance of GPT-4 is significantly improved when incorporating external tools, with an increase from $30.36 \%$ to $43.22 \%$. Meanwhile, the up-to-date model GPT-4-Turbo exhibits superior performance in zero-shot learning settings. However, despite its advanced capabilities demonstrated by its outstanding zero-shot learning performance, it falls short compared to GPT-4 in few-shot learning when leveraging Python for numerical computation. This suggests a potential reduction in its program understanding capabilities. In summary, such findings illustrate SCIBENCH can reveal the nuanced differences in the ability of LLMs to utilize prompting strategies and external tools effectively.</li>
</ul>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2. Performance of LLMs on the multimodal subset. GPT-4 models are augmented with image captions and OCR text.</p>
<h3>4.3. Additional Experiments</h3>
<p>Evaluation on the multimodal subset. We evaluate two categories of models on problems with visual contexts: (1) GPT-4 (OpenAI., 2023) augmented with image captions from Multimodal Bard (Google, 2023) and OCR texts from EasyOCR (JaidedAI, 2022) and (2) open-source Large Multimodal Models (LMMs): InternLM-XComposer2VL (Dong et al., 2024), Qwen-VL-Plus (Bai et al., 2023), SPHINX-MoE (Lin et al., 2023), and LLaVA-LLaMA-213B (Liu et al., 2023a). For GPT-4, we explore two prompting strategies: Chain-of-Thought (CoT) (Wei et al., 2022) and Program-of-Thoughts (PoT) (Chen et al., 2023a). The results presented in Figure 2 reveal that proprietary models augmented with image captions and OCR-detected text, significantly outperform their open-source counterparts. GPT-4 (PoT) that combines programming capabilities achieves an accuracy of $13.8 \%$, markedly higher than $7.4 \%$ obtained by the best open model LLaVA-LLaMA-2-13B. This demonstrates the substantial potential for LLMs to effectively utilize visual contexts in scientific problem solving.</p>
<p>Evaluation on the exam subset. To mirror real-world testing conditions with no few-shot examples provided, we evaluate GPT-3.5, GPT-4, Claude, LLaMA-2-7B, and LLaMA-2-70B on the closed exam dataset under zero-shot and zeroshot CoT settings. The experiment results summarized in Table S2 indicate a notable performance advantage of GPT4, which achieves an averaged score of $57.54 \%$. However, we note that their performance remains significantly lower than human benchmarking. For instance, in the Data Mining course, GPT-4 scores $64.44 \%$ and $42.67 \%$ in the midterm and final exams, lower than the average student scores of $80.18 \%$ and $72.71 \%$, respectively, as reported by the course instructor. The results once again underline the challenging nature of our dataset.</p>
<p>Comparison with other scientific computing tools. We further utilize another famous scientific computing library Wolfram Language as the external tool and conduct experiments using GPT-3.5, Claude, LLaMA-2-7B, and LLaMA-2-70B. The experiment results reported in Figure S7 show
that utilizing Wolfram Language does not help few-shot learning and even results in a deteriorated performance, with a decrease of $6.70 \%$ compared to the CoT prompting for Claude2, and a decrease of $6.17 \%$ for LLaMA-2-70B. A plausible explanation is the introduction of syntax errors when translating solution steps into the Wolfram Language, which could be a potential direction for improvement. For a detailed error analysis, readers are directed to Appendix C.3.</p>
<h2>5. Error Analysis of Prompting Strategies</h2>
<p>Considering the substantial advancements of current LLMs, an in-depth analysis of the particular skills that are either enhanced or limited under certain settings becomes imperative. Previous works have relied on human labor to annotate error reasons into different categories, which is both expensive and time-consuming (Zhong et al., 2023). In this section, we present an evaluation protocol that automates the classification of error reasons into deficient skills. This time-efficient approach enables large-scale analyses in future research.</p>
<p>In order to quantify the impact of each setting on scientific problem-solving, we first define an essential skill set that is required by solving scientific problems. Then, an LLM verifier is employed to automatically classify each incorrectly solved problem based on the absence of a specific skill from the essential skill set. This approach generates error profiles, showcasing a direct comparison of different strategies. This evaluation protocol is summarized in Figure 3.</p>
<p>Firstly, we analyze the incorrect solutions made by GPT-3.5 for problems that provide detailed solutions. We hire two college students, who are highly familiar with the problems in our datasets, to annotate the source of the error for each problem, indicating the specific line where the model makes a mistake and why. From 112 such error annotations and with the assistance of GPT-4, we distill these errors into ten essential skills that GPT-3.5 might lack:</p>
<ul>
<li>Logical decomposition and analysis skills. This ability involves decomposing the problem into smaller, manageable parts, and understanding the relationships between these parts.</li>
<li>Assumption identification. This skill involves the ability to recognize relevant and necessary assumptions in the problem.</li>
<li>Spatial perception. This is important for understanding problems in areas such as Physics and Chemistry, where models need to visualize molecules, forces, fields, etc.</li>
<li>Causal reasoning. This is the ability to understand cause and effect relationships.</li>
<li>Problem deduction skills. This pertains to the ability to infer and deduce potential solutions or underlying principles from the given information in a problem.</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<h1>Datasets</h1>
<h2>Evaluation</h2>
<p>Figure 3. Pipeline of the evaluation protocol. The evaluation protocol involves analyzing both LLMs and reference (correct) solutions with the assistance of human annotators to identify error reasons. These reasons are then summarized into ten essential scientific problem-solving skills in which LLM may face challenges. Subsequently, a LLM verifier is employed to automatically attribute each incorrectly answered problem to a lack of a specific skill. The resulting error profiles enable the interpretation of the improved skills by certain prompting strategies and the direct comparison of various strategies.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4. Error profiles of GPT-3.5 on the textbook dataset under four settings, which reveal the distribution of their deficiencies in ten essential problem-solving abilities.</p>
<ul>
<li>Abstract reasoning. This skill involves the ability to understand complex concepts that cannot be perceived physically, and to recognize patterns or relationships beyond concrete examples.</li>
<li>Scientific literacy. This skill involves a comprehensive understanding of key scientific principles, terminology, and methodologies across a range of disciplines.</li>
<li>Code conversion skills. This involves the ability to accurately translate solution steps into different programming languages, like Python or Wolfram Language.</li>
<li>Logical reasoning. This is the ability to make a reasoned argument and to identify fallacies or inconsistencies in an argument or set of data.</li>
<li>Calculation skills. This involves the ability to accurately carry out mathematical operations and computations.</li>
</ul>
<p>After identifying this essential skill set, we assess the performance of the LLMs under different settings to discern the specific problem-solving skills they lack. Given the high cost of human annotations required to attribute the cause of incorrect solutions to specific skill deficiencies, we propose a novel self-critique protocol: we design a specific prompt that outlines these abilities, and employ another LLM to serve as a classifier and determine whether a specific error results from the lack of a particular problem-solving skill. Finally, we ask human annotators to scrutinize the classification results, which results in approximately $20 \%$ of incorrectly classified skills being discarded. To be specific, we utilize a GPT-3.5 model as the verifier to determine the reason behind each error and pinpoint the missing skill. The details regarding the specific prompts used are provided in</p>
<p>Appendix C.1. This verification process is conducted for four settings, with results represented in bar charts (Figure 4). Additional examples of the evaluation protocol are elaborated in Appendix D.</p>
<p>Our findings suggest that there is a lack of a universally effective setting: each configuration only enhances some specific abilities and occasionally even hurts other skills that the original model possesses. First, CoT prompting significantly improves calculation skills in the zero-shot scenario, with $13.6 \%$ error rates caused by calculation ability, considerably lower than the $29.0 \%$ error rate of the vanilla zero-shot baseline. However, CoT shows limitations in improving other skills, with $32.2 \%$ and $25.4 \%$ error rates in casual ability and logical decomposition ability in the zero-shot CoT setting, respectively, compared to $18.3 \%$ and $18.3 \%$ in the zero-shot setting. This contradicts previous claims about universal skill enhancement through zero-shot CoT and carefully-designed few-shot CoT prompts (Wei et al., 2022). An example in Figure S9 shows that the zeroshot learning setting without CoT has generated the correct formula but fails in the calculation steps. In this case, CoT prompting is even unable to use the correct formula as it misinterprets the specific conditions (non-necessity) in the problem. Second, the use of external tools significantly reduces calculation errors compared to the few-shot Cot setting, with a notable decrease from $14.5 \%$ to $6.2 \%$. However, the use of external tools can weaken other skills, particularly the code conversion skills, i.e., generating the correct programs for the solution. Third, few-shot learning does not universally improve scientific problem-solving skills, as</p>
<p>indicated in the comparison between zero-shot and few-shot CoT settings. The improvement in one skill is offset by the shortcomings in others: although the few-shot CoT setting results in a reduction of $12.8 \%$ in errors related to causal reasoning, it also leads to an increase in errors associated with other skills, such as logical decomposition.</p>
<h2>6. Conclusion</h2>
<p>This paper presents SCIBENCH, a college-level benchmark that includes scientific problems from Mathematics, Physics, and Chemistry, as well as exam questions in Computer Science and Mathematics. Our comprehensive evaluation includes a diverse array of Large Language Models (LLMs), spanning both open-source and proprietary models, including unimodal as well as multimodal settings, and employing a variety of prompting strategies. The evaluation protocol we employ serves as a framework for evaluating advanced problem-solving skills of LLMs in scientific domains. The findings of this study highlight that while large language models (LLMs) exhibit impressive performance on introductory mathematical benchmarks, their mastery of problem solving ability remains weak. These findings underscore the limitations of current LLMs in achieving satisfactory performance, even with the assistance of various tools. We envision that the SCIBENCH benchmark dataset and evaluation protocol presented in this paper could lay a foundation for future research and enable advancements in understanding and enhancing problem-solving capabilities of LLMs.</p>
<h2>Reproducibility Statement</h2>
<p>To foster reproducible research, we include all dataset processing and experiment details of SCIBENCH. We detail data processing in Section 3 and provide the UI design of data collection in Appendix A.3. We include all experiment details with LLM prompts in Appendix C. Finally, we make our dataset and code publicly available at this repository.</p>
<h2>Ethical Statement</h2>
<p>The questions of SCIBENCH are sourced from science textbooks and exams. We conduct a manual examination of our dataset to ensure the absence of potential sensitive background or ethical concerns. The inclusion of exam questions has been authorized by the instructors of the respective courses.</p>
<p>The purpose of the textbook dataset is solely for academic use. Its collection adheres to the Fair Use Law in the US, where only a certain number of questions from each textbook are selected, ensuring that only a small portion of the textbook is utilized.</p>
<h2>Impact Statement</h2>
<p>The introduction of SCIBENCH represents a significant advancement in the evaluation of Large Language Models (LLMs) for scientific problem-solving tasks. By focusing on collegiate-level problems in mathematics, chemistry, and physics, SCIBENCH addresses a critical gap in existing benchmarks, which have primarily focused on high-school subjects and basic algebraic operations. This development underscores the necessity of developing specialized benchmarks that challenge LLMs with higher complexity problems, thereby pushing the boundaries of the capabilities of LLMs in academic and research settings.</p>
<p>While the current scope of SCIBENCH encompasses a select group of scientific disciplines, the potential for future extensions is vast. Incorporating additional subjects such as biology, computer science, and engineering could provide a more comprehensive understanding of LLM capabilities across a broader spectrum of scientific knowledge. Moreover, extending the benchmark to social sciences, humanities, and other human-centric domains would be equally beneficial, as these areas often involve nuanced reasoning and interpretation of complex social dynamics and ethical considerations, posing unique challenges that could further enhance the versatility and applicability of LLMs.</p>
<h2>Acknowledgements</h2>
<p>This work was supported by the National Science Foundation (NSF) under Grant Nos. 1829071, 1937599, 2106859, 2119643, 2202693, 2211557, 2303037, and 2312501; the National Institutes of Health (NIH) under Grant No. U54HG012517; the Defense Advanced Research Projects Agency (DARPA) under Grant No. HR00112490370; NASA; SRC JUMP 2.0 Center; Amazon Research Awards; and Snapchat Gifts.</p>
<h2>References</h2>
<p>Anthropic. Claude2. https://www.anthropic.com/index/ claude-2, 2023. 5</p>
<p>Arora, D., Singh, H. G., et al. Have llms advanced enough? a challenging problem solving benchmark for large language models. arXiv preprint arXiv:2305.15074, 2023. 1, 3</p>
<p>Atkins, P., Atkins, P. W., and de Paula, J. Atkins' physical chemistry. Oxford university press, 2014a. 4, 12</p>
<p>Atkins, P., De Paula, J., and Friedman, R. Physical chemistry: quanta, matter, and change. Oxford University Press, USA, 2014b. 2, 4, 12</p>
<p>Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 7</p>
<p>Boyce, W. E., DiPrima, R. C., and Meade, D. B. Elementary differential equations and boundary value problems. John Wiley \&amp; Sons, 2021. 4, 12</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. 1</p>
<p>Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. 4</p>
<p>Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. 1</p>
<p>Chen, W., Ma, X., Wang, X., and Cohen, W. W. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research (TMLR), 2023a. 1, 5, 7</p>
<p>Chen, W., Yin, M., Ku, M., Lu, P., Wan, E., Ma, X., Xu, J., Xia, T., and Wang, X. Theoremqa: A theorem-driven question answering dataset. arXiv preprint arXiv:2305.12524, 2023b. 3, 5</p>
<p>Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 1, 3, 4</p>
<p>Dong, X., Zhang, P., Zang, Y., Cao, Y., Wang, B., Ouyang, L., Wei, X., Zhang, S., Duan, H., Cao, M., Zhang, W., Li, Y., Yan, H., Gao, Y., Zhang, X., Li, W., Li, J., Chen, K., He, C., Zhang, X., Qiao, Y., Lin, D., and Wang, J. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024. 7</p>
<p>Engel, T. and Reid, P. J. Thermodynamics, statistical thermodynamics, and kinetics. Prentice Hall Upper saddle River, 2010. 4,12</p>
<p>Frieder, S., Pinchetti, L., Griffiths, R.-R., Salvatori, T., Lukasiewicz, T., Petersen, P. C., Chevalier, A., and Berner, J. Mathematical capabilities of chatgpt. arXiv preprint arXiv:2301.13867, 2023. 4</p>
<p>Fu, Y., Ou, L., Chen, M., Wan, Y., Peng, H., and Khot, T. Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance. arXiv preprint arXiv:2305.17306, 2023. 3</p>
<p>Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and Neubig, G. PAL: Program-aided language models. arXiv preprint arXiv:2211.10435, 2022. 1, 5</p>
<p>Gao, P., Han, J., Zhang, R., Lin, Z., Geng, S., Zhou, A., Zhang, W., Lu, P., He, C., Yue, X., Li, H., and Qiao, Y. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023. 1</p>
<p>Ghazal, A., Rabl, T., Hu, M., Raab, F., Poess, M., Crolotte, A., and Jacobsen, H.-A. Bigbench: Towards an industry standard benchmark for big data analytics. In Proceedings of the 2013 ACM SIGMOD international conference on Management of data, pp. 1197-1208, 2013. 3</p>
<p>Google. Bard. https://bard.google.com, 2023. 7
Guo, T., Guo, K., Liang, Z., Guo, Z., Chawla, N. V., Wiest, O., Zhang, X., et al. What indeed can gpt models do in chemistry? a comprehensive benchmark on eight tasks. arXiv preprint arXiv:2305.18365, 2023. 3</p>
<p>Halliday, D., Resnick, R., and Walker, J. Fundamentals of physics. John Wiley \&amp; Sons, 2013. 4, 12</p>
<p>Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. 1, 3</p>
<p>Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. 1, 3, 4</p>
<p>Hogg, R. V., Tanis, E. A., and Zimmerman, D. L. Probability and statistical inference, volume 993. Macmillan New York, 1977. 4,13</p>
<p>Huang, J., Gu, S. S., Hou, L., Wu, Y., Wang, X., Yu, H., and Han, J. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022. 1</p>
<p>JaidedAI. Easyocr: Ready-to-use ocr. https://github.com/J aidedAI/EasyOCR, 2022. 7</p>
<p>Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. 5</p>
<p>Kabir, S., Udo-Imeh, D. N., Kou, B., and Zhang, T. Who answers it better? an in-depth analysis of chatgpt and stack overflow answers to software engineering questions. arXiv preprint arXiv:2308.02312, 2023. 4</p>
<p>Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022. 1</p>
<p>Levine, I. N., Busch, D. H., and Shull, H. Quantum chemistry, volume 6. Pearson Prentice Hall Upper Saddle River, NJ, 2009. 4,12</p>
<p>Lin, Z., Liu, C., Zhang, R., Gao, P., Qiu, L., Xiao, H., Qiu, H., Lin, C., Shao, W., Chen, K., et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575, 2023. 7</p>
<p>Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. In NeurIPS, 2023a. 7</p>
<p>Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023b. 1</p>
<p>Liu, H., Ning, R., Teng, Z., Liu, J., Zhou, Q., and Zhang, Y. Evaluating the logical reasoning ability of chatgpt and gpt-4. arXiv preprint arXiv:2304.03439, 2023c. 4</p>
<p>Lu, P., Gong, R., Jiang, S., Qiu, L., Huang, S., Liang, X., and Zhu, S.-C. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. In The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021), 2021a. 5</p>
<p>Lu, P., Qiu, L., Chen, J., Xia, T., Zhao, Y., Zhang, W., Yu, Z., Liang, X., and Zhu, S.-C. Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021b. 3</p>
<p>Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:2507-2521, 2022. 1, 3, 5</p>
<p>Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M., and Gao, J. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023a. 3</p>
<p>Lu, P., Peng, B., Cheng, H., Galley, M., Chang, K.-W., Wu, Y. N., Zhu, S.-C., and Gao, J. Chameleon: Plug-and-play compositional reasoning with large language models. arXiv preprint arXiv:2304.09842, 2023b. 1</p>
<p>Lu, P., Qiu, L., Chang, K.-W., Wu, Y. N., Zhu, S.-C., Rajpurohit, T., Clark, P., and Kalyan, A. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In International Conference on Learning Representations (ICLR), 2023c. 3</p>
<p>Lu, P., Qiu, L., Yu, W., Welleck, S., and Chang, K.-W. A survey of deep learning for mathematical reasoning. In The 61st Annual Meeting of the Association for Computational Linguistics (ACL), 2023d. 3</p>
<p>McQuarrie, D. A. Quantum chemistry. University Science Books, 2008. 4, 12</p>
<p>Mishra, S., Finlayson, M., Lu, P., Tang, L., Welleck, S., Baral, C., Rajpurohit, T., Tafjord, O., Sabharwal, A., Clark, P., et al. Lila: A unified benchmark for mathematical reasoning. In The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022. 3</p>
<p>OpenAI. Chatgpt: Optimizing language models for dialogue. https://openai.com/blog/chatgpt/., 2022. 1, 5</p>
<p>OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1, 5, 7</p>
<p>Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023. 1</p>
<p>Stewart, J., Watson, S., and Clegg, D. Calculus: Early transcendentals, 8th. Edition, Brooks/Cole, Cengae learning, 2012. 4, 13</p>
<p>Sun, L., Han, Y., Zhao, Z., Ma, D., Shen, Z., Chen, B., Chen, L., and Yu, K. Scieval: A multi-level large language model evaluation benchmark for scientific research. arXiv preprint arXiv:2308.13149, 2023. 3</p>
<p>Suzgun, M., Scales, N., Schärli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. 3</p>
<p>Taylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn, A., Saravia, E., Poulton, A., Kerkez, V., and Stojnic, R. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022. 3</p>
<p>Thornton, S. T. and Marion, J. B. Classical dynamics of particles and systems. Cengage Learning, 2021. 4, 12</p>
<p>Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. 1</p>
<p>Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. 5</p>
<p>Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. 1</p>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022. $1,7,8$</p>
<p>Welleck, S., Liu, J., Bras, R. L., Hajishirzi, H., Choi, Y., and Cho, K. Naturalproofs: Mathematical theorem proving in natural language. arXiv preprint arXiv:2104.01112, 2021. 3</p>
<p>Zhang, D., Hu, Z., Zhoubian, S., Du, Z., Yang, K., Wang, Z., Yue, Y., Dong, Y., and Tang, J. Sciglm: Training scientific language models with self-reflective instruction annotation and tuning. arXiv preprint arXiv:2401.07950, 2024. 4</p>
<p>Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Gao, P., and Qiao, Y. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023a. 1</p>
<p>Zhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., and Smola, A. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023b. 1</p>
<p>Zhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang, Y., Saied, A., Chen, W., and Duan, N. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023. 1, 3, 7</p>
<p>Zhou, D., Schärli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Bousquet, O., Le, Q., and Chi, E. Least-tomost prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022. 1</p>
<h1>Supplementary Material for SCIBENCH</h1>
<p>A The Textbook Dataset ..... 12
A. 1 Textbook Sources ..... 12
A. 2 Textbook Examples ..... 13
A. 3 UI Design of the Labeling Tool ..... 14
B The Exam Dataset ..... 14
C Experimental Details ..... 18
C. 1 Prompts ..... 18
C. 2 Implementation Details ..... 20
C. 3 Additional Experiment on Wolfram Language ..... 20
D Problem Solving Abilities of Current LLMs ..... 21
D. 1 Assessment of the Evaluation Protocol ..... 21
D. 2 Examples ..... 21</p>
<h2>A. The Textbook Dataset</h2>
<h2>A.1. Textbook Sources</h2>
<ul>
<li>Physical Chemistry (Atkins et al., 2014a) (atkins) provides an exploration of equilibrium, structure, and reactions, integrating contemporary techniques like nanoscience, spectroscopy, and computational chemistry.</li>
<li>Quantum Chemistry (McQuarrie, 2008) (chemmc) meticulously covers Quantum Mechanics, from foundational principles like blackbody radiation and Heisenberg's Uncertainty Principle to complex topics such as Schrödinger equation, quantum mechanical operators, and the application of quantum mechanics in chemical bonding.</li>
<li>Quantum Chemistry (Levine et al., 2009) (quan) explores quantum chemistry, providing a detailed understanding of the Schrödinger equation, particle behavior in various scenarios, quantum mechanics operators, and other foundational quantum principles. It delves into specific applications like the electronic structure of diatomic and polyatomic molecules, variation methods, perturbation theory, electron spin and its implications in quantum mechanics, as well as various computational methods for molecular quantum mechanics.</li>
<li>Physical Chemistry, Quanta, Matter, and Change (Atkins et al., 2014b) (matter) combines physics and mathematics, beginning with basics like differentiation and integration, advancing through quantum mechanics and atomic structure, then exploring thermodynamics, molecular motion, and chemical kinetics. Each section is supplemented with mathematical concepts such as differential equations, vectors, and probability theory.</li>
<li>Classical Dynamics Of Partical and Systems (Thornton \&amp; Marion, 2021) (class) initiates with an exploration of fundamental mathematical concepts, discussing scalars, vectors, matrix operations, coordinate transformations, differentiation, and integration of vectors, using these constructs to illustrate concepts like velocity, acceleration, and angular velocity. It then transitions into the realm of Newtonian mechanics, detailing Newton's laws, frames of reference, and the equation of motion for a single particle.</li>
<li>Thermodynamics, Statistical Thermodynamics, and Kinetics (Engel \&amp; Reid, 2010) (thermo) navigates through thermodynamics' principles, from fundamental concepts to complex laws, further discussing real and ideal gases, solutions, electrochemical cells, and statistical thermodynamics. It concludes with an examination of the kinetic theory of gases, transport phenomena, and chemical kinetics.</li>
<li>Fundamentals of Physics (Halliday et al., 2013) (fund) covers undergraduate physics topics, ranging from fundamental concepts like motion and energy to more advanced areas such as quantum physics and nuclear physics.</li>
<li>Elementary Differential Equations and Boundary Value Problems (Boyce et al., 2021) (diff) provides a detailed exploration of differential equations, progressing from basic mathematical models to advanced topics</li>
</ul>
<p>like the Laplace Transform, linear systems, numerical methods, and Fourier series. It culminates with a deep dive into nonlinear equations, partial differential equations, and boundary value problems.</p>
<ul>
<li>Probability and Statistical Inference (Hogg et al., 1977) (stat) covers probability and statistics, including fundamental concepts, discrete and continuous distributions, bivariate distributions, functions of random variables, and estimation techniques.</li>
<li>Calculus: Early Transcendentals (Stewart et al., 2012) (calculus) begins with diagnostic tests in foundational topics, and explores functions from multiple perspectives. It comprehensively covers calculus concepts from limits to three-dimensional analytic geometry, incorporating applications in various fields.</li>
</ul>
<h1>A.2. Textbook Examples</h1>
<p>The textbook examples are provided in Figure S1. The examples from the multimodal subset are provided in Figures S2 to S5.</p>
<p>Problem (fund)
Two charged particles are fixed to an $x$ axis: Particle 1 of charge $q_{1}=2.1 \times 10^{-8} \mathrm{C}$ is at position $x=20 \mathrm{~cm}$ and particle 2 of charge $q_{2}=-4.00 q_{1}$ is at position $x=70 \mathrm{~cm}$. At what coordinate on the axis (other than at infinity) is the net electric field produced by the two particles equal to zero?
Answer: -30 cm
Problem (thorns)
$\mathrm{N}<em 3="3">{2} \mathrm{O}</em>}$ dissociates according to the equilibrium $\mathrm{N<em 3="3">{2} \mathrm{O}</em>}(\mathrm{~g}) \rightleftharpoons \mathrm{NO<em 2="2">{2}(\mathrm{~g})+\mathrm{NO}(\mathrm{g})$. At 298 K and one bar pressure, the degree of dissociation defined as the ratio of moles of $\mathrm{NO}</em>$ for this reaction.
Answer: $28 \mathrm{~kJ} \mathrm{~mol}^{-1}$
Problem (class)
Halley's comet, which passed around the sun early in 1986, moves in a highly elliptical orbit with an eccentricity of 0.967 and a period of 76 years. Calculate its minimum distances from the Sun.
Answer: $8.8 \times 10^{10} \mathrm{~m}$
Problem (quan)
A one-particle, one-dimensional system has $\Psi=a^{-1 / 3} e^{-|x| / a}$ at $t=0$, where $a=1.0000 \mathrm{~nm}$. At $t=0$, the particle's position is measured. Find the probability that the measured value is between $x=0$ and $x=2 \mathrm{~nm}$.
Answer: 0.4908
Problem (channc)
One of the most powerful modern techniques for studying structure is neutron diffraction. This technique involves generating a collimated beam of neutrons at a particular temperature from a high-energy neutron source and is accomplished at several accelerator facilities around the world. If the speed of a neutron is given by $v_{\mathrm{n}}=\left(3 k_{\mathrm{B}} T / m\right)^{1 / 2}$, where $m$ is the mass of a neutron, then what temperature is needed so that the neutrons have a de Broglie wavelength of 50 pm ?
Answer: 2500 K
Problem (stiling)
The change in molar internal energy when $\mathrm{CaCO}_{3}(\mathrm{~s})$ as calcite converts to another form, aragonite, is $+0.21 \mathrm{~kJ} \mathrm{~mol}^{-1}$. Calculate the difference between the molar enthalpy and internal energy changes when the pressure is 1.0 bar given that the densities of the polymorphs are $2.71 \mathrm{~g} \mathrm{~cm}^{-3}$ and $2.93 \mathrm{~g} \mathrm{~cm}^{-3}$, respectively.
Answer: $-0.28 \mathrm{~Pa} \mathrm{~m}^{3} \mathrm{~mol}^{-1}$
Problem (cutter)
In an industrial process, nitrogen is heated to 500 K at a constant volume of $1.000 \mathrm{~m}^{3}$. The gas enters the container at 300 K and 100 atm . The mass of the gas is 92.4 kg . Use the van der Waals equation to determine the approximate pressure of the gas at its working temperature of 500 K . For nitrogen, $a=1.39 \mathrm{dm}^{6} \mathrm{~atm} \mathrm{~mol}^{-2}, h=$ $0.0391 \mathrm{dm}^{3} \mathrm{~mol}^{-1}$.
Answer: 140 atm
Problem (cold)
A planning engineer for a new alum plant must present some estimates to his company regarding the capacity of a silo designed to contain bauxite ore until it is processed into alum. The ore resembles pink talcum powder and is poured from a conveyor at the top of the silo. The silo is a cylinder 100 ft high with a radius of 200 ft . The conveyor carries ore at a rate of $60,000 \pi \mathrm{ft}^{3} / \mathrm{h}$ and the ore maintains a conical shape whose radius is 1.5 times its height. If, at a certain time $t$, the pile is 60 ft high, how long will it take for the pile to reach the top of the silo?
Answer: 9.8 h
Problem (stat)
In a study concerning a new treatment of a certain disease, two groups of 25 participants in each were followed for five years. Those in one group took the old treatment and those in the other took the new treatment. The theoretical dropout rate for an individual was $50 \%$ in both groups over that 5 -year period. Let $X$ be the number that dropped out in the first group and $Y$ the number in the second group. Assuming independence where needed, give the sum that equals the probability that $Y \geq X+2$. HINT: What is the distribution of $Y-X+25$ ?
Answer: 0.3359
Problem (diff)
Newton's law of cooling states that the temperature of an object changes at a rate proportional to the difference between its temperature and that of its surroundings. Suppose that the temperature of a cup of coffee obeys Newton's law of cooling. If the coffee has a temperature of $200^{\circ} \mathrm{F}$ when freshly poured, and 1 min later has cooled to $190^{\circ} \mathrm{F}$ in a room at $70^{\circ} \mathrm{F}$, determine when the coffee reaches a temperature of $150^{\circ} \mathrm{F}$
Answer: 6.07 min
Figure S1. Textbook examples with acronym highlighted in brown.}(g)$ or $\mathrm{NO}(g)$ to the moles of the reactant assuming no dissociation occurs is $3.5 \times 10^{-3}$. Calculate $\Delta G_{R}^{\circ</p>
<h1>Problem</h1>
<p>The region $\mathscr{R}$ enclosed by the curves $y=x$ and $y=x^{2}$ is rotated about the $x$-axis. Find the volume of the resulting solid.</p>
<h2>Image</h2>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<h2>Correct Solution</h2>
<p>The curves $y=x$ and $y=x^{2}$ intersect at the points $(0,0)$ and $(1,1)$. The region between them, the solid of rotation, and a cross-section perpendicular to the $x$-axis are shown in the Figure. A cross-section in the plane $P_{x}$ has the shape of a washer (an annular ring) with inner radius $x^{2}$ and outer radius $x$, so we find the cross-sectional area by subtracting the area of the inner circle from the area of the outer circle:</p>
<p>$$
A(x)=\pi x^{2}-\pi\left(x^{2}\right)^{2}=\pi\left(x^{2}-x^{4}\right)
$$</p>
<p>Therefore we have</p>
<p>$$
\begin{aligned}
V &amp; =\int_{0}^{1} A(x) d x=\int_{0}^{1} \pi\left(x^{2}-x^{4}\right) d x \
&amp; =\pi\left[\frac{x^{3}}{3}-\frac{x^{5}}{5}\right]_{0}^{1}=\frac{2 \pi}{15}
\end{aligned}
$$</p>
<p>Final Answer: $\frac{2 \pi}{15}$
Figure S2. The example from the textbook Calculus: Early Transcendentals.</p>
<h2>A.3. UI Design of the Labeling Tool</h2>
<p>We employed a team of seven individuals to gather data from textbooks using an annotation tool. Each individual was responsible for one to two books, encompassing approximately 100 examples. The user interface of the annotation tool is depicted in Figure S6. For subsequent verification, we preserved images of problems and their corresponding answers. To ensure clarity in future references, we have maintained the original sequence of problems as they appear in the textbooks.</p>
<h2>B. The Exam Dataset</h2>
<p>The exam dataset is drawn from the following sources:</p>
<ul>
<li>Introduction to Data Mining provides an introductory survey of data mining, which involves the automatic discovery of patterns, associations, changes, and anomalies in large databases. It explores various application areas of data mining, including bioinformatics, e-commerce, environmental studies, financial markets, multimedia data processing, network monitoring, and social service analysis.</li>
<li>Fundamentals Artificial Intelligence provides an introduction to the core problem-solving and knowledge representation paradigms in artificial intelligence. It covers Lisp programming with regular assignments, as well as topics such as search methods, planning techniques, knowledge structures, natural language processing, expert systems, vision, and parallel architectures.</li>
<li>Differential Equations covers various topics in differential equations, including first-order and second-order linear equations with constant coefficients, power series solutions, and linear systems. Students will explore the principles and applications of these mathematical concepts.</li>
</ul>
<p>A detailed statistics of the exam dataset is summarized in Table S1. The experiment results of exam dataset are provided in Table S2.</p>
<h1>Problem</h1>
<p>A 2.00 kg particle moves along an $x$ axis in one-dimensional motion while a conservative force along that axis acts on it. The potential energy $U(x)$ associated with the force is plotted in the Figure. That is, if the particle were placed at any position between $x=0$ and $x=7.00 \mathrm{~m}$, it would have the plotted value of $U$. At $x=6.5 \mathrm{~m}$, the particle has velocity $\vec{v}<em 1="1">{0}=(-4.00 \mathrm{~m} / \mathrm{s}) \hat{\mathrm{i}}$. From the Figure, determine the particle's speed at $x</em>$.
}=4.5 \mathrm{~m<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<h2>Correct Solution</h2>
<p>The particle's kinetic energy is given by Eq $\left(K=\frac{1}{2} m v^{2}\right)$. Because only a conservative force acts on the particle, the mechanical energy $E_{\mathrm{mec}}(=K+U)$ is conserved as the particle moves. Therefore, on a plot of $U(x)$, the kinetic energy is equal to the difference between $E_{\mathrm{mec}}$ and $U$.
Calculations: At $x=6.5 \mathrm{~m}$, the particle has kinetic energy</p>
<p>$$
\begin{aligned}
K_{0} &amp; =\frac{1}{2} m v_{0}^{2}=\frac{1}{2}(2.00 \mathrm{~kg})(4.00 \mathrm{~m} / \mathrm{s})^{2} \
&amp; =16.0 \mathrm{~J}
\end{aligned}
$$</p>
<p>Because the potential energy there is $U=0$, the mechanical energy is $E_{\mathrm{mec}}=K_{0}+U_{0}=16.0 \mathrm{~J}+0=16.0 \mathrm{~J}$. This value for $E_{\mathrm{mec}}$ is plotted as a horizontal line in the Figure. From that figure we see that at $x=4.5 \mathrm{~m}$, the potential energy is $U_{1}=7.0 \mathrm{~J}$. The kinetic energy $K_{1}$ is the difference between $E_{\mathrm{mec}}$ and $U_{1}$ :</p>
<p>$$
K_{1}=E_{\mathrm{mec}}-U_{1}=16.0 \mathrm{~J}-7.0 \mathrm{~J}=9.0 \mathrm{~J}
$$</p>
<p>Because $K_{1}=\frac{1}{2} m v_{1}^{2}$, we find $v_{1}=3.0 \mathrm{~m} / \mathrm{s}$.
Final Answer: $3.0 \mathrm{~m} / \mathrm{s}$
Figure S3. An example problem from the textbook Fundamentals of Physics.</p>
<p>Table S1. Statistics of the close exam dataset. We report the number of problem instances in each exam and the ratio of problems in the exam that include detailed solutions. We further report the ratio of problems in different formats, including free-response, multiple-choice, and true-false. For reference, the number in parentheses denotes the grading points assigned to the problems.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Data Mining</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Machine Learning</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Differential Equations</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Midterm</td>
<td style="text-align: center;">Final</td>
<td style="text-align: center;">Midterm</td>
<td style="text-align: center;">Final</td>
<td style="text-align: center;">Exam 1</td>
<td style="text-align: center;">Exam 2</td>
<td style="text-align: center;">Final</td>
</tr>
<tr>
<td style="text-align: center;"># Problems</td>
<td style="text-align: center;">25 (90)</td>
<td style="text-align: center;">24 (75)</td>
<td style="text-align: center;">12 (56)</td>
<td style="text-align: center;">16 (75)</td>
<td style="text-align: center;">8 (100)</td>
<td style="text-align: center;">8 (100)</td>
<td style="text-align: center;">11 (95)</td>
</tr>
<tr>
<td style="text-align: center;">\% Solutions</td>
<td style="text-align: center;">56.0\% (58)</td>
<td style="text-align: center;">16.7\% (19)</td>
<td style="text-align: center;">100.0\% (56)</td>
<td style="text-align: center;">31.2\% (26)</td>
<td style="text-align: center;">100.0\% (100)</td>
<td style="text-align: center;">100.0\% (100)</td>
<td style="text-align: center;">90.9\% (90)</td>
</tr>
<tr>
<td style="text-align: center;">\% Free-response</td>
<td style="text-align: center;">40.0\% (46)</td>
<td style="text-align: center;">33.3\% (29)</td>
<td style="text-align: center;">66.7\% (38)</td>
<td style="text-align: center;">81.3\% (62)</td>
<td style="text-align: center;">100.0\% (100)</td>
<td style="text-align: center;">100.0\% (100)</td>
<td style="text-align: center;">90.9\% (90)</td>
</tr>
<tr>
<td style="text-align: center;">\% Multiple-choice</td>
<td style="text-align: center;">28.0\% (28)</td>
<td style="text-align: center;">29.2\% (28)</td>
<td style="text-align: center;">33.3\% (18)</td>
<td style="text-align: center;">18.7\% (13)</td>
<td style="text-align: center;">0.0\% (0)</td>
<td style="text-align: center;">0.0\% (0)</td>
<td style="text-align: center;">9.1\% (5)</td>
</tr>
<tr>
<td style="text-align: center;">\% True-false</td>
<td style="text-align: center;">32.0\% (16)</td>
<td style="text-align: center;">37.5\% (18)</td>
<td style="text-align: center;">0.0\% (0)</td>
<td style="text-align: center;">0.0\% (0)</td>
<td style="text-align: center;">0.0\% (0)</td>
<td style="text-align: center;">0.0\% (0)</td>
<td style="text-align: center;">0.0\% (0)</td>
</tr>
</tbody>
</table>
<p>Table S2. Experimental results in terms of total scores under zero-shot learning on the exam dataset. The best performing score is highlighted in bold and second-best is underlined.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Setting</th>
<th style="text-align: center;">Data Mining</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Machine Learning</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Differential Equations</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Midterm</td>
<td style="text-align: center;">Final</td>
<td style="text-align: center;">Midterm</td>
<td style="text-align: center;">Final</td>
<td style="text-align: center;">Exam 1</td>
<td style="text-align: center;">Exam 2</td>
<td style="text-align: center;">Final</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2-7B</td>
<td style="text-align: center;">Zero</td>
<td style="text-align: center;">24 / 90</td>
<td style="text-align: center;">14 / 75</td>
<td style="text-align: center;">6 / 56</td>
<td style="text-align: center;">6/ 75</td>
<td style="text-align: center;">5 / 100</td>
<td style="text-align: center;">0 / 100</td>
<td style="text-align: center;">0 / 95</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero+CoT</td>
<td style="text-align: center;">18 / 90</td>
<td style="text-align: center;">14 / 75</td>
<td style="text-align: center;">2 / 56</td>
<td style="text-align: center;">10 / 75</td>
<td style="text-align: center;">10 / 100</td>
<td style="text-align: center;">0 / 100</td>
<td style="text-align: center;">10 / 95</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2-70B</td>
<td style="text-align: center;">Zero</td>
<td style="text-align: center;">23 / 90</td>
<td style="text-align: center;">18 / 75</td>
<td style="text-align: center;">18 / 56</td>
<td style="text-align: center;">12 / 75</td>
<td style="text-align: center;">20 / 100</td>
<td style="text-align: center;">5 / 100</td>
<td style="text-align: center;">0 / 95</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero+CoT</td>
<td style="text-align: center;">31 / 90</td>
<td style="text-align: center;">18 / 75</td>
<td style="text-align: center;">10 / 56</td>
<td style="text-align: center;">11/ 75</td>
<td style="text-align: center;">35 / 100</td>
<td style="text-align: center;">10 / 100</td>
<td style="text-align: center;">0 / 95</td>
</tr>
<tr>
<td style="text-align: center;">Claude2</td>
<td style="text-align: center;">Zero</td>
<td style="text-align: center;">37 / 90</td>
<td style="text-align: center;">26 / 75</td>
<td style="text-align: center;">28 / 56</td>
<td style="text-align: center;">35 / 75</td>
<td style="text-align: center;">35 / 100</td>
<td style="text-align: center;">30 / 100</td>
<td style="text-align: center;">20 / 95</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero+CoT</td>
<td style="text-align: center;">33 / 90</td>
<td style="text-align: center;">38 / 75</td>
<td style="text-align: center;">22 / 56</td>
<td style="text-align: center;">41 / 75</td>
<td style="text-align: center;">25 / 100</td>
<td style="text-align: center;">15 / 100</td>
<td style="text-align: center;">20 / 95</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">Zero</td>
<td style="text-align: center;">44 / 90</td>
<td style="text-align: center;">39 / 75</td>
<td style="text-align: center;">16 / 56</td>
<td style="text-align: center;">32 / 75</td>
<td style="text-align: center;">0 / 100</td>
<td style="text-align: center;">45 / 100</td>
<td style="text-align: center;">15 / 95</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero+CoT</td>
<td style="text-align: center;">38 / 90</td>
<td style="text-align: center;">33 / 75</td>
<td style="text-align: center;">32 / 56</td>
<td style="text-align: center;">37 / 75</td>
<td style="text-align: center;">28 / 100</td>
<td style="text-align: center;">30 / 100</td>
<td style="text-align: center;">10 / 95</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">Zero</td>
<td style="text-align: center;">56 / 90</td>
<td style="text-align: center;">44 / 75</td>
<td style="text-align: center;">30 / 56</td>
<td style="text-align: center;">37 / 75</td>
<td style="text-align: center;">25 / 100</td>
<td style="text-align: center;">80 / 100</td>
<td style="text-align: center;">25 / 95</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero+CoT</td>
<td style="text-align: center;">58 / 90</td>
<td style="text-align: center;">32 / 75</td>
<td style="text-align: center;">40 / 56</td>
<td style="text-align: center;">35 / 75</td>
<td style="text-align: center;">50 / 100</td>
<td style="text-align: center;">70 / 100</td>
<td style="text-align: center;">15 / 95</td>
</tr>
</tbody>
</table>
<h1>Problem</h1>
<p>If the particles in a system all move together, the com moves with them-no trouble there. But what happens when they move in different directions with different accelerations? Here is an example. The three particles in the Figure are initially at rest. Each experiences an external force due to bodies outside the threeparticle system. The directions are indicated, and the magnitudes are $F_{1}=6.0 \mathrm{~N}, F_{2}=12 \mathrm{~N}$, and $F_{3}=14 \mathrm{~N}$. What is the acceleration of the center of mass of the system?</p>
<h2>Image</h2>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<h2>Correct Solution</h2>
<p>The position of the center of mass is marked by a dot in the figure. We can treat the center of mass as if it were a real particle, with a mass equal to the system's total mass $M=16 \mathrm{~kg}$. We can also treat the three external forces as if they act at the center of mass (Figure b). We can now apply Newton's second law $\left(\vec{F}_{\text {net }}=m \vec{a}\right)$ to the center of mass, writing</p>
<p>$$
\begin{gathered}
\vec{F}<em _com="{com" _text="\text">{\text {net }}=M \vec{a}</em> \
\vec{F}}<em 2="2">{1}+\vec{F}</em>}+\vec{F<em _com="{com" _text="\text">{3}=M \vec{a}</em> \
\vec{a}}<em 1="1">{\text {com }}=\frac{\vec{F}</em>}+\vec{F<em 3="3">{2}+\vec{F}</em>
\end{gathered}
$$}}{M</p>
<p>The equation tells us that the acceleration $\vec{a}<em _net="{net" _text="\text">{\text {com }}$ of the center of mass is in the same direction as the net external force $\vec{F}</em>}}$ on the system (Figure b). Because the particles are initially at rest, the center of mass must also be at rest. As the center of mass then begins to accelerate, it must move off in the common direction of $\vec{a<em _net="{net" _text="\text">{\text {com }}$ and $\vec{F}</em>}}$. We can evaluate the right side of Eq. S6 directly on a vector-capable calculator, or we can rewrite Eq. S6 in component form, find the components of $\vec{a<em _com="{com" _text="\text">{\text {com }}$, and then find $\vec{a}</em>$. Along the $x$ axis, we have}</p>
<p>$$
a_{\mathrm{com}, x}=\frac{F_{1 x}+F_{2 x}+F_{3 x}}{M}=\frac{-6.0 \mathrm{~N}+(12 \mathrm{~N}) \cos 45^{\circ}+14 \mathrm{~N}}{16 \mathrm{~kg}}=1.03 \mathrm{~m} / \mathrm{s}^{2}
$$</p>
<p>Along the $y$ axis, we have</p>
<p>$$
a_{\mathrm{com}, y}=\frac{F_{1 y}+F_{2 y}+F_{3 y}}{M}=\frac{0+(12 \mathrm{~N}) \sin 45^{\circ}+0}{16 \mathrm{~kg}}=0.530 \mathrm{~m} / \mathrm{s}^{2}
$$</p>
<p>From these components, we find that $\vec{a}_{\text {com }}$ has the magnitude</p>
<p>$$
a_{\mathrm{com}}=\sqrt{\left(a_{\mathrm{com}, x}\right)^{2}+\left(a_{\mathrm{com}, y}\right)^{2}}=1.16 \mathrm{~m} / \mathrm{s}^{2}
$$</p>
<p>Final Answer: $1.16 \mathrm{~m} / \mathrm{s}^{2}$
Figure S4. The example from the textbook Fundamentals of Physics.</p>
<h1>Problem</h1>
<p>At time $t=0$ a tank contains $Q_{0} \mathrm{lb}$ of salt dissolved in 100 gal of water; see Figure 2.3.1. Assume that water containing $\frac{1}{4} \mathrm{lb}$ of salt/gal is entering the tank at a rate of $r \mathrm{gal} / \mathrm{min}$ and that the well-stirred mixture is draining from the tank at the same rate. Set up the initial value problem that describes this flow process. By finding the amount of salt $Q(t)$ in the tank at any time, and the limiting amount $Q_{L}$ that is present after a very long time, if $r=3$ and $Q_{0}=2 Q_{L}$, find the time $T$ after which the salt level is within $2 \%$ of $Q_{L}$.</p>
<h2>Image</h2>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>FIGURE 2.3.1 The water tank in Example 1.</p>
<h2>Correct Solution</h2>
<p>We assume that salt is neither created nor destroyed in the tank. Therefore variations in the amount of salt are due solely to the flows in and out of the tank. More precisely, the rate of change of salt in the tank, $d Q / d t$, is equal to the rate at which salt is flowing in minus the rate at which it is flowing out. In symbols,</p>
<p>$$
\frac{d Q}{d t}=\text { rate in }- \text { rate out }
$$</p>
<p>The rate at which salt enters the tank is the concentration $\frac{1}{4} \mathrm{lb} / \mathrm{gal}$ times the flow rate $\mathrm{rg} / \mathrm{min}$, or $(r / 4) \mathrm{lb} / \mathrm{min}$. To find the rate at which salt leaves the tankl we need to multiply the concentration of salt in the tank by the rate of outflow, $r \mathrm{gal} / \mathrm{min}$. Since the rates of flow in and out are equal, the volume of water in the tank remains constant at 100 gal , and since the mixture is "well-stirred," the concentration throughout the tank is the same, namely, $[Q(t) / 100] \mathrm{lb} / \mathrm{gal}$. Therefore the rate at which salt leaves the tank is $[r Q(t) / 100] \mathrm{lb} / \mathrm{min}$. Thus the differential equation governing this process is</p>
<p>$$
\frac{d Q}{d t}=\frac{r}{4}-\frac{r Q}{100}
$$</p>
<p>The initial condition is</p>
<p>$$
Q(0)=Q_{0}
$$</p>
<p>Upon thinking about the problem physically, we might anticipate that eventually the mixture originally in the tank will be essentially replaced by the mixture flowing in, whose concentration is $\frac{1}{4} \mathrm{lb} / \mathrm{gal}$. Consequently, we might expect that ultimately the amount of salt in the tank would be very close to 25 lb . We can also find the limiting amount $Q_{L}=25$ by setting $d Q / d t$ equal to zero and solving the resulting algebraic equation for $Q$. Rewriting it in the standard form for a linear equation, we have</p>
<p>$$
\frac{d Q}{d t}+\frac{r Q}{100}=\frac{r}{4}
$$</p>
<p>Thus the integrating factor is $e^{r t / 100}$ and the general solution is</p>
<p>$$
Q(t)=25+c e^{-r t / 100}
$$</p>
<p>where $c$ is an arbitrary constant. To satisfy the initial condition, we must choose $c=Q_{0}-25$. Therefore the solution of the initial value problem is</p>
<p>$$
\begin{gathered}
Q(t)=25+\left(Q_{0}-25\right) e^{-r t / 100} \
Q(t)=25\left(1-e^{-r t / 100}\right)+Q_{0} e^{-r t / 100}
\end{gathered}
$$</p>
<p>From above Equations, you can see that $Q(t) \rightarrow 25(\mathrm{lb})$ as $t \rightarrow \infty$, so the limiting value $Q_{L}$ is 25 , confirming our physical intuition. Further, $Q(t)$ approaches the limit more rapidly as $r$ increases. In interpreting the solution, note that the second term on the right side is the portion of the original salt that remains at time $t$, while the first term gives the amount of salt in the tank due to the action of the flow processes. Now suppose that $r=3$ and $Q_{0}=2 Q_{L}=50$; then</p>
<p>$$
Q(t)=25+25 e^{-0.03 t}
$$</p>
<p>Since $2 \%$ of 25 is 0.5 , we wish to find the time $T$ at which $Q(t)$ has the value 25.5. Substituting $t=T$ and $Q=25.5$ and solving for $T$, we obtain</p>
<p>$$
T=(\ln 50) / 0.03 \cong 130.400766848(\min )
$$</p>
<p>Final Answer: $(\ln 50) / 0.03$
Figure S5. The example from the textbook Elementary Differential Equations and Boundary Value Problems.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure S6. The UI design of data annotation.</p>
<h1>C. Experimental Details</h1>
<h2>C.1. Prompts</h2>
<p>The APIs of ChatGPT and GPT-4 have three message parameters: SYSTEM, USER, and ASSISTANT. The SYSTEM parameter represents the system prompt, which provides context and instructions to the model. The USER parameter is the training prompt or input provided by the user, and the ASSISTANT parameter contains the output of the model or the response. All system prompts and training prompts used in our experiments are provided below.</p>
<h2>System Prompt for Zero-Shot, Few-Shot, and Chain-of-Thought settings.</h2>
<p>Please provide a clear and step-by-step solution for a scientific problem in the categories of Chemistry, Physics, or Mathematics. The problem will specify the unit of measurement, which should not be included in the answer. Express the final answer as a decimal number with three digits after the decimal point. Conclude the answer by stating "The answer is therefore \boxed{ [ANSWER]}."</p>
<h2>System Prompt for Few-Shot Learning + Python.</h2>
<p>Please provide a clear and step-by-step solution for a scientific problem in the categories of Chemistry, Physics, or Mathematics. The problem will specify the unit of measurement. Please translate the solution steps into Python code and encase the Python code within triple backticks for clarity.</p>
<h2>System Prompt for Few-Show Learning + Wolfram Language.</h2>
<p>Please provide a clear and step-by-step solution for a scientific problem in the categories of Chemistry, Physics, or Mathematics. The problem will specify the unit of measurement. Please translate the solution steps into Wolfram code and encase the Wolfram Language code within triple backticks for clarity.</p>
<h1>System Prompt for Evaluation Protocol.</h1>
<p>Examine the given problem, the correct solution, and the model's solution. Identify the reason for the error in the model's solution based on the following 10 categories:</p>
<ol>
<li>Logical Decomposition and Analysis Skills: This ability involves decomposing the problem into smaller, manageable parts, and understanding the relationships between these parts.</li>
<li>Identification of Assumptions: This skill involves the AI's ability to recognize relevant and necessary assumptions in the problem.</li>
<li>Spatial Perception: This is important for understanding problems in areas such as physics and chemistry, where you need to visualize molecules, forces, fields, etc.</li>
<li>Causal Reasoning: This is the ability to understand cause and effect relationships.</li>
<li>Problem Deduction Skills: This pertains to the ability to infer and deduce potential solutions or underlying principles from the given information in a problem.</li>
<li>Abstract Reasoning: This skill involves the ability to understand complex concepts that can't be perceived physically, and to recognize patterns or relationships beyond concrete examples.</li>
<li>Scientific Literacy: This skill involves a comprehensive understanding of key scientific principles, terminology, and methodologies across a range of disciplines.</li>
<li>Code Conversion Skills: This denotes the ability to accurately translate solution steps into different programming languages, like Python or Wolfram, without syntax errors.</li>
<li>Logical Reasoning: This is the ability to make a reasoned argument and to identify fallacies or inconsistencies in an argument or set of data.</li>
<li>Calculation Skills: This involves the ability to accurately carry out mathematical operations and computations. Conclude your final error reason category number within \boxed $}$.</li>
</ol>
<h2>Training Prompt for Zero-Shot Chain-of-Thought.</h2>
<p>Stage 1:
Input: [Input-Question] Let's think step by step.
Output: <explanation>
Stage 2:
Input: [Input-Question] Let's think step by step. [Explanation]. Therefore, the answer is:
Output: <answer></p>
<h2>Training Prompt for Few-Shot Chain-of-Thought.</h2>
<p>Input:
Problem 1: [Question 1] Explanation for Problem 1: [Explanation 1]. The answer is \boxed{ [Answer 1]}.
Problem 2: [Question 2] Explanation for Problem 2: [Explanation 2]. The answer is \boxed{ [Answer 2]}. ...</p>
<p>Problem n: [Question n] Explanation for Problem n: [Explanation n]. The answer is \boxed{ [Answer n]}.
Problem $\mathrm{n}+1$ : [Question $\mathrm{n}+1]$
Output: Explanation for Problem n+1: <explanation>. The answer is \boxed{<answer>}.</p>
<h1>Training Prompt for Few-Shot Python or Wolfram Language.</h1>
<p>Input:
Problem 1: [Question 1] Explanation for Problem 1: [Explanation 1]. Python/Wolfram language for Problem 1: " " [Python/Wolfram code 1] " ".
Problem 2: [Question 2] Explanation for Problem 2: [Explanation 2]. Python/Wolfram language for Problem 2: " " [Python/Wolfram code 2] " ".
...
Problem n: [Question n] Explanation for Problem n: [Explanation n]. Python/Wolfram language for Problem n: " " [Python/Wolfram code n] " ".
Problem $\mathrm{n}+1$ : [Question $\mathrm{n}+1$ ]
Output: Explanation for Problem n+1: <explanation>. Python/Wolfram language for Problem n+1: " " [Python/Wolfram code n+1] " ".</p>
<h2>Training Prompt for Evaluation Protocol.</h2>
<p>Input: The question is [input-question]. The correct solution is [Correct-Solution]. The model solution is [ModelSolution].</p>
<p>Output: <Error Type></p>
<h2>Training Prompt for Evaluation Protocol in Python or Wolfram Language.</h2>
<p>Input: The question is [input-question]. The correct solution is [Correct-Solution]. The model solution is [ModelSolution]. The translated program generates the answer as [Program Generated Answer], which is treated as model's output answer.</p>
<p>Output: <Error Type></p>
<h2>C.2. Implementation Details</h2>
<p>All model output is extracted using $\backslash$ boxed $}$ notation. To prevent any missed extractions, we supplement this process with a manual check. For both Python and Wolfram settings, we extract the programming language with the triple backtick " ", subsequently executing it within the corresponding language. The entirety of our code can be accessed via this repository.</p>
<h2>C.3. Additional Experiment on Wolfram Language</h2>
<p>The experiment results and error analysis for using Wolfram Language as external tools are presented in Figure S7 and Figure S8, compared with using CoT and Python Language. We observe that the use of external tools can weaken other
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure S7. Comparison between few-shot learning with external tools.</p>
<p>skills, particularly the code conversion skills. This issue becomes particularly prominent when using the Wolfram Language, with $46.9 \%$ error rate in code conversion skill. Despite providing grammar specifications in system prompts and a few examples as demonstrations, most attempts of code conversion result in syntax errors. In Wolfram Language, the error mainly comes from the violation of variable rules (for instance, Wolfram Language reserves certain letters such as $E$ as protected symbols and disallows underscores in variable names) or incorrect usage of certain functions. This observation suggests a potential improvement for LLM when using Wolfram Language.</p>
<h1>D. Problem Solving Abilities of Current LLMs</h1>
<h2>D.1. Assessment of the Evaluation Protocol</h2>
<p>In order to assess the effectiveness of our evaluation protocol's classification, we enlisted the assistance of two annotators to determine whether the errors identified by the model verifier were accurate or not. Through the annotation of 151 samples across different settings, we observed that 123 of them were correctly classified, resulting in an accuracy rate of $81.45 \%$. Two human annotators participate in the process. Decisions on the final abilities are determined by annotators, aided by assistants. By going through errors, these two annotators develop ten abilities and then employ a Language Learning Model (LLM) as a third evaluator to suggest additional abilities. They then compare and refine their findings based on this input. Ultimately, the final outcomes are determined by the annotators. After LLM annotate the error reasons, we conduct human-check by sampling 151 examples across all settings to make sure the annotations make sense. We make this human-AI cooperated analysis pipeline to reduce the cost of human post-analysis, while incorporate human checking to make sure the correctness of LLM decision and try to reduce the risk that reviewer mentioned. Though not perfect, we believe it can be another type of analyzing framework for future study of LLM problem-solving.</p>
<h2>D.2. Examples</h2>
<p>In the context of each specific capability, we present several exemplary errors accompanied by their corresponding classifications and explanations derived from the GPT model. Referencing Figure S9, the ChatGPT solution employing the Chain-of-Thought method corresponds to error category 4, "Causal Reasoning". The model explanation posits that "The error reason category is 4. Causal Reasoning. The model solution uses the relativistic momentum formula to calculate the momentum of the electron, which is not necessary for this problem since the electron is traveling at only $1.00 \%$ of the speed of light. The relativistic momentum formula is only needed when the velocity of the particle is a significant fraction of the speed of light. Therefore, using this formula in this problem leads to an incorrect value for the momentum and consequently an incorrect value for the de Broglie wavelength." Meanwhile, the ChatGPT solution that doesn't utilize the Chain-of-Thought method is categorized under error case 10, "Calculation Skills". This example is further highlighted as an illustrative showcase for explanations of the Chain-of-Thought method in Section 5. More examples resulting in different error types are shown in Figures S10 to S19.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure S8. Error Analysis between few-shot learning with external tools.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal contribution ${ }^{1}$ University of California, Los Angeles, Los Angeles, CA, USA ${ }^{2}$ California Institute of Technology, Pasadena, CA, USA ${ }^{3}$ University of Washington, Seattle, WA, USA. Correspondence to: Xiaoxuan Wang <a href="mailto:xw27@cs.ucla.edu">xw27@cs.ucla.edu</a>.</p>
<p>Proceedings of the $41^{\text {st }}$ International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
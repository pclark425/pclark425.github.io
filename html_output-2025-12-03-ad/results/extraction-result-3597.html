<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3597 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3597</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3597</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-79.html">extraction-schema-79</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <p><strong>Paper ID:</strong> paper-1823b8aecd62ccfca0cb6caa8e2a1159754afc5e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1823b8aecd62ccfca0cb6caa8e2a1159754afc5e" target="_blank">LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> SMolInstruct, a large-scale, comprehensive, and high-quality dataset for instruction tuning, is proposed and a set of open-source LLMs are fine-tune, among which, Mistral serves as the best base model for chemistry tasks.</p>
                <p><strong>Paper Abstract:</strong> Chemistry plays a crucial role in many domains, such as drug discovery and material science. While large language models (LLMs) such as GPT-4 exhibit remarkable capabilities on natural language processing tasks, existing research indicates that their performance on chemistry tasks is discouragingly low. In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 and Claude 3 Opus by a substantial margin. To accomplish this, we propose SMolInstruct, a large-scale, comprehensive, and high-quality dataset for instruction tuning. It contains 14 selected chemistry tasks and over three million samples, laying a solid foundation for training and evaluating LLMs for chemistry. Using SMolInstruct, we fine-tune a set of open-source LLMs, among which, we find that Mistral serves as the best base model for chemistry tasks. Our analysis further demonstrates the critical role of the proposed dataset in driving the performance improvements.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3597.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3597.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LlaSMol_Mistral</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LlaSMol (Mistral-based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Mistral-7B large language model fine-tuned on the SMolInstruct instruction-tuning dataset using LoRA to perform a wide range of small-molecule tasks including molecule generation, captioning, forward synthesis and retrosynthesis; shown to be the best-performing LlaSMol variant in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LlaSMol (Mistral-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base: Mistral 7B. Fine-tuned with LoRA applied to all attention and FFN linear layers (lora.r and lora.alpha = 16). Fine-tuning: 3 epochs, 8-bit AdamW optimizer, learning rate 1e-4, cosine scheduler, input length 512. Only a small fraction of parameters were trainable (~41.9M, ~0.58% of model params). Inference used beam search.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct sequence generation of canonical SMILES (canonicalization applied during training and evaluation); models generate SMILES outputs for molecule generation (MG) and product SMILES for forward synthesis (FS) and reactants for retrosynthesis (RS); beam search decoding used. Special tags used to mark SMILES segments in prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Small-molecule generation and design (molecule generation from text descriptions), reaction product prediction (forward synthesis), retrosynthesis planning; domains relevant to drug discovery and related small-molecule applications.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>For molecule generation (MG): Exact Match (EM), Fingerprint Tanimoto Similarity (FTS, Morgan fingerprint), Validity (Valid). For forward synthesis (FS) and retrosynthesis (RS): EM, FTS, Valid. For other tasks (MC) METEOR used. (Validity = percent of syntactically/chemically valid SMILES.)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MG: EM 19.2%, FTS 61.7%, Valid 99.7%. FS: EM 63.3%, FTS 84.9%, Valid 99.8%. RS: EM 32.9%, FTS 70.4%, Valid 100.0%. Outperforms other LLM baselines (GPT-4, Claude 3 Opus, base LLMs, and other chemistry LLMs evaluated) on MG/FS/RS in this work, and narrows gap with task-specific SoTA models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared to SoTA task-specific models: SoTA MG (MolT5) EM 31.7% / FTS 73.2% / Valid 95.3% — LlaSMol_Mistral has lower EM and FTS but higher validity. For FS and RS SoTA models (task-specific transformers) have higher EM (FS SoTA EM 78.7%, RS SoTA EM 47.0%), so LlaSMol_Mistral approaches but does not surpass SoTA on most tasks. Substantially outperforms off-the-shelf LLMs (GPT-4, Claude 3 Opus) and prior chemistry-tuned LLMs (Molinst, ChemLLM) on these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Limitations reported include: evaluation for MG (and MC) cannot fully assess chemical correctness because molecular descriptions are ambiguous and ground-truth data is limited; model has not yet surpassed task-specific SoTA, possibly due to small proportion of trainable parameters or suboptimal training procedures; generalization beyond trained tasks not evaluated; dataset and models may still contain inaccurate/harmful content; canonical SMILES preferred (without canonicalization performance drops); using SELFIES increases validity slightly but worsens overall performance likely due to longer sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3597.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3597.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LlaSMol_Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LlaSMol (Galactica-based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Galactica-6.7B base LLM fine-tuned on SMolInstruct with LoRA to perform molecule generation, property prediction, and reaction tasks; benefits from Galactica's science-focused pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LlaSMol (Galactica-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base: Galactica 6.7B (pretrained on scientific text, exposed to some chemistry data). Fine-tuned with LoRA (same LoRA settings as other LlaSMol variants). Training details: 3 epochs, 8-bit AdamW, lr 1e-4, input length 512. Inference with beam search.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct SMILES generation for molecule generation and reaction prediction; canonical SMILES used; prompts/instruction templates from SMolInstruct.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Small-molecule generation from textual descriptions (MG), forward synthesis and retrosynthesis, molecule captioning — relevant to drug discovery workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MG: EM, FTS, Valid; FS/RS: EM, FTS, Valid; MC: METEOR.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MG: EM 7.7%, FTS 52.2%, Valid 99.6%. FS: EM 53.1%, FTS 79.9%, Valid 99.7%. RS: EM 25.7%, FTS 67.0%, Valid 99.9%. Improves substantially over the Galactica base model without fine-tuning and over off-the-shelf LLMs, but generally underperforms the Mistral-based LlaSMol.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Outperforms the Galactica base model and many off-the-shelf LLMs (GPT-4, Claude 3 Opus on these tasks in this evaluation), but is below SoTA task-specific models (e.g., MolT5 for MG and specialized reaction models for FS/RS).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Same dataset/evaluation limitations as other LlaSMol variants: MG/MC evaluation ambiguity, does not surpass SoTA, limited fraction fine-tuned; Galactica benefits from domain pretraining but overall still limited by instruction-tuning scale used here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3597.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3597.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LlaSMol_Llama2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LlaSMol (Llama 2-based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Llama-2-7B base LLM fine-tuned on SMolInstruct with LoRA for molecular tasks; used as one of the four LlaSMol variants to evaluate base-model influence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LlaSMol (Llama 2-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base: Llama 2 (7B). Fine-tuned with LoRA (lora.r and lora.alpha = 16) on SMolInstruct for 3 epochs with 8-bit AdamW and lr 1e-4. Inference via beam search. ~same LoRA settings as other LlaSMol variants.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct canonical SMILES generation for MG and reaction tasks; instruction-following via SMolInstruct templates; beam search decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Small-molecule generation (MG), reaction prediction (FS/RS), molecule captioning — tasks relevant to drug discovery and chemistry.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MG: EM, FTS, Valid; FS/RS: EM, FTS, Valid; MC: METEOR.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MG: EM 6.4%, FTS 47.1%, Valid 99.6%. FS: EM 47.1%, FTS 76.9%, Valid 99.8%. RS: EM 22.5%, FTS 65.2%, Valid 99.9%. Performance improved relative to base Llama 2 but lower than Mistral-based and Galactica-based LlaSMol on many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Outperforms Llama-2 base and some off-the-shelf LLMs but underperforms the Mistral-based LlaSMol and task-specific SoTA models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Same reported limitations: constrained by small fine-tuning ratio and dataset ambiguities for MG/MC; lower performance suggests base model choice strongly affects chemistry downstream capability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3597.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3597.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LlaSMol_CodeLlama</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LlaSMol (Code Llama-based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Code Llama 7B base LLM (code-oriented variant of Llama 2) fine-tuned on SMolInstruct with LoRA to perform molecule generation and reaction tasks; evaluated to probe synergy between code knowledge and molecular representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LlaSMol (Code Llama-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base: Code Llama 7B. Fine-tuned with LoRA (same LoRA settings) on SMolInstruct for 3 epochs, 8-bit AdamW optimizer, lr 1e-4. Inference via beam search.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct canonical SMILES generation for MG, FS, RS using instruction templates; beam search decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule generation from text, reaction prediction, molecule captioning — relevant to drug discovery and chemical informatics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MG: EM, FTS, Valid; FS/RS: EM, FTS, Valid; MC: METEOR.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MG: EM 6.5%, FTS 46.6%, Valid 99.7%. FS: EM 52.0%, FTS 79.2%, Valid 99.8%. RS: EM 25.7%, FTS 66.7%, Valid 100.0%. Shows better performance than LlaSMol_Llama2 in many tasks, suggesting potential benefit from code-pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Better than base Code Llama and many off-the-shelf LLMs evaluated in this paper, but lower than Mistral-based LlaSMol and SoTA task-specific models in most metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Same general limitations: limited fine-tuning fraction, evaluation ambiguity for MG/MC, and not yet surpassing SoTA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3597.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3597.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (gpt-4-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large closed-source transformer LLM from OpenAI evaluated zero-shot (best-performing setting in the paper) on SMolInstruct tasks using structured prompt templates with in-context examples removed for 0-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source large language model from OpenAI (versioned gpt-4-0613 in experiments). Evaluated via OpenAI API with carefully designed templates containing task instructions; used 0-shot as that setting yielded the best results in this evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt engineering with structured templates (general description, task-specific format, optional ICL examples). For molecule/reaction generation, GPT-4 produced SMILES outputs in response to prompts; default API generation settings were used.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Evaluated for molecule generation (MG), molecule captioning (MC), forward synthesis (FS), retrosynthesis (RS), and property/name conversion tasks — demonstration of LLM capability on chemistry generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same metrics as LlaSMol for the respective tasks (EM, FTS, Valid for MG/FS/RS; METEOR for MC; RMSE/Acc for property tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MG: EM 6.4%, FTS 42.6%, Valid 81.4%. FS: EM 1.6%, FTS 40.5%, Valid 87.0%. RS: EM 0.0%, FTS 33.4%, Valid 42.6%. Overall much weaker than LlaSMol variants on these tasks in this evaluation; often produced chemically implausible or invalid outputs in zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Underperformed the fine-tuned LlaSMol models and many task-specific SoTA models on chemical generation tasks in this paper; better than some base open-source LLMs for certain property predictions but generally not competitive on precise SMILES-manipulation tasks without fine-tuning/instruction-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Poor performance on tasks requiring precise SMILES understanding; lower validity of generated SMILES for MG/FS/RS; needs careful prompt engineering and/or fine-tuning for chemistry tasks; evaluated only on up to 500 samples due to resource limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3597.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3597.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude3_Opus</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3 Opus</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Anthropic's Claude 3 Opus model evaluated zero-shot on SMolInstruct tasks using the same structured templates as GPT-4; included as a SoTA LLM baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The claude 3 model family: Opus, sonnet, haiku</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 3 Opus</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source large language model from Anthropic (Claude 3 Opus). Evaluated via Anthropic API in zero-shot using the same prompt formats as used for GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt engineering with structured templates; generated SMILES for chemical generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Evaluated on molecule generation (MG), reaction prediction (FS/RS), and other SMolInstruct tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MG: EM, FTS, Valid; FS/RS: EM, FTS, Valid; other standard metrics as used in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MG: EM 12.3%, FTS 57.6%, Valid 92.6%. FS: EM 3.7%, FTS 45.7%, Valid 97.0%. RS: EM 1.1%, FTS 46.2%, Valid 94.8%. Better than GPT-4 on many tasks in this evaluation, but still substantially below fine-tuned LlaSMol models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Outperforms GPT-4 on several metrics in this evaluation but is outperformed by LlaSMol models fine-tuned on SMolInstruct and by SoTA task-specific models for many generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Also limited in precise SMILES-oriented tasks without fine-tuning; zero-shot outputs sometimes require heavy post-processing to extract valid SMILES; evaluated on at most 500 samples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3597.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3597.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolInst (Mol-Instructions tuned LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Molinst (Llama 2 fine-tuned on Mol-Instructions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Llama 2-based model tuned on the Mol-Instructions dataset (Fang et al., 2023) used as a prior chemistry instruction-tuned LLM baseline in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mol-instructions: A large-scale biomolecular instruction dataset for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Molinst (Llama 2 tuned on Mol-Instructions)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A Llama 2 variant instruction-tuned on the Mol-Instructions dataset (1.3M instructions across biomolecular tasks). Evaluated in this paper as a baseline for tasks overlapping with SMolInstruct (MC, MG, FS, RS).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Instruction-tuned generation producing SMILES for MG/FS/RS tasks, following Mol-Instructions training; used in comparison to LlaSMol trained on SMolInstruct.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule generation from description (MG), molecule captioning, and reaction prediction (FS/RS).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MG: EM, FTS, Valid; FS/RS: EM, FTS, Valid (same metrics used for comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MG: EM 6.0%, FTS 43.6%, Valid 84.8%. FS: EM 2.1%, FTS 31.7%, Valid 99.8%. RS: EM 5.7%, FTS 48.0%, Valid 97.8%. Performance substantially worse than LlaSMol models trained on SMolInstruct (especially LlaSMol_Mistral), indicating SMolInstruct provides superior training signal.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Molinst is a direct comparator that used a previous instruction dataset; LlaSMol trained on SMolInstruct substantially outperformed Molinst on shared tasks in the authors' experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Mol-Instructions appears to be lower quality or less comprehensive for these molecule-oriented tasks compared to SMolInstruct; Molinst produced lower validity and lower EM/FTS for generation tasks in this evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3597.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3597.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemLLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recently proposed chemical LLM (Zhang et al., 2024) evaluated as a baseline in this paper; dataset and evaluation details were limited so only limited analysis possible in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chemllm: A chemical large language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemLLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A chemistry-focused LLM proposed concurrently; included as a comparative chemistry LLM baseline. Paper notes dataset/evaluation details for ChemLLM were not fully available to authors.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Reportedly instruction/fine-tuned for chemistry tasks (exact fine-tuning details not provided in this paper). Evaluated here on SMolInstruct tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemistry tasks including molecule generation and property/reaction tasks as part of comparison suite.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MG: EM, FTS, Valid; FS/RS: EM, FTS, Valid; other standard metrics used in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MG: EM 0.9%, FTS 14.3%, Valid 4.3% (very low validity reported in table). Other task performances varied; overall ChemLLM underperformed LlaSMol variants on many tasks in this evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Performed worse than the LlaSMol models in this study on many metrics. Authors could not deeply analyze ChemLLM because of lack of publicly available dataset/evaluation details.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Low reported validity on MG suggests training/evaluation mismatch or different output formatting; lack of public dataset/evaluation details prevented deeper diagnostic analysis by the paper's authors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3597.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3597.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolT5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolT5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A T5-style model pretrained on SMILES and natural language and fine-tuned for molecule captioning and generation; used as a task-specific SoTA baseline for MG and MC in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Translation between molecules and natural language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolT5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A sequence-to-sequence T5-style model pretrained on SMILES and natural language corpora and fine-tuned for molecule ↔ text translation tasks (MolT5 authors' released checkpoint used in comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Seq2seq SMILES ↔ natural-language translation (for MG and MC); generates SMILES sequences for MG task.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule generation from textual descriptions and molecule captioning (bridging natural language and molecular representations).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MG: EM, FTS, Valid; MC: METEOR, EM; used as SoTA reference for generation/captioning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>As reported in the paper, SoTA (MolT5) MG: EM 31.7%, FTS 73.2%, Valid 95.3% — higher EM/FTS than LlaSMol variants but lower validity in this comparison than LlaSMol_Mistral.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>MolT5 is stronger on EM and FTS for MG/MC tasks (SoTA task-specific model) than the LlaSMol series in this study, demonstrating that task-specific pretraining/fine-tuning still yields best absolute performance on these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Authors of this paper noted inability to fully reproduce MolT5 training and ensured that MolT5 training samples were not in their test set; MolT5 remains a strong task-specific baseline but LlaSMol narrows the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mol-instructions: A large-scale biomolecular instruction dataset for large language models <em>(Rating: 2)</em></li>
                <li>What can large language models do in chemistry? a comprehensive benchmark on eight tasks <em>(Rating: 2)</em></li>
                <li>Translation between molecules and natural language <em>(Rating: 2)</em></li>
                <li>Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction <em>(Rating: 2)</em></li>
                <li>Chemllm: A chemical large language model <em>(Rating: 2)</em></li>
                <li>Selfies: a robust representation of semantically constrained graphs with an example application in chemistry <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3597",
    "paper_id": "paper-1823b8aecd62ccfca0cb6caa8e2a1159754afc5e",
    "extraction_schema_id": "extraction-schema-79",
    "extracted_data": [
        {
            "name_short": "LlaSMol_Mistral",
            "name_full": "LlaSMol (Mistral-based)",
            "brief_description": "A Mistral-7B large language model fine-tuned on the SMolInstruct instruction-tuning dataset using LoRA to perform a wide range of small-molecule tasks including molecule generation, captioning, forward synthesis and retrosynthesis; shown to be the best-performing LlaSMol variant in this work.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LlaSMol (Mistral-based)",
            "model_description": "Base: Mistral 7B. Fine-tuned with LoRA applied to all attention and FFN linear layers (lora.r and lora.alpha = 16). Fine-tuning: 3 epochs, 8-bit AdamW optimizer, learning rate 1e-4, cosine scheduler, input length 512. Only a small fraction of parameters were trainable (~41.9M, ~0.58% of model params). Inference used beam search.",
            "generation_method": "Direct sequence generation of canonical SMILES (canonicalization applied during training and evaluation); models generate SMILES outputs for molecule generation (MG) and product SMILES for forward synthesis (FS) and reactants for retrosynthesis (RS); beam search decoding used. Special tags used to mark SMILES segments in prompts.",
            "application_domain": "Small-molecule generation and design (molecule generation from text descriptions), reaction product prediction (forward synthesis), retrosynthesis planning; domains relevant to drug discovery and related small-molecule applications.",
            "evaluation_metrics": "For molecule generation (MG): Exact Match (EM), Fingerprint Tanimoto Similarity (FTS, Morgan fingerprint), Validity (Valid). For forward synthesis (FS) and retrosynthesis (RS): EM, FTS, Valid. For other tasks (MC) METEOR used. (Validity = percent of syntactically/chemically valid SMILES.)",
            "results_summary": "MG: EM 19.2%, FTS 61.7%, Valid 99.7%. FS: EM 63.3%, FTS 84.9%, Valid 99.8%. RS: EM 32.9%, FTS 70.4%, Valid 100.0%. Outperforms other LLM baselines (GPT-4, Claude 3 Opus, base LLMs, and other chemistry LLMs evaluated) on MG/FS/RS in this work, and narrows gap with task-specific SoTA models.",
            "comparison_to_baselines": "Compared to SoTA task-specific models: SoTA MG (MolT5) EM 31.7% / FTS 73.2% / Valid 95.3% — LlaSMol_Mistral has lower EM and FTS but higher validity. For FS and RS SoTA models (task-specific transformers) have higher EM (FS SoTA EM 78.7%, RS SoTA EM 47.0%), so LlaSMol_Mistral approaches but does not surpass SoTA on most tasks. Substantially outperforms off-the-shelf LLMs (GPT-4, Claude 3 Opus) and prior chemistry-tuned LLMs (Molinst, ChemLLM) on these tasks.",
            "limitations_challenges": "Limitations reported include: evaluation for MG (and MC) cannot fully assess chemical correctness because molecular descriptions are ambiguous and ground-truth data is limited; model has not yet surpassed task-specific SoTA, possibly due to small proportion of trainable parameters or suboptimal training procedures; generalization beyond trained tasks not evaluated; dataset and models may still contain inaccurate/harmful content; canonical SMILES preferred (without canonicalization performance drops); using SELFIES increases validity slightly but worsens overall performance likely due to longer sequences.",
            "uuid": "e3597.0",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LlaSMol_Galactica",
            "name_full": "LlaSMol (Galactica-based)",
            "brief_description": "A Galactica-6.7B base LLM fine-tuned on SMolInstruct with LoRA to perform molecule generation, property prediction, and reaction tasks; benefits from Galactica's science-focused pretraining.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LlaSMol (Galactica-based)",
            "model_description": "Base: Galactica 6.7B (pretrained on scientific text, exposed to some chemistry data). Fine-tuned with LoRA (same LoRA settings as other LlaSMol variants). Training details: 3 epochs, 8-bit AdamW, lr 1e-4, input length 512. Inference with beam search.",
            "generation_method": "Direct SMILES generation for molecule generation and reaction prediction; canonical SMILES used; prompts/instruction templates from SMolInstruct.",
            "application_domain": "Small-molecule generation from textual descriptions (MG), forward synthesis and retrosynthesis, molecule captioning — relevant to drug discovery workflows.",
            "evaluation_metrics": "MG: EM, FTS, Valid; FS/RS: EM, FTS, Valid; MC: METEOR.",
            "results_summary": "MG: EM 7.7%, FTS 52.2%, Valid 99.6%. FS: EM 53.1%, FTS 79.9%, Valid 99.7%. RS: EM 25.7%, FTS 67.0%, Valid 99.9%. Improves substantially over the Galactica base model without fine-tuning and over off-the-shelf LLMs, but generally underperforms the Mistral-based LlaSMol.",
            "comparison_to_baselines": "Outperforms the Galactica base model and many off-the-shelf LLMs (GPT-4, Claude 3 Opus on these tasks in this evaluation), but is below SoTA task-specific models (e.g., MolT5 for MG and specialized reaction models for FS/RS).",
            "limitations_challenges": "Same dataset/evaluation limitations as other LlaSMol variants: MG/MC evaluation ambiguity, does not surpass SoTA, limited fraction fine-tuned; Galactica benefits from domain pretraining but overall still limited by instruction-tuning scale used here.",
            "uuid": "e3597.1",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LlaSMol_Llama2",
            "name_full": "LlaSMol (Llama 2-based)",
            "brief_description": "A Llama-2-7B base LLM fine-tuned on SMolInstruct with LoRA for molecular tasks; used as one of the four LlaSMol variants to evaluate base-model influence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LlaSMol (Llama 2-based)",
            "model_description": "Base: Llama 2 (7B). Fine-tuned with LoRA (lora.r and lora.alpha = 16) on SMolInstruct for 3 epochs with 8-bit AdamW and lr 1e-4. Inference via beam search. ~same LoRA settings as other LlaSMol variants.",
            "generation_method": "Direct canonical SMILES generation for MG and reaction tasks; instruction-following via SMolInstruct templates; beam search decoding.",
            "application_domain": "Small-molecule generation (MG), reaction prediction (FS/RS), molecule captioning — tasks relevant to drug discovery and chemistry.",
            "evaluation_metrics": "MG: EM, FTS, Valid; FS/RS: EM, FTS, Valid; MC: METEOR.",
            "results_summary": "MG: EM 6.4%, FTS 47.1%, Valid 99.6%. FS: EM 47.1%, FTS 76.9%, Valid 99.8%. RS: EM 22.5%, FTS 65.2%, Valid 99.9%. Performance improved relative to base Llama 2 but lower than Mistral-based and Galactica-based LlaSMol on many tasks.",
            "comparison_to_baselines": "Outperforms Llama-2 base and some off-the-shelf LLMs but underperforms the Mistral-based LlaSMol and task-specific SoTA models.",
            "limitations_challenges": "Same reported limitations: constrained by small fine-tuning ratio and dataset ambiguities for MG/MC; lower performance suggests base model choice strongly affects chemistry downstream capability.",
            "uuid": "e3597.2",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LlaSMol_CodeLlama",
            "name_full": "LlaSMol (Code Llama-based)",
            "brief_description": "A Code Llama 7B base LLM (code-oriented variant of Llama 2) fine-tuned on SMolInstruct with LoRA to perform molecule generation and reaction tasks; evaluated to probe synergy between code knowledge and molecular representations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LlaSMol (Code Llama-based)",
            "model_description": "Base: Code Llama 7B. Fine-tuned with LoRA (same LoRA settings) on SMolInstruct for 3 epochs, 8-bit AdamW optimizer, lr 1e-4. Inference via beam search.",
            "generation_method": "Direct canonical SMILES generation for MG, FS, RS using instruction templates; beam search decoding.",
            "application_domain": "Molecule generation from text, reaction prediction, molecule captioning — relevant to drug discovery and chemical informatics.",
            "evaluation_metrics": "MG: EM, FTS, Valid; FS/RS: EM, FTS, Valid; MC: METEOR.",
            "results_summary": "MG: EM 6.5%, FTS 46.6%, Valid 99.7%. FS: EM 52.0%, FTS 79.2%, Valid 99.8%. RS: EM 25.7%, FTS 66.7%, Valid 100.0%. Shows better performance than LlaSMol_Llama2 in many tasks, suggesting potential benefit from code-pretraining.",
            "comparison_to_baselines": "Better than base Code Llama and many off-the-shelf LLMs evaluated in this paper, but lower than Mistral-based LlaSMol and SoTA task-specific models in most metrics.",
            "limitations_challenges": "Same general limitations: limited fine-tuning fraction, evaluation ambiguity for MG/MC, and not yet surpassing SoTA.",
            "uuid": "e3597.3",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (gpt-4-0613)",
            "brief_description": "A large closed-source transformer LLM from OpenAI evaluated zero-shot (best-performing setting in the paper) on SMolInstruct tasks using structured prompt templates with in-context examples removed for 0-shot.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-0613)",
            "model_description": "Closed-source large language model from OpenAI (versioned gpt-4-0613 in experiments). Evaluated via OpenAI API with carefully designed templates containing task instructions; used 0-shot as that setting yielded the best results in this evaluation.",
            "generation_method": "Prompt engineering with structured templates (general description, task-specific format, optional ICL examples). For molecule/reaction generation, GPT-4 produced SMILES outputs in response to prompts; default API generation settings were used.",
            "application_domain": "Evaluated for molecule generation (MG), molecule captioning (MC), forward synthesis (FS), retrosynthesis (RS), and property/name conversion tasks — demonstration of LLM capability on chemistry generation tasks.",
            "evaluation_metrics": "Same metrics as LlaSMol for the respective tasks (EM, FTS, Valid for MG/FS/RS; METEOR for MC; RMSE/Acc for property tasks).",
            "results_summary": "MG: EM 6.4%, FTS 42.6%, Valid 81.4%. FS: EM 1.6%, FTS 40.5%, Valid 87.0%. RS: EM 0.0%, FTS 33.4%, Valid 42.6%. Overall much weaker than LlaSMol variants on these tasks in this evaluation; often produced chemically implausible or invalid outputs in zero-shot.",
            "comparison_to_baselines": "Underperformed the fine-tuned LlaSMol models and many task-specific SoTA models on chemical generation tasks in this paper; better than some base open-source LLMs for certain property predictions but generally not competitive on precise SMILES-manipulation tasks without fine-tuning/instruction-tuning.",
            "limitations_challenges": "Poor performance on tasks requiring precise SMILES understanding; lower validity of generated SMILES for MG/FS/RS; needs careful prompt engineering and/or fine-tuning for chemistry tasks; evaluated only on up to 500 samples due to resource limits.",
            "uuid": "e3597.4",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Claude3_Opus",
            "name_full": "Claude 3 Opus",
            "brief_description": "Anthropic's Claude 3 Opus model evaluated zero-shot on SMolInstruct tasks using the same structured templates as GPT-4; included as a SoTA LLM baseline in experiments.",
            "citation_title": "The claude 3 model family: Opus, sonnet, haiku",
            "mention_or_use": "use",
            "model_name": "Claude 3 Opus",
            "model_description": "Closed-source large language model from Anthropic (Claude 3 Opus). Evaluated via Anthropic API in zero-shot using the same prompt formats as used for GPT-4.",
            "generation_method": "Prompt engineering with structured templates; generated SMILES for chemical generation tasks.",
            "application_domain": "Evaluated on molecule generation (MG), reaction prediction (FS/RS), and other SMolInstruct tasks.",
            "evaluation_metrics": "MG: EM, FTS, Valid; FS/RS: EM, FTS, Valid; other standard metrics as used in the paper.",
            "results_summary": "MG: EM 12.3%, FTS 57.6%, Valid 92.6%. FS: EM 3.7%, FTS 45.7%, Valid 97.0%. RS: EM 1.1%, FTS 46.2%, Valid 94.8%. Better than GPT-4 on many tasks in this evaluation, but still substantially below fine-tuned LlaSMol models.",
            "comparison_to_baselines": "Outperforms GPT-4 on several metrics in this evaluation but is outperformed by LlaSMol models fine-tuned on SMolInstruct and by SoTA task-specific models for many generation tasks.",
            "limitations_challenges": "Also limited in precise SMILES-oriented tasks without fine-tuning; zero-shot outputs sometimes require heavy post-processing to extract valid SMILES; evaluated on at most 500 samples.",
            "uuid": "e3597.5",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "MolInst (Mol-Instructions tuned LLM)",
            "name_full": "Molinst (Llama 2 fine-tuned on Mol-Instructions)",
            "brief_description": "A Llama 2-based model tuned on the Mol-Instructions dataset (Fang et al., 2023) used as a prior chemistry instruction-tuned LLM baseline in comparisons.",
            "citation_title": "Mol-instructions: A large-scale biomolecular instruction dataset for large language models",
            "mention_or_use": "use",
            "model_name": "Molinst (Llama 2 tuned on Mol-Instructions)",
            "model_description": "A Llama 2 variant instruction-tuned on the Mol-Instructions dataset (1.3M instructions across biomolecular tasks). Evaluated in this paper as a baseline for tasks overlapping with SMolInstruct (MC, MG, FS, RS).",
            "generation_method": "Instruction-tuned generation producing SMILES for MG/FS/RS tasks, following Mol-Instructions training; used in comparison to LlaSMol trained on SMolInstruct.",
            "application_domain": "Molecule generation from description (MG), molecule captioning, and reaction prediction (FS/RS).",
            "evaluation_metrics": "MG: EM, FTS, Valid; FS/RS: EM, FTS, Valid (same metrics used for comparison).",
            "results_summary": "MG: EM 6.0%, FTS 43.6%, Valid 84.8%. FS: EM 2.1%, FTS 31.7%, Valid 99.8%. RS: EM 5.7%, FTS 48.0%, Valid 97.8%. Performance substantially worse than LlaSMol models trained on SMolInstruct (especially LlaSMol_Mistral), indicating SMolInstruct provides superior training signal.",
            "comparison_to_baselines": "Molinst is a direct comparator that used a previous instruction dataset; LlaSMol trained on SMolInstruct substantially outperformed Molinst on shared tasks in the authors' experiments.",
            "limitations_challenges": "Mol-Instructions appears to be lower quality or less comprehensive for these molecule-oriented tasks compared to SMolInstruct; Molinst produced lower validity and lower EM/FTS for generation tasks in this evaluation.",
            "uuid": "e3597.6",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ChemLLM",
            "name_full": "ChemLLM",
            "brief_description": "A recently proposed chemical LLM (Zhang et al., 2024) evaluated as a baseline in this paper; dataset and evaluation details were limited so only limited analysis possible in the paper.",
            "citation_title": "Chemllm: A chemical large language model",
            "mention_or_use": "use",
            "model_name": "ChemLLM",
            "model_description": "A chemistry-focused LLM proposed concurrently; included as a comparative chemistry LLM baseline. Paper notes dataset/evaluation details for ChemLLM were not fully available to authors.",
            "generation_method": "Reportedly instruction/fine-tuned for chemistry tasks (exact fine-tuning details not provided in this paper). Evaluated here on SMolInstruct tasks.",
            "application_domain": "Chemistry tasks including molecule generation and property/reaction tasks as part of comparison suite.",
            "evaluation_metrics": "MG: EM, FTS, Valid; FS/RS: EM, FTS, Valid; other standard metrics used in the paper.",
            "results_summary": "MG: EM 0.9%, FTS 14.3%, Valid 4.3% (very low validity reported in table). Other task performances varied; overall ChemLLM underperformed LlaSMol variants on many tasks in this evaluation.",
            "comparison_to_baselines": "Performed worse than the LlaSMol models in this study on many metrics. Authors could not deeply analyze ChemLLM because of lack of publicly available dataset/evaluation details.",
            "limitations_challenges": "Low reported validity on MG suggests training/evaluation mismatch or different output formatting; lack of public dataset/evaluation details prevented deeper diagnostic analysis by the paper's authors.",
            "uuid": "e3597.7",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "MolT5",
            "name_full": "MolT5",
            "brief_description": "A T5-style model pretrained on SMILES and natural language and fine-tuned for molecule captioning and generation; used as a task-specific SoTA baseline for MG and MC in this paper.",
            "citation_title": "Translation between molecules and natural language",
            "mention_or_use": "use",
            "model_name": "MolT5",
            "model_description": "A sequence-to-sequence T5-style model pretrained on SMILES and natural language corpora and fine-tuned for molecule ↔ text translation tasks (MolT5 authors' released checkpoint used in comparisons).",
            "generation_method": "Seq2seq SMILES ↔ natural-language translation (for MG and MC); generates SMILES sequences for MG task.",
            "application_domain": "Molecule generation from textual descriptions and molecule captioning (bridging natural language and molecular representations).",
            "evaluation_metrics": "MG: EM, FTS, Valid; MC: METEOR, EM; used as SoTA reference for generation/captioning tasks.",
            "results_summary": "As reported in the paper, SoTA (MolT5) MG: EM 31.7%, FTS 73.2%, Valid 95.3% — higher EM/FTS than LlaSMol variants but lower validity in this comparison than LlaSMol_Mistral.",
            "comparison_to_baselines": "MolT5 is stronger on EM and FTS for MG/MC tasks (SoTA task-specific model) than the LlaSMol series in this study, demonstrating that task-specific pretraining/fine-tuning still yields best absolute performance on these tasks.",
            "limitations_challenges": "Authors of this paper noted inability to fully reproduce MolT5 training and ensured that MolT5 training samples were not in their test set; MolT5 remains a strong task-specific baseline but LlaSMol narrows the gap.",
            "uuid": "e3597.8",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mol-instructions: A large-scale biomolecular instruction dataset for large language models",
            "rating": 2
        },
        {
            "paper_title": "What can large language models do in chemistry? a comprehensive benchmark on eight tasks",
            "rating": 2
        },
        {
            "paper_title": "Translation between molecules and natural language",
            "rating": 2
        },
        {
            "paper_title": "Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction",
            "rating": 2
        },
        {
            "paper_title": "Chemllm: A chemical large language model",
            "rating": 2
        },
        {
            "paper_title": "Selfies: a robust representation of semantically constrained graphs with an example application in chemistry",
            "rating": 1
        }
    ],
    "cost": 0.02125225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset</h1>
<p>Botao Yu Frazier N. Baker<em> ${ }^{</em>}$ Ziqi Chen* Xia Ning Huan Sun<br>The Ohio State University<br>Columbus, OH 43210, USA<br>{yu.3737, baker.3239, chen.8484, ning.104, sun.397}@osu.edu</p>
<h4>Abstract</h4>
<p>Chemistry plays a crucial role in many domains, such as drug discovery and material science. While large language models (LLMs) such as GPT4 exhibit remarkable capabilities on natural language processing tasks, existing research indicates that their performance on chemistry tasks is discouragingly low. In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 and Claude 3 Opus by a substantial margin. To accomplish this, we propose SMolInstruct, a large-scale, comprehensive, and high-quality dataset for instruction tuning. It contains 14 selected chemistry tasks and over three million samples, laying a solid foundation for training and evaluating LLMs for chemistry. Using SMolInstruct, we fine-tune a set of open-source LLMs named as LlaSMol, among which, we find that Mistral serves as the best base model for chemistry tasks. Our analysis further demonstrates the critical role of the proposed dataset in driving the performance improvements. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Chemistry is a fundamental science that underpins countless aspects of modern life, ranging from drug discovery and materials science to energy production. To facilitate research and applications in this domain, deep learning models including graph neural networks (Kipf \&amp; Welling, 2017) and Transformer-based models (Vaswani et al., 2017) have been developed for various chemistry tasks such as forward reaction prediction, retrosynthesis, property prediction (Schwaller et al., 2019; Zhong et al., 2022; Chen et al., 2023; Zhou et al., 2023). However, these models are usually task-specific models, which neglect shared chemistry knowledge across tasks and can hardly be adapted to different tasks.
On the other hand, large language models (LLMs) such as GPT-4 (OpenAI, 2023), Llama series (Touvron et al., 2023a,b), and Mistral (Jiang et al., 2023) have emerged as general-purpose foundation models and demonstrate remarkable abilities on various natural language processing tasks (Chang et al., 2024; Thirunavukarasu et al., 2023; Yue et al., 2023; Zhang et al., 2023; Deng et al., 2023). However, when applied to chemistry tasks, LLMs show only limited capabilities (Jablonka et al., 2022; Guo et al., 2023; Hatakeyama-Sato et al., 2023). For example, Guo et al. (2023) conducted evaluations on eight chemistry tasks and observed that while GPT-4 outperforms other closed- and open-source LLMs, its performance is far from that of task-specific deep learning models. Particularly, they found that GPT models perform poorly when a precise understanding of SMILES (Weininger, 1988), a widely used textual representation for molecules, is required. In addition to directly applying pretrained LLMs, Fang et al. (2023) fine-tuned LLMs on an instruction tuning dataset, but their performance remains very low, far behind the state-of-the-art (SoTA) models designed and trained for specific tasks.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An overview of tasks in the proposed SMolInstruct dataset.</p>
<p>Given these discouraging results, some critical questions arise: Are LLMs actually able to effectively perform chemistry tasks? Or, Are they fundamentally limited for chemistry? In this paper, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, substantially outperforming the most advanced GPT-4 OpenAI (2023) and Claude 3 Opus Anthropic (2024).</p>
<p>What makes such LLMs possible? First, we construct a large-scale, comprehensive, and high-quality dataset for instruction tuning named SMolInstruct. We incorporate tasks with meaningful applications, collect data from diverse data sources, and apply rigorous scrutiny for quality control. The resulting dataset consists of 14 tasks (illustrated in Figure 1) and over 3M samples, laying a solid foundation for training and evaluating LLMs for chemistry tasks. Based on the dataset, we build a series of LLMs for chemistry named LlaSMol by fine-tuning four open-source LLMs namely Galactica, Llama 2, Code Llama, and Mistral, on SMolInstruct with LoRA (Hu et al., 2022).</p>
<p>We conduct comprehensive experiments to evaluate our models and explore their insights, yielding some interesting findings. Firstly, among the four LlaSMol models, the Mistral-based model surpasses others by a substantial margin, showcasing the considerable influence of base models on downstream chemistry tasks. Moreover, contrast to claims made in previous work (Fang et al., 2023), using SMILES as the molecular representation achieves sufficient validity of generated molecules and better performance compared to using SELFIES (Krenn et al., 2019). Furthermore, employing canonicalized SMILES during model training and applications can alleviate learning burdens and increase performance. Finally, while instruction tuning can inject chemistry task-related knowledge into models, the dataset plays a crucial role. Our experiments demonstrate that training on our SMolInstruct leads to substantially better performance compared to training on previous dataset, emphasizing the contribution of the proposed dataset. Although LlaSMol models do not yet surpass state-of-the-art (SoTA) task-specific models that are designed and trained specifically for each individual task, they approach SoTA performance with only 0.58% of parameters being fine-tuned, suggesting their great potential for further improvements and to serve as strong foundation models for the field.</p>
<h1>2 Related Work</h1>
<p>Task-specific Models for Chemistry. In recent years, many deep learning models have been developed to tackle different chemistry tasks. For example, Molecular Transformer Schwaller et al. (2019) and RSMILES Zhong et al. (2022) formulate forward synthesis and retrosynthesis prediction as sequence-to-sequence translation problems. Chemformer Irwin et al. (2022) pretrains a transformer model on a large-scale SMILES dataset and fine-tunes it for various downstream tasks, such as forward synthesis and property prediction. MolT5 Edwards et al. (2022) first pretrains a T5 model on both SMILES and natural language, and then fine-tunes it to translate SMILES into natural language (i.e., molecule captioning) or vice versa (i.e., molecule generation). Graph neural networks (GNNs), which directly leverage the graph structure of the molecule Wang et al. (2023), have also shown promise in many chemistry applications, such as property prediction Yang et al. (2019); Han et al. (2023), retrosynthesis Chen et al. (2023); Somnath et al. (2021), and molecule optimization Chen et al. (2021); Zhang et al. (2022b). Recent studies Zhou et al. (2023); Zhang et al. (2022a) have shown the promise of leveraging equivariant representations of molecular 3D structures for chemistry tasks, such as property prediction Zhou et al. (2023) and docking Zhang et al. (2022a). Uni-Mol Zhou et al. (2023) incorporates this 3D information into the pretraining of a transformer model and fine-tunes it for downstream tasks. Despite their effectiveness, these models operate on single tasks and therefore cannot harness knowledge shared across diverse chemistry tasks like LLMs.
LLMs for Chemistry. Recent efforts have integrated LLMs with chemistry to solve key chemistry problems, which can be divided into two categories: (1) benchmark studies, and (2) fine-tuning LLMs with new datasets. Multiple benchmark studies White et al. (2023); Guo et al. (2023); Jablonka et al. (2023); Liu et al. (2023a) have evaluated the capabilities and limitations of different off-the-shelf LLMs, such as GPT-4 and Llama, on chemistry problems. For example, Guo et al. (2023) finds that these LLMs do not perform well on chemistry tasks and often produce chemically implausible outputs. These findings highlight the need for further efforts to improve LLMs via fine-tuning for chemistry tasks.
To improve LLMs for chemistry, multiple instruction tuning datasets have been developed. Mol-Instructions Fang et al. (2023) consists of 1.3M instructions for multiple small molecule tasks. However, fine-tuning on the dataset does not significantly improve LLMs' performance (Section 4.3). Drugchat Liang et al. (2023) collects an instruction tuning dataset on drug properties with 10.8 K drug molecules. MolOpt-Instructions Ye et al. (2023) consists of instructions with 1 M molecule pairs for molecule optimization on six properties, in which each pair has similar molecules with different properties. Recent works also develop 2D or 3D molecular graph-centric datasets and integrate the graph understanding ability into LLMs Liu et al. (2023b); Cao et al. (2023); Li et al. (2024). Compared with these datasets, SMolInstruct is much larger and covers a more diverse and comprehensive set of chemistry tasks, which enables LLMs to better understand molecule representations and learn chemistry knowledge across tasks.</p>
<h2>3 SMolInstruct</h2>
<p>This section introduces our proposed dataset SMolInstruct and its construction. Readers may refer to Appendix A for preliminaries and background.</p>
<h3>3.1 Overview of SMolInstruct</h3>
<p>SMolInstruct is a large-scale instruction tuning dataset that centers around small molecules. It contains 14 chemistry tasks, illustrated in Figure 1.
(1) We include four name conversion tasks, namely converting IUPAC name to molecular formula (NC-I2F), converting IUPAC name to SMILES (NC-I2S), converting SMILES to molecular formula (NC-S2F), and converting SMILES to IUPAC name (NC-S2I). They are designed to enable deep understanding of molecular structures and representations, which should serve as the fundamental knowledge for chemistry LLMs.</p>
<p>(2) Additionally, six property prediction tasks (Wu et al., 2018) are integrated, including PPESOL for water solubility (Mobley \&amp; Guthrie, 2014), PP-Lipo for octanol/water distribution coefficient (Poole \&amp; Poole, 2003), PP-BBBP for blood-brain barrier penetration (Martins et al., 2012), PP-ClinTox for toxicity to human body (Gayvert et al., 2016), PP-HIV for HIV replication inhibition (Institute, 2004), and PP-SIDER for side effects of drugs (Kuhn et al., 2015). These involved properties are crucial especially for drug development.
(3) Two tasks focus on the textual descriptions of molecules: molecule captioning (MC) is to generate a textual description of a given molecule, and molecule generation (MG) is to generate a molecule based on the given textual description. They require comprehensive understanding of molecules - their structures and properties, from their textual descriptions. They also bridge the gap between natural language and molecules.
(4) Lastly, two tasks revolve around chemical reaction knowledge. Forward synthesis (FS) aims to predict potential products from reactants and reagents, and retrosynthesis (RS) involves predicting potential reactants given a product. These tasks play vital roles in real-world applications (Coley et al., 2018). For example, retrosynthesis is essential for synthesis planning, while forward synthesis is used to validate retrosynthetic suggestions.
SMolInstruct contains 3.3M samples. Each sample is a query-response pair, where the query describes a task and any task-specific information (e.g., input molecule, textual description, etc.), and the response is a sentence containing the answer to the queried task. For all the tasks, unless explicitly defined in the tasks (NC-I2F, NC-I2S, NC-S2F, and NC-S2I), we use SMILES as the default representation for molecules, but also provide the SELFIES (Krenn et al., 2019) representation.</p>
<h1>3.2 SMolInstruct Construction</h1>
<p>We construct the SMolInstruct dataset by following a four-step pipeline: data collection, quality control, data splitting, and instruction construction.
Data Collection. After consulting domain experts and pinpointing the set of meaningful tasks (summarized in Section 3.1), we collect data for these tasks from various sources, as listed in Table 5. Specifically, for the name conversion tasks (NC-I2F, NC-I2S, NC-S2F, and NC-S2I), we leverage PubChem ${ }^{2}$ (Kim et al., 2019), one of the most comprehensive molecule databases. Within this database, we randomly select a large set of molecule entries, and extract their IUPAC names, SMILES representations, and molecular formulas. This obtained data is then re-organized as input-output pairs for the tasks. For molecular descriptionrelated tasks (MC and MG), we utilize a combination of ChEBI-20 (Edwards et al., 2021; 2022) and Mol-Instructions (Fang et al., 2023), as they both contain high-quality moleculetext paired data. For property prediction tasks (PP-ESOL, PP-Lipo, PP-BBBP, PP-ClinTox, PP-HIV, and PP-SIDER), we employ the well-established MoleculeNet datasets (Wu et al., 2018). We select the 6 datasets from MoleculeNet that represent the essential properties for real-world applications such as drug discovery. For chemical reaction tasks (FS and RS), we collect the reaction data from USPTO-full (Lowe, 2017), which is an extensive collection encompassing over 1 M reaction samples extracted from U.S. patents. All the aforementioned datasets are also widely used in previous studies (He et al., 2021; Zhong et al., 2022; Edwards et al., 2022; Irwin et al., 2022; Chen et al., 2023; Zhou et al., 2023).
Quality Control. To guarantee high quality, we apply rigorous scrutiny. The collected data contains many problematic and low-quality samples, which can be roughly categorized into the following three types, along with our curation methods: (1) Chemically invalid SMILES. Numerous SMILES strings are chemically invalid (e.g., deviating from the SMILES grammar, or violating chemical valence). To address this issue, we employ RDKit (RDKit, 2023), a widely used toolkit for cheminformatics, to parse molecules and detect errors. (2) Wrong or inaccurate information. Based on manual check, we observed wrong and inaccurate information recorded in the data. For instance, within the USPTO-full dataset (Lowe, 2017), we identify and correct mislabeled reactants and reagents in chemical reactions by comparing their atom mappings with products. For the MC and MG tasks, we filter out those</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>textual descriptions that lack pertinent, molecule-specific information, with a set of rules based on wording patterns, lengths and keywords. For PP-SIDER, we eliminate disorders with ambiguous names that could impede the creation of precise and comprehensible instructions. (3) Duplicated samples. We detect and remove them.
Data Splitting. Data splitting for multi-task datasets requires careful handling in order to avoid data leakage across tasks. For instance, FS and RS are a pair of reverse tasks, so data leakage occurs when the training set contains an FS sample for a certain chemical reaction and the test set has an RS sample for the same reaction. This can lead to biased evaluation. Therefore, we identify sample pairs across related tasks (FS and RS, MC and MG, and the four NC tasks) that correspond to the same molecules/reactions, and ensure that matched samples are placed together in either training or evaluation set. Moreover, some samples may share the same input but have different outputs. For instance, in the RS task, one product (the same input) may be synthesized from multiple sets of reactants (different outputs). If these samples are placed into both training and test set, it may lead to exaggerated performance. Therefore we ensure that samples with identical inputs are placed together either in or outside of the test set. Additionally, to achieve fair comparisons with Mol-instructions (Fang et al., 2023), for tasks shared between the two datasets (MC, MG, FS, and RS), we ensure that their training examples are not included in the test set of SMolInstruct, allowing for a direct evaluation of their models on our test set. Following these necessary limitations, samples are randomly split into training/validation/test set, except for PP task samples that undergo a scaffold splitting following the canonical method (Wu et al., 2018).
Instruction Creation. To create query-response textual pairs for instruction tuning, we manually craft several templates, each including a query and a corresponding response, and apply GPT-4 to rephrase them. Unlike those in (Fang et al., 2023) which consist of highly formatted queries (containing three explicitly labeled parts namely instruction, input, and output) and answer-only responses (e.g., responses for FS and RS only contain answer SMILES alone, without any natural text), our templates exhibit a more natural and diverse set of formats in both queries and responses, allowing for more variations and naturalness in input-output interactions. Moreover, all the SMILES representations are canonicalized, establishing a standardized data format. In light of the dataset's inclusion of multi-type sequences (SMILES, molecular formula, numbers, etc.) beyond natural language text alone, we utilize special tags to encapsulate corresponding segments (e.g., <SMILES> . . </SMILES> for SMILES, <MOLFORMULA> . . </MOLFORMULA> for molecular formula, <NUMBER>. . </NUMBER> for numbers). This design does not only explicitly inform models about the information types within the tagged content, but also facilitate answer extraction during evaluation.
For more details of dataset construction, please refer to Appendix B.2.</p>
<h1>3.3 Merits of SMolInstruct</h1>
<p>Compared to previous work (Fang et al., 2023; Liang et al., 2023; Ye et al., 2023), SMolInstruct stands out in several key aspects:
(1) Large-Scale. SMolInstruct consists of 3.3M samples and 1.6M distinct molecules, with a diverse range of sizes, structures, and properties (see Appendix B.1), showcasing an extensive coverage of diverse chemical knowledge.
(2) Comprehensive. SMolInstruct contains 4 types of chemical tasks (14 tasks in total), emerging as the most comprehensive instruction tuning dataset for small molecules. Notably, the tasks are meticulously selected to build a strong chemistry foundation model and to adapt to real-world applications.
(3) High-Quality. Rigorous processing steps have been implemented to exclude problematic and low-quality samples. Along with careful data splitting and canonicalization of SMILES representations, SMolInstruct stands as a high-quality resource valuable for future research.
A detailed introduction and statistics of the SMolInstruct dataset can be found in Appendix B. For a comparison with the previous work, Mol-Instructions (Fang et al., 2023), please refer to Appendix C.</p>
<h1>4 Experiments</h1>
<h3>4.1 Our LlaSMol Models</h3>
<p>By fine-tuning base models on the proposed SMolInstruct dataset, we create LLMs capable of performing chemistry tasks, which we name LlaSMol (Large language models on Small Molecules). Specifically, we extensively consider four different LLMs as our base models, namely Galactica 6.7B (Taylor et al., 2022), Llama 2 (Touvron et al., 2023b) 7B, Code Llama (Roziere et al., 2023) 7B, and Mistral (Jiang et al., 2023) 7B, where Galactica is trained for scientific applications and has already been exposed to chemistry-related data during its pretraining, Llama 2 and Mistral are general-purpose LLMs, while Code Llama is based on Llama 2 and trained for code. We conduct instruction tuning on the proposed SMolInstruct dataset, and name the resulting models as LlaSMol ${ }<em 2="2" _Llama="{Llama" _text="\text">{\text {Galactica }}$, LlaSMol ${ }</em>}}$, LlaSMol ${ <em _Mistral="{Mistral" _text="\text">{\text {Code Llama }}$, and LlaSMol ${ }</em>-4$, and a cosine scheduler. The input length for training is set to 512 , which covers $99.7 \%$ of the samples. During inference, we adopt beam search as the generation strategy for simplicity.}}$, respectively. All the LlaSMol models are trained with LoRA (Hu et al., 2022), which is applied to all weight matrices in the self-attention and feedforward neural network (FFN) modules with lora.r and lora.alpha set to 16. The finetuning process utilizes the Huggingface Transformers library (Wolf et al., 2020). Training spans three epochs, employing the 8 -bit AdamW optimizer, a learning rate of $1 \mathrm{e</p>
<h3>4.2 Experimental Setup</h3>
<p>Compared Models. We compare our LlaSMol models with two types of models:
(1) LLMs without fine-tuning on SMolInstruct. This type includes our four base models, namely Galactica (Taylor et al., 2022), Llama 2 (Touvron et al., 2023b), Code Llama (Roziere et al., 2023), Mistral (Jiang et al., 2023). we also benchmark against GPT-4 (OpenAI, 2023) and the more recent Claude 3 Opus (Anthropic, 2024), the current state-of-the-art (SoTA) LLMs ${ }^{3}$. For Llama 2, Code Llama, and Mistral, we use 1-shot, due to their poor instruction following ability; for GPT-4, we report its results under a zero-shot setting, as GPT-4 performs best on this setting in our experiments (Appendix E); for Claude 3 Opus, we report its zero-shot results as well. We also include two LLMs tuned specifically for chemistry tasks: Molinst, a Llama 2 model tuned on the Mol-Instructions dataset by Fang et al. (2023), which shares the training tasks of MC, MG, FS, and RS with LlaSMol; and ChemLLM (Zhang et al., 2024), an LLM for chemistry proposed concurrently to our work.
(2) SoTA task-specific models. To provide a comprehensive view of LlaSMol's performance, we present results from SoTA task-specific models. For NC-I2S and NC-S2I, we compare with STOUT (Rajan et al., 2021), an encoder-decoder model trained on SMILES-IUPAC name paired data. For NC-S2F, a task achievable with a fixed algorithm, we implement a program with RDKit (RDKit, 2023), a widely used Python toolkit for cheminformatics, and report its results. For NC-I2F where no dedicated models exist, we construct a baseline called STOUT+RDKit by aggregating STOUT for I2S conversion and RDKit for S2F conversion. For the PP tasks, our compared model is Uni-Mol (Zhou et al., 2023). It incorporates molecular 3D representations and follows a pretraining and fine-tuning paradigm. Following its original settings, we fine-tune the model on our SMolInstruct dataset with its pretrained checkpoint. In the case of MC and MG, we compare with MolT5 (Edwards et al., 2022) and directly use their released checkpoint. The reasons why we do not use our re-trained model are: (1) we were unable to reproduce results close to those reported in the paper as no original code was provided; and (2) we take great care to ensure that our test set is devoid of training examples used by MolT5, ensuring fairness in the evaluation. Lastly, regarding FS and RS, we re-train RSMILES (Zhong et al., 2022) and Molecular Transformer (Schwaller et al., 2019) for the two tasks, respectively, following their reported settings. Both of the models are transformer encoder-decoder models (Vaswani et al., 2017), specifically adapted for the FS and RS tasks.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Results for name conversion (NC) and property prediction (PP) tasks. Metrics EM, Valid, and Acc are in percentage.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">NC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">I2F</td>
<td style="text-align: center;">I2S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">S2F</td>
<td style="text-align: center;">S2I</td>
<td style="text-align: center;">ESOL</td>
<td style="text-align: center;">Lipo</td>
<td style="text-align: center;">BBBP</td>
<td style="text-align: center;">Clintox</td>
<td style="text-align: center;">HIV</td>
<td style="text-align: center;">SIDER</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">RMSE $\downarrow$</td>
<td style="text-align: center;">RMSE $\downarrow$</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Acc</td>
</tr>
<tr>
<td style="text-align: center;">Task-Specific, Non-LLM Based Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SoTA</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">0.819</td>
<td style="text-align: center;">0.612</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">70.0</td>
</tr>
<tr>
<td style="text-align: center;">Existing LLMs without fine-tuning on SMoIInstruct</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">2.570</td>
<td style="text-align: center;">1.545</td>
<td style="text-align: center;">62.9</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">57.6</td>
</tr>
<tr>
<td style="text-align: center;">Claude 3 Opus</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.036</td>
<td style="text-align: center;">1.194</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">67.0</td>
</tr>
<tr>
<td style="text-align: center;">Galactica</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">4.184</td>
<td style="text-align: center;">2.979</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">68.1</td>
</tr>
<tr>
<td style="text-align: center;">Llama 2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">3.287</td>
<td style="text-align: center;">1.634</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">61.9</td>
</tr>
<tr>
<td style="text-align: center;">Code Llama</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">3.483</td>
<td style="text-align: center;">1.733</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">91.8</td>
<td style="text-align: center;">60.2</td>
</tr>
<tr>
<td style="text-align: center;">Mistral</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">3.079</td>
<td style="text-align: center;">1.730</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">38.1</td>
</tr>
<tr>
<td style="text-align: center;">Molinst (chemistry LLM)</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">2.271</td>
<td style="text-align: center;">1.691</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">52.4</td>
</tr>
<tr>
<td style="text-align: center;">ChemLLM (chemistry LLM)</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.946</td>
<td style="text-align: center;">1.797</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">32.6</td>
</tr>
<tr>
<td style="text-align: center;">Our LlaSMol Series</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LlaSMol $_{\text {Galactica }}$</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">1.959</td>
<td style="text-align: center;">1.213</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">70.1</td>
</tr>
<tr>
<td style="text-align: center;">LlaSMol $_{\text {Llama 2 }}$</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">12.9</td>
<td style="text-align: center;">2.791</td>
<td style="text-align: center;">1.338</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">68.7</td>
</tr>
<tr>
<td style="text-align: center;">LlaSMol $_{\text {Code Llama }}$</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">2.959</td>
<td style="text-align: center;">1.203</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">69.9</td>
</tr>
<tr>
<td style="text-align: center;">LlaSMol $_{\text {Mistral }}$</td>
<td style="text-align: center;">87.9</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">1.150</td>
<td style="text-align: center;">1.010</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">70.7</td>
</tr>
</tbody>
</table>
<p>Evaluation Metrics. We employ metrics commonly used in previous work (Schwaller et al., 2019; Zhong et al., 2022; Fang et al., 2023; Zhou et al., 2023; Chen et al., 2023), which include: (1) Exact Match (EM), indicating the proportion of predicted results that exactly match the gold standards. (2) Fingerprint Tanimoto Similarity (FTS), quantifying structural similarities between molecules using Tanimoto similarities of their Morgan fingerprints (Morgan, 1965). (3) METEOR score, a comprehensive text-based metric considering both exact matches and semantic similarity (Lavie \&amp; Agarwal, 2007) for the MC task. (4) Root Mean Square Error (RMSE), measuring the square root of the average squared differences between predicted and actual values for the PP-ESOL and PP-Lipo tasks (5) Accuracy (Acc), the ratio of correct predictions for the binary classification tasks (PP-BBBP, PP-ClinTox, PP-HIV, and PP-SIDER). (6) Validity (Valid), the ratio of valid predictions following SMILES grammar and chemical valence rules for tasks with SMILES outputs (NC-I2S, MG, FS, and RS). For all the metrics except RMSE, higher values indicate better performance.</p>
<h1>4.3 Main Results</h1>
<p>Table 1 and 2 show the performance on SMoIInstruct. Key observations are as follows:
(1) Among all the LLMs, our LlaSMol models demonstrate the best performance, underscoring the effectiveness of the proposed SMoIInstruct dataset and fine-tuning. Specifically, compared to the base models (Galactica, Llama 2, Code Llama, and Mistral), LlaSMol models exhibit substantial performance improvements, which highlights the effectiveness of SMoIInstruct in enhancing the understanding of molecular representations and the taskrelated knowledge, and signifies the effective learning of chemistry-related tasks by LLMs. Furthermore, LlaSMol substantially outperforms GPT-4 on all the tasks and Claude 3 Opus on most tasks, despite their larger parameter size. LlaSMol also surpasses the two chemistry LLMs namely ChemLLM ${ }^{4}$, which is similarly trained on chemistry instruction data. and Molinst. Notably, LlaSMol $<em _Mistral="{Mistral" _text="\text">{\text {Llama 2 }}$, which uses the same base model and LoRA setting as Molinst, outperforms it even on the shared training tasks (MC, MG, FS, and RS). This finding highlights the benefits of our dataset.
(2) Our four LlaSMol models show substantial differences in their performance, emphasizing the considerable impact of base models on downstream tasks. Despite sharing identical training, inference settings, and comparable model sizes, LlaSMol $</em>$ consistently outperforms LlaSMol $}<em Llama="Llama" _Code="{Code" _text="\text">{\text {Llama 2 }}$ by a substantial margin, highlighting Mistral's potential on chemistry tasks. In addition, LlaSMol $</em>$ on most tasks, indicating a potential synergy between programming language knowledge}}$ exhibits better performance than LlaSMol $_{\text {Llama 2 }</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Results for molecule captioning (MC), molecule generation (MG), forward synthesis (FS), and retrosynthesis (RS). Metrics EM, FTS, and Valid are in percentage.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>MC</th>
<th>MG</th>
<th></th>
<th></th>
<th>FS</th>
<th></th>
<th></th>
<th>RS</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>METEOR</td>
<td>EM</td>
<td>FTS</td>
<td>Valid</td>
<td>EM</td>
<td>FTS</td>
<td>Valid</td>
<td>EM</td>
<td>FTS</td>
<td>Valid</td>
</tr>
<tr>
<td>Task-Specific, Non-LLM Based Models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>SoTA</td>
<td>0.515</td>
<td>31.7</td>
<td>73.2</td>
<td>95.3</td>
<td>78.7</td>
<td>92.2</td>
<td>100.0</td>
<td>47.0</td>
<td>77.5</td>
<td>99.7</td>
</tr>
<tr>
<td>Existing LLMs Without Fine-Tuning on SMoIInstruct</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-4</td>
<td>0.188</td>
<td>6.4</td>
<td>42.6</td>
<td>81.4</td>
<td>1.6</td>
<td>40.5</td>
<td>87.0</td>
<td>0.0</td>
<td>33.4</td>
<td>42.6</td>
</tr>
<tr>
<td>Claude 3 Opus</td>
<td>0.219</td>
<td>12.3</td>
<td>57.6</td>
<td>92.6</td>
<td>3.7</td>
<td>45.7</td>
<td>97.0</td>
<td>1.1</td>
<td>46.2</td>
<td>94.8</td>
</tr>
<tr>
<td>Galactica</td>
<td>0.050</td>
<td>0.0</td>
<td>11.6</td>
<td>94.7</td>
<td>0.0</td>
<td>25.9</td>
<td>83.7</td>
<td>0.0</td>
<td>34.6</td>
<td>93.0</td>
</tr>
<tr>
<td>Llama 2</td>
<td>0.150</td>
<td>0.0</td>
<td>4.8</td>
<td>93.5</td>
<td>0.0</td>
<td>13.7</td>
<td>97.7</td>
<td>0.0</td>
<td>27.5</td>
<td>87.7</td>
</tr>
<tr>
<td>Code Llama</td>
<td>0.143</td>
<td>0.0</td>
<td>8.5</td>
<td>95.2</td>
<td>0.0</td>
<td>15.8</td>
<td>99.6</td>
<td>0.0</td>
<td>25.3</td>
<td>97.1</td>
</tr>
<tr>
<td>Mistral</td>
<td>0.193</td>
<td>0.0</td>
<td>9.0</td>
<td>35.9</td>
<td>0.0</td>
<td>19.9</td>
<td>95.8</td>
<td>0.0</td>
<td>24.2</td>
<td>98.0</td>
</tr>
<tr>
<td>Molinst (chemistry LLM)</td>
<td>0.124</td>
<td>6.0</td>
<td>43.6</td>
<td>84.8</td>
<td>2.1</td>
<td>31.7</td>
<td>99.8</td>
<td>5.7</td>
<td>48.0</td>
<td>97.8</td>
</tr>
<tr>
<td>ChemLLM (chemistry LLM)</td>
<td>0.050</td>
<td>0.9</td>
<td>14.3</td>
<td>4.3</td>
<td>0.0</td>
<td>1.6</td>
<td>38.5</td>
<td>0.0</td>
<td>2.9</td>
<td>10.9</td>
</tr>
<tr>
<td>Our LlaSMol Series</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LlaSMol $_{\text {Galactica }}$</td>
<td>0.394</td>
<td>7.7</td>
<td>52.2</td>
<td>99.6</td>
<td>53.1</td>
<td>79.9</td>
<td>99.7</td>
<td>25.7</td>
<td>67.0</td>
<td>99.9</td>
</tr>
<tr>
<td>LlaSMol $_{\text {Llama 2 }}$</td>
<td>0.377</td>
<td>6.4</td>
<td>47.1</td>
<td>99.6</td>
<td>47.1</td>
<td>76.9</td>
<td>99.8</td>
<td>22.5</td>
<td>65.2</td>
<td>99.9</td>
</tr>
<tr>
<td>LlaSMol $_{\text {Code Llama }}$</td>
<td>0.366</td>
<td>6.5</td>
<td>46.6</td>
<td>99.7</td>
<td>52.0</td>
<td>79.2</td>
<td>99.8</td>
<td>25.7</td>
<td>66.7</td>
<td>100.0</td>
</tr>
<tr>
<td>LlaSMol $_{\text {Mistral }}$</td>
<td>0.452</td>
<td>19.2</td>
<td>61.7</td>
<td>99.7</td>
<td>63.3</td>
<td>84.9</td>
<td>99.8</td>
<td>32.9</td>
<td>70.4</td>
<td>100.0</td>
</tr>
</tbody>
</table>
<p>Table 3: Results of ablation study on NC and PP tasks. Metrics EM, Valid, and Acc are in percentage. Orange cells represent better results than LlaSMol $_{\text {Mistral }}$ while blue cells represent worse results.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>NC</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>PP</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>I2F</td>
<td>I2S</td>
<td></td>
<td>S2F</td>
<td>S2I</td>
<td>ESOL</td>
<td>Lipo</td>
<td>BBBP</td>
<td>Clintox</td>
<td>HIV</td>
<td>SIDER</td>
</tr>
<tr>
<td></td>
<td>EM</td>
<td>EM</td>
<td>Valid</td>
<td>EM</td>
<td>EM</td>
<td>RMSE $_{1}$</td>
<td>RMSE $_{1}$</td>
<td>Acc</td>
<td>Acc</td>
<td>Acc</td>
<td>Acc</td>
</tr>
<tr>
<td>LlaSMol $_{\text {Mistral }}$</td>
<td>87.9</td>
<td>70.1</td>
<td>99.6</td>
<td>93.2</td>
<td>29.0</td>
<td>1.150</td>
<td>1.010</td>
<td>74.6</td>
<td>93.1</td>
<td>96.7</td>
<td>70.7</td>
</tr>
<tr>
<td>w/o canonical</td>
<td>88.5</td>
<td>67.2</td>
<td>99.6</td>
<td>93.4</td>
<td>24.5</td>
<td>1.224</td>
<td>1.072</td>
<td>71.6</td>
<td>93.1</td>
<td>96.8</td>
<td>70.3</td>
</tr>
<tr>
<td>using SELFIES</td>
<td>86.9</td>
<td>47.7</td>
<td>100.0</td>
<td>94.7</td>
<td>19.7</td>
<td>1.456</td>
<td>1.106</td>
<td>69.5</td>
<td>91.7</td>
<td>96.5</td>
<td>64.4</td>
</tr>
<tr>
<td>train on Mol-Instructions</td>
<td>0.0</td>
<td>0.0</td>
<td>75.2</td>
<td>0.0</td>
<td>0.0</td>
<td>4.416</td>
<td>2.282</td>
<td>0.0</td>
<td>0.0</td>
<td>2.6</td>
<td>0.4</td>
</tr>
</tbody>
</table>
<p>in Code Llama and molecular representations. Furthermore, LlaSMol $<em 2="2" _Llama="{Llama" _text="\text">{\text {Galactica }}$ outperforms LlaSMol $</em>$, and LlaSMol $}<em _Mistral="{Mistral" _text="\text">{\text {Code Llama }}$ in most cases, suggesting the benefits of pretraining on chemistry-related documents.
(3) Although LlaSMol models do not outperform SoTA models, they demonstrate considerable potential for further improvements. Specifically, LlaSMol $</em>$ surpasses the SoTA models on PP-Clintox and PP-SIDER, but has yet to achieve the success on other tasks. However, LlaSMol has greatly narrowed the performance gap between LLMs and SoTA task-specific models, compared to previous efforts (Fang et al., 2023; Zhang et al., 2024). Remarkably, LlaSMol $}<em _Mistral="{Mistral" _text="\text">{\text {Mistral }}$ attains such performance with only a small proportion of its parameters fine-tuned (approximately $41.9 \mathrm{M}, 0.58 \%$ of its parameters). As shown in Appendix F.2, increasing the number of trainable parameters can substantially boost performance, suggesting that LlaSMol $</em>$ has immense potential to surpass task-specific models through more extensive fine-tuning and serve as a strong foundation model for chemistry applications.}</p>
<h1>4.4 Ablation Study</h1>
<p>To investigate the advantages of SMoIInstruct, we conduct an ablation study by comparing LlaSMol $<em _Mistral="{Mistral" _text="\text">{\text {Mistral }}$ with the following variants: (1) w/o canonical, which uses uncanonicalized SMILES, to examine the benefits of canonicalization. (2) using SELFIES, which uses SELFIES Krenn et al. (2019) instead of SMILES to explore their differences. (3) train on Mol-Instructions, which is trained on Mol-Instructions (Fang et al., 2023), to compare the performance improvements of our dataset against the previously proposed dataset.
The results in Table 3 and Table 4 lead to the following observations: (1) The "w/o canonical" model underperforms LlaSMol $</em>$ on most tasks, with a substantial performance drop on FS and RS. This suggests that canonicalizing SMILES can reduce learning difficulty and improve performance. As canonicalization can be easily performed using fixed algorithms}</p>
<p>Table 4: Results of ablation study on MC, MG, FS, and RS. Metrics EM, FTS, and Valid are in percentage. Orange represents better results than LlaSMol ${ }_{\text {Mistral }}$, while blue represents worse results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">MC</th>
<th style="text-align: center;">MG</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">FS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">RS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">METEOR</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">FTS</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">FTS</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">FTS</td>
<td style="text-align: center;">Valid</td>
</tr>
<tr>
<td style="text-align: center;">LlaSMol ${ }_{\text {Mistral }}$</td>
<td style="text-align: center;">0.452</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">w/o canonical</td>
<td style="text-align: center;">0.457</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">99.9</td>
</tr>
<tr>
<td style="text-align: center;">using SELFIES</td>
<td style="text-align: center;">0.466</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">99.9</td>
</tr>
<tr>
<td style="text-align: center;">train on Mol-Instructions</td>
<td style="text-align: center;">0.195</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">88.2</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">76.7</td>
</tr>
</tbody>
</table>
<p>before feeding into models, we recommend using canonical SMILES when training and applying LLMs for chemistry. (2) While using SELFIES slightly improves the validity of generated molecules, which aligns with the motivation behind SELFIES (Krenn et al., 2019), the validity of using SMILES is also sufficiently high. Moreover, using SELFIES results in worse performance on most tasks, possibly due to SELFIES being typically longer than SMILES, making it more difficult for the model to accurately understand and generate. Therefore, using SELFIES over SMILES may not be necessary, contrast to claims made in previous work (Krenn et al., 2019; Fang et al., 2023). (3) Despite using identical base models and training settings, the model trained on Mol-Instructions (Fang et al., 2023) performs much worse than LlaSMol ${ }_{\text {Mistral }}$ trained on SMolInstruct even on the shared tasks (MC, MG, FS, and RS). This demonstrates the superiority of our dataset. A detailed comparison with Mol-Instructions can be found in Appendix C.</p>
<p>To gain deeper insights into the models' performance and behavior, we conduct further analytical experiments: (1) To investigate the synergistic effects among different tasks, we evaluate models trained on a single task and models with certain tasks removed. The results demonstrate multiple-task training outperforms single-task training, indicating its benefits. However, each task generally does not heavily rely on the presence of other tasks, suggesting a degree of independence among them. (2) To investigate the influence of LoRA (Hu et al., 2022) settings, we vary the involved LoRA modules. We observe that adding LoRA modules (and trainable parameters) leads to a substantial boost in performance, indicates the models' great potential for further improvements if with larger-scale fine-tuning. Please refer to Appendix F for more details.</p>
<h1>5 Conclusion</h1>
<p>While LLMs have shown promise as versatile assistants, their performance on chemistryrelated tasks remains notably subpar. To address this issue, we introduces SMolInstruct, a large-scale, comprehensive, and high-quality instruction tuning dataset. It comprises 14 tasks highly relevant to real-world applications and contains over 3M rigorously curated samples. Using SMolInstruct, we develop LlaSMol, a series of LLMs for performing chemistry tasks. Our experiments demonstrate LlaSMol's superiority over existing LLMs, and highlight SMolInstruct's crucial role in boosting the performance. Further analytical experiments also provide significant insights towards developing LLMs for chemistry.
However, this work has the following limitations. First, the evaluations for the MC and MG tasks cannot accurately assess models' abilities to generate chemically correct descriptions and molecules. Since the definition of molecular descriptions remain ambiguous and the available data is limited, it is challenging to assess whether the generated descriptions or molecules are accurate and correct. Second, this work does not delve into the models' generalization capabilities beyond the trained tasks. While we recognize the importance of such capabilities, how to meaningfully test generalization abilities is nontrivial and needs careful design, which falls outside the purview of this work. Third, our models do not yet outperform SoTA task-specific models, possibly due to the small ratio of trainable parameters or suboptimal training procedures. Nevertheless, we propose a high-quality instruction tuning dataset, demonstrate its effectiveness, and gain deeper insights, which we hope can be valuable for future research. We will try to address the aforementioned limitations in our future work.</p>
<h1>Ethics Statement</h1>
<p>Despite our best efforts to maintain the high quality of the SMollnstruct dataset and the integrity of the LlaSMol models, we cannot guarantee that the dataset is free of inaccurate, incorrect, or harmful content, nor can we prevent the models from generating such content. Users should engage with our dataset and models at their own discretion and uphold the highest ethical standards in their use.</p>
<h2>Acknowledgement</h2>
<p>The authors would thank colleagues from the OSU NLP group and the OSU Ning Lab for constructive feedback. This research was supported in part by NSF IIS-2133650, NIH 1R01LM014385-01, and NSF CAREER #1942980, as well as Ohio Supercomputer Center (Ohio Supercomputer Center, 1987). The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notice herein.</p>
<h2>References</h2>
<p>Mikhail Andronov, Varvara Voinarovska, Natalia Andronova, Michael Wand, Djork-Arné Clevert, and Jürgen Schmidhuber. Reagent prediction with a molecular transformer improves reaction data quality. Chemical Science, 14(12):3235-3246, 2023.</p>
<p>Anthropic. The claude 3 model family: Opus, sonnet, haiku. Anthropic, 2024.
Theodore L. Brown. Chemistry: the central science. Pearson, 14th edition edition, 2018.
Andrew R. Burns, Trevor C. Y. Kwok, Al Howard, Ed Houston, Karl Johanson, Anthony Chan, Sean R. Cutler, Peter McCourt, and Peter J. Roy. High-throughput screening of small molecules for bioactivity and target identification in Caenorhabditis elegans. Nature Protocols, 1:1906-1914, 2006.</p>
<p>He Cao, Zijing Liu, Xingyu Lu, Yuan Yao, and Yu Li. Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery. arXiv preprint arXiv:2311.16208, 2023.</p>
<p>Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 2024.</p>
<p>Heng-Yi Chen, Michael Hsu, and Chan-Wang Jerry Lio. Micro but mighty - Micronutrients in the epigenetic regulation of adaptive immune responses. Immunological reviews, 305: $152-164,2022$.</p>
<p>Ziqi Chen, Martin Renqiang Min, Srinivasan Parthasarathy, and Xia Ning. A deep generative model for molecule optimization via one fragment modification. Nature machine intelligence, 3(12):1040-1049, 2021.</p>
<p>Ziqi Chen, Oluwatosin R Ayinde, James R Fuchs, Huan Sun, and Xia Ning. G2retro as a twostep graph generative models for retrosynthesis prediction. Communications Chemistry, 6 (1):102, 2023.</p>
<p>Connor W. Coley, William H. Green, and Klavs F. Jensen. Machine learning in computeraided synthesis planning. Accounts of Chemical Research, 51(5):1281-1289, 2018.</p>
<p>Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. In Proceedings of Conference on Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track, 2023.</p>
<p>Joseph L Durant, Burton A Leland, Douglas R Henry, and James G Nourse. Reoptimization of mdl keys for use in drug discovery. Journal of chemical information and computer sciences, 42(6):1273-1280, 2002.</p>
<p>Carl Edwards, ChengXiang Zhai, and Heng Ji. Text2mol: Cross-modal molecule retrieval with natural language queries. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 595-607, 2021.</p>
<p>Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. Translation between molecules and natural language. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 375-413, 2022.</p>
<p>Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, and Huajun Chen. Mol-instructions: A large-scale biomolecular instruction dataset for large language models. arXiv preprint arXiv:2306.08018, 2023.</p>
<p>Henri A. Favre and Warren H. Powell. Nomenclature of organic chemistry: IUPAC recommendations and preferred names 2013. Royal Society of Chemistry, 2014.</p>
<p>Kaitlyn M. Gayvert, Neel S. Madhukar, and Olivier Elemento. A data-driven approach to predicting successes and failures of clinical trials. Cell Chemical Biology, 23(10):1294-1301, 2016.</p>
<p>Taicheng Guo, Kehan Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. What can large language models do in chemistry? a comprehensive benchmark on eight tasks. In Proceedings of Conference on Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track, 2023.</p>
<p>Shen Han, Haitao Fu, Yuyang Wu, Ganglan Zhao, Zhenyu Song, Feng Huang, Zhongfei Zhang, Shichao Liu, and Wen Zhang. Himgnn: a novel hierarchical molecular graph representation learning framework for property prediction. Briefings in Bioinformatics, 24 (5):bbad305, 2023.</p>
<p>Kan Hatakeyama-Sato, Naoki Yamane, Yasuhiko Igarashi, Yuta Nabae, and Teruaki Hayakawa. Prompt engineering of gpt-4 for chemical research: what can/cannot be done? Science and Technology of Advanced Materials: Methods, 3(1), 2023.</p>
<p>Jiazhen He, Huifang You, Emil Sandström, Eva Nittinger, Esben Jannik Bjerrum, Christian Tyrchan, Werngard Czechtizky, and Ola Engkvist. Molecular optimization by capturing chemist's intuition using deep neural networks. Journal of cheminformatics, 13(1):1-17, 2021.</p>
<p>Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations (ICLR), 2022.</p>
<p>National Cancer Institute. AIDS antiviral screen data, 2004. URL https://wiki.nci.nih. gov/display/NCIDTPdata/AIDS+Antiviral+Screen+Data. Accessed on 1 Fec 2024.</p>
<p>Ross Irwin, Spyridon Dimitriadis, Jiazhen He, and Esben Jannik Bjerrum. Chemformer: a pre-trained transformer for computational chemistry. Machine Learning: Science and Technology, 3(1):015022, 2022.</p>
<p>Kevin Maik Jablonka, Philippe Schwaller, and Berend Smit. Is gpt-3 all you need for machine learning for chemistry? In AI for Accelerated Materials Design NeurIPS 2022 Workshop, 2022.</p>
<p>Kevin Maik Jablonka, Philippe Schwaller, Andres Ortega-Guerrero, and Berend Smit. Leveraging large language models for predictive chemistry. ChemRxiv, 2023. doi: 10.26434/chemrxiv-2023-fw8n4-v3.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.</p>
<p>Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, et al. Pubchem 2019 update: improved access to chemical data. Nucleic acids research, 47:D1102-D1109, 2019.</p>
<p>Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations (ICLR), 2017.</p>
<p>Mario Krenn, Florian Häse, A Nigam, Pascal Friederich, and Alán Aspuru-Guzik. Selfies: a robust representation of semantically constrained graphs with an example application in chemistry. arXiv preprint arXiv:1905.13741, 1(3), 2019.</p>
<p>Michael Kuhn, Ivica Letunic, Lars Juhl Jensen, and Peer Bork. The sider database of drugs and side effects. Nucleic Acids Research, 44:D1075-D1079, 2015.</p>
<p>Alon Lavie and Abhaya Agarwal. Meteor: an automatic metric for mt evaluation with high levels of correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT '07, pp. 228-231. Association for Computational Linguistics, 2007.</p>
<p>Elena Lenci and Andrea Trabocchi. Chapter 1 - Synthetic approaches toward small molecule libraries. In Small Molecule Drug Discovery, pp. 1-34. Elsevier, 2020.</p>
<p>Sihang Li, Zhiyuan Liu, Yanchen Luo, Xiang Wang, Xiangnan He, Kenji Kawaguchi, TatSeng Chua, and Qi Tian. Towards 3d molecule-text interpretation in language models. In Proceedings of International Conference on Learning Representations (ICLR), 2024.</p>
<p>Youwei Liang, Ruiyi Zhang, Li Zhang, and Pengtao Xie. Drugchat: towards enabling chatgpt-like capabilities on drug molecule graphs. arXiv preprint arXiv:2309.03907, 2023.</p>
<p>Shengchao Liu, Jiongxiao Wang, Yijin Yang, Chengpeng Wang, Ling Liu, Hongyu Guo, and Chaowei Xiao. Chatgpt-powered conversational drug editing using retrieval and domain feedback. arXiv preprint arXiv:2305.18090, 2023a.</p>
<p>Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, and Tat-Seng Chua. MolCA: Molecular graph-language modeling with cross-modal projector and uni-modal adapter. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. $15623-15638,2023 b$.</p>
<p>Daniel Lowe. Chemical reactions from us patents (1976-sep2016), 2017. URL https://doi . org/10.6084/m9.figshare.5104873.v1.</p>
<p>Ines Filipa Martins, Ana L. Teixeira, Luis Pinheiro, and Andre O. Falcao. A bayesian approach to in silico blood-brain barrier penetration modeling. Journal of Chemical Information and Modeling, 52(6):1686-1697, 2012.</p>
<p>Brian W Matthews. Comparison of the predicted and observed secondary structure of t4 phage lysozyme. Biochimica et Biophysica Acta (BBA)-Protein Structure, 405(2):442-451, 1975.</p>
<p>Monica P McNerney and Mark P Styczynski. Small molecule signaling, regulation, and potential applications in cellular therapeutics. Wiley Interdisciplinary Reviews: Systems Biology and Medicine, 10:e1405, 2018.</p>
<p>David L. Mobley and J. Peter Guthrie. Freesolv: a database of experimental and calculated hydration free energies, with input files. Journal of Computer-Aided Molecular Design, 28(7): $711-720,2014$.
H. L. Morgan. The generation of a unique machine description for chemical structures-a technique developed at chemical abstracts service. Journal of Chemical Documentation, 5(2): $107-113,1965$.</p>
<p>Ohio Supercomputer Center. Ohio supercomputer center, 1987. URL http://osc.edu/ark: /19495/f5s1ph73.</p>
<p>OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
Salwa K Poole and Colin F Poole. Separation methods for estimating octanol-water partition coefficients. Journal of Chromatography B, 797(1-2):3-19, 2003.</p>
<p>Kohulan Rajan, Achim Zielesny, and Christoph Steinbeck. Stout: Smiles to iupac names using neural machine translation. Journal of Cheminformatics, 13(1):1-14, 2021.</p>
<p>RDKit. Rdkit: Open-source cheminformatics, 2023. URL https://doi.org/10.5281/zenodo. 8254217. Accessed on 27 Jan 2024.</p>
<p>Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.</p>
<p>Nadine Schneider, Roger A Sayle, and Gregory A Landrum. Get your atoms in order: An open-source implementation of a novel and robust molecular canonicalization algorithm. Journal of chemical information and modeling, 55(10):2111-2120, 2015.</p>
<p>Philippe Schwaller, Teodoro Laino, Théophile Gaudin, Peter Bolgar, Christopher A Hunter, Costas Bekas, and Alpha A Lee. Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. ACS central science, 5(9):1572-1583, 2019.
T. W. Graham Solomons, Craig B. Fryhle, and Scott A. Snyder. Organic Chemistry, Integrated E-Text with E-Solutions Manual. Wiley, 13th edition, 2022.</p>
<p>Vignesh Ram Somnath, Charlotte Bunne, Connor Coley, Andreas Krause, and Regina Barzilay. Learning graph models for retrosynthesis prediction. Advances in Neural Information Processing Systems, 34:9405-9415, 2021.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.</p>
<p>Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. Large language models in medicine. Nature Medicine, 29(8):1930-1940, 2023.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of Conference on Advances in neural information processing systems (NeurIPS), volume 30, 2017.</p>
<p>Yuyang Wang, Zijie Li, and Amir Barati Farimani. Graph Neural Networks for Molecules. In Chen Qu and Hanchao Liu (eds.), Machine Learning in Molecular Sciences, pp. 21-66. Springer International Publishing, Cham, 2023. ISBN 978-3-031-37196-7. doi: 10.1007/ 978-3-031-37196-7_2. URL https://doi.org/10.1007/978-3-031-37196-7_2.</p>
<p>David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28 (1):31-36, 1988.</p>
<p>Andrew D. White, Glen M. Hocky, Heta A. Gandhi, Mehrad Ansari, Sam Cox, Geemi P. Wellawatte, Subarna Sasmal, Ziyue Yang, Kangxin Liu, Yuvraj Singh, and Willmor J. Peña Ccoa. Assessment of chemistry knowledge in large language models that generate code. Digital Discovery, 2(2):368-376, 2023.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural language processing. In Proceedings of conference on empirical methods in natural language processing: system demonstrations (EMNLP), pp. 38-45, 2020.</p>
<p>Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513-530, 2018.</p>
<p>Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp Eiden, Hua Gao, Angel Guzman-Perez, Timothy Hopper, Brian Kelley, Miriam Mathea, Andrew Palmer, Volker Settels, Tommi Jaakkola, Klavs Jensen, and Regina Barzilay. Analyzing Learned Molecular Representations for Property Prediction. Journal of Chemical Information and Modeling, 59(8):3370-3388, August 2019. ISSN 1549-9596. doi: 10.1021/acs.jcim.9b00237. URL https://doi.org/10.1021/acs.jcim.9b00237. Publisher: American Chemical Society.</p>
<p>Geyan Ye, Xibao Cai, Houtim Lai, Xing Wang, Junhong Huang, Longyue Wang, Wei Liu, and Xiangxiang Zeng. Drugassist: A large language model for molecule optimization. arXiv preprint arXiv:2401.10334, 2023.</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023.</p>
<p>Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Dongzhan Zhou, et al. Chemllm: A chemical large language model. arXiv preprint arXiv:2402.06852, 2024.</p>
<p>Tianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun. Tablellama: Towards open large generalist models for tables. arXiv preprint arXiv:2311.09206, 2023.</p>
<p>Yangtian Zhang, Huiyu Cai, Chence Shi, and Jian Tang. E3bind: An end-to-end equivariant network for protein-ligand docking. In The Eleventh International Conference on Learning Representations, 2022a.</p>
<p>Zaixi Zhang, Yaosen Min, Shuxin Zheng, and Qi Liu. Molecule generation for target protein binding with structural motifs. In The Eleventh International Conference on Learning Representations, 2022b.</p>
<p>Zipeng Zhong, Jie Song, Zunlei Feng, Tiantao Liu, Lingxiang Jia, Shaolun Yao, Min Wu, Tingjun Hou, and Mingli Song. Root-aligned smiles: a tight representation for chemical reaction prediction. Chemical Science, 13(31):9023-9034, 2022.</p>
<p>Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. Uni-mol: A universal 3d molecular representation learning framework. In International Conference on Learning Representations (ICLR), 2023.</p>
<h1>Table of Contents in Appendix</h1>
<p>A Preliminaries ..... 16
B Details of SMolInstruct ..... 16
B. 1 The statistics of SMolInstruct ..... 16
B. 2 Details of Dataset Construction ..... 17
C Comparison with Mol-Instructions ..... 18
D Details of Experimental Setup ..... 19
D. 1 LlaSMol Models ..... 20
D. 2 Compared LLMs ..... 20
D.2.1 GPT-4 ..... 20
D.2.2 Claude 3 Opus ..... 21
D.2.3 Galactica ..... 21
D.2.4 Llama 2, Code Llama, and Mistral ..... 21
D.2.5 Molinst ..... 22
D.2.6 ChemLLM ..... 22
D. 3 Task-Specific, Non-LLM Based SoTA Models ..... 22
D.3.1 STOUT for NC-I2S and NC-S2I ..... 22
D.3.2 RDKit for NC-S2F ..... 22
D.3.3 STOUT+RDKit for NC-I2F ..... 22
D.3.4 Uni-Mol for All The PP Tasks ..... 22
D.3.5 MolT5 for MC and MG ..... 23
D.3.6 RSMILES for FS and RS ..... 23
D.3.7 Molecular Transformer for FS and RS ..... 23
D. 4 Evaluation Metrics ..... 24
E Detailed Experimental Results ..... 24
E. 1 Name Conversion Tasks ..... 24
E. 2 Property Prediction ..... 27
E. 3 Molecule Description ..... 28
E. 4 Chemical Reaction ..... 28
E. 5 Other Common Findings ..... 28
F More Analytical Experiments ..... 30
F. 1 Task Synergy ..... 30
F. 2 Influence of LoRA Modules and Trainable Parameters ..... 31</p>
<h1>A Preliminaries</h1>
<p>Molecules form the basis of chemistry, which fundamentally determines the properties and behaviors of most substances. A molecule is a group of atoms held together by chemical bonds (Brown, 2018). In this paper, we focus on small molecules, which typically have no more than 100 atoms and a low molecular weight under 1,500 Daltons (Lenci \&amp; Trabocchi, 2020). Small molecules perform many important functions, such as signaling in cellular biology (McNerney \&amp; Styczynski, 2018), pest control in agriculture (Burns et al., 2006), micronutrients in nutrition (Chen et al., 2022), and drug therapy in medicine (Lenci \&amp; Trabocchi, 2020). Given the importance of small molecules, it is essential to integrate LLMs into the study of small molecules to further advance their design or development.
Molecules can be represented in multiple ways, such as SMILES strings, IUPAC names, and molecular formulas. SMILES strings use a sequence of symbols to encode the 2D structures of molecules (Weininger, 1988). A molecule can have multiple SMILES strings; a canonical SMILES for the molecule is unique and deterministic. For example, the canonical SMILES representation of glucose is " $\left.\mathrm{C}(\mathrm{C} 1 \mathrm{C}(\mathrm{C}(\mathrm{C}(\mathrm{O} 1) \mathrm{O}) \mathrm{O}) \mathrm{O}) \mathrm{O}\right) \mathrm{O}$ ". SELFIES (Krenn et al., 2019) is an alternative representation to SMILES that also uses a sequence of symbols to denote molecular structures. Its key advantage is robustness, as every SELFIES string is guaranteed to correspond to a valid molecule. The SELFIES representation corresponding to the above SMILES representation of glucose is "[C][Branch2][Ring1][Branch1][C][C][Branch1][S][C][Branch1][N][C][Branch1][Branch2] [C][Branch1][Ring2][O][Ring1][=Branch1][O][O][O][O][O]". Molecular formulas represent a molecule by enumerating the type and number of atoms in the molecule (Solomons et al., 2022). For example, the molecular formula for glucose is " $\mathrm{C}<em 12="12">{6} \mathrm{H}</em>$ ". IUPAC names are formal names based on natural language elements, which follow the systematic rules set by the International Union of Preferred and Applied Chemistry (IUPAC) (Favre \&amp; Powell, 2014). These names are derived from the structures and functional groups of molecules, and are intended to be human-readable. For example, the IUPAC name for glucose is "(3R,4S,5S,6R)-6-(hydroxymethyl)oxane-2,3,4,5-tetrol".
Molecules are one of the fundamental units of chemistry that participate in reactions (Brown, 2018). A reaction is a process which converts input molecules (reactants) into output molecules (products) through the breaking and forming of chemical bonds. Other molecules (reagents) may be present to enhance or facilitate the reaction.} \mathrm{O}_{6</p>
<h2>B Details of SMolInstruct</h2>
<p>In this section, we introduce the details of our proposed dataset SMolInstruct, including statistics and construction details.</p>
<h2>B. 1 The statistics of SMolInstruct</h2>
<p>Table 5 shows the statistics of SMolInstruct. It contains 4 types of altogether 14 tasks, which are selected to be meaningful and useful. There are about 3.3 M samples, and each of them is a distinct sample. In other words, there does not exist a pair of samples who share the same chemical information (i.e., the core input and output information, such as input molecules and output molecules), but with the same or different natural language templates (i.e., the task description in the query and the sentence templates in the response). When needed, one can easily create more instruction tuning samples by combining one piece of chemical information with multiple natural language templates. All in all, SMolInstruct can serve as a good benchmark for training and evaluating LLMs on various chemistry tasks.
To know more about the diversity of SMolInstruct, we conduct a statistics on the molecules. Altogether, there exist 1.6 M distinct molecules, and several important statistical values are shown in Figure 2. Specifically, Bertz complexity is a topological index that measures the complexity of molecules based on the number and types of bonds and atoms. Atom count shows the number of atoms in a molecule, and it represents the size of a molecule. Molecular weight is the sum of the atomic weights of the atoms in a molecule. And ring count shows</p>
<p>Table 5: The statistics of SMolInstruct. "Qry." and "Resp." are average lengths of queries and responses, respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Task abbr.</th>
<th style="text-align: center;">#Train</th>
<th style="text-align: center;">#Valid</th>
<th style="text-align: center;">#Test</th>
<th style="text-align: center;">#All</th>
<th style="text-align: center;">Qry.</th>
<th style="text-align: center;">Resp.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Name Conversion. Data Source: PubChem</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">IUPAC to Molecular Formula</td>
<td style="text-align: center;">NC-I2F</td>
<td style="text-align: center;">300,000</td>
<td style="text-align: center;">1,497</td>
<td style="text-align: center;">2,993</td>
<td style="text-align: center;">304,490</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">25</td>
</tr>
<tr>
<td style="text-align: center;">IUPAC to SMILES</td>
<td style="text-align: center;">NC-I2S</td>
<td style="text-align: center;">299,890</td>
<td style="text-align: center;">1,496</td>
<td style="text-align: center;">2,993</td>
<td style="text-align: center;">304,379</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">59</td>
</tr>
<tr>
<td style="text-align: center;">SMILES to Molecular Formula</td>
<td style="text-align: center;">NC-S2F</td>
<td style="text-align: center;">299,890</td>
<td style="text-align: center;">1,496</td>
<td style="text-align: center;">2,993</td>
<td style="text-align: center;">304,379</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">26</td>
</tr>
<tr>
<td style="text-align: center;">SMILES to IUPAC</td>
<td style="text-align: center;">NC-S2I</td>
<td style="text-align: center;">299,890</td>
<td style="text-align: center;">1,496</td>
<td style="text-align: center;">2,993</td>
<td style="text-align: center;">304,379</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">68</td>
</tr>
<tr>
<td style="text-align: center;">Property Prediction. Data Source: MoleculeNet</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ESOL</td>
<td style="text-align: center;">PP-ESOL</td>
<td style="text-align: center;">888</td>
<td style="text-align: center;">111</td>
<td style="text-align: center;">112</td>
<td style="text-align: center;">1,111</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">22</td>
</tr>
<tr>
<td style="text-align: center;">Lipo</td>
<td style="text-align: center;">PP-Lipo</td>
<td style="text-align: center;">3,360</td>
<td style="text-align: center;">420</td>
<td style="text-align: center;">420</td>
<td style="text-align: center;">4,200</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;">BBBP</td>
<td style="text-align: center;">PP-BBBP</td>
<td style="text-align: center;">1,569</td>
<td style="text-align: center;">196</td>
<td style="text-align: center;">197</td>
<td style="text-align: center;">1,962</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;">ClinTox</td>
<td style="text-align: center;">PP-ClinTox</td>
<td style="text-align: center;">1,144</td>
<td style="text-align: center;">143</td>
<td style="text-align: center;">144</td>
<td style="text-align: center;">1,431</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;">HIV</td>
<td style="text-align: center;">PP-HIV</td>
<td style="text-align: center;">32,864</td>
<td style="text-align: center;">4,104</td>
<td style="text-align: center;">4,107</td>
<td style="text-align: center;">41,075</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;">SIDER</td>
<td style="text-align: center;">PP-SIDER</td>
<td style="text-align: center;">22,820</td>
<td style="text-align: center;">2,860</td>
<td style="text-align: center;">2,860</td>
<td style="text-align: center;">28,540</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;">Molecule Description. Data Source: Mol-Instructions, ChEBI-20</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Molecule Captioning</td>
<td style="text-align: center;">MC</td>
<td style="text-align: center;">56,498</td>
<td style="text-align: center;">1,269</td>
<td style="text-align: center;">2,538</td>
<td style="text-align: center;">60,305</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">102</td>
</tr>
<tr>
<td style="text-align: center;">Molecule Generation</td>
<td style="text-align: center;">MG</td>
<td style="text-align: center;">56,498</td>
<td style="text-align: center;">1,269</td>
<td style="text-align: center;">2,493</td>
<td style="text-align: center;">60,260</td>
<td style="text-align: center;">117</td>
<td style="text-align: center;">75</td>
</tr>
<tr>
<td style="text-align: center;">Chemical Reaction. Data Source: USPTO-full</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Forward Synthesis</td>
<td style="text-align: center;">FS</td>
<td style="text-align: center;">971,809</td>
<td style="text-align: center;">2,049</td>
<td style="text-align: center;">4,062</td>
<td style="text-align: center;">977,920</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">52</td>
</tr>
<tr>
<td style="text-align: center;">Retrosynthesis</td>
<td style="text-align: center;">RS</td>
<td style="text-align: center;">941,735</td>
<td style="text-align: center;">2,092</td>
<td style="text-align: center;">4,156</td>
<td style="text-align: center;">947,983</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">70</td>
</tr>
<tr>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3,288,855</td>
<td style="text-align: center;">20,498</td>
<td style="text-align: center;">33,061</td>
<td style="text-align: center;">3,342,414</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">55</td>
</tr>
</tbody>
</table>
<p>the number of rings in the molecular structures. As we can see, the values varies much, showing a extensive coverage in terms of complexity, size, and structure. Notably, when compared to Mol-Instructions (Fang et al., 2023), molecules in SMolInstruct show a higher complexity and diversity, which indicates that SMolInstruct is more comprehensive and complicated than Mol-Instructions. The scale, complexity, and diversity of SMolInstruct makes it well-suited for learning chemistry LLMs.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The statistics of molecules in SMolInstruct, with the long tail parts removed for a clear presentation.</p>
<h1>B. 2 Details of Dataset Construction</h1>
<p>Dataset construction involves four key steps (Section 3.2): data collection, quality control, data splitting, and instruction creation. This section provides task-specific details, omitting the common steps of canonicalizing SMILES/SELFIES and verbalizing information into query and response sentences, which have been introduced in Section 3.2.
Name Conversion (NC). The raw data for name conversion is collected from PubChem (Kim et al., 2019). Approximately 300k molecule/compound entries are randomly selected from the database, and their SMILES, IUPAC names, and molecular formulas are extracted. Entries with incomplete or missing information in these three domains are discarded. Finally, the SMILES, IUPAC names, molecular formulas are paired to create samples for the four name conversion tasks.
Property Prediction (PP). The raw data for property prediction is sourced from MoleculeNet (Wu et al., 2018). Out of its 16 core datasets ${ }^{5}$, we select 6 that are only related to small</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>molecules and are useful especially in drug discovery. Answers for regression tasks (e.g., ESOL and Lipo) are formulated as strings of numbers, and answers for binary classification tasks (e.g., BBBP and SIDER) are formulated as "Yes" or "No".
Molecule Captioning (MC) and Molecule Generation (MG). The raw data is collected from ChEBI-20 (Edwards et al., 2021; 2022) and Mol-Instructions (Fang et al., 2023). Despite the large number of samples in Mol-Instructions, many are found to be of low quality. For example, numerous molecular descriptions end with the ambiguous phrase "with data available", while others are overly general, making it difficult to generate a specific molecule based on the description. To ensure data quality, regular expressions and heuristic rules are employed to filter out low-quality samples.
Forward Synthesis (FS). USPTO-full (Lowe, 2017), one of the most comprehensive chemical reaction datasets, serve as the data source. The following processing steps are performed to clean the data: (1) Reactants and reagents are combined as input, and the product(s) serve as output, consistent with other datasets such as Mol-Instructions (Fang et al., 2023). (2) Duplicate chemicals in both input and output are removed to avoid redundancy. (3) If a chemical appears in both input and output, it is removed from the output to maintain data integrity. (4) Products in the output containing fewer than 5 molecules are considered non-main products and excluded. (5) If the above steps result in an empty output, the entire sample is discarded.
Retrosynthesis (RS). The data is also sourced from USPTO-full (Lowe, 2017), with the product as input and the reactants (excluding reagents) as output. During data exploration, we observe instances where reactants are mislabeled as reagents and vice versa. To address this issue, we compare the atom mapping numbers of the reactants and reagents with the products and relabel them accordingly. Subsequently, we apply the following processing steps: (1) Duplicate chemicals in both input and output are removed. (2) If a chemical appears in both input and output, it is removed from the input. (3) Products in the input containing fewer than 5 molecules are excluded. (4) In cases where multiple products exist in the input, the reaction is split into multiple samples, with each product serving as the input once. (5) If the above steps result in an empty input, the entire sample is discarded.
For all the tasks, samples containing invalid SMILES strings (i.e., those that cannot be parsed into a valid molecule with RDKit(RDKit, 2023)) are discarded, and duplicate samples are removed to avoid redundancy. Finally, since some molecules contain multiple components and they are separated by dots in SMILES, which is the same delimiter used to separate different reactants/reagents/products in FS and RS, the dots in SMILES strings for NC, PP, MC, and MG are replaced with semicolons to differentiate between these two usages.</p>
<h1>C Comparison with Mol-Instructions</h1>
<p>In this section, we present a comprehensive comparison between our work and MolInstructions (Fang et al., 2023).
We begin by comparing our dataset, SMolInstruct, with the Mol-Instructions dataset. While Mol-Instructions covers a broader scope (including molecule-oriented, protein-oriented, and biomolecular text instructions), SMolInstruct focuses exclusively on small molecules, providing a deeper and more comprehensive exploration of this domain.
If focusing on the molecule-related data, as shown in Table 6, SMolInstruct is a larger, more comprehensive, and higher-quality dataset. It incorporates more tasks, samples, and molecular representations, and involves more careful curation. Both datasets share the tasks of MC, MG, FS, and RS. Although SMolInstruct has fewer samples for MC and MG, the included samples are of higher quality (see Appendix B.2). Furthermore, SMolInstruct contains substantially more samples for FS and RS, which have been carefully cleaned and processed. Additionally, SMolInstruct incorporates four NC tasks to facilitate the understanding of various molecular representations. Unlike Mol-Instructions, we do not include the reagent prediction task mainly due to the lack of sufficient high-quality data (Andronov et al., 2023) and the limited practicality of this task in real world applications.</p>
<p>Table 6: Comparison between Mol-Instructions (the molecule-oriented part) (Fang et al., 2023) and our SMolInstruct.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Mol-Instructions</th>
<th style="text-align: center;">SMolInstruct (ours)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">name conversion property prediction</td>
<td style="text-align: center;">1.2 M samples <br> 78.3 k samples on 6 useful properties. <br> ergy. <br> 298.3 k samples. <br> 298.3 k samples. <br> 298.3 k samples. <br> 60.3 k samples.</td>
</tr>
<tr>
<td style="text-align: center;">Tasks</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { molecule } \quad \text { genera- } \ &amp; \text { tion } \end{aligned}$</td>
<td style="text-align: center;">60.3k samples.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">forward synthesis</td>
<td style="text-align: center;">60.3k samples.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { retrosynthesis } \ &amp; \text { reagent prediction } \end{aligned}$</td>
<td style="text-align: center;">977.9k samples. <br> 948.0k samples. <br> Not included due to its insufficient data and limited practicality.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.2 M</td>
<td style="text-align: center;">3.3 M</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.3 M</td>
<td style="text-align: center;">3.3 M</td>
</tr>
<tr>
<td style="text-align: center;">#Samples</td>
<td style="text-align: center;">SELFIES.</td>
<td style="text-align: center;">Supports SMILES (default) and SELFIES, also involves IUPAC names and molecular formula in the NC tasks.</td>
</tr>
<tr>
<td style="text-align: center;">Molecular representations</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Carefully split into train/validation/test set, removing potential data leakage (see Section 3.2)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Yes, all the SMILES/SELFIES representations are canonicalized, providing a standardized data format.</td>
</tr>
<tr>
<td style="text-align: center;">Data splitting</td>
<td style="text-align: center;">Provides test set, while train/validation sets are not explicitly split.</td>
<td style="text-align: center;">Higher (see Appendix B.1).</td>
</tr>
</tbody>
</table>
<p>Beyond the dataset, our work makes contributions to the exploration of chemistry LLMs. While Fang et al. (2023) primarily focus on the dataset itself and provide a preliminary exploration of the models, we conduct comprehensive experiments to investigate the abilities of LLMs in the chemistry domain. Our experiments in Section 4 demonstrates that our LlaSMol models achieves superior performance compared to the LLMs trained on MolInstructions and the strongest LLMs such as GPT-4 and Claude 3 Opus, greatly diminishing the gap between LLMs and SoTA task-specific models. Moreover, we provides valuable insights about multi-task training, LoRA (Hu et al., 2022) settings, and other aspects that could be helpful for future research in this field.</p>
<h1>D Details of Experimental Setup</h1>
<p>In this section, we introduce the details of our experimental setups, including the training and inference details of our LlaSMol models and the compared models. We also give detailed explanations of the metrics used in Section 4.3, as well extra metrics that we will use in Appendix E.</p>
<h1>D. 1 LlaSMol Models</h1>
<p>The base models used for developing LlaSMol are Galactica ${ }^{6}$ (Taylor et al., 2022), Llama $2^{7}$ (Touvron et al., 2023b), Code Llama ${ }^{8}$ (Roziere et al., 2023) and Mistral ${ }^{9}$ (Jiang et al., 2023). We conduct instruction tuning on our SMolInstruct, and the resulting models are called named as LlaSMol ${ }<em 2="2" _Llama="{Llama" _text="\text">{\text {Galactica }}$, LlaSMol $</em>$, LlaSMol $}<em _Mistral="{Mistral" _text="\text">{\text {Code Llama }}$, and LlaSMol $</em>$, respectively. Expect for being based on different base models, their training and evaluation configurations are identical, as described as follows.
We used LoRA (Hu et al., 2022) during training, which is applied to all linear layers in the self-attention and FFN modules with lora.r and lora_alpha set to 16 . With the 8 -bit AdamW optimizer, a learning rate of $1 \mathrm{e}-4$, and a cosine scheduler, we train each model for three epochs. The input length is set to 512 , and sequences longer than 512 are truncated.
During inference, we adopt beam search as the generation strategy for simplicity. Due to the need of evaluations on the top- $k$ predicted answers (as in Appendix E, where $k$ varies for different tasks, we generate different numbers of sequences for different tasks by setting the num_return_sequences argument in the Huggingface Transformers library (Wolf et al., 2020). Specifically, it is set to 5 for NC-I2S, NC-S2I, FS, and MG; 3 for NC-I2F and NC-S2F; 1 for all the PP tasks; and 10 for RS. The beam size is set to num_return_sequences +3 for all the tasks. The maximum number of new generated tokens is set to 1024 .}</p>
<h2>D. 2 Compared LLMs</h2>
<p>We introduce each of the compared LLMs in details, including their training (if applicable) and inference process.</p>
<h2>D.2.1 GPT-4</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">General <br> Template</th>
<th style="text-align: center;">You are an expert chemist. Given the SMILES representation of reactants and reagents, your task is to predict the potential product using your chemical reaction knowledge.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Task-Specific <br> Template</td>
<td style="text-align: center;">The input contains both reactants and reagents, and different reactants and reagents are separated by ' $\$$. Your reply should contain only the SMILES representation of the predicted product and no other text. Your reply must be valid and chemically reasonable.</td>
</tr>
<tr>
<td style="text-align: center;">ICL</td>
<td style="text-align: center;">Reactants and reagents SMILES: C1CCOC1.CCN(CC)CC.CS( $=0)(=0) \mathrm{Cl} . \mathrm{CS}(\mathrm{C})=0$. <br> N[C@@H]1CC2=CC=C(CN3C=C(CO)C(C(F)(F)F)=N3)C=C2C1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Product SMILES: CS( $=0)(=0) \mathrm{N}[\mathrm{C@@H}] 1 \mathrm{CC} 2=\mathrm{CC}=\mathrm{C}(\mathrm{CN} 3 \mathrm{C}=\mathrm{C}(\mathrm{CO}) \mathrm{C}(\mathrm{C}(\mathrm{F})(\mathrm{F}) \mathrm{F})=\mathrm{N} 3) \mathrm{C}=$ C2C1</td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">Reactants and reagents SMILES: CCN.CN1C=CC=C1C=O <br> Product SMILES:</td>
</tr>
</tbody>
</table>
<p>Figure 3: An example of query template for GPT-4.
GPT-4 (OpenAI, 2023) is one of the SoTA LLMs. We use the model versioned as gpt-4-0613 and evaluate it on 500 samples from SMolInstruct test set via OpenAI's API. Since GPT-4 is not fine-tuned on our dataset and thus is not familiar with the flexible queries, to ensure it generates answers in an expected format, we follow the prompt format proposed in (Guo et al., 2023) and create a query template for each of the tasks. The template for FS is shown in Figure 3. It contains 4 parts: (1) General template describes the task in a general way. (2) Task-specific template describes the detailed content requirements and format</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>requirements for the specific task. (3) ICL contains the in-context learning examples. It provides examples in the format of <input.title>: <input.content>\n <output.title>: <output.content>\n, where <input.title> and <output.title> serve as straightforward prompts to the input and output content. This design make the queried task more clear. (4) Question has the same format as ICL, with <output.content> being empty for the model to generate.
We conduct both $s$-shot evaluations, where $s=0,1,3,5$ is the number of provided ICL examples. For 0 -shot evaluation, the ICL part in the template is removed from the queries. In $k$-shot evaluation, for each sample,the ICL examples are randomly selected from the training set. The results of these settings are shown in Appendix E, which reals that these settings' performance is not consistent across all the tasks. Since 0 -shot shows the best performance on most tasks, we report its results in Section 4.3.
In the evaluations, we use the default generation strategy set in the API. To generate the same number of results for each sample (as described in Appendix D.1), we set the argument n in the API, which controls the number of output sequences.
GPT-4 can always follow the formatted instructions introduced above, so we do not bother to extract the answers from its outputs, but directly use its outputs as the predicted answers.</p>
<h1>D.2.2 Claude 3 Opus</h1>
<p>Claude 3 Opus (Anthropic, 2024) is a newly proposed SoTA LLM to date. Similarly to GPT-4, we evaluate Claude 3 Opus on 500 samples from SMolInstruct test set via Anthropic's API, and the generation strategy is the default one. The used prompt format is identical to the one used for GPT-4 (Appendix D.2.1. For each sample, we generate one response. Since Claude 3 Opus can always follow the formatted instructions, we do not bother to extract the answers from its outputs, but directly use its outputs as the predicted answers.</p>
<h2>D.2.3 Galactica</h2>
<p>Galactica (Taylor et al., 2022) is a LLM without instruction tuning. To evaluate it on SMolInstruct, we follow the instructions in the paper (Taylor et al., 2022) and the repository ${ }^{10}$ to create the queries for each task. We use zero-shot setting, as its official instruction does not suggest using few-shot setting. The generation configuration is set identical to that of our LlaSMol models (Appendix D.1).
Galatica's outputs may contain extra text other than the expected answers. Therefore, with heuristic rules and regular expression matching, we implement a program to extract the answers from the outputs of the models. Since the extraction cannot possibly cover all the possible output formats, some answers might not be correctly extracted, which might lead to validities lower than the actual value.</p>
<h2>D.2.4 Llama 2, Code Llama, and Mistral</h2>
<p>For our base models (Llama 2, Code Llama, and mistral), since they are not trained on SMolInstruct and have not seen the diverse queries in the dataset, we use the same query templates as those used for GPT-4 (Appendix D.2.1). We use the one-shot setting for them, as it would improve models' abiltity to follow the instructions and generate answers in a more formated way. In addition, the generation configuration (including beam size, output sequence numbers, etc) is set identical to that of our LlaSMol models (Appendix D.1).
Although we try our best to make the output format as clear as possible in the queries, these three models still cannot follow the instructions and their outputs are in various formats. By heuristic rules and regular expression matching, we implement a program to extract the answers from the outputs of each of the models. Since the extraction cannot possibly cover all the possible output formats, some answers might not be correctly extracted, which might lead to validities lower than the actual value.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ https://github.com/paperswithcode/galai&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
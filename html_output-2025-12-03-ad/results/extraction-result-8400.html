<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8400 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8400</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8400</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-2922768fd451ecdb45f48c1a83eb57f54a91221b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2922768fd451ecdb45f48c1a83eb57f54a91221b" target="_blank">Textbooks Are All You Need</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper Abstract:</strong> We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality"data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8400.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8400.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>phi-1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>phi-1 (finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 1.3B-parameter decoder-only Transformer model trained on a small, high-quality code-oriented corpus (CodeTextbook) and then finetuned on a synthetic exercises dataset (CodeExercises); shows large gains on code tasks including many numerically grounded programming problems after finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>phi-1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer, 1.3B parameters, 24 layers, hidden dim 2048, MLP diemnsion 8192, 32 attention heads (64-d each), rotary position embeddings (rotary dim 32). Pretrained on ~7B tokens (CodeTextbook: filtered real code + <1B synthetic GPT-3.5 textbooks) and finetuned on ~180M tokens CodeExercises; trained with next-token prediction, seq length 2048, FlashAttention implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Arithmetic and numeric computation encountered inside code-generation tasks: integer divisibility checks, floor(sqrt(...)) computations, Euclidean and cosine distance using norms, determinant checks (matrix singularity), sorting/concatenation/squaring/filtering by numeric threshold, computing frequency sums and squared-frequency sums, SVD and top-k singular-value operations, elementwise arithmetic on arrays (cos(x)/sqrt(x+1)), basic integer random sampling and bounding, and other numeric loop/array operations.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>No neuron-level or attention-level internal arithmetic mechanism is identified in the paper. The authors hypothesize that improvements arise from (a) higher-quality 'textbook' data mapping natural language to algorithmic code, and (b) a short finetuning stage on explicit function-completion exercises that 'reorganizes and consolidates' pretraining knowledge, enabling more consistent logical and numeric code generation rather than memorized token-level arithmetic patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Behavioral and dataset interventions: 1) finetuning phi-1-base on the synthetic CodeExercises dataset (<200M tokens); 2) qualitative example comparisons (before/after finetuning) on numeric/logic tasks; 3) creation of 50 unconventional problems (held-out, human-authored) and LLM grading (GPT-4) as evaluation; 4) aggressive data pruning / decontamination using n-gram overlap, embedding L2 distance (CodeGen-Mono-350M embeddings) and AST-based edit-distance thresholds; no mechanistic probes such as activation patching, linear probes, or attention/neuron attribution are used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>HumanEval pass@1: reported ~50.6% (Table 1). MBPP pass@1: 55.5% (Table 1). phi-1-base (pre-finetune) HumanEval: ~29%. LLM-graded Understanding on 50 unconventional problems (GPT-4 as grader): phi-1 ~52% (Table 2). Large gain attributable to finetuning: HumanEval improves from ~29% (phi-1-base) to ~50.6% (phi-1) after CodeExercises finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Reported failure modes relevant to numeric/algorithmic tasks include: sensitivity to prompt variations (longer prompts or small prompt changes can cause failure), degraded performance for grammatical errors in prompts, occasional incorrect API usage or semantically irrelevant API calls, incorrect or imprecise string handling (e.g., mis-copied text), inability to handle larger/complex system-level tasks (e.g., full Flask apps), and remaining errors originating in the synthetic training data (GPT-3.5-generated examples had non-negligible error rates).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Behavioral evidence: qualitative before/after examples show finetuned model produces correct numeric logic (e.g., correct divisibility check using floor(sqrt(bob_num)) or correct SVD-based numeric updates) and correct API usage for numeric libraries; finetuning on a small targeted exercises dataset resulted in large performance jumps on numeric code tasks, implying consolidation of algorithmic mappings. Data-pruning experiments (removing problems similar to test via embedding+AST thresholds) show retained performance, arguing gains are not solely from memorization of exercises similar to test items.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>No internal mechanistic analysis is provided (no neuron, attention, or circuit-level probes). The model still fails on many prompts and remains sensitive to prompt formulation; synthetic data contains errors yet the model sometimes achieves correct outputs (an unresolved puzzle). The authors explicitly note the absence of finer-grained mechanistic evidence and acknowledge limitations (capacity, domain-specific knowledge, robustness).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Textbooks Are All You Need', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8400.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8400.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>phi-1-base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>phi-1-base (pre-finetuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The pretrained checkpoint of the 1.3B model trained on CodeTextbook (filtered real code + GPT-3.5 synthetic textbooks) prior to finetuning on CodeExercises; provides a baseline showing the effect of targeted finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>phi-1-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same architecture as phi-1 (1.3B decoder Transformer) but evaluated prior to finetuning on CodeExercises. Obtained checkpoint at 24k steps (~8 epochs over CodeTextbook, ~50B tokens seen).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same kinds of numeric programming tasks as phi-1 (divisibility, sqrt, linear algebraic ops, array arithmetic), as these appear in coding examples and tests used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>No internal representations probed. Behavior suggests phi-1-base stores some algorithmic knowledge from CodeTextbook pretraining but lacks the reorganization/alignment that finetuning provides; it tends to produce syntactically plausible but semantically incorrect numeric/code logic.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Compared directly to phi-1 and phi-1-small using qualitative prompt examples (divisibility, PyGame, Tkinter, SVD, pyplot animation). Performance comparisons and LLM-graded tests act as behavioral probes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>HumanEval pass@1 (phi-1-base): ~29% (reported). LLM-graded Understanding on 50 unconventional problems: ~37% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Common failure modes include confusion about logical relationships in prompts (mixing indices vs elements), semantically irrelevant or incorrect API/function calls for numeric libraries, incorrect implementation of numeric conditions and loops, and inability to follow multi-step numeric instructions reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Qualitative prompt comparisons in Section 3 show phi-1-base producing incorrect logical/numeric code on examples where phi-1 (finetuned) succeeds; this behavioral difference is cited as evidence that finetuning reorganizes pretraining knowledge to improve arithmetic-like program synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>phi-1-base can still produce partly correct templates (e.g., pyplot template) but fails to implement numeric update logic; no internal probing to explain failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Textbooks Are All You Need', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8400.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8400.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>phi-1-small</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>phi-1-small (350M)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller 350M-parameter model trained with the same pipeline as phi-1, used to demonstrate the role of model capacity in emergent capabilities for numeric and algorithmic coding tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>phi-1-small</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer with 350M parameters, 20 layers, hidden dim 1024, MLP dim 4096, 16 attention heads (64-d each). Trained on same CodeTextbook and finetuning pipeline as phi-1-small described in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Numeric/code tasks like simple divisibility, random integer updates, numpy/pytorch numeric ops and SVD-like tasks, simple loops and arithmetic used in example prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>No mechanistic internal representation analysis; observed behavior suggests limited capacity yields partial understanding of numeric logic but frequent API/library mistakes and incorrect numeric computations.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Behavioral comparison across identical prompts with phi-1 and phi-1-base; qualitative analysis of sample completions for numeric tasks (PyGame, Tkinter, SVD, pyplot, divisibility examples).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>HumanEval pass@1: reported ~45% (paper reports a 350M model achieving ~45% on HumanEval in the introduction). LLM-graded understanding score on 50 unconventional problems: ~45% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>API mismatches (e.g., using numpy functions when PyTorch tensors expected), incorrect algorithmic steps (assigning SVD outputs wrongly), producing semantically wrong numeric operations despite some syntactic correctness, and limited capacity for handling multi-step arithmetic/code logic.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Comparisons show phi-1-small sometimes grasps logic but fails in numeric/library specifics, supporting the claim that parameter count contributes to emergent correct numeric/programmatic behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Despite matching some behavioral metrics, phi-1-small frequently outputs incorrect numeric code and wrong API usage; capacity alone is insufficient without aligned finetuning/data quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Textbooks Are All You Need', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8400.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8400.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeExercises</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeExercises (synthetic finetuning dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic dataset (~180M tokens) of short Python exercises and solutions generated with GPT-3.5 where each item is a docstring describing a function to be implemented; used to finetune phi-1-base and markedly improve numeric/algorithmic code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>phi-1 (finetuned using this dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Synthetic exercises dataset intended to align the model to perform function completion tasks from natural language docstrings; generated with GPT-3.5 with diversity constraints (function name constraints) and decontamination steps to reduce overlap with evaluation benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Exercises include many arithmetic- and algorithm-focused tasks (e.g., letter-frequency and numeric-frequency computations, threshold filtering, sorting and squaring, SVD top-k singular-value computations, determinant checks, array elementwise arithmetic and square-root operations).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Acts as a behavior-shaping intervention rather than an explicit representation; authors argue finetuning on these concise, aligned tasks causes the model to reorganize pretraining knowledge enabling improved numeric/algorithmic code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Used directly as a finetuning intervention. To probe contamination and effect, authors performed: n-gram overlap checks (found minimal 13-gram overlaps and false positives), embedding L2 + AST-based similarity pruning at various thresholds (τ), and retraining on pruned versions to assess robustness of the finetuning effect.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Finetuning on CodeExercises produced the largest single improvement: HumanEval pass@1 improved from ~29% (phi-1-base) to ~50.6% (phi-1). Even with >40% aggressive pruning of CodeExercises, retrained phi-1 retained substantially higher HumanEval performance than competing models (see Table 3 numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Dataset contains generation errors (GPT-3.5 produced erroneous examples), prompting potential quality/noise issues; possible but investigated contamination risks with evaluation sets (mitigated by pruning and similarity analyses).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical: large jump in HumanEval and other held-out evaluations after finetuning on this small dataset; qualitative examples show improved logical and arithmetic code completions after finetuning; pruning experiments show effect persists even when removing items similar to evaluation problems.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Authors note synthetic data has non-negligible error rate and that using GPT-4 to generate synthetic data may improve future versions; mechanism remains behavioral and not mechanistically explained at neuron or circuit level.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Textbooks Are All You Need', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8400.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8400.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Data-pruning / decontamination</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embedding + AST similarity based pruning and n-gram overlap decontamination</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline used to remove CodeExercises items similar to evaluation problems to rule out contamination: includes n-gram overlap checks, L2 embedding distance (CodeGen-Mono-350M embeddings), and AST edit-distance thresholds (τ) to decide similarity and prune; retraining experiments assess whether performance gains were memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>phi-1 (retrained on pruned CodeExercises)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model but an intervention/analysis procedure applied to phi-1 training data: (1) n-gram overlap analysis (found minimal direct overlap); (2) embedding L2 distance to find semantically similar items; (3) AST edit-distance matching at thresholds τ∈{0.95,0.9,0.85,0.8} to prune varying amounts of the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used to remove exercises that are similar to numeric/algorithmic HumanEval problems (so affects arithmetic-related training examples such as sorting/squaring/filtering, frequency computations, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>This is a dataset-intervention method, not a representation mechanism; aims to distinguish memorization-based performance from generalization by removing similar training examples.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Intervention: prune CodeExercises by embedding+AST similarity thresholds and retrain phi-1; evaluation compares original phi-1 and retrained phi-1 performance on HumanEval and subsets (similar vs non-similar problems). Also used n-gram checking for direct literal overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 3: across τ thresholds, retrained phi-1 on pruned data often retained high total HumanEval pass@1 (e.g., original phi-1 total 50.6%; retrained varies but remains competitive: at τ=0.95 retrained total 50.6% (no change), at τ=0.9 retrained total 45.1%, at τ=0.85 retrained total 46.3%, at τ=0.8 retrained total 45.1%). These results indicate robustness of gains even after aggressive pruning (removing 42.5K–354K of 879.5K problems).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Pruning is heuristic and may remove items that are only vaguely similar; thresholds trade off false positives/negatives. Embedding and AST distances capture different similarity aspects and may miss some contamination modes (e.g., semantic equivalence with different structure).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Retraining on aggressively pruned datasets still yields high HumanEval performance, providing behavioral evidence that improved test performance is not purely due to direct memorization of near-duplicate training items.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Pruning cannot fully prove absence of memorization; authors note that n-gram checks found a small number of false-positive overlaps and that similarity methods have limits. No mechanistic probe to show how pruned training changes internal arithmetic computations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Textbooks Are All You Need', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8400.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8400.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM grading (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-based LLM grading for understanding on unconventional problems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using GPT-4 as a grader to produce more nuanced, test-like evaluation scores (0–10 scaled) on 50 new, intentionally unconventional coding problems; used to obtain fine-grained 'Understanding' scores rather than binary unit-test pass/fail.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>phi-1 and competing code models (evaluated with GPT-4 as grader)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 is used as an external evaluation LLM (not trained or modified here) to grade candidate solutions produced by phi-1, phi-1-base, phi-1-small, StarCoder, etc., on 50 novel problems.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Graded problems include numeric and algorithmic code tasks unlikely to have appeared in training data (e.g., sort_concat_square_deduplicate, numerical filtering and squaring at even indices, other bespoke numeric operations).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Evaluation method; does not probe internal arithmetic representations but provides a graded behavioral assessment that can capture near-correct numeric reasoning that unit tests might miss.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Uses GPT-4 to produce short verbal evaluations and numeric grades (0–10) for each candidate solution; aggregated into Understanding scores reported in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>LLM-graded Understanding (50 unconventional problems): phi-1 scored ~52%; phi-1-base ~37%; phi-1-small ~45%; StarCoder ~51% (see Table 2). These graded scores align with HumanEval rankings and support phi-1's superior behavioral performance on numeric/algorithmic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>LLM grading introduces grader-model bias; grading is subjective to GPT-4's assessment policy and could differ from human or unit-test judgments. It does not reveal mechanistic causes of errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Concordance between GPT-4 graded scores and HumanEval pass@1 provides convergent behavioral evidence that finetuning and dataset choices improved arithmetic/algorithmic capabilities rather than just overfitting to standard tests.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>LLM-based grading is not a mechanistic probe and could reflect grader biases; it cannot diagnose internal failure modes or provide causal evidence for computational mechanisms of arithmetic in models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Textbooks Are All You Need', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Physics of language models: Part 1, context-free grammar <em>(Rating: 2)</em></li>
                <li>Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation <em>(Rating: 2)</em></li>
                <li>Program synthesis with large language models <em>(Rating: 1)</em></li>
                <li>Tinystories: How small can language models be and still speak coherent english? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8400",
    "paper_id": "paper-2922768fd451ecdb45f48c1a83eb57f54a91221b",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "phi-1",
            "name_full": "phi-1 (finetuned)",
            "brief_description": "A 1.3B-parameter decoder-only Transformer model trained on a small, high-quality code-oriented corpus (CodeTextbook) and then finetuned on a synthetic exercises dataset (CodeExercises); shows large gains on code tasks including many numerically grounded programming problems after finetuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "phi-1",
            "model_description": "Decoder-only Transformer, 1.3B parameters, 24 layers, hidden dim 2048, MLP diemnsion 8192, 32 attention heads (64-d each), rotary position embeddings (rotary dim 32). Pretrained on ~7B tokens (CodeTextbook: filtered real code + &lt;1B synthetic GPT-3.5 textbooks) and finetuned on ~180M tokens CodeExercises; trained with next-token prediction, seq length 2048, FlashAttention implementation.",
            "arithmetic_task_type": "Arithmetic and numeric computation encountered inside code-generation tasks: integer divisibility checks, floor(sqrt(...)) computations, Euclidean and cosine distance using norms, determinant checks (matrix singularity), sorting/concatenation/squaring/filtering by numeric threshold, computing frequency sums and squared-frequency sums, SVD and top-k singular-value operations, elementwise arithmetic on arrays (cos(x)/sqrt(x+1)), basic integer random sampling and bounding, and other numeric loop/array operations.",
            "mechanism_or_representation": "No neuron-level or attention-level internal arithmetic mechanism is identified in the paper. The authors hypothesize that improvements arise from (a) higher-quality 'textbook' data mapping natural language to algorithmic code, and (b) a short finetuning stage on explicit function-completion exercises that 'reorganizes and consolidates' pretraining knowledge, enabling more consistent logical and numeric code generation rather than memorized token-level arithmetic patterns.",
            "probing_or_intervention_method": "Behavioral and dataset interventions: 1) finetuning phi-1-base on the synthetic CodeExercises dataset (&lt;200M tokens); 2) qualitative example comparisons (before/after finetuning) on numeric/logic tasks; 3) creation of 50 unconventional problems (held-out, human-authored) and LLM grading (GPT-4) as evaluation; 4) aggressive data pruning / decontamination using n-gram overlap, embedding L2 distance (CodeGen-Mono-350M embeddings) and AST-based edit-distance thresholds; no mechanistic probes such as activation patching, linear probes, or attention/neuron attribution are used.",
            "performance_metrics": "HumanEval pass@1: reported ~50.6% (Table 1). MBPP pass@1: 55.5% (Table 1). phi-1-base (pre-finetune) HumanEval: ~29%. LLM-graded Understanding on 50 unconventional problems (GPT-4 as grader): phi-1 ~52% (Table 2). Large gain attributable to finetuning: HumanEval improves from ~29% (phi-1-base) to ~50.6% (phi-1) after CodeExercises finetuning.",
            "error_types_or_failure_modes": "Reported failure modes relevant to numeric/algorithmic tasks include: sensitivity to prompt variations (longer prompts or small prompt changes can cause failure), degraded performance for grammatical errors in prompts, occasional incorrect API usage or semantically irrelevant API calls, incorrect or imprecise string handling (e.g., mis-copied text), inability to handle larger/complex system-level tasks (e.g., full Flask apps), and remaining errors originating in the synthetic training data (GPT-3.5-generated examples had non-negligible error rates).",
            "evidence_for_mechanism": "Behavioral evidence: qualitative before/after examples show finetuned model produces correct numeric logic (e.g., correct divisibility check using floor(sqrt(bob_num)) or correct SVD-based numeric updates) and correct API usage for numeric libraries; finetuning on a small targeted exercises dataset resulted in large performance jumps on numeric code tasks, implying consolidation of algorithmic mappings. Data-pruning experiments (removing problems similar to test via embedding+AST thresholds) show retained performance, arguing gains are not solely from memorization of exercises similar to test items.",
            "counterexamples_or_challenges": "No internal mechanistic analysis is provided (no neuron, attention, or circuit-level probes). The model still fails on many prompts and remains sensitive to prompt formulation; synthetic data contains errors yet the model sometimes achieves correct outputs (an unresolved puzzle). The authors explicitly note the absence of finer-grained mechanistic evidence and acknowledge limitations (capacity, domain-specific knowledge, robustness).",
            "uuid": "e8400.0",
            "source_info": {
                "paper_title": "Textbooks Are All You Need",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "phi-1-base",
            "name_full": "phi-1-base (pre-finetuning)",
            "brief_description": "The pretrained checkpoint of the 1.3B model trained on CodeTextbook (filtered real code + GPT-3.5 synthetic textbooks) prior to finetuning on CodeExercises; provides a baseline showing the effect of targeted finetuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "phi-1-base",
            "model_description": "Same architecture as phi-1 (1.3B decoder Transformer) but evaluated prior to finetuning on CodeExercises. Obtained checkpoint at 24k steps (~8 epochs over CodeTextbook, ~50B tokens seen).",
            "arithmetic_task_type": "Same kinds of numeric programming tasks as phi-1 (divisibility, sqrt, linear algebraic ops, array arithmetic), as these appear in coding examples and tests used for comparison.",
            "mechanism_or_representation": "No internal representations probed. Behavior suggests phi-1-base stores some algorithmic knowledge from CodeTextbook pretraining but lacks the reorganization/alignment that finetuning provides; it tends to produce syntactically plausible but semantically incorrect numeric/code logic.",
            "probing_or_intervention_method": "Compared directly to phi-1 and phi-1-small using qualitative prompt examples (divisibility, PyGame, Tkinter, SVD, pyplot animation). Performance comparisons and LLM-graded tests act as behavioral probes.",
            "performance_metrics": "HumanEval pass@1 (phi-1-base): ~29% (reported). LLM-graded Understanding on 50 unconventional problems: ~37% (Table 2).",
            "error_types_or_failure_modes": "Common failure modes include confusion about logical relationships in prompts (mixing indices vs elements), semantically irrelevant or incorrect API/function calls for numeric libraries, incorrect implementation of numeric conditions and loops, and inability to follow multi-step numeric instructions reliably.",
            "evidence_for_mechanism": "Qualitative prompt comparisons in Section 3 show phi-1-base producing incorrect logical/numeric code on examples where phi-1 (finetuned) succeeds; this behavioral difference is cited as evidence that finetuning reorganizes pretraining knowledge to improve arithmetic-like program synthesis.",
            "counterexamples_or_challenges": "phi-1-base can still produce partly correct templates (e.g., pyplot template) but fails to implement numeric update logic; no internal probing to explain failure modes.",
            "uuid": "e8400.1",
            "source_info": {
                "paper_title": "Textbooks Are All You Need",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "phi-1-small",
            "name_full": "phi-1-small (350M)",
            "brief_description": "A smaller 350M-parameter model trained with the same pipeline as phi-1, used to demonstrate the role of model capacity in emergent capabilities for numeric and algorithmic coding tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "phi-1-small",
            "model_description": "Decoder-only Transformer with 350M parameters, 20 layers, hidden dim 1024, MLP dim 4096, 16 attention heads (64-d each). Trained on same CodeTextbook and finetuning pipeline as phi-1-small described in the paper.",
            "arithmetic_task_type": "Numeric/code tasks like simple divisibility, random integer updates, numpy/pytorch numeric ops and SVD-like tasks, simple loops and arithmetic used in example prompts.",
            "mechanism_or_representation": "No mechanistic internal representation analysis; observed behavior suggests limited capacity yields partial understanding of numeric logic but frequent API/library mistakes and incorrect numeric computations.",
            "probing_or_intervention_method": "Behavioral comparison across identical prompts with phi-1 and phi-1-base; qualitative analysis of sample completions for numeric tasks (PyGame, Tkinter, SVD, pyplot, divisibility examples).",
            "performance_metrics": "HumanEval pass@1: reported ~45% (paper reports a 350M model achieving ~45% on HumanEval in the introduction). LLM-graded understanding score on 50 unconventional problems: ~45% (Table 2).",
            "error_types_or_failure_modes": "API mismatches (e.g., using numpy functions when PyTorch tensors expected), incorrect algorithmic steps (assigning SVD outputs wrongly), producing semantically wrong numeric operations despite some syntactic correctness, and limited capacity for handling multi-step arithmetic/code logic.",
            "evidence_for_mechanism": "Comparisons show phi-1-small sometimes grasps logic but fails in numeric/library specifics, supporting the claim that parameter count contributes to emergent correct numeric/programmatic behavior.",
            "counterexamples_or_challenges": "Despite matching some behavioral metrics, phi-1-small frequently outputs incorrect numeric code and wrong API usage; capacity alone is insufficient without aligned finetuning/data quality.",
            "uuid": "e8400.2",
            "source_info": {
                "paper_title": "Textbooks Are All You Need",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "CodeExercises",
            "name_full": "CodeExercises (synthetic finetuning dataset)",
            "brief_description": "A synthetic dataset (~180M tokens) of short Python exercises and solutions generated with GPT-3.5 where each item is a docstring describing a function to be implemented; used to finetune phi-1-base and markedly improve numeric/algorithmic code generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "phi-1 (finetuned using this dataset)",
            "model_description": "Synthetic exercises dataset intended to align the model to perform function completion tasks from natural language docstrings; generated with GPT-3.5 with diversity constraints (function name constraints) and decontamination steps to reduce overlap with evaluation benchmarks.",
            "arithmetic_task_type": "Exercises include many arithmetic- and algorithm-focused tasks (e.g., letter-frequency and numeric-frequency computations, threshold filtering, sorting and squaring, SVD top-k singular-value computations, determinant checks, array elementwise arithmetic and square-root operations).",
            "mechanism_or_representation": "Acts as a behavior-shaping intervention rather than an explicit representation; authors argue finetuning on these concise, aligned tasks causes the model to reorganize pretraining knowledge enabling improved numeric/algorithmic code generation.",
            "probing_or_intervention_method": "Used directly as a finetuning intervention. To probe contamination and effect, authors performed: n-gram overlap checks (found minimal 13-gram overlaps and false positives), embedding L2 + AST-based similarity pruning at various thresholds (τ), and retraining on pruned versions to assess robustness of the finetuning effect.",
            "performance_metrics": "Finetuning on CodeExercises produced the largest single improvement: HumanEval pass@1 improved from ~29% (phi-1-base) to ~50.6% (phi-1). Even with &gt;40% aggressive pruning of CodeExercises, retrained phi-1 retained substantially higher HumanEval performance than competing models (see Table 3 numbers).",
            "error_types_or_failure_modes": "Dataset contains generation errors (GPT-3.5 produced erroneous examples), prompting potential quality/noise issues; possible but investigated contamination risks with evaluation sets (mitigated by pruning and similarity analyses).",
            "evidence_for_mechanism": "Empirical: large jump in HumanEval and other held-out evaluations after finetuning on this small dataset; qualitative examples show improved logical and arithmetic code completions after finetuning; pruning experiments show effect persists even when removing items similar to evaluation problems.",
            "counterexamples_or_challenges": "Authors note synthetic data has non-negligible error rate and that using GPT-4 to generate synthetic data may improve future versions; mechanism remains behavioral and not mechanistically explained at neuron or circuit level.",
            "uuid": "e8400.3",
            "source_info": {
                "paper_title": "Textbooks Are All You Need",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Data-pruning / decontamination",
            "name_full": "Embedding + AST similarity based pruning and n-gram overlap decontamination",
            "brief_description": "A pipeline used to remove CodeExercises items similar to evaluation problems to rule out contamination: includes n-gram overlap checks, L2 embedding distance (CodeGen-Mono-350M embeddings), and AST edit-distance thresholds (τ) to decide similarity and prune; retraining experiments assess whether performance gains were memorization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "phi-1 (retrained on pruned CodeExercises)",
            "model_description": "Not a model but an intervention/analysis procedure applied to phi-1 training data: (1) n-gram overlap analysis (found minimal direct overlap); (2) embedding L2 distance to find semantically similar items; (3) AST edit-distance matching at thresholds τ∈{0.95,0.9,0.85,0.8} to prune varying amounts of the dataset.",
            "arithmetic_task_type": "Used to remove exercises that are similar to numeric/algorithmic HumanEval problems (so affects arithmetic-related training examples such as sorting/squaring/filtering, frequency computations, etc.).",
            "mechanism_or_representation": "This is a dataset-intervention method, not a representation mechanism; aims to distinguish memorization-based performance from generalization by removing similar training examples.",
            "probing_or_intervention_method": "Intervention: prune CodeExercises by embedding+AST similarity thresholds and retrain phi-1; evaluation compares original phi-1 and retrained phi-1 performance on HumanEval and subsets (similar vs non-similar problems). Also used n-gram checking for direct literal overlap.",
            "performance_metrics": "Table 3: across τ thresholds, retrained phi-1 on pruned data often retained high total HumanEval pass@1 (e.g., original phi-1 total 50.6%; retrained varies but remains competitive: at τ=0.95 retrained total 50.6% (no change), at τ=0.9 retrained total 45.1%, at τ=0.85 retrained total 46.3%, at τ=0.8 retrained total 45.1%). These results indicate robustness of gains even after aggressive pruning (removing 42.5K–354K of 879.5K problems).",
            "error_types_or_failure_modes": "Pruning is heuristic and may remove items that are only vaguely similar; thresholds trade off false positives/negatives. Embedding and AST distances capture different similarity aspects and may miss some contamination modes (e.g., semantic equivalence with different structure).",
            "evidence_for_mechanism": "Retraining on aggressively pruned datasets still yields high HumanEval performance, providing behavioral evidence that improved test performance is not purely due to direct memorization of near-duplicate training items.",
            "counterexamples_or_challenges": "Pruning cannot fully prove absence of memorization; authors note that n-gram checks found a small number of false-positive overlaps and that similarity methods have limits. No mechanistic probe to show how pruned training changes internal arithmetic computations.",
            "uuid": "e8400.4",
            "source_info": {
                "paper_title": "Textbooks Are All You Need",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "LLM grading (GPT-4)",
            "name_full": "GPT-4-based LLM grading for understanding on unconventional problems",
            "brief_description": "Using GPT-4 as a grader to produce more nuanced, test-like evaluation scores (0–10 scaled) on 50 new, intentionally unconventional coding problems; used to obtain fine-grained 'Understanding' scores rather than binary unit-test pass/fail.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "phi-1 and competing code models (evaluated with GPT-4 as grader)",
            "model_description": "GPT-4 is used as an external evaluation LLM (not trained or modified here) to grade candidate solutions produced by phi-1, phi-1-base, phi-1-small, StarCoder, etc., on 50 novel problems.",
            "arithmetic_task_type": "Graded problems include numeric and algorithmic code tasks unlikely to have appeared in training data (e.g., sort_concat_square_deduplicate, numerical filtering and squaring at even indices, other bespoke numeric operations).",
            "mechanism_or_representation": "Evaluation method; does not probe internal arithmetic representations but provides a graded behavioral assessment that can capture near-correct numeric reasoning that unit tests might miss.",
            "probing_or_intervention_method": "Uses GPT-4 to produce short verbal evaluations and numeric grades (0–10) for each candidate solution; aggregated into Understanding scores reported in Table 2.",
            "performance_metrics": "LLM-graded Understanding (50 unconventional problems): phi-1 scored ~52%; phi-1-base ~37%; phi-1-small ~45%; StarCoder ~51% (see Table 2). These graded scores align with HumanEval rankings and support phi-1's superior behavioral performance on numeric/algorithmic tasks.",
            "error_types_or_failure_modes": "LLM grading introduces grader-model bias; grading is subjective to GPT-4's assessment policy and could differ from human or unit-test judgments. It does not reveal mechanistic causes of errors.",
            "evidence_for_mechanism": "Concordance between GPT-4 graded scores and HumanEval pass@1 provides convergent behavioral evidence that finetuning and dataset choices improved arithmetic/algorithmic capabilities rather than just overfitting to standard tests.",
            "counterexamples_or_challenges": "LLM-based grading is not a mechanistic probe and could reflect grader biases; it cannot diagnose internal failure modes or provide causal evidence for computational mechanisms of arithmetic in models.",
            "uuid": "e8400.5",
            "source_info": {
                "paper_title": "Textbooks Are All You Need",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Physics of language models: Part 1, context-free grammar",
            "rating": 2
        },
        {
            "paper_title": "Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation",
            "rating": 2
        },
        {
            "paper_title": "Program synthesis with large language models",
            "rating": 1
        },
        {
            "paper_title": "Tinystories: How small can language models be and still speak coherent english?",
            "rating": 1
        }
    ],
    "cost": 0.018494249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Textbooks Are All You Need</h1>
<p>Suriya Gunasekar Yi Zhang Jyoti Aneja Caio César Teodoro Mendes Allie Del Giorno Sivakanth Gopi Mojan Javaheripi Piero Kauffmann Gustavo de Rosa Olli Saarikivi Adil Salim Shital Shah Harkirat Singh Behl Xin Wang Sébastien Bubeck Ronen Eldan Adam Tauman Kalai Yin Tat Lee Yuanzhi Li<br>Microsoft Research</p>
<h4>Abstract</h4>
<p>We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of "textbook quality" data from the web ( 6 B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy $50.6 \%$ on HumanEval and $55.5 \%$ on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves $45 \%$ on HumanEval.</p>
<h2>1 Introduction</h2>
<p>The art of training large artificial neural networks has made extraordinary progress in the last decade, especially after the discovery of the Transformer architecture [VSP ${ }^{+} 17$ ], yet the science behind this success remains limited. Amidst a vast and confusing array of results, a semblance of order emerged around the same time as Transformers were introduced, namely that performance improves somewhat predictably as one scales up either the amount of compute or the size of the network [ $\mathrm{HNA}^{+} 17$ ], a phenomenon which is now referred to as scaling laws $\left[\mathrm{KMH}^{+} 20\right]$. The subsequent exploration of scale in deep learning was guided by these scaling laws $\left[\mathrm{BMR}^{+} 20\right]$, and discoveries of variants of these laws led to rapid jump in performances $\left[\mathrm{HBM}^{+} 22\right]$. In this work, following the footsteps of Eldan and Li [EL23], we explore the improvement that can be obtained along a different axis: the quality of the data. It has long been known that higher quality data leads to better results, e.g., data cleaning is an important part of modern dataset creation $\left[\mathrm{RSR}^{+} 20\right]$, and it can yield other side benefits such as somewhat smaller datasets $\left[\mathrm{LYR}^{+} 23, \mathrm{YGK}^{+} 23\right]$ or allowing for more passes on the data $\left[\mathrm{MRB}^{+} 23\right]$. The recent work of Eldan and Li on TinyStories (a high quality dataset synthetically generated to teach English to neural networks) showed that in fact the effect of high quality data extends well past this: improving data quality can dramatically change the shape of the scaling laws, potentially allowing to match the performance of large-scale models with much leaner training/models. In this work we go beyond the initial foray of Eldan and Li to show that high quality data can even improve the SOTA of large language models (LLMs), while dramatically reducing the dataset size and training compute. Importantly, smaller models requiring less training can significantly reduce the environmental cost of LLMs [BGMMS21].</p>
<p>We focus our attention on LLMs trained for code, and specifically writing simple Python functions from their docstrings as in $\left[\mathrm{CTJ}^{+} 21\right]$. The evaluation benchmark proposed in the latter work, HumanEval, has been widely adopted for comparing LLMs' performance on code. We demonstrate the power of high</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Date</th>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Model size <br> (Parameters)</th>
<th style="text-align: left;">Dataset size <br> (Tokens)</th>
<th style="text-align: left;">HumanEval <br> (Pass@1)</th>
<th style="text-align: left;">MBPP <br> (Pass@1)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">2021 Jul</td>
<td style="text-align: left;">Codex-300M [CTJ ${ }^{+} 21$ ]</td>
<td style="text-align: left;">300 M</td>
<td style="text-align: left;">100 B</td>
<td style="text-align: left;">$13.2 \%$</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">2021 Jul</td>
<td style="text-align: left;">Codex-12B [CTJ ${ }^{+} 21$ ]</td>
<td style="text-align: left;">12 B</td>
<td style="text-align: left;">100 B</td>
<td style="text-align: left;">$28.8 \%$</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">2022 Mar</td>
<td style="text-align: left;">CodeGen-Mono-350M [NPH ${ }^{+} 23$ ]</td>
<td style="text-align: left;">350 M</td>
<td style="text-align: left;">577 B</td>
<td style="text-align: left;">$12.8 \%$</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">2022 Mar</td>
<td style="text-align: left;">CodeGen-Mono-16.1B [NPH ${ }^{+} 23$ ]</td>
<td style="text-align: left;">16.1 B</td>
<td style="text-align: left;">577 B</td>
<td style="text-align: left;">$29.3 \%$</td>
<td style="text-align: left;">$35.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">2022 Apr</td>
<td style="text-align: left;">PaLM-Coder [CND ${ }^{+} 22$ ]</td>
<td style="text-align: left;">540 B</td>
<td style="text-align: left;">780 B</td>
<td style="text-align: left;">$35.9 \%$</td>
<td style="text-align: left;">$47.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">2022 Sep</td>
<td style="text-align: left;">CodeGeeX [ZXZ ${ }^{+} 23$ ]</td>
<td style="text-align: left;">13 B</td>
<td style="text-align: left;">850 B</td>
<td style="text-align: left;">$22.9 \%$</td>
<td style="text-align: left;">$24.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">2022 Nov</td>
<td style="text-align: left;">GPT-3.5 [Ope23]</td>
<td style="text-align: left;">175 B</td>
<td style="text-align: left;">N.A.</td>
<td style="text-align: left;">$47 \%$</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">2022 Dec</td>
<td style="text-align: left;">SantaCoder [ALK ${ }^{+} 23$ ]</td>
<td style="text-align: left;">1.1 B</td>
<td style="text-align: left;">236 B</td>
<td style="text-align: left;">$14.0 \%$</td>
<td style="text-align: left;">$35.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">2023 Mar</td>
<td style="text-align: left;">GPT-4 [Ope23]</td>
<td style="text-align: left;">N.A.</td>
<td style="text-align: left;">N.A.</td>
<td style="text-align: left;">$67 \%$</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">2023 Apr</td>
<td style="text-align: left;">Replit [Rep23]</td>
<td style="text-align: left;">2.7 B</td>
<td style="text-align: left;">525 B</td>
<td style="text-align: left;">$21.9 \%$</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">2023 Apr</td>
<td style="text-align: left;">Replit-Finetuned [Rep23]</td>
<td style="text-align: left;">2.7 B</td>
<td style="text-align: left;">525 B</td>
<td style="text-align: left;">$30.5 \%$</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">2023 May</td>
<td style="text-align: left;">CodeGen2-1B [NHX ${ }^{+} 23$ ]</td>
<td style="text-align: left;">1 B</td>
<td style="text-align: left;">N.A.</td>
<td style="text-align: left;">$10.3 \%$</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">2023 May</td>
<td style="text-align: left;">CodeGen2-7B [NHX ${ }^{+} 23$ ]</td>
<td style="text-align: left;">7 B</td>
<td style="text-align: left;">N.A.</td>
<td style="text-align: left;">$19.1 \%$</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">2023 May</td>
<td style="text-align: left;">StarCoder [LAZ ${ }^{+} 23$ ]</td>
<td style="text-align: left;">15.5 B</td>
<td style="text-align: left;">1 T</td>
<td style="text-align: left;">$33.6 \%$</td>
<td style="text-align: left;">$52.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">2023 May</td>
<td style="text-align: left;">StarCoder-Prompted [LAZ ${ }^{+} 23$ ]</td>
<td style="text-align: left;">15.5 B</td>
<td style="text-align: left;">1 T</td>
<td style="text-align: left;">$40.8 \%$</td>
<td style="text-align: left;">$49.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">2023 May</td>
<td style="text-align: left;">PaLM 2-S [ADF ${ }^{+} 23$ ]</td>
<td style="text-align: left;">N.A.</td>
<td style="text-align: left;">N.A.</td>
<td style="text-align: left;">$37.6 \%$</td>
<td style="text-align: left;">$50.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">2023 May</td>
<td style="text-align: left;">CodeT5+ [WLG ${ }^{+} 23$ ]</td>
<td style="text-align: left;">2 B</td>
<td style="text-align: left;">52 B</td>
<td style="text-align: left;">$24.2 \%$</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">2023 May</td>
<td style="text-align: left;">CodeT5+ [WLG ${ }^{+} 23$ ]</td>
<td style="text-align: left;">16 B</td>
<td style="text-align: left;">52 B</td>
<td style="text-align: left;">$30.9 \%$</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">2023 May</td>
<td style="text-align: left;">InstructCodeT5+ [WLG ${ }^{+} 23$ ]</td>
<td style="text-align: left;">16 B</td>
<td style="text-align: left;">52 B</td>
<td style="text-align: left;">$35.0 \%$</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">2023 Jun</td>
<td style="text-align: left;">WizardCoder [LXZ ${ }^{+} 23$ ]</td>
<td style="text-align: left;">16 B</td>
<td style="text-align: left;">1 T</td>
<td style="text-align: left;">$57.3 \%$</td>
<td style="text-align: left;">$51.8 \%$</td>
</tr>
<tr>
<td style="text-align: left;">2023 Jun</td>
<td style="text-align: left;">phi-1</td>
<td style="text-align: left;">1.3 B</td>
<td style="text-align: left;">7 B</td>
<td style="text-align: left;">$50.6 \%$</td>
<td style="text-align: left;">$55.5 \%$</td>
</tr>
</tbody>
</table>
<p>Table 1: We use self-reported scores whenever available. Despite being trained at vastly smaller scale, phi-1 outperforms competing models on HumanEval and MBPP, except for GPT-4 (also WizardCoder obtains better HumanEval but worse MBPP).
quality data in breaking existing scaling laws by training a 1.3B-parameter model, which we call phi-1, for roughly 8 passes over 7B tokens (slightly over 50B total tokens seen) followed by finetuning on less than 200 M tokens. Roughly speaking we pretrain on "textbook quality" data, both synthetically generated (with GPT-3.5) and filtered from web sources, and we finetune on "textbook-exercise-like" data. Despite being several orders of magnitude smaller than competing models, both in terms of dataset and model size (see Table 1), we attain $50.6 \%$ pass@1 accuracy on HumanEval and $55.5 \%$ pass@1 accuracy on MBPP (Mostly Basic Python Programs), which are one of the best self-reported numbers using only one LLM generation. In Section 2, we give some details of our training process, and we discuss evidence for the importance of our data selection process in achieving this result. Moreover, despite being trained on much fewer tokens compared to existing models, phi-1 still displays emergent properties. In Section 3 we discuss these emergent properties, and in particular we confirm the hypothesis that the number of parameters plays a key role in emergence (see e.g., $\left[\mathrm{WTB}^{+} 22\right]$ ), by comparing the outputs of phi-1 with those of phi-1-small, a model trained with the same pipeline but with only 350 M parameters. The methodology used in this section is reminiscent of the Sparks of AGI paper [BCE ${ }^{+} 23$ ] that argued for moving away from static benchmarks to test LLMs' performance. Finally in Section 4 we discuss alternative benchmarks to evaluate the model and in Section 5 we study possible contamination of our training data with respect to HumanEval. We release the model for usage and evaluation by the broader community, but omit some details of the synthetic data generation, for proprietary reasons.</p>
<p>More related works Our work is part of the recent program of using LLMs for program synthesis, see $\left[\mathrm{CTJ}^{+} 21, \mathrm{NPH}^{+} 22\right]$ for more references on this. Our approach is also part of the emerging trend of</p>
<p>using existing LLMs to synthesize data for the training of new generations of LLMs, [WKM $\left.{ }^{+} 22, \mathrm{TGZ}{ }^{+} 23\right.$, $\mathrm{MMJ}^{+} 23, \mathrm{LGK}^{+} 23, \mathrm{JWJ}^{+} 23]$. There is an ongoing debate about whether such "recursive training" might lead to narrower scope for the resulting LLM $\left[\mathrm{SSZ}^{+} 23, \mathrm{GWS}^{+} 23\right]$, see $\left[\mathrm{MMJ}^{+} 23\right]$ for a counterviewpoint. Note that in this paper we focus on a narrow task, similarly to $\left[\mathrm{JWJ}^{+} 23\right]$, in which case it seems plausible to attain better performance than the teacher LLM on that specific task (as is argued in the latter paper).</p>
<h1>2 Training details and the importance of high-quality data</h1>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2.1: Pass@1 accuracy (\%) on HumanEval. The grouping of bar plots correspond to the usual scaling dimensions of either increasing the compute time (more passes on the data, here from 26B tokens seen to 76B) or increasing the number of parameters of the model (here from 350 M to 1.3 B ). Each column within a group corresponds to different training datasets: (A) The first (orange) column represents the performance of models trained on the standard dataset of deduplicated Python files from The Stack (plus StackOverflow for 1.3B parameter model); (B) The second (light green) column represents the performance of models trained with our new dataset composition CodeTextbook; (C) Finally, the third (dark green) column corresponds to the respective second column models finetuned on our new CodeExercises dataset. For the 1.3B models, phi-1 and phi-1-base are checkpoints after training on 51B tokens ( 770 GPU hours) and The Stack+ model was trained for 76B tokens and 1090 GPU hours. We highlight that even without any finetuning, our phi-1-base model trained on CodeTextbook dataset achieves $29 \%$ HumanEval performance with a mere 1.3B parameter model. The previous smallest model that achieves close to $30 \%$ performance on HumanEval was Replit-Finetuned at 2.7B parameters, which was trained with 100 times more training tokens than us [Rep23]. On top of this, finetuning on our CodeExercises dataset to obtain phi-1 not only gives us our top performance of $51 \%$ on HumanEval, but also unlocks further unexpected coding capabilities (see Section 3).</p>
<p>As alluded to in the title of the paper, the central ingredient our model relies on textbook-quality training data. Unlike previous work that used standard sources of text data for code generation, such as The Stack $\left[\mathrm{KLA}^{+} 22\right]$ (which contains sourcecode from repositories with permissive licenses) and other web-based datasets (e.g., StackOverflow and CodeContest $\left[\mathrm{LCC}^{+} 22\right]$ ), we argue that these sources are not optimal for teaching the model how to reason and plan algorithmically. On the other hand, our model architecture and training methods are fairly conventional (Section 2.3), so we devote this section primarily to explaining how we curated our data.</p>
<p>The standard code datasets $\left[\mathrm{KLA}^{+} 22, \mathrm{LCC}^{+} 22\right]$ form a large and diverse corpus covering broad range of topics and use cases. However, based on manual inspection of random samples we observe that many of these snippets are not very instructive for learning the basics of coding, and suffer from several drawbacks:</p>
<ul>
<li>Many samples are not self-contained, meaning that they depend on other modules or files that are external to the snippet, making them hard to understand without additional context.</li>
<li>Typical examples do not involve any meaningful computation, but rather consist of trivial or boilerplate code, such as defining constants, setting parameters, or configuring GUI elements.</li>
<li>Samples that do contain algorithmic logic are often buried inside complex or poorly documented functions, making them difficult to follow or learn from.</li>
<li>The examples are skewed towards certain topics or use cases, resulting in an unbalanced distribution of coding concepts and skills across the dataset.</li>
</ul>
<p>One can only imagine how frustrating and inefficient it would be for a human learner to try to acquire coding skills from these datasets, as they would have to deal with a lot of noise, ambiguity, and incompleteness in the data. We hypothesize that these issues also affect the performance of language models, as they reduce the quality and quantity of the signal that maps natural language to code. We conjecture that language models would benefit from a training set that has the same qualities as a good "textbook": it should be clear, self-contained, instructive, and balanced.</p>
<p>In this work, we address this challenge directly and show that by intentionally selecting and generating high-quality data, we can achieve state-of-the-art results on code-generation tasks with a much smaller model and less compute than existing approaches. Our training relies on three main datasets:</p>
<ul>
<li>A filtered code-language dataset, which is a subset of The Stack and StackOverflow, obtained by using a language model-based classifier (consisting of about 6B tokens).</li>
<li>A synthetic textbook dataset consisting of $&lt;1$ B tokens of GPT-3.5 generated Python textbooks.</li>
<li>A small synthetic exercises dataset consisting of $\sim 180 \mathrm{M}$ tokens of Python exercises and solutions.</li>
</ul>
<p>We describe those datasets in more detail in the next subsections. Taken together, the above datasets contain less than 7B tokens. We refer to the combination of filtered code-language and synthetic textbook datasets as "CodeTextbook" and use it in the pretraining phase to obtain our base model phi-1-base -this model already achieves a competitive HumanEval performance of $29 \%$. Then we use the 180M token synthetic exercises dataset, referred to as "CodeExercises", to finetune our phi-1-base model to obtain phi-1. Despite the small size of the "CodeExercises" dataset, finetuning with this dataset is crucial not only for large improvements in generating simple Python function as shown in Figure 2.1, but more broadly to unlock many interesting emergent capabilities in our phi-1 model that are not observed in phi-1-base (see Section 3).</p>
<h1>2.1 Filtering of existing code datasets using a transformer-based classifier</h1>
<p>We begin with publicly available Python code datasets: we use the Python subset of the deduplicated version of The Stack and the StackOverflow, which together contain over 35 million files/samples, totalling over 35B tokens. We annotate the quality of a small subset of these files (about 100k samples) using GPT-4: given a code snippet, the model is prompted to "determine its educational value for a student whose goal is to learn basic coding concepts".</p>
<p>We then use this annotated dataset to train a random forest classifier that predicts the quality of a file/sample using its output embedding from a pretrained codegen model as features. We note that unlike GPT-3.5, which we use extensively to generate synthetic content (discussed below), we use GPT-4 minimally only for annotations on the quality of a small subset of The Stack and StackOverflow samples. We thus view our usage of GPT-4 as merely a way to avoid tedious human-annotation efforts [DLT*23].</p>
<h1>Educational values deemed by the filter</h1>
<h2>High educational value</h2>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="k">def</span><span class="w"> </span><span class="nf">normalize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Performs L2-Norm.&quot;&quot;&quot;</span>
    <span class="n">num</span> <span class="o">=</span> <span class="n">x</span>
    <span class="n">denom</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">le</span><span class="o">-</span><span class="mi">12</span>
    <span class="k">return</span> <span class="n">num</span> <span class="o">/</span> <span class="n">denom</span>
<span class="k">def</span><span class="w"> </span><span class="nf">euclidean_dist</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Computes Euclidean distance.&quot;&quot;&quot;</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">xx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span>
    <span class="n">expand</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">yy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span>
    <span class="n">expand</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
    <span class="n">dist</span> <span class="o">=</span> <span class="n">xx</span> <span class="o">+</span> <span class="n">yy</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
    <span class="n">dist</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">)</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">dist</span>
<span class="k">def</span><span class="w"> </span><span class="nf">cosine_dist</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Computes Cosine Distance.&quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">dist</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">dist</span>
</code></pre></div>

<h2>Low educational value</h2>
<p>import re
import typing
・.
class Default(object):
def <strong>init</strong>(self, vim: Nvim) -&gt; None:
self._vim = vim
self._denite: typing. Optional[SyncParent]
$=$ None
self._selected_candidates: typing.List[int
$]=[]$
self._candidates: Candidates = []
self._cursor = 0
self._entire_len = 0
self._result: typing. List[typing.Any] = []
self._context: UserContext = {}
self._bufnr $=-1$
self._winid $=-1$
self._winrestcmd $=$ "
self._initialized = False
self._winheight $=0$
self._winwidth $=0$
self._winminheight $=-1$
self._is_multi = False
self._is_async = False
self._matched_pattern = "
・.</p>
<p>Our filtering methodology boosts our model performance significantly even without the synthetic datasets discussed below: for 350 M parameter models trained on unfiltered Stack (deduplicated python) and StackOverflow, the HumanEval performance saturates at $12.19 \%$ even after training for 96 k steps ( 200B tokens), while training on the filtered subset achieves $17.68 \%$ on HumanEval after 36k steps. We further improve this to $20.12 \%$ (reported in Figure 2.1) by training on a combination of the filtered dataset and the synthetic textbooks dataset discussed below.</p>
<h3>2.2 Creation of synthetic textbook-quality datasets</h3>
<p>One of the main challenges in creating a high-quality dataset for code generation is ensuring that the examples are diverse and non-repetitive. By diversity, we mean that the examples should cover a wide range of coding concepts, skills, and scenarios, and that they should vary in their level of difficulty, complexity, and style. Diversity is important for several reasons: it exposes the language model to different ways of expressing and solving problems in code, it reduces the risk of overfitting or memorizing specific patterns or solutions, and it increases the generalization and robustness of the model to unseen or novel tasks. However, achieving diversity is not trivial, especially when using synthetic data generated by another language model. Simply prompting the model to produce a coding textbook or a set of exercises, even with some variation in the instructions or the parameters, will likely result in a very homogeneous and redundant dataset, where the same concepts and solutions are repeated over and over with minor changes. This is because language models tend to follow the most probable or common paths given their training data and their priors, and they lack the creativity or the incentive to explore alternative or novel ways of generating code. Therefore, one needs to find the right "trick" that will induce the language model to be more creative and diverse in its output, while still maintaining the quality and the coherence of the examples. Inspired by [EL23], where a diverse set of short stories were created by including a</p>
<p>random subset of words chosen from some fixed vocabulary in the prompt and requiring that they would be somehow combined in the generated text, we look for ways to inject randomness into the prompt in a way that gives rise to the generation of a diverse dataset.</p>
<h1>The synthetic textbook dataset</h1>
<p>This dataset consists of less that 1B tokens of GPT-3.5 generated Python textbooks, synthesized to provide a high-quality source of natural language heavy text interleaved with relevant code snippets. We further targeted the content of these textbooks to cover topics that promote reasoning and basic algorithmic skills. Here, diversity is obtained by providing constraints on topics and target audience of the generated textbook. The following is an example text from the synthetic textbook:</p>
<div class="codehilite"><pre><span></span><code><span class="n">To</span> <span class="n">begin</span><span class="p">,</span> <span class="n">let</span> <span class="n">us</span> <span class="n">define</span> <span class="n">singular</span> <span class="ow">and</span> <span class="n">nonsingular</span> <span class="n">matrices</span><span class="o">.</span> <span class="n">A</span> <span class="kp">matrix</span> <span class="ow">is</span> <span class="n">said</span> <span class="n">to</span> <span class="n">be</span> <span class="n">singular</span> <span class="k">if</span> <span class="n">its</span>
<span class="n">determinant</span> <span class="ow">is</span> <span class="n">zero</span><span class="o">.</span> <span class="n">On</span> <span class="n">the</span> <span class="n">other</span> <span class="n">hand</span><span class="p">,</span> <span class="n">a</span> <span class="kp">matrix</span> <span class="ow">is</span> <span class="n">said</span> <span class="n">to</span> <span class="n">be</span> <span class="n">nonsingular</span> <span class="k">if</span> <span class="n">its</span> <span class="n">determinant</span> <span class="ow">is</span> <span class="ow">not</span>
    <span class="n">zero</span><span class="o">.</span> <span class="n">Now</span><span class="p">,</span> <span class="n">let</span><span class="s1">&#39;s explore these concepts through examples.</span>
<span class="n">Example</span> <span class="mi">1</span><span class="p">:</span> <span class="n">Consider</span> <span class="n">the</span> <span class="kp">matrix</span> <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span><span class="o">.</span> <span class="n">We</span> <span class="n">can</span> <span class="n">check</span> <span class="k">if</span> <span class="n">this</span> <span class="kp">matrix</span> <span class="ow">is</span>
<span class="n">singular</span> <span class="ow">or</span> <span class="n">nonsingular</span> <span class="n">using</span> <span class="n">the</span> <span class="n">determinant</span> <span class="n">function</span><span class="o">.</span> <span class="n">We</span> <span class="n">can</span> <span class="n">define</span> <span class="n">a</span> <span class="n">Python</span> <span class="n">function</span><span class="p">,</span> <span class="s1">&#39;</span>
<span class="n">is_singular</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="s1">&#39;, which returns true if the determinant of A is zero, and false otherwise.</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="k">def</span><span class="w"> </span><span class="nf">is_singular</span><span class="p">(</span><span class="n">A</span><span class="p">):</span>
    <span class="n">det</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">det</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">is_singular</span><span class="p">(</span><span class="n">A</span><span class="p">))</span> <span class="c1"># True</span>
</code></pre></div>

<h2>The CodeExercises dataset</h2>
<p>This is a small synthetic exercises dataset consisting of less than 180M tokens of Python exercises and solutions. Each exercise is a docstring of a function that needs to be completed. The goal of this dataset is to align the model to perform function completion tasks based on natural language instructions. This dataset was also generated by GPT-3.5, where the main means of eliciting diversity is by constraining the function names. For this dataset in particular, we conduct explicit decontamination and alternative evaluations in the following sections to ensure that problems similar to those from HumanEval benchmark are not seen during finetuning. The following snippet illustrates a synthetically generated exercise.</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">valid_guessing_letters</span><span class="p">(</span><span class="nl">word</span><span class="p">:</span><span class="w"> </span><span class="nf">str</span><span class="p">,</span><span class="w"> </span><span class="nl">guesses</span><span class="p">:</span><span class="w"> </span><span class="n">List</span><span class="o">[</span><span class="n">str</span><span class="o">]</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">List</span><span class="o">[</span><span class="n">str</span><span class="o">]</span><span class="err">:</span>
<span class="w">    </span><span class="ss">&quot;&quot;&quot;</span>
<span class="ss">    Returns a list of valid guessing letters, which are letters that have not been guessed yet and</span>
<span class="ss">    are present in the word.</span>
<span class="ss">    Parameters:</span>
<span class="ss">    word (str): The word to guess.</span>
<span class="ss">    guesses (List[str]): A list of letters that have already been guessed.</span>
<span class="ss">    Returns:</span>
<span class="ss">    List[str]: A list of valid guessing letters.</span>
<span class="ss">    &quot;&quot;&quot;</span>
<span class="w">    </span><span class="n">valid_letters</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">letter</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nl">word</span><span class="p">:</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">letter</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">guesses</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">letter</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nl">valid_letters</span><span class="p">:</span>
<span class="w">            </span><span class="n">valid_letters</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">letter</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">valid_letters</span>
</code></pre></div>

<h1>2.3 Model architecture and training</h1>
<p>We use a decoder only transformer [VSP ${ }^{+} 17$ ] model using the FlashAttention implementation of multihead attention (MHA) [DFE ${ }^{+} 22$ ]. We also use MHA and MLP layers in parallel configuration following some recent models like CodeGen [NPH ${ }^{+} 22$ ], PaLM [CND ${ }^{+} 22$ ], and GPT-NeoX [BBH ${ }^{+} 22$ ]. The architecture for our 1.3B parameter phi-1 model consists of 24 layers, hidden dimension of 2048, MLP-inner dimension of 8192, and 32 attention heads of dimension 64 each. The smaller 350M parameter phi-1-small model consists of 20 layers, hidden dimension of 1024, MLP-inner dimension of 4096, and 16 attention heads of dimension 64 each. We also use a rotary position embedding [SLP ${ }^{+} 21$ ] with rotary dimension 32. These architectural choices were adopted from [NPH ${ }^{+} 22$ ]. We also use the same tokenizer as codegen-350M-mono [NPH ${ }^{+} 22$ ]. Aside from FlashAttention, our models do not use other techniques like Fill-In-the-Middle (FIM) [BJT ${ }^{+} 22$ ], or Multi-Query-Attention (MQA) [RSR ${ }^{+} 20$ ] that could further boost performance and efficiency $\left[\mathrm{LAZ}^{+} 23\right]$.</p>
<p>For both pretraining and finetuning, we concatenate our respective datasets into a single dimensional array with "(endoftext|)" token used for separating the files. We train our models on sequence length of 2048 sliced from our dataset array with next-token prediction loss. We use fp16 training with AdamW optimizer, linear-warmup-linear-decay learning rate schedule, and attention and residual dropout of 0.1 . We train on 8 Nvidia-A100 GPUs using deepspeed. Our pretrained base model phi-1-base was obtained in under 4 days of training. Finetuning to obtain phi-1 used an additional 7 hours on the same hardware.</p>
<p>Pretraining. phi-1-base was trained on the CodeTextbook dataset (filtered code-language corpus and synthetic textbooks). We use effective batch size 1024 (including data parallelism and gradient accumulation), maximum learning rate $1 \mathrm{e}-3$ with warmup over 750 steps, and weight decay 0.1 , for a total of 36,000 steps. We use the checkpoint at 24,000 steps as our phi-1-base - this is equivalent to 8 epochs on our CodeTextbook dataset for a total of little over 50B total training tokens. Despite the small size and computation, this model already achieves a $29 \%$ accuracy on HumanEval.</p>
<p>Finetuning. phi-1 is obtained by finetuning phi-1-base on the CodeExercises dataset. For finetuning, we use the same setup as pretraining, but different hyperparameters: we use effective batchsize of 256, maximum learning rate $1 \mathrm{e}-4$ with 50 steps of warmup, and weight decay 0.01 . We train for total of 6,000 steps and pick the best checkpoint (saved every 1000 steps).</p>
<h2>3 Spikes of model capability after finetuning on CodeExercises</h2>
<p>Figure 2.1 showed that the largest improvement in HumanEval resulted from finetuning on the small CodeExercises dataset ( $&lt;200$ M tokens). CodeExercises consist exclusively of short Python tasks using only basic Python libraries. In this section, we demonstrate that, quite remarkably the model after finetuning also exhibits a substantial improvement in executing tasks that are not featured in the finetuning dataset. This includes managing intricate algorithmic tasks and using external libraries. This suggests that our finetuning process might have helped the model in reorganizing and consolidating the knowledge acquired during pretraining, even if such knowledge is not explicitly present in our CodeExercises dataset. In this section we will focus on qualitatively comparing and contrasting the capabilities of our finetuned model phi-1 and its pretrained 1.3B parameter base model phi-1-base.</p>
<h3>3.1 Finetuning improves the model's understanding</h3>
<p>Using a simple Python function that we created ourselves, we observe below that the model shows a much higher level of understanding and compliance with the instructions after finetuning. In particular, we see</p>
<p>that phi-1-base struggles with the logical relationships in the prompts, while phi-1 can interpret the question and generate the answer correctly. In this example, even our 350 M phi-1-small model shows some level of understanding of the problem even though the resulting solution is wrong. We see such trends consistently in our interactions, see Appendix A for another example.</p>
<h1>Prompt:</h1>
<p>"" Alice, Bob and Charles are playing games. Alice first pick a random number from 333 to 678, Bob then pick a number starting from Alice's number to 888, Charles then pick a number starting from 123 to Bob's number. Alice gets one point if Alice's number minus Charles's number is divisible by the floor of the square root of Bob's number, otherwise Bob gets one point. Simulate Alice's and Bob's points in 30 iterations.
"""
phi-1
points $=[0,0]$
for i in range(30):
alice_num = random.randint
$(333,678)$
bob_num = random.randint(
alice_num, 888)
charles_num = random.randint
(123, bob_num)
if (alice_num - charles_num)
$\%$ int(bob_num $* * 0.5$ ) == 0: points[0] $+=1$
else:
points[1] $+=1$
return f"Alice: {points[0]}, Bob: {points[1]}"
phi-1-base
self. points $=0$
self.alice $=333$
self.bob $=678$
self.charles $=888$
self.bob_points $=0$
self.alice_points $=0$
self.iterations $=30$
self.alice_turn = True
self.bob_turn = True
self.alice_turn_points $=0$
self.bob_turn_points $=0$
self.alice_turn_points_history $=$
[ ]
self.bob_turn_points_history $=[]$
self.alice_turn_points_history
.append(self.alice_points)
self.bob_turn_points_history
.append(self.bob_points)</p>
<h2>phi-1-small</h2>
<div class="codehilite"><pre><span></span><code><span class="n">indices</span> <span class="o">=</span> <span class="p">[]</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="n">alice_points</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">bob_points</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">30</span><span class="p">):</span>
    <span class="n">alice_num</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span>
    <span class="p">(</span><span class="mi">333</span><span class="p">,</span> <span class="mi">678</span><span class="p">)</span>
    <span class="n">bob_num</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">123</span><span class="p">,</span>
        <span class="n">bob_num</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">alice_num</span> <span class="o">~</span> <span class="n">bob_num</span><span class="p">)</span> <span class="o">%</span>
    <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">bob_num</span>
    <span class="p">))</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">alice_points</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">bob_points</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Alice gets </span><span class="si">{</span><span class="n">alice_points</span><span class="si">}</span>
    <span class="n">points</span><span class="p">,</span> <span class="k">while</span> <span class="n">Bob</span> <span class="n">gets</span> <span class="p">{</span>
    <span class="n">bob_points</span><span class="p">}</span> <span class="n">points</span><span class="o">.</span><span class="s2">&quot;)</span>
</code></pre></div>

<h3>3.2 Finetuning improves the model's ability to use external libraries</h3>
<p>We demonstrate here that finetuning on CodeExercises unexpectedly improves the model's ability to use external libraries such as Pygame and Tkinter, eventhough our exercises do not contain these libraries. This suggests that our finetuning not only improves the tasks we targeted, but also makes unrelated tasks easier to distill from pretraining. For reference, Figure 3.1 shows the distribution of package imports in our CodeExercises dataset.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3.1: The number of imports among $\sim 880 \mathrm{~K}$ exercises in CodeExercises (we ignore libraries imported less than 10 times). The plot code is generated by phi-1 with the prompt: "I have a dictionary, first sort the dictionary using the value, from largest to smallest. Then generate a pyplot bar plot. First set font size to be 7 , then rotate the x -axis label by 90 degree, the x -axis is the key, y -axis is the value of the dictionary. Use log-scale on y-axis. Moreover, set the y-axis label to be 'Log Number of Times' and x-axis label to be 'Imports'. Set dpi to be 1000."</p>
<p>PyGame example. We start with a PyGame that asks the model to generate code to move a ball:</p>
<h1>Prompt:</h1>
<p>"" Write a PyGame. There is a ball. At every iteration, (1). The x-axis of the ball increases by a random integer between $(-10,10),(2)$. The $y$-axis of the ball increases by a random integer between $(-20,20)$. The $x$-axis of the ball should stay inside 0-400, and $y$-axis of the ball should stay inside 0-600. When the user press 'space', set the $x$-axis of the ball to 200 and $y$-axis of the ball to 400 .
" " "</p>
<h2>phi-1</h2>
<div class="codehilite"><pre><span></span><code><span class="o">...</span>
<span class="nx">screen</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">pygame</span><span class="p">.</span><span class="nx">display</span><span class="p">.</span><span class="nx">set_mode</span>
<span class="w">    </span><span class="p">((</span><span class="mi">400</span><span class="p">,</span><span class="w"> </span><span class="mi">600</span><span class="p">))</span>
<span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">y</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span>
<span class="k">while</span><span class="w"> </span><span class="nx">True</span><span class="p">:</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="nx">event</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">pygame</span><span class="p">.</span><span class="nx">event</span><span class="p">.</span><span class="nx">get</span>
<span class="w">        </span><span class="p">():</span><span class="w"> </span><span class="o">...</span>
<span class="w">            </span><span class="nx">elif</span><span class="w"> </span><span class="nx">event</span><span class="p">.</span><span class="k">type</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nx">pygame</span>
<span class="w">    </span><span class="p">.</span><span class="nx">KEYDOWN</span><span class="p">:</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="nx">event</span><span class="p">.</span><span class="nx">key</span><span class="w"> </span><span class="o">==</span>
<span class="w">    </span><span class="nx">pygame</span><span class="p">.</span><span class="nx">K_SPACE</span><span class="p">:</span>
<span class="w">                        </span><span class="nx">x</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">200</span>
<span class="w">                            </span><span class="nx">y</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">400</span>
<span class="w">    </span><span class="nx">x</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="nx">random</span><span class="p">.</span><span class="nx">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">)</span>
<span class="nx">y</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="nx">random</span><span class="p">.</span><span class="nx">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span><span class="w"> </span><span class="mi">20</span><span class="p">)</span>
<span class="k">if</span><span class="w"> </span><span class="nx">x</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="mi">0</span><span class="p">:</span>
<span class="w">        </span><span class="nx">x</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">0</span>
<span class="nx">elif</span><span class="w"> </span><span class="nx">x</span><span class="w"> </span><span class="p">&gt;</span><span class="w"> </span><span class="mi">400</span><span class="p">:</span>
<span class="w">            </span><span class="nx">x</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">400</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">y</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="mi">0</span><span class="p">:</span>
<span class="w">        </span><span class="nx">y</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">0</span>
<span class="nx">elif</span><span class="w"> </span><span class="nx">y</span><span class="w"> </span><span class="p">&gt;</span><span class="w"> </span><span class="mi">600</span><span class="p">:</span>
<span class="w">            </span><span class="nx">y</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">600</span>
<span class="w">    </span><span class="nx">screen</span><span class="p">.</span><span class="nx">fill</span><span class="p">((</span><span class="mi">255</span><span class="p">,</span><span class="w"> </span><span class="mi">255</span><span class="p">,</span><span class="w"> </span><span class="mi">255</span><span class="p">))</span>
<span class="nx">pygame</span><span class="p">.</span><span class="nx">draw</span><span class="p">.</span><span class="nx">circle</span><span class="p">(</span><span class="nx">screen</span><span class="p">,</span>
<span class="w">    </span><span class="p">(</span><span class="mi">255</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">),</span><span class="w"> </span><span class="p">(</span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">y</span><span class="p">),</span><span class="w"> </span><span class="mi">10</span><span class="p">)</span>
<span class="nx">pygame</span><span class="p">.</span><span class="nx">display</span><span class="p">.</span><span class="nx">update</span><span class="p">()</span>
</code></pre></div>

<h2>phi-1-base</h2>
<div class="codehilite"><pre><span></span><code>ball_x = random.randint(-10, 10)
ball_y = random.randint(-20, 20)
ball_vx = random.randint(-5, 5)
ball_vy = random.randint(-5, 5)
screen = pygame.display.set_mode
    ((800, 600))
</code></pre></div>

<p>while not done:
for event in pygame.event.get
():</p>
<div class="codehilite"><pre><span></span><code><span class="nx">elif</span><span class="w"> </span><span class="nx">event</span><span class="p">.</span><span class="k">type</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nx">pygame</span>
</code></pre></div>

<p>. KEYDOWN:
if event.key ==
pygame.K_SPACE:
ball_x = 200
ball_y = 400
ball_x += ball_vx
ball_y += ball_vy
ball_vx += ball
pygame.display.flip()</p>
<h2>phi-1-small</h2>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">x</span>,<span class="w"> </span><span class="nv">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span>,<span class="w"> </span><span class="mi">0</span>
<span class="nv">dx</span>,<span class="w"> </span><span class="nv">dy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">random</span>.<span class="nv">randint</span><span class="ss">(</span><span class="o">-</span><span class="mi">10</span>,<span class="w"> </span><span class="mi">10</span><span class="ss">)</span>,
<span class="w">        </span><span class="k">random</span>.<span class="nv">randint</span><span class="ss">(</span><span class="o">-</span><span class="mi">20</span>,<span class="w"> </span><span class="mi">20</span><span class="ss">)</span>
<span class="k">while</span><span class="w"> </span><span class="nv">True</span>:
<span class="w">    </span><span class="nv">screen</span>.<span class="nv">fill</span><span class="ss">((</span><span class="mi">0</span>,<span class="w"> </span><span class="mi">0</span>,<span class="w"> </span><span class="mi">0</span><span class="ss">))</span>
<span class="w">    </span><span class="nv">pygame</span>.<span class="nv">draw</span>.<span class="nv">rect</span><span class="ss">(</span><span class="nv">screen</span>,
<span class="w">    </span><span class="ss">(</span><span class="mi">255</span>,<span class="w"> </span><span class="mi">255</span>,<span class="w"> </span><span class="mi">255</span><span class="ss">)</span>,<span class="w"> </span><span class="ss">(</span><span class="nv">x</span>,<span class="w"> </span><span class="nv">y</span>,<span class="w"> </span><span class="mi">400</span>,
<span class="w">        </span><span class="mi">600</span><span class="ss">))</span>
<span class="w">    </span><span class="nv">pygame</span>.<span class="nv">display</span>.<span class="nv">update</span><span class="ss">()</span>
<span class="w">    </span><span class="nv">x</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="nv">dx</span>
<span class="w">    </span><span class="nv">y</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="nv">dy</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nv">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">dx</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">400</span>:
<span class="w">        </span><span class="nv">dx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="nv">dx</span>
<span class="w">    </span><span class="nv">elif</span><span class="w"> </span><span class="nv">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">dx</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">0</span>:
<span class="w">        </span><span class="nv">dx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">dx</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nv">y</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">dy</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">600</span>:
<span class="w">        </span><span class="nv">dy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="nv">dy</span>
<span class="w">    </span><span class="nv">elif</span><span class="w"> </span><span class="nv">y</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">dy</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">0</span>:
<span class="w">        </span><span class="nv">dy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">dy</span>
<span class="w">    </span><span class="nv">pygame</span>.<span class="nv">time</span>.<span class="k">wait</span><span class="ss">(</span><span class="mi">50</span><span class="ss">)</span>
</code></pre></div>

<p>The above code snippet shows the main loop of a simple PyGame program that bounces a ball on the screen. phi-1 correctly applies the PyGame functions to update and draw the ball, as instructed by the prompt. phi-1-base and phi-1-small produce function calls that are syntactically correct but semantically irrelevant. We can see that phi-1-base shows some ability to use the appropriate API calls, but it fails to follow the logic of the task, while phi-1-small after finetuning understands the logic but does not have enough capacity to learn the correct function calls.</p>
<p>TKinter example. Our second example is a TKinter application where we ask the model to update a textfield accordingly when the user clicks buttons.</p>
<h1>Prompt:</h1>
<p>"" Write a tkinter application, there is a textfield on top, a submit button at the button and a recover button next to it. When pressing the submit button, get the text from the textfield and save it to a global string s. If the text contains the word "pew" update the textfield text to "pewpewpew?", otherwise, add "no pew..." to the end. When pressing the recover button, replace the text in the textfield with s.
" " "</p>
<div class="codehilite"><pre><span></span><code><span class="n">phi</span><span class="o">-</span><span class="mi">1</span>
<span class="p">...</span>
<span class="n">global</span> <span class="n">s</span>
<span class="n">s</span> <span class="o">=</span> <span class="s">&quot;&quot;</span>
<span class="n">def</span> <span class="n">submit</span><span class="p">():</span>
    <span class="n">global</span> <span class="n">s</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">textfield</span><span class="p">.</span><span class="n">get</span><span class="p">()</span>
    <span class="n">if</span> <span class="s">&quot;pew&quot;</span> <span class="n">in</span> <span class="n">text</span><span class="p">:</span>
        <span class="n">textfield</span><span class="p">.</span><span class="n">delete</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">tk</span><span class="p">.</span><span class="kr">END</span><span class="p">)</span>
        <span class="n">textfield</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s">&quot;pewpewpew...&quot;</span><span class="p">)</span>
    <span class="n">else</span><span class="p">:</span>
        <span class="n">textfield</span><span class="p">.</span><span class="n">delete</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">tk</span><span class="p">.</span><span class="kr">END</span><span class="p">)</span>
        <span class="n">textfield</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">text</span> <span class="o">+</span> <span class="s">&quot;no pew...&quot;</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">text</span>
<span class="n">def</span> <span class="n">recover</span><span class="p">():</span>
    <span class="n">global</span> <span class="n">s</span>
    <span class="n">textfield</span><span class="p">.</span><span class="n">delete</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">tk</span><span class="p">.</span><span class="kr">END</span><span class="p">)</span>
    <span class="n">textfield</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
<span class="n">textfield</span> <span class="o">=</span> <span class="n">tk</span><span class="p">.</span><span class="n">Entry</span><span class="p">(</span><span class="n">root</span><span class="p">)</span>
<span class="n">textfield</span><span class="p">.</span><span class="n">pack</span><span class="p">()</span>
<span class="n">submit_button</span> <span class="o">=</span> <span class="n">tk</span><span class="p">.</span><span class="n">Button</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="s">&quot;Submit&quot;</span><span class="p">,</span>
    <span class="n">command</span><span class="o">=</span><span class="n">submit</span><span class="p">)</span>
<span class="n">submit_button</span><span class="p">.</span><span class="n">pack</span><span class="p">()</span>
<span class="n">recover_button</span> <span class="o">=</span> <span class="n">tk</span><span class="p">.</span><span class="n">Button</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="s">&quot;Recover&quot;</span>
    <span class="p">,</span> <span class="n">command</span><span class="o">=</span><span class="n">recover</span><span class="p">)</span>
<span class="n">recover_button</span><span class="p">.</span><span class="n">pack</span><span class="p">()</span>
<span class="n">root</span><span class="p">.</span><span class="n">mainloop</span><span class="p">()</span>
</code></pre></div>

<h2>phi-1-base</h2>
<div class="codehilite"><pre><span></span><code><span class="n">global</span> <span class="n">s</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">s</span><span class="p">.</span><span class="n">replace</span><span class="p">(</span><span class="s">&quot;pew&quot;</span><span class="p">,</span> <span class="s">&quot;pewpewpew&quot;</span><span class="p">)</span>
<span class="n">if</span> <span class="s">&quot;pew&quot;</span> <span class="n">in</span> <span class="n">s</span><span class="p">:</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">s</span><span class="p">.</span><span class="n">replace</span><span class="p">(</span><span class="s">&quot;pew&quot;</span><span class="p">,</span> <span class="s">&quot;pewpewpew&quot;</span><span class="p">)</span>
<span class="n">else</span><span class="p">:</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">s</span> <span class="o">+</span> <span class="s">&quot;no pew...&quot;</span>
<span class="n">text</span><span class="p">.</span><span class="n">delete</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="kr">END</span><span class="p">)</span>
<span class="n">text</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
<span class="n">def</span> <span class="n">main</span><span class="p">():</span>
    <span class="n">global</span> <span class="n">s</span>
    <span class="n">s</span> <span class="o">=</span> <span class="s">&quot;pewpewpew&quot;</span>
    <span class="n">text</span><span class="p">.</span><span class="n">delete</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="kr">END</span><span class="p">)</span>
    <span class="n">text</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="n">text</span><span class="p">.</span><span class="n">tag_add</span><span class="p">(</span><span class="s">&quot;button&quot;</span><span class="p">,</span> <span class="s">&quot;&lt;Button-1&gt;&quot;</span><span class="p">,</span> <span class="err">&quot;</span><span class="o">&lt;</span><span class="n">Button</span><span class="o">-</span><span class="mi">3</span><span class="o">&gt;</span>
    <span class="err">&quot;</span><span class="p">)</span>
    <span class="n">text</span><span class="p">.</span><span class="n">tag_add</span><span class="p">(</span><span class="s">&quot;button&quot;</span><span class="p">,</span> <span class="s">&quot;&lt;Button-2&gt;&quot;</span><span class="p">,</span> <span class="err">&quot;</span><span class="o">&lt;</span><span class="n">Button</span><span class="o">-</span><span class="mi">4</span><span class="o">&gt;</span>
    <span class="err">&quot;</span><span class="p">)</span>
    <span class="err">\</span><span class="o">#</span> <span class="n">The</span> <span class="n">model</span> <span class="n">keeps</span> <span class="n">adding</span> <span class="n">buttons</span> <span class="n">from</span> <span class="n">here</span><span class="p">.</span>
</code></pre></div>

<p>phi-1-small
import tkinter as tk
root $=\mathrm{tk} . \mathrm{Tk}($ )
global s
text $=$ ""
while True:
event, values = tk.popup()
if event == "submit":
text += values.split()[0] + " "
elif event == "textfield":
s += values.split()[0] + " "
elif event == "submit-button-text":
text = values.split()[0] + " "
elif event == "text-button-text":
s = text
if "pew" in text:
text = text.replace("pew", "pewpewpew")
if "no pew" in text:
break
tk.destroy()</p>
<p>The three models' completions show a huge gap in their prompt understanding. Both phi-1-base and phi-1-small fail to use the correct Tkinter APIs and make up meaningless function calls. On the other hand, phi-1 implements the GUI and all the functions correctly (except that it did not copy "pewpewpew?" correctly). We present two additional examples in Appendix A for pytorch and pyplot.</p>
<p>Chat mode example. Finally, we show that phi-1 has a better chat capability than phi-1-base despite that chat data is exclusive in pretraining, but not in the finetuning.</p>
<h1>Prompt:</h1>
<div class="codehilite"><pre><span></span><code><span class="n">Student</span><span class="p">:</span> <span class="n">I</span> <span class="n">have</span> <span class="n">a</span> <span class="n">Python</span> <span class="n">pyplot</span><span class="p">,</span> <span class="n">I</span> <span class="n">want</span> <span class="n">to</span> <span class="n">increase</span> <span class="n">its</span> <span class="n">resolution</span> <span class="ow">and</span> <span class="n">rotate</span> <span class="n">it</span><span class="p">,</span> <span class="n">what</span>
<span class="n">should</span> <span class="n">I</span> <span class="n">do</span><span class="err">?</span> <span class="n">TA</span><span class="p">:</span>
<span class="n">phi</span><span class="o">-</span><span class="mi">1</span>
<span class="mf">1.</span> <span class="n">Set</span> <span class="n">the</span> <span class="s1">&#39;dpi&#39;</span> <span class="n">parameter</span> <span class="n">to</span> <span class="n">the</span> <span class="n">desired</span>
<span class="n">resolution</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span> <span class="mi">300</span> <span class="k">for</span> <span class="mi">300</span> <span class="n">DPI</span><span class="p">)</span>
<span class="mf">2.</span> <span class="n">Use</span> <span class="n">the</span> <span class="s1">&#39;rotate&#39;</span> <span class="n">function</span> <span class="kn">from</span><span class="w"> </span><span class="nn">the</span> <span class="s1">&#39;matplotlib.</span>
<span class="n">pyplot</span><span class="s1">&#39; module to rotate the plot by 90 degrees</span>
<span class="n">clockwise</span>
<span class="n">Here</span><span class="s1">&#39;s an example:</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="c1"># Set the resolution to 300 DPI</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_dpi</span><span class="p">(</span><span class="mi">300</span><span class="p">)</span>
<span class="c1"># Rotate the plot by 90 degrees clockwise</span>
<span class="n">ax</span><span class="o">.</span><span class="n">rotate</span><span class="p">(</span><span class="mi">90</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">phi</span><span class="o">-</span><span class="mi">1</span><span class="o">-</span><span class="k">base</span>
<span class="n">You</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="k">use</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="o">`</span><span class="n">setp</span><span class="bp">()</span><span class="o">`</span><span class="w"> </span><span class="k">function</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">the</span>
<span class="n">resolution</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="o">`</span><span class="n">set_rotation</span><span class="bp">()</span><span class="o">`</span><span class="w"> </span><span class="k">function</span><span class="w"> </span><span class="k">to</span>
<span class="n">rotate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">plot</span><span class="o">.</span><span class="w"> </span><span class="o">&lt;|</span><span class="n">endoftext</span><span class="o">|&gt;</span>
<span class="n">phi</span><span class="o">-</span><span class="mi">1</span><span class="o">-</span><span class="n">small</span>
<span class="n">I</span><span class="w"> </span><span class="n">am</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">Python</span><span class="w"> </span><span class="n">pyplot</span><span class="o">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">want</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">increase</span><span class="w"> </span><span class="n">its</span>
<span class="n">resolution</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">rotate</span><span class="w"> </span><span class="n">it</span><span class="o">,</span><span class="w"> </span><span class="n">what</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="k">do</span><span class="o">?</span>
<span class="n">SECOND</span><span class="o">:</span>
<span class="n">I</span><span class="w"> </span><span class="n">am</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">Python</span><span class="w"> </span><span class="n">pyplot</span><span class="o">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">want</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">increase</span><span class="w"> </span><span class="n">its</span>
<span class="n">resolution</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">rotate</span><span class="w"> </span><span class="n">it</span><span class="o">,</span><span class="w"> </span><span class="n">what</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="k">do</span><span class="o">?</span>
<span class="n">THIRD</span><span class="o">:...</span>
</code></pre></div>

<h2>4 Evaluation on unconventional problems with LLM grading</h2>
<p>A potential concern with the surprisingly good performance of phi-1 on HumanEval (see Table 1 and Figure 2.1) is that there might be memorization stemming from contamination of the synthetic CodeExercises dataset. We study this potential contamination directly in Section 5, while this section addresses the concern with a new evaluation that is designed to be unconventional enough to be unlikely to appear in our training dataset.</p>
<p>To minimize bias and leakage, the new evaluation problems were created by a dedicated team that did not access the CodeExercises dataset or the final model. They created 50 new problems in the same format as HumanEval with instructions to design problems that are unlikely to appear in real-world code bases or as coding exercises. Here is an example of such a problem:</p>
<div class="codehilite"><pre><span></span><code>def sort_concat_square_deduplicate(list1, list2, my_threshold):
    &quot;&quot;&quot;
    This functions takes two lists of integers, sorts each of them in ascending order,
    concatenates them, squares the entries at even indices, filters out entries
    smaller than my_threshold and then removes duplicates. The resulting list is
    returned.
    &quot;&quot;&quot;
</code></pre></div>

<p>One of the challenges of evaluating language models on coding tasks is that the output of the model is often binary: either the code passes all the unit tests or it fails. However, this does not capture the nuances of the model's performance, as it might have produced a code that is almost correct but has a minor error, or a code that is completely wrong but coincidentally passes some tests. Arguably, a more informative way of assessing the model's coding skills is to compare its output with the correct solution and grade it based on how well it matches the expected logic. This is similar to how humans are evaluated on coding interviews, where the interviewer does not only run the code but also examines the reasoning and the quality of the solution.</p>
<p>To evaluate candidate solutions, we therefore adopt the approach of using GPT-4 to grade the solution (such as in [EL23]). This approach has two distinct advantages: (1) by using GPT-4 as a grader, we can leverage its knowledge and generative abilities to obtain a more fine-grained and meaningful signal of the</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Size</th>
<th>Training tokens</th>
<th>Score</th>
<th>HumanEval</th>
</tr>
</thead>
<tbody>
<tr>
<td>CodeGen-Mono-350M [NPH*23]</td>
<td>350 M</td>
<td>577 B</td>
<td>$19 \%$</td>
<td>$13 \%$</td>
</tr>
<tr>
<td>CodeGen-Mono-16.1B [NPH*23]</td>
<td>16.1 B</td>
<td>577 B</td>
<td>$38 \%$</td>
<td>$29 \%$</td>
</tr>
<tr>
<td>Replit [Rep23]</td>
<td>2.7 B</td>
<td>525 B</td>
<td>$37 \%$</td>
<td>$22 \%$</td>
</tr>
<tr>
<td>StarCoder [LAZ*23]</td>
<td>15.5 B</td>
<td>1 T</td>
<td>$51 \%$</td>
<td>$34 \%$</td>
</tr>
<tr>
<td>phi-1-base</td>
<td>1.3 B</td>
<td>7 B</td>
<td>$37 \%$</td>
<td>$29 \%$</td>
</tr>
<tr>
<td>phi-1-small</td>
<td>350 M</td>
<td>7 B</td>
<td>$45 \%$</td>
<td>$45 \%$</td>
</tr>
<tr>
<td>phi-1</td>
<td>1.3 B</td>
<td>7 B</td>
<td>$52 \%$</td>
<td>$51 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: LLM graded Understanding scores on 50 new unconventional coding problems.
student model's coding capabilities, and (2) it obviates the need for tests ${ }^{1}$. Our prompt instructs the LLM to evaluate a student's solution first in a short verbal evaluation followed by grades from 0 to 10 .</p>
<p>See Table 2 for our results with phi-1 and competing models. The grades on our new unconventional problems give the same ranking as HumanEval (see Table 1). phi-1 again achieves a score significantly higher than StarCoder, as it did on HumanEval. Given that the new problems have had no chance to contaminate the training data and, furthermore, were designed to be outside the training distribution, these results greatly increase our confidence in the validity of phi-1's performance.</p>
<h1>5 Data pruning for unbiased performance evaluation</h1>
<p>In Figure 2.1, we see that training on CodeExercises leads to a substantial boost in the performance of the model on the HumanEval benchmark. To investigate this boost, we propose to prune the CodeExercises dataset by removing files that are "similar" to those in HumanEval. This process can be viewed as a "strong form" of data decontamination. We then retrain our model on such pruned data, and still observe strong performance on HumanEval. In particular, even after aggressively pruning more than $40 \%$ of the CodeExercises dataset (this even prunes files that are only vaguely similar to HumanEval, see Appendix C), the retrained phi-1 still outperforms StarCoder.</p>
<p>We believe that such data pruning experiment is a fair way to evaluate performance, and is more insightful than standard "contamination" studies in the literature that are usually based on measures of overlap between training and test data (e.g., Section 4.8 of $\left[\mathrm{AON}^{\star} 21\right]$ ). For sake of completeness we start this section by conducting a standard contamination experiment, which shows that CodeExercises is not contaminated by HumanEval in this standard sense.</p>
<h3>5.1 N-gram overlap</h3>
<p>N-gram measures the similarity of text segments based on the shared n-word sequences. We calculate the n-gram overlap between the docstrings of each humaneval question and each exercise in the CodeExercises dataset that was generated. We found 4 humaneval questions with 13-gram overlap with at least one of the entries in our dataset. After further investigating, we found out that all the 4 overlap cases in the 13-gram are all false positives such as the example below. Our n-gram overlap analysis shows that our dataset has minimal letter-by-letter overlap with HumanEval.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>HumanEval:</h1>
<p>You are given a non-empty list of positive integers. Return the greatest integer that is greater than zero, and has a frequency greater than or equal to the value of the integer itself. The frequency of an integer is the number of times it appears in the list.</p>
<h2>CodeExercises:</h2>
<p>Calculates the power frequency analysis sum of a list of integers. The power frequency analysis sum is calculated by taking the sum of the squares of the frequencies of each unique integer in the list. The frequency of an integer is the number of times it appears in the list.</p>
<h3>5.2 Embedding and syntax-based similarity analysis</h3>
<p>As we just saw, the n-gram analysis is not refined enough to find similar code snippets between HumanEval and CodeExercises. Instead we use a combination of embedding and syntax-based distances. For the embedding distance we compute the L2 distance between the embedding of the code snippets where the embedding is derived from a pre-trained CodeGen-Mono 350M model $\left[\mathrm{NPH}^{+} 23\right]$. We observe that the embedding distance is successful in capturing code pairs where the overall code semantics are similar, which can be inferred via the Python Docstring, function/class names, as well as the code structure. For the syntax-based distance we calculate the (string) edit distance between the abstract syntax trees (ASTs) of two given code snippets. The AST distance successfully identifies overlapping sections between code pairs while being agnostic to non-syntax text such as variable/function naming, comments, and Python Docstrings. For our pruning of CodeExercises we fix a threshold for the embedding distance, and we test several match rate $\tau$ for the AST distance. See Appendix C for examples of code pairs that are captured with the embedding distance and various AST match rates $\tau$. We vary $\tau$ between 0.95 and 0.8 , which corresponds to removing between $42.5 K$ to $354 K$ of the $879.5 K$ total problems in CodeExercises.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$\tau$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Problem <br> Count</th>
<th style="text-align: center;">phi-1</th>
<th style="text-align: center;">phi-1 retrained on pruned data</th>
<th style="text-align: center;">StarCoder-Prompted $\left[\mathrm{LAZ}^{+} 23\right]$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">similar</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">$81.7 \%$</td>
<td style="text-align: center;">$74.6 \%$</td>
<td style="text-align: center;">$57.7 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">non-similar</td>
<td style="text-align: center;">93</td>
<td style="text-align: center;">$26.9 \%$</td>
<td style="text-align: center;">$32.3 \%$</td>
<td style="text-align: center;">$29.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">total</td>
<td style="text-align: center;">164</td>
<td style="text-align: center;">$50.6 \%$</td>
<td style="text-align: center;">$50.6 \%$</td>
<td style="text-align: center;">$41.5 \%$</td>
</tr>
<tr>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">similar</td>
<td style="text-align: center;">93</td>
<td style="text-align: center;">$63.4 \%$</td>
<td style="text-align: center;">$51.6 \%$</td>
<td style="text-align: center;">$48.4 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">non-similar</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">$33.8 \%$</td>
<td style="text-align: center;">$36.6 \%$</td>
<td style="text-align: center;">$32.4 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">total</td>
<td style="text-align: center;">164</td>
<td style="text-align: center;">$50.6 \%$</td>
<td style="text-align: center;">$45.1 \%$</td>
<td style="text-align: center;">$41.5 \%$</td>
</tr>
<tr>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">similar</td>
<td style="text-align: center;">106</td>
<td style="text-align: center;">$62.3 \%$</td>
<td style="text-align: center;">$52.8 \%$</td>
<td style="text-align: center;">$47.2 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">non-similar</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">$29.3 \%$</td>
<td style="text-align: center;">$34.5 \%$</td>
<td style="text-align: center;">$31.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">total</td>
<td style="text-align: center;">164</td>
<td style="text-align: center;">$50.6 \%$</td>
<td style="text-align: center;">$46.3 \%$</td>
<td style="text-align: center;">$41.5 \%$</td>
</tr>
<tr>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">similar</td>
<td style="text-align: center;">116</td>
<td style="text-align: center;">$59.5 \%$</td>
<td style="text-align: center;">$52.6 \%$</td>
<td style="text-align: center;">$45.7 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">non-similar</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">$29.2 \%$</td>
<td style="text-align: center;">$27.1 \%$</td>
<td style="text-align: center;">$31.2 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">total</td>
<td style="text-align: center;">164</td>
<td style="text-align: center;">$50.6 \%$</td>
<td style="text-align: center;">$45.1 \%$</td>
<td style="text-align: center;">$41.5 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3: Percentage of similar versus non-similar HumanEval problems correctly solved by different models. Similarity is determined based on whether or not the corresponding HumanEval problem has any close matches inside the CodeExercises dataset (for a given $\tau$ ). The problem count denotes the number of HumanEval problems within each subset. Here, $\tau$ is the threshold on AST-based match rate between codes for similarity check.</p>
<p>Table 3 summarizes the performance of our retrained phi-1 on pruned datasets (with $\tau=0.95,0.9,0.85$ and 0.8 ) versus the original phi-1 trained on full CodeExercises and the $15.5 B$-parameter StarCoderprompted. We divide the HumanEval problems into two subsets ("similar" and "non-similar") based on whether or not they have at least one close match (for this given $\tau$ ) inside the original CodeExercises dataset. We then report the accuracy of the models on each subset of HumanEval separately. As one can see, even after heavily pruning our dataset, phi-1 still outperforms StarCoder-Prompted by a large</p>
<p>margin, which validates that our performance boost is not due to dataset "contamination", even when the latter term is understood loosely. Note also that the accuracy of all models is lower on the HumanEval non-similar subset versus the similar one.</p>
<h1>6 Conclusion</h1>
<p>Just as a comprehensive, well-crafted textbook can provide a student with the necessary knowledge to master a new subject, our work demonstrates the remarkable impact of high-quality data in honing a language model's proficiency in code-generation tasks. By crafting "textbook quality" data we were able to train a model that surpasses almost all open-source models on coding benchmarks such as HumanEval and MBPP despite being 10x smaller in model size and 100x smaller in dataset size. We hypothesize that such high quality data dramatically improves the learning efficiency of language models for code as they provide clear, self-contained, instructive, and balanced examples of coding concepts and skills.</p>
<p>There remains a number of limitations of our model compared to larger models for code. Firstly, phi-1 is specialized in Python coding, which restricts its versatility compared to multi-language models. Secondly, phi-1 lacks the domain-specific knowledge of larger models such as programming with specific APIs or using less common packages. Lastly, due to the structured nature of the datasets and the lack of diversity in terms of language and style, phi-1 is less robust to stylistic variations or errors in the prompt (for instance, its performance substantially degrades when there are grammatical mistakes in the prompt). We expand on these limitations and give examples of the failure modes of phi-1 in Appendix B.</p>
<p>None of these limitations seem fundamental, and with more work our approach could be used to tackle each one of them, although it is unclear what scaling might be necessary to overcome them (both for the model size and the dataset size). We also believe that significant gains could be achieved by using GPT-4 to generate the synthetic data instead of GPT-3.5, as we noticed that GPT-3.5 data has a high error rate. It is interesting that phi-1 is able to achieve such high coding proficiency despite those errors (a similar phenomenon was observed in [AZL23] where a language model can be trained on data with $100 \%$ error rate and still generate correct answers at test time).</p>
<p>More generally, our work provides evidence that developing good methodology for creating highquality datasets is a central direction of research for advancing natural language processing and related fields (see also $\left[\mathrm{JWJ}^{+} 23\right]$ for further evidence). However, creating high-quality datasets is not a trivial task, and it poses several challenges that need to be addressed. One challenge is to ensure that the dataset covers all the relevant content and concepts that one wants the model to learn, and that it does so in a balanced and representative way. Another challenge is to ensure that the dataset is truly diverse and non-repetitive, so that the model does not simply overfit to the data or memorize specific patterns or solutions. This requires finding ways to inject randomness and creativity into the data generation process, while still maintaining the quality and the coherence of the examples. Moreover, even after creating such datasets, we lack a good methodology to measure and evaluate the amount of diversity and redundancy in the data. For example, if we have a dataset with coding exercises, it is hard to determine how many different variations of each exercise exist, and how they are distributed across the dataset. Finally, as language models themselves will be used to curate data for future language models, it further increases the urgency on the ethical and social implications of training such models, such as the accountability, the transparency, and the bias of the data and the models that are involved in this process.</p>
<h1>References</h1>
<p>$\left[\mathrm{ADF}^{+} 23\right]$ Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.
$\left[\mathrm{ALK}^{+} 23\right]$ Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. Santacoder: don't reach for the stars! arXiv preprint arXiv:2301.03988, 2023.
$\left[\mathrm{AON}^{+} 21\right]$ Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.
[AZL23] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 1, context-free grammar. arXiv preprint arXiv:2305.13673, 2023.
$\left[\mathrm{BBH}^{+} 22\right]$ Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-20B: An open-source autoregressive language model. In Proceedings of the ACL Workshop on Challenges $\&amp;$ Perspectives in Creating Large Language Models, 2022.
$\left[\mathrm{BCE}^{+} 23\right]$ Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.
[BGMMS21] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610-623, 2021.
$\left[\mathrm{BJT}^{+} 22\right]$ Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, and Mark Chen. Efficient training of language models to fill in the middle. arXiv preprint arXiv:2207.14255, 2022.
$\left[\mathrm{BMR}^{+} 20\right]$ Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 18771901, 2020.
$\left[\mathrm{CND}^{+} 22\right]$ Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022 .</p>
<p>[CTJ ${ }^{+}$21] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
[DFE ${ }^{+}$22] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344-16359, 2022.
[DLT ${ }^{+}$23] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023.
[EL23] Ronen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speak coherent english? arXiv preprint arXiv:2305.07759, 2023.
[GWS ${ }^{+}$23] Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717, 2023.
[HBM ${ }^{+}$22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack William Rae, and Laurent Sifre. An empirical analysis of compute-optimal large language model training. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.
[HNA ${ }^{+}$17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017.
[JWJ ${ }^{+}$23] Jaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian Fisher, Taylor Sorensen, and Yejin Choi. Impossible distillation: from low-quality model to high-quality dataset \&amp; model for summarization and paraphrasing. arXiv preprint arXiv:2305.16635, 2023.
[KLA ${ }^{+}$22] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et al. The stack: 3 tb of permissively licensed source code. arXiv preprint arXiv:2211.15533, 2022.
[ $\mathrm{KMH}^{+}$20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.
[LAZ ${ }^{+}$23] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023.
[LCC ${ }^{+}$22] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092-1097, 2022.</p>
<p>[LGK ${ }^{+}$23] Zinan Lin, Sivakanth Gopi, Janardhan Kulkarni, Harsha Nori, and Sergey Yekhanin. Differentially private synthetic data via foundation model apis 1: Images. arXiv preprint arXiv:2305.15560, 2023.
[LXWZ23] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. arXiv preprint arXiv:2305.01210, 2023.
[LXZ ${ }^{+}$23] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct, 2023.
[LYR ${ }^{+}$23] Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et al. A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, \&amp; toxicity. arXiv preprint arXiv:2305.13169, 2023.
[MMJ ${ }^{+}$23] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707, 2023.
[MRB ${ }^{+}$23] Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. arXiv preprint arXiv:2305.16264, 2023.
[NHX ${ }^{+}$23] Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, and Yingbo Zhou. Codegen2: Lessons for training llms on programming and natural languages. arXiv preprint arXiv:2305.02309, 2023.
[NPH ${ }^{+}$22] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint, 2022.
[NPH ${ }^{+}$23] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. $I C L R, 2023$.
[Ope23] OpenAI. Gpt-4 technical report, 2023. arXiv preprint arXiv:2303.08774 [cs.CL].
[Rep23] Replit. Replit dev day. https://twitter.com/Replit/status/ 1651344184593506304, 2023.
[RSR ${ }^{+}$20] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.
[SLP ${ }^{+}$21] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.
[SSZ ${ }^{+}$23] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. Model dementia: Generated data makes models forget. arXiv preprint arXiv:2305.17493, 2023.</p>
<p>[TGZ+23] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.
[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30, 2017.
[WKM ${ }^{+}$22] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.
[WLG ${ }^{+}$23] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi. Codet5+: Open code large language models for code understanding and generation. arXiv preprint arXiv:2305.07922, 2023.
$\left[\mathrm{WTB}^{+}\right.$22] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022. Survey Certification.
[YGK ${ }^{+}$23] Da Yu, Sivakanth Gopi, Janardhan Kulkarni, Zinan Lin, Saurabh Naik, Tomasz Lukasz Religa, Jian Yin, and Huishuai Zhang. Selective pre-training for private fine-tuning. arXiv preprint arXiv:2305.13865, 2023.
[ZXZ ${ }^{+}$23] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x, 2023.</p>
<h1>A Additional examples for Section 3</h1>
<p>In Section 3, we discussed with a few examples on how finetuned modes shows a substantial improvement over the base model in executing tasks that are not featured in the finetuning dataset. Here we provide additional examples and details to further illustrate improvements.</p>
<h2>A. 1 Finetuning improves the model's understanding ability</h2>
<p>The example below is challenging due to the prevalence of various logical operators such as "and" and "not" that link numerous statements. Prior to finetuning, the model is confused by the logic embedded in the statement. It struggles to differentiate between "elements" and "indices", and is unsure about which array to use. Conversely, the finetuned model is adept at accurately identifying all relevant arrays and indices, while correctly implementing the logical operators.</p>
<h2>Prompt:</h2>
<p>"" Input is an array A, an array B, an array C, an array E, a numpy matrix D. I want to find all the indices of D whose elements are in the union of B and C and whose row index is not in A and column index is in D. Loop through the elements in D. Return the list of indices.
" " "
phi-1
indices $=[]$
for i in range(D.shape[0]):
for j in range(D.shape[1]):
if D[i][j] in B or D[i][j] in
C:
if i not in A and j in D: indices.append $((i, j))$
return indices</p>
<h2>phi-1-base</h2>
<p>indices $=[]$
for i in range(len(D)):
for j in range(len(D[i]))
if D[i][j] in B and D[0][i]
if D[i][j] in B and D
[i][j] not in A:
indices.append(j)
return indices</p>
<h2>phi-1-small</h2>
<p>indices $=[]$
for i in range(D.shape[1]):
for j in range(lens[1]))
if D[0][i] in B and D[0][i]
not in A and D[i][0] in C
and D[i][0] not in E:
indices.append(i)
return indices</p>
<h2>A. 2 Finetuning improves the model's ability to use external libraries</h2>
<p>Pytorch example. In this PyTorch example, we ask the model to write a modified gradient update:</p>
<h2>Prompt:</h2>
<p>"" Write a pytorch function. The input is a model, take the gradient of each parameter of the model, do an SVD of each gradient to get U, Sigma, V, take the top k singular values of Sigma as Sigmak, then do a update step to the parameter using Uk<em>Sigmak</em>Vk times the norm of the parameter.
" " "
phi-1
for p in model.parameters():
if p.grad is not None:
U, Sigma, V = p.grad.svd()
Sigmak = Sigma[:k].diag()
p.data.add_(torch.mm(U[:,:k],
torch.mm(Sigmak, V[:k,:])).mul_(
norm))</p>
<h2>phi-1-base</h2>
<p>...
self.model = model
self.k = k
self.eps = eps
self.max_iter = max_iter
self.verbose = verbose
self.svd_u = None
self.svd_sigma = None
self.svd_v = None
self.svd_k = None
self.svd_u_k = None
...
phi-1-small
def svd(x):
$u, s, v=$ np.linalg.svd(x)
return $u, s, v$
def update_parameter(parameter, grad
):
$u, s, v=\operatorname{svd}$ (grad)
parameter $=u[1,: k]$
np.diag(s[:k]) v[:k, :]
parameter += np.diag(sigmoid( parameter))
return parameter</p>
<p>The code by phi-1-small reveals a lack api usage and limitations in understanding, leading to multiple significant missteps. First, it uses np.linalg.svd(x), which is incompatible with PyTorch tensors. Second, it erroneously assigns the SVD result directly to the parameter, deviating from the prompt's instruction to add it. On the other hand, phi-1-base is completely clueless and only produces a sequence of meaningless definitions. This indicates that improving the model's logical reasoning ability (even only on simple functions) can greatly enhance the model's general coding ability.</p>
<p>Pyplot example. The final API example is a Pyplot application. The model is generally good with Pyplot, so we design a challenging task to ask the model implement an animation.</p>
<h1>Prompt:</h1>
<p>""Create an x array that are integers from 1 to $20, y$ array that is equal to the elementwise cos of x divided by element-wise sqrt $(\mathrm{x}+1)$. Write a Python code of line plot that animates the change of y according to x <strong><em>step by step</em></strong> with red lines. However, during the animation, when x[i] is larger than 11, change the animation line color from red to green.
"""
phi-1
...
$\mathrm{x}=$ np.arange $(1,21)$
$\mathrm{y}=\mathrm{np} . \cos (\mathrm{x}) / \mathrm{np}$.sqrt $(\mathrm{x}+1)$
fig, ax = plt.subplots ( )
line, = ax.plot([], [], color='
red')
...
ax.set_title('Animation of Cosine and Cosine-Square-Root')
def animate(i):
if x[i] &gt; 11:
line.set_color('green')
else:
line.set_color('red')
line.set_data(x[:i+1], y[:i
+1])
return line,
anim = animation.FuncAnimation(
fig, animate, frames=len(x),
interval=500, blit=True)
plt.show()</p>
<h2>phi-1-base</h2>
<p>・.
fig, ax = plt.subplots ( )
$\mathrm{x}=$ np.arange $(1,21)$
$\mathrm{y}=\mathrm{np} . \cos (\mathrm{x}) / \mathrm{np}$.sqrt $(\mathrm{x}+1)$
line, = ax.plot([], [], lw=2)
def init():
line.set_data([], [])
return line,
def animate(i):
if $\mathrm{x}[i]&gt;11$ :
line.set_color('green')
else:
line.set_color('red')
return line,
anim = FuncAnimation(fig, animate , init_func=init, frames=len (x), interval=50, blit=True)</p>
<h2>phi-1-small</h2>
<p>・.
fig, ax = plt.subplots ( )
x_array = list (range (1, 21))
y_array = [math.cos(xi/10) for xi in x_array]
def animate(i):
...
ax.plot(x_array, y_array)
if x_array[i] &gt; 11:
color = "red"
else:
color = "green"
ax.scatter(x_array[i], y_array[i], color=color)
return ax
ani = animation.FuncAnimation(fig , animate, frames=len( x_array), interval=50, blit= True)
plt.show()</p>
<p>phi-1-small produces a very incorrect completion. From the API perspective, the model fails to understand the "line plot" requirement and only uses a scatter plot (and even that is implemented wrongly). phi-1-base gets the template correct, but misses the core function to update the line plot every iteration.</p>
<h2>B Limitation of phi-1</h2>
<p>While finetuning through simple exercises significantly enhances the model's overall performance, there are certain constraints intrinsic to our model that cannot be overcome solely by finetuning. Firstly, our model has only 1.3B parameters trained with only 7B tokens, this restricts our model's capacity to manage more complex tasks such as developing an intricate Flask application, in comparison to other models like Starcoder. Beyond our model's limitations in terms of generality when contrasted with StarCoder or ChatGPT, we've also outlined several other weaknesses as follows:</p>
<p>Sensitivity to prompt variations. Our model is sensitive to various perturbations of prompts. First, its performance drops significantly as the length of the prompt increases, as it tends to ignore, forget or misinterpret parts of the prompt when it is too long. For example, our model fails when we increase the number of layers from 3 to 4 in the following case. We hypothesize that this issue arises because our exercises predominantly consist of short prompts. Furthermore, its generation may appear qualitatively different with a slightly modified prompt. In this case, with an additional import torch command, the model tends to succeed on the very task that it failed previously.</p>
<div class="codehilite"><pre><span></span><code><span class="nl">Prompt:</span>
<span class="s">&quot;&quot;&quot; Create a neural network</span>
<span class="n">class</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">Pytorch</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="mh">3</span>
<span class="n">layers</span><span class="p">.</span><span class="w"> </span><span class="n">Implement</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="s">&quot;rr&quot;</span>
<span class="n">method</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="k">function</span><span class="o">:</span>
<span class="n">It</span><span class="w"> </span><span class="n">takes</span><span class="w"> </span><span class="k">input</span><span class="w"> </span><span class="n">\(x\).</span><span class="w"> </span><span class="n">Apply</span><span class="w"> </span><span class="n">the</span>
<span class="n">first</span><span class="w"> </span><span class="n">layer</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">\(x\)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">assign</span>
<span class="n">the</span><span class="w"> </span><span class="k">output</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">\(y\),</span><span class="w"> </span><span class="n">then</span><span class="w"> </span><span class="n">apply</span>
<span class="n">the</span><span class="w"> </span><span class="n">second</span><span class="w"> </span><span class="n">layer</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">\(y\)</span><span class="w"> </span><span class="k">and</span>
<span class="k">assign</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">output</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">\(z\),</span><span class="w"> </span><span class="n">then</span>
<span class="n">apply</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">third</span><span class="w"> </span><span class="n">layer</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">\(z\)</span>
<span class="k">and</span><span class="w"> </span><span class="k">assign</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">\(w\),</span><span class="w"> </span><span class="n">then</span><span class="w"> </span><span class="n">sum</span>
<span class="n">up</span><span class="w"> </span><span class="n">\(y,</span><span class="w"> </span><span class="n">z</span><span class="p">,</span><span class="w"> </span><span class="n">w\)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">assign</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">\(Q\).</span>
<span class="n">Return</span><span class="w"> </span><span class="n">\(Q\).</span>
<span class="s">&quot;&quot;&quot; Prompt:</span>
<span class="s">&quot;&quot;&quot; Create a neural network</span>
<span class="n">class</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">Pytorch</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="mh">4</span>
<span class="n">layers</span><span class="p">.</span><span class="w"> </span><span class="n">Implement</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="s">&quot;rr&quot;</span>
<span class="n">method</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="k">function</span><span class="o">:</span>
<span class="n">It</span><span class="w"> </span><span class="n">takes</span><span class="w"> </span><span class="k">input</span><span class="w"> </span><span class="n">\(x\).</span><span class="w"> </span><span class="n">Apply</span><span class="w"> </span><span class="n">the</span>
<span class="n">first</span><span class="w"> </span><span class="n">layer</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">\(x\)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">assign</span>
<span class="n">the</span><span class="w"> </span><span class="k">output</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">\(y\),</span><span class="w"> </span><span class="n">then</span><span class="w"> </span><span class="n">apply</span>
<span class="n">the</span><span class="w"> </span><span class="n">second</span><span class="w"> </span><span class="n">layer</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">\(y\)</span><span class="w"> </span><span class="k">and</span>
<span class="k">assign</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">output</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">\(z\),</span><span class="w"> </span><span class="n">then</span>
<span class="n">apply</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">third</span><span class="w"> </span><span class="n">layer</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">\(z\)</span>
<span class="k">and</span><span class="w"> </span><span class="k">assign</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">\(w\),</span><span class="w"> </span><span class="n">then</span>
<span class="n">apply</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">forth</span><span class="w"> </span><span class="n">layer</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">\(w\)</span>
<span class="k">and</span><span class="w"> </span><span class="k">assign</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">\(p\),</span><span class="w"> </span><span class="n">then</span><span class="w"> </span><span class="n">sum</span>
<span class="n">up</span><span class="w"> </span><span class="n">\(y,</span><span class="w"> </span><span class="n">z</span><span class="p">,</span><span class="w"> </span><span class="n">w</span><span class="p">,</span><span class="w"> </span><span class="n">p\)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">assign</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">to</span>
<span class="n">Q</span><span class="p">.</span><span class="w"> </span><span class="n">Return</span><span class="w"> </span><span class="n">\(Q\).</span>
<span class="s">&quot;&quot;&quot;| class NeuralNetwork(nn.Module</span>
<span class="p">)</span><span class="o">:</span><span class="w"> </span><span class="n">def</span><span class="w"> </span><span class="n">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">)</span><span class="o">:</span>
<span class="w">            </span><span class="n">super</span><span class="p">(</span><span class="n">NeuralNetwork</span><span class="p">,</span>
<span class="w">    </span><span class="n">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">layer1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nn</span><span class="p">.</span>
<span class="w">    </span><span class="n">Linear</span><span class="p">(</span><span class="mh">2</span><span class="p">,</span><span class="w"> </span><span class="mh">5</span><span class="p">)</span>
<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">layer2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nn</span><span class="p">.</span>
<span class="w">    </span><span class="n">Linear</span><span class="p">(</span><span class="mh">5</span><span class="p">,</span><span class="w"> </span><span class="mh">3</span><span class="p">)</span>
<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">layer3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nn</span><span class="p">.</span>
<span class="w">    </span><span class="n">Linear</span><span class="p">(</span><span class="mh">3</span><span class="p">,</span><span class="w"> </span><span class="mh">1</span><span class="p">)</span>
<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">rr</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="o">:</span>
<span class="w">        </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">        </span><span class="n">z</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="w">        </span><span class="n">w</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="w">        </span><span class="n">Q</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">z</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">w</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">Q</span><span class="o">|</span><span class="w"> </span><span class="n">class</span><span class="w"> </span><span class="n">NeuralNetwork</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">)</span>
<span class="w">    </span><span class="o">:</span><span class="w"> </span><span class="n">def</span><span class="w"> </span><span class="n">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">)</span><span class="o">:</span>
<span class="w">            </span><span class="n">super</span><span class="p">(</span><span class="n">NeuralNetwork</span><span class="p">,</span><span class="w"> </span><span class="n">self</span><span class="p">).</span>
<span class="w">        </span><span class="n">__init__</span><span class="p">()</span>
<span class="w">            </span><span class="n">self</span><span class="p">.</span><span class="n">layer1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span>
<span class="w">        </span><span class="n">Linear</span><span class="p">(</span><span class="mh">10</span><span class="p">,</span><span class="w"> </span><span class="mh">20</span><span class="p">)</span>
<span class="w">            </span><span class="p">...</span>
<span class="w">            </span><span class="n">def</span><span class="w"> </span><span class="n">rr</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="o">:</span>
<span class="w">            </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">            </span><span class="n">z</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="w">            </span><span class="n">w</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="w">            </span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<span class="w">            </span><span class="n">Q</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">z</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">w</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">p</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="n">Q</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="sd">&quot;&quot;&quot; Create a neural network class in</span>
<span class="sd">Pytorch with 4 layers. Implement</span>
<span class="sd">a new &quot;rr&quot; method to this new</span>
<span class="sd">function: It takes input x. Apply</span>
<span class="sd">the first layer on x and assign the</span>
<span class="sd">output to y, then apply the second</span>
<span class="sd">layer on y and assign the output to</span>
<span class="sd">z, then apply the third layer on z</span>
<span class="sd">and assign it to w, then apply the</span>
<span class="sd">forth layer on w and assign it to</span>
<span class="sd">p, then sum up y,z,w,p and assign it</span>
<span class="sd">to Q. Return Q.</span>
<span class="sd">&quot;&quot;&quot;</span><span class="o">|</span>
<span class="o">...</span>
<span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">NeuralNetwork</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span>
        <span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span>
        <span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
            <span class="o">...</span>
            <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
            <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer4</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
            <span class="n">p</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
            <span class="n">Q</span> <span class="o">=</span> <span class="n">p</span> <span class="o">+</span> <span class="n">y</span> <span class="o">+</span> <span class="n">z</span> <span class="o">+</span> <span class="n">w</span>
            <span class="k">return</span> <span class="n">Q</span>
</code></pre></div>

<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Developing rigorous sets of tests can be a significant undertaking, as demonstrated by [LXWZ23].&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2466 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2466</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2466</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-209713370</p>
                <p><strong>Paper Title:</strong> Constrained Bayesian optimization for automatic chemical design using variational autoencoders</p>
                <p><strong>Paper Abstract:</strong> Automatic Chemical Design is a framework for generating novel molecules with optimized properties.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2466.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2466.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CBO-VAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Constrained Bayesian Optimization for Automatic Chemical Design using Variational Autoencoders</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that performs Bayesian optimization over a VAE latent space under a learned probabilistic feasibility constraint (a BNN) to avoid querying latent-space 'dead zones' and thereby allocate evaluations to points likely to decode to valid, realistic molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Constrained Bayesian Optimization for Automatic Chemical Design (CBO-VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Combines a variational autoencoder (VAE) that maps molecules <-> continuous latent vectors with a constrained Bayesian optimization loop in latent space. A surrogate sparse Gaussian process (GP) models the noisy black-box objective (composite metrics like penalized logP, QED, or predicted PCE). A probabilistic constraint model (a Bayesian neural network, BNN) is trained to predict the probability that decoding a latent point yields a realistic/valid molecule. The acquisition function is Expected Improvement with Constraints (EIC = EI * Pr(constraint)), so candidate latent points are ranked by expected objective improvement multiplied by decoding feasibility. Batch (parallel) evaluations are performed using the Kriging-Believer parallelization strategy; batches of latent points are decoded multiple times (stochastic decoder) to evaluate noisy objectives. If no feasible region is yet found, the acquisition switches to maximizing Pr(constraint) to locate feasibility before optimizing the objective.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>De novo molecular design (drug discovery) and materials design (example: organic photovoltaics — PCE optimization).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Acquisitions select latent points by maximizing EIC: EI(z) (expected improvement of surrogate GP over incumbent) multiplied by Pr(C(z)) from the BNN constraint. Parallel batches are chosen using Kriging-Believer: the surrogate is temporarily 'filled in' with predicted values for pending points to select subsequent points in the batch. When no feasible point has yet been observed, acquisition prioritizes Pr(C(z)) (searching for feasible region) rather than the objective. The procedure therefore allocates evaluation budget towards points that balance objective improvement and decoding feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Implicitly measured as number of latent-point evaluations / decode attempts and surrogate updates: in the experiments, 20 BO iterations × batch size 50 (1000 latent points decoded); decoding attempts per latent point varied (100 decode attempts used to label BNN negative points, diagnostics used up to 500 attempts). No explicit wall-clock time or monetary cost is reported; cost is operationalized by number of decodings and GP/BNN model training/evaluation steps.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected Improvement (EI) from the surrogate Gaussian process (posterior predictive mean and variance) is used as the utility measure; EIC multiplies EI by Pr(constraint).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration/exploitation is managed via the EI acquisition component (exploration via predictive variance, exploitation via expected improvement) modulated by the feasibility probability Pr(C(z)). Multiplication by Pr(C(z)) penalizes high-EI points with low decoding feasibility, biasing selection toward points likely to produce useful (decodable) outputs; when no feasible points exist, selection uses Pr(C(z)) only to explore for feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity-promoting term (e.g., determinantal point processes or explicit coverage objective) is introduced. Diversity arises implicitly from: (1) the EI acquisition's balance of mean and variance, (2) stochasticity in decoding (multiple decode attempts per latent point), and (3) batch selection via Kriging-Believer which can yield diverse batch members depending on surrogate posterior geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed-budget in terms of number of function evaluations (fixed number of BO iterations and batch size); implicit computational budget tied to number of decode attempts and model-training resources.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Handled by limiting the number of BO iterations and batch sizes (20 iterations, batch size 50 in experiments). Parallelization (Kriging-Believer) is used to exhaust batch budgets per iteration; acquisition prioritizes feasibility early (if no feasible points) to avoid wasting budget on invalid decodings.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>High-impact discoveries are identified by objective scores relative to the training-set distribution (percentile ranks) and raw objective values (e.g., penalized logP scores). The paper reports percentiles (e.g., new molecules occupying >90th percentile of training set scores) and best absolute penalized logP scores.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics include: validity rate (percentage of decoded molecules syntactically valid), percentage of 'realistic' molecules (valid and SMILES length > 5), percentage of unique novel realistic molecules, proportion passing structural/functional-group filters (ChEMBL/PAINS/etc.), and percentile rank of generated molecules' objective scores relative to the training set. Key reported numbers: validity (Constrained BO runs = 94%, 97%, 90%, 93%, 86% across five runs; Baseline runs = 29%, 51%, 12%, 37%, 49%); realistic decoding rate >80% for constrained vs <5% for unconstrained; percent passing ChEMBL filters: baseline 6.6% vs constrained 35.7%; constrained-generated molecules occupy ~92nd percentile (±4) vs baseline ~36th percentile (±14) for the penalized logP composite objective (Table 3/4; numbers reported in paper). Batch configuration: 20 iterations × batch size 50 → 1000 latent points decoded per run.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared primarily against the original unconstrained Bayesian optimization over VAE latent space (Automatic Chemical Design by Gómez-Bombarelli et al.). Other literature baselines are listed (Grammar VAE, SD-VAE, JT-VAE) but direct experimental comparisons in this paper are against the unconstrained BO baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Constrained BO substantially outperforms the unconstrained baseline on validity and practical utility: validity improved from averages ~35.6% (baseline across five runs) to ~92.0% (constrained) — an increase of ≈56 percentage points; realistic decoding rate >80% (constrained) vs <5% (baseline); fraction of novel molecules passing ChEMBL structural alerts improved from 6.6% (baseline) to 35.7% (constrained). Objective percentiles improved (e.g., penalized logP composite: baseline ≈36th percentile vs constrained ≈92nd percentile).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Qualitative and quantitative gains reported: large reductions in wasted evaluations on invalid/degenerate decodings (validity increased from ~36% to ~92%), higher rate of chemically realistic outputs (realistic >80% vs <5%), and higher objective percentiles (e.g., +~56 percentile-points in validity and major gains in downstream metric percentiles). The paper does not express gains as reductions in wall-clock time or monetary cost.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>The paper frames the constrained acquisition as an explicit tradeoff: EI rewards promising/high-uncertainty regions (information-seeking) while multiplication by Pr(constraint) reduces allocation to regions likely to produce invalid decodings (avoiding wasted compute). There is no formal, quantitative analysis of computational cost vs information gain (no costs in time/dollars), but qualitative findings: constrained BO reduces wasted evaluations in dead latent-space regions (thus reducing computational waste) and maintains or improves discovery of high-scoring molecules. The authors discuss batch-size tradeoffs: suspect that to benefit from sequential sampling the batch size should be on the same order as the initialization dataset size, otherwise parallel batches may blunt surrogate update effects.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Key recommendations: (1) Incorporate a probabilistic feasibility constraint (learned model of decoding success) into acquisition to avoid wasted evaluations in 'dead zones' of latent space; (2) use EIC (EI × Pr(feasible)) so allocation balances expected objective improvement and decoding feasibility; (3) if no feasible point found, prioritize searching for feasible regions (maximize Pr(constraint)) before objective optimization; (4) consider batch size relative to surrogate initialization size — batch sizes comparable to initialization may be preferable to maintain informative sequential updates. Overall conclusion: constrained Bayesian optimization is an effective allocation strategy to focus computational evaluations on informative, decodable hypotheses and thereby increase practical discovery yield.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Constrained Bayesian optimization for automatic chemical design using variational autoencoders', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2466.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2466.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EIC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expected Improvement with Constraints</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An acquisition function that multiplies the Expected Improvement (EI) acquisition by the probability a candidate satisfies a feasibility constraint: EIC(z) = EI(z) * Pr(C(z)). It is used to allocate evaluations to points expected to both improve the objective and be feasible.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Expected Improvement with Constraints (EIC) acquisition</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>EIC is an acquisition function for constrained Bayesian optimization. EI(z) = E[max(0, f(z) - h)] is computed from the surrogate GP posterior (mean and variance). A probabilistic constraint Pr(C(z)) is estimated by a separate model (here a BNN predicting decoding success). The acquisition is EIC(z) = EI(z) * Pr(C(z)). If no feasible point has yet been observed, the algorithm selects points by maximizing Pr(C(z)) (i.e., prioritizes discovery of the feasible region). The incumbent h is set as the minimum of the posterior mean such that constraints are satisfied (or alternative scheme described in constrained BO literature).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Constrained Bayesian optimization for molecular design (and general constrained black-box optimization problems).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates evaluation budget to latent points that maximize the product of expected objective improvement and feasibility probability, thereby steering evaluations away from points likely to produce invalid/low-value outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not explicitly stated; acquisition evaluation cost is that of computing EI from GP posterior and Pr(C(z)) from the BNN; experimental cost measured in number of objective evaluations (decodings) performed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected Improvement (EI) serves as the expected utility/information measure guiding selection.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>EI balances exploration (via posterior variance) and exploitation (via posterior mean improvement); multiplication by Pr(C(z)) reduces exploration into infeasible regions, biasing toward feasible exploration-exploitation tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>None intrinsic to EIC; diversity must come from surrogate uncertainty and batch selection strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of evaluations (BO iterations and batches); implicit computational budget for decoding calls.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Budget allocation determined by maximizing EIC under the given evaluation budget and batch schedule; if feasibility is absent, maximize Pr(C(z)) to find feasible points before optimizing EI.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Same as the system: breakthrough identified by high EI leading to high objective scores; in practice breakthroughs identified by percentile ranks or best observed objective values.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>The paper uses EIC within constrained BO and reports the improved downstream metrics (validity, realistic rate, objective percentiles) when EIC is used; see system-level reported numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared implicitly against using EI without constraints (unconstrained BO).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>EIC (with learned Pr(C)) leads to far fewer invalid decodings and higher-quality discovered molecules versus unconstrained EI: constrained (EIC) validity ~92% vs unconstrained ~36% (avg), realistic decoding >80% vs <5%.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>EIC reduces wasted evaluations on invalid decodings (empirically large increases in the fraction of useful decodings), but paper does not convert this to wall-clock or monetary savings.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>EIC operationalizes the tradeoff by downweighting EI in regions unlikely to be feasible; the paper qualitatively argues this reduces costly invalid evaluations and improves effective information gained per evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>EIC is recommended where the black-box objective is noisy and decoding/measurement can fail: multiply the expected improvement by the feasibility probability and, if feasibility is absent across the search space, prioritize discovering feasible regions first.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Constrained Bayesian optimization for automatic chemical design using variational autoencoders', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2466.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2466.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kriging-Believer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Kriging-Believer parallel selection algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A heuristic for batch (parallel) Bayesian optimization that sequentially selects batch members by pretending (believing) that pending evaluations are equal to the surrogate's predicted value, enabling efficient parallel batch construction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Kriging-Believer batch selection for parallel Bayesian optimization</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>To build a batch of k points to evaluate in parallel, Kriging-Believer iteratively selects one point by optimizing the acquisition, then augments the surrogate's training set with a fake observation at that point equal to the surrogate's predictive mean (or other imputed value), recomputes the acquisition, and repeats until k points are chosen. This produces a diverse batch informed by the surrogate while accounting approximately for the impact of pending evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Parallel Bayesian optimization in molecular design (and general expensive black-box optimization).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates a fixed per-iteration parallel budget (here batch size 50) by greedily constructing batch members using the Kriging-Believer imputation so that the surrogate and acquisition reflect the expected influence of the rest of the batch.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Measured in number of simultaneous evaluations per BO iteration (batch size) and total number of BO iterations (here: 20 iterations × batch 50 → 1000 latent points).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Batch members are chosen using an acquisition function (EIC) that relies on the surrogate's posterior mean and variance; the Kriging-Believer heuristic approximates the effect of pending evaluations on information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration/exploitation within a batch is influenced by the acquisition (EI × Pr(constraint)) and by the imputed values used when 'believing' pending points, which tends to spread batch members across locations of high acquisition value and predictive variance.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Kriging-Believer implicitly promotes batch diversity because after each selected point is imputed into the surrogate, subsequent selections consider the updated (imputed) surrogate, discouraging the batch from selecting many very similar points.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Parallel evaluation budget per iteration (fixed batch size), and fixed total number of iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Batch construction consumes the per-iteration budget; surrogate updates only occur after the batch completes, so Kriging-Believer approximates within-batch allocation to maximize aggregate expected utility under that budget.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Same as the system-level metric (best objective values or percentile relative to training set) evaluated over the generated batch of molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Experimental configuration: 20 iterations, batch size 50. No separate numerical 'Kriging-Believer vs other batch strategies' comparison is reported; Kriging-Believer is used as implementation detail for parallelization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not directly compared to other parallelization heuristics in this paper (reference to Ginsbourger et al. for multipoint criteria).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not quantified in this work; Kriging-Believer is adopted as a standard practical method to construct parallel batches.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Enables parallel consumption of evaluation budget (batch decoding) to accelerate wall-clock throughput; no quantitative speedup numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Authors note consideration of batch-size relative to initialization set size and suspect that large batches relative to the init set may blunt the benefit of sequential updates; recommendation to choose batch size on order of initialization dataset to preserve informative sequential learning.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Use Kriging-Believer to construct batches for parallel decoding, but be mindful that excessively large batch sizes relative to the surrogate's initialization data may reduce the effectiveness of sequential learning; choose batch sizes to balance parallel throughput and informative sequential updates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Constrained Bayesian optimization for automatic chemical design using variational autoencoders', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Bayesian optimization with unknown constraints <em>(Rating: 2)</em></li>
                <li>Constrained Bayesian Optimization and Applications <em>(Rating: 2)</em></li>
                <li>Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space <em>(Rating: 2)</em></li>
                <li>A multi-points criterion for deterministic parallel global optimization based on Gaussian processes <em>(Rating: 2)</em></li>
                <li>Phoenics: A Bayesian optimizer for chemistry <em>(Rating: 1)</em></li>
                <li>Achieving Robustness to Aleatoric Uncertainty with Heteroscedastic Bayesian Optimisation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2466",
    "paper_id": "paper-209713370",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "CBO-VAE",
            "name_full": "Constrained Bayesian Optimization for Automatic Chemical Design using Variational Autoencoders",
            "brief_description": "A system that performs Bayesian optimization over a VAE latent space under a learned probabilistic feasibility constraint (a BNN) to avoid querying latent-space 'dead zones' and thereby allocate evaluations to points likely to decode to valid, realistic molecules.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Constrained Bayesian Optimization for Automatic Chemical Design (CBO-VAE)",
            "system_description": "Combines a variational autoencoder (VAE) that maps molecules &lt;-&gt; continuous latent vectors with a constrained Bayesian optimization loop in latent space. A surrogate sparse Gaussian process (GP) models the noisy black-box objective (composite metrics like penalized logP, QED, or predicted PCE). A probabilistic constraint model (a Bayesian neural network, BNN) is trained to predict the probability that decoding a latent point yields a realistic/valid molecule. The acquisition function is Expected Improvement with Constraints (EIC = EI * Pr(constraint)), so candidate latent points are ranked by expected objective improvement multiplied by decoding feasibility. Batch (parallel) evaluations are performed using the Kriging-Believer parallelization strategy; batches of latent points are decoded multiple times (stochastic decoder) to evaluate noisy objectives. If no feasible region is yet found, the acquisition switches to maximizing Pr(constraint) to locate feasibility before optimizing the objective.",
            "application_domain": "De novo molecular design (drug discovery) and materials design (example: organic photovoltaics — PCE optimization).",
            "resource_allocation_strategy": "Acquisitions select latent points by maximizing EIC: EI(z) (expected improvement of surrogate GP over incumbent) multiplied by Pr(C(z)) from the BNN constraint. Parallel batches are chosen using Kriging-Believer: the surrogate is temporarily 'filled in' with predicted values for pending points to select subsequent points in the batch. When no feasible point has yet been observed, acquisition prioritizes Pr(C(z)) (searching for feasible region) rather than the objective. The procedure therefore allocates evaluation budget towards points that balance objective improvement and decoding feasibility.",
            "computational_cost_metric": "Implicitly measured as number of latent-point evaluations / decode attempts and surrogate updates: in the experiments, 20 BO iterations × batch size 50 (1000 latent points decoded); decoding attempts per latent point varied (100 decode attempts used to label BNN negative points, diagnostics used up to 500 attempts). No explicit wall-clock time or monetary cost is reported; cost is operationalized by number of decodings and GP/BNN model training/evaluation steps.",
            "information_gain_metric": "Expected Improvement (EI) from the surrogate Gaussian process (posterior predictive mean and variance) is used as the utility measure; EIC multiplies EI by Pr(constraint).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploration/exploitation is managed via the EI acquisition component (exploration via predictive variance, exploitation via expected improvement) modulated by the feasibility probability Pr(C(z)). Multiplication by Pr(C(z)) penalizes high-EI points with low decoding feasibility, biasing selection toward points likely to produce useful (decodable) outputs; when no feasible points exist, selection uses Pr(C(z)) only to explore for feasibility.",
            "diversity_mechanism": "No explicit diversity-promoting term (e.g., determinantal point processes or explicit coverage objective) is introduced. Diversity arises implicitly from: (1) the EI acquisition's balance of mean and variance, (2) stochasticity in decoding (multiple decode attempts per latent point), and (3) batch selection via Kriging-Believer which can yield diverse batch members depending on surrogate posterior geometry.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed-budget in terms of number of function evaluations (fixed number of BO iterations and batch size); implicit computational budget tied to number of decode attempts and model-training resources.",
            "budget_constraint_handling": "Handled by limiting the number of BO iterations and batch sizes (20 iterations, batch size 50 in experiments). Parallelization (Kriging-Believer) is used to exhaust batch budgets per iteration; acquisition prioritizes feasibility early (if no feasible points) to avoid wasting budget on invalid decodings.",
            "breakthrough_discovery_metric": "High-impact discoveries are identified by objective scores relative to the training-set distribution (percentile ranks) and raw objective values (e.g., penalized logP scores). The paper reports percentiles (e.g., new molecules occupying &gt;90th percentile of training set scores) and best absolute penalized logP scores.",
            "performance_metrics": "Reported metrics include: validity rate (percentage of decoded molecules syntactically valid), percentage of 'realistic' molecules (valid and SMILES length &gt; 5), percentage of unique novel realistic molecules, proportion passing structural/functional-group filters (ChEMBL/PAINS/etc.), and percentile rank of generated molecules' objective scores relative to the training set. Key reported numbers: validity (Constrained BO runs = 94%, 97%, 90%, 93%, 86% across five runs; Baseline runs = 29%, 51%, 12%, 37%, 49%); realistic decoding rate &gt;80% for constrained vs &lt;5% for unconstrained; percent passing ChEMBL filters: baseline 6.6% vs constrained 35.7%; constrained-generated molecules occupy ~92nd percentile (±4) vs baseline ~36th percentile (±14) for the penalized logP composite objective (Table 3/4; numbers reported in paper). Batch configuration: 20 iterations × batch size 50 → 1000 latent points decoded per run.",
            "comparison_baseline": "Compared primarily against the original unconstrained Bayesian optimization over VAE latent space (Automatic Chemical Design by Gómez-Bombarelli et al.). Other literature baselines are listed (Grammar VAE, SD-VAE, JT-VAE) but direct experimental comparisons in this paper are against the unconstrained BO baseline.",
            "performance_vs_baseline": "Constrained BO substantially outperforms the unconstrained baseline on validity and practical utility: validity improved from averages ~35.6% (baseline across five runs) to ~92.0% (constrained) — an increase of ≈56 percentage points; realistic decoding rate &gt;80% (constrained) vs &lt;5% (baseline); fraction of novel molecules passing ChEMBL structural alerts improved from 6.6% (baseline) to 35.7% (constrained). Objective percentiles improved (e.g., penalized logP composite: baseline ≈36th percentile vs constrained ≈92nd percentile).",
            "efficiency_gain": "Qualitative and quantitative gains reported: large reductions in wasted evaluations on invalid/degenerate decodings (validity increased from ~36% to ~92%), higher rate of chemically realistic outputs (realistic &gt;80% vs &lt;5%), and higher objective percentiles (e.g., +~56 percentile-points in validity and major gains in downstream metric percentiles). The paper does not express gains as reductions in wall-clock time or monetary cost.",
            "tradeoff_analysis": "The paper frames the constrained acquisition as an explicit tradeoff: EI rewards promising/high-uncertainty regions (information-seeking) while multiplication by Pr(constraint) reduces allocation to regions likely to produce invalid decodings (avoiding wasted compute). There is no formal, quantitative analysis of computational cost vs information gain (no costs in time/dollars), but qualitative findings: constrained BO reduces wasted evaluations in dead latent-space regions (thus reducing computational waste) and maintains or improves discovery of high-scoring molecules. The authors discuss batch-size tradeoffs: suspect that to benefit from sequential sampling the batch size should be on the same order as the initialization dataset size, otherwise parallel batches may blunt surrogate update effects.",
            "optimal_allocation_findings": "Key recommendations: (1) Incorporate a probabilistic feasibility constraint (learned model of decoding success) into acquisition to avoid wasted evaluations in 'dead zones' of latent space; (2) use EIC (EI × Pr(feasible)) so allocation balances expected objective improvement and decoding feasibility; (3) if no feasible point found, prioritize searching for feasible regions (maximize Pr(constraint)) before objective optimization; (4) consider batch size relative to surrogate initialization size — batch sizes comparable to initialization may be preferable to maintain informative sequential updates. Overall conclusion: constrained Bayesian optimization is an effective allocation strategy to focus computational evaluations on informative, decodable hypotheses and thereby increase practical discovery yield.",
            "uuid": "e2466.0",
            "source_info": {
                "paper_title": "Constrained Bayesian optimization for automatic chemical design using variational autoencoders",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "EIC",
            "name_full": "Expected Improvement with Constraints",
            "brief_description": "An acquisition function that multiplies the Expected Improvement (EI) acquisition by the probability a candidate satisfies a feasibility constraint: EIC(z) = EI(z) * Pr(C(z)). It is used to allocate evaluations to points expected to both improve the objective and be feasible.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Expected Improvement with Constraints (EIC) acquisition",
            "system_description": "EIC is an acquisition function for constrained Bayesian optimization. EI(z) = E[max(0, f(z) - h)] is computed from the surrogate GP posterior (mean and variance). A probabilistic constraint Pr(C(z)) is estimated by a separate model (here a BNN predicting decoding success). The acquisition is EIC(z) = EI(z) * Pr(C(z)). If no feasible point has yet been observed, the algorithm selects points by maximizing Pr(C(z)) (i.e., prioritizes discovery of the feasible region). The incumbent h is set as the minimum of the posterior mean such that constraints are satisfied (or alternative scheme described in constrained BO literature).",
            "application_domain": "Constrained Bayesian optimization for molecular design (and general constrained black-box optimization problems).",
            "resource_allocation_strategy": "Allocates evaluation budget to latent points that maximize the product of expected objective improvement and feasibility probability, thereby steering evaluations away from points likely to produce invalid/low-value outputs.",
            "computational_cost_metric": "Not explicitly stated; acquisition evaluation cost is that of computing EI from GP posterior and Pr(C(z)) from the BNN; experimental cost measured in number of objective evaluations (decodings) performed.",
            "information_gain_metric": "Expected Improvement (EI) serves as the expected utility/information measure guiding selection.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "EI balances exploration (via posterior variance) and exploitation (via posterior mean improvement); multiplication by Pr(C(z)) reduces exploration into infeasible regions, biasing toward feasible exploration-exploitation tradeoffs.",
            "diversity_mechanism": "None intrinsic to EIC; diversity must come from surrogate uncertainty and batch selection strategy.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed number of evaluations (BO iterations and batches); implicit computational budget for decoding calls.",
            "budget_constraint_handling": "Budget allocation determined by maximizing EIC under the given evaluation budget and batch schedule; if feasibility is absent, maximize Pr(C(z)) to find feasible points before optimizing EI.",
            "breakthrough_discovery_metric": "Same as the system: breakthrough identified by high EI leading to high objective scores; in practice breakthroughs identified by percentile ranks or best observed objective values.",
            "performance_metrics": "The paper uses EIC within constrained BO and reports the improved downstream metrics (validity, realistic rate, objective percentiles) when EIC is used; see system-level reported numbers.",
            "comparison_baseline": "Compared implicitly against using EI without constraints (unconstrained BO).",
            "performance_vs_baseline": "EIC (with learned Pr(C)) leads to far fewer invalid decodings and higher-quality discovered molecules versus unconstrained EI: constrained (EIC) validity ~92% vs unconstrained ~36% (avg), realistic decoding &gt;80% vs &lt;5%.",
            "efficiency_gain": "EIC reduces wasted evaluations on invalid decodings (empirically large increases in the fraction of useful decodings), but paper does not convert this to wall-clock or monetary savings.",
            "tradeoff_analysis": "EIC operationalizes the tradeoff by downweighting EI in regions unlikely to be feasible; the paper qualitatively argues this reduces costly invalid evaluations and improves effective information gained per evaluation.",
            "optimal_allocation_findings": "EIC is recommended where the black-box objective is noisy and decoding/measurement can fail: multiply the expected improvement by the feasibility probability and, if feasibility is absent across the search space, prioritize discovering feasible regions first.",
            "uuid": "e2466.1",
            "source_info": {
                "paper_title": "Constrained Bayesian optimization for automatic chemical design using variational autoencoders",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "Kriging-Believer",
            "name_full": "Kriging-Believer parallel selection algorithm",
            "brief_description": "A heuristic for batch (parallel) Bayesian optimization that sequentially selects batch members by pretending (believing) that pending evaluations are equal to the surrogate's predicted value, enabling efficient parallel batch construction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Kriging-Believer batch selection for parallel Bayesian optimization",
            "system_description": "To build a batch of k points to evaluate in parallel, Kriging-Believer iteratively selects one point by optimizing the acquisition, then augments the surrogate's training set with a fake observation at that point equal to the surrogate's predictive mean (or other imputed value), recomputes the acquisition, and repeats until k points are chosen. This produces a diverse batch informed by the surrogate while accounting approximately for the impact of pending evaluations.",
            "application_domain": "Parallel Bayesian optimization in molecular design (and general expensive black-box optimization).",
            "resource_allocation_strategy": "Allocates a fixed per-iteration parallel budget (here batch size 50) by greedily constructing batch members using the Kriging-Believer imputation so that the surrogate and acquisition reflect the expected influence of the rest of the batch.",
            "computational_cost_metric": "Measured in number of simultaneous evaluations per BO iteration (batch size) and total number of BO iterations (here: 20 iterations × batch 50 → 1000 latent points).",
            "information_gain_metric": "Batch members are chosen using an acquisition function (EIC) that relies on the surrogate's posterior mean and variance; the Kriging-Believer heuristic approximates the effect of pending evaluations on information gain.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploration/exploitation within a batch is influenced by the acquisition (EI × Pr(constraint)) and by the imputed values used when 'believing' pending points, which tends to spread batch members across locations of high acquisition value and predictive variance.",
            "diversity_mechanism": "Kriging-Believer implicitly promotes batch diversity because after each selected point is imputed into the surrogate, subsequent selections consider the updated (imputed) surrogate, discouraging the batch from selecting many very similar points.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Parallel evaluation budget per iteration (fixed batch size), and fixed total number of iterations.",
            "budget_constraint_handling": "Batch construction consumes the per-iteration budget; surrogate updates only occur after the batch completes, so Kriging-Believer approximates within-batch allocation to maximize aggregate expected utility under that budget.",
            "breakthrough_discovery_metric": "Same as the system-level metric (best objective values or percentile relative to training set) evaluated over the generated batch of molecules.",
            "performance_metrics": "Experimental configuration: 20 iterations, batch size 50. No separate numerical 'Kriging-Believer vs other batch strategies' comparison is reported; Kriging-Believer is used as implementation detail for parallelization.",
            "comparison_baseline": "Not directly compared to other parallelization heuristics in this paper (reference to Ginsbourger et al. for multipoint criteria).",
            "performance_vs_baseline": "Not quantified in this work; Kriging-Believer is adopted as a standard practical method to construct parallel batches.",
            "efficiency_gain": "Enables parallel consumption of evaluation budget (batch decoding) to accelerate wall-clock throughput; no quantitative speedup numbers provided.",
            "tradeoff_analysis": "Authors note consideration of batch-size relative to initialization set size and suspect that large batches relative to the init set may blunt the benefit of sequential updates; recommendation to choose batch size on order of initialization dataset to preserve informative sequential learning.",
            "optimal_allocation_findings": "Use Kriging-Believer to construct batches for parallel decoding, but be mindful that excessively large batch sizes relative to the surrogate's initialization data may reduce the effectiveness of sequential learning; choose batch sizes to balance parallel throughput and informative sequential updates.",
            "uuid": "e2466.2",
            "source_info": {
                "paper_title": "Constrained Bayesian optimization for automatic chemical design using variational autoencoders",
                "publication_date_yy_mm": "2019-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Bayesian optimization with unknown constraints",
            "rating": 2,
            "sanitized_title": "bayesian_optimization_with_unknown_constraints"
        },
        {
            "paper_title": "Constrained Bayesian Optimization and Applications",
            "rating": 2,
            "sanitized_title": "constrained_bayesian_optimization_and_applications"
        },
        {
            "paper_title": "Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space",
            "rating": 2,
            "sanitized_title": "parallel_and_distributed_thompson_sampling_for_largescale_accelerated_exploration_of_chemical_space"
        },
        {
            "paper_title": "A multi-points criterion for deterministic parallel global optimization based on Gaussian processes",
            "rating": 2,
            "sanitized_title": "a_multipoints_criterion_for_deterministic_parallel_global_optimization_based_on_gaussian_processes"
        },
        {
            "paper_title": "Phoenics: A Bayesian optimizer for chemistry",
            "rating": 1,
            "sanitized_title": "phoenics_a_bayesian_optimizer_for_chemistry"
        },
        {
            "paper_title": "Achieving Robustness to Aleatoric Uncertainty with Heteroscedastic Bayesian Optimisation",
            "rating": 1,
            "sanitized_title": "achieving_robustness_to_aleatoric_uncertainty_with_heteroscedastic_bayesian_optimisation"
        }
    ],
    "cost": 0.01654,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Constrained Bayesian optimization for automatic chemical design using variational autoencoders †</p>
<p>Ryan-Rhys Griffiths 
José Miguel Hernández-Lobato 
Constrained Bayesian optimization for automatic chemical design using variational autoencoders †
10.1039/c9sc04026a
Automatic Chemical Design is a framework for generating novel molecules with optimized properties. The original scheme, featuring Bayesian optimization over the latent space of a variational autoencoder, suffers from the pathology that it tends to produce invalid molecular structures. First, we demonstrate empirically that this pathology arises when the Bayesian optimization scheme queries latent space points far away from the data on which the variational autoencoder has been trained. Secondly, by reformulating the search procedure as a constrained Bayesian optimization problem, we show that the effects of this pathology can be mitigated, yielding marked improvements in the validity of the generated molecules. We posit that constrained Bayesian optimization is a good approach for solving this kind of training set mismatch in many generative tasks involving Bayesian optimization over the latent space of a variational autoencoder.</p>
<p>Introduction</p>
<p>Machine learning in chemical design has shown promise along a number of fronts. In quantitative structure activity relationship (QSAR) modelling, deep learning models have achieved state-of-the-art results in molecular property prediction 1-8 as well as property uncertainty quantication. [9][10][11][12] Progress is also being made in the interpretability and explainability of machine learning solutions to chemical design, a subeld concerned with extracting chemical insight from learned models. 13 The focus of this paper however, will be on molecule generation, leveraging machine learning to propose novel molecules that optimize a target objective.</p>
<p>One existing approach for nding molecules that maximize an application-specic metric involves searching a large library of compounds, either physically or virtually. 14,15 This has the disadvantage that the search is not open-ended; if the molecule is not contained in the library, the search won't nd it.</p>
<p>A second method involves the use of genetic algorithms. In this approach, a known molecule acts as a seed and a local search is performed over a discrete space of molecules. Although these methods have enjoyed success in producing biologically active compounds, an approach featuring a search over an open-ended, continuous space would be benecial. The use of geometrical cues such as gradients to guide the search in continuous space in conjunction with advances in Bayesian optimization methodologies 16,17 could accelerate both drug 14,18 and materials 19,20 discovery by functioning as a high-throughput virtual screen of unpromising candidates.</p>
<p>Recently, Gómez-Bombarelli et al. 21 presented Automatic Chemical Design, a variational autoencoder (VAE) architecture capable of encoding continuous representations of molecules. In continuous latent space, gradient-based optimization is leveraged to nd molecules that maximize a design metric.</p>
<p>Although a strong proof of concept, Automatic Chemical Design possesses a deciency in so far as it fails to generate a high proportion of valid molecular structures. The authors hypothesize 21 that molecules selected by Bayesian optimization lie in "dead regions" of the latent space far away from any data that the VAE has seen in training, yielding invalid structures when decoded.</p>
<p>The principle contribution of this paper is to present an approach based on constrained Bayesian optimization that generates a high proportion of valid sequences, thus solving the training set mismatch problem for VAE-based Bayesian optimization schemes.</p>
<p>Methods</p>
<p>SMILES representation</p>
<p>SMILES strings 22 are a means of representing molecules as a character sequence. This text-based format facilitates the use of tools from natural language processing for applications such as chemical reaction prediction [23][24][25][26][27][28] and chemical reaction classication. 29 To make the SMILES representation compatible with the VAE architecture, the SMILES strings are in turn converted to one-hot vectors indicating the presence or absence of a particular character within a sequence as illustrated in Fig. 1.</p>
<p>Variational autoencoders</p>
<p>Variational autoencoders 30,31 allow us to map molecules m to and from continuous values z in a latent space. The encoding z is interpreted as a latent variable in a probabilistic generative model over which there is a prior distribution p(z). The probabilistic decoder is dened by the likelihood function p q (m|z). The posterior distribution p q (z|m) is interpreted as the probabilistic encoder. The parameters of the likelihood p q (m|z) as well as the parameters of the approximate posterior distribution q f (z|m) are learned by maximizing the evidence lower bound (ELBO) Lðf; q; mÞ ¼ E qfðz|mÞ Â log p q ðm; zÞ À log q f ðz|mÞ Ã :</p>
<p>Variational autoencoders have been coupled with recurrent neural networks by ref. 32 to encode sentences into a continuous latent space. This approach is followed for the SMILES format both by ref. 21 and here. The SMILES variational autoencoder, together with our constraint function, is shown in Fig. 2.</p>
<p>The origin of dead regions in the latent space</p>
<p>The approach introduced in this paper aims to solve the problem of dead regions in the latent space of the VAE. It is rst however, important to understand the origin of these dead zones. Three ways in which a dead zone can arise are:</p>
<p>(1) Sampling locations that are very unlikely under the prior. This was noted in the original paper on variational autoencoders 30 where sampling was adjusted through the inverse conditional distribution function of a Gaussian.</p>
<p>(2) A latent space dimensionality that is articially high will yield dead zones in the manifold learned during training. 33 This has been demonstrated to be the case empirically in ref. 34.</p>
<p>(3) Inhomogenous training data; undersampled regions of the data space are liable to yield gaps in the latent space.</p>
<p>A schematic illustrating sampling from a dead zone, and the associated effect it has on the generated SMILES strings, is given in Fig. 3. In our case, the Bayesian optimization scheme is decoupled from the VAE and hence has no knowledge of the location of the learned manifold. In many instances the explorative behaviour in the acquisition phase of Bayesian optimization will drive the selection of invalid points lying far away from the learned manifold.</p>
<p>Objective functions for Bayesian optimization of molecules</p>
<p>Bayesian optimization is performed here in the latent space of the variational autoencoder in order to nd molecules that score highly under a specied objective function. We assess molecular quality on the following objectives:
J comp log P (z) ¼ log P(z) À SA(z) À ring-penalty(z), J comp QED (z) ¼ QED(z) À SA(z) À ring-penalty(z), J QED (z) ¼ QED(z).
z denotes a molecule's latent representation, log P(z) is the water-octanol partition coefficient, QED(z) is the quantitative estimate of drug-likeness 35 and SA(z) is the synthetic accessibility. 36 The ring penalty term is as featured in ref. 21. The "comp" subscript is designed to indicate that the objective function is a composite of standalone metrics. It is important to note, that the rst objective, a common metric of comparison in this area, is misspecied as has been pointed out by ref. 37. From a chemical standpoint it is undesirable to maximize the log P score as is being done here. Rather it is preferable to optimize log P to be in a range that is in accordance with the Lipinski rule of ve. 38 We use the penalized log P objective here because regardless of its relevance for chemistry, it serves as a point of comparison against other methods. Fig. 1 The SMILES representation and one-hot encoding for benzene. For purposes of illustration, only the characters present in benzene are shown in the one-hot encoding. In practice there is a column for each character in the SMILES alphabet.  </p>
<p>Constrained Bayesian optimization of molecules</p>
<p>We now describe our extension to the Bayesian optimization procedure followed by ref. 21. Expressed formally, the constrained optimization problem is max z f ðzÞ s:t: Pr
À CðzÞ Á $ 1 À d where f (z)
is a black-box objective function, Pr À CðzÞ Á denotes the probability that a Boolean constraint CðzÞ is satised and 1 À d is some user-specied minimum condence that the constraint is satised. 39 The constraint is that a latent point must decode successfully a large fraction of the times decoding is attempted. The specic fractions used are provided in the results section. The black-box objective function is noisy because a single latent point may decode to multiple molecules when the model makes a mistake, obtaining different values under the objective. In practice, f (z) is one of the objectives described in Section 2.3.</p>
<p>Expected improvement with constraints (EIC)</p>
<p>EIC may be thought of as expected improvement (EI),
EI(z) ¼ E f(z) [max(0,f(z) À h)],
that offers improvement only when a set of constraints are satised: 40
EICðzÞ ¼ EIðzÞPr À CðzÞ Á :
The incumbent solution h in EI(z), may be set in an analogous way to vanilla expected improvement 41 as either:</p>
<p>(1) The best observation in which all constraints are observed to be satised.</p>
<p>(2) The minimum of the posterior mean such that all constraints are satised.</p>
<p>The latter approach is adopted for the experiments performed in this paper. If at the stage in the Bayesian optimization procedure where a feasible point has yet to be located, the form of acquisition function used is that dened by ref. 41.
EICðzÞ ¼ &amp; Pr À CðzÞ Á EIðzÞ; if dz; Pr À CðzÞ Á $ 1 À d PrðCðzÞÞ; otherwise
with the intuition being that if the probabilistic constraint is violated everywhere, the acquisition function selects the point having the highest probability of lying within the feasible region. The algorithm ignores the objective until it has located the feasible region.</p>
<p>Related work</p>
<p>The literature concerning generative models of molecules has exploded since the rst work on the topic. 21 Current methods feature molecular representations such as SMILES 42-54 and graphs [55][56][57][58][59][60][61][62][63][64][65][66][67][68][69][70][71][72] and employ reinforcement learning 73-83 as well as generative adversarial networks 84 for the generative process. These methods are well-summarized by a number of recent review articles. [85][86][87][88][89] In terms of VAE-based approaches, two popular approaches for incorporating property information into the generative process are Bayesian optimization and conditional variational autoencoders (CVAEs). 90 When generating molecules using CVAEs, the target data y is embedded into the latent space and conditional sampling is performed 47,91 in place of a directed search via Bayesian optimization. In this work we focus solely on VAE-based Bayesian optimization schemes for molecule generation and so we do not benchmark model performance against the aforementioned methods. Principally, we are concerned with highlighting the issue of training set mismatch in VAE-based Bayesian optimizations schemes and demonstrating the superior performance of a constrained Bayesian optimization approach.</p>
<p>Results and discussion</p>
<p>Experiment I Drug design. In this section we conduct an empirical test of the hypothesis from ref. 21 that the decoder's lack of efficiency is due to data point collection in "dead regions" of the latent space far from the data on which the VAE was trained. We use this information to construct a binary classication Bayesian Neural Network (BNN) to serve as a constraint function that outputs the probability of a latent point being valid, the details of which will be discussed in the section on labelling criteria. The BNN implementation is adapted from the MNIST digit classication network of ref. 92 and is trained using black-box alpha divergence minimization. Secondly, we compare the performance of our constrained Bayesian optimization implementation against the original model (baseline) in terms of the numbers of valid, realistic and drug-like molecules generated. We introduce the concept of a realistic molecule i.e. one that has a SMILES length greater than 5 as a heuristic to gauge whether the decoder has been successful or not. Our denition of drug-like is that a molecule must pass 8 sets of structural alerts or functional group lters from the ChEMBL database. 93 Thirdly, we compare the quality of the molecules produced by constrained Bayesian optimization with those of the baseline model. The code for all experiments has been made publicly available at https://github.com/Ryan-Rhys/Constrained-Bayesian-Optimisation-for-Automatic-Chemical-Design.</p>
<p>Implementation. The implementation details of the encoder-decoder network as well as the sparse GP for modelling the objective remain unchanged from ref. 21. For the constrained Bayesian optimization algorithm, the BNN is constructed with 2 hidden layers each 100 units wide with ReLU activation functions and a logistic output. Minibatch size is set to 1000 and the network is trained for 5 epochs with a learning rate of 0.0005. 20 iterations of parallel Bayesian optimization are performed using the Kriging-Believer algorithm 94 in all cases. Data is collected in batch sizes of 50. The same training set as ref. 21 is used, namely 249, 456 drug-like molecules drawn at random from the ZINC database. 95 Diagnostic experiments and labelling criteria. These experiments were designed to test the hypothesis that points collected by Bayesian optimization lie far away from the training data in latent space. In doing so, they also serve as labelling criteria for the data collected to train the BNN acting as the constraint function. The resulting observations are summarized in Fig. 4.</p>
<p>There is a noticeable decrease in the percentage of valid molecules decoded as one moves further away from the training data in latent space. Points collected by Bayesian optimization do the worst in terms of the percentage of valid decodings. This would suggest that these points lie farthest from the training data. The decoder over-generates methane molecules when far away from the data. One hypothesis for why this is the case is that methane is represented as 'C' in the SMILES syntax and is by far the most common character. Hence far away from the training data, combinations such as 'C' followed by a stop character may have high probability under the distribution over sequences learned by the decoder.</p>
<p>Given that methane has far too low a molecular weight to be a suitable drug candidate, a third plot in Fig. 3(c), shows the percentage of decoded molecules such that the molecules are both valid and have a tangible molecular weight. The denition of a tangible molecular weight was interpreted somewhat arbitrarily as a SMILES length of 5 or greater. Henceforth, molecules that are both valid and have a SMILES length greater than 5 will be referred to as realistic. This denition serves the purpose of determining whether the decoder has been successful or not.</p>
<p>As a result of these diagnostic experiments, it was decided that the criteria for labelling latent points to initialize the binary classication neural network for the constraint would be the following: if the latent point decodes into realistic molecules in more than 20% of decode attempts, it should be classied as realistic and non-realistic otherwise.</p>
<p>Molecular validity. The BNN for the constraint was initialized with 117 440 positive class points and 117 440 negative class points. The positive points were obtained by running the training data through the decoder assigning them positive labels if they satised the criteria outlined in the previous section. The negative class points were collected by decoding points sampled uniformly at random across the 56 latent dimensions of the design space. Each latent point undergoes 100 decode attempts and the most probable SMILES string is retained. J comp log P (z) is the choice of objective function. The raw validity percentages for constrained and unconstrained Bayesian optimization are given in Table 1.</p>
<p>In terms of realistic molecules, the relative performance of constrained Bayesian optimization and unconstrained Bayesian optimization (baseline) 21 is compared in Fig. 5(a).</p>
<p>The results show that greater than 80% of the latent points decoded by constrained Bayesian optimization produce realistic molecules compared to less than 5% for unconstrained Bayesian optimization. One must account however, for the fact that the constrained approach may be decoding multiple instances of the same novel molecules. Constrained and unconstrained Bayesian optimization are compared on the metric of the percentage of unique novel molecules produced in Fig. 5(b).</p>
<p>One may observe that constrained Bayesian optimization outperforms unconstrained Bayesian optimization in terms of the generation of unique molecules, but not by a large margin. A manual inspection of the SMILES strings collected by the unconstrained optimization approach showed that there were many strings with lengths marginally larger than the cutoff point, which is suggestive of partially decoded molecules. We run a further test of drug-likeness for the unique novel molecules generated by both methods consisting of passing a number of functional group lters consisting of 8 sets of structural alerts from the ChEMBL database. The alerts consisted of the Pan Assay Interference Compounds (PAINS) 96 alert set for nuisance compounds that elude usual reactivity, the NIH MLSMR alert set for excluded functionality lters, the Inpharmatica alert set for unwanted fragments, the Dundee alert set, 97 the BMS alert set, 98 the Pzer Lint procedure alert set 99 and the Glaxo Wellcome alert set. 100 An additional screen  dictating that molecules should have a molecular weight between 150-500 daltons was also included. The results are given in Table 2.</p>
<p>In the next section we compare the quality of the novel molecules produced as judged by the scores from the black-box objective function.</p>
<p>Molecular quality. The results of Fig. 6 indicate that constrained Bayesian optimization is able to generate higher quality molecules relative to unconstrained Bayesian optimization across the three drug-likeness metrics introduced in Section 2.3. Over the 5 independent runs, the constrained optimization procedure in every run produced new molecules ranked in the 100th percentile of the distribution over training set scores for the J comp log P (z) objective and over the 90th percentile for the remaining objectives. Table 3 gives the percentile that the averaged score of the new molecules found by each process occupies in the distribution over training set scores. The J comp log P (z) objective is included as a metric for the generative performance of the models. It has been previously noted that it should not be benecial for the purposes of drug design. For the penalised log P objective function, scores for each run are presented in Table 4. The best score obtained from our constrained Bayesian optimization approach is compared against the scores reported by other methods in Table 5. The best molecule under the penalised log P objective obtained from our method is depicted in Fig. 7.    </p>
<p>Experiment II</p>
<p>Combining molecule generation and property prediction. In order to show that the constrained Bayesian optimization approach is extensible beyond the realm of drug design, we trained the model on data from the Harvard Clean Energy Project 19,20 to generate molecules optimized for power conversion efficiency (PCE). In the absence of ground truth values for the PCE of the novel molecules generated, we use the output of a neural network trained to predict PCE as a surrogate. As such, the predictive accuracy of the property prediction model will be a bottleneck for the quality of the generated molecules.</p>
<p>Implementation. A Bayesian neural network with 2 hidden layers and 50 ReLU units per layer was trained to predict the PCE of 200 000 molecules drawn at random from the Harvard Clean Energy Project dataset using 512 bit Morgan circular ngerprints 101 as input features with bond radius of 2 computed using RDKit. 102 While a larger radius may be appropriate for the prediction of PCE in order to represent conjugation, we are only interested in showing how a property predictor might be incorporated into the automatic chemical design framework and not in optimizing that predictor. The network was trained for 25 epochs with the ADAM optimizer 103 using black box alpha divergence minimization with an alpha parameter of 5, a learning rate of 0.01, and a batch size of 500. The RMSE on the training set of 200 000 molecules is 0.681 and the RMSE on the test set of 25 000 molecules is 0.999.</p>
<p>PCE scores. The results are given in Fig. 8. The averaged score of the new molecules generated lies above the 90th percentile in the distribution over training set scores. Given that the objective function in this instance was learned using a neural network, advances in predicting chemical properties from data 104,105 are liable to yield concomitant improvements in the optimized molecules generated through this approach.</p>
<p>Concluding remarks</p>
<p>The reformulation of the search procedure in the Automatic Chemical Design model as a constrained Bayesian optimization problem has led to concrete improvements on two fronts:</p>
<p>(1) Validitythe number of valid molecules produced by the constrained optimization procedure offers a marked improvement over the original model.</p>
<p>(2) Qualityfor ve independent train/test splits, the scores of the best molecules generated by the constrained optimization procedure consistently ranked above the 90th percentile of the distribution over training set scores for all objectives considered.</p>
<p>These improvements provide strong evidence that constrained Bayesian optimization is a good solution method for the training set mismatch pathology present in the unconstrained approach for molecule generation. More generally, we foresee that constrained Bayesian optimization is a workable solution to the training set mismatch problem in any VAE-based Bayesian optimization scheme. Our code is made publicly available at https://github.com/Ryan-Rhys/Constrained-Bayesian-Optimisation-for-Automatic-Chemical-Design. Further work could feature improvements to the constraint scheme [106][107][108][109][110][111] as well as extensions to model heteroscedastic noise. 112  . 7 The best molecule obtained by constrained Bayesian optimization as judged by the penalised log P objective function score. Fig. 8 The best scores for novel molecules generated by the constrained Bayesian optimization model optimizing for PCE. The results are averaged over 3 separate runs with train/test splits of 90/10. The PCE score is normalized to zero mean and unit variance by the empirical mean and variance of the training set.</p>
<p>In terms of objectives for molecule generation, recent work by 44,89,91,113,114 has featured a more targeted search for novel compounds. This represents a move towards more industriallyrelevant objective functions for Bayesian optimization which should ultimately replace the chemically misspecied objectives, such as the penalized log P score, identied both here and in ref. 37. In addition, efforts at benchmarking generative models of molecules 115,116 should also serve to advance the eld. Finally, in terms of improving parallel Bayesian optimization procedures in molecule generation applications one point of consideration is the relative batch size of collected points compared to the dataset size used to initialize the surrogate model. We suspect that in order to gain benet from sequential sampling the batch size should be on the same order of magnitude as the size of the initialization set as this will induce the uncertainty estimates of the updated surrogate model to change in a tangible manner.</p>
<p>Conflicts of interest</p>
<p>There are no conicts to declare.</p>
<p>Fig. 2
2The SMILES variational autoencoder with the learned constraint function illustrated by a circular feasible region in the latent space.</p>
<p>Fig. 3
3The dead zones in the latent space, adapted from ref.21. The x and y axes are the principle components computed by PCA. The colour bar gives the log P value of the encoded latent points and the histograms show the coordinate-projected density of the latent points. One may observe that the encoded molecules are not distributed uniformly across the box constituting the bounds of the latent space.</p>
<p>Fig. 4
4Experiments on 5 disjoint sets comprising 50 latent points each. Very small (VS) noise are training data latent points with approximately 1% noise added to their values, small (S) noise have 10% noise added to their values and big (B) noise have 50% noise added to their values. All latent points underwent 500 decode attempts and the results are averaged over the 50 points in each set. The percentage of decodings to: (a) valid molecules (b) methane molecule, (c) realistic molecules.</p>
<p>Fig. 5
5(a) The percentage of latent points decoded to realistic molecules. (b) The percentage of latent points decoded to unique, novel realistic molecules. The results are from 20 iterations of Bayesian optimization with batches of 50 data points collected at each iteration (1000 latent points decoded in total). The standard error is given for 5 separate train/test set splits of 90/10.</p>
<p>Fig. 6
6The best scores for new molecules generated from the baseline model (blue) and the model with constrained Bayesian optimization (red). The vertical lines show the best scores averaged over 5 separate train/test splits of 90/10. For reference, the histograms are presented against the backdrop of the top 10% of the training data in the case of composite log P and QED, and the top 20% of the training data in the case of composite QED.</p>
<p>Table 1
1Percentage of valid molecules producedRun 
Baseline 
Constrained </p>
<p>1 
29% 
94% 
2 
51% 
97% 
3 
12% 
90% 
4 
37% 
93% 
5 
49% 
86% </p>
<p>Table 2
2Percentage of novel generated molecules passing ChemBL structural alertsBaseline 
Constrained </p>
<p>6.6% 
35.7% </p>
<p>Table 3
3Percentile of the averaged new molecule score relative to the 
training data. The results of 5 separate train/test set splits of 90/10 are 
provided </p>
<p>Objective 
Baseline 
Constrained </p>
<p>log P composite 
36 AE 14 
92 AE 4 
QED composite 
14 AE 3 
7 2 AE 10 
QED 
11 AE 2 
7 9 AE 4 </p>
<p>Table 4
4Penalised log P objective scores with the best score obtained highlighted in boldTable 5Comparison of penalised log P objective function scores against other models. Note that the results are taken from the original works and as such don't constitute a direct performance comparison due to different run configurationsRun 
Baseline 
Constrained </p>
<p>1 
2.02 
4.01 
2 
2.81 
3.86 
3 
1.45 
3.62 
4 
2.56 
3.82 
5 
2.47 
3.63 </p>
<p>Grammar VAE 59 </p>
<p>Constrained 
BO VAE 
SD-VAE 60 
JT-VAE 57 </p>
<p>2.94 
4.01 
4.04 
5.30 </p>
<p>AcknowledgementsThe authors thank Jonathan Gordon for useful discussions on the interaction between the Bayesian optimization scheme and the latent space of the variational autoencoder.
Deeply learning molecular structure-property relationships using attention-and gate-augmented graph convolutional network. S Ryu, J Lim, S H Hong, W Y Kim, arXiv:1805.10988arXiv preprintS. Ryu, J. Lim, S. H. Hong and W. Y. Kim, Deeply learning molecular structure-property relationships using attention-and gate-augmented graph convolutional network, arXiv preprint arXiv:1805.10988, 2018.</p>
<p>Deep learning improves prediction of drug-drug and drug-food interactions. J Y Ryu, H U Kim, S Y Lee, Proc. Natl. Acad. Sci. U. S. A. 115J. Y. Ryu, H. U. Kim and S. Y. Lee, Deep learning improves prediction of drug-drug and drug-food interactions, Proc. Natl. Acad. Sci. U. S. A., 2018, 115, E4304-E4311.</p>
<p>Machine Learning for Organic Cage Property Prediction. L Turcani, R L Greenaway, K E Jelfs, Chem. Mater. 31L. Turcani, R. L. Greenaway and K. E. Jelfs, Machine Learning for Organic Cage Property Prediction, Chem. Mater., 2018, 31, 714-727.</p>
<p>Predicting adverse drug reactions through interpretable deep learning framework. S Dey, H Luo, A Fokoue, J Hu, P Zhang, BMC Bioinf. 19476S. Dey, H. Luo, A. Fokoue, J. Hu and P. Zhang, Predicting adverse drug reactions through interpretable deep learning framework, BMC Bioinf., 2018, 19, 476.</p>
<p>Convolutional embedding of attributed molecular graphs for physical property prediction. C W Coley, R Barzilay, W H Green, T S Jaakkola, K F Jensen, J. Chem. Inf. Model. 57C. W. Coley, R. Barzilay, W. H. Green, T. S. Jaakkola and K. F. Jensen, Convolutional embedding of attributed molecular graphs for physical property prediction, J. Chem. Inf. Model., 2017, 57, 1757-1772.</p>
<p>Machine learning for renewable energy materials. G H Gu, J Noh, I Kim, Y Jung, J. Mater. Chem. A. 7G. H. Gu, J. Noh, I. Kim and Y. Jung, Machine learning for renewable energy materials, J. Mater. Chem. A, 2019, 7, 17096-17117.</p>
<p>M Zeng, J N Kumar, Z Zeng, R Savitha, V R Chandrasekhar, K Hippalgaonkar, arXiv:1811.06231Graph convolutional neural networks for polymers property prediction. arXiv preprintM. Zeng, J. N. Kumar, Z. Zeng, R. Savitha, V. R. Chandrasekhar and K. Hippalgaonkar, Graph convolutional neural networks for polymers property prediction, arXiv preprint arXiv:1811.06231, 2018.</p>
<p>A graphconvolutional neural network model for the prediction of chemical reactivity. C W Coley, W Jin, L Rogers, T F Jamison, T S Jaakkola, W H Green, R Barzilay, K F Jensen, Chem. Sci. 10C. W. Coley, W. Jin, L. Rogers, T. F. Jamison, T. S. Jaakkola, W. H. Green, R. Barzilay and K. F. Jensen, A graph- convolutional neural network model for the prediction of chemical reactivity, Chem. Sci., 2019, 10, 370-377.</p>
<p>Deep condence: a computationally efficient framework for calculating reliable prediction errors for deep neural networks. I Cortés-Ciriano, A Bender, J. Chem. Inf. Model. 59I. Cortés-Ciriano and A. Bender, Deep condence: a computationally efficient framework for calculating reliable prediction errors for deep neural networks, J. Chem. Inf. Model., 2018, 59, 1269-1281.</p>
<p>Bayesian semi-supervised learning for uncertainty-calibrated prediction of molecular properties and active learning. Y Zhang, A A Lee, Chem. Sci. Y. Zhang and A. A. Lee, Bayesian semi-supervised learning for uncertainty-calibrated prediction of molecular properties and active learning, Chem. Sci., 2019.</p>
<p>A quantitative uncertainty metric controls error in neural network-driven chemical discovery. J P Janet, C Duan, T Yang, A Nandy, H Kulik, Chem. Sci. J. P. Janet, C. Duan, T. Yang, A. Nandy and H. Kulik, A quantitative uncertainty metric controls error in neural network-driven chemical discovery, Chem. Sci., 2019.</p>
<p>S Ryu, Y Kwon, W Y Kim, arXiv:1903.08375Uncertainty quantication of molecular property prediction with Bayesian neural networks. arXiv preprintS. Ryu, Y. Kwon and W. Y. Kim, Uncertainty quantication of molecular property prediction with Bayesian neural networks, arXiv preprint arXiv:1903.08375, 2019.</p>
<p>Using attribution to decode binding mechanism in neural network models for chemistry. K Mccloskey, A Taly, F Monti, M P Brenner, L J Colwell, Proc. Natl. Acad. Sci. U. S. A. 116K. McCloskey, A. Taly, F. Monti, M. P. Brenner and L. J. Colwell, Using attribution to decode binding mechanism in neural network models for chemistry, Proc. Natl. Acad. Sci. U. S. A., 2019, 116, 11624-11629.</p>
<p>What is high-throughput virtual screening? A perspective from organic materials discovery. E O Pyzer-Knapp, C Suh, R Gómez-Bombarelli, J Aguilera-Iparraguirre, A Aspuru-Guzik, Annu. Rev. Mater. Res. 45E. O. Pyzer-Knapp, C. Suh, R. Gómez-Bombarelli, J. Aguilera-Iparraguirre and A. Aspuru-Guzik, What is high-throughput virtual screening? A perspective from organic materials discovery, Annu. Rev. Mater. Res., 2015, 45, 195-216.</p>
<p>Méthodes d'apprentissage statistique pour le criblage virtuel de médicament. B Playe, Paris Sciences et LettresPhD thesisB. Playe, Méthodes d'apprentissage statistique pour le criblage virtuel de médicament, PhD thesis, Paris Sciences et Lettres, 2019.</p>
<p>Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space. J M Hernández-Lobato, J Requeima, E O Pyzer-Knapp, A Aspuru-Guzik, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70J. M. Hernández-Lobato, J. Requeima, E. O. Pyzer-Knapp and A. Aspuru-Guzik, Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space, Proceedings of the 34th International Conference on Machine Learning, 2017, vol. 70, pp. 1470- 1479.</p>
<p>Bayesian optimization for accelerated drug discovery. E Pyzer-Knapp, IBM J. Res. Dev. 62E. Pyzer-Knapp, Bayesian optimization for accelerated drug discovery, IBM J. Res. Dev., 2018, 62, 2-1.</p>
<p>Design of efficient molecular organic light-emitting diodes by a high-throughput virtual screening and experimental approach. R Gómez-Bombarelli, J Aguilera-Iparraguirre, T D Hirzel, D Duvenaud, D Maclaurin, M A Blood-Forsythe, H S Chae, M Einzinger, D.-G Ha, T Wu, Nat. Mater. 15R. Gómez-Bombarelli, J. Aguilera-Iparraguirre, T. D. Hirzel, D. Duvenaud, D. Maclaurin, M. A. Blood-Forsythe, H. S. Chae, M. Einzinger, D.-G. Ha, T. Wu, et al., Design of efficient molecular organic light-emitting diodes by a high-throughput virtual screening and experimental approach, Nat. Mater., 2016, 15, 1120-1127.</p>
<p>The Harvard Clean Energy Project: Large-Scale Computational Screening and Design of Organic Photovoltaics on the World Community Grid. J Hachmann, R Olivares-Amaya, S Atahan-Evrenk, C Amador-Bedolla, R S Sãčâąnchez-Carrera, A Gold-Parker, L Vogt, A M Brockway, A Aspuru-Guzik, J. Phys. Chem. Lett. 2J. Hachmann, R. Olivares-Amaya, S. Atahan-Evrenk, C. Amador-Bedolla, R. S. SÃČÂąnchez-Carrera, A. Gold- Parker, L. Vogt, A. M. Brockway and A. Aspuru-Guzik, The Harvard Clean Energy Project: Large-Scale Computational Screening and Design of Organic Photovoltaics on the World Community Grid, J. Phys. Chem. Lett., 2011, 2, 2241-2251.</p>
<p>Lead candidates for high-performance organic photovoltaics from highthroughput quantum chemistry -the Harvard Clean Energy Project. J Hachmann, R Olivares-Amaya, A Jinich, A L Appleton, M A Blood-Forsythe, L R Seress, C Roman-Salgado, K Trepte, S Atahan-Evrenk, S Er, Energy Environ. Sci. 7J. Hachmann, R. Olivares-Amaya, A. Jinich, A. L. Appleton, M. A. Blood-Forsythe, L. R. Seress, C. Roman-Salgado, K. Trepte, S. Atahan-Evrenk, S. Er, et al., Lead candidates for high-performance organic photovoltaics from high- throughput quantum chemistry -the Harvard Clean Energy Project, Energy Environ. Sci., 2014, 7, 698-704.</p>
<p>. R Gómez-Bombarelli, J N Wei, D Duvenaud, J M Hernández-Lobato, B Sánchez-Lengeling, R. Gómez-Bombarelli, J. N. Wei, D. Duvenaud, J. M. Hernández-Lobato, B. Sánchez-Lengeling,</p>
<p>Automatic chemical design using a data-driven continuous representation of molecules. D Sheberla, J Aguilera-Iparraguirre, T D Hirzel, R P Adams, A Aspuru-Guzik, ACS Cent. Sci. 4D. Sheberla, J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams and A. Aspuru-Guzik, Automatic chemical design using a data-driven continuous representation of molecules, ACS Cent. Sci., 2018, 4, 268-276.</p>
<p>SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. D Weininger, J. Chem. Inf. Comput. Sci. 28D. Weininger, SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules, J. Chem. Inf. Comput. Sci., 1988, 28, 31-36.</p>
<p>Found in Translation': predicting outcomes of complex organic chemistry reactions using neural sequence-tosequence models. P Schwaller, T Gaudin, D Lanyi, C Bekas, T Laino, Chem. Sci. 9P. Schwaller, T. Gaudin, D. Lanyi, C. Bekas and T. Laino, 'Found in Translation': predicting outcomes of complex organic chemistry reactions using neural sequence-to- sequence models, Chem. Sci., 2018, 9, 6091-6098.</p>
<p>Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network. W Jin, C Coley, R Barzilay, T Jaakkola, Advances in Neural Information Processing Systems. W. Jin, C. Coley, R. Barzilay and T. Jaakkola, Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network, Advances in Neural Information Processing Systems, 2017, pp 2604-2613.</p>
<p>A graphconvolutional neural network model for the prediction of chemical reactivity. C W Coley, W Jin, L Rogers, T F Jamison, T S Jaakkola, W H Green, R Barzilay, K F Jensen, Chem. Sci. 10C. W. Coley, W. Jin, L. Rogers, T. F. Jamison, T. S. Jaakkola, W. H. Green, R. Barzilay and K. F. Jensen, A graph- convolutional neural network model for the prediction of chemical reactivity, Chem. Sci., 2019, 10, 370-377.</p>
<p>Molecular transformer for chemical reaction prediction and uncertainty estimation. P Schwaller, T Laino, T Gaudin, P Bolgar, C Bekas, A A Lee, arXiv:1811.02633arXiv preprintP. Schwaller, T. Laino, T. Gaudin, P. Bolgar, C. Bekas and A. A. Lee, Molecular transformer for chemical reaction prediction and uncertainty estimation, arXiv preprint arXiv:1811.02633, 2018.</p>
<p>J Bradshaw, M J Kusner, B Paige, M H Segler, J M Hernández-Lobato, A Generative Model of Electron Paths, International Conference on Learning Representations. J. Bradshaw, M. J. Kusner, B. Paige, M. H. Segler and J. M. Hernández-Lobato, A Generative Model of Electron Paths, International Conference on Learning Representations, 2019.</p>
<p>J Bradshaw, B Paige, M J Kusner, M H Segler, J M Hernández-Lobato, arXiv:1906.05221A Model to Search for Synthesizable Molecules. arXiv preprintJ. Bradshaw, B. Paige, M. J. Kusner, M. H. Segler and J. M. Hernández-Lobato, A Model to Search for Synthesizable Molecules, arXiv preprint arXiv:1906.05221, 2019.</p>
<p>Data-Driven Chemical Reaction Classication with Attention-Based Neural Networks. P Schwaller, A C Vaucher, V H Nair, T Laino, ChemRxiv. P. Schwaller, A. C. Vaucher, V. H. Nair and T. Laino, Data- Driven Chemical Reaction Classication with Attention- Based Neural Networks, ChemRxiv, 2019.</p>
<p>D P Kingma, M Welling, Auto-Encoding Variational Bayes, International Conference on Learning Representations. D. P. Kingma and M. Welling, Auto-Encoding Variational Bayes, International Conference on Learning Representations, 2014.</p>
<p>Semi-supervised learning with deep generative models. D P Kingma, S Mohamed, D J Rezende, M Welling, Advances in Neural Information Processing Systems. D. P. Kingma, S. Mohamed, D. J. Rezende and M. Welling, Semi-supervised learning with deep generative models, Advances in Neural Information Processing Systems, 2014, pp. 3581-3589.</p>
<p>Generating Sentences from a Continuous Space. S R Bowman, L Vilnis, O Vinyals, A M Dai, R Józefowicz, S Bengio, CoNLLS. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Józefowicz and S. Bengio Generating Sentences from a Continuous Space, CoNLL, 2015.</p>
<p>T White, arXiv:1609.04468Sampling Generative Networks. arXiv preprintT. White, Sampling Generative Networks, arXiv preprint arXiv:1609.04468, 2016.</p>
<p>A Makhzani, J Shlens, N Jaitly, I Goodfellow, B Frey, arXiv:1511.05644Adversarial autoencoders. arXiv preprintA. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow and B. Frey, Adversarial autoencoders, arXiv preprint arXiv:1511.05644, 2015.</p>
<p>Quantifying the chemical beauty of drugs. R G Bickerton, G V Paolini, J Besnard, S Muresan, A L Hopkins, Nat. Chem. 4R. G. Bickerton, G. V. Paolini, J. Besnard, S. Muresan and A. L. Hopkins, Quantifying the chemical beauty of drugs, Nat. Chem., 2012, 4, 90-98.</p>
<p>Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. P Ertl, A Schuffenhauer, J. Cheminf. P. Ertl and A. Schuffenhauer, Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions, J. Cheminf., 2009, 1, 8.</p>
<p>Dataset Bias in the Natural Sciences: A Case Study in Chemical Reaction Prediction and Synthesis Design. R.-R Griffiths, P Schwaller, A Lee, ChemRxiv. R.-R. Griffiths, P. Schwaller and A. Lee, Dataset Bias in the Natural Sciences: A Case Study in Chemical Reaction Prediction and Synthesis Design, ChemRxiv, 2018.</p>
<p>Experimental and computational approaches to estimate solubility and permeability in drug discovery and development settings. C A Lipinski, F Lombardo, B W Dominy, P J Feeney, Adv. Drug Delivery Rev. 23C. A. Lipinski, F. Lombardo, B. W. Dominy and P. J. Feeney, Experimental and computational approaches to estimate solubility and permeability in drug discovery and development settings, Adv. Drug Delivery Rev., 1997, 23, 3- 25.</p>
<p>Bayesian optimization with unknown constraints. M A Gelbart, J Snoek, R P Adams, Proceedings of the Thirtieth Conference on Uncertainty in Articial Intelligence. the Thirtieth Conference on Uncertainty in Articial IntelligenceM. A. Gelbart, J. Snoek and R. P. Adams, Bayesian optimization with unknown constraints, Proceedings of the Thirtieth Conference on Uncertainty in Articial Intelligence, 2014, pp. 250-259.</p>
<p>Global versus local search in constrained optimization of computer models. M Schonlau, W J Welch, D R Jones, Lecture Notes -Monograph Series. M. Schonlau, W. J. Welch and D. R. Jones, Global versus local search in constrained optimization of computer models, Lecture Notes -Monograph Series, 1998, pp. 11-25.</p>
<p>Constrained Bayesian Optimization and Applications. M A Gelbart, Harvard UniversityPhD thesisM. A. Gelbart, Constrained Bayesian Optimization and Applications, PhD thesis, Harvard University, 2015.</p>
<p>D Janz, J Van Der Westhuizen, B Paige, M Kusner, J M H Lobato, Learning a Generative Model for Validity in Complex Discrete Structures, International Conference on Learning Representations. D. Janz, J. van der Westhuizen, B. Paige, M. Kusner and J. M. H. Lobato, Learning a Generative Model for Validity in Complex Discrete Structures, International Conference on Learning Representations, 2018.</p>
<p>Generating focused molecule libraries for drug discovery with recurrent neural networks. M H Segler, T Kogej, C Tyrchan, M P Waller, ACS Cent. Sci. M. H. Segler, T. Kogej, C. Tyrchan and M. P. Waller, Generating focused molecule libraries for drug discovery with recurrent neural networks, ACS Cent. Sci., 2017.</p>
<p>Application of generative autoencoder in de novo molecular design. T Blaschke, M Olivecrona, O Engkvist, J Bajorath, H Chen, Mol. T. Blaschke, M. Olivecrona, O. Engkvist, J. Bajorath and H. Chen, Application of generative autoencoder in de novo molecular design, Mol. Inf., 2017.</p>
<p>Shape-Based Generative Modeling for de Novo Drug Design. M Skalic, J Jiménez, D Sabbadin, G De Fabritiis, J. Chem. Inf. Model. 59M. Skalic, J. Jiménez, D. Sabbadin and G. De Fabritiis, Shape-Based Generative Modeling for de Novo Drug Design, J. Chem. Inf. Model., 2019, 59, 1205-1214.</p>
<p>P Ertl, R Lewis, E J Martin, V Polyakov, arXiv:1712.07449silico generation of novel, drug-like chemical matter using the LSTM neural network. arXiv preprintP. Ertl, R. Lewis, E. J. Martin and V. Polyakov, In silico generation of novel, drug-like chemical matter using the LSTM neural network, arXiv preprint arXiv:1712.07449, Dec 20, 2017.</p>
<p>Molecular generative model based on conditional variational autoencoder for de novo molecular design. J Lim, S Ryu, J W Kim, W Y Kim, J. Cheminf. 31J. Lim, S. Ryu, J. W. Kim and W. Y. Kim, Molecular generative model based on conditional variational autoencoder for de novo molecular design, J. Cheminf., 2018, 10, 31.</p>
<p>Conditional molecular design with deep generative models. S Kang, K Cho, J. Chem. Inf. Model. 59S. Kang and K. Cho, Conditional molecular design with deep generative models, J. Chem. Inf. Model., 2018, 59, 43- 52.</p>
<p>De Novo Molecular Design by Combining Deep Autoencoder Recurrent Neural Networks with Generative Topographic Mapping. B Sattarov, I I Baskin, D Horvath, G Marcou, E J Bjerrum, A Varnek, J. Chem. Inf. Model. 59B. Sattarov, I. I. Baskin, D. Horvath, G. Marcou, E. J. Bjerrum and A. Varnek, De Novo Molecular Design by Combining Deep Autoencoder Recurrent Neural Networks with Generative Topographic Mapping, J. Chem. Inf. Model., 2019, 59, 1182-1196.</p>
<p>Generative recurrent networks for de novo drug design. A Gupta, A T Müller, B J Huisman, J A Fuchs, P Schneider, G Schneider, Mol. Inf. 1700111A. Gupta, A. T. Müller, B. J. Huisman, J. A. Fuchs, P. Schneider and G. Schneider, Generative recurrent networks for de novo drug design, Mol. Inf., 2018, 37, 1700111.</p>
<p>Prototype-based compound discovery using deep generative models. S Harel, K Radinsky, Mol. Pharm. 15S. Harel and K. Radinsky, Prototype-based compound discovery using deep generative models, Mol. Pharm., 2018, 15, 4406-4416.</p>
<p>Population-based de novo molecule generation, using grammatical evolution. N Yoshikawa, K Terayama, M Sumita, T Homma, K Oono, K Tsuda, Chem. Lett. 47N. Yoshikawa, K. Terayama, M. Sumita, T. Homma, K. Oono and K. Tsuda, Population-based de novo molecule generation, using grammatical evolution, Chem. Lett., 2018, 47, 1431-1434.</p>
<p>Improving Chemical Autoencoder Latent Space and Molecular De Novo Generation Diversity with Heteroencoders. E Bjerrum, B Sattarov, Biomolecules. 8131E. Bjerrum and B. Sattarov, Improving Chemical Autoencoder Latent Space and Molecular De Novo Generation Diversity with Heteroencoders, Biomolecules, 2018, 8, 131.</p>
<p>Penalized Variational Autoencoder for Molecular Design. S Mohammadi, B O&apos;dowd, C Paulitz-Erdmann, L Görlitz, ChemRxiv. S. Mohammadi, B. O'Dowd, C. Paulitz-Erdmann and L. Görlitz, Penalized Variational Autoencoder for Molecular Design, ChemRxiv, 2019.</p>
<p>GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders. M Simonovsky, N Komodakis, Articial Neural Networks and Machine Learning. M. Simonovsky and N. Komodakis, GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders, Articial Neural Networks and Machine Learning, 2018, pp. 412-422.</p>
<p>Y Li, O Vinyals, C Dyer, R Pascanu, P Battaglia, arXiv:1803.03324Learning deep generative models of graphs. arXiv preprintY. Li, O. Vinyals, C. Dyer, R. Pascanu and P. Battaglia, Learning deep generative models of graphs, arXiv preprint arXiv:1803.03324, 2018.</p>
<p>Junction Tree Variational Autoencoder for Molecular Graph Generation. W Jin, R Barzilay, T Jaakkola, International Conference on Machine Learning. W. Jin, R. Barzilay and T. Jaakkola, Junction Tree Variational Autoencoder for Molecular Graph Generation, International Conference on Machine Learning, 2018, pp 2328-2337.</p>
<p>N De Cao, T Kipf, arXiv:1805.11973MolGAN: An implicit generative model for small molecular graphs. arXiv preprintN. De Cao and T. Kipf, MolGAN: An implicit generative model for small molecular graphs, arXiv preprint arXiv:1805.11973, 2018.</p>
<p>M J Kusner, B Paige, J M Hernández-Lobato, Grammar Variational Autoencoder, International Conference on Machine Learning. M. J. Kusner, B. Paige and J. M. Hernández-Lobato, Grammar Variational Autoencoder, International Conference on Machine Learning, 2017, pp. 1945-1954.</p>
<p>H Dai, Y Tian, B Dai, S Skiena, L Song, Syntax-Directed Variational Autoencoder for Structured Data, International Conference on Learning Representations. H. Dai, Y. Tian, B. Dai, S. Skiena and L. Song, Syntax- Directed Variational Autoencoder for Structured Data, International Conference on Learning Representations, 2018.</p>
<p>Nevae: A deep generative model for molecular graphs. B Samanta, D Abir, G Jana, P K Chattaraj, N Ganguly, M G Rodriguez, Proceedings of the AAAI Conference on Articial Intelligence. the AAAI Conference on Articial IntelligenceB. Samanta, D. Abir, G. Jana, P. K. Chattaraj, N. Ganguly and M. G. Rodriguez, Nevae: A deep generative model for molecular graphs, Proceedings of the AAAI Conference on Articial Intelligence, 2019, pp. 1110-1117.</p>
<p>Multi-objective de novo drug design with conditional graph generative model. Y Li, L Zhang, Z Liu, J. Cheminf. 33Y. Li, L. Zhang and Z. Liu, Multi-objective de novo drug design with conditional graph generative model, J. Cheminf., 2018, 10, 33.</p>
<p>Molecular Hypergraph Grammar with Its Application to Molecular Optimization. H Kajino, International Conference on Machine Learning. H. Kajino, Molecular Hypergraph Grammar with Its Application to Molecular Optimization, International Conference on Machine Learning, 2019, pp. 3183-3191.</p>
<p>Learning Multimodal Graph-to-Graph Translation for Molecule Optimization. W Jin, K Yang, R Barzilay, T Jaakkola, International Conference on Learning Representations. W. Jin, K. Yang, R. Barzilay and T. Jaakkola, Learning Multimodal Graph-to-Graph Translation for Molecule Optimization, International Conference on Learning Representations, 2019.</p>
<p>X Bresson, T Laurent, arXiv, abs/1906.03412A Two-Step Graph Convolutional Decoder for Molecule Generation. X. Bresson and T. Laurent, A Two-Step Graph Convolutional Decoder for Molecule Generation, arXiv, abs/1906.03412, 2019.</p>
<p>J Lim, S.-Y Hwang, S Kim, S Moon, W Y Kim, arXiv:1905.13639Scaffold-based molecular design using graph generative model. arXiv preprintJ. Lim, S.-Y. Hwang, S. Kim, S. Moon and W. Y. Kim, Scaffold-based molecular design using graph generative model, arXiv preprint arXiv:1905.13639, 2019.</p>
<p>Likelihood-Free Inference and Generation of Molecular Graphs. S Pölsterl, C Wachinger, arXiv:1905.10310arXiv preprintS. Pölsterl and C. Wachinger, Likelihood-Free Inference and Generation of Molecular Graphs, arXiv preprint arXiv:1905.10310, 2019.</p>
<p>SELFIES: a robust representation of semantically constrained graphs with an example application in chemistry. M Krenn, F Häse, A Nigam, P Friederich, A Aspuru-Guzik, arXiv:1905.13741arXiv preprintM. Krenn, F. Häse, A. Nigam, P. Friederich and A. Aspuru- Guzik, SELFIES: a robust representation of semantically constrained graphs with an example application in chemistry, arXiv preprint arXiv:1905.13741, 2019.</p>
<p>Ł Maziarka, A Pocha, J Kaczmarczyk, K Rataj, M Warchoł, arXiv:1902.02119Mol-CycleGAN-a generative model for molecular optimization. arXiv preprintŁ. Maziarka, A. Pocha, J. Kaczmarczyk, K. Rataj and M. Warchoł, Mol-CycleGAN-a generative model for molecular optimization, arXiv preprint arXiv:1902.02119, 2019.</p>
<p>K Madhawa, K Ishiguro, K Nakago, M Abe, arXiv:1905.11600GraphNVP: An Invertible Flow Model for Generating Molecular Graphs. arXiv preprintK. Madhawa, K. Ishiguro, K. Nakago and M. Abe, GraphNVP: An Invertible Flow Model for Generating Molecular Graphs, arXiv preprint arXiv:1905.11600, 2019.</p>
<p>Automatic Chemical Design with Molecular Graph Variational Autoencoders. R D Shen, University of CambridgeM.Sc. thesisR. D. Shen, Automatic Chemical Design with Molecular Graph Variational Autoencoders, M.Sc. thesis, University of Cambridge, 2018.</p>
<p>K Korovina, S Xu, K Kandasamy, W Neiswanger, B Poczos, J Schneider, E P Xing, arXiv:1908.01425ChemBO: Bayesian Optimization of Small Organic Molecules with Synthesizable Recommendations, arXiv e-prints. K. Korovina, S. Xu, K. Kandasamy, W. Neiswanger, B. Poczos, J. Schneider and E. P. Xing, ChemBO: Bayesian Optimization of Small Organic Molecules with Synthesizable Recommendations, arXiv e-prints, arXiv:1908.01425, 2019.</p>
<p>Objective-reinforced generative adversarial networks (ORGAN) for sequence generation models. G L Guimaraes, B Sanchez-Lengeling, P L C Farias, A Aspuru-Guzik, arXiv:1705.10843arXiv preprintG. L. Guimaraes, B. Sanchez-Lengeling, P. L. C. Farias and A. Aspuru-Guzik, Objective-reinforced generative adversarial networks (ORGAN) for sequence generation models, arXiv preprint arXiv:1705.10843, May 30, 2017.</p>
<p>Optimization of molecules via deep reinforcement learning. Z Zhou, S Kearnes, L Li, R N Zare, P Riley, Sci. Rep. 910752Z. Zhou, S. Kearnes, L. Li, R. N. Zare and P. Riley, Optimization of molecules via deep reinforcement learning, Sci. Rep., 2019, 9, 10752.</p>
<p>Adversarial threshold neural computer for molecular de novo design. E Putin, A Asadulaev, Q Vanhaelen, Y Ivanenkov, A V Aladinskaya, A Aliper, A Zhavoronkov, Mol. Pharm. 15E. Putin, A. Asadulaev, Q. Vanhaelen, Y. Ivanenkov, A. V. Aladinskaya, A. Aliper and A. Zhavoronkov, Adversarial threshold neural computer for molecular de novo design, Mol. Pharm., 2018, 15, 4386-4397.</p>
<p>Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation. J You, B Liu, Z Ying, V Pande, J Leskovec, Advances in Neural Information Processing Systems. 31J. You, B. Liu, Z. Ying, V. Pande and J. Leskovec, Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation, Advances in Neural Information Processing Systems, 2018, vol. 31, pp 6410-6421.</p>
<p>Reinforced adversarial neural computer for de novo molecular design. E Putin, A Asadulaev, Y Ivanenkov, V Aladinskiy, B Sanchez-Lengeling, A Aspuru-Guzik, A Zhavoronkov, J. Chem. Inf. Model. 58E. Putin, A. Asadulaev, Y. Ivanenkov, V. Aladinskiy, B. Sanchez-Lengeling, A. Aspuru-Guzik and A. Zhavoronkov, Reinforced adversarial neural computer for de novo molecular design, J. Chem. Inf. Model., 2018, 58, 1194-1204.</p>
<p>ChemTS: an efficient python library for de novo molecular generation. X Yang, J Zhang, K Yoshizoe, K Terayama, K Tsuda, Sci. Technol. Adv. Mater. 18X. Yang, J. Zhang, K. Yoshizoe, K. Terayama and K. Tsuda, ChemTS: an efficient python library for de novo molecular generation, Sci. Technol. Adv. Mater., 2017, 18, 972-976.</p>
<p>H Wei, M Olarte, G B Goh, Multiple-objective Reinforcement Learning for Inverse Design and Identication. H. Wei, M. Olarte and G. B. Goh, Multiple-objective Reinforcement Learning for Inverse Design and Identication, 2019.</p>
<p>Deep reinforcement learning for multiparameter optimization in de novo drug design. N Ståhl, G Falkman, A Karlsson, G Mathiason, J Boström, J. Chem. Inf. Model. N. Ståhl, G. Falkman, A. Karlsson, G. Mathiason and J. Boström, Deep reinforcement learning for multiparameter optimization in de novo drug design, J. Chem. Inf. Model., 2019.</p>
<p>E Kraev, arXiv:1811.11222Grammars and reinforcement learning for molecule optimization. arXiv preprintE. Kraev, Grammars and reinforcement learning for molecule optimization, arXiv preprint arXiv:1811.11222, 2018.</p>
<p>Molecular de-novo design through deep reinforcement learning. M Olivecrona, T Blaschke, O Engkvist, H Chen, J. Cheminf. 48M. Olivecrona, T. Blaschke, O. Engkvist and H. Chen, Molecular de-novo design through deep reinforcement learning, J. Cheminf., 2017, 9, 48.</p>
<p>M Popova, M Shvets, J Oliva, O Isayev, arXiv:1905.13372MolecularRNN: Generating realistic molecular graphs with optimized properties. arXiv preprintM. Popova, M. Shvets, J. Oliva and O. Isayev, MolecularRNN: Generating realistic molecular graphs with optimized properties, arXiv preprint arXiv:1905.13372, 2019.</p>
<p>A de novo molecular generation method using latent vector based generative adversarial network. O Prykhodko, S Johansson, P.-C Kotsias, E J Bjerrum, O Engkvist, H Chen, ChemRxivO. Prykhodko, S. Johansson, P.-C. Kotsias, E. J. Bjerrum, O. Engkvist and H. Chen, A de novo molecular generation method using latent vector based generative adversarial network, ChemRxiv, 2019.</p>
<p>Advances and challenges in deep generative models for de novo molecule generation. D Xue, Y Gong, Z Yang, G Chuai, S Qu, A Shen, J Yu, Q Liu, Wiley Interdiscip. Rev.: Comput. Mol. Sci. 1395D. Xue, Y. Gong, Z. Yang, G. Chuai, S. Qu, A. Shen, J. Yu and Q. Liu, Advances and challenges in deep generative models for de novo molecule generation, Wiley Interdiscip. Rev.: Comput. Mol. Sci., 2019, 9, e1395.</p>
<p>Deep learning for molecular design -a review of the state of the art. D C Elton, Z Boukouvalas, M D Fuge, P W Chung, Mol. Syst. Des. Eng. 4D. C. Elton, Z. Boukouvalas, M. D. Fuge and P. W. Chung, Deep learning for molecular design -a review of the state of the art, Mol. Syst. Des. Eng., 2019, 4, 828-849.</p>
<p>D Schwalbe-Koda, R Gómez-Bombarelli, arXiv:1907.01632Generative Models for Automatic Chemical Design. arXiv preprintD. Schwalbe-Koda and R. Gómez-Bombarelli, Generative Models for Automatic Chemical Design, arXiv preprint arXiv:1907.01632, 2019.</p>
<p>D T Chang, arXiv:1902.05148Probabilistic Generative Deep Learning for Molecular Design. arXiv preprintD. T. Chang, Probabilistic Generative Deep Learning for Molecular Design, arXiv preprint arXiv:1902.05148, 2019.</p>
<p>Inverse molecular design using machine learning: Generative models for matter engineering. B Sanchez-Lengeling, A Aspuru-Guzik, Science. 361B. Sanchez-Lengeling and A. Aspuru-Guzik, Inverse molecular design using machine learning: Generative models for matter engineering, Science, 2018, 361, 360-365.</p>
<p>Learning structured output representation using deep conditional generative models. K Sohn, H Lee, X Yan, Advances in neural information processing systems. K. Sohn, H. Lee and X. Yan, Learning structured output representation using deep conditional generative models, Advances in neural information processing systems, 2015, pp 3483-3491.</p>
<p>Entangled Conditional Adversarial Autoencoder for de-novo Drug Discovery. D Polykovskiy, A Zhebrak, D Vetrov, Y Ivanenkov, V Aladinskiy, M Bozdaganyan, P Mamoshina, A Aliper, A Zhavoronkov, A Kadurin, Mol. Pharm. D. Polykovskiy, A. Zhebrak, D. Vetrov, Y. Ivanenkov, V. Aladinskiy, M. Bozdaganyan, P. Mamoshina, A. Aliper, A. Zhavoronkov and A. Kadurin, Entangled Conditional Adversarial Autoencoder for de-novo Drug Discovery, Mol. Pharm., 2018.</p>
<p>Black-Box Alpha Divergence Minimization. J M Hernández-Lobato, Y Li, M Rowland, T Bui, D Hernández-Lobato, R E Turner, Proceedings of The 33rd International Conference on Machine Learning. The 33rd International Conference on Machine LearningNew York, New York, USAJ. M. Hernández-Lobato, Y. Li, M. Rowland, T. Bui, D. Hernández-Lobato and R. E. Turner, Black-Box Alpha Divergence Minimization, Proceedings of The 33rd International Conference on Machine Learning, New York, New York, USA, 2016, pp 1511-1520.</p>
<p>A Gaulton, L J Bellis, A P Bento, J Chambers, M Davies, A Hersey, Y Light, S Mcglinchey, D Michalovich, B Al-Lazikani, ChEMBL: a large-scale bioactivity database for drug discovery. 40A. Gaulton, L. J. Bellis, A. P. Bento, J. Chambers, M. Davies, A. Hersey, Y. Light, S. McGlinchey, D. Michalovich and B. Al-Lazikani, ChEMBL: a large-scale bioactivity database for drug discovery, Nucleic Acids Res., 2011, 40, D1100- D1107.</p>
<p>A multi-points criterion for deterministic parallel global optimization based on Gaussian processes. D Ginsbourger, R Le Riche, L Carraro, preprint hal-00260579D. Ginsbourger, R. Le Riche and L. Carraro, A multi-points criterion for deterministic parallel global optimization based on Gaussian processes, HAL preprint hal-00260579, 2008.</p>
<p>ZINC: a free tool to discover chemistry for biology. J J Irwin, T Sterling, M M Mysinger, E S Bolstad, R G Coleman, J. Chem. Inf. Model. 52J. J. Irwin, T. Sterling, M. M. Mysinger, E. S. Bolstad and R. G. Coleman, ZINC: a free tool to discover chemistry for biology, J. Chem. Inf. Model., 2012, 52, 1757-1768.</p>
<p>New substructure lters for removal of pan assay interference compounds (PAINS) from screening libraries and for their exclusion in bioassays. J B Baell, G A Holloway, J. Med. Chem. 53J. B. Baell and G. A. Holloway, New substructure lters for removal of pan assay interference compounds (PAINS) from screening libraries and for their exclusion in bioassays, J. Med. Chem., 2010, 53, 2719-2740.</p>
<p>Lessons learnt from assembling screening libraries for drug discovery for neglected diseases. R Brenk, A Schipani, D James, A Krasowski, I H Gilbert, J Frearson, P G Wyatt, ChemMedChem. 3R. Brenk, A. Schipani, D. James, A. Krasowski, I. H. Gilbert, J. Frearson and P. G. Wyatt, Lessons learnt from assembling screening libraries for drug discovery for neglected diseases, ChemMedChem, 2008, 3, 435-444.</p>
<p>An empirical process for the design of highthroughput screening deck lters. B C Pearce, M J Soa, A C Good, D M Drexler, D A Stock, J. Chem. Inf. Model. 46B. C. Pearce, M. J. Soa, A. C. Good, D. M. Drexler and D. A. Stock, An empirical process for the design of high- throughput screening deck lters, J. Chem. Inf. Model., 2006, 46, 1060-1068.</p>
<p>Identication and evaluation of molecular properties related to preclinical optimization and clinical fate. J F Blake, Med. Chem. 1J. F. Blake, Identication and evaluation of molecular properties related to preclinical optimization and clinical fate, Med. Chem., 2005, 1, 649-655.</p>
<p>Strategic pooling of compounds for highthroughput screening. M Hann, B Hudson, X Lewell, R Lifely, L Miller, N Ramsden, J. Chem. Inf. Comput. Sci. 39M. Hann, B. Hudson, X. Lewell, R. Lifely, L. Miller and N. Ramsden, Strategic pooling of compounds for high- throughput screening, J. Chem. Inf. Comput. Sci., 1999, 39, 897-902.</p>
<p>Extended-connectivity ngerprints. D Rogers, M Hahn, J. Chem. Inf. Model. 50D. Rogers and M. Hahn, Extended-connectivity ngerprints, J. Chem. Inf. Model., 2010, 50, 742-754.</p>
<p>RDKit: open-source cheminformatics soware. G Landrum, G. Landrum, RDKit: open-source cheminformatics soware, 2016.</p>
<p>D Kingma, J Ba, arXiv:1412.6980Adam: A method for stochastic optimization. arXiv preprintD. Kingma and J. Ba, Adam: A method for stochastic optimization, arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Convolutional Networks on Graphs for Learning Molecular Fingerprints. D Duvenaud, D Maclaurin, J Aguilera-Iparraguirre, R Gómez-Bombarelli, T Hirzel, A Aspuru-Guzik, R P Adams, Proceedings of the 28th International Conference on Neural Information Processing Systems. the 28th International Conference on Neural Information Processing SystemsD. Duvenaud, D. Maclaurin, J. Aguilera-Iparraguirre, R. Gómez-Bombarelli, T. Hirzel, A. Aspuru-Guzik and R. P. Adams, Convolutional Networks on Graphs for Learning Molecular Fingerprints, Proceedings of the 28th International Conference on Neural Information Processing Systems, 2015, pp 2224-2232.</p>
<p>B Ramsundar, S M Kearnes, P Riley, D Webster, D E Konerding, V S Pande, arXiv:1502.02072Massively multitask networks for drug discovery. arXiv preprintB. Ramsundar, S. M. Kearnes, P. Riley, D. Webster, D. E. Konerding and V. S. Pande, Massively multitask networks for drug discovery, arXiv preprint arXiv:1502.02072, Feb 6, 2015.</p>
<p>Bayesian optimization for probabilistic programs. T Rainforth, T A Le, J.-W Van De Meent, M A Osborne, F Wood, Advances in Neural Information Processing Systems. T. Rainforth, T. A. Le, J.-W. van de Meent, M. A. Osborne and F. Wood, Bayesian optimization for probabilistic programs, Advances in Neural Information Processing Systems, 2016, pp 280-288.</p>
<p>O Mahmood, J M Hernández-Lobato, arXiv:1905.09885A COLD Approach to Generating Optimal Samples. arXiv preprintO. Mahmood and J. M. Hernández-Lobato, A COLD Approach to Generating Optimal Samples, arXiv preprint arXiv:1905.09885, 2019.</p>
<p>R Astudillo, P Frazier, Bayesian Optimization of Composite Functions, International Conference on Machine Learning. R. Astudillo and P. Frazier, Bayesian Optimization of Composite Functions, International Conference on Machine Learning, 2019, pp 354-363.</p>
<p>F Hase, L M Roch, C Kreisbeck, A Aspuru-Guzik, Phoenics: A Bayesian optimizer for chemistry. 4F. Hase, L. M. Roch, C. Kreisbeck and A. Aspuru-Guzik, Phoenics: A Bayesian optimizer for chemistry, ACS Cent. Sci., 2018, 4, 1134-1145.</p>
<p>R Moriconi, K Kumar, M P Deisenroth, arXiv:1902.10675High-Dimensional Bayesian Optimization with Manifold Gaussian Processes. arXiv preprintR. Moriconi, K. Kumar and M. P. Deisenroth, High- Dimensional Bayesian Optimization with Manifold Gaussian Processes, arXiv preprint arXiv:1902.10675, 2019.</p>
<p>Model-based methods for continuous and discrete global optimization. T Bartz-Beielstein, M Zaefferer, Appl. So Comput. 55T. Bartz-Beielstein and M. Zaefferer, Model-based methods for continuous and discrete global optimization, Appl. So Comput., 2017, 55, 154-167.</p>
<p>Achieving Robustness to Aleatoric Uncertainty with Heteroscedastic Bayesian Optimisation. R.-R Griffiths, M Garcia-Ortegon, A A Aldrick, A A Lee, arXiv:1910.07779arXiv preprintR.-R. Griffiths, M. Garcia-Ortegon, A. A. Aldrick and A. A. Lee, Achieving Robustness to Aleatoric Uncertainty with Heteroscedastic Bayesian Optimisation, arXiv preprint arXiv:1910.07779, 2019.</p>
<p>Accelerating the discovery of materials for clean energy in the era of smart automation. D P Tabor, L M Roch, S K Saikin, C Kreisbeck, D Sheberla, J H Montoya, S Dwaraknath, M Aykol, C Ortiz, H Tribukait, Nat. Rev. Mater. 3D. P. Tabor, L. M. Roch, S. K. Saikin, C. Kreisbeck, D. Sheberla, J. H. Montoya, S. Dwaraknath, M. Aykol, C. Ortiz, H. Tribukait, et al., Accelerating the discovery of materials for clean energy in the era of smart automation, Nat. Rev. Mater., 2018, 3.</p>
<p>T Aumentado-Armstrong, arXiv:1809.02032Latent Molecular Optimization for Targeted Therapeutic Design. arXiv preprintT. Aumentado-Armstrong, Latent Molecular Optimization for Targeted Therapeutic Design, arXiv preprint arXiv:1809.02032, 2018.</p>
<p>Guacamol: benchmarking models for de novo molecular design. N Brown, M Fiscato, M H Segler, A C Vaucher, J. Chem. Inf. Model. 59N. Brown, M. Fiscato, M. H. Segler and A. C. Vaucher, Guacamol: benchmarking models for de novo molecular design, J. Chem. Inf. Model., 2019, 59, 1096-1108.</p>
<p>D Polykovskiy, A Zhebrak, B Sanchez-Lengeling, S Golovanov, O Tatanov, S Belyaev, R Kurbanov, A Artamonov, V Aladinskiy, M Veselov, arXiv, abs/1811.12823Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models. D. Polykovskiy, A. Zhebrak, B. Sanchez-Lengeling, S. Golovanov, O. Tatanov, S. Belyaev, R. Kurbanov, A. Artamonov, V. Aladinskiy and M. Veselov, et al., Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models, arXiv, abs/1811.12823, 2018.</p>            </div>
        </div>

    </div>
</body>
</html>
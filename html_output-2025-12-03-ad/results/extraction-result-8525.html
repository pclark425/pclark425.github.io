<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8525 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8525</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8525</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-320b227027030fc291de2896fc3c6da49d7614be</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/320b227027030fc291de2896fc3c6da49d7614be" target="_blank">Solving Rubik's Cube with a Robot Hand</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is demonstrated that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot, made possible by a novel algorithm, which is called automatic domain randomization (ADR), and a robot platform built for machine learning.</p>
                <p><strong>Paper Abstract:</strong> We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key components: a novel algorithm, which we call automatic domain randomization (ADR) and a robot platform built for machine learning. ADR automatically generates a distribution over randomized environments of ever-increasing difficulty. Control policies and vision state estimators trained with ADR exhibit vastly improved sim2real transfer. For control policies, memory-augmented models trained on an ADR-generated distribution of environments show clear signs of emergent meta-learning at test time. The combination of ADR with our custom robot platform allows us to solve a Rubik's cube with a humanoid robot hand, which involves both control and state estimation problems. Videos summarizing our results are available: this https URL</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8525",
    "paper_id": "paper-320b227027030fc291de2896fc3c6da49d7614be",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00467475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Solving Rubik's Cube with a Robot Hand</h1>
<p>A Preprint</p>
<p>OpenAI<br>Ilge Akkaya,<em>, Marcin Andrychowicz,</em>,Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron,<br>Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak,<br>Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, Lei Zhang*</p>
<p>October 17, 2019
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A five-fingered humanoid hand trained with reinforcement learning and automatic domain randomization solving a Rubik's cube.</p>
<h4>Abstract</h4>
<p>We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key components: a novel algorithm, which we call automatic domain randomization (ADR) and a robot platform built for machine learning. ADR automatically generates a distribution over randomized environments of ever-increasing difficulty. Control policies and vision state estimators trained with ADR exhibit vastly improved sim2real transfer. For control policies, memory-augmented models trained on an ADR-generated distribution of environments show clear signs of emergent meta-learning at test time. The combination of ADR with our custom robot platform allows us to solve a Rubik's cube with a humanoid robot hand, which involves both control and state estimation problems. Videos summarizing our results are available: https://openai.com/blog/solving-rubiks-cube/</p>
<h2>1 Introduction</h2>
<p>Building robots that are as versatile as humans remains a grand challenge of robotics. While humanoid robotics systems exist $[28,99,110,95,5]$, using them in the real world for complex tasks remains a daunting challenge. Machine learning</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Train in Simulation
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Transfer to the Real World
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 2: System Overview. (a) We use automatic domain randomization (ADR) to generate a growing distribution of simulations with randomized parameters and appearances. We use this data for both the control policy and vision-based state estimator. (b) The control policy receives observed robot states and rewards from the randomized simulations and learns to solve them using a recurrent neural network and reinforcement learning. (c) The vision-based state estimator uses rendered scenes collected from the randomized simulations and learns to predict the pose as well as face angles of the Rubik's cube using a convolutional neural network (CNN), trained separately from the control policy. (d) To transfer to the real world, we predict the Rubik's cube's pose from 3 real camera feeds with the CNN and measure the robot fingertip locations using a 3D motion capture system. The face angles that describe the internal rotational state of the Rubik's cube are provided by either the same vision state estimator or the Giiker cube, a custom cube with embedded sensors and feed it into the policy network.</p>
<p>has the potential to change this by learning how to use sensor information to control the robot system appropriately instead of hand-programming the robot using expert knowledge.
However, learning requires vast amount of training data, which is hard and expensive to acquire on a physical system. Collecting all data in simulation is therefore appealing. However, the simulation does not capture the environment or the robot accurately in every detail and therefore we also need to solve the resulting sim2real transfer problem. Domain randomization techniques $[106,80]$ have shown great potential and have demonstrated that models trained only in simulation can transfer to the real robot system.
In prior work, we have demonstrated that we can perform complex in-hand manipulation of a block [77]. This time, we aim to solve the manipulation and state estimation problems required to solve a Rubik's cube with the Shadow Dexterous Hand [99] using only simulated data. This problem is much more difficult since it requires significantly more dexterity and precision for manipulating the Rubik's cube. The state estimation problem is also much harder as we need to know with high accuracy what the pose and internal state of the Rubik's cube are. We achieve this by introducing a novel method for automatically generating a distribution over randomized environments for training reinforcement learning policies and vision state estimators. We call this algorithm automatic domain randomization (ADR). We also built a robot platform for solving a Rubik's cube in the real world in a way that complements our machine learning approach. Figure 2 shows an overview of our system.
We investigate why policies trained with ADR transfer so well from simulation to the real robot. We find clear signs of emergent learning that happens at test time within the recurrent internal state of our policy. We believe that this is a direct result of us training on an ever-growing distribution over randomized environments with a memory-augmented policy. In other words, training an LSTM over an ADR distribution is implicit meta-learning. We also systematically study and quantify this observation in our work.
The remainder of this manuscript is structured as follows. Section 2 introduces two manipulation tasks we consider here. Section 3 describes our physical setup and Section 4 describes how our setup is modeled in simulation. We introduce a new algorithm called automatic domain randomization (ADR), in Section 5. In Section 6 and Section 7 we describe how we train control policies and vision state estimators, respectively. We present our key quantitative and qualitative results on the two tasks in Section 8. In Section 9 we systematically analyze our policy for signs of emergent meta-learning. Section 10 reviews related work and we conclude with Section 11.
If you are mostly interested in the machine learning aspects of this manuscript, Section 5, Section 6, Section 7, Section 8, and Section 9 are especially relevant. If you are interested in the robotics aspects, Section 3, Section 4, and Section 8.4 are especially relevant.</p>
<h1>2 Tasks</h1>
<p>In this work, we consider two different tasks that both use the Shadow Dexterous Hand [99]: the block reorientation task from our previous work [77, 84] and the task of solving a Rubik's cube. Both tasks are visualized in Figure 3. We briefly describe the details of each task in this section.</p>
<h3>2.1 Block Reorientation</h3>
<p>The block reorientation task was previously proposed in [84] and solved on a physical robot hand in [77]. We briefly review it here; please refer to the aforementioned citations for additional details.
The goal of the block reorientation task is to rotate a block into a desired goal orientation. For example, in Figure 3a, the desired orientation is shown next to the hand with the red face facing up, the blue face facing to the left and the green face facing forward. A goal is considered achieved if the block's rotation matches the goal rotation within 0.4 radians. After a goal is achieved, a new random goal is generated.</p>
<h3>2.2 Rubik's Cube</h3>
<p>We introduce a new and significantly more difficult problem in this work: solving a Rubik's cube ${ }^{2}$ with the same Shadow Dexterous Hand. In brief, a Rubik's cube is a puzzle with 6 internal degrees of freedom. It consists of 26 cubelets that are connected via a system of joints and springs. Each of the 6 faces of the cube can be rotated, allowing the Rubik's cube to be scrambled. A Rubik's cube is considered solved if all 6 faces have been returned to a single color each. Figure 3b depicts a Rubik's cube that is a single 90 degree rotation of the top face away from being solved.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: Visualization of the block reorientation task (left) and the Rubik's cube task (right). In both cases, we use a single Shadow Dexterous Hand to solve the task. We also depict the goal that the policy is asked to achieve in the upper left corner.</p>
<p>We consider two types of subgoals: A rotation corresponds to rotating a single face of the Rubik's cube by 90 degrees in the clockwise or counter-clockwise direction. A flip corresponds to moving a different face of the Rubik's cube to the top. We found rotating the top face to be far simpler than rotating other faces. Thus, instead of rotating arbitrary faces, we combine together a flip and a top face rotation in order to perform the desired operation. These subgoals can then be performed sequentially to eventually solve the Rubik's cube.
The difficulty of solving a Rubik's cube obviously depends on how much it has been scrambled before. We use the official scrambling method used by the World Cube Association ${ }^{3}$ to obtain what they refer to as a fair scramble. A fair scramble typically consists of around 20 moves that are applied to a solved Rubik's cube to scramble it.
When it comes to solving the Rubik's cube, computing a solution sequence can easily be done with existing software libraries like the Kociemba solver [111]. We use this solver to produce a solution sequence of subgoals for the hand to perform. In this work, the key problem is thus about sensing and control, not finding the solution sequence. More concretely, we need to obtain the state of the Rubik's cube (i.e. its pose as well as its 6 face angles) and use that information to control the robot hand such that each subgoal is successfully achieved.</p>
<h1>3 Physical Setup</h1>
<p>Having described the task, we next describe the physical setup that we use to solve the block and the Rubik's cube in the real world. We focus on the differences that made it possible to solve the Rubik's cube since [77] has already described our physical setup for solving the block reorientation task.</p>
<h3>3.1 Robot Platform</h3>
<p>Our robot platform is based on the configuration described in [77]. We still use the Shadow Dexterous E Series Hand (E3M5R) [99] as a humanoid robot hand and the PhaseSpace motion capture system to track the Cartesian coordinates of all five fingertips. We use the same 3 RGB Basler cameras for vision pose estimation.
However, a number of improvements have been made since our previous publication. Figure 4a depicts the latest iteration of our robot cage. The cage is now fully contained, i.e. all computers are housed within the system. The cage is also on coasters and can therefore be moved more easily. The larger dimensions of the new cage make calibration of</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: The latest version of our cage (left) that houses the Shadow Dexterous Hand, RGB cameras, and the PhaseSpace motion capture system. We made some modifications to the Shadow Dexterous Hand (right) to improve reliability for our setup by moving the PhaseSpace LEDs and cables inside the fingers and by adding rubber to the fingertips.
the PhaseSpace motion capture system easier and help prevent disturbing calibration when taking the hand in and out of the cage.
We have made a number of customizations to the E3M5R since our last publication (see also Figure 4b). We moved routing of the cables that connect the PhaseSpace LEDs on each fingertip to the PhaseSpace micro-driver within the hand, thus reducing the wear and tear on those cables. We worked with The Shadow Robot Company ${ }^{4}$ to improve the robustness and reliability of some components for which we noticed breakages over time. We also modified the distal part of the fingers to extend the rubber area to cover a larger span to increase the grip of the hand when it interacts with an object. We increased the diameter of the wrist flexion/extension pulley in order to reduce tendon stress which has extended the life of the tendon to more than three times its typical mean time before failure (MTBF). Finally, the tendon tensioners in the hand have been upgraded and this has improved the MTBF of the finger tendons by approximately five to ten times.</p>
<p>We also made improvements to our software stack that interfaces with the E3M5R. For example, we found that manual tuning of the maximum torque that each motor can exercise was superior to our automated methods in avoiding physical breakage and ensuring consistent policy performance. More concretely, torque limits were minimized such that the hand can reliably achieve a series of commanded positions.
We also invested in real-time system monitoring so that issues with the physical setup could be identified and resolved more quickly. We describe our monitoring system in greater detail in Appendix A.</p>
<h1>3.2 Giiker Cube</h1>
<p>Sensing the state of a Rubik's cube from vision only is a challenging task. We therefore use a "smart" Rubik's cube with built-in sensors and a Bluetooth module as a stepping stone: We used this cube while face angle predictions from vision were not yet ready in order to continue work on the control policy. We also used the Giiker cube for some of our experiments to test the control policy without compounding errors made by the vision model's face angle predictions (we always use the vision model for pose estimation).
Our hardware is based on the Xiaomi Giiker cube. ${ }^{5}$ This cube is equipped with a Bluetooth module and allows us to sense the state of the Rubik's cube. However, it only has a face angle resolution of $90^{\circ}$, which is not sufficient for state tracking purposes on the robot setup. We therefore replace some of the components of the original Giiker cube with custom ones in order to achieve a tracking accuracy of approximately $5^{\circ}$. Figure 5a shows the components of the unmodified Giiker cube and our custom replacements side by side, as well as the assembled modified Giiker cube. Since we only use our modified version, we henceforth refer to it as only "Giiker cube".</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" />
(a) The components of the Giiker cube.
<img alt="img-6.jpeg" src="img-6.jpeg" />
(b) An assembled Giiker cube while charging.</p>
<p>Figure 5: We use an off-the-shelf Giiker cube but modify its internals (subfigure a, right) to provider higher resolution for the 6 face angles. The components from left to right are (i) bottom center enclosure, (ii) lithium polymer battery, (iii) main PCBa with BLE, (iv) top center enclosure, (v) cubelet bottom, (vi) compression spring, (vii) contact brushes, (viii) absolute resistive rotary encoder, (ix) locking cap, (x) cubelet top. Once assembled, the Giiker cube can be charged with its "headphones on" (right).</p>
<h1>3.2.1 Design</h1>
<p>We have redesigned all parts of the Giiker cube but the exterior cubelet elements. The central support was redesigned to move the parting line off of the central line of symmetry to facilitate a more friendly development platform because the off-the-shelf design would have required de-soldering in order to program the microcontroller. The main Bluetooth and signal processing board is based on the NRF52 integrated circuit [73]. Six separately printed circuit boards (Figure 6 b ) were designed to improve the resolution from $90^{\circ}$ to $5^{\circ}$ using an absolute resistive encoder layout. The position is read with a linearizing circuit shown in Figure 6a. The linearized, analog signal is then read by an ADC pin on the microcontroller and sent as a face angle over the Bluetooth Low Energy (BLE) connection to the host.
The custom firmware implements a protocol that is based on the Nordic UART service (NUS) to emulate a serial port over BLE [73]. We then use a Node.js ${ }^{6}$ based client application to periodically request angle readings from the UART module and to send calibration requests to reset angle references when needed. Starting from a solved Rubik's cube, the client is able to track face rotations performed on the cube in real time and thus is able to reconstruct the Rubik's cube state given periodic angle readings.
<img alt="img-7.jpeg" src="img-7.jpeg" />
(a) The linearizing circuit used to read the position of the faces.
<img alt="img-8.jpeg" src="img-8.jpeg" />
(b) The absolute resistive encoders used to read the position of the faces.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3.2.2 Data Accuracy and Timing</h1>
<p>In order to ensure reliability of physical experiments, we performed regular accuracy tracking tests on integrated Giiker cubes. To assess accuracy, we considered all four right angle rotations as reference points on each cube face and estimated sensor accuracy based on measurements collected at each reference point. Across two custom cubes, the resistive encoders were subject to an absolute mean tracking error of $5.90^{\circ}$ and the standard deviation of reference point readings was $7.61^{\circ}$.
During our experiments, we used a 12.5 Hz update frequency ${ }^{7}$ for the angle readings, which was sufficient to provide low-latency observations to the robot policy.</p>
<h3>3.2.3 Calibration</h3>
<p>We perform a combination of firmware and software-side calibration of the sensors to ensure zero-positions can be dynamically set for each face angle sensor. On connecting to a cube for the first time, we record ADC offsets for each sensor in the firmware via a reset request. Furthermore, we add a software-side reset of the angle readings before starting each physical trial on the robot to ensure sensor errors do not accumulate across trials.
In order to track any physical degradation in the sensor accuracy of the fully custom hardware, we created a calibration procedure which instructs an operator to rotate each face a full $360^{\circ}$, stopping at each $90^{\circ}$ alignment of the cube. We then record the expected and actual angles to measure the accuracy over time.</p>
<h2>4 Simulation</h2>
<p>The simulation setup is similar to [77]: we simulate the physical system with the MuJoCo physics engine [108], and we use ORRB [16], a remote rendering backend built on top of Unity3D, ${ }^{8}$ to render synthetic images for training the vision based pose estimator.
While the simulation cannot perfectly match reality, we still found it beneficial to help bridge the gap by modeling our physical setup accurately. Our MuJoCo model of the Shadow Dexterous Hand has thus been further improved since [77] to better match the physical system via new dynamics calibration and modeling of a subset of tendons existing in the physical hand and we developed an accurate model of the Rubik's cube.</p>
<h3>4.1 Hand Dynamics Calibration</h3>
<p>We measured joint positions for the same time series of actions for the real and simulated hands in an environment where the hand can move freely and made two observations:</p>
<ol>
<li>The joint positions recorded on a physical robot and in simulation were visibly different (see Figure 8a).</li>
<li>The dynamics of coupled joints (i.e. distal two joints of non-thumb fingers, see [77, Appendix B.1]) were different on a physical robot and in simulation. In the original simulation used in [77], movement of coupled joints was modeled with two fixed tendons which resulted in both joints traveling roughly the same distance for each action. However, on the physical robot, movement of coupled joints depends on the current position of each joint. For instance, like in the human hand, the proximal segment of a finger bends before the distal segment when bending a finger.</li>
</ol>
<p>To address the dynamics of coupled joints, we added a non-actuated spatial tendon and pulleys to the simulated non-thumb fingers (see Figure 7), analogous to the non-actuated tendon present in the physical robot. Parameters relevant to the joint movement in the new MuJoCo model were then calibrated to minimize root mean square error between reference joint positions recorded on a physical robot and joint positions recorded in simulation for the same time series of actions. We observe that better modeling of coupling and dynamics calibration improves performance significantly and present full results in Section D.1. We use this version of the simulation throughout the rest of this work.</p>
<h3>4.2 Rubik's Cube</h3>
<p>Behind the apparent simplicity of a cube-like exterior, a Rubik's cube hides a high degree of internal complexity and surprisingly nontrivial interactions between elements. A regular $3 \times 3 \times 3$ cube consists of 26 externally facing cubelets</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 7: Transparent view of the hand in the new simulation. One spatial tendon (green lines) and two cylindrical geometries acting as pulleys (yellow cylinders) have been added for each non-thumb finger in order to achieve coupled joints dynamics similar to the physical robot.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 8: Comparison of positions of the LFJ3 joint on a real and simulated robot hand for the same control sequence, for the original simulation (a) and for the new simulation (b)
that are bound together to constitute a larger cubic shape. Six cubelets that reside in the center of each face are connected by axles to the inner core and can only rotate in place with one degree of freedom. In contrast to that, the edge and corner cubelets are not fixed and can move around the cube whenever the larger faces are rotated. To prevent the cube from falling apart, these cubelets have little plastic tabs that extend towards the core and allow each piece to be held in place by its neighbors, which in the end are retained by the center elements. Additionally, most Rubik's cubes are to a certain degree elastic and allow for small deformations from their original shape, constituting additional degrees of freedom.</p>
<p>The components of the cube constantly exert pressure on each other which results in a certain base level of friction in the system both between the cubelets and in the joints. It is enough to apply force to a single cubelet to rotate a face, as it will be propagated between the neighboring elements via contact forces. Although a cube has six faces that can be rotated, not all of them can be rotated simultaneously - whenever one face has already been moved by a certain</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 9: Our MuJoCo model of the Rubik’s cube. On the left, we show a rendered version. On the right, we show the individual cublets that make up our model and visualize the different axis and degrees of freedom of our model.</p>
<p>angle, perpendicular faces are in a locked state and prevented from moving. However, if this angle is small enough, the original face often "snaps" back into its nearest aligned state and in that way we can proceed with rotating the perpendicular face. This property is commonly called the "forgiveness" of a Rubik’s Cube and its strength varies greatly among models available on the market.</p>
<p>Since we train entirely in simulation and need to successfully transfer to the real world without ever experiencing it, we needed to create a model rich enough to include all of the aforementioned behaviors, while at the same time keeping software complexity and computational costs manageable. We used the MuJoCo [108] physics engine, which implements a stable and fast numerical solutions for simulating body dynamics with soft contacts.</p>
<p>Inspired by the physical cube, our simulated model consists of 26 rigid body convex cubelets. MuJoCo allows for these shapes to penetrate each other by a small margin when a force is applied. Six central cubelets have a single <em>hinge joint</em> representing a single rotational degree of freedom about the axes running through the center of the cube orthogonal to each face. All remaining 20 corner and edge cubelets have three hinge joints corresponding to full Euler angle representation, with rotation axes passing through the center of the cube. In that way, our cube has 6 × 1 + 20 × 3 = 66 degrees of freedom, that allow us to represent effectively not only <em>43 quintillion</em> fully aligned cube configurations but also all physically valid intermediate states.</p>
<p>Each cubelet mesh was created on the basis of the cube of size 1.9 cm. Our preliminary experiments have shown that with perfectly cubic shape, the overall Rubik’s cube model was highly unforgiving. Therefore, we beveled all the edges of the mesh 1.425 mm inwards, which gave satisfactory results. <sup id="fnref:9"><a class="footnote-ref" href="#fn:9">2</a></sup>. We do not implement any custom physics in our modelling, but rely on the cubelet shapes, contact forces and friction to drive the movement of the cube. We conducted experiments with spring joints which would correspond to additional degrees of freedom for cube deformation, but found they were not necessary and that native MuJoCo soft contacts already exhibit similar dynamics.</p>
<p>We performed a very rudimentary dynamics calibration of the parameters which MuJoCo allows us to specify, in order to roughly match a physical Rubik’s cube. Our goal was not to get an exact match, but rather to have a plausible model as a starting point for domain randomization.</p>
<h1>5 Automatic Domain Randomization</h1>
<p>In [77], we were able to train a control policy and a vision model in simulation and then transfer both to a real robot through the use of domain randomization [106, 80]. However, this required a significant amount of manual tuning and a tight iteration loop between randomization design in simulation and validation on a robot. In this section, we describe how automatic domain randomization (ADR) can be used to automate this process and how we apply ADR to both policy and vision training.
Our main hypothesis that motivates ADR is that training on a maximally diverse distribution over environments leads to transfer via emergent meta-learning. More concretely, if the model has some form of memory, it can learn to adjust its behavior during deployment to improve performance on the current environment over time, i.e. by implementing a learning algorithm internally. We hypothesize that this happens if the training distribution is so large that the model cannot memorize a special-purpose solution per environment due to its finite capacity. ADR is a first step in this direction of unbounded environmental complexity: it automates and gradually expands the randomization ranges that parameterize a distribution over environments. Related ideas were also discussed in [27, 11, 117, 20].
In the remainder of this section, we first describe how ADR works at a high level and then describe the algorithm and our implementation in greater detail.</p>
<h3>5.1 ADR Overview</h3>
<p>We use ADR both to train our vision models (supervised learning) and our policy (reinforcement learning). In each case, we generate a distribution over environments by randomizing certain aspects, e.g. the visual appearance of the cube or the dynamics of the robotic hand. While domain randomization requires us to define the ranges of this distribution manually and keep it fixed throughout model training, in ADR the distribution ranges are defined automatically and allowed to change.
A top-level diagram of ADR is given in Figure 10. We give an intuitive overview of ADR below. See Section 5.2 for a formal description of the algorithm.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 10: Overview of ADR. ADR controls the distribution over environments. We sample environments from this distribution and use it to generate training data, which is then used to optimize our model (either a policy or a vision state estimator). We further evaluate performance of our model on the current distribution and use this information to update the distribution over environments automatically.</p>
<p>At its core, ADR realizes a training curriculum that gradually expands a distribution over environments for which the model can perform well. The initial distribution over environments is concentrated on a single environment. For example, in policy training the initial environment is based on calibration values measured from the physical robot.
The distribution over environments is sampled to obtain environments used to generate training data and evaluate model performance. ADR is independent of the algorithm used for model training. It only generates training data. This allows us to use ADR for both policy and vision model training.
As training progresses and model performance improves sufficiently on the initial environment, the distribution is expanded. This expansion continues as long as model performance is considered acceptable. With a sufficiently powerful model architecture and training algorithm, the distribution is expected to expand far beyond manual domain randomization ranges since every improvement in the model's performance results in an increase in randomization.
ADR has two key benefits over manual domain randomization (DR):</p>
<ul>
<li>Using a curriculum that gradually increases difficulty as training progresses simplifies training, since the problem is first solved on a single environment and additional environments are only added when a minimum level of performance is achieved [35, 67].</li>
<li>It removes the need to manually tune the randomizations. This is critical, because as more randomization parameters are incorporated, manual adjustment becomes increasingly difficult and non-intuitive.</li>
</ul>
<p>Acceptable performance is defined by performance thresholds. For policy training, they are configured as the lower and upper bounds on the number of successes in an episode. For vision training, we first configure target performance thresholds for each output (e.g. position, orientation). During evaluation, we then compute the percentage of samples which achieve these targets for all outputs; if the resulting percentage is above the upper threshold or below the lower threshold, the distribution is adjusted accordingly.</p>
<h1>5.2 Algorithm</h1>
<p>Each environment $e_{\lambda}$ is parameterized by $\lambda \in \mathbb{R}^{d}$, where $d$ is the number of parameters we can randomize in simulation. In domain randomization (DR), the environment parameter $\lambda$ comes from a fixed distribution $P_{\phi}$ parameterized by $\phi \in \mathbb{R}^{d^{\prime}}$. However, in automatic domain randomization (ADR), $\phi$ is changing dynamically with training progress. The sampling process in Figure 10 works out as $\lambda \sim P_{\phi}$, resulting in one randomized environment instance $e_{\lambda}$.
To quantify the amount of ADR expansion, we define $A D R$ entropy as $\mathcal{H}\left(P_{\phi}\right)=-\frac{1}{d} \int P_{\phi}(\lambda) \log P_{\phi}(\lambda) d \lambda$ in units of nats/dimension. The higher the ADR entropy, the broader the randomization sampling distribution. The normalization allows us to compare between different environment parameterizations.
In this work, we use a factorized distribution parameterized by $d^{\prime}=2 d$ parameters. To simplify notation, let $\phi^{L}, \phi^{H} \in \mathbb{R}^{d}$ be a certain partition of $\phi$. For the $i$-th ADR parameter $\lambda_{i}, i=1, \ldots, d$, the pair $\left(\phi_{i}^{L}, \phi_{i}^{H}\right)$ is used to describe a uniform distribution for sampling $\lambda_{i}$ such that $\lambda_{i} \sim U\left(\phi_{i}^{L}, \phi_{i}^{H}\right)$. Note that the boundary values are inclusive. The overall distribution is given by</p>
<p>$$
P_{\phi}(\lambda)=\prod_{i=1}^{d} U\left(\phi_{i}^{L}, \phi_{i}^{H}\right)
$$</p>
<p>with ADR entropy</p>
<p>$$
\mathcal{H}\left(P_{\phi}\right)=\frac{1}{d} \sum_{i=1}^{d} \log \left(\phi_{i}^{H}-\phi_{i}^{L}\right)
$$</p>
<p>The ADR algorithm is listed in Algorithm 1. For the factorized distribution, Algorithm 1 is applied to $\phi^{L}$ and $\phi^{H}$ separately.
At each iteration, the ADR algorithm randomly selects a dimension of the environment $\lambda_{i}$ to fix to a boundary value $\phi_{i}^{L}$ or $\phi_{i}^{H}$ (we call this "boundary sampling"), while the other parameters are sampled as per $P_{\phi}$. Model performance for the sampled environment is then evaluated and appended to the buffer associated with the selected boundary of the selected parameter. Once enough performance data is collected, it is averaged and compared to thresholds. If the average model performance is better than the high threshold $t_{H}$, the parameter for the chosen dimension is increased. It is decreased if the average model performance is worse than the low threshold $t_{L}$.
As described, the ADR algorithm modifies $P_{\phi}$ by always fixing one environment parameter to a boundary value. To generate model training data, we use Algorithm 2 in conjunction with ADR. The algorithm samples $\lambda$ from $P_{\phi}$ and runs the model in the sampled environment to generate training data.
To combine ADR and training data generation, at every iteration we execute Algorithm 1 with probability $p_{b}$ and Algorithm 2 with probability $1-p_{b}$. We refer to $p_{b}$ as the boundary sampling probability.</p>
<h3>5.3 Distributed Implementation</h3>
<p>We used a distributed version of ADR in this work. The system architecture is illustrated in Figure 11 for both our policy and vision training setup. We describe policy training in greater detail in Section 6 and vision training in Section 7. Here we focus on ADR.
The highly-parallel and asynchronous implementation depends on several centralized storage of (policy or vision) model parameters $\Theta$, ADR parameters $\Phi$, training data $T$, and performance data buffers $\left{D_{i}\right}_{i=1}^{d}$. We use Redis to implement them.</p>
<h1>Algorithm 1 ADR</h1>
<p>Require: $\phi^{0}$
Require: $\left{D_{i}^{L}, D_{i}^{H}\right}<em L="L">{i=1}^{d}$
Require: $m, t</em>$
Require: $\Delta$
$\phi \leftarrow \phi^{0}$
repeat
$\lambda \sim P_{\phi}$
$i \sim U{1, \ldots, d}, x \sim U(0,1)$
if $x&lt;0.5$ then
$D_{i} \leftarrow D_{i}^{L}, \lambda_{i} \leftarrow \phi_{i}^{L}$
else
$D_{i} \leftarrow D_{i}^{H}, \lambda_{i} \leftarrow \phi_{i}^{H}$
end if
$p \leftarrow$ EVALUATEPERFORMANCE $(\lambda)$
$D_{i} \leftarrow D_{i} \cup{p}$
if $\operatorname{LENOTH}\left(D_{i}\right) \geq m$ then
$\bar{p} \leftarrow \operatorname{AVERAGE}\left(D_{i}\right)$
$\operatorname{CLEAR}\left(D_{i}\right)$
if $\bar{p} \geq t_{H}$ then
$\phi_{i} \leftarrow \phi_{i}+\Delta$
else if $\bar{p} \leq t_{L}$ then
$\phi_{i} \leftarrow \phi_{i}-\Delta$
end if
end if
until training is complete}, t_{H}$, where $t_{L}&lt;t_{H</p>
<h2>Algorithm 2 Training Data Generation</h2>
<h2>Require: $\phi$</h2>
<p>$\triangleright$ ADR distribution parameters
repeat
$\lambda \sim P_{\phi}$
$\operatorname{GeNERATEDATA}(\lambda)$
until training is complete</p>
<p>By using centralized storage, the ADR algorithm is decoupled from model optimization. However, to train a good policy or vision model using ADR, it is necessary to have a concurrent optimizer that consumes the training data in $T$ and pushes updated model parameters to $\Theta$.</p>
<p>We use $W$ parallel worker threads instead of the sequential while-loop. For training the policy, each worker pulls the latest distribution and model parameters from $\Phi$ and $\Theta$ and executes Algorithm 1 with probability $p_{b}$ (denoted as "ADR Eval Worker" in Figure 11a). Otherwise, it executes Algorithm 2 and pushes the generated data to $T$ (denoted as "Rollout Worker" in Figure 11a). To avoid wasting a large amount of data for only ADR, we also use this data to train the policy. The setup for vision is similar. Instead of rolling out a policy, we use the ADR parameters to render images and use those to train the supervised vision state estimator. Since data is cheaper to generate, we do not use the ADR evaluator data to train the model in this case but only used the data produced by the "Data Producer" (compare Figure 11b).</p>
<p>In the policy model, $\phi^{0}$ is set based on a calibrated environment parameter according to $\phi_{i}^{0, L}=\phi_{i}^{0, H}=\lambda_{i}^{\text {calib }}$ for all $i=1, \ldots, d$. In the vision model, the initial randomizations are set to zero, i.e. $\phi_{i}^{0, L}=\phi_{i}^{0, H}=0$. The distribution parameters are pushed to $\Phi$ to be used by all workers at the beginning of the algorithm.</p>
<h3>5.4 Randomizations</h3>
<p>Here, we describe the categories of randomizations used in this work. The vast majority of randomizations are for a scalar environment parameter $\lambda_{i}$ and are parameterized in ADR by two boundary parameters $\left(\phi_{i}^{L}, \phi_{i}^{H}\right)$. For a full listing of randomizations used in policy and vision training, see Appendix B.</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 11: The distributed ADR architecture for policy (left) and vision (right). In both cases, we use Redis for centralized storage of ADR parameters $(\Phi)$, model parameters $(\Theta)$, and training data $(T)$. ADR eval workers run Algorithm 1 to estimate performance using boundary sampling and report results using performance buffers $\left(\left{D_{i}\right}_{i=1}^{d}\right)$. The ADR updater uses those buffers to obtain average performance and increases or decreases boundaries accordingly. Rollout workers (for the policy) and data producers (for vision) produce data by sampling an environment as parameterized by the current set of ADR parameters (see Algorithm 2). This data is then used by the optimizer to improve the policy and vision model, respectively.</p>
<p>A few randomizations, such as observation noise, are controlled by more than one environment parameter and are parameterized by a larger set of boundary parameters. For full details on these randomizations and their ADR parameterization, see Appendix B.</p>
<p>Simulator physics. We randomize simulator physics parameters such as geometry, friction, gravity, etc. See Section B. 1 for details of their ADR parameterization.</p>
<p>Custom physics. We model additional physical robot effects that are not modelled by the simulator, for example, action latency or motor backlash. See [77, Appendix C.2] for implementation details of these models. We randomize the parameters in these models in a similar way to simulator physics randomizations.</p>
<p>Adversarial. We use an adversarial approach similar to [82, 83] to capture any remaining unmodeled physical effects in the target domain. However, we use random networks instead of a trained adversary. See Section B. 3 for details on implementation and ADR parameterization.</p>
<p>Observation. We add Gaussian noise to policy observations to better approximate observation conditions in reality. We apply both correlated noise, which is sampled once at the start of an episode and uncorrelated noise, which is sampled at each time step. We randomize the parameters of the added noise. See Section B. 4 for details of their ADR parameterization.</p>
<p>Vision. We randomize several aspects in ORRB [16] to control the rendered scene, including lighting conditions, camera positions and angles, materials and appearances of all the objects, the texture of the background, and the post-processing effects on the rendered images. See Section B. 5 for details.</p>
<h1>6 Policy Training in Simulation</h1>
<p>In this section we describe how we train control policies using Proximal Policy Optimization [98] and reinforcement learning. Our setup is similar to [77]. However, we use ADR as described in Section 5 to train on a large distribution over randomized environments.</p>
<h1>6.1 Actions, Rewards, and Goals</h1>
<p>Our setup for the action space and rewards is unchanged from [77] so we only briefly recap them here. We use a discretized action space with 11 bins per actuated joint (of which there are 20). We use a multi-categorical distribution. Actions are relative changes in generalized joint position coordinates.
There are three types of rewards we provide to our agent during training: (a) The difference between the previous and the current distance of the system state from the goal state, (b) an additional reward of 5 whenever a goal is achieved, (c) and a penalty of -20 whenever a cube/block is dropped.</p>
<p>We generate random goals during training. For the block, the target rotation is randomly sampled but constrained such that any face points directly upwards. For the Rubik's cube the task generation is slightly more convoluted as it depends on the state of the cube at the time when the goal is generated. If the cube faces are not aligned, we make sure to align them and additionally rotate the whole cube according to a sampled random orientation just like with the block (called a flip). Alternatively, if the faces are aligned, we rotate the top cube face with $50 \%$ probability either clockwise or counter-clockwise. Otherwise we again perform a flip. Detailed listings of the goal generation algorithms can be found in the Section C.1.</p>
<p>We consider a training episode to be finished whenever one of the following conditions is satisfied: (a) the agent achieves 50 consecutive successes (of reaching a goal within the required threshold), (b) the agent drops the cube, (c) or the agent times out when trying to reach the next goal. Time out limits are 400 timesteps for block reorientation and 800 timesteps ${ }^{10}$ for the Rubik's Cube.</p>
<h3>6.2 Policy Architecture</h3>
<p>We base our policy architecture on [77] but extend it in a few important ways. The policy is still recurrent since only a policy with access to some form of memory can perform meta-learning. We still use a single feed-forward layer with a ReLU activation [72] followed by a single LSTM layer [45]. However, we increase the capacity of the network by doubling the number of units: the feed-forward layer now has 2048 units and the LSTM layer has 1024 units.
The value network is separate from the policy network (but uses the same architecture) and we project the output of the LSTM onto a scalar value. We also add L2 regularization with a coefficient of $10^{-6}$ to avoid ever-growing weight norms for long-running experiments.
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 12: Neural network architecture for (a) value network and (b) policy network.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Inputs for the Rubik's cube task of the policy and value networks, respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Input</th>
<th style="text-align: left;">Dimensionality</th>
<th style="text-align: center;">Policy network</th>
<th style="text-align: center;">Value network</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Fingertip positions</td>
<td style="text-align: left;">15D</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Noisy fingertip positions</td>
<td style="text-align: left;">15D</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Cube position</td>
<td style="text-align: left;">3D</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Noisy cube position</td>
<td style="text-align: left;">3D</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Cube orientation</td>
<td style="text-align: left;">4D (quaternion)</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Noisy cube orientation</td>
<td style="text-align: left;">4D (quaternion)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Goal orientation</td>
<td style="text-align: left;">4D (quaternion)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Relative goal orientation</td>
<td style="text-align: left;">4D (quaternion)</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Noisy relative goal orientation</td>
<td style="text-align: left;">4D (quaternion)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Goal face angles</td>
<td style="text-align: left;">$12 \mathrm{D}^{11}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Relative goal face angles</td>
<td style="text-align: left;">$12 \mathrm{D}^{11}$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Noisy relative goal face angles</td>
<td style="text-align: left;">$12 \mathrm{D}^{11}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Hand joint angles</td>
<td style="text-align: left;">$48 \mathrm{D}^{11}$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">All simulation positions \&amp; orientations (qpos)</td>
<td style="text-align: left;">170D</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">All simulation velocities (qvel)</td>
<td style="text-align: left;">168D</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>An important difference between our architecture and the architecture used in [77] is how inputs are handled. In [77], the inputs for the policy and value networks consisted of different observations (e.g. fingertip positions, block pose, ...) in noisy and non-noisy versions. For each network, all observation fields were concatenated into a single vector. Noisy observations were provided to the policy network while the value network had access to non-noisy observations (since the value network is not needed when rolling out the policy on the robot and can thus use privileged information, as described in [81]). We still use the same Asymmetric Actor-Critic architecture [81] but replace the concatenation with what we call an "embed-and-add" approach. More concretely, we first embed each type of observation separately (without any weight sharing) into a latent space of dimensionality 512 . We then combine all inputs by adding the latent representation of each and applying a ReLU non-linearity after. The main motivation behind this change was to easily add new observations to an existing policy and to share embeddings between value and policy network for inputs that feed into both. The network architecture of our control policy is illustrated in Figure 12. More details of what inputs are fed into the networks can be found in Section C. 3 and Table 1. We list the inputs for the block reorientation task in Section C.2.</p>
<h1>6.3 Distributed Training with Rapid</h1>
<p>We use our own internal distributed training framework, Rapid. Rapid was previously used to train OpenAI Five [76] and was also used in [77].
For the block reorientation task, we use $4 \times 8=32$ NVIDIA V100 GPUs and $4 \times 100=400$ worker machines with 32 CPU cores each. For the Rubik's cube task, we use $8 \times 8=64$ NVIDIA V100 GPUs and $8 \times 115=920$ worker machines with 32 CPU cores each. We've been training the Rubik's Cube policy continuously for several months at this scale while concurrently improving the simulation fidelity, ADR algorithm, tuning hyperparameters, and even changing the network architecture. The cumulative amount of experience over that period used for training on the Rubik's cube is roughly 13 thousand years, which is on the same order of magnitude as the 40 thousand years used by OpenAI Five [76].
The hyperparameters that we used and more details on optimization can be found in Section C.3.</p>
<h3>6.4 Policy Cloning</h3>
<p>With ADR, we found that training the same policy for a very long time is helpful since ADR allows us to always have a challenging training distribution. We therefore rarely trained experiments from scratch but instead updated existing experiments and initialized from previous checkpoints for both the ADR and policy parameters. Our new "embed-and-add" approach in Figure 12 makes it easier to change the observation space of the agent, but doesn't allow</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>us to experiment with changes to the policy architecture, e.g. modify the number of units in each layer or add a second LSTM layer. Restarting training from an uninitialized model would have caused us to lose weeks or months of training progress, making such changes prohibitively expensive. Therefore, we successfully implemented behavioral cloning in the spirit of the DAGGER [88] algorithm (sometimes also called policy distillation [23]) to efficiently initialize new policies with a level of performance very close to the teacher policy.
Our setup for cloning closely mimics reinforcement learning, except that we now have both teacher and student policies loaded in memory. During a rollout, we use the student actions to interact with the environment, while minimizing the difference between the student and the teacher's action distributions (by minimizing KL divergence) and value predictions (by minimizing L2 loss). This has worked surprisingly well, allowing us to iterate on the policy architecture quickly without losing the accumulated training progress. Our cloning approach works with arbitrary policy architecture changes as long as the action space remains unchanged.
The best ADR policies used in this work were obtained using this approach. We trained them for multiple months while making multiple changes to the model architecture, training environment, and hyperparameters.</p>
<h1>7 State Estimation from Vision</h1>
<p>As in [77], the control policy described in Section 6 receives object state estimates from a vision system consisting of three cameras and a neural network predictor. In this work, the policy requires estimates for all six face angles in addition to the position and orientation of the cube.
Note that the absolute rotation of each face angle in $[-\pi, \pi]$ radians is required by the policy. Due to the rotational symmetry of the stickers on a standard Rubik's cube, it is not possible to predict these absolute face angles from a single camera frame; the system must either have some ability to track state temporally ${ }^{12}$ or the cube has to be modified.
We therefore use two different options for the state estimation of the Rubik's cube throughout this work:</p>
<ol>
<li>Vision only via asymmetric center stickers. In this case, the vision model is used to produce the cube position, rotation, and six face angles. We cut out one corner of each center sticker on the cube (see Figure 13), thus breaking rotational symmetry and allowing our model to determine absolute face angles from a single frame. No further customizations were made to the Rubik's cube. We use this model to estimate final performance of a vision only solution to solving the Rubik's cube.</li>
<li>Vision for pose and Giiker cube for face angles. In this case, the vision model is used to produce the cube position and rotation. For the face angles, we use the previously described customized Giiker cube (see Section 3) with built-in sensors. We use this model for most experiments in order to not compound errors of the challenging face angle estimation from vision only with errors of the policy.</li>
</ol>
<p>Since our long-term goal is to build robots that can interact in the real world with arbitrary objects, ideally we would like to fully solve this problem from vision alone using a standard Rubik's cube (i.e. without any special stickers). We believe this is possible, though it may require either more extensive work on a recurrent model or moving to an end-to-end training setup (i.e. where the vision model is learned jointly with the policy). This remains an active area of research for us.</p>
<h3>7.1 Vision Model</h3>
<p>Our vision model has a similar setup as in [77], taking as input an image from each of three RGB Basler cameras located at the left, right, and top of the cage (see Figure 4(a)). The full model architecture is illustrated in Figure 14. We produce a feature map for each image by processing it through identically parameterized ResNet50 [43] networks (i.e. using common weights). These three feature maps are then flattened, concatenated, and fed into a stack of fully-connected layers which ultimately produce predictions sufficient for tracking the full state of the cube, including the position, orientation, and face angles.
While predicting position and orientation directly works well, we found predicting all six face angles directly to be much more challenging due to heavy occlusion, even when using a cube with asymmetric center stickers. To work around this, we decomposed face angle prediction into several distinct predictions:</p>
<ol>
<li>Active axis: We make a slight simplifying assumption that only one of the three axes of a cube can be "active" (i.e. be in an non-aligned state), and have the model predict which of the three axes is currently active.
<sup id="fnref11:0"><a class="footnote-ref" href="#fn:0">1</a></sup></li>
</ol>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 13: The Rubik's cube with a corner cut out of each center sticker (a) in simulation and (b) in reality. We used this cube instead of the Giiker cube for some vision state estimation experiments and for evaluating the performance of the policy for solving the Rubik's cube from vision only.
2. Active face angles: We predict the angles of the two faces relevant for the active axis modulo $\pi / 2$ radians (i.e. in $[-\pi / 4, \pi / 4])$. It is hard to predict the absolute angles in $[-\pi, \pi]$ radians directly due to heavy occlusion (e.g. when a face is on the bottom and hidden by the palm). Predicting these modulo $\pi / 2$ angles only requires recognizing the shape and the relative positions of cube edges, and therefore it is an easier task.
3. Top face angle: The last piece to predict is the absolute angle in $[-\pi, \pi]$ radians of the "top" face, that is the face visible from a camera mounted directly above the hand. Note that this angle is only possible to predict from frames at a single timestamp because of the asymmetric center stickers (See Figure 13). We configure the model to make a prediction only for the top face because the top face's center cubelet is rarely occluded. This gives us a stateless estimate of each face's absolute angle of rotation whenever that face is placed on top.</p>
<p>These decomposed face angle predictions are then fed into post-processing logic (See Appendix C Algorithm 5) to track the rotation of all face angles, which are in turn passed along to the policy. The top face angle prediction is especially important, as it allows us to correct the tracked absolute face angle state mid-trial. For example, if the tracking of a face angle becomes off by some number of rotations (i.e. a multiple of $\pi / 2$ radians), we are still able to correct it with a stateless absolute angle prediction from the model whenever this face is placed on top after a flip. Predictions (1) and (2) are primarily important because the policy is unable to rotate a non-active face if the active face angles are too large (in which case the cube becomes interlocked along non-active axes).</p>
<p>For all angle predictions, we found that discretizing angles into 90 bins per $\pi$ radians yielded better performance than directly predicting angles via regression; see Table 2 for details.</p>
<p>In the meantime, domain randomization in the rendering process remains a critical role in the sim2real transfer. As shown in Table 2, a model trained without domain randomization can achieve perfectly low errors in simulation but fails dramatically on real world data.</p>
<h1>7.2 Distributed Training with Rapid</h1>
<p>As in control policy training (Section 6), the vision model is trained entirely from synthetic data, without any images from the real world. This necessarily entails a more complicated training setup, wherein the synthetic image generation must be coupled with optimization. To manage this complexity, we leverage the same Rapid framework [76] which is used in policy training for distributed training.</p>
<p><img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Figure 14: Vision model architecture, which is largely built upon a ResNet50 [43] backbone. Network weights are shared across the three camera frames, as indicated by the dashed line. Our model produces the position, orientation, and a specific representation of the six face angles of the Rubik's cube. We specify ranges with [...] and dimensionality with (...).</p>
<p>Table 2: Ablation experiments for the vision model. For each experiment, we ran training with 3 different seeds and report the best performance here. Orientation error is computed as rotational distance over a quaternion representation. Position error is the euclidean distance in 3D space, in millimeters. Face angle error is measured in degrees ( ${ }^{\circ}$ ). "Real" errors are computed using data collected over multiple physical trials, where the position and orientation ground truths are from PhaseSpace (Section 3) and all face angle ground truths are from the Giiker cube. The full evaluation results, including errors on active axis and active face angles, are reported in Appendix D Table 22.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Experiment</th>
<th style="text-align: center;">Errors (Sim)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Errors (Real)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Orientation</td>
<td style="text-align: center;">Position</td>
<td style="text-align: center;">Top Face</td>
<td style="text-align: center;">Orientation</td>
<td style="text-align: center;">Position</td>
<td style="text-align: center;">Top face</td>
</tr>
<tr>
<td style="text-align: center;">Full Model</td>
<td style="text-align: center;">$6.52^{\circ}$</td>
<td style="text-align: center;">2.63 mm</td>
<td style="text-align: center;">$11.95^{\circ}$</td>
<td style="text-align: center;">$7.81^{\circ}$</td>
<td style="text-align: center;">6.47 mm</td>
<td style="text-align: center;">$15.92^{\circ}$</td>
</tr>
<tr>
<td style="text-align: center;">No Domain Randomization</td>
<td style="text-align: center;">$3.95^{\circ}$</td>
<td style="text-align: center;">2.97 mm</td>
<td style="text-align: center;">$8.56^{\circ}$</td>
<td style="text-align: center;">$128.83^{\circ}$</td>
<td style="text-align: center;">69.40 mm</td>
<td style="text-align: center;">$85.33^{\circ}$</td>
</tr>
<tr>
<td style="text-align: center;">No Focal Loss</td>
<td style="text-align: center;">$15.94^{\circ}$</td>
<td style="text-align: center;">5.02 mm</td>
<td style="text-align: center;">$10.17^{\circ}$</td>
<td style="text-align: center;">$19.10^{\circ}$</td>
<td style="text-align: center;">9.416 mm</td>
<td style="text-align: center;">$17.54^{\circ}$</td>
</tr>
<tr>
<td style="text-align: center;">Non-discrete Angles</td>
<td style="text-align: center;">$9.02^{\circ}$</td>
<td style="text-align: center;">3.78 mm</td>
<td style="text-align: center;">$42.46^{\circ}$</td>
<td style="text-align: center;">$10.40^{\circ}$</td>
<td style="text-align: center;">7.97 mm</td>
<td style="text-align: center;">$35.27^{\circ}$</td>
</tr>
</tbody>
</table>
<p>Figure 11b gives an overview of the setup for a typical vision experiment. In the case of vision training, the "data workers" are standalone Unity renderers, responsible for rendering simulated images using OpenAI Remote Rendering Backend (ORRB) [16]. These images are rendered according to ADR parameters pulled from the ADR subsystem (see Section 5). A list of randomization parameters is available in Section B. 5 Table 11. Each rendering node uses 1 NVIDIA V100 GPU and 8 CPU cores, and the size of the rendering pool is tuned such that rendering is not a bottleneck in training. The data from these rendering nodes is then propagated to a cluster of Redis nodes where it is stored in separate queues for training and evaluation. The training data is then read by a pool of optimizer nodes, each of which</p>
<p>uses 8 NVIDIA V100 GPUs and 64 CPU cores, in order to perform optimization in a data-parallel fashion. Meanwhile, the evaluation data is read by the “ADR eval workers” in order to provide feedback on ADR parameters, per Section 5.</p>
<p>As noted above, the vision model produces several distinct predictions, each of which has its own loss function to be optimized: mean squared error for both position and orientation, and cross entropy for each of the decomposed face angle predictions. To balance these many losses, which lie on different scales, we use focal loss weighting as described in [37] to dynamically and automatically assign loss weights. One modification we made in order to better fit our multiple regression tasks is that we define a low target error for each prediction and then use the percentage of samples that obtain errors below the target as the probability p in the focal loss, i.e. FL(p;γ) = -(1 - p)^γlog(p), where γ = 1 in all our experiments. This both removes the need to manually tune loss weights and improves optimization, as it allows loss weights to change dynamically during training (see Table 2 for performance details).</p>
<p>Optimization is then performed against this aggregate loss using the LARS optimizer [118]. We found LARS to be more stable than the Adam optimizer [51] when using larger batches and higher learning rates (we use at most a batch size of 1024 with a peak learning rate of 6.0). See Section C.3 for further hyperparameter details.</p>
<h2>8 Results</h2>
<p>In this section, we investigate the effect ADR has on transfer (Section 8.1), empirically show the importance of having a curriculum for policy training (Section 8.2), quantify vision performance (Section 8.3), and finally present our results that push the limits of what is possible by solving a Rubik's cube on the real Shadow hand (Section 8.4).</p>
<h3>8.1 Effect of ADR on Policy Transfer</h3>
<p>To understand the transfer performance impact of training policies with ADR, we study the problem on the simpler block reorientation task previously introduced in [77]. We use this task since it is computationally more tractable and because baseline performance has been established. As in [77], we measure performance in terms of the number of consecutive successes. We terminate an episode if the block is either dropped or if 50 consecutive successes are achieved. An optimal policy would therefore be one that achieves a mean of 50 successes.</p>
<h4>8.1.1 Sim2Sim</h4>
<p><img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>Figure 15: Sim2sim performance (left) and ADR entropy (right) over the course of training. We can see that a policy trained with ADR has better sim2sim transfer as ADR increases the randomization level over time.</p>
<p>We first consider the sim2sim case. More concretely, we train a policy with ADR and continuously benchmark its performance on a distribution of environments with manually tuned randomizations, very similar to the ones we used in [77]. Note that no ADR experiment has ever been trained on this distribution directly. Instead we use ADR to decide what distribution to train on, making the manually designed distribution over environments a test set for sim2sim transfer. We report our results in Figure 15.</p>
<p>As seen in Figure 15, the policy trained with ADR transfers to the manually randomized distribution. Furthermore, the sim2sim transfer performance increases as ADR increases the randomization entropy.</p>
<h1>8.1.2 Sim2Real</h1>
<p>Next, we evaluate the sim2real transfer capabilities of our policies. Since rollouts on the robot are expensive, we limit ourselves to 7 different policies that we evaluate. For each of them, we collect a total of 10 trials on the robot and measure the number of consecutive successes. As before, a trial ends when we either achieve 50 successes, the robot times out or the block is dropped. For each policy we deploy, we also report simulation performance by measuring the number of successes across 500 trials each for reference. As before, we use the manually designed randomizations as described in [77] for sim evaluations. We summarize our results in Table 3 and report detailed results in Appendix D.</p>
<p>Table 3: Performance of different policies on the block reorientation task. We evaluate each policy in simulation ( $\mathrm{N}=500$ trials) and on the real robot ( $\mathrm{N}=10$ trials) and report the mean $\pm$ standard error and median number of successes. For ADR policies, we report the entropy in nats per dimension (npd). For "Manual DR", we obtain an upper bound on its ADR entropy by running ADR with the policy fixed and report the entropy once the distribution stops changing (marked with an "*").</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Policy</th>
<th style="text-align: center;">Training Time</th>
<th style="text-align: center;">ADR Entropy</th>
<th style="text-align: center;">Successes (Sim)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Successes (Real)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mean</td>
<td style="text-align: center;">Median</td>
<td style="text-align: center;">Mean</td>
<td style="text-align: center;">Median</td>
</tr>
<tr>
<td style="text-align: center;">Baseline (data from [77])</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">$43.4 \pm 0.6$</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">$18.8 \pm 5.4$</td>
<td style="text-align: center;">13.0</td>
</tr>
<tr>
<td style="text-align: center;">Baseline (re-run of [77])</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">$33.8 \pm 0.9$</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">$4.0 \pm 1.7$</td>
<td style="text-align: center;">2.0</td>
</tr>
<tr>
<td style="text-align: center;">Manual DR</td>
<td style="text-align: center;">13.78 days</td>
<td style="text-align: center;">$-0.348^{*}$ npd</td>
<td style="text-align: center;">$42.5 \pm 0.7$</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">$2.7 \pm 1.1$</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: center;">ADR (Small)</td>
<td style="text-align: center;">0.64 days</td>
<td style="text-align: center;">-0.881 npd</td>
<td style="text-align: center;">$21.0 \pm 0.8$</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">$1.4 \pm 0.9$</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: center;">ADR (Medium)</td>
<td style="text-align: center;">4.37 days</td>
<td style="text-align: center;">-0.135 npd</td>
<td style="text-align: center;">$34.4 \pm 0.9$</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">$3.2 \pm 1.2$</td>
<td style="text-align: center;">2.0</td>
</tr>
<tr>
<td style="text-align: center;">ADR (Large)</td>
<td style="text-align: center;">13.76 days</td>
<td style="text-align: center;">0.126 npd</td>
<td style="text-align: center;">$40.5 \pm 0.7$</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">$13.3 \pm 3.6$</td>
<td style="text-align: center;">11.5</td>
</tr>
<tr>
<td style="text-align: center;">ADR (XL)</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">0.305 npd</td>
<td style="text-align: center;">$45.0 \pm 0.6$</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">$16.0 \pm 4.0$</td>
<td style="text-align: center;">12.5</td>
</tr>
<tr>
<td style="text-align: center;">ADR (XXL)</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">0.393 npd</td>
<td style="text-align: center;">$46.7 \pm 0.5$</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">$32.0 \pm 6.4$</td>
<td style="text-align: center;">42.0</td>
</tr>
</tbody>
</table>
<p>The first two rows connect our results in this work to the previous results reported in [77]. For convenience, we repeat the numbers reported in [77] in the first row. We also re-deploy the exact same policy we used back then on our setup today. We find that the same policy performs much worse today, presumably because both our physical setup and simulation have changed since (as described in Section 3 and Section 4, respectively).
The next section of the table compares a policy trained with ADR and a policy trained with manual domain randomization (denoted as "Manual DR"). Note that "Manual DR" uses the same randomizations as the baseline from [77] but is trained on our current setup with the same model architecture and hyperparameters as the ADR policy. For the ADR policy, we select snapshots at different points during training at varying levels of entropy and denote them as small, medium, and large. ${ }^{13}$ We can clearly see a pattern: increased ADR entropy corresponds to increased sim2sim and sim2real transfer. The policy trained with manual domain randomization achieves high performance in simulation. However, when deployed on the robot, it fails. This is because, in contrast to our results obtained in [77], we did not tune our simulation and randomization setup by hand to match changes in hardware. Our ADR policies transfer because ADR automates this process and results in training distributions that are vastly broader than our manually tuned distribution was in the past. Also note that "ADR (Large)" and "Manual DR" were trained for the same amount of wall-clock time and share all training hyperparameters except for the environment distribution, i.e. they are fully comparable. Due to compute constraints, we train those policies at $1 / 4$ th of our usual scale in terms of compute (compare Section 6).
The last block of the table lists results that we obtained when scaling ADR up. We report results for "ADR (XL)" and "ADR (XXL)", referring to two long-running experiments that were continuously trained for extended periods of time and at larger scale. We can see that they exhibit the best sim2sim and sim2real transfer and that, again, an increase in ADR entropy corresponds to vastly improved sim2real transfer. Our best result significantly beat the baseline reported in [77] even though we did not tune the simulation and robot setup for peak performance on the block reorientation task: we increase mean performance by almost $2 \times$ and median performance by more than $3 \times$. As a side note, we also see that policies trained with ADR eventually achieve near-perfect performance for sim2sim transfer as well.</p>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>In summary, ADR clearly leads to improved transfer with much less need for hand-engineered randomizations. We significantly outperformed our previous best results, which were the result of multiple months of iterative manual tuning.</p>
<h1>8.2 Effect of Curriculum on Policy Training</h1>
<p>We designed ADR to expand the complexity of the training distribution gradually. This makes intuitive sense: start with a single environment and then grow the distribution over environments as the agent progresses. The resulting curriculum should make it possible to eventually master a highly diverse set of environments. However, it is not clear if this curriculum property is important or if we can train with a fixed set of domain randomization parameters once they have been found.</p>
<p>To test for this, we conduct the following experiment. We train one policy with ADR on the block reorientation task and compare it against multiple policies with different fixed randomizations. We use 4 different fixed levels: small, medium, large, and XL. They correspond to the ADR parameters of the policies from the previous section (compare Table 3). However, note that we only use the ADR parameters, not the policies from Table 3. Instead, we train new policies from scratch using these parameters and train all of them for the same amount of wall-clock time. We evaluate performance of all policies continuously on the same manually randomized distribution from [77], i.e. we test for sim2sim transfer in all cases. We depict our results in Figure 16. Note that for all DR runs the randomization entropy is constant; only the one for ADR gradually increases.
<img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<p>Figure 16: Sim2sim performance (left) and ADR entropy (right) over the course of training. $A D R$ refers to a regular training run, i.e. we start with zero randomization and let ADR gradually expand the randomization level. We compare ADR against runs with domain randomization (DR) fixed at different levels and train policies on each of those environment distributions. We can see that ADR makes progress much faster due to its curriculum property.</p>
<p>Our results in Figure 16 clearly demonstrate that adaptively increasing the randomization entropy is important: the ADR run achieves high sim2sim transfer much more quickly than all other runs with fixed randomization entropy. There is also a clear pattern: the larger the fixed randomization entropy, the longer it takes to train from scratch. We hypothesize that for a sufficiently difficult task and randomization entropy, training from scratch becomes infeasible altogether. More concretely, we believe that for too complex environments the policy would never learn due to the task being so hard that there is no sufficient reinforcement learning signal.</p>
<h3>8.3 Effect of ADR on Vision Model Performance</h3>
<p>When training vision models, ADR controls both the ranges of randomization in ORRB (i.e. light distance, material metallic and glossiness) and TensorFlow distortion operations (i.e. adding Gaussian noise and channel noise). A full list of vision ADR randomization parameters are available in Section B. 5 Table 11. We train ADR-enhanced vision models to do state estimation for both the block reorientation [77] and Rubik's cube task. As shown in Table 4, we are able to reduce the prediction errors on both block orientation and position further than our manual domain randomization</p>
<p><sup id="fnref10:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{13}$ The attentive reader will notice that the sim2sim performance reported in Figure 16 is different from the sim2sim performance reported in Table 3. This is because here we train the policies for longer but on a fixed ADR entropy whereas in Table 3 we had a single ADR run and took snapshots at different points over the course of training.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref10:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref11:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:9">
<p>Real Rubik’s cubes also have cubelets with rounded corners, for the same reason.&#160;<a class="footnote-backref" href="#fnref:9" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8965 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8965</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8965</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-c8efcc854d97dfc2a42b83316a2109f9d166e43f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c8efcc854d97dfc2a42b83316a2109f9d166e43f" target="_blank">Self-Attention with Relative Position Representations</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work presents an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements, on the WMT 2014 English-to-German and English- to-French translation tasks.</p>
                <p><strong>Paper Abstract:</strong> Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8965.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8965.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Relative Position Representations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relation-aware Relative Position Representations for Self-Attention</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension to Transformer self-attention that represents pairwise relations between input elements as learned edge vectors a_{ij}^K and a_{ij}^V (indexed by clipped relative distance) which are added into the attention compatibility computation and the value projection, enabling the model to be relation-aware and to encode relative positions as labeled, directed graph edges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Relative position edge-label encoding (relation-aware self-attention)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Input is modeled as a labeled, directed, fully-connected graph where each directed edge (i->j) has two learned vector labels a_{ij}^K and a_{ij}^V in R^{d_a}. For linear sequences these edge labels are parameterized by the clipped relative distance between positions (clip(j-i, k)), yielding 2k+1 learned embeddings w^K_{-k..k} and w^V_{-k..k}. a_{ij}^K is added to the key-side representation when computing attention logits, and a_{ij}^V is added to the value-side representations when computing attention outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Labeled, directed, fully-connected graphs induced by token sequences (relative-distance-labelled edges); proposed extension to arbitrary labeled, directed graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Not a text-serialization; instead the graph is incorporated directly into model computations: compute e_{ij} = (x_i W^Q) ((x_j W^K) + a_{ij}^K)^T / sqrt(d_z) and z_i = sum_j softmax_j(e_{ij}) (x_j W^V + a_{ij}^V). For sequences the conversion of pairwise relation to an index is clip(j-i, k), mapping relative distance to an edge embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Neural machine translation (WMT 2014 English->German and English->French). The paper frames the mechanism generally for sequence tasks and as an extension toward arbitrary graph-labeled inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BLEU on newstest2014: Base Transformer (absolute sinusoidal positions) EN-DE 26.5, EN-FR 38.2; Base with relative positions EN-DE 26.8, EN-FR 38.7. Big Transformer absolute: EN-DE 27.9, EN-FR 41.2; Big with relative: EN-DE 29.2, EN-FR 41.5. Additional reported numbers on dev (newstest2013) for varying clipping k: k=0 -> 12.5 BLEU; k=1 -> 25.5; k>=2 around 25.8-25.9. Ablation: removing both a_{ij}^K and a_{ij}^V (i.e., no relative reps) yields 12.5 BLEU; using only a_{ij}^K (No V, Yes K) yields 25.8; using only a_{ij}^V (Yes V, No K) yields 25.3; both yields 25.8 (base config on dev). Runtime: relative position implementation caused ~7% decrease in training steps/sec in their setup.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared directly to absolute (sinusoidal) position encodings used in the original Transformer: relative position representations improved BLEU by 0.3 (base EN-DE), 1.3 (big EN-DE), 0.5 (base EN-FR), and 0.3 (big EN-FR). The paper reports that combining relative and absolute (sinusoidal) representations produced no further improvement. The method is contrasted conceptually with graph-attention approaches (cites Veličković et al. 2017) but is implemented and evaluated in the sequence/translation setting rather than general graph inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Encodes relative positional information explicitly and is invariant to total sequence length; improves translation BLEU over absolute sinusoidal encodings in experiments; clipping of distances enables generalization to longer sequences and reduces number of distinct edge labels; can be shared across attention heads to reduce memory; modest runtime overhead (reported ~7% training speed reduction) and modest additional memory O(n^2 d_a) on top of standard attention.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Introduces additional space complexity O(n^2 d_a) (though shared across heads reduces factor), and modest runtime slowdown (~7% fewer steps/sec in reported setup). Requires choosing a clipping distance k (k=0 collapses position info and fails). The approach as implemented increases implementation complexity (tensor reshaping to compute second term efficiently).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When all relative representations are removed (a_{ij}^K and a_{ij}^V absent) or clipping k=0, model performance collapses (reported BLEU 12.5 in dev EN-DE). Ablation shows that removing the key-side relative representation (a_{ij}^K) hurts more than removing the value-side in their experiments (YesV NoK BLEU 25.3 vs NoV YesK 25.8), indicating sensitivity to which edge vectors are used. The paper also notes no observed benefit from combining relative and absolute sinusoidal encodings (i.e., redundancy) and that further work is needed to evaluate whether propagating a_{ij}^V to downstream layers is necessary for tasks beyond translation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases_additional_notes</strong></td>
                            <td>Method tested for linear sequences; full generalization to arbitrary labeled graph inputs is proposed but not evaluated here. Efficiency tricks are needed to compute pairwise terms without excessive broadcasting; naive implementations would be memory/time prohibitive.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Attention with Relative Position Representations', 'publication_date_yy_mm': '2018-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Graph Attention Networks <em>(Rating: 2)</em></li>
                <li>Attention is All You Need <em>(Rating: 2)</em></li>
                <li>End-to-end Memory Networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8965",
    "paper_id": "paper-c8efcc854d97dfc2a42b83316a2109f9d166e43f",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "Relative Position Representations",
            "name_full": "Relation-aware Relative Position Representations for Self-Attention",
            "brief_description": "An extension to Transformer self-attention that represents pairwise relations between input elements as learned edge vectors a_{ij}^K and a_{ij}^V (indexed by clipped relative distance) which are added into the attention compatibility computation and the value projection, enabling the model to be relation-aware and to encode relative positions as labeled, directed graph edges.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Relative position edge-label encoding (relation-aware self-attention)",
            "representation_description": "Input is modeled as a labeled, directed, fully-connected graph where each directed edge (i-&gt;j) has two learned vector labels a_{ij}^K and a_{ij}^V in R^{d_a}. For linear sequences these edge labels are parameterized by the clipped relative distance between positions (clip(j-i, k)), yielding 2k+1 learned embeddings w^K_{-k..k} and w^V_{-k..k}. a_{ij}^K is added to the key-side representation when computing attention logits, and a_{ij}^V is added to the value-side representations when computing attention outputs.",
            "graph_type": "Labeled, directed, fully-connected graphs induced by token sequences (relative-distance-labelled edges); proposed extension to arbitrary labeled, directed graphs.",
            "conversion_method": "Not a text-serialization; instead the graph is incorporated directly into model computations: compute e_{ij} = (x_i W^Q) ((x_j W^K) + a_{ij}^K)^T / sqrt(d_z) and z_i = sum_j softmax_j(e_{ij}) (x_j W^V + a_{ij}^V). For sequences the conversion of pairwise relation to an index is clip(j-i, k), mapping relative distance to an edge embedding.",
            "downstream_task": "Neural machine translation (WMT 2014 English-&gt;German and English-&gt;French). The paper frames the mechanism generally for sequence tasks and as an extension toward arbitrary graph-labeled inputs.",
            "performance_metrics": "BLEU on newstest2014: Base Transformer (absolute sinusoidal positions) EN-DE 26.5, EN-FR 38.2; Base with relative positions EN-DE 26.8, EN-FR 38.7. Big Transformer absolute: EN-DE 27.9, EN-FR 41.2; Big with relative: EN-DE 29.2, EN-FR 41.5. Additional reported numbers on dev (newstest2013) for varying clipping k: k=0 -&gt; 12.5 BLEU; k=1 -&gt; 25.5; k&gt;=2 around 25.8-25.9. Ablation: removing both a_{ij}^K and a_{ij}^V (i.e., no relative reps) yields 12.5 BLEU; using only a_{ij}^K (No V, Yes K) yields 25.8; using only a_{ij}^V (Yes V, No K) yields 25.3; both yields 25.8 (base config on dev). Runtime: relative position implementation caused ~7% decrease in training steps/sec in their setup.",
            "comparison_to_others": "Compared directly to absolute (sinusoidal) position encodings used in the original Transformer: relative position representations improved BLEU by 0.3 (base EN-DE), 1.3 (big EN-DE), 0.5 (base EN-FR), and 0.3 (big EN-FR). The paper reports that combining relative and absolute (sinusoidal) representations produced no further improvement. The method is contrasted conceptually with graph-attention approaches (cites Veličković et al. 2017) but is implemented and evaluated in the sequence/translation setting rather than general graph inputs.",
            "advantages": "Encodes relative positional information explicitly and is invariant to total sequence length; improves translation BLEU over absolute sinusoidal encodings in experiments; clipping of distances enables generalization to longer sequences and reduces number of distinct edge labels; can be shared across attention heads to reduce memory; modest runtime overhead (reported ~7% training speed reduction) and modest additional memory O(n^2 d_a) on top of standard attention.",
            "disadvantages": "Introduces additional space complexity O(n^2 d_a) (though shared across heads reduces factor), and modest runtime slowdown (~7% fewer steps/sec in reported setup). Requires choosing a clipping distance k (k=0 collapses position info and fails). The approach as implemented increases implementation complexity (tensor reshaping to compute second term efficiently).",
            "failure_cases": "When all relative representations are removed (a_{ij}^K and a_{ij}^V absent) or clipping k=0, model performance collapses (reported BLEU 12.5 in dev EN-DE). Ablation shows that removing the key-side relative representation (a_{ij}^K) hurts more than removing the value-side in their experiments (YesV NoK BLEU 25.3 vs NoV YesK 25.8), indicating sensitivity to which edge vectors are used. The paper also notes no observed benefit from combining relative and absolute sinusoidal encodings (i.e., redundancy) and that further work is needed to evaluate whether propagating a_{ij}^V to downstream layers is necessary for tasks beyond translation.",
            "failure_cases_additional_notes": "Method tested for linear sequences; full generalization to arbitrary labeled graph inputs is proposed but not evaluated here. Efficiency tricks are needed to compute pairwise terms without excessive broadcasting; naive implementations would be memory/time prohibitive.",
            "uuid": "e8965.0",
            "source_info": {
                "paper_title": "Self-Attention with Relative Position Representations",
                "publication_date_yy_mm": "2018-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Graph Attention Networks",
            "rating": 2
        },
        {
            "paper_title": "Attention is All You Need",
            "rating": 2
        },
        {
            "paper_title": "End-to-end Memory Networks",
            "rating": 1
        }
    ],
    "cost": 0.006281499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Self-Attention with Relative Position Representations</h1>
<p>Peter Shaw<br>Google<br>petershaw@google.com</p>
<p>Jakob Uszkoreit<br>Google Brain<br>usz@google.com</p>
<p>Ashish Vaswani<br>Google Brain<br>avaswani@google.com</p>
<h4>Abstract</h4>
<p>Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graphlabeled inputs.</p>
<h2>1 Introduction</h2>
<p>Recent approaches to sequence to sequence learning typically leverage recurrence (Sutskever et al., 2014), convolution (Gehring et al., 2017; Kalchbrenner et al., 2016), attention (Vaswani et al., 2017), or a combination of recurrence and attention (Bahdanau et al., 2014; Cho et al., 2014; Luong et al., 2015; Wu et al., 2016) as basic building blocks. These approaches incorporate information about the sequential position of elements differently.</p>
<p>Recurrent neural networks (RNNs) typically compute a hidden state $h_{t}$, as a function of their input at time $t$ and a previous hidden state $h_{t-1}$, capturing relative and absolute positions along the
time dimension directly through their sequential structure. Non-recurrent models do not necessarily consider input elements sequentially and may hence require explicitly encoding position information to be able to use sequence order.</p>
<p>One common approach is to use position encodings which are combined with input elements to expose position information to the model. These position encodings can be a deterministic function of position (Sukhbaatar et al., 2015; Vaswani et al., 2017) or learned representations. Convolutional neural networks inherently capture relative positions within the kernel size of each convolution. They have been shown to still benefit from position encodings (Gehring et al., 2017), however.</p>
<p>For the Transformer, which employs neither convolution nor recurrence, incorporating explicit representations of position information is an especially important consideration since the model is otherwise entirely invariant to sequence ordering. Attention-based models have therefore used position encodings or biased attention weights based on distance (Parikh et al., 2016).</p>
<p>In this work we present an efficient way of incorporating relative position representations in the self-attention mechanism of the Transformer. Even when entirely replacing its absolute position encodings, we demonstrate significant improvements in translation quality on two machine translation tasks.</p>
<p>Our approach can be cast as a special case of extending the self-attention mechanism of the Transformer to considering arbitrary relations between any two elements of the input, a direction we plan to explore in future work on modeling labeled, directed graphs.</p>
<h2>2 Background</h2>
<h3>2.1 Transformer</h3>
<p>The Transformer (Vaswani et al., 2017) employs an encoder-decoder structure, consisting of stacked encoder and decoder layers. Encoder layers consist of two sublayers: self-attention followed by a position-wise feed-forward layer. Decoder layers consist of three sublayers: selfattention followed by encoder-decoder attention, followed by a position-wise feed-forward layer. It uses residual connections around each of the sublayers, followed by layer normalization (Ba et al., 2016). The decoder uses masking in its selfattention to prevent a given output position from incorporating information about future output positions during training.</p>
<p>Position encodings based on sinusoids of varying frequency are added to encoder and decoder input elements prior to the first layer. In contrast to learned, absolute position representations, the authors hypothesized that sinusoidal position encodings would help the model to generalize to sequence lengths unseen during training by allowing it to learn to attend also by relative position. This property is shared by our relative position representations which, in contrast to absolute position representations, are invariant to the total sequence length.</p>
<p>Residual connections help propagate position information to higher layers.</p>
<h3>2.2 Self-Attention</h3>
<p>Self-attention sublayers employ $h$ attention heads. To form the sublayer output, results from each head are concatenated and a parameterized linear transformation is applied.</p>
<p>Each attention head operates on an input sequence, $x=\left(x_{1}, \ldots, x_{n}\right)$ of $n$ elements where $x_{i} \in \mathbb{R}^{d_{x}}$, and computes a new sequence $z=$ $\left(z_{1}, \ldots, z_{n}\right)$ of the same length where $z_{i} \in \mathbb{R}^{d_{z}}$.</p>
<p>Each output element, $z_{i}$, is computed as weighted sum of a linearly transformed input elements:</p>
<p>$$
z_{i}=\sum_{j=1}^{n} \alpha_{i j}\left(x_{j} W^{V}\right)
$$</p>
<p>Each weight coefficient, $\alpha_{i j}$, is computed using a softmax function:</p>
<p>$$
\alpha_{i j}=\frac{\exp e_{i j}}{\sum_{k=1}^{n} \exp e_{i k}}
$$</p>
<p>And $e_{i j}$ is computed using a compatibility function that compares two input elements:</p>
<p>$$
e_{i j}=\frac{\left(x_{i} W^{Q}\right)\left(x_{j} W^{K}\right)^{T}}{\sqrt{d_{z}}}
$$</p>
<p>Scaled dot product was chosen for the compatibility function, which enables efficient computation. Linear transformations of the inputs add sufficient expressive power.
$W^{Q}, W^{K}, W^{V} \in \mathbb{R}^{d_{x} \times d_{z}}$ are parameter matrices. These parameter matrices are unique per layer and attention head.</p>
<h2>3 Proposed Architecture</h2>
<h3>3.1 Relation-aware Self-Attention</h3>
<p>We propose an extension to self-attention to consider the pairwise relationships between input elements. In this sense, we model the input as a labeled, directed, fully-connected graph.</p>
<p>The edge between input elements $x_{i}$ and $x_{j}$ is represented by vectors $a_{i j}^{V}, a_{i j}^{K} \in \mathbb{R}^{d_{a}}$. The motivation for learning two distinct edge representations is that $a_{i j}^{V}$ and $a_{i j}^{K}$ are suitable for use in eq. (3) and eq. (4), respectively, without requiring additional linear transformations. These representations can be shared across attention heads. We use $d_{a}=d_{z}$.</p>
<p>We modify eq. (1) to propagate edge information to the sublayer output:</p>
<p>$$
z_{i}=\sum_{j=1}^{n} \alpha_{i j}\left(x_{j} W^{V}+a_{i j}^{V}\right)
$$</p>
<p>This extension is presumably important for tasks where information about the edge types selected by a given attention head is useful to downstream encoder or decoder layers. However, as explored in 4.3, this may not be necessary for machine translation.</p>
<p>We also, importantly, modify eq. (2) to consider edges when determining compatibility:</p>
<p>$$
e_{i j}=\frac{x_{i} W^{Q}\left(x_{j} W^{K}+a_{i j}^{K}\right)^{T}}{\sqrt{d_{z}}}
$$</p>
<p>The primary motivation for using simple addition to incorporate edge representations in eq. (3) and eq. (4) is to enable an efficient implementation described in 3.3.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example edges representing relative positions, or the distance between elements. We learn representations for each relative position within a clipping distance $k$. The figure assumes $2&lt;=k&lt;=n-4$. Note that not all edges are shown.</p>
<h3>3.2 Relative Position Representations</h3>
<p>For linear sequences, edges can capture information about the relative position differences between input elements. The maximum relative position we consider is clipped to a maximum absolute value of $k$. We hypothesized that precise relative position information is not useful beyond a certain distance. Clipping the maximum distance also enables the model to generalize to sequence lengths not seen during training. Therefore, we consider $2 k+1$ unique edge labels.</p>
<p>$$
\begin{aligned}
a_{i j}^{K} &amp; =w_{\text {clip }(j-i, k)}^{K} \
a_{i j}^{V} &amp; =w_{\text {clip }(j-i, k)}^{V} \
\operatorname{clip}(x, k) &amp; =\max (-k, \min (k, x))
\end{aligned}
$$</p>
<p>We then learn relative position representations $w^{K}=\left(w_{-k}^{K}, \ldots, w_{k}^{K}\right)$ and $w^{V}=\left(w_{-k}^{V}, \ldots, w_{k}^{V}\right)$ where $w_{i}^{K}, w_{i}^{V} \in \mathbb{R}^{d_{a}}$.</p>
<h3>3.3 Efficient Implementation</h3>
<p>There are practical space complexity concerns when considering edges between input elements, as noted by Veličković et al. (2017), which considers unlabeled graph inputs to an attention model.</p>
<p>For a sequence of length $n$ and $h$ attention heads, we reduce the space complexity of storing relative position representations from $O\left(h n^{2} d_{a}\right)$ to $O\left(n^{2} d_{a}\right)$ by sharing them across each heads. Additionally, relative position representations can be shared across sequences. Therefore, the overall self-attention space complexity increases from $O\left(b h n d_{z}\right)$ to $O\left(b h n d_{z}+n^{2} d_{a}\right)$. Given $d_{a}=d_{z}$, the size of the relative increase depends on $\frac{n}{b h}$.</p>
<p>The Transformer computes self-attention efficiently for all sequences, heads, and positions in
a batch using parallel matrix multiplication operations (Vaswani et al., 2017). Without relative position representations, each $e_{i j}$ can be computed using $b h$ parallel multiplications of $n \times d_{z}$ and $d_{z} \times n$ matrices. Each matrix multiplication computes $e_{i j}$ for all sequence positions, for a particular head and sequence. For any sequence and head, this requires sharing the same representation for each position across all compatibility function applications (dot products) with other positions.</p>
<p>When we consider relative positions the representations differ with different pairs of positions. This prevents us from computing all $e_{i j}$ for all pairs of positions in a single matrix multiplication. We also want to avoid broadcasting relative position representations. However, both issues can be resolved by splitting the computation of eq. (4) into two terms:</p>
<p>$$
e_{i j}=\frac{x_{i} W^{Q}\left(x_{j} W^{K}\right)^{T}+x_{i} W^{Q}\left(a_{i j}^{K}\right)^{T}}{\sqrt{d_{z}}}
$$</p>
<p>The first term is identical to eq. (2), and can be computed as described above. For the second term involving relative position representations, tensor reshaping can be used to compute $n$ parallel multiplications of $b h \times d_{z}$ and $d_{z} \times n$ matrices. Each matrix multiplication computes contributions to $e_{i j}$ for all heads and batches, corresponding to a particular sequence position. Further reshaping allows adding the two terms. The same approach can be used to efficiently compute eq. (3).</p>
<p>For our machine translation experiments, the result was a modest $7 \%$ decrease in steps per second, but we were able to maintain the same model and batch sizes on P100 GPUs as Vaswani et al. (2017).</p>
<h2>4 Experiments</h2>
<h3>4.1 Experimental Setup</h3>
<p>We use the tensor2tensor ${ }^{1}$ library for training and evaluating our model.</p>
<p>We evaluated our model on the WMT 2014 machine translation task, using the WMT 2014 English-German dataset consisting of approximately 4.5 M sentence pairs and the 2014 WMT English-French dataset consisting of approximately 36 M sentence pairs.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Position Information</th>
<th style="text-align: left;">EN-DE BLEU</th>
<th style="text-align: left;">EN-FR BLEU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Transformer (base)</td>
<td style="text-align: left;">Absolute Position Representations</td>
<td style="text-align: left;">26.5</td>
<td style="text-align: left;">38.2</td>
</tr>
<tr>
<td style="text-align: left;">Transformer (base)</td>
<td style="text-align: left;">Relative Position Representations</td>
<td style="text-align: left;">$\mathbf{2 6 . 8}$</td>
<td style="text-align: left;">$\mathbf{3 8 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">Transformer (big)</td>
<td style="text-align: left;">Absolute Position Representations</td>
<td style="text-align: left;">27.9</td>
<td style="text-align: left;">41.2</td>
</tr>
<tr>
<td style="text-align: left;">Transformer (big)</td>
<td style="text-align: left;">Relative Position Representations</td>
<td style="text-align: left;">$\mathbf{2 9 . 2}$</td>
<td style="text-align: left;">$\mathbf{4 1 . 5}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Experimental results for WMT 2014 English-to-German (EN-DE) and English-to-French (EN-FR) translation tasks, using newstest2014 test set.</p>
<p>For all experiments, we split tokens into a 32,768 word-piece vocabulary (Wu et al., 2016). We batched sentence pairs by approximate length, and limited input and output tokens per batch to 4096 per GPU. Each resulting training batch contained approximately 25,000 source and 25,000 target tokens.</p>
<p>We used the Adam optimizer (Kingma and Ba, 2014) with $\beta_{1}=0.9, \beta_{2}=0.98$, and $\epsilon=10^{-9}$. We used the same warmup and decay strategy for learning rate as Vaswani et al. (2017), with 4,000 warmup steps. During training, we employed label smoothing of value $\epsilon_{l s}=0.1$ (Szegedy et al., 2016). For evaluation, we used beam search with a beam size of 4 and length penalty $\alpha=0.6$ (Wu et al., 2016).</p>
<p>For our base model, we used 6 encoder and decoder layers, $d_{x}=512, d_{z}=64,8$ attention heads, 1024 feed forward inner-layer dimensions, and $P_{\text {dropout }}=0.1$. When using relative position encodings, we used clipping distance $k=16$, and used unique edge representations per layer and head. We trained for 100,000 steps on 8 K 40 GPUs, and did not use checkpoint averaging.</p>
<p>For our big model, we used 6 encoder and decoder layers, $d_{x}=1024, d_{z}=64,16$ attention heads, 4096 feed forward inner-layer dimensions, and $P_{\text {dropout }}=0.3$ for EN-DE and $P_{\text {dropout }}=0.1$ for EN-FR. When using relative position encodings, we used $k=8$, and used unique edge representations per layer. We trained for 300,000 steps on 8 P100 GPUs, and averaged the last 20 checkpoints, saved at 10 minute intervals.</p>
<h3>4.2 Machine Translation</h3>
<p>We compared our model using only relative position representations to the baseline Transformer (Vaswani et al., 2017) with sinusoidal position encodings. We generated baseline results to isolate the impact of relative position representations from any other changes to the underlying library and experimental configuration.</p>
<p>For English-to-German our approach improved performance over our baseline by 0.3 and 1.3 BLEU for the base and big configurations, respectively. For English-to-French it improved by 0.5 and 0.3 BLEU for the base and big configurations, respectively. In our experiments we did not observe any benefit from including sinusoidal position encodings in addition to relative position representations. The results are shown in Table 1.</p>
<h3>4.3 Model Variations</h3>
<p>We performed several experiments modifying various aspects of our model. All of our experiments in this section use the base model configuration without any absolute position representations. BLEU scores are calculated on the WMT English-to-German task using the development set, newstest2013.</p>
<p>We evaluated the effect of varying the clipping distance, $k$, of the maximum absolute relative position difference. Notably, for $k \geq 2$, there does not appear to be much variation in BLEU scores. However, as we use multiple encoder layers, precise relative position information may be able to propagate beyond the clipping distance. The results are shown in Table 2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$k$</th>
<th style="text-align: left;">EN-DE BLEU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">0</td>
<td style="text-align: left;">12.5</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">25.5</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: left;">25.8</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: left;">25.9</td>
</tr>
<tr>
<td style="text-align: left;">16</td>
<td style="text-align: left;">25.8</td>
</tr>
<tr>
<td style="text-align: left;">64</td>
<td style="text-align: left;">25.9</td>
</tr>
<tr>
<td style="text-align: left;">256</td>
<td style="text-align: left;">25.8</td>
</tr>
</tbody>
</table>
<p>Table 2: Experimental results for varying the clipping distance, $k$.</p>
<p>We also evaluated the impact of ablating each of the two relative position representations defined in section 3.1, $a_{i j}^{V}$ in eq. (3) and $a_{i j}^{K}$ in eq. (4). Including relative position representations solely when determining compatibility between elements may</p>
<p>be sufficient, but further work is needed to determine whether this is true for other tasks. The results are shown in Table 3.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$a_{i j}^{V}$</th>
<th style="text-align: left;">$a_{i j}^{K}$</th>
<th style="text-align: left;">EN-DE BLEU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">25.8</td>
</tr>
<tr>
<td style="text-align: left;">No</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">25.8</td>
</tr>
<tr>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">25.3</td>
</tr>
<tr>
<td style="text-align: left;">No</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">12.5</td>
</tr>
</tbody>
</table>
<p>Table 3: Experimental results for ablating relative position representations $a_{i j}^{V}$ and $a_{i j}^{K}$.</p>
<h2>5 Conclusions</h2>
<p>In this paper we presented an extension to selfattention that can be used to incorporate relative position information for sequences, which improves performance for machine translation.</p>
<p>For future work, we plan to extend this mechanism to consider arbitrary directed, labeled graph inputs to the Transformer. We are also interested in nonlinear compatibility functions to combine input representations and edge representations. For both of these extensions, a key consideration will be determining efficient implementations.</p>
<h2>References</h2>
<p>Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450 .</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 .</p>
<p>Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078 .</p>
<p>Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. 2017. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122 .</p>
<p>Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. 2016. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099 .</p>
<p>Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 .</p>
<p>Minh-Thang Luong, Hieu Pham, and Christopher D Manning. 2015. Effective approaches to attentionbased neural machine translation. arXiv preprint arXiv:1508.04025 .</p>
<p>Ankur P Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In Empirical Methods in Natural Language Processing.</p>
<p>Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. 2015. End-to-end memory networks. In Advances in neural information processing systems. pages $2440-2448$.</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems. pages 3104-3112.</p>
<p>Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. 2016. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 2818-2826.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems. pages 6000-6010.</p>
<p>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint arXiv:1710.10903 .</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144 .</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ The tensor2tensor library is available at https:// github.com/tensorflow/tensor2tensor.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3002 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3002</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3002</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-f86fb205cd4d85478e65304fc38bdf1e4bed2440</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f86fb205cd4d85478e65304fc38bdf1e4bed2440" target="_blank">Pre-trained Large Language Models Use Fourier Features to Compute Addition</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This paper shows that pre-trained LLMs add numbers using Fourier features -- dimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain, demonstrating that appropriate pre-trained representations can unlock the ability of Transformers to learn precise mechanisms for algorithmic tasks.</p>
                <p><strong>Paper Abstract:</strong> Pre-trained large language models (LLMs) exhibit impressive mathematical reasoning capabilities, yet how they compute basic arithmetic, such as addition, remains unclear. This paper shows that pre-trained LLMs add numbers using Fourier features -- dimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain. Within the model, MLP and attention layers use Fourier features in complementary ways: MLP layers primarily approximate the magnitude of the answer using low-frequency features, while attention layers primarily perform modular addition (e.g., computing whether the answer is even or odd) using high-frequency features. Pre-training is crucial for this mechanism: models trained from scratch to add numbers only exploit low-frequency features, leading to lower accuracy. Introducing pre-trained token embeddings to a randomly initialized model rescues its performance. Overall, our analysis demonstrates that appropriate pre-trained representations (e.g., Fourier features) can unlock the ability of Transformers to learn precise mechanisms for algorithmic tasks.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3002.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3002.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2-XL (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2-XL (pre-trained, fine-tuned on synthetic addition dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 48-layer, ~1.5B-parameter decoder-only Transformer (GPT-2-XL) pre-trained on language and then fine-tuned on a synthetic two-number addition dataset; the paper analyzes its internal computations for addition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2-XL (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer (GPT-2-XL), 48 layers, ~1.5 billion parameters; pre-trained on large language corpora and then fine-tuned on a synthetic addition dataset (pairs of integers ≤ 260 presented in natural-language templates).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Integer addition of two numbers (each ≤ 260) presented in natural-language templates; evaluated as next-token prediction for the sum.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Computes addition using Fourier features in the hidden/residual stream: low-frequency Fourier components (large period) encode an approximate magnitude (approximation), primarily contributed by MLP layers; high-frequency components (small period, e.g., periods 2, 2.5, 5, 10) implement modular classification (e.g., unit digit mod 2/5/10), primarily contributed by attention layers; the final logits are a superposition of a sparse set of Fourier components that together identify the exact sum.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Layer-wise Logit Lens shows progressive computation (approximate answer early, refined later); discrete Fourier transform of intermediate logits reveals sparse outlier components concentrated at specific frequencies (periods ~2, 2.5, 5, 10); token embeddings of numbers show corresponding Fourier components; targeted ablations via low-pass/high-pass filters (linear projection removing selected Fourier components from W^U outputs) produce predictable accuracy degradations consistent with the hypothesized separation (MLP ↦ approximation; Attn ↦ modular classification); combining top Fourier components reconstructs final prediction peaks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No strong contradictory evidence in the paper; large-period (very low-frequency) components do not perfectly localize the peak (i.e., they approximate magnitude only), requiring high-frequency components for exactness — discussed as a limitation of relying solely on low frequencies.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Fine-tuning (on synthetic addition dataset); targeted representational interventions (low-pass and high-pass Fourier filtering of MLP and attention outputs); probing via Logit Lens and DFT; ablation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Fine-tuning yields near-perfect performance (99.74% test accuracy). Applying filters: removing low-frequency components from MLPs severely harms approximation (many off-by-10/50/100 errors); removing high-frequency components from attention harms unit-digit/modular predictions (small errors < 6); combined removals cause much larger accuracy drops, confirming causal roles. See Table 1 for numeric effects.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Fine-tuned GPT-2-XL test accuracy 99.74% on held-out addition dataset. Ablation/filtering results (validation accuracy): None (no filtering) 0.9974; ATTN & MLP low-frequency removed 0.0594; ATTN low-frequency 0.9912; MLP low-frequency 0.3589; ATTN & MLP high-frequency 0.2708; ATTN high-frequency 0.7836; MLP high-frequency 0.9810.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>When low-frequency components are ablated (especially in MLPs) the model makes large-magnitude errors (off-by-10, 50, 100) — i.e., fails at magnitude approximation; when high-frequency components are ablated (especially in attention) the model mispredicts unit digits (small errors, typically < 6). Without pre-training inductive biases, the model tends to only learn approximate answers (off-by-one or off-by-digit errors).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct behavioral comparison to humans or exact symbolic algorithms provided; mechanistically the model decomposes the problem into magnitude approximation plus modular classification (an algorithmic-like decomposition analogous to separating coarse magnitude estimation and digit-wise modular arithmetic).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Large Language Models Use Fourier Features to Compute Addition', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3002.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3002.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2-XL (trained from scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2-XL (randomly initialized, trained on addition from scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same architecture as above but randomly initialized and trained only on the synthetic addition dataset; used to probe the role of pre-training and token embeddings in learning Fourier-based mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2-XL (trained-from-scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer (GPT-2-XL architecture), same capacity (~1.5B) but randomly initialized and trained from scratch on the addition dataset (no prior language pre-training).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Integer addition of two numbers (≤ 260) in the same supervised format as fine-tuned model.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Lacks the sparse high-frequency Fourier features observed in pretrained models; primarily uses low-frequency features (approximation) and fails to develop robust modular classification components, leading to systematic errors in exact digit prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Fourier analysis of intermediate logits and token embeddings shows absence of outlier high-frequency components; behavioral performance is worse (more off-by-one and similar errors); accuracy reported lower than pretrained model (94.44% test accuracy after convergence).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>None reported beyond poorer performance; adding pre-trained embeddings later demonstrates that the lack of Fourier features (rather than capacity limits) explains the failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Injection of pre-trained token embeddings (swap in or freeze pre-trained number token embeddings) as an inductive bias.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Introducing pre-trained token embeddings to a randomly initialized model rescues its performance (paper reports that pre-trained embeddings allow the network to learn the more precise Fourier-based mechanism; specific rescue accuracy is shown concretely in smaller models — see GPT-2-small results).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Trained-from-scratch GPT-2-XL test accuracy: 94.44% (compared to 99.74% for pretrained+fine-tuned). Error profile: more frequent off-by-one (unit-digit) errors and other exact-match failures.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Frequent exact-match failures (off-by-one/unit-digit mistakes); absence of modular (high-frequency) components causes the network to rely on coarse magnitude cues.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct comparison; highlights that pre-training supplies representational priors (Fourier features) that enable more algorithmic-like solutions versus purely supervised-from-scratch learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Large Language Models Use Fourier Features to Compute Addition', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3002.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3002.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2-small (with/without pre-trained embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2-small (124M parameters), training ablations with and without pre-trained token embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller 12-layer GPT-2 variant trained from scratch on the addition task shows poor performance unless given pre-trained number token embeddings, demonstrating the causal importance of pre-trained embeddings with Fourier structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2-small (trained-from-scratch and with pre-trained embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-2-small: decoder-only Transformer, ~124M parameters, 12 layers; experiments: trained-from-scratch on the same addition dataset, and trained-from-scratch with frozen pre-trained token embedding layer.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Integer addition of two numbers (≤ 260) in synthetic natural-language templates.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Pre-trained token embeddings encode Fourier features (periodic components for mod 2/5/10 etc.) that provide an inductive prior enabling the network to learn the modular-classification + magnitude-approximation mechanism; without these embeddings the model largely fails to develop high-frequency modular features.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Training curves across 5 seeds show GPT-2-small trained-from-scratch achieves ~53.95% test accuracy, while freezing pre-trained token embeddings raises test accuracy to 100% across seeds with faster convergence; Fourier analysis of token embeddings shows outlier frequency components at periods 2, 2.5, 5, 10.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>None reported; the intervention (freezing embeddings) provides causal evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Initialize (and optionally freeze) token embedding matrix with pre-trained embeddings containing Fourier features, while randomly initializing other weights.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Massive improvement: test accuracy goes from ~53.95% (random init embeddings) to 100% (frozen pre-trained token embeddings) across 5 random seeds; training converges faster.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GPT-2-small trained-from-scratch: test accuracy ≈ 53.95%; GPT-2-small with frozen pre-trained token embeddings: test accuracy = 100% (mean across 5 seeds, with low variance).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Without pre-trained embeddings, poor learning and inability to reliably predict exact sums (low accuracy); with embeddings, this failure mode disappears.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct comparison; result emphasizes the role of representational priors (embeddings) in enabling algorithmic-like decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Large Language Models Use Fourier Features to Compute Addition', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3002.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3002.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompted LLMs (GPT-J, Phi-2, GPT-3.5/4, PaLM-2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Various pre-trained LLMs evaluated under prompting (GPT-J 6B, Phi-2 2.7B, GPT-3.5, GPT-4, PaLM-2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source and closed-source pre-trained LLMs tested with prompting/in-context examples for the addition task; open-source models' internals analyzed show Fourier components, and closed-source behavior-consistent error patterns suggest similar mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Prompted LLMs: GPT-J (6B), Phi-2 (2.7B), GPT-3.5, GPT-4, PaLM-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source models: GPT-J (6B) and Phi-2 (2.7B) analyzed under 4-shot in-context learning; closed-source models (GPT-3.5, GPT-4, PaLM-2) evaluated behaviorally (0-shot). Models are large pretrained autoregressive Transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>In-context (few-shot) integer addition of two numbers (≤ 260) evaluated via prompting; closed-source models evaluated 0-shot due to instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Evidence that these pre-trained LLMs also rely on Fourier features during in-context addition: attention/MLP outputs show sparse Fourier components (open-source), and behavioral error modes (errors concentrated on multiples of 10) in closed-source models are consistent with modular-classification + approximation mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>For open-source models (Phi-2, GPT-J) the DFT of intermediate logits across last layers shows outlier Fourier components at periods ~2, 2.5, 5, 10 (similar to GPT-2-XL fine-tuned). Behavioral error analysis: absolute errors are multiples of 10 a large fraction of time (GPT-J: 93%; Phi-2: 73%); closed-source models show errors that are multiples of 10: GPT-3.5 and GPT-4: 100% of absolute errors; PaLM-2: 87% — suggesting modular-digit errors consistent with missing/faulty high-frequency components.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Closed-source models' internals cannot be inspected, so mechanism inference rests on behavioral error distributions rather than direct representations; thus evidence is suggestive but not definitive for closed-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>In-context learning (4-shot) for GPT-J and Phi-2; 0-shot for instruction-tuned closed-source models; Fourier analysis/ablation only possible for open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>4-shot prompting enables open-source models to perform addition but with error patterns dominated by multiples-of-10 failures; closed-source models perform better overall but still show modular-digit error clustering indicative of Fourier-like mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Behavioral error distributions reported as fraction of absolute errors that are multiples of 10: GPT-J (4-shot): 93%; Phi-2 (4-shot): 73%; GPT-3.5 (0-shot): 100%; GPT-4 (0-shot): 100%; PaLM-2 (0-shot): 87%. No single overall accuracy numbers reported for each model in the text, emphasis is on error-mode statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Errors concentrated on multiples of 10 (i.e., unit-digit mistakes), indicating models often get magnitude right but fail at modular/classification detail; for closed-source models the pattern is consistent but cannot be causally probed internally.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct comparison; behavioral patterns again align with an internal decomposition into magnitude estimation plus modular digit classification, analogous to digit-wise algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Large Language Models Use Fourier Features to Compute Addition', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3002.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3002.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fourier filtering (low/high-pass) ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Low-pass and high-pass Fourier-component filtering of intermediate module outputs (analysis intervention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A causal intervention method: project MLP/attention outputs onto the subspace whose projected W^U logits have selected Fourier components zeroed (i.e., remove low- or high-frequency Fourier components) to test their causal role in prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Fourier filtering ablation (analysis method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method applied to intermediate outputs (Attn^{(ℓ)}, MLP^{(ℓ)}) across layers: compute projection that minimizes L2 change while enforcing zero coefficients at a selected set of Fourier frequencies in W^U y (closed-form linear projection based on nullspace of BFW^U).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Diagnostic intervention applied to addition prediction pipeline to test causal importance of specific Fourier frequencies for integer addition.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Used to show causality: removing high-frequency components impairs modular (units-digit) classification, removing low-frequency components impairs magnitude approximation; different modules (MLP vs Attn) primarily rely on different frequency bands.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Applying low-pass/high-pass filters in experiments produced predictable degradations matching the hypothesized mappings (see Table 1). Single-frequency retention experiments (single-pass filters) show the contribution of specific periods (e.g., period 2 for mod-2 behavior). Error patterns after filtering match mechanistic expectations (off-by-10/50/100 when approximation components removed; small unit-digit errors when classification components removed).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No direct contradiction; the filter is linear projection constrained on W^U's Fourier decomposition, so conclusions are robust within that analysis framework.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Representational intervention: linear projection to remove selected Fourier components from the module outputs (low-pass or high-pass), and single-frequency retention experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Quantitatively large effects: e.g., removing low-frequency components from both ATTN & MLP dropped validation accuracy to 0.0594; removing ATTN high-frequency dropped accuracy to 0.7836; removing MLP low-frequency dropped accuracy to 0.3589, etc., demonstrating causal necessity of these components.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>See Table 1 (repeated): None 0.9974; ATTN & MLP low-frequency removed 0.0594; ATTN low-frequency 0.9912; MLP low-frequency 0.3589; ATTN & MLP high-frequency 0.2708; ATTN high-frequency 0.7836; MLP high-frequency 0.9810 (validation accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Ablating specific frequency bands produces characteristic failure modes (large magnitude errors when approximation bands removed; incorrect unit digits when classification bands removed), supporting interpretability claims.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>This is an analysis tool rather than a model; it enables causal claims about which representational frequencies map to algorithmic subroutines (magnitude vs modular operations), analogous to probing parts of a symbolic algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Large Language Models Use Fourier Features to Compute Addition', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Progress measures for grokking via mechanistic interpretability <em>(Rating: 2)</em></li>
                <li>Fourier circuits in neural networks: Unlocking the potential of large language models in mathematical reasoning and modular arithmetic <em>(Rating: 2)</em></li>
                <li>Harmonics of learning: Universal fourier features emerge in invariant networks <em>(Rating: 2)</em></li>
                <li>Transformers learn in-context by gradient descent <em>(Rating: 1)</em></li>
                <li>Language models implement simple word2vec-style vector arithmetic <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3002",
    "paper_id": "paper-f86fb205cd4d85478e65304fc38bdf1e4bed2440",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "GPT-2-XL (fine-tuned)",
            "name_full": "GPT-2-XL (pre-trained, fine-tuned on synthetic addition dataset)",
            "brief_description": "A 48-layer, ~1.5B-parameter decoder-only Transformer (GPT-2-XL) pre-trained on language and then fine-tuned on a synthetic two-number addition dataset; the paper analyzes its internal computations for addition.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2-XL (fine-tuned)",
            "model_description": "Decoder-only Transformer (GPT-2-XL), 48 layers, ~1.5 billion parameters; pre-trained on large language corpora and then fine-tuned on a synthetic addition dataset (pairs of integers ≤ 260 presented in natural-language templates).",
            "arithmetic_task_type": "Integer addition of two numbers (each ≤ 260) presented in natural-language templates; evaluated as next-token prediction for the sum.",
            "reported_mechanism": "Computes addition using Fourier features in the hidden/residual stream: low-frequency Fourier components (large period) encode an approximate magnitude (approximation), primarily contributed by MLP layers; high-frequency components (small period, e.g., periods 2, 2.5, 5, 10) implement modular classification (e.g., unit digit mod 2/5/10), primarily contributed by attention layers; the final logits are a superposition of a sparse set of Fourier components that together identify the exact sum.",
            "evidence_for_mechanism": "Layer-wise Logit Lens shows progressive computation (approximate answer early, refined later); discrete Fourier transform of intermediate logits reveals sparse outlier components concentrated at specific frequencies (periods ~2, 2.5, 5, 10); token embeddings of numbers show corresponding Fourier components; targeted ablations via low-pass/high-pass filters (linear projection removing selected Fourier components from W^U outputs) produce predictable accuracy degradations consistent with the hypothesized separation (MLP ↦ approximation; Attn ↦ modular classification); combining top Fourier components reconstructs final prediction peaks.",
            "evidence_against_mechanism": "No strong contradictory evidence in the paper; large-period (very low-frequency) components do not perfectly localize the peak (i.e., they approximate magnitude only), requiring high-frequency components for exactness — discussed as a limitation of relying solely on low frequencies.",
            "intervention_type": "Fine-tuning (on synthetic addition dataset); targeted representational interventions (low-pass and high-pass Fourier filtering of MLP and attention outputs); probing via Logit Lens and DFT; ablation experiments.",
            "effect_of_intervention": "Fine-tuning yields near-perfect performance (99.74% test accuracy). Applying filters: removing low-frequency components from MLPs severely harms approximation (many off-by-10/50/100 errors); removing high-frequency components from attention harms unit-digit/modular predictions (small errors &lt; 6); combined removals cause much larger accuracy drops, confirming causal roles. See Table 1 for numeric effects.",
            "performance_metrics": "Fine-tuned GPT-2-XL test accuracy 99.74% on held-out addition dataset. Ablation/filtering results (validation accuracy): None (no filtering) 0.9974; ATTN & MLP low-frequency removed 0.0594; ATTN low-frequency 0.9912; MLP low-frequency 0.3589; ATTN & MLP high-frequency 0.2708; ATTN high-frequency 0.7836; MLP high-frequency 0.9810.",
            "notable_failure_modes": "When low-frequency components are ablated (especially in MLPs) the model makes large-magnitude errors (off-by-10, 50, 100) — i.e., fails at magnitude approximation; when high-frequency components are ablated (especially in attention) the model mispredicts unit digits (small errors, typically &lt; 6). Without pre-training inductive biases, the model tends to only learn approximate answers (off-by-one or off-by-digit errors).",
            "comparison_to_humans_or_symbolic": "No direct behavioral comparison to humans or exact symbolic algorithms provided; mechanistically the model decomposes the problem into magnitude approximation plus modular classification (an algorithmic-like decomposition analogous to separating coarse magnitude estimation and digit-wise modular arithmetic).",
            "uuid": "e3002.0",
            "source_info": {
                "paper_title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-2-XL (trained from scratch)",
            "name_full": "GPT-2-XL (randomly initialized, trained on addition from scratch)",
            "brief_description": "Same architecture as above but randomly initialized and trained only on the synthetic addition dataset; used to probe the role of pre-training and token embeddings in learning Fourier-based mechanisms.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2-XL (trained-from-scratch)",
            "model_description": "Decoder-only Transformer (GPT-2-XL architecture), same capacity (~1.5B) but randomly initialized and trained from scratch on the addition dataset (no prior language pre-training).",
            "arithmetic_task_type": "Integer addition of two numbers (≤ 260) in the same supervised format as fine-tuned model.",
            "reported_mechanism": "Lacks the sparse high-frequency Fourier features observed in pretrained models; primarily uses low-frequency features (approximation) and fails to develop robust modular classification components, leading to systematic errors in exact digit prediction.",
            "evidence_for_mechanism": "Fourier analysis of intermediate logits and token embeddings shows absence of outlier high-frequency components; behavioral performance is worse (more off-by-one and similar errors); accuracy reported lower than pretrained model (94.44% test accuracy after convergence).",
            "evidence_against_mechanism": "None reported beyond poorer performance; adding pre-trained embeddings later demonstrates that the lack of Fourier features (rather than capacity limits) explains the failure modes.",
            "intervention_type": "Injection of pre-trained token embeddings (swap in or freeze pre-trained number token embeddings) as an inductive bias.",
            "effect_of_intervention": "Introducing pre-trained token embeddings to a randomly initialized model rescues its performance (paper reports that pre-trained embeddings allow the network to learn the more precise Fourier-based mechanism; specific rescue accuracy is shown concretely in smaller models — see GPT-2-small results).",
            "performance_metrics": "Trained-from-scratch GPT-2-XL test accuracy: 94.44% (compared to 99.74% for pretrained+fine-tuned). Error profile: more frequent off-by-one (unit-digit) errors and other exact-match failures.",
            "notable_failure_modes": "Frequent exact-match failures (off-by-one/unit-digit mistakes); absence of modular (high-frequency) components causes the network to rely on coarse magnitude cues.",
            "comparison_to_humans_or_symbolic": "No direct comparison; highlights that pre-training supplies representational priors (Fourier features) that enable more algorithmic-like solutions versus purely supervised-from-scratch learning.",
            "uuid": "e3002.1",
            "source_info": {
                "paper_title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-2-small (with/without pre-trained embeddings)",
            "name_full": "GPT-2-small (124M parameters), training ablations with and without pre-trained token embeddings",
            "brief_description": "A smaller 12-layer GPT-2 variant trained from scratch on the addition task shows poor performance unless given pre-trained number token embeddings, demonstrating the causal importance of pre-trained embeddings with Fourier structure.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2-small (trained-from-scratch and with pre-trained embeddings)",
            "model_description": "GPT-2-small: decoder-only Transformer, ~124M parameters, 12 layers; experiments: trained-from-scratch on the same addition dataset, and trained-from-scratch with frozen pre-trained token embedding layer.",
            "arithmetic_task_type": "Integer addition of two numbers (≤ 260) in synthetic natural-language templates.",
            "reported_mechanism": "Pre-trained token embeddings encode Fourier features (periodic components for mod 2/5/10 etc.) that provide an inductive prior enabling the network to learn the modular-classification + magnitude-approximation mechanism; without these embeddings the model largely fails to develop high-frequency modular features.",
            "evidence_for_mechanism": "Training curves across 5 seeds show GPT-2-small trained-from-scratch achieves ~53.95% test accuracy, while freezing pre-trained token embeddings raises test accuracy to 100% across seeds with faster convergence; Fourier analysis of token embeddings shows outlier frequency components at periods 2, 2.5, 5, 10.",
            "evidence_against_mechanism": "None reported; the intervention (freezing embeddings) provides causal evidence.",
            "intervention_type": "Initialize (and optionally freeze) token embedding matrix with pre-trained embeddings containing Fourier features, while randomly initializing other weights.",
            "effect_of_intervention": "Massive improvement: test accuracy goes from ~53.95% (random init embeddings) to 100% (frozen pre-trained token embeddings) across 5 random seeds; training converges faster.",
            "performance_metrics": "GPT-2-small trained-from-scratch: test accuracy ≈ 53.95%; GPT-2-small with frozen pre-trained token embeddings: test accuracy = 100% (mean across 5 seeds, with low variance).",
            "notable_failure_modes": "Without pre-trained embeddings, poor learning and inability to reliably predict exact sums (low accuracy); with embeddings, this failure mode disappears.",
            "comparison_to_humans_or_symbolic": "No direct comparison; result emphasizes the role of representational priors (embeddings) in enabling algorithmic-like decomposition.",
            "uuid": "e3002.2",
            "source_info": {
                "paper_title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Prompted LLMs (GPT-J, Phi-2, GPT-3.5/4, PaLM-2)",
            "name_full": "Various pre-trained LLMs evaluated under prompting (GPT-J 6B, Phi-2 2.7B, GPT-3.5, GPT-4, PaLM-2)",
            "brief_description": "Open-source and closed-source pre-trained LLMs tested with prompting/in-context examples for the addition task; open-source models' internals analyzed show Fourier components, and closed-source behavior-consistent error patterns suggest similar mechanisms.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Prompted LLMs: GPT-J (6B), Phi-2 (2.7B), GPT-3.5, GPT-4, PaLM-2",
            "model_description": "Open-source models: GPT-J (6B) and Phi-2 (2.7B) analyzed under 4-shot in-context learning; closed-source models (GPT-3.5, GPT-4, PaLM-2) evaluated behaviorally (0-shot). Models are large pretrained autoregressive Transformers.",
            "arithmetic_task_type": "In-context (few-shot) integer addition of two numbers (≤ 260) evaluated via prompting; closed-source models evaluated 0-shot due to instruction tuning.",
            "reported_mechanism": "Evidence that these pre-trained LLMs also rely on Fourier features during in-context addition: attention/MLP outputs show sparse Fourier components (open-source), and behavioral error modes (errors concentrated on multiples of 10) in closed-source models are consistent with modular-classification + approximation mechanism.",
            "evidence_for_mechanism": "For open-source models (Phi-2, GPT-J) the DFT of intermediate logits across last layers shows outlier Fourier components at periods ~2, 2.5, 5, 10 (similar to GPT-2-XL fine-tuned). Behavioral error analysis: absolute errors are multiples of 10 a large fraction of time (GPT-J: 93%; Phi-2: 73%); closed-source models show errors that are multiples of 10: GPT-3.5 and GPT-4: 100% of absolute errors; PaLM-2: 87% — suggesting modular-digit errors consistent with missing/faulty high-frequency components.",
            "evidence_against_mechanism": "Closed-source models' internals cannot be inspected, so mechanism inference rests on behavioral error distributions rather than direct representations; thus evidence is suggestive but not definitive for closed-source models.",
            "intervention_type": "In-context learning (4-shot) for GPT-J and Phi-2; 0-shot for instruction-tuned closed-source models; Fourier analysis/ablation only possible for open-source models.",
            "effect_of_intervention": "4-shot prompting enables open-source models to perform addition but with error patterns dominated by multiples-of-10 failures; closed-source models perform better overall but still show modular-digit error clustering indicative of Fourier-like mechanisms.",
            "performance_metrics": "Behavioral error distributions reported as fraction of absolute errors that are multiples of 10: GPT-J (4-shot): 93%; Phi-2 (4-shot): 73%; GPT-3.5 (0-shot): 100%; GPT-4 (0-shot): 100%; PaLM-2 (0-shot): 87%. No single overall accuracy numbers reported for each model in the text, emphasis is on error-mode statistics.",
            "notable_failure_modes": "Errors concentrated on multiples of 10 (i.e., unit-digit mistakes), indicating models often get magnitude right but fail at modular/classification detail; for closed-source models the pattern is consistent but cannot be causally probed internally.",
            "comparison_to_humans_or_symbolic": "No direct comparison; behavioral patterns again align with an internal decomposition into magnitude estimation plus modular digit classification, analogous to digit-wise algorithms.",
            "uuid": "e3002.3",
            "source_info": {
                "paper_title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Fourier filtering (low/high-pass) ablation",
            "name_full": "Low-pass and high-pass Fourier-component filtering of intermediate module outputs (analysis intervention)",
            "brief_description": "A causal intervention method: project MLP/attention outputs onto the subspace whose projected W^U logits have selected Fourier components zeroed (i.e., remove low- or high-frequency Fourier components) to test their causal role in prediction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Fourier filtering ablation (analysis method)",
            "model_description": "Method applied to intermediate outputs (Attn^{(ℓ)}, MLP^{(ℓ)}) across layers: compute projection that minimizes L2 change while enforcing zero coefficients at a selected set of Fourier frequencies in W^U y (closed-form linear projection based on nullspace of BFW^U).",
            "arithmetic_task_type": "Diagnostic intervention applied to addition prediction pipeline to test causal importance of specific Fourier frequencies for integer addition.",
            "reported_mechanism": "Used to show causality: removing high-frequency components impairs modular (units-digit) classification, removing low-frequency components impairs magnitude approximation; different modules (MLP vs Attn) primarily rely on different frequency bands.",
            "evidence_for_mechanism": "Applying low-pass/high-pass filters in experiments produced predictable degradations matching the hypothesized mappings (see Table 1). Single-frequency retention experiments (single-pass filters) show the contribution of specific periods (e.g., period 2 for mod-2 behavior). Error patterns after filtering match mechanistic expectations (off-by-10/50/100 when approximation components removed; small unit-digit errors when classification components removed).",
            "evidence_against_mechanism": "No direct contradiction; the filter is linear projection constrained on W^U's Fourier decomposition, so conclusions are robust within that analysis framework.",
            "intervention_type": "Representational intervention: linear projection to remove selected Fourier components from the module outputs (low-pass or high-pass), and single-frequency retention experiments.",
            "effect_of_intervention": "Quantitatively large effects: e.g., removing low-frequency components from both ATTN & MLP dropped validation accuracy to 0.0594; removing ATTN high-frequency dropped accuracy to 0.7836; removing MLP low-frequency dropped accuracy to 0.3589, etc., demonstrating causal necessity of these components.",
            "performance_metrics": "See Table 1 (repeated): None 0.9974; ATTN & MLP low-frequency removed 0.0594; ATTN low-frequency 0.9912; MLP low-frequency 0.3589; ATTN & MLP high-frequency 0.2708; ATTN high-frequency 0.7836; MLP high-frequency 0.9810 (validation accuracy).",
            "notable_failure_modes": "Ablating specific frequency bands produces characteristic failure modes (large magnitude errors when approximation bands removed; incorrect unit digits when classification bands removed), supporting interpretability claims.",
            "comparison_to_humans_or_symbolic": "This is an analysis tool rather than a model; it enables causal claims about which representational frequencies map to algorithmic subroutines (magnitude vs modular operations), analogous to probing parts of a symbolic algorithm.",
            "uuid": "e3002.4",
            "source_info": {
                "paper_title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Progress measures for grokking via mechanistic interpretability",
            "rating": 2,
            "sanitized_title": "progress_measures_for_grokking_via_mechanistic_interpretability"
        },
        {
            "paper_title": "Fourier circuits in neural networks: Unlocking the potential of large language models in mathematical reasoning and modular arithmetic",
            "rating": 2,
            "sanitized_title": "fourier_circuits_in_neural_networks_unlocking_the_potential_of_large_language_models_in_mathematical_reasoning_and_modular_arithmetic"
        },
        {
            "paper_title": "Harmonics of learning: Universal fourier features emerge in invariant networks",
            "rating": 2,
            "sanitized_title": "harmonics_of_learning_universal_fourier_features_emerge_in_invariant_networks"
        },
        {
            "paper_title": "Transformers learn in-context by gradient descent",
            "rating": 1,
            "sanitized_title": "transformers_learn_incontext_by_gradient_descent"
        },
        {
            "paper_title": "Language models implement simple word2vec-style vector arithmetic",
            "rating": 1,
            "sanitized_title": "language_models_implement_simple_word2vecstyle_vector_arithmetic"
        }
    ],
    "cost": 0.01673225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Pre-trained Large Language Models Use Fourier Features to Compute Addition</h1>
<p>Tianyi Zhou Deqing Fu Vatsal Sharan Robin Jia<br>Department of Computer Science<br>University of Southern California<br>Los Angeles, CA 90089<br>{tzhou029, deqingfu, vsharan, robinjia}@usc.edu</p>
<h4>Abstract</h4>
<p>Pre-trained large language models (LLMs) exhibit impressive mathematical reasoning capabilities, yet how they compute basic arithmetic, such as addition, remains unclear. This paper shows that pre-trained LLMs add numbers using Fourier features-dimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain. Within the model, MLP and attention layers use Fourier features in complementary ways: MLP layers primarily approximate the magnitude of the answer using low-frequency features, while attention layers primarily perform modular addition (e.g., computing whether the answer is even or odd) using high-frequency features. Pre-training is crucial for this mechanism: models trained from scratch to add numbers only exploit low-frequency features, leading to lower accuracy. Introducing pre-trained token embeddings to a randomly initialized model rescues its performance. Overall, our analysis demonstrates that appropriate pre-trained representations (e.g., Fourier features) can unlock the ability of Transformers to learn precise mechanisms for algorithmic tasks.</p>
<h1>Contents</h1>
<p>1 Introduction ..... 2
2 Problem Setup ..... 3
3 Language Models Solve Addition with Fourier Features ..... 3
3.1 Behavioral Analysis ..... 3
3.2 Fourier Features in MLP \&amp; Attention Outputs ..... 4
3.3 Fourier Features are Causally Important for Model Predictions ..... 7
4 Effects of Pre-training ..... 8
4.1 Fourier features in Token Embedding ..... 8
4.2 Contrasting Pre-trained Models with Models Trained from Scratch ..... 8
4.3 Fourier Features in Prompted Pre-Trained Models ..... 10
5 Related Work ..... 11
6 Conclusion ..... 12
A Formal Definition of Transformer and Logits in Fourier Space ..... 17
B Fourier Components Separation and Selection of $\tau$ ..... 20
C Does Fourier Features Generalize? ..... 23
C. 1 Token Embedding for Other LMs ..... 23
C. 2 Multiplication Task ..... 24
C. 3 Same Results for other format ..... 25
C. 4 Fourier Features in Other Pre-trained LM ..... 26
D Supporting Evidence For the Fourier Features ..... 27
E More Experiments on GPT-2-XL Trained from Scratch ..... 28
F Details of Experimental Settings ..... 29</p>
<h1>1 Introduction</h1>
<p>Mathematical problem solving has become a crucial task for evaluating the reasoning capabilities of large language models (LLMs) [HBK ${ }^{+} 21, \mathrm{CKB}^{+} 21, \mathrm{LBX}^{+} 24, \mathrm{FKL}^{+} 24$ ]. While LLMs exhibit impressive mathematical abilities [Ope23, Goo23b, Ant24, WLS17, TPSI21, BMR ${ }^{+} 20, \mathrm{FPG}^{+} 24$ ], it remains unclear how they perform even basic mathematical tasks. Do LLMs apply mathematical principles when solving math problems, or do they merely reproduce memorized patterns from the training data?</p>
<p>In this work, we unravel how pre-trained language models solve simple mathematical problems such as "Put together 15 and 93. Answer: __". Prior work has studied how Transformers, the underlying architecture of LLMs, perform certain mathematical tasks. Most studies [Cha23, GTLV22, vONR ${ }^{+} 22, \mathrm{BCW}^{+} 23, \mathrm{FCJS} 23, \mathrm{NCL}^{+} 23, \mathrm{GLL}^{+} 24, \mathrm{PBE}^{+} 22 \mathrm{~b}$ ] focus on Transformers with a limited number of layers or those trained from scratch; [HLV23] analyzes how the pre-trained GPT-2-small performs the greater-than task. Our work focuses on a different task from prior interpretability work-integer addition-and shows that pre-trained LLMs learn distinct mechanisms from randomly initialized Transformers.</p>
<p>In $\S 3$, we show that pre-trained language models compute addition with Fourier featuresdimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain. First, we analyze the behavior of pre-trained LLMs on the addition task after fine-tuning, which leads to almost perfect accuracy on the task. Rather than merely memorizing answers from the training data, the models progressively compute the final answer layer by layer. Next, we analyze the contributions of individual model components using Logit Lens [BFS ${ }^{+} 23$ ]. We observe that some components primarily approximate the answer-they promote all numbers close to the correct answer in magnitude-while other components primarily classify the answer modulo $m$ for various numbers $m$. Then, we use Fourier analysis to isolate features in the residual stream responsible for the low-frequency "approximation" and high-frequency "classification" subtasks. Identifying these features allows us to precisely ablate the ability of the model to perform either approximation or classification by applying a low-pass or high-pass filter, respectively, to the outputs of different model components. We find that MLP layers contribute primarily to approximation, whereas attention layers contribute primarily to classification.</p>
<p>In $\S 4$, we show that pre-training is crucial for learning this mechanism. The same network trained from scratch with random initialization not only shows no signs of Fourier features, but also has lower accuracy. We identify pre-trained token embeddings as a key source of inductive bias that help the pre-trained model learn a more precise mechanism for addition. Across the pretrained token embeddings of many different pre-trained models, Fourier analysis uncovers large magnitudes of components with periods 2,5 , and 10 . Introducing pre-trained token embeddings when training the model from scratch enables the model to achieve perfect test accuracy. Finally, we show that the same Fourier feature mechanism is present not only in models that were pretrained and then fine-tuned, but also in frozen pre-trained LLMs when prompted with arithmetic problems.</p>
<p>Overall, our work provides a mechanistic perspective on how pre-trained LLMs compute addition through the lens of Fourier analysis. It not only broadens the scope from only investigating few-layer Transformers trained to fit a particular data distribution to understanding LLMs as a whole, but also hints at how pre-training can lead to more precise model capabilities.</p>
<h1>2 Problem Setup</h1>
<p>Task and Dataset. We constructed a synthetic addition dataset for fine-tuning and evaluation purposes. Each example involves adding two numbers $\leq 260$, chosen because the maximum number that can be represented by a single token in the GPT-2-XL tokenizer is 520 . For each pair of numbers between 0 and 260 , we randomly sample one of five natural language question templates and combine it with the two numbers. The dataset is shuffled and then split into training ( $80 \%$ ), validation ( $10 \%$ ), and test ( $10 \%$ ) sets. More details are provided in Appendix F. In Appendix C.3, we show our that results generalize to a different dataset formatted with reverse Polish notation.</p>
<p>Model. Unless otherwise stated, all experiments focus on the pre-trained GPT-2-XL model that has been fine-tuned on our addition dataset. This model, which consists of 48 layers and approximately 1.5 billion parameters, learns the task almost perfectly, with an accuracy of $99.74 \%$ on the held-out test set. We examine other models in $\S 4.2$ and $\S 4.3$.</p>
<p>Transformers. We focus on decoder-only Transformer models [VSP ${ }^{+} 17$ ], which process text sequentially, token by token, from left to right. Each layer $\ell$ in the Transformer has an attention module with output Attn $^{(l)}$ and an MLP module with output $\mathrm{MLP}^{(l)}$. Their outputs are added together to create a continuous residual stream $h\left[\mathrm{ENO}^{+} 21\right]$, meaning that the token representation accumulates all additive updates within the residual stream, with the representation $h^{(\ell)}$ in the $\ell$-th layer given by:</p>
<p>$$
h^{(\ell)}=h^{(\ell-1)}+\operatorname{Attn}^{(\ell)}+\operatorname{MLP}^{(\ell)}
$$</p>
<p>The output embedding $W^{U}$ projects the residual stream to the space of the vocabulary; applying the softmax function then yields the model's prediction. We provide formal definitions in Appendix A.</p>
<h2>3 Language Models Solve Addition with Fourier Features</h2>
<p>In this section, we analyze the internal mechanisms of LLMs when solving addition tasks, employing a Fourier analysis framework. We first show that the model initially approximates the solution before iteratively converging to the correct answer (§3.1). We then show that the model refines its initial approximation by computing the exact answer modulo 2, 5, and 10, employing Fourier components of those same periods (§3.2). Finally, we demonstrate through targeted ablations that the identified Fourier components are causally important for the model's computational processes (§3.3). Specifically, we show that MLP layers primarily approximate the magnitude of the answer, using low-frequency features, while attention layers primarily perform modular addition using high-frequency components.</p>
<h3>3.1 Behavioral Analysis</h3>
<p>Our first goal is to understand whether the model merely memorizes and recombines pieces of information learned during training, or it performs calculations to add two numbers.</p>
<p>Extracting intermediate predictions. To elucidate how LLMs perform computations and progressively refine their outputs towards the correct answer, we extract model predictions at each layer from the residual stream. Let $L$ denote the number of layers. Using the Logit Lens method [BFS $\left.{ }^{+} 23\right]$, instead of generating predictions by computing logits $W^{U} h^{(L)}$, predictions are derived through $W^{U} h^{(\ell)}$ where $\ell \in[L]$. We compute the accuracy of the prediction using each intermediate state $h^{(\ell)}$. If the models merely retrieve and recombine pieces of information learned during</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) Visualization of predictions extracted from fine-tuned GPT-2-XL at intermediate layers. Between layers 20 and 30, the model's accuracy is low, but its prediction is often within 10 of the correct answer: the model first approximates the answer, then refines it. (b) Heatmap of the logits from different MLP layers for the running example, "Put together 15 and 93. Answer: 108". The y-axis represents the subset of the number space around the correct prediction, while the x-axis represents the layer index. The 33-rd layer performs mod 2 operations (favoring even numbers), while other layers perform other modular addition operations, such as mod 10 (45-th layer). Additionally, most layers allocate more weight to numbers closer to the correct answer, 108. (c) Analogous plot for attention layers. Nearly all attention modules perform modular addition.</p>
<p>Training, certain layers will directly map this information to predictions. For instance, [MEP23] demonstrates that there is a specific MLP module directly that maps a country to its capital.</p>
<p><strong>LLMs progressively compute the final answers.</strong> Figure 1a instead shows that the model progressively approaches the correct answer, layer by layer. The model is capable of making predictions that fall within the range of ±2 and ±10 relative to the correct answer in the earlier layers, compared to the exact-match accuracy. This observation implies that the Transformer's layer-wise processing structure is beneficial for gradually refining predictions through a series of transformations and updates applied to the token representations.</p>
<h3>3.2 Fourier Features in MLP &amp; Attention Outputs</h3>
<p><strong>Logits for MLP and attention have periodic structures.</strong> We now analyze how each MLP and attention module contributes to the final prediction. We transform the output of the attention and MLP output at layer ℓ into the token space using W<sup>U</sup>Attn<sup>(ℓ)</sup> and W<sup>U</sup>MLP<sup>(ℓ)</sup> at each layer, thereby obtaining the logits L for each MLP and attention module. We use the running example "Put together 15 and 93. Answer: 108" to demonstrate how the fine-tuned GPT-2-XL performs the computation. As illustrated in Figure 1b and Figure 1c, both the MLP and attention modules exhibit a periodic pattern in their logits across the output number space, e.g., the MLP in layer 33, outlined in green, promotes all numbers that are congruent to 108 mod 2 (in Figure 19 in the appendix, we zoom into such layers to make this clearer). Overall, we observe two distinct types of computation within these components. Some components predominantly assign a high weight to numbers around the correct answer, which we term <em>approximation</em>. Meanwhile, other components predominantly assign a high weight to all numbers congruent to a + b mod c for some constant c, which we term <em>classification</em>.</p>
<p><strong>Logits for MLP and attention are approximately sparse in the Fourier space.</strong> It is natural to transform the logits into Fourier space to gain a better understanding of their properties such as the periodic pattern. We apply the discrete Fourier transform to represent the logits as the sum of sine and cosine waves of different periods: the k-th component in Fourier space has period</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The intermediate logits in Fourier space. We annotate the top-10 outlier high-frequency Fourier components based on their magnitudes. $T$ stands for the period of that Fourier component. (a) The logits in Fourier space for the MLP output of the 33-rd layer, i.e., $\tilde{\mathcal{L}}<em _mathrm_Attn="\mathrm{Attn">{\mathrm{MLP}}^{(33)}$. The component with period 2 has the largest magnitude, aligning with the observations in Figures 1b and 19a. (b) The logits in Fourier space for the attention output of the 40-th layer, i.e., $\tilde{\mathcal{L}}</em>$. The components with periods 5 and 10 have the largest magnitude, aligning with the observations in Figures 1c and 19b.
$520 / k$ and frequency $k / 520$ (see Appendix A for more details). Let $\tilde{\mathcal{L}}$ denote the logits in Fourier space. Figure 2 shows the Fourier space logits for two layers from Figure 1b and Figure 1c that have a clear periodic pattern. We find that the high-frequency components in Fourier space, which we define as components with index greater or equal to 50, are approximately sparse as depicted in Figure 2. This observation aligns with $\left[\mathrm{NCL}^{+} 23\right]$, which found that a one-layer Transformer utilizes particular Fourier components within the Fourier space to solve the modular addition task.}}^{(40)</p>
<p>In Figure 3, we show that similar sparsity patterns in Fourier space hold across the entire dataset. We compute the logits in Fourier space for the last 15 layers, i.e., $\tilde{\mathcal{L}}<em _mathrm_MLP="\mathrm{MLP">{\mathrm{Attn}}^{(t)}$ and $\tilde{\mathcal{L}}</em>$ where $\ell \in[32,47]$, for all test examples and average them. We annotate the top-10 outlier high-frequency components based on their magnitude. The MLPs also exhibit some strong low-frequency components; the attention modules do not exhibit strong low-frequency components, only high-frequency components.}}^{(t)</p>
<p>Final logits are superpositions of these outlier Fourier components. The final logits, $\mathcal{L}^{(L)}$, are the sum of all $\mathcal{L}<em _mathrm_Attn="\mathrm{Attn">{\mathrm{MLP}}^{(l)}$ and $\mathcal{L}</em>$ based on their magnitudes and transfer them back to logits in number space via the inverse discrete Fourier transform (Figure 4a). The large-period (low-frequency) components approximate the magnitude while the small-period (high-frequency) components are crucial for modular addition. Figure 4b shows that aggregating these 5 waves is sufficient to predict the correct answer.}}^{(l)}$ across all layers $l \in[L]$. Figure 4 elucidates how these distinct Fourier components contribute to the final prediction, for the example "Put together 15 and 93. Answer: 108". We select the top-5 Fourier components of $\tilde{\mathcal{L}}^{(L)</p>
<p>Why is high-frequency classification helpful? The Fourier basis comprises both cos and sin waves (see Definition A.3). By adjusting the coefficients of cos and sin, the trained model can manipulate the phase of the logits in Fourier space (number shift in number space), aligning the peak</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Analysis of logits in Fourier space for all the test data across the last 15 layers. For both the MLP and attention modules, outlier Fourier components have periods around 2, 2.5, 5, and 10.
of the wave more closely with the correct answer. As shown in Figure 4a, consider a wave with a period of 2 . Here, the peak occurs at every even number in the number space, corresponding to the $\bmod 2$ task. In contrast, for components with a large period such as 520 , the model struggles to accurately position the peak at 108 (also see Figure 13 in the appendix for the plot of this component with period 520 in the full number space). This scenario can be interpreted as solving a "mod 520 " task-a classification task among 520 classes-which is challenging for the model to learn accurately. Nevertheless, even though the component with a period of 520 does not solve the "mod 520 " task precisely, it does succeed in assigning more weight to numbers near 108. The classification results from the high-frequency components can then provide finer-grained resolution to distinguish between all the numbers around 108 assigned a large weight by the lower frequencies. Due to this, the low-frequency components need not be perfectly aligned with the answer to make accurate predictions.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Visualization of how a sparse subset Fourier components can identify the correct answer. (a) Shows the top-5 Fourier components for the final logits. (b) Shows the sum of these top-5 Fourier components, highlighting how the cumulative effect identifies the correct answer, 108.</p>
<h1>3.3 Fourier Features are Causally Important for Model Predictions</h1>
<p>In the previous section, we demonstrated that there are outlier Fourier components in the logits generated by both the MLP and attention modules, as shown in Figure 3. We also illustrated that, in one example, the high-frequency components primarily approximate the magnitude, while the low-frequency components are crucial for modular addition tasks, as depicted in Figure 4. In this section, through an ablation study conducted across the entire test dataset, we show that both types of components are essential for correctly computing sums. Moreover, we reveal that the MLP layers primarily approximate the magnitude of the answer using low-frequency features, whereas the attention layers are responsible for modular addition using high-frequency features.</p>
<p>Filtering out Fourier components. To understand the role various frequency components play for the addition task, we introduce low-pass and high-pass filters $\mathcal{F}$. For an intermediate state $h$, and a set of frequencies $\Gamma=\left{\gamma_{1}, \ldots, \gamma_{k}\right}$, the filter $\mathcal{F}(h ; \Gamma)$ returns the vector $\widehat{h}$ that is closest in $L_{2}$ distance to $h$ subject to the constraint that the Fourier decomposition of $W^{U} \widehat{h}$ at every frequency $\gamma_{i}$ is 0 . We show in Appendix A that this has a simple closed-form solution involving a linear projection. We then apply either a low-pass filter by taking $\Gamma$ to be all the components whose frequencies are greater than the frequency of the $\tau$-th component for some threshold $\tau$ (i.e., removing high-frequency components), and a high-pass filter by taking $\Gamma$ to be all the components whose frequencies are less than the frequency of the $\tau$-th component (i.e., removing low-frequency components). As in the previous subsection, we take the high-frequency threshold $\tau=50$ for the following experiments (see Appendix B for more details).</p>
<p>Different roles of frequency components in approximation and classification tasks. We evaluated the fine-tuned GPT-2-XL model on the test dataset with different frequency filters applied to all of the output of MLP and attention modules. The results, presented in Table 1, indicate that removing low-frequency components from attention modules or high-frequency components from MLP modules does not impact performance. This observation suggests that attention modules are not crucial for approximation tasks, and MLP modules are less significant for classification tasks.</p>
<p>Eliminating high-frequency components from attention results in a noticeable decrease in accuracy. Furthermore, removing high-frequency components from both the attention and MLP modules simultaneously leads to an even greater reduction in accuracy. This finding corresponds with observations from Figure 1b,c and Figure 3, which indicate that both MLP and attention modules are involved in classification tasks due to the presence of high-frequency components in the logits. However, the approximation tasks are primarily performed by the MLP modules alone.</p>
<p>The errors induced by these ablations align with our mechanistic understanding. Ablating low-frequency parts of MLPs leads to off-by 10,50 , and 100 errors: the model fails to perform the approximation subtask, though it still accurately predicts the unit digit. Conversely, ablating highfrequency parts of attention leads to small errors less than 6 in magnitude: the model struggles to accurately predict the units digit, but it can still estimate the overall magnitude of the answer. See Figure 20 in the Appendix for more details. These observations validate our hypothesis that low-frequency components are crucial for approximation, while high-frequency components are vital for classification. The primary function of MLP modules is to approximate the magnitude of outcomes using low-frequency components, while the primary role of attention modules is to ensure accurate classification by determining the correct unit digit.</p>
<p>Table 1: Impact of Filtering out Fourier Components on Model Performance. Removing low-frequency components from attention modules (blue) or high-frequency components from MLP modules (red) does not impact performance</p>
<table>
<thead>
<tr>
<th>Module</th>
<th>Fourier Component Removed</th>
<th>Validation Loss</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>Without Filtering</td>
<td>0.0073</td>
<td>0.9974</td>
</tr>
<tr>
<td>ATTN &amp; MLP</td>
<td>Low-Frequency</td>
<td>4.0842</td>
<td>0.0594</td>
</tr>
<tr>
<td>ATTN</td>
<td>Low-Frequency</td>
<td>0.0352</td>
<td>0.9912</td>
</tr>
<tr>
<td>MLP</td>
<td>Low-Frequency</td>
<td>2.1399</td>
<td>0.3589</td>
</tr>
<tr>
<td>ATTN &amp; MLP</td>
<td>High-Frequency</td>
<td>1.8598</td>
<td>0.2708</td>
</tr>
<tr>
<td>ATTN</td>
<td>High-Frequency</td>
<td>0.5943</td>
<td>0.7836</td>
</tr>
<tr>
<td>MLP</td>
<td>High-Frequency</td>
<td>0.1213</td>
<td>0.9810</td>
</tr>
</tbody>
</table>
<h2>4 Effects of Pre-training</h2>
<p>The previous section shows that pre-trained LLMs leverage Fourier features to solve the addition problem. Now, we study where the models' reliance on Fourier features comes from. In this section, we demonstrate that LLMs learn Fourier features in the token embeddings for numbers during pre-training. These token embeddings are important for achieving high accuracy on the addition task: models trained from scratch achieve lower accuracy, but adding just the pre-trained token embeddings fixes this problem. We also show that pre-trained models leverage Fourier features not only when fine-tuned, but also when prompted.</p>
<h3>4.1 Fourier features in Token Embedding</h3>
<p>Number embedding exhibits approximate sparsity in the Fourier space. Let $W^{E} \in \mathbb{R}^{p \times D}$, where $p=521$ and $D$ is the size of the token embeddings, denote the token embedding for numbers. We apply the discrete Fourier transform to each column of $W^{E}$ to obtain a matrix $V \in \mathbb{R}^{p \times D}$, where each row represents a different Fourier component. Then we take the $L_{2}$ norm of each row to yield a $p$-dimensional vector. Each component $j$ in this vector measures the overall magnitude of the $j$-th Fourier component across all the token embedding dimensions. Figure 5a shows the magnitude of different Fourier components in the token embedding of GPT-2-XL. We see that the token embedding has outlier components whose periods are $2,2.5,5$, and 10. Therefore, similar to how the model uses different Fourier components to represent its prediction (as shown in Section 3.2), the token embeddings represent numbers with different Fourier components. Figure 14 in the Appendix shows that the token embeddings of other pre-trained models have similar patterns the Fourier space. This suggests that Fourier features are a common attribute in the token embedding of pre-trained LLMs. In Figure 5b, we use t-SNE and $k$-means to visualize the token embedding clustering. We can see that numbers cluster not only by magnitude but also by their multiples of 10.</p>
<h3>4.2 Contrasting Pre-trained Models with Models Trained from Scratch</h3>
<p>To understand the necessity of Fourier features for the addition problem, we trained the GPT-2-XL model from scratch on the addition task with random initialization. After convergence, it achieved only $94.44\%$ test accuracy (recall that the fine-tuned GPT-2-XL model achieved $99.74\%$ accuracy).</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: (a) Number embedding in Fourier space for fine-tuned GPT-2-XL. $T$ stands for the period of that Fourier component.(b) Visualization of token embedding clustering of GPT-2 using T-SNE and $k$-means with 10 clusters. The numbers are clustered based on their magnitude and whether they are multiples of 10 .
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Visualization of the logits in Fourier space on the test dataset from the last 15 layers for the GPT-2-XL model trained from scratch. For both the MLP and attention modules, there are no outlier Fourier components, in contrast with the clear outlier components in the fine-tuned model (Figure 3).</p>
<p>Fourier features are learned during pre-training. Figure 6 shows that there are no Fourier features in the intermediate logits of the GPT-2-XL model trained from scratch on the addition task. Furthermore, Figure 7a shows that the token embeddings also have no Fourier features. Without leveraging Fourier features, the model merely approximates the correct answer without performing modular addition, resulting in frequent off-by-one errors between the prediction and the correct answer (see details in Figure 22).</p>
<p>Pre-trained token embeddings improve model training. We also trained GPT-2-small, with 124 million parameters and 12 layers, from scratch on the addition task. GPT-2-small often struggles with mathematical tasks [MMV+22]. This model achieved a test accuracy of only $53.95 \%$</p>
<p>after convergence. However, when we freeze the token embedding layer and randomly initialize the weights for all other layers before training on the addition task, the test accuracy increases to $100 \%$, with a significantly faster convergence rate. This outcome was consistently observed across five different random seeds, as illustrated in Figure 7b. This demonstrates that given the number embeddings with Fourier features, the model can effectively learn to leverage these features to solve the addition task.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: (a) The number embedding in Fourier space for GPT-2-XL trained from scratch. There are no high-frequency outlier components, in contrast with the pre-trained embeddings (Figure 5a). (b) Validation accuracy of GPT-2-small trained from scratch either with or without pre-trained token embeddings. We show the mean and the standard deviation of the validation accuracy across 5 random seeds. GPT-2-small with pre-trained token embedding consistently achieves $100 \%$ accuracy, while GPT-2-small without pre-trained token embedding only achieves less than $60 \%$ accuracy.</p>
<h1>4.3 Fourier Features in Prompted Pre-Trained Models</h1>
<p>Finally, we ask whether larger language models use similar Fourier features during prompting.
Pre-trained LLMs use Fourier features to compute addition during in-context learning. We first test on the open-source models GPT-J [WK21] with 6B parameters, and Phi-2 [JBA ${ }^{+} 23$ ] with 2.7B parameters on the test dataset. Without in-context learning, the model cannot perform addition tasks. Therefore, we use 4 -shot in-context learning to test its performance. Their absolute errors are predominantly multiples of 10: $93 \%$ of the time for GPT-J, and $73 \%$ for Phi-2 . Using the Fourier analysis framework proposed in Section 3.2, we demonstrate that for Phi-2 and GPT-J, the outputs of MLP and attention modules exhibit approximate sparsity in Fourier space across the last 15 layers (Figure 8 and Figure 18). This evidence strongly suggests that these models leverage Fourier features to compute additions.</p>
<p>Closed-source models exhibit similar behavior. We study the closed-source models GPT-3.5 [Ope22], GPT-4 [Ope23], and PaLM-2 [Goo23a]. While we cannot analyze their internal representations, we can study whether their behavior on addition problems is consistent with reliance on Fourier features. Since closed-source LLMs are instruction tuned and perform well without incontext learning, we conduct error analysis with 0 -shot. Most absolute errors by these models are also multiples of 10: $100 \%$ of the time for GPT-3.5 and GPT-4, and $87 \%$ for PaLM-2. The similarity in error distribution to that of open-source models leads us to hypothesize that Fourier features play a critical role in their computational mechanism.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: For Phi-2 (4-shot), we analyzed the logits in Fourier space for all the test data across the last 15 layers. For both the MLP and attention modules, the outlier Fourier components have periods around 2, 2.5, 5, and 10, similar to the fine-tuned GPT-2-XL logits (Figure 3).</p>
<h1>5 Related Work</h1>
<p>Learning mathematical tasks. Previous studies primarily explore what pre-trained LMs can achieve on arithmetic tasks, with less emphasis on the underlying mechanisms [NJL21, QWL ${ }^{+} 22$ ]. For instance, [LSL $\left.{ }^{+} 23\right]$ demonstrates that small Transformer models can effectively learn arithmetic by altering the question format and utilizing a scratchpad method [NAGA $\left.{ }^{+} 21\right]$. [HLV23] identifies activation patterns for the "greater-than" operation in GPT-2, and [Cha23] focuses on the enumeration and selection processes in GCD computation. In this paper, we dive into the specific roles of MLP and attention layers in solving mathematical tasks. Our research analyzes these components' distinct contributions to integer addition tasks.</p>
<p>Mechanisms of pre-trained LMs. Recent studies have significantly advanced our understanding of the underlying mechanisms of pre-trained Transformer models. For instance, research on "skill neurons" by [WWZ $\left.{ }^{+} 22\right]$ and "knowledge neurons" by [DDH $\left.{ }^{+} 21\right]$ underscores the development of specialized neural components that encode task-specific capabilities or hold explicit factual information in the pre-trained LMs, enhancing model performance on related tasks. [MEP23] and [GCWG22] discuss how MLPs and FFNs transform and update token representations for general language tasks. In contrast, we show that the pre-trained LMs use multiple layers to compute addition by combining the results of approximation and classification. Additionally, [ZL23] demonstrated the capacity of GPT-2 to consolidate similar information through pre-training in the model weights, which aligns with our observations on the importance of pre-training in developing effective number embedding and arithmetic computation strategies in LMs.</p>
<p>Fourier features in Neural Networks. Fourier features are commonly observed in image models, particularly in the early layers of vision models [OF97, OCS ${ }^{+} 20$, FS24]. These features enable the model to detect edges, textures, and other spatial patterns effectively. Recently, Fourier features have been noted in networks trained for tasks that allow cyclic wraparound, such as modular addition $\left[\mathrm{NCL}^{+} 23, \mathrm{MEO}^{+} 23\right]$, general group compositions [CCN23], or invariance to cyclic translations [SSOH22]. [NCL $\left.{ }^{+} 23\right]$ demonstrates that learning Fourier features can induce 'grokking' $\left[\mathrm{PBE}^{+} 22 \mathrm{a}\right]$. Furthermore, [MHKS23] provides a mathematical framework explaining the emergence of Fourier features when the network exhibits invariance to a finite group. We extend these</p>
<p>insights by observing Fourier features in tasks that do not involve cyclic wraparound. [TSM ${ }^{+20}$ ] found that by selecting problem-specific Fourier features, the performance of MLPs can be improved on a computer vision-related task.</p>
<h1>6 Conclusion</h1>
<p>In this paper, we provide a comprehensive analysis of how pre-trained LLMs compute numerical sums, revealing a nuanced interplay of Fourier features within their architecture. Our findings demonstrate that LLMs do not simply memorize answers from training data but actively compute solutions through a combination of approximation and classification processes encoded in the frequency domain of their hidden states. Specifically, MLP layers contribute to approximating the magnitude of sums, while attention layers contribute to modular operations.</p>
<p>Our work also shows that pre-training plays a critical role in equipping LLMs with the Fourier features necessary for executing arithmetic operations. Models trained from scratch lack these crucial features and achieve lower accuracy; introducing pre-trained token embeddings greatly improves their convergence rate and accuracy. This insight into the arithmetic problem-solving capabilities of LLMs through Fourier features sets the stage for potential modifications to training approaches. By imposing specific constraints on model training, we could further enhance the ability of LLMs to learn and leverage these Fourier features, thereby improving their performance in mathematical tasks.</p>
<h2>Acknowledgments</h2>
<p>DF and RJ were supported by a Google Research Scholar Award. RJ was also supported by an Open Philanthropy research grant. VS was supported by NSF CAREER Award CCF-2239265 and an Amazon Research Award. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of the funding agencies.</p>
<h1>References</h1>
<p>[Ant24] Anthropic. The claude 3 model family: Opus, sonnet, haiku. 2024.
[BCW ${ }^{+}$23] Yu Bai, Fan Chen, Haiquan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. ArXiv, abs/2306.04637, 2023.
[BFS ${ }^{+}$23] Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. Eliciting latent predictions from transformers with the tuned lens. arXiv preprint arXiv:2303.08112, 2023.
[BMR ${ }^{+}$20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[CCN23] Bilal Chughtai, Lawrence Chan, and Neel Nanda. A toy model of universality: Reverse engineering how networks learn group operations. In International Conference on Machine Learning, pages 6243-6267. PMLR, 2023.
[Cha23] François Charton. Can transformers learn the greatest common divisor? arXiv preprint arXiv:2308.15594, 2023.
[CKB ${ }^{+}$21] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021.
$\left[\mathrm{DDH}^{+}\right.$21] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained transformers. arXiv preprint arXiv:2104.08696, 2021.
$\left[\mathrm{ENO}^{+}\right.$21] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. https://transformercircuits.pub/2021/framework/index.html.
[FCJS23] Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. Transformers learn higherorder optimization methods for in-context learning: A study with linear models, 2023.
[FKL ${ }^{+}$24] Deqing Fu, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani Yogatama, Robin Jia, and Willie Neiswanger. Isobench: Benchmarking multimodal foundation models on isomorphic representations, 2024.
[FPG ${ }^{+}$24] Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Petersen, and Julius Berner. Mathematical capabilities of chatgpt. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>[FS24] Pierre-Étienne Fiquet and Eero Simoncelli. A polar prediction model for learning to represent visual transformations. Advances in Neural Information Processing Systems, 36, 2024.
[GCWG22] Mor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav Goldberg. Transformer feedforward layers build predictions by promoting concepts in the vocabulary space. arXiv preprint arXiv:2203.14680, 2022.
[GLL ${ }^{+}$24] Jiuxiang Gu, Chenyang Li, Yingyu Liang, Zhenmei Shi, Zhao Song, and Tianyi Zhou. Fourier circuits in neural networks: Unlocking the potential of large language models in mathematical reasoning and modular arithmetic, 2024.
[Goo23a] Google. Palm 2 technical report, 2023.
[Goo23b] Gemini Team Google. Gemini: A family of highly capable multimodal models, 2023.
[GTLV22] Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. ArXiv, abs/2208.01066, 2022.
$\left[\mathrm{HBK}^{+}\right.$21] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021.
[HLV23] Michael Hanna, Ollie Liu, and Alexandre Variengien. How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. arXiv preprint arXiv:2305.00586, 2023.
$\left[\mathrm{JBA}^{+}\right.$23] Mojan Javaheripi, Sebastien Bubeck, Marah Abdin, Jyoti Anejaand Caio Cesar Teodoro Mendes, Allie Del Giorno Weizhu Chen, Ronen Eldan, Sivakanth Gopi, Suriya Gunasekar, Piero Kauffmann, Yin Tat Lee, Yuanzhi L, Anh Nguyen, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Michael Santacroce, Harkirat Singh Behl, Adam Taumann Kalai, Xin Wang, Rachel Ward, Philipp Witte, Cyril Zhang, and Yi Zhang. Phi-2: The surprising power of small language models, 2023.
[LBX ${ }^{+}$24] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, 2024.
[LSL ${ }^{+}$23] Nayoung Lee, Kartik Sreenivasan, Jason D Lee, Kangwook Lee, and Dimitris Papailiopoulos. Teaching arithmetic to small transformers. arXiv preprint arXiv:2307.03381, 2023.
[MEO ${ }^{+}$23] Depen Morwani, Benjamin L Edelman, Costin-Andrei Oncescu, Rosie Zhao, and Sham Kakade. Feature emergence via margin maximization: case studies in algebraic tasks. arXiv preprint arXiv:2311.07568, 2023.
[MEP23] Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. Language models implement simple word2vec-style vector arithmetic. arXiv preprint arXiv:2305.16130, 2023.
[MHKS23] Giovanni Luca Marchetti, Christopher Hillar, Danica Kragic, and Sophia Sanborn. Harmonics of learning: Universal fourier features emerge in invariant networks. arXiv preprint arXiv:2312.08550, 2023.</p>
<p>[MMV ${ }^{+}$22] Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan. Numglue: A suite of fundamental yet challenging mathematical reasoning tasks. arXiv preprint arXiv:2204.05660, 2022.
[NAGA ${ }^{+}$21] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.
[NCL ${ }^{+}$23] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217, 2023.
[NJL21] Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. Investigating the limitations of transformers with simple arithmetic tasks. arXiv preprint arXiv:2102.13019, 2021.
[OCS ${ }^{+}$20] Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. An overview of early vision in inceptionv1. Distill, 5(4):e00024-002, 2020.
[OF97] Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision research, 37(23):3311-3325, 1997.
[Ope22] OpenAI. Introducing ChatGPT. https://openai.com/blog/chatgpt, 2022. Accessed: 2023-09-10.
[Ope23] OpenAI. Gpt-4 technical report, 2023.
$\left[\mathrm{PBE}^{+}\right.$22a] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022.
[PBE ${ }^{+}$22b] Alethea Power, Yuri Burda, Harrison Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. ArXiv, abs/2201.02177, 2022.
[QWL ${ }^{+}$22] Jing Qian, Hong Wang, Zekun Li, Shiyang Li, and Xifeng Yan. Limitations of language models in arithmetic and symbolic induction. arXiv preprint arXiv:2208.05051, 2022.
[SSOH22] Sophia Sanborn, Christian Shewmake, Bruno Olshausen, and Christopher Hillar. Bispectral neural networks. arXiv preprint arXiv:2209.03416, 2022.
[TPSI21] Avijit Thawani, Jay Pujara, Pedro A Szekely, and Filip Ilievski. Representing numbers in nlp: a survey and a vision. arXiv preprint arXiv:2103.13136, 2021.
[TSM ${ }^{+}$20] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in neural information processing systems, 33:7537-7547, 2020.
[vONR ${ }^{+}$22] Johannes von Oswald, Eyvind Niklasson, E. Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn incontext by gradient descent. In International Conference on Machine Learning, 2022.</p>
<p>[VSP ${ }^{+}$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv:1706.03762, 2017.
[WK21] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax, May 2021.
[WLS17] Yan Wang, Xiaojiang Liu, and Shuming Shi. Deep neural solver for math word problems. In Proceedings of the 2017 conference on empirical methods in natural language processing, pages 845-854, 2017.
[WWZ ${ }^{+}$22] Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, and Juanzi Li. Finding skill neurons in pre-trained transformer-based language models. arXiv preprint arXiv:2211.07349, 2022.
[ZL23] Zeyuan Allen Zhu and Yuanzhi Li. Physics of language models: Part 3.1, knowledge storage and extraction. arXiv preprint arXiv:2309.14316, 2023.</p>
<h1>Appendix</h1>
<p>Roadmap. In Appendix A, we introduce some formal definitions that used in our main content. In Appendix B, we show why we separate the Fourier components into the high-frequency part and the low-frequency part and why we choose $\tau$ to be 50 . In Appendix C, we show our observation generalizes to another format of dataset, another arithmetic task and other models. In Appendix D, we provide more evidence that shows the Fourier features in the model when computing addition. In Appendix E, we provide more evidence that shows the GPT-2-XL trained from scratch does not use Fourier feature to solve the addition task. In Appendix F, we give the details of our experimental settings.</p>
<h2>A Formal Definition of Transformer and Logits in Fourier Space</h2>
<p>We first introduce the formal definition of the Transformer structure that we used in this paper.
Definition A. 1 (Transformer). An autoregressive Transformer language model $G: \mathcal{X} \rightarrow \mathcal{Y}$ over vocabulary Vocab maps a token sequence $x=\left[x_{1}, \ldots, x_{N}\right] \in \mathcal{X}, x_{t} \in$ Vocab to a probability distribution $y \in \mathcal{Y} \subset \mathbb{R}^{|\text {Vocab }|}$ that predicts next-token continuations of $x$. Within the Transformer, the $i$-th token is embedded as a series of hidden state vectors $h_{t}^{(\ell)}$, beginning with $h_{t}^{(0)}=\mathrm{emb}\left(x_{t}\right)+\operatorname{pos}(i) \in \mathbb{R}^{D}$. Let $W^{U} \in \mathbb{R}^{|\operatorname{Vocab}| \times D}$ denote the output embedding. The final output $y=\operatorname{softmax}\left(W^{U}\left(h_{N}^{(L)}\right)\right)$ is read from the last hidden state. In the autoregressive case, tokens only draw information from past tokens:</p>
<p>$$
h_{t}^{(\ell)}=h_{t}^{(\ell-1)}+\operatorname{Attn}<em t="t">{t}^{(\ell)}+\operatorname{MLP}</em>
$$}^{(\ell)</p>
<p>where</p>
<p>$$
\operatorname{Attn}<em 1="1">{t}^{(\ell)}:=\operatorname{Attn}^{(\ell)}\left(h</em>}^{(\ell-1)}, h_{2}^{(\ell-1)}, \ldots, h_{t}^{(\ell-1)}\right) \quad \text { and } \quad \operatorname{MLP<em t="t">{t}^{(\ell)}:=\operatorname{MLP}</em>}^{(\ell)}\left(\operatorname{Attn<em t="t">{t}^{(\ell)}, h</em>\right)
$$}^{(\ell-1)</p>
<p>In this paper, we only consider the output tokens to be numbers. Hence, we have the unembedding matrix $W^{U} \in \mathbb{R}^{p \times D}$, where $p$ is the size of the number space. As we are given the length- $N$ input sequences and predict the $(N+1)$-th, we only consider $h_{N}^{(\ell)}=h_{N}^{(\ell-1)}+\operatorname{Attn}<em N="N">{N}^{(\ell)}+\operatorname{MLP}</em>$. For simplicity, we ignore the subscript $N$ in the following paper, so we get Eq. (1).}^{(\ell)</p>
<p>Definition A. 2 (Intermediate Logits). Let $\mathcal{L}<em _mathrm_MLP="\mathrm{MLP">{\text {Attn }}^{(\ell)}:=W^{U} \operatorname{Attn}^{(\ell)}$ denote the intermediate logits of the attention module at the $\ell$-th layer. Let $\mathcal{L}</em>$.}}^{(\ell)}:=W^{U} \mathrm{MLP}^{(\ell)}$ denote the intermediate logits of the MLP module at the $\ell$-th layer. Let $\mathcal{L}^{(\ell)}:=W^{U} h^{(\ell)}$ denote the logits on intermediate state $h^{(\ell)</p>
<p>Throughout the model, $h$ undergoes only additive updates (Eq. (1)), creating a continuous residual stream $\left[\mathrm{ENO}^{+} 21\right]$, meaning that the token representation $h$ accumulates all additive updates within the residual stream up to layer $t$.</p>
<p>To analyze the logits in Fourier space, we give the formal definition of the Fourier basis as follows:</p>
<p>Definition A. 3 (Fourier Basis). Let $p$ denote the size of the number space. Let $\overrightarrow{\mathrm{x}}:=(0,1, \ldots,(p-1))$.</p>
<p>Let $\omega_{k}:=\frac{2 \pi k}{p-1}$. We denote the normalized Fourier basis $F$ as the $p \times p$ matrix:</p>
<p>$$
F:=\left[\begin{array}{c}
\sqrt{\frac{1}{p-1} \cdot \overrightarrow{\mathbf{1}}} \
\sqrt{\frac{2}{p-1}} \cdot \sin \left(\omega_{1} \overrightarrow{\mathbf{x}}\right) \
\sqrt{\frac{2}{p-1}} \cdot \cos \left(\omega_{1} \overrightarrow{\mathbf{x}}\right) \
\sqrt{\frac{2}{p-1}} \cdot \sin \left(\omega_{2} \overrightarrow{\mathbf{x}}\right) \
\vdots \
\sqrt{\frac{2}{p-1}} \cdot \cos \left(\omega_{(p-1) / 2} \overrightarrow{\mathbf{x}}\right)
\end{array}\right] \in \mathbb{R}^{p \times p}
$$</p>
<p>The first component $F[0]$ is defined as a constant component. For $i \in[0, p-1], F[i]$ is defined as the $k$-th component in Fourier space, where $k=\left\lfloor\frac{i+1}{2}\right\rfloor$. The frequency of the $k$-th component is $f_{k}:=\frac{k}{p-1}$. The period of the $k$-th component is $T_{k}:=\frac{p-1}{k}$</p>
<p>We can compute the discrete Fourier transform under that Fourier basis as follows:
Remark A. 4 (Discrete Fourier transformer (DFT) and inverse DFT). We can transform any logits $u \in \mathbb{R}^{p}$ to Fourier space by computing $\widehat{u}=F \cdot u$. We can transform $\widehat{u}$ back to $u$ by $u=F^{\top} \cdot \widehat{u}$</p>
<p>Next, we define the logits in Fourier space.
Definition A. 5 (Logits in Fourier Space). Let $\mathcal{L}^{(L)}, \mathcal{L}<em _mathrm_MLP="\mathrm{MLP">{\mathrm{Attn}}^{(\ell)}$ and $\mathcal{L}</em>$. The logits of the MLP and attention modules in Fourier space are defined as:}}^{(\ell)}$ denote the logits (Definition A.2). The output logits before softmax in Fourier space is defined as: $\widetilde{\mathcal{L}}^{(L)}=F \cdot \mathcal{L}^{(L)</p>
<p>$$
\tilde{\mathcal{L}}<em _mathrm_Attn="\mathrm{Attn">{\mathrm{Attn}}^{(\ell)}=F \cdot \mathcal{L}</em>}}^{(\ell)} \quad \text { and } \quad \tilde{\mathcal{L}<em _mathrm_MLP="\mathrm{MLP">{\mathrm{MLP}}^{(\ell)}=F \cdot \mathcal{L}</em>
$$}}^{(\ell)</p>
<p>We ignore the first elements in $\widetilde{\mathcal{L}}^{(L)}, \widetilde{\mathcal{L}}<em _mathrm_MLP="\mathrm{MLP">{\mathrm{Attn}}^{(\ell)}$ and $\widetilde{\mathcal{L}}</em>$ for the Fourier analysis in this paper as they are the constant terms. Adding a constant to the logits will not change the prediction.}}^{(\ell)</p>
<p>Let $\tau \in \mathbb{R}$ denote a constant threshold. The low-frequency components for the logits in Fourier space are defined as $\widetilde{\mathcal{L}}^{(\ell)}[1: 2 \tau]$. The high-frequency components for the logits in Fourier space are defined as $\widetilde{\mathcal{L}}^{(\ell)}[2 \tau:]$. For the following analysis, we choose $\tau=50$ (the specific choice of $\tau=50$ is explained in Appendix B).</p>
<p>Next, we propose the formal definition of low-pass/high-pass filter that is used in the following ablation study.</p>
<p>Definition A. 6 (Loss-pass / High-pass Filter). Let $x \in \mathbb{R}^{D}$ denote the output of MLP or attention modules. Let $F$ denote the Fourier Basis (Definition A.3). Let $\tau \in R$ denote the frequency threshold. Let $W^{U} \in R^{p \times D}$ denote the output embedding. For low-pass filter, we define a diagonal binary matrix $B \in$ ${0,1}^{p \times p}$ as $b_{i i}= \begin{cases}1 &amp; \text { if } i \geq \tau \ 0 &amp; \text { otherwise }\end{cases}$. For high-pass filter, we define a diagonal binary matrix $B \in{0,1}^{p \times p}$ as $b_{i i}= \begin{cases}1 &amp; \text { if } 1 \leq i&lt;\tau \ 0 &amp; \text { otherwise }\end{cases}$. Note that we retain the constant component, so $b_{i, i}=0$. The output of the filter $\mathcal{F}(x): \mathbb{R}^{D} \rightarrow \mathbb{R}^{D}$ is defined by the following objective function:</p>
<p>$$
\begin{array}{cl}
\min <em 2="2">{y} &amp; |x-y|</em> \
\text { subject to } &amp; B F W^{U} y=0
\end{array}
$$}^{2</p>
<p>The solution to the above optimization problem is given by a linear projection.
Remark A.7. The result of the optimization problem defined in Definition A. 6 is the projection of $x$ to the null space of $B F W^{U}$. Let $\mathcal{N}\left(B F W^{U}\right)$ denote the null space of $B F W^{U}$. We have</p>
<p>$$
\mathcal{F}(x)=\mathcal{N}\left(B F W^{U}\right) \cdot \mathcal{N}\left(B F W^{U}\right)^{\top} \cdot x^{\top}
$$</p>
<p>B Fourier Components Separation and Selection of $\tau$
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: We analyzed the logits in Fourier space for all the test data across the last 15 layers. For both the MLP and attention modules. We only plot the first 50 Fourier components (a) The MLP exhibits some outlier low-frequency Fourier components. (b) The attention module's lowfrequency Fourier components are not as obvious as the ones in MLP.</p>
<p>Following Definition A.6, we define single-pass filter as follows:
Definition B. 1 (Single-Pass Filter). Let $x \in \mathbb{R}^{D}$ denote the output of MLP or attention modules. Let $F$ denote the Fourier Basis (Definition A.3). Let $\gamma \in R$ denote the $\gamma$-th Fourier component (Definition A.3) that we want to retain. Let $W^{U} \in R^{V \times D}$ denote the output embedding. We define a diagonal binary matrix $B \in{0,1}^{V \times V}$ as $b_{i i}= \begin{cases}0 &amp; \text { if }\left\lfloor\frac{i+1}{2}\right\rfloor=\gamma \text { or } i=0, \ 1 &amp; \text { otherwise. }\end{cases}$
The output of the filter $\mathcal{F}_{\gamma}(x): \mathbb{R}^{D} \rightarrow \mathbb{R}^{D}$ is defined as the following objective function:</p>
<p>$$
\begin{aligned}
\min <em 2="2">{y} &amp; |x-y|</em> \
\text { subject to } &amp; B F W^{U} y=0
\end{aligned}
$$}^{2</p>
<p>Remark B.2. The result of the optimization problem defined in Definition B. 1 is the projection of $x$ to the null space of $B F W^{U}$. Let $\mathcal{N}\left(B F W^{U}\right)$ denote the null space of $B F W^{U}$. We have</p>
<p>$$
\mathcal{F}_{\gamma}(x)=\mathcal{N}\left(B F W^{U}\right) \cdot \mathcal{N}\left(B F W^{U}\right)^{\top} \cdot x^{\top}
$$</p>
<p>For the single-pass filter, we only retrain one Fourier component and analyze how this component affects the model's prediction. The residual stream is then updated as follows:</p>
<p>$$
h^{(\ell)}=h^{(\ell-1)}+\mathcal{F}<em _gamma="\gamma">{\gamma}\left(\operatorname{Attn}^{(\ell-1)}\right)+\mathcal{F}</em>\right)
$$}\left(\operatorname{MLP}^{(\ell-1)</p>
<p>We evaluated the fine-tuned GPT-2-XL model on the addition dataset with the Fourier components period 520 and 2. Given that $T_{k}:=\frac{V-1}{k}$ (Definition A.3), we retained only the Fourier components with $\gamma=1$ and 260 , respectively.</p>            </div>
        </div>

    </div>
</body>
</html>
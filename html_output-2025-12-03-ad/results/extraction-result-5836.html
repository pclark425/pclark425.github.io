<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5836 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5836</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5836</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-4988b3d378b79eb8669112620baf1ff4e3e536fd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4988b3d378b79eb8669112620baf1ff4e3e536fd" target="_blank">Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is revealed that a symbiotic relationship between text and patterns explains the success of few-shot prompting: text helps extract commonsense from the question to help patterns, and patterns enforce task understanding and direct text generation.</p>
                <p><strong>Paper Abstract:</strong> The past decade has witnessed dramatic gains in natural language processing and an unprecedented scaling of large language models. These developments have been accelerated by the advent of few-shot techniques such as chain of thought (CoT) prompting. Specifically, CoT pushes the performance of large language models in a few-shot setup by augmenting the prompts with intermediate steps. Despite impressive results across various tasks, the reasons behind their success have not been explored. This work uses counterfactual prompting to develop a deeper understanding of CoT-based few-shot prompting mechanisms in large language models. We first systematically identify and define the key components of a prompt: symbols, patterns, and text. Then, we devise and conduct an exhaustive set of experiments across four different tasks, by querying the model with counterfactual prompts where only one of these components is altered. Our experiments across three models (PaLM, GPT-3, and CODEX) reveal several surprising findings and brings into question the conventional wisdom around few-shot prompting. First, the presence of factual patterns in a prompt is practically immaterial to the success of CoT. Second, our results conclude that the primary role of intermediate steps may not be to facilitate learning how to solve a task. The intermediate steps are rather a beacon for the model to realize what symbols to replicate in the output to form a factual answer. Further, text imbues patterns with commonsense knowledge and meaning. Our empirical and qualitative analysis reveals that a symbiotic relationship between text and patterns explains the success of few-shot prompting: text helps extract commonsense from the question to help patterns, and patterns enforce task understanding and direct text generation.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5836.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5836.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbol counterfactuals (GSM-8K)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Counterfactual replacement of symbols in prompts (abstract / out-of-distribution) for GSM-8K</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic replacement of numeric symbols in chain-of-thought few-shot prompts with abstract placeholders or out-of-distribution values to measure sensitivity of PaLM-62B to exact symbol identity in math word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>62B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM-8K (mathematical word problems)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Solve grade-school math word problems requiring step-by-step arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot chain-of-thought (CoT) prompts where in-context example thoughts have numbers replaced by abstract placeholders (e.g., α, β, λ) or out-of-distribution numeric forms (e.g., decimals/fractions).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard CoT few-shot prompts and DIRECT (no-chain-of-thought) few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>C_symb_abx / C_symb_ood (counterfactual symbol prompts) solve rate ≈ 25.7% (PaLM-62B)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT: 27.37%; DIRECT: 10.11%</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Counterfactual-symbol prompts ≈ -1.7 percentage points vs. CoT (25.7% vs 27.37%); large +15.6pp vs DIRECT (25.7% vs 10.11%).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>no substantial effect (symbol identity largely immaterial)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that exact symbol values are largely immaterial because the model primarily imitates patterns/tokens in the prompt; placeholders can cue the model to produce the same structural tokens. The intermediate steps (thoughts) act mainly as a beacon for what symbols/patterns to replicate rather than teaching arithmetic procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Deleting all symbols (removing numbers/dates) nullifies CoT gains (i.e., symbols cannot be entirely removed). Some OOD changes can have task-dependent effects (e.g., Sorting improved with larger integers, SPORTS suffered when names/activities became artificial).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5836.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5836.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbol counterfactuals (SPORTS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Counterfactual replacement of symbols/entities in prompts for SPORTS commonsense task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Replace sportsperson and activity entities in CoT thoughts with abstract or out-of-distribution names/activities to measure sensitivity on a plausibility commonsense task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>62B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SPORTS (BIG-bench commonsense subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary plausibility judgments: is a sentence plausible given the sportsperson and an activity (does activity belong to the sport)?</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot CoT prompts where in-context thoughts' entities (person, activity) are replaced with abstract placeholders or artificial names/activities (C_symb_abx, C_symb_ood).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard CoT few-shot prompts and DIRECT few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>C_symb_abx: 92.11% (PaLM-62B); C_symb_ood: 79.72%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT: 93.67%; DIRECT: 71.08%</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>C_symb_abx approx -1.6pp vs CoT (92.11% vs 93.67); C_symb_ood drops ≈ -13.95pp vs CoT (79.72% vs 93.67).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>generally no effect for abstract placeholders; OOD/artificial entities can reduce performance</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>SPORTS is more sensitive to realistic entities because text+patterns need to convey real-world commonsense associations; artificial activities/names can reduce the model's ability to map prompt patterns to factual knowledge. Still, CoT often remains above DIRECT even with some OOD entities.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Even with artificial names/activities, CoT performance remained marginally better than DIRECT in many cases; but large perturbations to entities can collapse CoT advantage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5836.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5836.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbol counterfactuals (DATE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Counterfactual replacement of date/time symbols in prompts for DATE task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Replace dates in in-context CoT thoughts with abstract or out-of-distribution dates to test sensitivity on date-manipulation problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>62B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>DATE (commonsense/date arithmetic)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer date arithmetic questions (e.g., what is date 24 hours later?) in MM/DD/YYYY format.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot CoT prompts where dates in thoughts are replaced with abstract placeholders or shifted to far-future/out-of-distribution dates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard CoT few-shot prompts and DIRECT few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>C_symb_abx: 37.41%; C_symb_ood: 44.50% (PaLM-62B)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT: 45.18%; DIRECT: 31.61%</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>C_symb_abx ≈ -7.77pp vs CoT (37.41% vs 45.18); C_symb_ood ≈ -0.68pp vs CoT (44.50% vs 45.18).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>mostly no effect for moderate OOD; minor decreases for some abstract substitutions</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Date tasks rely on textual/calculation scaffolding as well as symbols; replacing dates with unrealistic or malformed dates can slightly affect the model but not dramatically if patterns/text remain informative.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Some OOD date substitutions performed almost identically to CoT; deleting dates entirely nullifies performance gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5836.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5836.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbol counterfactuals (Sorting)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Counterfactual replacement of numeric symbols in prompts for Sorting (symbolic) task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Replace integers in CoT thoughts with abstract symbols or larger integers to evaluate effect on sorting task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>62B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SORTING (symbolic)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a list of numbers, return them sorted (task emphasizes symbolic/pattern understanding).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot CoT prompts where numbers in example thoughts are replaced with abstract placeholders or sampled from a different numeric distribution (e.g., larger integers).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard CoT few-shot prompts and DIRECT few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>C_symb_abx: 61.8%; C_symb_ood (larger integers): 80.0% (PaLM-62B)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT: 60.6%; DIRECT: 46.0%</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Using larger integers (C_symb_ood) increased solve rate by ≈ +19.4pp vs vanilla CoT (80.0% vs 60.6). Abstract placeholders performed similarly to CoT (+1.2pp).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>format can improve or have no effect depending on OOD choice (larger integers improved performance)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>In Sorting, particular numeric instantiations in prompts can make the underlying pattern clearer to the model (e.g., larger integers better exhibit the ordering task), so some OOD symbol choices can actually improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Abstract placeholders generally do not harm performance; removing symbols entirely removes CoT benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5836.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5836.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pattern removal / pattern-only</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablating patterns in CoT thoughts: pattern-absent and pattern-only prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experiments that remove structural patterns from the in-context thoughts or keep only patterns (e.g., only equations) to measure the role of patterns for PaLM-62B across multiple tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>62B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple (GSM-8K, SPORTS, DATE, SORTING)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Varied: mathematical reasoning (GSM-8K), commonsense (SPORTS, DATE), symbolic (SORTING).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>CoT few-shot prompts modified to either remove patterns from thoughts (C_pat_none) or include only patterns while removing explanatory text (C_pat_only).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard CoT few-shot prompts and DIRECT few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Examples (PaLM-62B): GSM-8K C_pat_none 21.46%, C_pat_only 10.01%; SPORTS C_pat_none 79.01%, C_pat_only 74.13%; DATE C_pat_none 34.19%, C_pat_only 33.52%; SORTING C_pat_none 45.0%, C_pat_only 46.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT: GSM-8K 27.37% / SPORTS 93.67% / DATE 45.18% / SORTING 60.6%; DIRECT: GSM-8K 10.11% / SPORTS 71.08% / DATE 31.61% / SORTING 46.0%</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Removing patterns (C_pat_none) reduces performance toward DIRECT (e.g., GSM-8K -5.9pp vs CoT, SPORTS -14.66pp vs CoT). Pattern-only nearly collapses to DIRECT-level (e.g., GSM-8K ~10.01% ≈ DIRECT).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (absence of patterns reduces or eliminates CoT benefits)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Patterns provide crucial structural signals that reinforce task understanding and constrain generation; patterns without text are insufficient (they lack semantic glue), and text without patterns is often insufficient as well—the two form a symbiotic relationship.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>In some tasks (GSM-8K, DATE, SORTING) small errors in patterns (wrong arithmetic) did not catastrophically reduce accuracy; in SPORTS, wrong patterns caused a dramatic drop (93.67% → 46.02%).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5836.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5836.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wrong-pattern counterfactuals (SPORTS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Introducing factually incorrect patterns in CoT thoughts (C_pat_wrong) for SPORTS</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Injecting wrong structural patterns (e.g., incorrect sport-activity pairings or erroneous arithmetic) in example thoughts to probe robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>62B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SPORTS (commonsense plausibility)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary plausibility judgments where model must map person→sport and activity→sport.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot CoT prompts with deliberately wrong patterns in the thoughts (C_pat_wrong).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard CoT few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>In SPORTS, wrong-pattern prompts caused performance drop from CoT 93.67% to ≈46.02% (PaLM-62B).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT: 93.67% vs C_pat_wrong: 46.02%</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>-47.65 percentage points (very large negative effect)</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (catastrophic)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Wrong patterns can actively mislead the model by causing it to incorrectly connect clauses; in datasets where patterns are tightly coupled to factual knowledge (SPORTS), incorrect patterns disorient the model and produce actively wrong outputs (often below random).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>In some other tasks (GSM-8K, DATE, SORTING) modest pattern errors did not strongly reduce performance, indicating task-dependent sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5836.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5836.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text perturbations (entities & grammar)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Altering text in CoT thoughts: entity mismatch and grammatical style changes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Modify the free-text parts of CoT thoughts by substituting entities that do not match the question or by changing grammatical style (e.g., Yodish) to evaluate their role.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>62B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM-8K, SPORTS, DATE</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mathematical word problems (GSM-8K), commonsense sports plausibility (SPORTS), date arithmetic (DATE).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot CoT prompts with (a) discrepant entities in thoughts vs questions (C_text_diff,entities) and (b) thoughts/questions rephrased in Yodish/nonstandard grammatical style (C_text_yoda*).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard CoT few-shot prompts and DIRECT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Examples (PaLM-62B): GSM-8K C_text_diff_entities ≈ 16.60%; SPORTS C_text_diff_entities ≈ 69.18%; DATE C_text_emba_thought ≈ 30.75%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT: GSM-8K 27.37% / SPORTS 93.67% / DATE 45.18%; DIRECT: GSM-8K 10.11% / SPORTS 71.08% / DATE 31.61%</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>GSM-8K: -10.77pp vs CoT (16.6% vs 27.37); SPORTS: -24.49pp vs CoT (69.18% vs 93.67); DATE: -14.43pp vs CoT (30.75% vs 45.18).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (text alterations degrade performance; degree proportional to extent of change)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Text acts as conceptual glue linking symbols and patterns and provides commonsense/semantic cues; altering entities or grammar reduces the model's ability to extract semantic meaning, often collapsing performance toward DIRECT. Yodish/strong syntactic drift hurts tasks where text and patterns are intertwined.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>When substituted entities are semantically similar (e.g., 'toys'→'cookies'), performance degrades less and models may form analogical thoughts; more random or semantically inconsistent text is worse.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5836.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5836.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yodish / grammar perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grammatical-style perturbation of prompts (Yodish) to test robustness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Rewriting questions and/or thoughts in non-colloquial but syntactically valid Yodish to evaluate sensitivity to sentence structure frequency and order.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>62B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM-8K, SPORTS, DATE</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mathematical and commonsense tasks where prompt grammar and sentence ordering might affect ability to extract patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>CoT prompts with thoughts and/or questions rephrased into Yodish (XSV ordering) in varying combinations (C_text_yodathoughts, C_text_yodaquestions, C_text_yode).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard CoT few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Authors report: moderate negative impact for GSM-8K; drastic negative effects for SPORTS and DATE (exact per-task numbers in Appendix H).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (task dependent; severe when text-pattern coupling is tight)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Syntactic reordering and less frequent phrasing make it harder for the model to align textual cues to patterns; when patterns and text are intertwined (SPORTS, DATE) grammatical perturbations substantially hurt performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>GSM-8K less affected because patterns (explicit equations) remain salient despite Yodish rephrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5836.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5836.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attention pattern analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Per-layer attention comparison across original and counterfactual prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Compare token-level attention heatmaps between standard CoT prompts and counterfactual prompts to examine whether similar performance coincides with similar attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>62B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Representative sample across tasks (illustrative)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Analysis of attention distributions during generation to probe internal model behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Compare attention per token for vanilla CoT p and counterfactual prompts (e.g., symbolic counterfactuals).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: near-identical attention scores for CoT and symbolic counterfactual prompts when performance is similar; attention more sensitive to presence/absence of patterns than to exact symbols.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>no effect on attention when symbol swaps preserve pattern structure; patterns strongly affect attention</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>If a model performs similarly on p and C_f(p), attention patterns are comparable, suggesting the model processes counterfactual prompts similarly. Patterns (structural cues) drive attention more than exact symbols.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5836.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e5836.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CCoT (concise CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Concise Chain-of-Thought (CCoT): pruned CoT prompts retaining essential text and patterns</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A distilled chain-of-thought prompting approach that prunes unnecessary tokens (~20% fewer) while preserving the essential text-pattern symbiosis, aiming to retain or improve task solve rates and reduce token usage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM, GPT-3, CODEX, PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>62B (PaLM-62B), null (GPT-3 text-davinci-002), null (CODEX code-davinci-002), 540B (PaLM-540B)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM-8K, DATE, SPORTS, SORTING</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same reasoning and symbolic tasks as used throughout the paper, evaluated with concise prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot CCoT prompts: in-context examples with pruned thoughts (20% fewer tokens) that retain the 'text + pattern' essentials; tested across multiple LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard CoT few-shot prompts (baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Selected results (task solve rates):
- PaLM-62B GSM-8K: CoT 27.4% → CCoT 29.1%
- PaLM-62B DATE: 44.7% → 51.3%
- PaLM-62B SPORTS: 93.7% → 94.6%
- PaLM-62B SORTING: 55.3% → 60.2%
Also reported: GPT-3 and PaLM-540B generally show equal or improved performance with CCoT; CODEX mixed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Matrix above compares CoT -> CCoT per task and model (see performance field).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Improvements range from small to moderate (examples: PaLM-62B DATE +6.6pp, GSM-8K +1.7pp, SORTING +4.9pp); average input/output token reduction approx 1.39x / 1.58x respectively (~20% fewer tokens in prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved or maintained (more efficient prompts with similar or better accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Because CoT's effectiveness stems from the symbiosis of concise text and patterns, pruning redundant tokens preserves the essential signals and reduces noise; concise examples can be as or more effective and reduce compute/token generation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Some model/task combinations (e.g., CODEX GSM-8K) saw slight drops; effective pruning may require task-specific choices and cannot always be automated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language Models are Few-Shot Learners <em>(Rating: 2)</em></li>
                <li>PaLM: Scaling Language Modeling with Pathways <em>(Rating: 2)</em></li>
                <li>Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? <em>(Rating: 2)</em></li>
                <li>Prompt Programming for Large Language Models: Beyond the Fewshot Paradigm <em>(Rating: 1)</em></li>
                <li>Training Verifiers to Solve Math Word Problems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5836",
    "paper_id": "paper-4988b3d378b79eb8669112620baf1ff4e3e536fd",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Symbol counterfactuals (GSM-8K)",
            "name_full": "Counterfactual replacement of symbols in prompts (abstract / out-of-distribution) for GSM-8K",
            "brief_description": "Systematic replacement of numeric symbols in chain-of-thought few-shot prompts with abstract placeholders or out-of-distribution values to measure sensitivity of PaLM-62B to exact symbol identity in math word problems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM",
            "model_size": "62B",
            "task_name": "GSM-8K (mathematical word problems)",
            "task_description": "Solve grade-school math word problems requiring step-by-step arithmetic reasoning.",
            "problem_format": "Few-shot chain-of-thought (CoT) prompts where in-context example thoughts have numbers replaced by abstract placeholders (e.g., α, β, λ) or out-of-distribution numeric forms (e.g., decimals/fractions).",
            "comparison_format": "Standard CoT few-shot prompts and DIRECT (no-chain-of-thought) few-shot prompts.",
            "performance": "C_symb_abx / C_symb_ood (counterfactual symbol prompts) solve rate ≈ 25.7% (PaLM-62B)",
            "performance_comparison": "CoT: 27.37%; DIRECT: 10.11%",
            "format_effect_size": "Counterfactual-symbol prompts ≈ -1.7 percentage points vs. CoT (25.7% vs 27.37%); large +15.6pp vs DIRECT (25.7% vs 10.11%).",
            "format_effect_direction": "no substantial effect (symbol identity largely immaterial)",
            "explanation_or_hypothesis": "Authors hypothesize that exact symbol values are largely immaterial because the model primarily imitates patterns/tokens in the prompt; placeholders can cue the model to produce the same structural tokens. The intermediate steps (thoughts) act mainly as a beacon for what symbols/patterns to replicate rather than teaching arithmetic procedure.",
            "counterexample_or_null_result": "Deleting all symbols (removing numbers/dates) nullifies CoT gains (i.e., symbols cannot be entirely removed). Some OOD changes can have task-dependent effects (e.g., Sorting improved with larger integers, SPORTS suffered when names/activities became artificial).",
            "uuid": "e5836.0",
            "source_info": {
                "paper_title": "Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Symbol counterfactuals (SPORTS)",
            "name_full": "Counterfactual replacement of symbols/entities in prompts for SPORTS commonsense task",
            "brief_description": "Replace sportsperson and activity entities in CoT thoughts with abstract or out-of-distribution names/activities to measure sensitivity on a plausibility commonsense task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM",
            "model_size": "62B",
            "task_name": "SPORTS (BIG-bench commonsense subset)",
            "task_description": "Binary plausibility judgments: is a sentence plausible given the sportsperson and an activity (does activity belong to the sport)?",
            "problem_format": "Few-shot CoT prompts where in-context thoughts' entities (person, activity) are replaced with abstract placeholders or artificial names/activities (C_symb_abx, C_symb_ood).",
            "comparison_format": "Standard CoT few-shot prompts and DIRECT few-shot prompts.",
            "performance": "C_symb_abx: 92.11% (PaLM-62B); C_symb_ood: 79.72%",
            "performance_comparison": "CoT: 93.67%; DIRECT: 71.08%",
            "format_effect_size": "C_symb_abx approx -1.6pp vs CoT (92.11% vs 93.67); C_symb_ood drops ≈ -13.95pp vs CoT (79.72% vs 93.67).",
            "format_effect_direction": "generally no effect for abstract placeholders; OOD/artificial entities can reduce performance",
            "explanation_or_hypothesis": "SPORTS is more sensitive to realistic entities because text+patterns need to convey real-world commonsense associations; artificial activities/names can reduce the model's ability to map prompt patterns to factual knowledge. Still, CoT often remains above DIRECT even with some OOD entities.",
            "counterexample_or_null_result": "Even with artificial names/activities, CoT performance remained marginally better than DIRECT in many cases; but large perturbations to entities can collapse CoT advantage.",
            "uuid": "e5836.1",
            "source_info": {
                "paper_title": "Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Symbol counterfactuals (DATE)",
            "name_full": "Counterfactual replacement of date/time symbols in prompts for DATE task",
            "brief_description": "Replace dates in in-context CoT thoughts with abstract or out-of-distribution dates to test sensitivity on date-manipulation problems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM",
            "model_size": "62B",
            "task_name": "DATE (commonsense/date arithmetic)",
            "task_description": "Answer date arithmetic questions (e.g., what is date 24 hours later?) in MM/DD/YYYY format.",
            "problem_format": "Few-shot CoT prompts where dates in thoughts are replaced with abstract placeholders or shifted to far-future/out-of-distribution dates.",
            "comparison_format": "Standard CoT few-shot prompts and DIRECT few-shot prompts.",
            "performance": "C_symb_abx: 37.41%; C_symb_ood: 44.50% (PaLM-62B)",
            "performance_comparison": "CoT: 45.18%; DIRECT: 31.61%",
            "format_effect_size": "C_symb_abx ≈ -7.77pp vs CoT (37.41% vs 45.18); C_symb_ood ≈ -0.68pp vs CoT (44.50% vs 45.18).",
            "format_effect_direction": "mostly no effect for moderate OOD; minor decreases for some abstract substitutions",
            "explanation_or_hypothesis": "Date tasks rely on textual/calculation scaffolding as well as symbols; replacing dates with unrealistic or malformed dates can slightly affect the model but not dramatically if patterns/text remain informative.",
            "counterexample_or_null_result": "Some OOD date substitutions performed almost identically to CoT; deleting dates entirely nullifies performance gains.",
            "uuid": "e5836.2",
            "source_info": {
                "paper_title": "Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Symbol counterfactuals (Sorting)",
            "name_full": "Counterfactual replacement of numeric symbols in prompts for Sorting (symbolic) task",
            "brief_description": "Replace integers in CoT thoughts with abstract symbols or larger integers to evaluate effect on sorting task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM",
            "model_size": "62B",
            "task_name": "SORTING (symbolic)",
            "task_description": "Given a list of numbers, return them sorted (task emphasizes symbolic/pattern understanding).",
            "problem_format": "Few-shot CoT prompts where numbers in example thoughts are replaced with abstract placeholders or sampled from a different numeric distribution (e.g., larger integers).",
            "comparison_format": "Standard CoT few-shot prompts and DIRECT few-shot prompts.",
            "performance": "C_symb_abx: 61.8%; C_symb_ood (larger integers): 80.0% (PaLM-62B)",
            "performance_comparison": "CoT: 60.6%; DIRECT: 46.0%",
            "format_effect_size": "Using larger integers (C_symb_ood) increased solve rate by ≈ +19.4pp vs vanilla CoT (80.0% vs 60.6). Abstract placeholders performed similarly to CoT (+1.2pp).",
            "format_effect_direction": "format can improve or have no effect depending on OOD choice (larger integers improved performance)",
            "explanation_or_hypothesis": "In Sorting, particular numeric instantiations in prompts can make the underlying pattern clearer to the model (e.g., larger integers better exhibit the ordering task), so some OOD symbol choices can actually improve performance.",
            "counterexample_or_null_result": "Abstract placeholders generally do not harm performance; removing symbols entirely removes CoT benefits.",
            "uuid": "e5836.3",
            "source_info": {
                "paper_title": "Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Pattern removal / pattern-only",
            "name_full": "Ablating patterns in CoT thoughts: pattern-absent and pattern-only prompts",
            "brief_description": "Experiments that remove structural patterns from the in-context thoughts or keep only patterns (e.g., only equations) to measure the role of patterns for PaLM-62B across multiple tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM",
            "model_size": "62B",
            "task_name": "Multiple (GSM-8K, SPORTS, DATE, SORTING)",
            "task_description": "Varied: mathematical reasoning (GSM-8K), commonsense (SPORTS, DATE), symbolic (SORTING).",
            "problem_format": "CoT few-shot prompts modified to either remove patterns from thoughts (C_pat_none) or include only patterns while removing explanatory text (C_pat_only).",
            "comparison_format": "Standard CoT few-shot prompts and DIRECT few-shot prompts.",
            "performance": "Examples (PaLM-62B): GSM-8K C_pat_none 21.46%, C_pat_only 10.01%; SPORTS C_pat_none 79.01%, C_pat_only 74.13%; DATE C_pat_none 34.19%, C_pat_only 33.52%; SORTING C_pat_none 45.0%, C_pat_only 46.0%.",
            "performance_comparison": "CoT: GSM-8K 27.37% / SPORTS 93.67% / DATE 45.18% / SORTING 60.6%; DIRECT: GSM-8K 10.11% / SPORTS 71.08% / DATE 31.61% / SORTING 46.0%",
            "format_effect_size": "Removing patterns (C_pat_none) reduces performance toward DIRECT (e.g., GSM-8K -5.9pp vs CoT, SPORTS -14.66pp vs CoT). Pattern-only nearly collapses to DIRECT-level (e.g., GSM-8K ~10.01% ≈ DIRECT).",
            "format_effect_direction": "reduced (absence of patterns reduces or eliminates CoT benefits)",
            "explanation_or_hypothesis": "Patterns provide crucial structural signals that reinforce task understanding and constrain generation; patterns without text are insufficient (they lack semantic glue), and text without patterns is often insufficient as well—the two form a symbiotic relationship.",
            "counterexample_or_null_result": "In some tasks (GSM-8K, DATE, SORTING) small errors in patterns (wrong arithmetic) did not catastrophically reduce accuracy; in SPORTS, wrong patterns caused a dramatic drop (93.67% → 46.02%).",
            "uuid": "e5836.4",
            "source_info": {
                "paper_title": "Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Wrong-pattern counterfactuals (SPORTS)",
            "name_full": "Introducing factually incorrect patterns in CoT thoughts (C_pat_wrong) for SPORTS",
            "brief_description": "Injecting wrong structural patterns (e.g., incorrect sport-activity pairings or erroneous arithmetic) in example thoughts to probe robustness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM",
            "model_size": "62B",
            "task_name": "SPORTS (commonsense plausibility)",
            "task_description": "Binary plausibility judgments where model must map person→sport and activity→sport.",
            "problem_format": "Few-shot CoT prompts with deliberately wrong patterns in the thoughts (C_pat_wrong).",
            "comparison_format": "Standard CoT few-shot prompts.",
            "performance": "In SPORTS, wrong-pattern prompts caused performance drop from CoT 93.67% to ≈46.02% (PaLM-62B).",
            "performance_comparison": "CoT: 93.67% vs C_pat_wrong: 46.02%",
            "format_effect_size": "-47.65 percentage points (very large negative effect)",
            "format_effect_direction": "reduced (catastrophic)",
            "explanation_or_hypothesis": "Wrong patterns can actively mislead the model by causing it to incorrectly connect clauses; in datasets where patterns are tightly coupled to factual knowledge (SPORTS), incorrect patterns disorient the model and produce actively wrong outputs (often below random).",
            "counterexample_or_null_result": "In some other tasks (GSM-8K, DATE, SORTING) modest pattern errors did not strongly reduce performance, indicating task-dependent sensitivity.",
            "uuid": "e5836.5",
            "source_info": {
                "paper_title": "Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Text perturbations (entities & grammar)",
            "name_full": "Altering text in CoT thoughts: entity mismatch and grammatical style changes",
            "brief_description": "Modify the free-text parts of CoT thoughts by substituting entities that do not match the question or by changing grammatical style (e.g., Yodish) to evaluate their role.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM",
            "model_size": "62B",
            "task_name": "GSM-8K, SPORTS, DATE",
            "task_description": "Mathematical word problems (GSM-8K), commonsense sports plausibility (SPORTS), date arithmetic (DATE).",
            "problem_format": "Few-shot CoT prompts with (a) discrepant entities in thoughts vs questions (C_text_diff,entities) and (b) thoughts/questions rephrased in Yodish/nonstandard grammatical style (C_text_yoda*).",
            "comparison_format": "Standard CoT few-shot prompts and DIRECT.",
            "performance": "Examples (PaLM-62B): GSM-8K C_text_diff_entities ≈ 16.60%; SPORTS C_text_diff_entities ≈ 69.18%; DATE C_text_emba_thought ≈ 30.75%.",
            "performance_comparison": "CoT: GSM-8K 27.37% / SPORTS 93.67% / DATE 45.18%; DIRECT: GSM-8K 10.11% / SPORTS 71.08% / DATE 31.61%",
            "format_effect_size": "GSM-8K: -10.77pp vs CoT (16.6% vs 27.37); SPORTS: -24.49pp vs CoT (69.18% vs 93.67); DATE: -14.43pp vs CoT (30.75% vs 45.18).",
            "format_effect_direction": "reduced (text alterations degrade performance; degree proportional to extent of change)",
            "explanation_or_hypothesis": "Text acts as conceptual glue linking symbols and patterns and provides commonsense/semantic cues; altering entities or grammar reduces the model's ability to extract semantic meaning, often collapsing performance toward DIRECT. Yodish/strong syntactic drift hurts tasks where text and patterns are intertwined.",
            "counterexample_or_null_result": "When substituted entities are semantically similar (e.g., 'toys'→'cookies'), performance degrades less and models may form analogical thoughts; more random or semantically inconsistent text is worse.",
            "uuid": "e5836.6",
            "source_info": {
                "paper_title": "Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Yodish / grammar perturbations",
            "name_full": "Grammatical-style perturbation of prompts (Yodish) to test robustness",
            "brief_description": "Rewriting questions and/or thoughts in non-colloquial but syntactically valid Yodish to evaluate sensitivity to sentence structure frequency and order.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM",
            "model_size": "62B",
            "task_name": "GSM-8K, SPORTS, DATE",
            "task_description": "Mathematical and commonsense tasks where prompt grammar and sentence ordering might affect ability to extract patterns.",
            "problem_format": "CoT prompts with thoughts and/or questions rephrased into Yodish (XSV ordering) in varying combinations (C_text_yodathoughts, C_text_yodaquestions, C_text_yode).",
            "comparison_format": "Standard CoT few-shot prompts.",
            "performance": "Authors report: moderate negative impact for GSM-8K; drastic negative effects for SPORTS and DATE (exact per-task numbers in Appendix H).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "reduced (task dependent; severe when text-pattern coupling is tight)",
            "explanation_or_hypothesis": "Syntactic reordering and less frequent phrasing make it harder for the model to align textual cues to patterns; when patterns and text are intertwined (SPORTS, DATE) grammatical perturbations substantially hurt performance.",
            "counterexample_or_null_result": "GSM-8K less affected because patterns (explicit equations) remain salient despite Yodish rephrasing.",
            "uuid": "e5836.7",
            "source_info": {
                "paper_title": "Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Attention pattern analysis",
            "name_full": "Per-layer attention comparison across original and counterfactual prompts",
            "brief_description": "Compare token-level attention heatmaps between standard CoT prompts and counterfactual prompts to examine whether similar performance coincides with similar attention.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM",
            "model_size": "62B",
            "task_name": "Representative sample across tasks (illustrative)",
            "task_description": "Analysis of attention distributions during generation to probe internal model behavior.",
            "problem_format": "Compare attention per token for vanilla CoT p and counterfactual prompts (e.g., symbolic counterfactuals).",
            "comparison_format": null,
            "performance": "Qualitative: near-identical attention scores for CoT and symbolic counterfactual prompts when performance is similar; attention more sensitive to presence/absence of patterns than to exact symbols.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "no effect on attention when symbol swaps preserve pattern structure; patterns strongly affect attention",
            "explanation_or_hypothesis": "If a model performs similarly on p and C_f(p), attention patterns are comparable, suggesting the model processes counterfactual prompts similarly. Patterns (structural cues) drive attention more than exact symbols.",
            "counterexample_or_null_result": null,
            "uuid": "e5836.8",
            "source_info": {
                "paper_title": "Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "CCoT (concise CoT)",
            "name_full": "Concise Chain-of-Thought (CCoT): pruned CoT prompts retaining essential text and patterns",
            "brief_description": "A distilled chain-of-thought prompting approach that prunes unnecessary tokens (~20% fewer) while preserving the essential text-pattern symbiosis, aiming to retain or improve task solve rates and reduce token usage.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM, GPT-3, CODEX, PaLM",
            "model_size": "62B (PaLM-62B), null (GPT-3 text-davinci-002), null (CODEX code-davinci-002), 540B (PaLM-540B)",
            "task_name": "GSM-8K, DATE, SPORTS, SORTING",
            "task_description": "Same reasoning and symbolic tasks as used throughout the paper, evaluated with concise prompts.",
            "problem_format": "Few-shot CCoT prompts: in-context examples with pruned thoughts (20% fewer tokens) that retain the 'text + pattern' essentials; tested across multiple LLMs.",
            "comparison_format": "Standard CoT few-shot prompts (baseline).",
            "performance": "Selected results (task solve rates):\n- PaLM-62B GSM-8K: CoT 27.4% → CCoT 29.1%\n- PaLM-62B DATE: 44.7% → 51.3%\n- PaLM-62B SPORTS: 93.7% → 94.6%\n- PaLM-62B SORTING: 55.3% → 60.2%\nAlso reported: GPT-3 and PaLM-540B generally show equal or improved performance with CCoT; CODEX mixed.",
            "performance_comparison": "Matrix above compares CoT -&gt; CCoT per task and model (see performance field).",
            "format_effect_size": "Improvements range from small to moderate (examples: PaLM-62B DATE +6.6pp, GSM-8K +1.7pp, SORTING +4.9pp); average input/output token reduction approx 1.39x / 1.58x respectively (~20% fewer tokens in prompt).",
            "format_effect_direction": "improved or maintained (more efficient prompts with similar or better accuracy)",
            "explanation_or_hypothesis": "Because CoT's effectiveness stems from the symbiosis of concise text and patterns, pruning redundant tokens preserves the essential signals and reduces noise; concise examples can be as or more effective and reduce compute/token generation.",
            "counterexample_or_null_result": "Some model/task combinations (e.g., CODEX GSM-8K) saw slight drops; effective pruning may require task-specific choices and cannot always be automated.",
            "uuid": "e5836.9",
            "source_info": {
                "paper_title": "Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango",
                "publication_date_yy_mm": "2022-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language Models are Few-Shot Learners",
            "rating": 2
        },
        {
            "paper_title": "PaLM: Scaling Language Modeling with Pathways",
            "rating": 2
        },
        {
            "paper_title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
            "rating": 2
        },
        {
            "paper_title": "Prompt Programming for Large Language Models: Beyond the Fewshot Paradigm",
            "rating": 1
        },
        {
            "paper_title": "Training Verifiers to Solve Math Word Problems",
            "rating": 1
        }
    ],
    "cost": 0.024288499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Text and Patterns: For Effective Chain of Thought It Takes Two to Tango</h1>
<p>Aman Madaan<em> and Amir Yazdanbakhsh</em><br>Carnegie Mellon University ${ }^{\bullet}$ Google Research, Brain Team<br>amadaan@cs.cmu.edu, ayazdan@google.com<br>(Equal Contribution)</p>
<h4>Abstract</h4>
<p>In the past decade, we witnessed dramatic gains in natural language processing and an unprecedented scaling of large language models. These developments have been accelerated by the advent of few-shot techniques such as chain of thought ( COT ) prompting. Specifically, COT pushes the performance of large language models in a few-shot setup by augmenting the prompts with intermediate steps. Despite impressive results across various tasks, the reasons behind their success have not been explored. This work uses counterfactual prompting to develop a deeper understanding of COT-based few-shot prompting mechanisms in large language models. We first systematically identify and define the key components of a prompt: symbols, patterns, and text. Then, we devise and conduct an exhaustive set of deliberated experiments across four different tasks, by querying the model with counterfactual prompts where only one of these components is altered. Our experiments across three models-PaLM, GPT-3, and CODEX—reveal several surprising findings and brings into question the conventional wisdom around few-shot prompting. First, the presence of factual patterns in a prompt is practically immaterial to the success of COT. Second, our results conclude that the primary role of intermediate steps may not be to facilitate learning "how" to solve a task. The intermediate steps are rather a beacon for the model to realize "what" symbols to replicate in the output to form a factual answer. As such, the patterns are merely a channel to "trick" the model into forming sentences that resemble correct answers. This pathway is facilitated by text, which imbues patterns with commonsense knowledge and meaning. Our empirical and qualitative analysis reveals that a symbiotic relationship between text and patterns explains the success of fewshot prompting: text helps extract commonsense from the question to help patterns, and patterns enforce task understanding and direct text generation. Such systematic understanding of COT enables us to devise a concise chain of thought, dubbed as CCoT, where text and patterns are pruned by over $20 \%$, only retaining their key roles. We achieve this reduction in the number of tokens while delivering on par or slightly higher solve task rate.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>1. INTRODUCTION</h2>
<p>The ability to learn a previously unseen task by observing a few examples is one of the cornerstones of human intelligence (Lake et al., 2017). This is in stark contrast with modern deep learning methods, which typically rely on a substantial labeled corpus of data. Recently, large language models (LLMs) (Chowdhery et al., 2022; Brown et al., 2020; Chen et al., 2021a) have demonstrated remarkable performance in employing a prompt to perform a task, with no additional finetuning, commonly known as few-shot learning. Few-shot</p>
<p>learning has shown promising applications for a wide range of tasks (Gehrmann et al., 2021; Wei et al., 2021; Sanh et al., 2021; Thoppilan et al., 2022; Liu et al., 2021a; Reif et al., 2021; Wang et al., 2020; Chen et al., 2021b; Lewkowycz et al., 2022; Wu et al., 2022). While beneficial, this setting requires meticulous design of prompts (Le Scao \&amp; Rush, 2021; Liu et al., 2021c; Mishra et al., 2021).
Ling et al. (2017) pioneered the idea of using natural language rationales as the intermediate steps in prompts to help model performance for mathematical reasoning. Recently, Wei et al. (2022) proposed chain of thought (CoT) prompting, showing that the few-shot setting in LLMs similarly benefits from intermediate natural language rationale across a range of complex reasoning tasks (Ling et al., 2017; Cobbe et al., 2021; Patel et al., 2021; BIG-bench Collaboration, 2022). Despite its wide-range usage, the rationale behind the success of CoT remains unclear. Recent work draws (Ling et al., 2017; Wei et al., 2022) parallels to human thinking. Humans often think about a problem before deducing a solution. Akin to this process, it is argued that models should also be able to employ a similar mechanism. While intuitive, such restrictive abstract explanations fall short in explaining why, when, and how these mechanisms operate. Ultimately, LLMs are trained to estimate the next token distribution for a given context. Therefore, there is presumably a systematic rationale behind their successes and failures. In this work, we undertake initial steps towards understanding the mechanism behind CoT.</p>
<p>Contributions and findings. We construct a series of tailored counterfactual prompts (Goyal et al., 2019), deliberately sketched as controlled studies. First, we identify key components of an example in few-shot prompting as follows: Symbols, Patterns, and Text. Next, we perform counterfactual prompting-keeping all but one component fixed (e.g., replacing symbols (numbers) with Greek alphabets). Finally, we elicit meaningful findings via conducting a systematic and qualitative analysis of the performance divergence between different prompt queries. Our experiments on four diverse reasoning tasks and across three large language models-PaLM, GPT-3, and CODEX, reveal several surprising findings:
(1) We find that the exact type of symbols in the prompt virtually does not affect the model performance. In addition, our results and analysis demonstrate counterintuitive phenomena. For example, we identify that the correctness of symbols and patterns is immaterial to the task solve rate. (2) We learn that patterns contribute chiefly as a venue to reinforce task understanding (Ouyang et al., 2022) and prompt the model to attain correct outputs. (3) Most importantly, we find that text and patterns form a symbiotic relationship that plays a vital role in the success of CoT. Text helps generate useful patterns (e.g., by extracting commonsense knowledge), and patterns help reinforce task understanding, enabling the language model to generate text that helps solve the task. Overall, we argue that one of the primary reasons behind the success of COT is this interplay between text and patterns-CO T helps a language model in imitating the prompt and generating the right tokens for the task-and is conceivably less related to their reasoning abilities. Finally, as indicated by applications such as PaLM-SAYCAN (Ahn et al., 2022), we posit that techniques like CoT will play a key role in enabling the success of LLMs on diverse use cases. Thus, designing efficient prompts informed by a set of key design principles is an important challenge. To this end, we distill our findings to create concise prompting, dubbed CCoT. CCoT prunes the prompt ( $20 \% \downarrow$ ) to only retain indispensable tokens without negative repercussions on the task solve rate.</p>
<h1>2. Counterfactual Explanation for Chain of Thought</h1>
<p>The primary objective of our study is to understand CoT through counterfactual prompting and empirically establish the underpinnings of the reasoning ability of LLMs in the presence of CoT. Each counterfactual prompt $C_{f}(p)$ alters only one particular aspect of the in-context examples $\left\langle x_{k} \cdot t_{k} \cdot y_{k}\right\rangle$ in a $p$. For example, consider a sample thought for the math world problems in the GSM-8K dataset (See Table 1). A symbolic counterfactual prompt, $C_{\text {symbolic }}(p)$, may simply replace all the numbers in the thoughts with symbols (e.g., $X 1)$. Such analysis enables us to ask: "what would the performance of the model have been, if all the numbers in the prompt were replaced with symbols?". Analyzing the performance disparity of a LLM on</p>
<p>$C_{\text {symbolic }}(p)$ vs. $p$ can thus indicate the role that using actual numbers plays in the success or failure of a task ${ }^{1}$. The ability to successfully complete prompts $p$ for complex reasoning tasks is typically present for LLM at the scale of PaLM, GPT-3, and CODEX. Nonetheless, we do not make any assumptions about the underlying model architecture. In summary, our study on GSM-8K reveals that for solving math problems, neither the presence of numbers, nor the credibility of the thoughts is paramount to the success of CoT. Similarly, altering the style and wording of the texts in the thoughts has a modest impact on the model performance. Nevertheless, eradicating either of these components nullifies the efficacy of CoT. Finally, a per-layer analysis of the model reveals that if the model performs similarly for a pair of counterfactual prompts $C_{f}(p)$ vs. $p$, then the attention patterns are comparable as well. We defer the detailed background to Appendix A.</p>
<p>Limitations of counterfactual prompting. Relying on counterfactual examples could be misleading and precarious (Laugel et al., 2019; Slack et al., 2021). Nonetheless, counterfactual explanation presents a channel to gain insights into the workings of the model. This approach potentially yields more favorable explanations for state-of-the-art LLMs. Notably, unlike fine-tuned methods, one can readily identify and collect a set of prompts that are critical for the model to generate particular outputs. In particular, fewshot prompting augments the model with an additional dimension to calibrate the accuracy to a discernible degree. Thus, we deduce that the counterfactual examples that exhibit consistent and systematic performance divergence are more prone to reflect credible interpretations of the model. In this work, we neither rely on the results that do not exhibit such characteristics, nor reject prompts that pose contradictory observations. We discuss additional limitations in Appendix A.1.</p>
<h1>3. EXPERIMENTAL SETUP</h1>
<p>Large language models. To facilitate conducting an exhaustive number of experiments, we center the main analysis of this paper around PaLM-62B ${ }^{2}$. For reproducibility, we also conduct the experiments on publicly available models such as GPT-3 and CODEX. We present results from PaLM-62B in the main body and defer the results from other models to Appendix E. Nonetheless, our findings concur across the studied LLMs.</p>
<p>Reasoning tasks. We focus on reasoning tasks for which CoT presents ample improvements over DIRECT prompting (Wei et al., 2022), namely $\triangleleft$ MATHEMATICAL (GSM-8K Cobbe et al. (2021)), $\triangleleft$ COMMONSENSE (date and sports understanding BIG-bench Collaboration (2022)), and $\triangleleft$ SYMBOLIC (SORTING)—details in Appendix B and Appendix-Table 10.</p>
<h3>3.1. SEMANTIC COMPONENTS OF PROMPTS</h3>
<p>This work intends to tease apart the major semantic components of a prompt that play a critical role in the efficacy of COT. To achieve this, we identify and systematically construe three key semantic components of a prompt, listed as follows (See Appendix-Table 10 for the examples to which we refer in the definitions):
$\triangleleft$ Symbols are sequences of tokens in the prompt, about which the model reasons to solve a task. For GSM-8K and Sorting, symbols are numerical quantities (e.g., 5, 4, 2, 13). Similarly for the SPORTS dataset, we categorize players and activities as symbols. We define the symbols in the DATE dataset as date and time indicating expressions.
$\triangleleft$ Patterns are either composition of symbols and operators or a structure of prompt that reinforces task understanding. The isolation of patterns within a prompt is evident in datasets like GSM-8K (equations), SPORTS</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: $\triangleleft$ Symbols $\boldsymbol{\square}$, $\boldsymbol{\triangleleft}$ Patterns $\boldsymbol{\triangleright}$, and $\triangleleft$ Text $\boldsymbol{\triangleright}$ across different tasks.
$\triangle$ Mathematical
Question: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?
Thought: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. $5+4=9$.
$\triangle$ COMMONSENSE $\triangleleft$ (SPORTS)
Question: Is the following sentence plausible? "Jamal Murray was perfect from the line."
Thought: Jamal Murray is a basketball player. Being perfect from the line is part of basketball.
$\triangle$ COMMONSENSE $\boldsymbol{\square}$ (DATE)
Question: It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY?
Thought: 〈calculation〉 Today is 04/19/1969. 24 hours later is one day after today, which would be 04/20/1969. 〈output〉 The answer is 04/20/1969.
$\triangle$ Symbolic $\boldsymbol{\square}$ (Sorting)
Question: $3,1,2,7,8,5,6,9,4$
Thought: $1&lt;2 \ldots&lt;9$
(person is a sport, activity is a sport), and Sorting (1 less than 2). However, for the Date dataset, the pattern is semantically implicit but consistent. Each thought contains two parts: (a) 〈calculation〉 in which the information from the input is restated (e.g., "Today is 06/02/1943") and intermediate results are generated (e.g., "One day after 06/01/1943 is 06/02/1943") through mathematical calculations and (b) 〈output〉 in which the final requisite answer is generated using the intermediate results (e.g., "10 days before today is $05 / 23 / 1943$ ").
$\triangleleft$ Text $\boldsymbol{\square}$ are tokens that are neither symbols, nor part of patterns. Specifically, text in prompts assists in either outlining the target task (e.g., is the sentence plausible), connecting patterns to symbols (e.g., John is left with $4-2=2$ ), or contextualizing symbols ( 4 toys). In a nutshell, text is the conceptual glue that binds different parts of a prompt.
Relying on our analysis, we conjecture and hypothesize about the effects of each semantic components on the outcomes of CoT prompting. We discuss detailed results, including statistical significance tests in Appendix E.</p>
<h1>4. ROLE OF Symbols</h1>
<p>$\kappa \mathcal{H}<em _symb_abs="{symb_abs" _text="\text">{\mathbf{0}} \times$ The exact value and type of symbols are mostly immaterial to the model performance. Replacing symbols with abstract placeholders can do just as well at eliciting effective thoughts.
Visually inspecting the examples in Table 1, it seems intuitive to assume that symbols are important for steering the model towards comprehending (or presumably reasoning about) a target task. We form a set of counterfactual prompts in which the symbols are deliberately altered in distinct ways (Table 2 and AppendixTable 19 summarize the results). To test this hypothesis, we conducted two sets of experiments using counterfactual prompts: replacing the symbols with abstract values, and replacing them with out-of-distribution symbols.
Abstract symbols $\left\langle C</em>(p)$, a modified variant of prompt $p$ in which some or all the symbols are replaced with an abstract placeholder (See Table 2). The results in Table 2 illustrate that the performance has little to no impact when the symbols are replaced with abstract placeholders. Note that for the SPORTS dataset, we also experiment with changing sportsperson and sport activity, which mutates the baseline thoughts to vague and ungrammatical and drops the task rate to $52.96 \%$ (Table 20).}}(\boldsymbol{p})\right\rangle$. We first experiment with the role of symbols by creating $C_{\text {symb_abs }</p>
<p>Table 2: A sample modified thought for each category is depicted below. We accordingly update the questions associated with each thought. Appendix-Table 19 provides additional results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question / Thought</th>
<th style="text-align: center;">Prompt Type</th>
<th style="text-align: center;">Solve Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\triangleleft$ Mathematical $&gt;$ (Direct $=10.11 \%, \mathrm{CoT}=27.37 \%$ )</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Thought: Shawn started with $\alpha$ toys. If he got $\beta$ toys each from his mom and dad, then that is $\lambda$ more toys. $\alpha+\lambda=\pi$. <br> Thought: Shawn started with 5.5 toys. If he got 2.5 toys each from his mom and dad, then that is 5 more toys. $5.5+5=10.5$.</td>
<td style="text-align: center;">$C_{\text {symb_abx }}(p)$ (Table 43) $C_{\text {symb_ood }}(p)$ (Table 48)</td>
<td style="text-align: center;">$25.70 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$\triangleleft$ COMMONSENSE $&gt;$ (SporTS) (DIRECT $=71.08 \%, \mathrm{CoT}=93.67 \%$ )</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Thought: Jamal Murray is a basketball player. Being ACTIVITY is part of basketball. <br> Thought: Adair Foster is a basketball player. Juggling the paper cups is part of basketball.</td>
<td style="text-align: center;">$C_{\text {symb_abx }}(p)$ (Table 46) $C_{\text {symb_ood }}(p)$ (Table 50)</td>
<td style="text-align: center;">$92.11 \%$ <br> $79.72 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$\triangleleft$ COMMONSENSE $&gt;$ (DATE) (DIRECT $=31.61 \%, \mathrm{CoT}=45.18 \%$ )</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Thought: Today is DATE. 24 hours later is one day after today, which would be DATE. <br> Thought: Today is 04/30/3069. 24 hours later is one day after today, which would be 04/31/3069.</td>
<td style="text-align: center;">$C_{\text {symb_abx }}(p)$ (Table 42) $C_{\text {symb_ood }}(p)$ (Table 49)</td>
<td style="text-align: center;">$37.41 \%$ <br> $44.50 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$\triangleleft$ Symbolic $&gt;$ (Sorting) (Direct $=46.0 \%, \mathrm{CoT}=60.6 \%$ )</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Thought: $\leq&lt;\phi&lt;\gamma&lt;\delta&lt;\zeta&lt;\chi&lt;\epsilon&lt;\pi&lt;v$ <br> Thought: $11&lt;23&lt;34&lt;48&lt;56&lt;63&lt;72&lt;85&lt;95$</td>
<td style="text-align: center;">$C_{\text {symb_abx }}(p)$ (Table 44) $C_{\text {symb_ood }}(p)$ (Table 51)</td>
<td style="text-align: center;">$\begin{aligned} &amp; 61.8 \% \ &amp; 80.0 \% \end{aligned}$</td>
</tr>
</tbody>
</table>
<p>Out of distribution symbols ${ }<em _symb_ood="{symb_ood" _text="\text">{<em>}^{\prime} C_{\text {symb_ood }}(\boldsymbol{p})_{</em>}^{\prime}$. To test the operational utility of symbols, we design counterfactual prompts $C</em>(p)$, in which the symbols are sampled from a distinct distribution compared to the symbols in the questions. The operations include replacing integers in GSM-8K prompt with fractions, sportsperson in SPORTS prompt with random names, and changing dates in DATE to dates after 3000 AD.}</p>
<p>The results (Table 2) fail to reject our hypothesis and reinforce our initial finding that the type of symbols is primarily immaterial to the model performance. A notable exception is SPORTS, where including artificial names and activities closes the gap between DIRECT and COT. However, surprisingly, even with entirely artificial names and activities in the SPORTS dataset, the model performance is marginally better than direct. Another interesting exception occurs in the Sorting dataset. Compared to vanilla COT, using larger integers $(\geqslant 10)$ considerably improves the task solve rate $(60.6 \% \rightarrow 80.0 \%)$. We postulate that in this scenario, the modified thoughts more effectively inform the model about the underlying task of sorting numbers. These results indicate that placeholders and abstract values can do merely as well at eliciting effective thoughts. However, we find that completely deleting the symbols is not a viable option, as expected. Deleting all the symbols (e.g., numbers and dates) nullifies the gains carried over by COT. We show additional results in Appendix-Table 15.</p>
<p>Nature of generated answers. We observe that the task solve rates are relatively unaffected for both $C_{\text {symb_abx }}(p)$ and $C_{\text {symb_ood }}(p)$. In hindsight, it is not apparent whether systematic differences exist in the generated answers. To quantify this, we compute the Cohen's $\kappa$ agreement score (Cohen, 1960) between predictions generated by $p$ (DIRECT) and various counterfactual prompts. The results (Appendix E.2) show that there is a moderate ( $&gt;0.4$ ) to substantial ( $&gt;0.61$ ) agreement between COT and symbolic counterfactual prompts. In contrast, the agreement between DIRECT and the counterfactual prompts is meager ( $&lt;0.2$ ). These results reinforce our finding that the model may behave similarly regardless of the actual type/value of the symbols.</p>
<p>Analysis of employing in-distribution symbols in thoughts. We also delve into the details of generated answers for GSM-8K using $p$ and $C_{\text {symb_ood }}(p)$. As Table 2 delineates, $C_{\text {symb_ood }}(p)$ prompts for GSM-8K contain questions/thoughts with simple decimals. We investigate whether such prompts help to improve the solve rate for questions with decimals preferentially. Surprisingly, we observe that such prompts did not</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The average attention per token for a randomly sampled question using vanilla CoT prompt $p$ (above) and different counterfactual prompts (bottom). Near identical attention scores indicates that few-shot models are relatively indifferent to the exact symbols, but are sensitive to patterns. In addition, this study suggests that the model has a tendency to more profoundly attend to tokens at the vicinity of final question (brighter bars at the right side of each bar). Please see Appendix D for details on attention score calculation, and per-layer heatmaps. Appendix Figure 12 depicts additional results.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">(1)</th>
<th style="text-align: left;">Thats are 1 times the green. (Grow symbols will place trees in green today. After they are done, there will be 2.1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">trees. With more trees, the 2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.2-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.3-2.4-2.5-2.7-2.8-2.9-2.1-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.3-2.4-2.5-2.6-2.7-2.8-2.9-2.1-2.3-2.4-2.5-2.7-2.8-2.9-2.1-2.3-2.4-2.5-2.7-2.8-2.9-2.1-2.3-2.4-2.8-2.9-2.1-2.3-2.4-2.5-2.7-2.8-2.9-2.1-2.3-2.4-2.5-2.7-2.8-2.9-2.1-2.3-2.4-2.8-2.9-2.1-2.3-2.4-2.8-2.9-2.1-2.3-2.4-2.8-2.9-2.1-2.3-2.4-2.8-2.9-2.1-2.3-2.4-2.8-2.9-2.1-2.3-2.4-2.8-2.9-2.1-2.3-2.4-2.8-2.9-2.1-2.3-2.4-2.8-2.9-2.1-2.3-2.4-2.8-2.9-2.1-2.3-2.4-2.8-2.9-2.1-2.3-2.4-2.8-2.9-2.1-2.3-2.4-2.8-2.9-2.1-2.3-2.4-2.8-2.9-2.1-2.3-2.4-2.8-2.9-2.1-2.3-2.4-2.8-2.9-2.1-2.3-2.4-2.8-2.9-2.1-2.3-2.4-2.8-2.9-2.1-2.3-2.4-2.1-2.3-2.8-2.9-2.1-2.3-2.4-2.8-2.9-2.1-2.3-2.4-2.8-2.9-2.1-2.3-2.4-2.8-2.9-2.1-2.3-2.4-2.1-2.3-2.1-2.3-2.8-2.9-2.1-2.3-2.4-2.1-2.3-2.1-2.3-2.1-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.1-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.1-2.1-2.1-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.1-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.1-2.3-2.1-2.3-2.1-2.1-2.3-2.1-2.3-2.1-2.1-2.3-2.1-2.1-2.3-2.1-2.1-2.3-2.1-2.1-2.3-2.1-2.3-2.1-2.1-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.3-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1-2.1</td>
</tr>
</tbody>
</table>
<p>Table 3: The accuracy of patterns is not important, but their absence could be catastrophic. Additional results can be found in Appendix-Table 21.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question / Thought</th>
<th style="text-align: center;">Prompt Type</th>
<th style="text-align: center;">Solve Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\triangleleft$ Mathematical $&gt;$ (Direct $=10.11 \%, \operatorname{CoT}=27.37 \%)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Thought: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys.</td>
<td style="text-align: center;">$C_{\text {pat_none }}(p)$ (Table 57)</td>
<td style="text-align: center;">$21.46 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Thought: $5+\left(2 * 2\right)=9$.</td>
<td style="text-align: center;">$C_{\text {pat_only }}(p)$ (Table 58)</td>
<td style="text-align: center;">$10.01 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$\triangle$ COMMONSENSE $\boldsymbol{\sim}$ (SporTS) (DIRECT $=71.08 \%, \operatorname{CoT}=93.67 \%)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Thought: Jamal Murray and being perfect from the line are both part of basketball.</td>
<td style="text-align: center;">$C_{\text {pat_none }}(p)$ (Table 63)</td>
<td style="text-align: center;">$79.01 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Thought: Both are part of the same sport.</td>
<td style="text-align: center;">$C_{\text {pat_only }}(p)$ (Table 59)</td>
<td style="text-align: center;">$74.13 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$\triangleleft$ COMMONSENSE $\boldsymbol{\sim}$ (DATE) (DIRECT $=31.61 \%, \operatorname{CoT}=45.18 \%)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Thought: Today is 04/19/1969.</td>
<td style="text-align: center;">$C_{\text {pat_none }}(p)$ (Table 62)</td>
<td style="text-align: center;">$34.19 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Thought: $\langle$ calculation $\rangle$ Today $=04 / 19 / 1969.24$ hours $=1$ day. $\langle$ output $\rangle 04 / 19 / 1969$</td>
<td style="text-align: center;">$C_{\text {pat_only }}(p)$ (Table 60)</td>
<td style="text-align: center;">$33.52 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$+1=04 / 20 / 1969$.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\triangle$ Symbolic $\sim$ (Sorting) (Direct $=46.0 \%, \operatorname{CoT}=60.6 \%)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Thought $9&gt;8&gt;7&gt;6&gt;5&gt;4&gt;3&gt;2&gt;1$</td>
<td style="text-align: center;">$C_{\text {pat_none }}(p)$ (Table 61)</td>
<td style="text-align: center;">$45.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Thought: - (similar to DIRECT)</td>
<td style="text-align: center;">$C_{\text {pat_only }}(p)$</td>
<td style="text-align: center;">$46.0 \%$</td>
</tr>
</tbody>
</table>
<p>No patterns $\left\langle\boldsymbol{C}<em _pat_none="{pat_none" _text="\text">{\text {pat_none }}(\boldsymbol{p})\right\rangle$. We next gauge the sensitivity of the model performance to the existence of patterns. For GSM-8K, the dichotomy between text and patterns is clear-the equations in thoughts represent patterns, and everything else serves as text. Therefore, we can construct $C</em>}}(p)$ for GSM-8K by removing everything except equations. For SPORTS dataset, the patterns are an implicit way of structuring the thought in the following form: "person is a sport player. activity is part of sport ${ <em 1="1">{2}$ ". The answer is yes, if and only if sport ${ }</em>}$ and sport ${ <em 1="1">{2}$ are the same. In such cases, merely partially removing patterns is not sufficient. For example, just using "person is a sport ${ }</em>(p)$ still outperforms DIRECT, hinting at the relevance of blending semantically correct statements in thoughts to improved model performance, which we study in Section 6. Table 24 reveals that while the generated thoughts are structurally correct, the model can not establish an explicit connection between PERSON and SPORT, hence, attaining rather spurious outcomes. This analysis underscores the importance of blending explicit patterns in thoughts, which corroborates with the least-to-most prompting (Zhou et al., 2022).
Pattern-only prompts $\left\langle\boldsymbol{C}}$ player. as a thought is not equivalent to a counterfactual example with no patterns. This example resembles an experiment with a reduced pattern. To circumvent this, we simulate $C_{\text {pat_none }}(p)$ for SPORTS by crafting a prompt in which several variations of thoughts are mixed. The key insight is that if several different patterns are included in a single prompt, the induced "noise" from different examples creates a virtually equivalent variant of $C_{\text {pat_none }}(p)$ setup. That is, we imitate $C_{\text {pat_none }}(p)$ by creating a hodgepodge of thought variants (See Table 63) without explicitly submitting to a particular pattern. Identically, we conform to this terminology for DATE and Sorting. The results in Table 3 reveal that $C_{\text {pat_none }}(p)$ consistently underperforms COT, relatively yielding similar performance as DIRECT. This indicates that the existence of patterns in thoughts is crucial to the success of COT. Note that in all datasets, $C_{\text {pat_none }<em _pat_wrong="{pat_wrong" _text="\text">{\text {pat_only }}(\boldsymbol{p})\right\rangle$. Finally, we investigate counterfactual prompts in which we exclusively use patterns while wiping out the rest of the thoughts. The results in Table 3 reveal that pattern-only prompts are futile and annul the gains of COT.
Wrong pattern $\left\langle\boldsymbol{C}</em>)\right\rangle$. Examples of such wrong patterns are $5+3=7$ for the GSM-8K, and 2 days after 2/2/2022 is 2/6/2022 in DATE. In tasks like GSM-8K (24.39\%), DATE (44.84\%), and SortING $(64.80 \%)$, the task solve rate is robust to specific mistakes. For these tasks, the model performance with counterfactual prompts is on par with the vanilla COT. On the contrary, in the SPORTS dataset with}}(\boldsymbol{p</p>
<p>Table 4: Text is an important glue for symbols and patterns: modifications to text hampers the performance. Across tasks, the drop in performance is relatively proportional to the extent of change. Due to the nonexistent text in Sorting, we did not conduct counterfactual experiments for this dataset. Additional results can be found in Appendix-Table 25.</p>
<table>
<thead>
<tr>
<th>Question / Thought</th>
<th>Prompt Type</th>
<th>Solve Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\triangleleft$ Mathematical $\triangleright($ Direct $=10.11 \%, \operatorname{CoT}=27.37 \%)$</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Thought: Teddy started with 5 cookies. If he got 2 cookies each from his Jenna and Rehan, then that is 4 more cookies. $5+4=9$. <br> Thought: With 5 toys, Shawn started. 2 toys each from his mom and dad, if he got, then that is 4 more toys. $5+4=9$.</td>
<td>$C_{\text {text_diff,entities }}(p)$ (Table 68) $C_{\text {text_emba_thought }}(p)$ (Table 70)</td>
<td>$16.60 \%$</td>
</tr>
<tr>
<td>4 COMMONSENSE $\triangleleft$ (SPORTS) (DIRECT $=71.08 \%, \operatorname{CoT}=93.67 \%)$</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Thought: Adair Foster is a basketball player. Juggling the paper cups is part of basketball. <br> Thought: A basketball player Jamal Murray is. Perfect from the line is part of basketball being.</td>
<td>$C_{\text {text_diff,entities }}(p)$ (Table 69) $C_{\text {text_emba_thought }}(p)$ (Table 71)</td>
<td>$69.18 \%$</td>
</tr>
<tr>
<td>4 COMMONSENSE $\triangleleft$ (DATE) (DIRECT $=31.61 \%, \operatorname{CoT}=45.18 \%)$</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Thought: 04/19/1969, today is. Later is one day after today, 24 hours, 04/20/1969, which would be.</td>
<td>$C_{\text {text_emba_thought }}(p)$ (Table 72)</td>
<td>$30.75 \%$</td>
</tr>
</tbody>
</table>
<p>$C_{\text {pat_wrong }}(p)$, the model simply fails to form a connection between the first and second clauses, leading to a substantially lower task solve rate $(93.67 \% \mapsto 46.02 \%)$. Appendix-Table 21 shows additional results for using wrong patterns in thoughts.</p>
<p>Qualitative Analysis of the Role of Patterns Counterfactual prompts with wrong patterns strike an interesting perspective in SPORTS and Sorting. In the SPORTS dataset, replacing the sport activity with a factually wrong one (e.g., "basketball" $\mapsto$ "soccer") presumably disorients the model about the target task. Therefore, the model can not elicit factual connections between the player and activity, hence, it fails to attain a correct answer. Surprisingly, the accuracy of the model is slightly below random ( $50 \%$ ), hinting at the possibility that the model "actively" generates incorrect answers. We present additional analysis in Appendix G.</p>
<h1>6. ROLE OF TEXT</h1>
<p>$\triangleleft \mathcal{H}_{\mathbf{0}} \triangleright$ The presence of text is necessary for the model to arrive at the correct conclusions. While the model can form abstractions, employing concordant entities in questions and thoughts is crucial for the success of CoT. The performance of the model is proportionally sensitive to the degree of variations in the text.</p>
<p>Following the same methodology as symbols and patterns, we employ various tailored counterfactual prompts to retain the entire symbols and patterns, while altering the text or grammar. We present a subset of results in Table 4, and defer additional analysis and results to Appendix H.</p>
<p>Text with discrepant entities $\left{{ }<em _text="\text" _text_diff_entities="{text_diff,entities">{C</em>(p)$ is noticeably better than DIRECT, while still failing to match the performance of CoT. Analyzing the generated outputs for GSM-8K (Table 27) reveals that since the substituted entities are semantically similar (e.g., toys $\equiv$ cookies), the crafted prompts are still meaningful. Interestingly, the model often generates an analogical thought (e.g., using "sweaters" in the thought when the question mentioned apples "apples"). Establishing such connections is more challenging for SPORTS, as replacing the entities demolishes the factual correctness of the thoughts and possibly confuses the model.}}(\boldsymbol{p})}\right}$. To investigate the role of entities, we modify the entities in the thoughts to be discrepant with the ones in the questions. $C_{\text {text_diff,entities }}(p)$ is exclusively relevant for the GSM-8K and SPORTS datasets for which concrete entities exist: objects and people for GSM-8K and players and activities for SPORTS. For SPORTS, changing entities leads to model to disregard the prompt and achieve a similar performance as DIRECT (sample outputs in Table 26). The task solve rate for GSM-8K with $C_{\text {text_diff,entities }</p>
<p>Text with altered grammatical style $\left[C_{\text {text_style }}(p)\right]^{\prime}$. We experiment with Yodish (Kaminski, 2011; Walkden, 2012; Espindola, 2016; Pullum, 2005), a stylistic variation of English (IMDB, 1980; Wookieepedia, 2022) (less frequent on the web) ${ }^{3}$. Yodish forms syntactically valid but non-colloquial English sentences (Honeycutt, 2019). The sentences in Yodish typically use a "XSV" construct where " X " is a phrase that complements the verb " V " and " S " is a subject ${ }^{4}$. In addition, the "XSV" style drifts the object (" X ") apart from the verb ("V"), which leads to a more challenging sentence structure. We experiment with three variants of prompting by reconstructing (a) $C_{\text {text_yodathoughts }}(p)$ : thoughts, (b) $C_{\text {text_yodaquestions }}(p)$ : questions, and (c) $C_{\text {text_yode }}(p)$ : both questions and thoughts. We find that the $C_{\text {text_yodathoughts }}(p)$ (remaining results Appendix H) has moderate (GSM-8K) to drastically negative repercussion (Sports and Date) on task solve rate. We attribute this variation in the model performance to the relation between text and patterns. In some tasks, texts and patterns are intertwined (Sports and Date), and the answer is derived from the text, whereas in other tasks (GSM-8K), patterns are more explicit. In the Sports dataset, the "XSV" Yodish style entails the model to place the sport activity first. For instance, the model is compelled to generate " $A$ baseball player Bryce Harper is" instead of generating "Bryce Harper is a baseball player". We provide samples of generated thoughts in Tables 31 and 32. We also experiment with thoughts of varying degrees of randomness, and find that the amount of randomness directly affects the model performance (more random text is worse) (detailed results in Appendix H).</p>
<h1>7. Symbiosis between Text and Patterns</h1>
<p>The analysis in the preceding sections suggests that patterns ${ }^{5}$ and text form a symbiotic and harmonious relationship. Text without patterns is insufficient to instruct the model to the correct answer (Sections 4 and 5). Conversely, patterns without text cannot successfully recoup the model performance (Section 6). Thoughts purposefully glue patterns with text, forming a symbiotic association. This section attempts to deliver a qualitative analysis and tangible examples to elucidate this association. We analyze samples in which $\operatorname{CoT}(\mathrm{p})$ yields the correct answer to enable systematic analysis, whereas both $C_{\text {pat_none }}(p)$ and $C_{\text {pat_only }}(p)$ are wrong, and summarize the main findings below.</p>
<p>CoT is more effective in solving questions with more patterns. Questions with more patterns require more intermediate steps to arrive at correct answers. Thus, COT is expected to help more in such cases. For GSM-8K, for example, we find that the average number of entities in questions solved exclusively by $\operatorname{CoT}(\mathrm{p})$ is 3.98 compared to the overall average of 3.62 , a statistically significant difference (difference of means t-test $p=0.04$ )- $\boldsymbol{\sim} \mathbf{1 1} \boldsymbol{\sim} \boldsymbol{\mathcal { T } _ { \tau } ^ { \prime } \operatorname { CoT } _ { \tau } ^ { \prime }}$ in Appendix-Table 5 shows an example. Similarly, the presence of patterns not only leads to meaningful thoughts, but also assists the model in not neglecting the symbols. Without patterns, the model has intrinsic tendency to prematurely arrive at a conclusion ( $\boldsymbol{\sim} \mathbf{2} \boldsymbol{\sim} \boldsymbol{\sim} \mathbf{6}$ $\boldsymbol{\sim} \boldsymbol{\mathcal { T } _ { \tau } ^ { \prime } C _ { \text { pat } _ { \text { none } } ( } p \text { ) } ^ { \prime }}$ in Appendix-Table 5), which is more probable to be inaccurate. For Date, we observe several scenarios in which the $C_{\text {pat_only }}(p)$ prevents the model from generating requisite information (e.g., "10 days ago" $\boldsymbol{\sim} \mathbf{4}$ in Appendix-Table 5) and complex patterns (e.g., "day before yesterday" $\boldsymbol{\sim} \mathbf{5}$ in Appendix-Table 5) out of questions.</p>
<p>Text helps in eliciting semantic commonsense knowledge. For example, we observe that in $\boldsymbol{\sim} \mathbf{3}$ of Appendix-Table 5 the model with CoT infuses commonsense knowledge about animals (e.g., " 5 dogs have 4 legs each . . . 2 cats have 4 legs each . . . 10 birds have 2 legs each"). For Date, the model articulates the exact date for "Christmas Eve" in the generated thought ( $\boldsymbol{\sim} \mathbf{6} \boldsymbol{\sim} \boldsymbol{\mathcal { T } _ { \tau } ^ { \prime }}$ CoT $_{\text {, }}$ in Appendix-Table 5), helping the final answer. We summarize our findings as follows:</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 5: Language generates better patterns: error analysis of cases where lack of language leads to misleading and incorrect thoughts for various datasets.</p>
<ul>
<li>Q1 $\mapsto$ Skyler has 100 hats on his hand with the colors red, blue, and white. Half of the hats are red, $3 / 5$ of the remaining hats are blue, and the rest are white. How many white hats does Skyler have?</li>
<li>$\mathrm{T}_{\mathrm{t}}^{\prime} \mathrm{CoT}(\mathrm{p}) \mapsto$ Skyler has 100 hats. Half of the hats are red. So $100 / 2=50$ hats are red. $3 / 5$ of the remaining hats are blue. So $50-3 / 5=30$ hats are blue. The rest are white. So $100-50-30=20$ hats are white. The answer is $20 \checkmark$</li>
<li>$\mathrm{T}<em _pat_on="{pat_on" _text="\text">{\mathrm{t}}^{\prime} C</em> \mapsto 100 / 3=33$. The answer is $33 . X$} 2 y}(p)_{2}^{\prime</li>
<li>02 $\mapsto$ A crocodile grows 8 inches long in 4 years. At this rate, how many inches will it grow in 13 years?</li>
<li>$\mathrm{T}_{\mathrm{t}}^{\prime} \mathrm{CoT}(\mathrm{p}) \mapsto$ The crocodile grows 8 inches long in 4 years. So it grows $8 / 4=2$ inches long each year. 13 years is 13 times 2 , so it will grow $13 * 2=26$ inches long. The answer is $26 . \checkmark$</li>
<li>$\mathrm{T}<em _pat_on="{pat_on" _text="\text">{\mathrm{t}}^{\prime} C</em> \mapsto 8 * 13=104$. The answer is $104 . X$} 1 \mathrm{y}}(p)_{2}^{\prime</li>
<li>03 $\mapsto$ A pet store currently has 5 dogs, 2 cats, and 10 birds. How many legs in total do the pets in the store have?</li>
<li>$\mathrm{T}_{\mathrm{t}}^{\prime} \operatorname{CoT}(\mathrm{p}) \mapsto$ There are 5 dogs, 2 cats, and 10 birds. 5 dogs have 4 legs each, so $5 * 4=20$ legs. 2 cats have 4 legs each, so $2 * 4=8$ legs. 10 birds have 2 legs each, so $10 * 2=20$ legs. So the total number of legs is $20+8+20=48$. The answer is $48 . \checkmark$</li>
<li>$\mathrm{T}<em _pat_on="{pat_on" _text="\text">{\mathrm{t}}^{\prime} C</em> \mapsto 5+2+10=17$. The answer is $17 . X$} 1 \mathrm{y}}(p)_{2}^{\prime</li>
<li>04 $\mapsto$ The current local time is 3:02 pm of 5/4/2004. What is the date 10 days ago in MM/DD/YYYY?</li>
<li>$\mathrm{T}_{\mathrm{t}}^{\prime} \operatorname{CoT}(\mathrm{p}) \mapsto$ The current local time is 3:02 pm of 05/04/2004. 10 days ago would be 04/24/2004. So the answer is $04 / 24 / 2004 . \checkmark$</li>
<li>$\mathrm{T}<em _pat_on="{pat_on" _text="\text">{\mathrm{t}}^{\prime} C</em> \mapsto 5 / 4 / 2004+10$ days $=5 / 14 / 2004$. So the answer is $05 / 14 / 2004 . X$} 2 \mathrm{y}}(p)_{2}^{\prime</li>
<li>05 $\mapsto$ The day before yesterday was 11/23/1933. What is the date yesterday in MM/DD/YYYY?</li>
<li>$\mathrm{T}<em 2="2">{\mathrm{t}}^{\prime} \operatorname{CoT}(\mathrm{p}){ }</em> \mapsto$ The day before yesterday was 11/23/1933. Yesterday was 11/24/1933. So the answer is $11 / 24 / 1933 . \checkmark$}^{\prime</li>
<li>$\mathrm{T}<em _pat_on="{pat_on" _text="\text">{\mathrm{t}}^{\prime} C</em> \mapsto 11 / 23 / 1933-1$ day $=11 / 22 / 1933$. Today $=11 / 22 / 1933-1$ day $=11 / 21 / 1933$. So the answer is $11 / 21 / 1933 . X$} 2 \mathrm{y}}(p)_{2}^{\prime</li>
<li>06 $\mapsto$ Today is Christmas Eve of 1937. What is the date one week ago from today in MM/DD/YYYY?</li>
<li>$\mathrm{T}_{\mathrm{t}}^{\prime} \operatorname{CoT}(\mathrm{p}) \mapsto$ Today is 12/24/1937. One week ago from today is 12/17/1937. So the answer is 12/17/1937. $\checkmark$</li>
<li>$\mathrm{T}<em _pat_on="{pat_on" _text="\text">{\mathrm{t}}^{\prime} C</em> \mapsto 12 / 20 / 1937 X$} 1 \mathrm{y}}(p)_{2}^{\prime</li>
<li>07 $\mapsto$ The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date one year ago from today in MM/DD/YYYY?</li>
<li>$\mathrm{T}<em 2="2">{\mathrm{t}}^{\prime} \operatorname{CoT}(\mathrm{p}){ }</em> \mapsto$ One day after 06/01/1943 is 06/02/1943, so today is 06/02/1943. One year ago from today is 06/02/1942. So the answer is 06/02/1942. $\checkmark$}^{\prime</li>
<li>$\mathrm{T}<em _pat_on="{pat_on" _text="\text">{\mathrm{t}}^{\prime} C</em> \mapsto 1$ year ago from today is 06/01/1942. So the answer is 06/01/1942. $X$} 2 \mathrm{y}}(p)_{2}^{\prime</li>
</ul>
<p>Table 6: Patterns are important: error analysis of cases where lack of patterns leads to misleading and incorrect thoughts for various datasets.</p>
<ul>
<li>$Q 1 \mapsto$ Four dogs sat in a line within the veterinarian's waiting room. The first dog was the Affenpinscher, who weighed only 10 pounds. Next to him sat a Dachshund, who weighed twice as much as the Affenpinscher. Next to the Dachshund sat a Papillon, who weighed one-fourth as much as the Dachshund. And at the end of the line sat a Mastiff, who weighed 44 times the weight of the Papillon. How much did the Mastiff weigh, in pounds?</li>
<li>$\mathcal{T}<em _mathrm_i="\mathrm{i">{\text {i }}^{\prime} \operatorname{CoT}(\mathrm{p})</em> \mapsto$ The Affenpinscher weighed 10 pounds. The Dachshund weighed twice as much as the Affenpinscher, so the Dachshund weighed 20 pounds. The Papillon weighed one-fourth as much as the Dachshund, so the Papillon weighed 5 pounds. The Mastiff weighed 44 times as much as the Papillon, so the Mastiff weighed $44 \times 5=$ 220 pounds. The answer is 220 pounds. $\checkmark$}}^{\prime</li>
<li>$\mathcal{T}<em _pat_nona="{pat_nona" _text="\text">{\text {i }}^{\prime} C</em> \mapsto$ The Affenpinscher weighed 10 pounds. The Dachshund weighed twice as much as the Affenpinscher, so 20 pounds. The Papillon weighed one-fourth as much as the Dachshund, so 5 pounds. The Mastiff weighed 44 times the weight of the Papillon, so 216 pounds. The answer is 216. X}}(p)_{\text {i }}^{\prime</li>
<li>$Q 2 \mapsto$ John injured his back and needs to go to physical therapy. He went to physical therapy for 6 weeks. Each week he went twice for 2 hours at a time. If the sessions cost $\$ 125$ per hour how much did the physical therapy cost?</li>
<li>$\mathcal{T}<em _mathrm_i="\mathrm{i">{\text {i }}^{\prime} \operatorname{CoT}(\mathrm{p})</em> \mapsto$ John went to physical therapy for 6 weeks. Each week he went twice for 2 hours at a time. So he went 12 times. 12 times 2 hours is 24 hours. 24 hours at $\$ 125$ per hour is $24 \times 125=3000$. The answer is $3000 . \checkmark$}}^{\prime</li>
<li>$\mathcal{T}<em _pat_nona="{pat_nona" _text="\text">{\text {i }}^{\prime} C</em> \mapsto$ John went to physical therapy for 6 weeks. Each week he went twice for 2 hours at a time. So he went 12 times. The answer is 12. X}}(p)_{\text {i }}^{\prime</li>
<li>$Q 3 \mapsto$ A pet store currently has 5 dogs, 2 cats, and 10 birds. How many legs in total do the pets in the store have?</li>
<li>$\mathcal{T}<em _mathrm_i="\mathrm{i">{\text {i }}^{\prime} \operatorname{CoT}(\mathrm{p})</em> \mapsto$ There are 5 dogs, 2 cats, and 10 birds. 5 dogs have 4 legs each, so $5 * 4=20$ legs. 2 cats have 4 legs each, so $2 * 4=8$ legs. 10 birds have 2 legs each, so $10 * 2=20$ legs. So the total number of legs is $20+8+20=48$. The answer is $48 . \checkmark$}}^{\prime</li>
<li>$\mathcal{T}<em _pat_nona="{pat_nona" _text="\text">{\text {i }}^{\prime} C</em> \mapsto$ The pet store has 5 dogs, 2 cats, and 10 birds. So the total number of legs is $5+2+10=17$. The answer is 17.X}}(p)_{\text {i }}^{\prime</li>
<li>$Q 4 \mapsto$ Yes or no: Is the following sentence plausible? "Robert Woods killed the powerplay".</li>
<li>$\mathcal{T}<em _mathrm_i="\mathrm{i">{\text {i }}^{\prime} \operatorname{CoT}(\mathrm{p})</em> \mapsto$ Robert Woods is a football player. Killing the powerplay is part of hockey, not football. So the answer is no. $\checkmark$}}^{\prime</li>
<li>$\mathcal{T}<em _pat_nona="{pat_nona" _text="\text">{\text {i }}^{\prime} C</em> \mapsto$ Robert Woods is a football player. Killing the powerplay is a part of hockey. So the answer is yes. $X$}}(p)_{\text {i }}^{\prime</li>
<li>$Q 5 \mapsto$ The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date one year ago from today in MM/DD/YYYY?</li>
<li>$\mathcal{T}<em _mathrm_i="\mathrm{i">{\text {i }}^{\prime} \operatorname{CoT}(\mathrm{p})</em> \mapsto$ One day after 06/01/1943 is 06/02/1943, so today is 06/02/1943. One year ago from today is 06/02/1942. So the answer is 06/02/1942. $\checkmark$}}^{\prime</li>
<li>$\mathcal{T}<em _pat_nona="{pat_nona" _text="\text">{\text {i }}^{\prime} C</em> \mapsto 1$ year ago from today is 06/01/1942. So the answer is 06/01/1942. $X$}}(p)_{\text {i }}^{\prime</li>
</ul>
<p>The search engine analogy: larger language models are better search engines? To better understand the ability of the model to extract rare commonsense knowledge, we resort to the number of Google search results, which we refer to as "Popularity Metric", as a proxy to gauge the rarity of an entity. Employing this metric is germane to PaLM's training dataset, which is a web-based corpus (Chowdhery et al., 2022). We use this metric in the SPORTS dataset because the model is required to reason about factual commonsense knowledge to arrive at the correct conclusions. Table 7 shows a contingency table summarizing the average popularity metric of " $\langle$ Activity $\rangle$ " across correct and incorrect conclusions by COT and $C_{\text {pat_only }}(p)$. We observe that the average popularity metric of activities for which CoT exclusively yields correct answers is lower $(\approx 52 \times)$ compared to the ones for which $C_{\text {pat_only }}(p)$ exclusively lands correct answers. Compared to the average popularity metric across the entire SPORTS dataset (399k), this is still $\approx 6 \times$ lower.</p>
<p>We conjecture that the well-formed structure of thoughts in SPORTS-intertwined patterns and textempower the model to extract commonsense knowledge about " $\langle$ Activity $\rangle$ ", even when the activities are infrequently seen during training. On the contrary, the lack of explanatory thoughts in $C_{\text {pat_only }}(p)$ (See Table 3, Table 21) disorient the model to arrive at the correct conclusion. Therefore, in these cases, the model arrives at the correct conclusion only when the popularity of " $\langle$ Activity $\rangle$ " is significantly high $(3,575 \mathrm{k})$.</p>
<p>Table 7: The average number of Google search results, which we call "Popularity Metric", for activities across correct and incorrect conclusions by CoT and $C_{\text {pat_only }}(p)$. For each activity, we perform Google search with double quotes. Across the entire SPORTS dataset the average popularity metric for corresponding activities is 399k. When CoT is exclusively right, the average popularity metric is 68k. CoT is able to arrive at factual conclusions for activities that are $4 \times$ rarer on the web.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">$C_{\text {pat_only }}(p) \checkmark$</th>
<th style="text-align: left;">$C_{\text {pat_only }}(p) \times$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">COT $\checkmark$</td>
<td style="text-align: left;">472 k</td>
<td style="text-align: left;">$\mathbf{6 8 k}$</td>
</tr>
<tr>
<td style="text-align: left;">COT $\times$</td>
<td style="text-align: left;">$3,575 \mathrm{k}$</td>
<td style="text-align: left;">40 k</td>
</tr>
</tbody>
</table>
<ol>
<li>Patterns are the hidden force that helps generate meaningful text. Without patterns, the model is not purposefully prompted to generate meaningful intermediate text. In addition, patterns reinforce how the model should form connections between different clauses in the intermediate text. This intermediate text drives the model to elicit knowledge and attain correct conclusions.</li>
<li>Text imbues patterns with knowledge and meaning. Patterns need text to effectively impart the requisite information to the model about how to accomplish a task. Text imbues patterns with knowledge, and thereby assists the model in solving a task, such as GSM-8K (e.g., four-legged vs. two-legged animals).</li>
</ol>
<h1>8. CCoT: CONCISE ChAIN OF ThOUGHT</h1>
<p>Based on our initial findings, this section explores the idea of engineering effective prompts tailored to remove ineffectual tokens. The benefit of such tailored design is multi-fold: (1) "reducing noise": reducing noise from prompts could potentially lessen the confusion for the model and lead to a higher task solve rate and (2) "potential energy savings": the reduced number of tokens in the prompts instruct the model to generate less number of tokens per question. Less number of generated tokens directly translates to reduced runtime and carbon footprint per inference (Strubell et al., 2019). This is especially crucial because of the significant compute cost of large language models. Our proposed prompts, dubbed as CCoT, distill the essence of our hypothesis by supplying minimal requisite information to drive the model to factual conclusions. Table 8 compares CoT and CCoT across different tasks and four variants of LLMs. In all tasks, except GSM-8K, we use identical examples in the prompt and only rephrased the thoughts to use less number of tokens. For GSM-8K, we could not find a systematic mechanism to shorten the thoughts. Instead, we randomly harvest questions from the training set whose thoughts are shorter than CoT. Overall,</p>
<p>Table 8: Comparison of task solve rate between CoT and CCoT across PaLM-62B and PaLM-540B. The table in front of each task refers to the prompts related to CCoT.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>PaLM-62B</th>
<th></th>
<th>GPT-3</th>
<th></th>
<th>CODEX</th>
<th></th>
<th>PaLM-540B</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>COT</td>
<td>CCoT (ours)</td>
<td>COT</td>
<td>CCoT (ours)</td>
<td>COT</td>
<td>CCoT (ours)</td>
<td>COT</td>
<td>CCoT (ours)</td>
</tr>
<tr>
<td>GSM-8K (Table 33)</td>
<td>$27.4 \%$</td>
<td>$\mathbf{2 9 . 1 \%}$</td>
<td>$46.9 \%$</td>
<td>$\mathbf{5 2 . 4 \%}$</td>
<td>$\mathbf{6 5 . 6 \%}$</td>
<td>$62.5 \%$</td>
<td>$53.2 \%$</td>
<td>$\mathbf{5 6 . 2 \%}$</td>
</tr>
<tr>
<td>Date (Table 34)</td>
<td>$44.7 \%$</td>
<td>$\mathbf{5 1 . 3 \%}$</td>
<td>$54.1 \%$</td>
<td>$\mathbf{6 1 . 1 \%}$</td>
<td>$69.2 \%$</td>
<td>$\mathbf{7 0 . 0 \%}$</td>
<td>$65.3 \%$</td>
<td>$\mathbf{6 9 . 1 \%}$</td>
</tr>
<tr>
<td>SPORTS (Table 35)</td>
<td>$93.7 \%$</td>
<td>$\mathbf{9 4 . 6 \%}$</td>
<td>$63.5 \%$</td>
<td>$\mathbf{7 4 . 5 \%}$</td>
<td>$98.2 \%$</td>
<td>$\mathbf{9 8 . 4 \%}$</td>
<td>$95.4 \%$</td>
<td>$\mathbf{9 7 . 4 \%}$</td>
</tr>
<tr>
<td>SORTING (table 41)</td>
<td>$55.3 \%$</td>
<td>$\mathbf{6 0 . 2 \%}$</td>
<td>$26.8 \%$</td>
<td>$\mathbf{9 9 . 8 \%}$</td>
<td>$27.2 \%$</td>
<td>$\mathbf{1 0 0 \%}$</td>
<td>$71.2 \%$</td>
<td>$\mathbf{8 8 . 6 \%}$</td>
</tr>
</tbody>
</table>
<p>CCOT outperforms COT while employing prompts with fewer tokens. The task solve rate of CCoT remains relatively high as we scale the model to PaLM-540B, highlighting the efficiency of CCoT. AppendixTable 36 compares the average number of input/output tokens between COT and CCoT. On average, CCoT (our approach) reduces the number of input $(1.39 \times)$ and output tokens $(1.58 \times)$.</p>
<h1>9. Related Work</h1>
<p>Broadly, this paper intersects with a growing body of work on prompting and large language model reasoning (Brown et al., 2020; Chowdhery et al., 2022). Below, we review the most relevant work in these directions.</p>
<p>Exploring the role of examples in few-shot setup With the growing interest in few-shot prompting, several works have explored the role that in-context examples play in the success of few-shot prompting. Notably, Min et al. (2022) find that label correctness is not crucial for the success of the models, and even random labels might lead to competitive performance. Building on this work, Kim et al. (2022) find that the role of the correctness of the labels might be task-dependent. Our findings concur with these methods on label correctness-for GSM-8K, label correctness is not material, whereas it plays a larger role for SPORTS. Surprisingly, we find cases where wrong examples can improve performance by being better indicators of the end task. Our work goes beyond label correctness explored by these methods and teases apart the role of placeholder symbols, patterns, and text in the success of few-shot models. Finally, in addition to comparing the final results (outcome), we also focus on the mechanism (attention patterns), allowing us to reveal instances where model reasoning is identical. Our results also resonate with the work of Reynolds \&amp; McDonell (2021), who found that one of the key roles played by the prompt is to remind the model of the underlying task.</p>
<p>Razeghi et al. (2022) find that pre-training term frequencies can somewhat explain the success of few-shot methods. In line with their work, our experiments on SPORTS also show that DIRECT prompting method is most suited for easy questions (involving personalities and activities found on the web). Finally, Xie et al. (2021) show that in-context learning enables a large model to infer a shared concept between the examples, possibly leading to better task understanding. Our studies on the role of prompt, especially examples where wrong examples lead to better output (e.g., for SORTING), add more empirical evidence to this finding. Further, we show that a symbiotic relationship between text and patterns allows a more efficient inference of task instruction.</p>
<p>Least to most prompting. Zhou et al. (2022) help the model generate a chain of thought by first asking the model to generate the sub-questions for the given problem. Next, the model is asked to answer the subquestions, and finally, the sub-questions, along with sub-answers, are combined to generate the final result. This work is closely related to Kojima et al. (2022), the latter distinguished by generating the rationale from</p>
<p>a large language model directly. We posit that Zhou et al. (2022) derives its key strengths from its ability to generate useful sub-steps. This resonates with our finding that the key contribution of CoT is the extraction of meaningful sub-steps.</p>
<p>Rationale generation as an intermediate step The idea of generating rationales as an intermediate output for reasoning and structured generation tasks has shown promising results for fine-tuned models (Ling et al., 2017; Sun et al., 2019; Rajani et al., 2019; Shwartz et al., 2020; Madaan et al., 2021; Nye et al., 2021; Gu et al., 2022). Recently, Wei et al. (2022) proposed chain-of-thought prompting, which shows that few-shot setups can also be improved by making the model first generate an understanding of the output.</p>
<p>As a natural extension to CoT, Wang et al. (2022b) seek to improve CoT using over-generation using selfconsistency. They sample multiple outputs, and take a plurality vote (i.e. most frequently generated answer) to arrive at the final answer. This general idea of enforcing the model outputs to be consistent has also been explored for symbolic-commonsense reasoning (Kassner et al., 2021). The efficacy of this approach is corroborated by Wang et al. (2022a), who report that taking multiple samples helps a model become robust to settings in a few-shot setup. Our work looks at understanding the efficacy of CoT in the standard setup of generating a single output per input.</p>
<p>Prompt selection. Several works have recently explored the design of the prompt-a process often called "prompt engineering" (Le Scao \&amp; Rush, 2021; Liu et al., 2021c). The methods include dynamically creating prompts based on the question (Liu et al., 2021a; Rubin et al., 2021; Poesia et al., 2021), formatting the prompt as a list or questions (Mishra et al., 2021; Rubin et al., 2021), improving order of examples in the prompt (Lu et al., 2022), and providing instructions in the task (Ouyang et al., 2022). Unlike these techniques, CoT is relatively robust to minor changes in the prompt design. Thus, the findings of our work might be more generally applicable.</p>
<p>Explaining model behavior using counterfactual prompts and attention. As noted by Jacovi \&amp; Goldberg (2020), an explanation of a deep learning system typically serves two different purposes: i) plausibility, which aims to provide an interpretation of system outputs that is convincing for humans, and ii) faithfulness, which aims to capture the actual reasoning process of a model. Our study requires both and uses different means to achieve them. We utilize counterfactual prompts to interpret the system outputs to aid human understanding. This is similar to using posthoc analysis tools (Ribeiro et al., 2016; Lundberg \&amp; Lee, 2017; Liu et al., 2021b), which also focus on analyzing outputs without concern for the details of the model. To get a glimpse of the model's inner workings, we leverage attention (Vaswani et al., 2017), a ubiquitous mechanism in NLP. While the broader question on the utility of attention for posthoc analysis is still open (Jain \&amp; Wallace, 2019; Pruthi et al., 2020), there is some evidence to show that attention can act as an explanation (Wiegreffe \&amp; Pinter, 2019). Finally, the utility of any explanation mechanism is closely tied to the users and application domain (Kaur et al., 2020; Burkart \&amp; Huber, 2021). As our analysis shows, attention adds intuition and insights to the empirical findings.</p>
<p>Counterfactual explanations seek to explain the behavior of a model by performing a what if analysis on examples (Mothilal et al., 2020; Stepin et al., 2021; Verma et al., 2020; Poyiadzi et al., 2020; Goyal et al., 2019). While counterfactuals can be misleading due to artifacts (e.g., see (Laugel et al., 2019; Slack et al., 2021)), they offer a tractable solution for probing large models like PaLM and GPT-3. Notably, unlike finetuned methods, the most important examples for generating the model output are readily available. Thus, counterfactual inputs that show a consistent and systematic change in the model performance are more likely to reflect the model's behavior.</p>
<h1>10. CONCLUSIONS</h1>
<p>This work evaluates the capacity of CoT to elevate complex reasoning in three state-of-the-arts LLMs, PaLM, GPT-3, and CODEX. We systematically assembled a series of controlled counterfactual experiments.</p>
<p>Our results show the initial inklings of connection between text, patterns, and reasoning in LLMs. Our study indicates that the symbiosis of text and patterns bears more weight in the chain of thought reasoning process. In addition, we assert that text is a channel to extract semantic patterns, unlocking the ability of these models to mold correct answers.</p>
<h1>ACKNOWLEDGEMENTS</h1>
<p>We would like to extend our gratitude towards Kathy Meier-Hellstern, Denny Zhou, Victor Veitch, Saleem Abdulrasool, Shruthi Sukumar, Milad Hashemi, Douglas Eck, Christian Szegedy, and Stella Aslibekyan. We also thank the PaLM team and our extended team at Google Research, Brain Team who enabled this research and helped us conduct our experiments.</p>
<h1>REPRODUCIbILITY STATEMENT</h1>
<p>We take the following steps to enable the reproducibility of our work.
Controlling for randomness due to the order of examples. We run each experiment with multiple random seeds to control for randomness due to the order of examples in the prompt. We report the average and standard deviation of the results across all the random seeds. Additionally, we conduct statistical significance tests (McNemar's test (McNemar, 1947)) to compare the results across different prompts. Finally, we evaluate the agreement in output generated by different models using Cohen's kappa ( $\kappa$ ) metric.</p>
<p>Reproducing results. We open sourced the code at https://github.com/google-research/ google-research/tree/master/12da/learned2design. In addition, we have provided scripts for one-click reproduction of the results for the publicly available models in the paper.</p>
<p>Experiments with publicly available models. We experiment with three different language models: PaLM, GPT-3 (text-davinci-002), and CODEX (code-davinci-002). PaLM is not publicly available as of submission time, but the provided source code is compatible with OpenAI API v0.23.0, and can work with any OpenAI models. Finally, CODEX is free to use as of submission time that further helps with the reproducibility of the results.</p>
<p>Prompts and outputs. All the prompts are included in the prompts/ directory. The generated outputs from GPT-3 and CODEX are provided in the outputs/ directory. Each output file follows a standard naming convention: task_name_model_name_sseed.jsonl. Each line of the output file is a json with three fields: 1. prompt + test question ("question"), 2. generated answer ("generated_answer"), 3. true answer ("answer") . A shortened output example is as follows:</p>
<div class="codehilite"><pre><span></span><code>{
    &quot;question&quot;: &quot;Q: Is the following sentence plausible? &#39;Jonas
        Valanciunas beat the buzzer.&#39;\nA: Jonas Valanciunas is a
        basketball player. Beating the buzzer is part of basketball. The
        answer is yes...Q: Is the following sentence plausible? &#39;
        Malcolm Brogdon banked the shot in.&#39;\nA: Malcolm Brogdon is a
        basketball player. Banking the shot in is part of basketball.
        The answer is yes.\n\n\nQ: Is the following sentence plausible?
        &#39;Sam Darnold passed the puck.&#39;\nA: Sam Darnold is an American
        football player. Passing the puck is part of hockey, not
        American football. The answer is no.\n\n\nQ: Yes or no: Is the
        following sentence plausible? \&quot;Javi Martinez launched the
        desperation heave.\&quot;\nA:&quot;,
    &quot;generated_answer&quot;: &quot; Javi Martinez is a soccer player. Launching a
        desperation heave is part of basketball, not soccer. The answer
        is no.&quot;,
    &quot;answer&quot;: &quot;no&quot;,
    &quot;is_correct&quot;: 1
6 }
</code></pre></div>

<h1>ETHICS STATEMENT</h1>
<p>Disseminating reasoning into machines has numerous benefits and applications, from algorithmic reasoning (Li et al., 2022) to code generation (Chen et al., 2021b; Poesia et al., 2021) and formal verification (Wu et al., 2022). While this research does not directly enhance the reasoning capabilities of large language models, it identifies several systematic behavioral patterns in the functioning of few-shot models.
Similar to any technological advances, this work has risks of detrimental societal impact. However, anticipating potential future downsides of such methods is challenging. More than ever, the research community's utmost responsibility is to acknowledge these risks candidly and reflect on practices and strategies to prevent potential harm.</p>
<p>Environmental impact. Training large language models devour a nontrivial amount of compute resources, a limiting factor for frequent training. Few-shot prompting is an appealing solution for mitigating the unfavorable environmental impact of large language models by evading additional iterations of training and dataset collection. Ours and similar studies may lead to more effective prompting techniques and bring technological innovation to the architecture of large language models, especially regarding their reasoning capabilities. Therefore, we hope that the significant compute used in this work can help promote positive environmental outcomes.</p>
<p>Finally, aligned with the credible concerns of the research community, we recognize the longer-term risk of Excellence in artificial intelligence, primarily when it boils down to human reasoning. While distilling comparable human reasoning to machines offers many benefits, undisciplined and uncontrolled progress in this area could be alarming, especially in the presence of bad actors. Effective reasoning in machines as a result of our study, even though not directly, can lead to algorithmic advances that may facilitate bad actors in developing malicious software and systems with human-level capabilities.</p>
<p>We also want to acknowledge a large body of researchers that has greeted innovations in large language models and steady scaling of models with skepticism, questioning the connection between human reasoning and large language models (Han et al., 2022; Binz \&amp; Schulz, 2022). Additionally, recent efforts have started exploring the relationship between the structure of contemporary language models and the human mind (Schrimpf et al., 2021; Tang \&amp; Ha, 2021; Whittington et al., 2021). Despite their progress, the rationale behind CoT's mimicking human reasoning, and any potential connection with linguistics is so far an uncharted territory.</p>
<h2>BIBLIOGRAPHY</h2>
<p>Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as I Can, not as I Say: Grounding Language in Robotic Affordances. arXiv preprint arXiv:2204.01691, 2022.
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms. In ACL, 2019.</p>
<p>BIG-bench Collaboration. Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models. arXiv preprint arXiv:2206.04615, 2022.</p>
<p>Marcel Binz and Eric Schulz. Using Cognitive Psychology to Understand GPT-3. arXiv preprint arXiv:2206.14576, 2022.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. In NeurIPS, 2020.
Nadia Burkart and Marco F Huber. A Survey on the Explainability of Supervised Machine Learning. JAIR, 2021.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating Large Language Models Trained on Code. arXiv preprint arXiv:2107.03374, 2021a.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating Large Language Models Trained on Code. arXiv:2107.03374 [cs], 2021b. arXiv: 2107.03374.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling Language Modeling with Pathways. arXiv preprint arXiv:2204.02311, 2022.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training Verifiers to Solve Math Word Problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Jacob Cohen. A Coefficient of Agreement for Nominal Scales. Educational and psychological measurement, 1960 .</p>
<p>Elaine Espindola. A Systemic Functional Translation Analysis of Thematic Structure: Directing Attention to Yoda's Linguistic Manifestation. Word, 2016.
Amir Feder, Katherine A. Keith, Emaad Manzoor, Reid Pryzant, Dhanya Sridhar, Zach Wood-Doughty, Jacob Eisenstein, Justin Grimmer, Roi Reichart, Margaret E. Roberts, Brandon M. Stewart, Victor Veitch, and Diyi Yang. Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond. arXiv preprint arXiv:2109.00725, 2021.
Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Aremu Anuoluwapo, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna Clinciu, Dipanjan Das, Kaustubh D. Dhole, Wanyu Du, Esin Durmus, Ondřej Dušek, Chris Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Rubungo Andre Niyongabo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, João Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics. arXiv preprint arXiv:2102.01672, 2021.
Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, and Stefan Lee. Counterfactual Visual Explanations. In ICML, 2019.
Yuling Gu, Bhavana Dalvi, and Peter Clark. DREAM: Improving Situational QA by First Elaborating the Situation. In NAACL, 2022.
Simon Jerome Han, Keith Ransom, Andrew Perfors, and Charles Kemp. Human-like Property Induction is a Challenge for Large Language Models. PsyArXiv, 2022.
Micha Heilbron, Kristijan Armeni, Jan-Mathijs Schoffelen, Peter Hagoort, and Floris P De Lange. A Hierarchy of Linguistic Predictions During Natural Language Comprehension. Proceedings of the National Academy of Sciences, 2022.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The Curious Case of Neural Text Degeneration. In $I C L R, 2019$.
Curtis Honeycutt. Correct Grammar, Yoda's Speech Is? Correct Grammar, Yoda's Speech Is?, 2019. Accessed: 2022-08-15.
IMDB. Star Wars: Episode V - The Empire Strikes Back. Star Wars: Episode V - The Empire Strikes Back, 1980. Accessed: 2022-08-15.</p>
<p>Alon Jacovi and Yoav Goldberg. Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness? In ACL, 2020.
Sarthak Jain and Byron C. Wallace. Attention is not Explanation. In NAACL, 2019.
Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch, Naveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon. In-Datacenter Performance Analysis of a Tensor Processing Unit. In ISCA, 2017.
Norman P. Jouppi, Doe Hyun Yoon, Matthew Ashcraft, Mark Gottscho, Thomas B. Jablin, George Kurian, James Laudon, Sheng Li, Peter Ma, Xiaoyu Ma, Thomas Norrie, Nishant Patil, Sushma Prasad, Cliff</p>
<p>Young, Zongwei Zhou, and David Patterson. Ten Lessons from Three Generations Shaped Google's TPUv4i: Industrial Product. In ISCA, 2021.
Michael Kaminski. Yoda-Speak: A Study of Yoda's Speaking Pattern and Their Frequencies. The Secret History of Star Wars, 2011.
Nora Kassner, Oyvind Tafjord, Hinrich Schütze, and Peter Clark. BeliefBank: Adding Memory to a PreTrained Language Model for a Systematic Notion of Belief. In EMNLP, 2021.
Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna M. Wallach, and Jennifer Wortman Vaughan. Interpreting Interpretability: Understanding Data Scientists' Use of Interpretability Tools for Machine Learning. In CHI, 2020.
Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, Kang Min Yoo, and Taeuk Kim. Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations. arXiv preprint arXiv:2205.12685, 2022.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large Language Models are Zero-Shot Reasoners. arXiv preprint arXiv:2205.11916, 2022.
Taku Kudo and John Richardson. SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing. In EMNLP-Demo Track, 2018.
Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building Machines that Learn and Think Like People. Behavioral and brain sciences, 2017.
Thibault Laugel, Marie-Jeanne Lesot, Christophe Marsala, Xavier Renard, and Marcin Detyniecki. The Dangers of Post-hoc Interpretability: Unjustified Counterfactual Explanations. In IJCAI, 2019.
Teven Le Scao and Alexander M Rush. How Many Data Points is a Prompt Worth? In NAACL, 2021.
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving Quantitative Reasoning Problems with Language Models. arXiv preprint arXiv:2206.14858, 2022.
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-Level Code Generation with AlphaCode. arXiv preprint arXiv:2203.07814, 2022.
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems. arXiv preprint arXiv:1705.04146, 2017.
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What Makes Good In-Context Examples for GPT-3? arXiv:2101.06804 [cs], 2021a. arXiv: 2101.06804.
Pengfei Liu, Jinlan Fu, Yang Xiao, Weizhe Yuan, Shuaichen Chang, Junqi Dai, Yixin Liu, Zihuiwen Ye, and Graham Neubig. ExplainaBoard: An Explainable Leaderboard for NLP. In IJCNLP, 2021b.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. arXiv preprint arXiv:2107.13586, 2021c.
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity. In ACL, 2022.
Scott M. Lundberg and Su-In Lee. A Unified Approach to Interpreting Model Predictions. In NeurIPS, 2017.</p>
<p>Aman Madaan, Niket Tandon, Dheeraj Rajagopal, Peter Clark, Yiming Yang, and Eduard Hovy. Think about it! Improving Defeasible Reasoning by First Modeling the Question Scenario. In EMNLP, 2021.
Quinn McNemar. Note on the Sampling Error of the Difference between Correlated Proportions or Percentages. Psychometrika, 1947.
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? arXiv preprint</p>
<p>arXiv:2202.12837, 2022.
Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. Reframing Instructional Prompts to GPTk's Language. arXiv preprint arXiv:2109.07830, 2021.
Ramaravind K Mothilal, Amit Sharma, and Chenhao Tan. Explaining Machine Learning Classifiers Through Diverse Counterfactual Explanations. In $F A T^{*}, 2020$.
Guoshun Nan, Jiaqi Zeng, Rui Qiao, Zhijiang Guo, and Wei Lu. Uncovering Main Causalities for Longtailed Information Extraction. In EMNLP, 2021.
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your Work: Scratchpads for Intermediate Computation with Language Models. arXiv preprint arXiv:2112.00114, 2021.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training Language Models to Follow Instructions with Human Feedback. arXiv preprint arXiv:2203.02155, 2022.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP Models Really Able to Solve Simple Math Word Problems? arXiv preprint arXiv:2103.07191, 2021.
Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani. Synchromesh: Reliable Code Generation from Pre-trained Language Models. In ICLR, 2021.
Rafael Poyiadzi, Kacper Sokol, Raul Santos-Rodriguez, Tijl De Bie, and Peter Flach. FACE: Feasible and Actionable Counterfactual Explanations. In AAAI, 2020.
Danish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham Neubig, and Zachary C. Lipton. Learning to Deceive with Attention-Based Explanations. In ACL, 2020.
Geoffrey K. Pullum. YODA'S Syntax the Tribune Analyzes; Supply more Details I Will! YODA'S Syntax the Tribune Analyzes; Supply more Details I Will!, 2005. Accessed: 2022-08-15.
Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain Yourself! Leveraging Language Models for Commonsense Reasoning. In ACL, 2019.
Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. Impact of Pretraining Term Frequencies on Few-shot Reasoning. arXiv preprint arXiv:2202.07206, 2022.
Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. A Recipe for Arbitrary Text Style Transfer with Large Language Models. arXiv preprint arXiv:2109.03910, 2021.
Laria Reynolds and Kyle McDonell. Prompt Programming for Large Language Models: Beyond the Fewshot Paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, 2021.
Marco Túlio Ribeiro, Sameer Singh, and Carlos Guestrin. "Why Should I Trust You?": Explaining the Predictions of Any Classifier. In SIGKDD, 2016.
Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to Retrieve Prompts for In-context Learning. arXiv preprint arXiv:2112.08633, 2021.
Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. Multitask Prompted Training Enables Zero-Shot Task Generalization, 2021.
Martin Schrimpf, Idan Asher Blank, Greta Tuckute, Carina Kauf, Eghbal A Hosseini, Nancy Kanwisher, Joshua B Tenenbaum, and Evelina Fedorenko. The Neural Architecture of Language: Integrative Modeling Converges on Predictive Processing. Proceedings of the National Academy of Sciences, 2021.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ We did not choose Yodish for comical reasons, but to test whether the frequency of text plays a role in the model performance.
${ }^{4}$ For example, the sentence "Shawn started with 5 toys" is restructured as "With 5 toys, Shawn started" in Yodish.
${ }^{5}$ We use patterns in this section as a superset of symbols.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9304 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9304</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9304</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-269328851</p>
                <p><strong>Paper Title:</strong> Unlocking the Capabilities of Large Language Models for Accelerating Drug Development</p>
                <p><strong>Paper Abstract:</strong> Recent breakthroughs in natural language processing (</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9304.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9304.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Presentation format where an LLM is asked to perform a new task without being shown any examples; the model must generalize purely from the prompt description and pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unlocking the Capabilities of Large Language Models for Accelerating Drug Development</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>general LLMs (e.g., GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Novel task completion / question answering</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Learning or performing a new task (e.g., information extraction, question answering) without in-context examples, relying only on a task description in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>A prompt that describes the task but provides no exemplars (zero-shot); relies on clear, specific instructions and context in the prompt to elicit correct behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared conceptually to few-shot / in-context learning (ICL) where a few examples are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The paper states zero-shot prompting enables models to learn entirely new tasks from a prompt without examples; it is presented as one of the emergent prompting modalities for LLMs, but no quantitative performance is reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>No experimental evaluation or numeric metrics reported in this paper; discussion is conceptual.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unlocking the Capabilities of Large Language Models for Accelerating Drug Development', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9304.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9304.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot / ICL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot learning / In-Context Learning (ICL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Presentation format where a small number of labeled examples ('shots') are included in the prompt so the LLM can infer the task pattern and apply it to new inputs without parameter updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unlocking the Capabilities of Large Language Models for Accelerating Drug Development</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>general LLMs (e.g., GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Information extraction, data curation, question answering</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where a few demonstrations are given in the prompt to teach the model the expected output format and mapping (e.g., extracting structured fields from free-text clinical notes).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot prompts including several task demonstrations (shots) embedded in the context; examples and templated prompts to configure target output structures.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared conceptually to zero-shot (no examples) and to approaches that fine-tune models on annotated datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The paper notes that ICL/few-shot enables configurable tasks without task-specific tuning, can redirect curation effort from code-intensive mapping to manual review, and is effective for on-the-fly data extraction and synthetic data generation for weak supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>No quantitative experiments reported; guidance emphasizes templated prompts, examples in few-shot setting, and ensuring model verification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unlocking the Capabilities of Large Language Models for Accelerating Drug Development', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9304.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9304.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that provides or elicits intermediate reasoning steps in the prompt to improve an LLM's multi-step problem-solving performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unlocking the Capabilities of Large Language Models for Accelerating Drug Development</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>general LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-step reasoning / complex problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that require multi-step logical or arithmetic reasoning where intermediate steps help reach the final answer (e.g., multi-hop inference, reasoning-based QA).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Prompts that include a sequence of intermediate reasoning steps (a reasoning path) either provided in examples or elicited from the model to guide problem solving (chain-of-thought prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared conceptually to standard prompts without intermediate steps; also mentioned alongside Tree of Thought prompting which explores multiple reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The paper reports that CoT prompting has been developed to improve LLM performance in prompting settings by making reasoning explicit; it is presented as a method to improve outcomes on reasoning tasks, though no quantitative values are provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>No experimental details or metrics in this perspective; CoT is described conceptually.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unlocking the Capabilities of Large Language Models for Accelerating Drug Development', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9304.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9304.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree of Thought (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting approach that explores multiple possible intermediate reasoning paths (a combinatorial search of reasoning trajectories) before selecting a final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unlocking the Capabilities of Large Language Models for Accelerating Drug Development</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>general LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Combinatorial reasoning / planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Problems with multiple plausible reasoning paths where exploring a tree of intermediate reasoning states can improve solution quality.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Prompts or prompting strategies that generate multiple reasoning paths (a 'tree' of thought processes) to search for better solutions rather than a single linear chain.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared conceptually to chain-of-thought (single reasoning path) and to standard prompting without explicit reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Paper describes ToT as an advancement to improve performance on combinatorial problem spaces by enumerating multiple reasoning paths; no empirical data in this article to quantify gains.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>No experiments reported in this paper; only conceptual description.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unlocking the Capabilities of Large Language Models for Accelerating Drug Development', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9304.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9304.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Example sequencing & role assignment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequencing examples and assigning roles in few-shot prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt-design practices that include sequencing few-shot examples thoughtfully and assigning a role (e.g., 'act like a clinician') to the LLM to improve alignment with domain expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unlocking the Capabilities of Large Language Models for Accelerating Drug Development</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>general LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Domain-aligned information extraction and task specification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring domain-specific behavior (e.g., clinical information extraction) where examples and role prompts help shape the model's output format and tone.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot prompts where examples are sequenced deliberately, and the prompt assigns a specific role to the LLM (e.g., 'You are a clinician') to encourage domain-appropriate responses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Implicitly compared to few-shot prompts without careful sequencing or role instruction and to zero-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The paper suggests sequencing and role assignment are essential components of effective prompt design that help ensure model verification and better alignment to task goals, though no quantitative measures are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>No experimental evaluation reported; recommended as best-practice guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unlocking the Capabilities of Large Language Models for Accelerating Drug Development', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9304.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9304.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt syntax sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt design sensitivity / lack of consistency across models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observation that subtle syntactic variations in prompts can produce large, nonintuitive changes in LLM outputs, and that optimal prompt design can differ across models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unlocking the Capabilities of Large Language Models for Accelerating Drug Development</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>general LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General NLP tasks (various)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Any task relying on prompt-driven model behavior where prompt wording and formatting influence outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Not a single format, but an observation about sensitivity: small syntax or formatting changes in prompts (even nonintuitive ones) can dramatically alter outputs and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The paper states that prompt design lacks consistency across models and that subtle syntax variations can cause significant output changes, implying that prompt engineering must be iterative and model-specific to optimize performance.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>No experiments reported; this is a reported practical observation based on the literature and authors' perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unlocking the Capabilities of Large Language Models for Accelerating Drug Development', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Emergent abilities of large language models <em>(Rating: 2)</em></li>
                <li>Large language models are few-shot clinical information extractors <em>(Rating: 2)</em></li>
                <li>Holistic evaluation of language models <em>(Rating: 2)</em></li>
                <li>Structured prompt interrogation and recursive extraction of semantics (SPIRES): a method for populating knowledge bases using zeroshot learning <em>(Rating: 1)</em></li>
                <li>Lora: low-rank adaptation of large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9304",
    "paper_id": "paper-269328851",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Zero-shot",
            "name_full": "Zero-shot prompting",
            "brief_description": "Presentation format where an LLM is asked to perform a new task without being shown any examples; the model must generalize purely from the prompt description and pretraining.",
            "citation_title": "Unlocking the Capabilities of Large Language Models for Accelerating Drug Development",
            "mention_or_use": "mention",
            "model_name": "general LLMs (e.g., GPT-3.5)",
            "model_size": null,
            "task_name": "Novel task completion / question answering",
            "task_description": "Learning or performing a new task (e.g., information extraction, question answering) without in-context examples, relying only on a task description in the prompt.",
            "presentation_format": "A prompt that describes the task but provides no exemplars (zero-shot); relies on clear, specific instructions and context in the prompt to elicit correct behavior.",
            "comparison_format": "Compared conceptually to few-shot / in-context learning (ICL) where a few examples are provided.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "The paper states zero-shot prompting enables models to learn entirely new tasks from a prompt without examples; it is presented as one of the emergent prompting modalities for LLMs, but no quantitative performance is reported here.",
            "null_or_negative_result": null,
            "experimental_details": "No experimental evaluation or numeric metrics reported in this paper; discussion is conceptual.",
            "uuid": "e9304.0",
            "source_info": {
                "paper_title": "Unlocking the Capabilities of Large Language Models for Accelerating Drug Development",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Few-shot / ICL",
            "name_full": "Few-shot learning / In-Context Learning (ICL)",
            "brief_description": "Presentation format where a small number of labeled examples ('shots') are included in the prompt so the LLM can infer the task pattern and apply it to new inputs without parameter updates.",
            "citation_title": "Unlocking the Capabilities of Large Language Models for Accelerating Drug Development",
            "mention_or_use": "mention",
            "model_name": "general LLMs (e.g., GPT-3.5)",
            "model_size": null,
            "task_name": "Information extraction, data curation, question answering",
            "task_description": "Tasks where a few demonstrations are given in the prompt to teach the model the expected output format and mapping (e.g., extracting structured fields from free-text clinical notes).",
            "presentation_format": "Few-shot prompts including several task demonstrations (shots) embedded in the context; examples and templated prompts to configure target output structures.",
            "comparison_format": "Compared conceptually to zero-shot (no examples) and to approaches that fine-tune models on annotated datasets.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "The paper notes that ICL/few-shot enables configurable tasks without task-specific tuning, can redirect curation effort from code-intensive mapping to manual review, and is effective for on-the-fly data extraction and synthetic data generation for weak supervision.",
            "null_or_negative_result": null,
            "experimental_details": "No quantitative experiments reported; guidance emphasizes templated prompts, examples in few-shot setting, and ensuring model verification.",
            "uuid": "e9304.1",
            "source_info": {
                "paper_title": "Unlocking the Capabilities of Large Language Models for Accelerating Drug Development",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that provides or elicits intermediate reasoning steps in the prompt to improve an LLM's multi-step problem-solving performance.",
            "citation_title": "Unlocking the Capabilities of Large Language Models for Accelerating Drug Development",
            "mention_or_use": "mention",
            "model_name": "general LLMs",
            "model_size": null,
            "task_name": "Multi-step reasoning / complex problem solving",
            "task_description": "Tasks that require multi-step logical or arithmetic reasoning where intermediate steps help reach the final answer (e.g., multi-hop inference, reasoning-based QA).",
            "presentation_format": "Prompts that include a sequence of intermediate reasoning steps (a reasoning path) either provided in examples or elicited from the model to guide problem solving (chain-of-thought prompting).",
            "comparison_format": "Compared conceptually to standard prompts without intermediate steps; also mentioned alongside Tree of Thought prompting which explores multiple reasoning paths.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "The paper reports that CoT prompting has been developed to improve LLM performance in prompting settings by making reasoning explicit; it is presented as a method to improve outcomes on reasoning tasks, though no quantitative values are provided here.",
            "null_or_negative_result": null,
            "experimental_details": "No experimental details or metrics in this perspective; CoT is described conceptually.",
            "uuid": "e9304.2",
            "source_info": {
                "paper_title": "Unlocking the Capabilities of Large Language Models for Accelerating Drug Development",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Tree of Thought (ToT)",
            "name_full": "Tree of Thought prompting",
            "brief_description": "A prompting approach that explores multiple possible intermediate reasoning paths (a combinatorial search of reasoning trajectories) before selecting a final answer.",
            "citation_title": "Unlocking the Capabilities of Large Language Models for Accelerating Drug Development",
            "mention_or_use": "mention",
            "model_name": "general LLMs",
            "model_size": null,
            "task_name": "Combinatorial reasoning / planning",
            "task_description": "Problems with multiple plausible reasoning paths where exploring a tree of intermediate reasoning states can improve solution quality.",
            "presentation_format": "Prompts or prompting strategies that generate multiple reasoning paths (a 'tree' of thought processes) to search for better solutions rather than a single linear chain.",
            "comparison_format": "Compared conceptually to chain-of-thought (single reasoning path) and to standard prompting without explicit reasoning.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Paper describes ToT as an advancement to improve performance on combinatorial problem spaces by enumerating multiple reasoning paths; no empirical data in this article to quantify gains.",
            "null_or_negative_result": null,
            "experimental_details": "No experiments reported in this paper; only conceptual description.",
            "uuid": "e9304.3",
            "source_info": {
                "paper_title": "Unlocking the Capabilities of Large Language Models for Accelerating Drug Development",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Example sequencing & role assignment",
            "name_full": "Sequencing examples and assigning roles in few-shot prompts",
            "brief_description": "Prompt-design practices that include sequencing few-shot examples thoughtfully and assigning a role (e.g., 'act like a clinician') to the LLM to improve alignment with domain expectations.",
            "citation_title": "Unlocking the Capabilities of Large Language Models for Accelerating Drug Development",
            "mention_or_use": "mention",
            "model_name": "general LLMs",
            "model_size": null,
            "task_name": "Domain-aligned information extraction and task specification",
            "task_description": "Tasks requiring domain-specific behavior (e.g., clinical information extraction) where examples and role prompts help shape the model's output format and tone.",
            "presentation_format": "Few-shot prompts where examples are sequenced deliberately, and the prompt assigns a specific role to the LLM (e.g., 'You are a clinician') to encourage domain-appropriate responses.",
            "comparison_format": "Implicitly compared to few-shot prompts without careful sequencing or role instruction and to zero-shot prompts.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "The paper suggests sequencing and role assignment are essential components of effective prompt design that help ensure model verification and better alignment to task goals, though no quantitative measures are provided.",
            "null_or_negative_result": null,
            "experimental_details": "No experimental evaluation reported; recommended as best-practice guidance.",
            "uuid": "e9304.4",
            "source_info": {
                "paper_title": "Unlocking the Capabilities of Large Language Models for Accelerating Drug Development",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Prompt syntax sensitivity",
            "name_full": "Prompt design sensitivity / lack of consistency across models",
            "brief_description": "Observation that subtle syntactic variations in prompts can produce large, nonintuitive changes in LLM outputs, and that optimal prompt design can differ across models.",
            "citation_title": "Unlocking the Capabilities of Large Language Models for Accelerating Drug Development",
            "mention_or_use": "mention",
            "model_name": "general LLMs",
            "model_size": null,
            "task_name": "General NLP tasks (various)",
            "task_description": "Any task relying on prompt-driven model behavior where prompt wording and formatting influence outputs.",
            "presentation_format": "Not a single format, but an observation about sensitivity: small syntax or formatting changes in prompts (even nonintuitive ones) can dramatically alter outputs and performance.",
            "comparison_format": null,
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "The paper states that prompt design lacks consistency across models and that subtle syntax variations can cause significant output changes, implying that prompt engineering must be iterative and model-specific to optimize performance.",
            "null_or_negative_result": null,
            "experimental_details": "No experiments reported; this is a reported practical observation based on the literature and authors' perspective.",
            "uuid": "e9304.5",
            "source_info": {
                "paper_title": "Unlocking the Capabilities of Large Language Models for Accelerating Drug Development",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Emergent abilities of large language models",
            "rating": 2,
            "sanitized_title": "emergent_abilities_of_large_language_models"
        },
        {
            "paper_title": "Large language models are few-shot clinical information extractors",
            "rating": 2,
            "sanitized_title": "large_language_models_are_fewshot_clinical_information_extractors"
        },
        {
            "paper_title": "Holistic evaluation of language models",
            "rating": 2,
            "sanitized_title": "holistic_evaluation_of_language_models"
        },
        {
            "paper_title": "Structured prompt interrogation and recursive extraction of semantics (SPIRES): a method for populating knowledge bases using zeroshot learning",
            "rating": 1,
            "sanitized_title": "structured_prompt_interrogation_and_recursive_extraction_of_semantics_spires_a_method_for_populating_knowledge_bases_using_zeroshot_learning"
        },
        {
            "paper_title": "Lora: low-rank adaptation of large language models",
            "rating": 1,
            "sanitized_title": "lora_lowrank_adaptation_of_large_language_models"
        }
    ],
    "cost": 0.00872625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Unlocking the Capabilities of Large Language Models for Accelerating Drug Development</p>
<p>Wes Anderson 
Ian Braun 
Roop Bhatnagar 0000-0003-0113-4628
Klaus Romero 0009-0001-3922-3549
Ramona Walls 0009-0001-3922-3549
Marco Schito 
Jagdeep T Podichetty jpodichetty@c-path.org 0009-0001-3922-3549</p>
<p>1 Critical Path Institute,
Tucson, Arizona, USA.</p>
<p>Unlocking the Capabilities of Large Language Models for Accelerating Drug Development
3EEE8BDEDE06C18DD3C6E8983754289D10.1002/cpt.3279Received January 11, 2024; accepted March 27, 2024.
Recent breakthroughs in natural language processing (NLP), particularly in large language models (LLMs), offer substantial advantages in model-informed drug development (MIDD).With billions of parameters and comprehensive pre-training on diverse data, these models effectively extract information from unstructured and structured data throughout the drug development lifecycle.This perspective envisions LLMs supporting MIDD, enhancing drug development, and emphasizes C-Path's strategic use of LLM innovations for actionable real-world evidence from real-world data (RWD).OPPORTUNITIES FOR IMPLEMENTATION</p>
<p>Previously, our work has succinctly summarized the potential opportunities for implementation of NLP in MIDD, emphasizing areas for improvement in the use of smaller scale models. 1 Recent advancements in LLMs showcase unique abilities that are not present in smaller scale models (Figure 1). 2 One is known as prompt engineering, referring to the ability of an LLM to complete a task through a response based on a prompt without further training in both zero-or few-shot settings.In the zero-shot setting, a model learns an entirely new task without being given specific examples, while few-shot learning (or in-context learning [ICL]) involves presenting a LLM with a few task demonstrations ("shots").The model can then produce the anticipated output for test instances by completing an input text sequence without the need for further training or gradient updates.Prompting methods that involve providing an LLM with a sequence of intermediate reasoning steps that make up a reasoning path depicting how to solve a problem (i.e., chainof-thought [COT] prompting), as well as multiple reasoning path from a combinatorial problem space (i.e., Tree of Thought [TOT] prompting), have also been developed to improve the performance of LLMs in the prompting setting.Considerations such as prompt clarity, specificity, domain understanding, context of the topic, and iterative evaluation and prompt adjustment are crucial in prompt engineering.Using techniques such as effective prompting principles, along with ICL, COT, TOT, and similar prompting methods, enable a single LLM to be reused across various tasks (including MIDD) with minimal adaptation or retraining needed for different tasks, enabling rapid adaptation without extensive retraining or task-specific datasets.</p>
<p>C-Path has explored the utility of LLMs in data curation pipelines, especially for information relevant to downstream decisions in free-text or semi-structured fields.Unlike traditional biomedical NLP pipelines that extract specific entities or relationships from clinical notes through training or fine-tuning on large volumes of annotated data, LLMs enable zero-orfew shot question answering with configurable tasks through templated prompts without the need for task-specific tuning. 3-Path envisions leveraging this property of LLMs and simple templates with target output structures to perform arbitrary data cleaning or knowledge extraction tasks.Recently, the utility of this approach was demonstrated through defining and automating data extraction tasks "on-thefly, " 4 redirecting curation time from more code-intensive data-wrangling or mapping approaches to manual review and revision of the LLM-enabled automated extraction processes.</p>
<p>Leveraging prompt engineering to define LLM-assisted tasks dynamically through arbitrary data schemas extends to the data discovery process.Large language models excel in translating natural language inquiries into queries that conform to schemas defined in a few-shot setting to retrieve structured information from PERSPECTIVES or tabular databases.We anticipate leveraging this technique to enable data discovery (e.g., identifying relevant datasets for specific research questions).</p>
<p>Within C-Path, another opportunity for LLM implementation is in synthetic textual data generation, where LLMs such as GPT-3.5 strategically generate data that is representative in content and structure to the desired end point through ICL prompting for several different NLP settings, including fine-tuning a downstream model (i.e., weak supervision 5 ).This approach enhances effectiveness in the MIDD space (including real-world settings), reducing the burden of human annotation or manual data collection for this supervised fine-tuning. 3,5he future of LLMs within NLP tasks related to MIDD across the drug development lifecycle (e.g., biomarker discovery through named entity recognition and assertion status detection using freetext and EHR curation through LLMs) (Figure 2) presents significant implementation opportunities.</p>
<p>IMPROVING USABILITY</p>
<p>The massive size of many of the popular LLMs prohibits their operation on desktop computers.Furthermore, the proprietary nature of models accessible only via a paid API can hinder reproducibility, incur costs, and raise concerns about patient privacy and data security.There is, however, the opportunity to utilize open-source, commercially nonrestricted models stored and readily available for download on platforms such as HuggingFace (https:// huggi ngface.co/ ).Such models allow for flexibility in model usage, both in the form of prompt engineering and fine-tuning, and do not require the use of an API.The ability to fine-tune is especially important for MIDD, as adapting generic existing LLMs to be domain-specific experts can be done effectively through fine-tuning. 3ull model fine-tuning of LLMs demands storage and memory allocation for each task, incurring computational costs.However, recent advancements in parameter-efficient fine tuning (PEFT) methodologies, such as Low-Rank Adaptation (LoRA) 7 of LLMs, mitigate these demands.Low-Rank Adaptation Figure 2 Role of large language models in natural language processing tasks across the drug development lifecycle, as previously described. 1ncludes freezing of pre-trained model weights and the introduction of trainable decomposition matrices into the layers of the transformer architecture.This action diminishes the count of trainable parameters during the fine-tuning phase, enabling the capacity to train and incorporate multiple LoRAs tailored for distinct inference tasks, all based on a base LLM.</p>
<p>Creating impactful prompts is essential for leveraging the potential of zero or fewshot learning without requiring extensive training.This involves crafting task descriptions with clear, decomposable goals, offering model-friendly input data, and using examples in the few-shot setting.Essential components are examples in the few-shot setting, ensuring model verification, utilizing external tools when needed, sequencing examples thoughtfully, and assigning specific roles to the LLM (e.g., humans, like a clinician).Notably, prompt design lacks consistency across models, and subtle syntax variations, often nonintuitive to humans, can result in significant output changes.As research progresses, incorporating new insights into effective prompt design will enhance performance in tasks related to MIDD.</p>
<p>EVALUATING USABILITY/ PERFORMANCE</p>
<p>Evaluation of LLMs has continued to expand since the onset of popular models (e.g., GPT-3.5).This evaluation is dependent on the method of evaluation, the data, and the task of interest.In the scope of MIDD, rigorous evaluations must take place to determine the level at which they can assist in the field of MIDD.</p>
<p>Existing LLM benchmarks combine evaluations of general language tasks and specific downstream tasks, stratified into manual and automated assessments.Manual evaluation relies on human evaluation, introducing high variance and stability challenges due to cultural and individual differences among reviewers. 8Similarly, automated evaluation employs standard scenario-based evaluation metrics (e.g., accuracy, F1-score, ROUGE, and BLEU), with concerns about the limitations of these static metrics and necessitating holistic evaluations like those developed by Liang et al. 9 Large language model evaluation will benefit from real-world datasets, not just those that are smaller and publicly available (e.g., MIMIC-III).C-Path, in its position as a leader in data acquisition and harmonization, can contribute to this work.In the same sense, continued evaluation that furthers the explicit definition of scenarios in which the emergent capabilities of LLMs achieve their purported benefits must occur. 10Finally, we must present evaluations in combination with details in the training schema (e.g., the amount of training data used, along with other training details) to get a reported performance, as it demonstrates how LLMs (clinical LLMs specifically) exhibit improved sample efficiency. 10Obtaining a proper understanding of where LLMs are thriving and where they are not successful helps determine the scope of their use while also warning against the potential bias and inaccuracies that may be present in certain areas or tasks.</p>
<p>INHERENT LIMITATIONS AND POTENTIAL ETHICAL CONSIDERATIONS IN LLMS</p>
<p>C-Path recognizes that challenges exist around addressing the limitations and potential ethical considerations in LLMs.For example, in synthetic data generation, these approaches include noise in generated datasets and the difficulty in ensuring the representativeness of the distribution compared with actual data. 6Addressing these challenges is crucial for optimizing the utility of synthetic data in diverse NLP scenarios.Similarly, noise in the form of hallucinations, or false or misleading information generated by an LLM, leads to inaccurate outputs across NLP tasks, which is especially important in the drug development space, as there is the potential for increased risk to patients and decision making (depending on the task being addressed with the LLM).Although methods such as proper prompting and retrieval augmented generation aim to limit hallucinations, further improvements are needed.Concerns about patient data sharing can be addressed through open-science development of LLMs outside of proprietary models (e.g., GPT-3.5).</p>
<p>Interpretability of LLMs is challenging due to their scale and complexity, making them opaque, while bias in LLMs introduces an additional layer of complexity, potentially skewing the insights derived from these models.As these models evolve alongside their role in drug discovery, it is crucial to address the dual challenges of poor interpretability and potential bias.Developing methodologies to unravel the intricate decision-making process of LLMs enhances transparency, mitigating the risk of bias while advancing their application in MIDD.</p>
<p>CONCLUSION</p>
<p>With the advancement of LLMs, there are many key opportunities for organizations working in MIDD to continue working on implementing LLMs.The challenges of effective prompt design, addressing the lack of interpretability and potential bias in LLMs, and determining the scope of their use in different areas must be addressed head-on to utilize this technology to its full potential.Continued openscience development in this field is crucial to improve the evaluation of the utility of these models with respect to the different NLP tasks while increasing their utility in MIDD.</p>
<p>C-Path has identified implementation strategies, including data curation through ICL, synthetic data generation for weak supervision of more domain-specific models for information extraction, and fine-tuning through computationally efficient methodologies.This will allow for continued progress in harnessing unstructured and structured information to be used and with appropriate evaluation strategies, the implementation of LLMs in the drug development lifecycle will be successful.</p>
<p>Figure 1
1
Figure 1 Emerging considerations for large language models in model-informed drug development.Parameter-efficient fine-tuning; RWD, real-world data.</p>
<p>ACKNOWLEDGEMENTSAI tools were employed for drafting assistance, aiding in the creation of figures and tables, and improving overall readability and language.FUNDING Critical Path Institute is supported by the Food and Drug Administration (FDA) of the Department of Health and Human Services (HHS) and is 55% funded by the FDA/HHS, totalling $17,612,250, and 45% funded by nongovernment source(s), totalling $14,203,111.The contents are those of the author(s) and do not necessarily represent the official views of, nor an endorsement by, FDA/HHS or the US Government.PERSPECTIVESCONFLICT OF INTEREST
How can natural language processing help model informed drug development?: a review. R Bhatnagar, S Sardar, M Beheshti, J T Podichetty, JAMIA Open. 5432022</p>
<p>Emergent abilities of large language models. J Wei, 2022</p>
<p>A survey of large language models. W X Zhao, 2023</p>
<p>Structured prompt interrogation and recursive extraction of semantics (SPIRES): a method for populating knowledge bases using zeroshot learning. J H Caufield, 2023</p>
<p>Large language models are few-shot clinical information extractors. M Agrawal, S Hegselmann, H Lang, Y Kim, D Sontag, 2022</p>
<p>Challenges and applications of large language models. J Kaddour, 2023</p>
<p>Lora: low-rank adaptation of large language models. E J Hu, 2021</p>
<p>A survey on evaluation of large language models. Y Chang, 2023</p>
<p>Holistic evaluation of language models. P Liang, 2022</p>
<p>The shaky foundations of large language models and foundation models for electronic health records. M Wornow, NPJ Digit. Med. 61352023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5147 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5147</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5147</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-108.html">extraction-schema-108</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-c20dd209bfe5fd9d5935a69ae00f3be8530b40e9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c20dd209bfe5fd9d5935a69ae00f3be8530b40e9" target="_blank">Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery</a></p>
                <p><strong>Paper Venue:</strong> Drug Discovery Today</p>
                <p><strong>Paper TL;DR:</strong> The impact made by natural language processing methodologies in the processing of spoken languages accelerated the application of NLP to elucidate hidden knowledge in textual representations of biochemical entities and then use it to construct models to predict molecular properties or to design novel molecules.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5147.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5147.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM RNN generative models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long Short-Term Memory recurrent neural networks for SMILES generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recurrent neural networks with LSTM/GRU cells trained on corpora of SMILES strings to learn the SMILES language distribution and autoregressively generate novel molecules; can be fine-tuned or combined with filtering to produce target-focused libraries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM RNN (and GRU variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>recurrent neural network (autoregressive sequence model, LSTM/GRU)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Large corpora of SMILES strings / molecular databases (training corpus not specified in review; generally SMILES datasets such as ChEMBL/PubChem/GDB-13 are used in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery — de novo molecular generation and target-focused library design</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Language-modeling generation (autoregressive); can be fine-tuned (transfer learning) and combined with activity filtering</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity of generated SMILES, novelty, ability to generate target-specific molecules (after fine-tuning/filtering), downstream synthesizability and physicochemical property assessments (as performed in follow-up work)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>GDB-13 is mentioned as a rediscovery benchmark in related work; specific benchmark names not provided in the review</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>RNN/LSTM models can generate large numbers of novel SMILES and, when fine-tuned or filtered with bioactivity predictors, can produce target-focused libraries; follow-up studies even synthesized some generated compounds.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>RNN-based generators are effective at sampling chemical space and are straightforward to train on SMILES, but suffer from sequence-dependence issues relative to grammar-aware or graph-based models; they are comparable in utility for de novo generation but may require additional validation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>SMILES syntax complexity leads to invalid outputs; sequential RNNs can struggle with long-range dependencies (matching rings/branches); require post-filtering/validation for chemical and synthetic validity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5147.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5147.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transfer learning / fine-tuned LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transfer learning (fine-tuning) of LSTM SMILES generators for target-focused molecule design</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tuning a pre-trained LSTM SMILES generator on small target-active libraries (transfer learning) to bias generation toward structurally similar leads for targets with few known ligands; in some cited work, generated compounds were synthesized.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Fine-tuned LSTM (transfer learning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>recurrent neural network (LSTM) fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pre-trained on large SMILES corpus then fine-tuned on a small, target-focused set of known actives (datasets not specified in review)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery — target-specific lead generation for targets with limited ligand data</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Pretrain on general SMILES corpus; fine-tune weights on target-specific actives; generate molecules from fine-tuned model</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Structural similarity to known actives, property distributions, downstream experimental synthesis of selected compounds (reported in follow-up studies)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Not specified in the review (target-focused training sets implied)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Fine-tuning yields libraries biased toward target chemotypes; in cited examples some generated molecules were synthesized, demonstrating practical applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Transfer learning enables generation for low-data targets where other methods may struggle; complements activity-prediction filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires at least some known actives for fine-tuning; generated molecules still need validation for chemical and synthetic feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5147.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5147.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RL-guided generation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement learning–augmented molecular generative models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of reinforcement learning objectives to bias sequence-generative models toward molecules with desired properties (e.g., solubility, druglikeness, synthesizability) by designing reward functions that guide the generator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reinforcement Learning combined with SMILES generators (e.g., policy-gradient on LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>reinforcement learning (policy optimization) layered on autoregressive sequence generators</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>SMILES corpora for base model training; reward functions encode property estimators or domain heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery — property optimization during molecule generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Start from pre-trained sequence generator (e.g., LSTM) and apply RL to modify generation policy to maximize domain-specific reward functions</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Property scores targeted by rewards (e.g., solubility, QED/druglikeness), validity of generated SMILES, synthesizability proxies</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Not specified in review</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>RL methods can bias generators toward desired properties; cited works used RL to increase scores such as solubility and druglikeness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Provides direct optimization of desired properties during generation vs passive sampling; may outperform unguided generative sampling for target properties.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Designing appropriate reward functions is nontrivial; risk of mode collapse or generation of unrealistic molecules that exploit reward artifacts; still requires downstream validation for chemical realism and synthetic feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5147.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5147.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ORGAN (SeqGAN-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Objective-Reinforced Generative Adversarial Network (ORGAN) for molecular generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A SeqGAN-inspired adversarial sequence generation framework that integrates reinforcement learning with adversarial training to generate molecules optimized for domain-specific objectives (e.g., solubility, druglikeness).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ORGAN (SeqGAN + RL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>sequence-generative adversarial network with reinforcement learning-enhanced objective</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>SMILES corpora for discriminator and generator training; domain-specific reward functions used</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery — generation optimized for properties like solubility, druglikeness, synthesizability</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Adversarial training between generator and discriminator augmented with domain-specific RL rewards to steer molecule generation</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Property-based rewards, validity, diversity, domain-specific metrics (solubility, QED, synthesizability proxies)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Not specified in review</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ORGAN demonstrates the ability to incorporate domain objectives into adversarial generation and to bias outputs toward desirable molecular properties.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Combines advantages of adversarial learning (realistic-like samples) with RL-driven objective optimization; more flexible than vanilla GAN or RL alone.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Training instability typical of GANs; requires careful reward engineering; potential for unrealistic molecules that game the reward.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5147.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5147.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VAE molecular generator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Autoencoder (VAE) architectures for molecule generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>VAE models encode molecules into a continuous latent space and decode samples back to SMILES to generate novel compounds; offers latent-space based optimization but can produce invalid SMILES due to SMILES grammar challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Variational Autoencoder (VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder–decoder probabilistic latent-variable model (variational)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>SMILES corpora</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery — de novo molecule generation and property optimization in latent space</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Encode molecules into latent Gaussian space and sample/optimize in latent space then decode to SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity of decoded SMILES, chemical validity, property prediction and optimization ability</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Not specified in review</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>VAE can generate novel molecules and supports latent-space optimization, but decoders often produce invalid SMILES because SMILES is not context-free and has long-range dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Latent-space methods offer smooth optimization trajectories versus autoregressive RNNs, but face greater syntactic validity issues with SMILES decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>High fraction of invalid SMILES outputs due to SMILES syntax; motivates grammar-aware decoders or alternative representations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5147.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5147.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grammar VAE / SD-VAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grammar Variational Autoencoder (GVAE) and Syntax-Directed VAE (SD-VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Extensions of VAE that incorporate explicit grammar or syntax constraints into the decoder to enforce syntactic (and in SD-VAE, semantic/chemical) validity of generated sequences, improving the quality of generated molecules and druglikeness discriminative features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GVAE / SD-VAE</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>VAE with grammar-constrained / syntax-directed decoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>SMILES corpora; grammar rules for SMILES incorporated</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery — generation of syntactically and semantically valid molecule representations and improved property discrimination</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Enforce grammar production rules during decoding (GVAE) or apply syntax-directed generation to also constrain semantics (SD-VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES (generated via grammar/syntax-constrained rules)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Syntactic validity rate, chemical semantic validity, discriminative power for druglikeness</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Not specified in review</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GVAE ensures syntactic validity; SD-VAE further improves semantic (chemical) validity and yields better latent representations for druglikeness discrimination compared to plain VAE/GVAE.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Grammar-aware VAEs reduce invalid outputs compared to plain VAEs and RNN decoders; tradeoff includes more complex decoder design.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires design/specification of appropriate grammar and semantic constraints; may still require downstream chemical validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5147.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5147.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conditional VAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conditional Variational Autoencoder for conditional molecule generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>VAEs conditioned on desired properties to generate molecules that satisfy property constraints (conditional generation), enabling property-controlled de novo design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Conditional VAE (cVAE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>conditional variational autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>SMILES corpora with associated property labels/values</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery — property-conditional molecule generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Condition the encoder/decoder on target property values so sampling yields molecules with target properties</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Success in generating molecules meeting conditioned property values, validity, novelty</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Not specified in review</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Conditional VAEs are used to bias generation toward specified properties; cited as an approach to generate property-conditioned molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Provides direct conditional control in generation compared to unconstrained VAEs; complementary to RL-based optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Effectiveness depends on quality and size of labeled training data; SMILES validity issues remain unless grammar constraints applied.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5147.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5147.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adversarial AEs / GAN approaches</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adversarial autoencoders and generative adversarial networks for molecules</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of adversarial training (GANs) or adversarial autoencoders to generate molecule sequences that mimic the training distribution, sometimes combined with property objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Adversarial Autoencoder / GAN-based generators (e.g., SeqGAN derivatives)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>adversarial training (GAN) or adversarial autoencoder combined with sequence decoders</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>SMILES corpora</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery — realistic-looking molecule generation and objective-optimized generation when combined with RL</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Adversarial training between generator and discriminator; some architectures integrate RL or objective functions for property optimization</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Realism vs training distribution, property-targeted rewards, validity, diversity</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Not specified in review</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Adversarial approaches can produce realistic-like samples and have been adapted for property optimization, but require careful training to be stable and chemically meaningful.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>GANs can generate samples closer to training distribution compared to simple VAEs, but are typically harder to train and may produce mode collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Training instability, sensitivity to hyperparameters, potential for unrealistic outputs without property constraints or chemical checks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5147.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5147.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELFIES / DeepSMILES representations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-referencIng Embedded Strings (SELFIES) and DeepSMILES (SMILES variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Alternative sequence representations to SMILES designed to improve robustness of sequence-based generative models: SELFIES guarantees syntactic validity under mutation while DeepSMILES modifies ring/branch syntax to reduce certain SMILES errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SELFIES / DeepSMILES</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>SMILES-like string representation (grammar- or semantics-constrained)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Derived from SMILES corpora</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>All sequence-based generative tasks in cheminformatics (improving validity of generated molecules)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Replace SMILES with SELFIES or DeepSMILES as generator training/decoding representation to reduce invalid outputs</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SELFIES or DeepSMILES (convertible to molecular graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity under random mutations, syntactic validity rate of generated sequences, downstream learning performance</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>QM9 cited for evaluation of random-mutation validity comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>SELFIES provides superior robustness/validity under random mutations compared to SMILES/DeepSMILES in cited evaluations; DeepSMILES may reduce some errors but can be more grammar-sensitive and in some studies limited learning ability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>SELFIES reduces invalid-output problem of SMILES and is particularly useful for random-mutation robustness; however, representation choice can affect learning dynamics and model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Changing representation can alter model learning behavior; DeepSMILES' altered syntax may increase sequence length or grammar sensitivity and thus limit some models' learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5147.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e5147.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RNN rediscovery (GDB-13)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RNN-based exploration/rediscovery of GDB-13 chemical space</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of RNN generative models to sample chemical space and assess how much of a large enumerated database (GDB-13) can be rediscovered by a learned generative model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RNN generative model (LSTM/GRU)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>recurrent autoregressive generator</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Training on subsets or distributions relevant to GDB-13 or general SMILES corpora (specifics not provided in review)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Benchmarking generative coverage of enumerated chemical space (exploration of chemical space)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Train generative RNN on dataset(s) and sample to measure rediscovery rate of enumerated database entries</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Rediscovery fraction of GDB-13, validity, novelty</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>GDB-13 (explicitly mentioned as the rediscovery target)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Work explored how much of GDB-13 an RNN-based generative model could rediscover, demonstrating capabilities and limitations of RNN samplers for exhaustive chemical space coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Acts as an empirical benchmark for generative coverage compared to other generative paradigms; specific comparisons not detailed in review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Sampling biases and limitations in covering very large enumerated spaces; validity and diversity trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5147",
    "paper_id": "paper-c20dd209bfe5fd9d5935a69ae00f3be8530b40e9",
    "extraction_schema_id": "extraction-schema-108",
    "extracted_data": [
        {
            "name_short": "LSTM RNN generative models",
            "name_full": "Long Short-Term Memory recurrent neural networks for SMILES generation",
            "brief_description": "Recurrent neural networks with LSTM/GRU cells trained on corpora of SMILES strings to learn the SMILES language distribution and autoregressively generate novel molecules; can be fine-tuned or combined with filtering to produce target-focused libraries.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LSTM RNN (and GRU variants)",
            "model_type": "recurrent neural network (autoregressive sequence model, LSTM/GRU)",
            "model_size": null,
            "training_data": "Large corpora of SMILES strings / molecular databases (training corpus not specified in review; generally SMILES datasets such as ChEMBL/PubChem/GDB-13 are used in cited work)",
            "application_domain": "Drug discovery — de novo molecular generation and target-focused library design",
            "generation_method": "Language-modeling generation (autoregressive); can be fine-tuned (transfer learning) and combined with activity filtering",
            "output_representation": "SMILES",
            "evaluation_metrics": "Validity of generated SMILES, novelty, ability to generate target-specific molecules (after fine-tuning/filtering), downstream synthesizability and physicochemical property assessments (as performed in follow-up work)",
            "benchmarks_or_datasets": "GDB-13 is mentioned as a rediscovery benchmark in related work; specific benchmark names not provided in the review",
            "results_summary": "RNN/LSTM models can generate large numbers of novel SMILES and, when fine-tuned or filtered with bioactivity predictors, can produce target-focused libraries; follow-up studies even synthesized some generated compounds.",
            "comparison_to_other_methods": "RNN-based generators are effective at sampling chemical space and are straightforward to train on SMILES, but suffer from sequence-dependence issues relative to grammar-aware or graph-based models; they are comparable in utility for de novo generation but may require additional validation.",
            "limitations_or_challenges": "SMILES syntax complexity leads to invalid outputs; sequential RNNs can struggle with long-range dependencies (matching rings/branches); require post-filtering/validation for chemical and synthetic validity.",
            "uuid": "e5147.0",
            "source_info": {
                "paper_title": "Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "Transfer learning / fine-tuned LSTM",
            "name_full": "Transfer learning (fine-tuning) of LSTM SMILES generators for target-focused molecule design",
            "brief_description": "Fine-tuning a pre-trained LSTM SMILES generator on small target-active libraries (transfer learning) to bias generation toward structurally similar leads for targets with few known ligands; in some cited work, generated compounds were synthesized.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Fine-tuned LSTM (transfer learning)",
            "model_type": "recurrent neural network (LSTM) fine-tuning",
            "model_size": null,
            "training_data": "Pre-trained on large SMILES corpus then fine-tuned on a small, target-focused set of known actives (datasets not specified in review)",
            "application_domain": "Drug discovery — target-specific lead generation for targets with limited ligand data",
            "generation_method": "Pretrain on general SMILES corpus; fine-tune weights on target-specific actives; generate molecules from fine-tuned model",
            "output_representation": "SMILES",
            "evaluation_metrics": "Structural similarity to known actives, property distributions, downstream experimental synthesis of selected compounds (reported in follow-up studies)",
            "benchmarks_or_datasets": "Not specified in the review (target-focused training sets implied)",
            "results_summary": "Fine-tuning yields libraries biased toward target chemotypes; in cited examples some generated molecules were synthesized, demonstrating practical applicability.",
            "comparison_to_other_methods": "Transfer learning enables generation for low-data targets where other methods may struggle; complements activity-prediction filtering.",
            "limitations_or_challenges": "Requires at least some known actives for fine-tuning; generated molecules still need validation for chemical and synthetic feasibility.",
            "uuid": "e5147.1",
            "source_info": {
                "paper_title": "Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "RL-guided generation",
            "name_full": "Reinforcement learning–augmented molecular generative models",
            "brief_description": "Use of reinforcement learning objectives to bias sequence-generative models toward molecules with desired properties (e.g., solubility, druglikeness, synthesizability) by designing reward functions that guide the generator.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Reinforcement Learning combined with SMILES generators (e.g., policy-gradient on LSTM)",
            "model_type": "reinforcement learning (policy optimization) layered on autoregressive sequence generators",
            "model_size": null,
            "training_data": "SMILES corpora for base model training; reward functions encode property estimators or domain heuristics",
            "application_domain": "Drug discovery — property optimization during molecule generation",
            "generation_method": "Start from pre-trained sequence generator (e.g., LSTM) and apply RL to modify generation policy to maximize domain-specific reward functions",
            "output_representation": "SMILES",
            "evaluation_metrics": "Property scores targeted by rewards (e.g., solubility, QED/druglikeness), validity of generated SMILES, synthesizability proxies",
            "benchmarks_or_datasets": "Not specified in review",
            "results_summary": "RL methods can bias generators toward desired properties; cited works used RL to increase scores such as solubility and druglikeness.",
            "comparison_to_other_methods": "Provides direct optimization of desired properties during generation vs passive sampling; may outperform unguided generative sampling for target properties.",
            "limitations_or_challenges": "Designing appropriate reward functions is nontrivial; risk of mode collapse or generation of unrealistic molecules that exploit reward artifacts; still requires downstream validation for chemical realism and synthetic feasibility.",
            "uuid": "e5147.2",
            "source_info": {
                "paper_title": "Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "ORGAN (SeqGAN-based)",
            "name_full": "Objective-Reinforced Generative Adversarial Network (ORGAN) for molecular generation",
            "brief_description": "A SeqGAN-inspired adversarial sequence generation framework that integrates reinforcement learning with adversarial training to generate molecules optimized for domain-specific objectives (e.g., solubility, druglikeness).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ORGAN (SeqGAN + RL)",
            "model_type": "sequence-generative adversarial network with reinforcement learning-enhanced objective",
            "model_size": null,
            "training_data": "SMILES corpora for discriminator and generator training; domain-specific reward functions used",
            "application_domain": "Drug discovery — generation optimized for properties like solubility, druglikeness, synthesizability",
            "generation_method": "Adversarial training between generator and discriminator augmented with domain-specific RL rewards to steer molecule generation",
            "output_representation": "SMILES",
            "evaluation_metrics": "Property-based rewards, validity, diversity, domain-specific metrics (solubility, QED, synthesizability proxies)",
            "benchmarks_or_datasets": "Not specified in review",
            "results_summary": "ORGAN demonstrates the ability to incorporate domain objectives into adversarial generation and to bias outputs toward desirable molecular properties.",
            "comparison_to_other_methods": "Combines advantages of adversarial learning (realistic-like samples) with RL-driven objective optimization; more flexible than vanilla GAN or RL alone.",
            "limitations_or_challenges": "Training instability typical of GANs; requires careful reward engineering; potential for unrealistic molecules that game the reward.",
            "uuid": "e5147.3",
            "source_info": {
                "paper_title": "Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "VAE molecular generator",
            "name_full": "Variational Autoencoder (VAE) architectures for molecule generation",
            "brief_description": "VAE models encode molecules into a continuous latent space and decode samples back to SMILES to generate novel compounds; offers latent-space based optimization but can produce invalid SMILES due to SMILES grammar challenges.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Variational Autoencoder (VAE)",
            "model_type": "encoder–decoder probabilistic latent-variable model (variational)",
            "model_size": null,
            "training_data": "SMILES corpora",
            "application_domain": "Drug discovery — de novo molecule generation and property optimization in latent space",
            "generation_method": "Encode molecules into latent Gaussian space and sample/optimize in latent space then decode to SMILES",
            "output_representation": "SMILES",
            "evaluation_metrics": "Validity of decoded SMILES, chemical validity, property prediction and optimization ability",
            "benchmarks_or_datasets": "Not specified in review",
            "results_summary": "VAE can generate novel molecules and supports latent-space optimization, but decoders often produce invalid SMILES because SMILES is not context-free and has long-range dependencies.",
            "comparison_to_other_methods": "Latent-space methods offer smooth optimization trajectories versus autoregressive RNNs, but face greater syntactic validity issues with SMILES decoding.",
            "limitations_or_challenges": "High fraction of invalid SMILES outputs due to SMILES syntax; motivates grammar-aware decoders or alternative representations.",
            "uuid": "e5147.4",
            "source_info": {
                "paper_title": "Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "Grammar VAE / SD-VAE",
            "name_full": "Grammar Variational Autoencoder (GVAE) and Syntax-Directed VAE (SD-VAE)",
            "brief_description": "Extensions of VAE that incorporate explicit grammar or syntax constraints into the decoder to enforce syntactic (and in SD-VAE, semantic/chemical) validity of generated sequences, improving the quality of generated molecules and druglikeness discriminative features.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GVAE / SD-VAE",
            "model_type": "VAE with grammar-constrained / syntax-directed decoder",
            "model_size": null,
            "training_data": "SMILES corpora; grammar rules for SMILES incorporated",
            "application_domain": "Drug discovery — generation of syntactically and semantically valid molecule representations and improved property discrimination",
            "generation_method": "Enforce grammar production rules during decoding (GVAE) or apply syntax-directed generation to also constrain semantics (SD-VAE)",
            "output_representation": "SMILES (generated via grammar/syntax-constrained rules)",
            "evaluation_metrics": "Syntactic validity rate, chemical semantic validity, discriminative power for druglikeness",
            "benchmarks_or_datasets": "Not specified in review",
            "results_summary": "GVAE ensures syntactic validity; SD-VAE further improves semantic (chemical) validity and yields better latent representations for druglikeness discrimination compared to plain VAE/GVAE.",
            "comparison_to_other_methods": "Grammar-aware VAEs reduce invalid outputs compared to plain VAEs and RNN decoders; tradeoff includes more complex decoder design.",
            "limitations_or_challenges": "Requires design/specification of appropriate grammar and semantic constraints; may still require downstream chemical validation.",
            "uuid": "e5147.5",
            "source_info": {
                "paper_title": "Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "Conditional VAE",
            "name_full": "Conditional Variational Autoencoder for conditional molecule generation",
            "brief_description": "VAEs conditioned on desired properties to generate molecules that satisfy property constraints (conditional generation), enabling property-controlled de novo design.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Conditional VAE (cVAE)",
            "model_type": "conditional variational autoencoder",
            "model_size": null,
            "training_data": "SMILES corpora with associated property labels/values",
            "application_domain": "Drug discovery — property-conditional molecule generation",
            "generation_method": "Condition the encoder/decoder on target property values so sampling yields molecules with target properties",
            "output_representation": "SMILES",
            "evaluation_metrics": "Success in generating molecules meeting conditioned property values, validity, novelty",
            "benchmarks_or_datasets": "Not specified in review",
            "results_summary": "Conditional VAEs are used to bias generation toward specified properties; cited as an approach to generate property-conditioned molecules.",
            "comparison_to_other_methods": "Provides direct conditional control in generation compared to unconstrained VAEs; complementary to RL-based optimization.",
            "limitations_or_challenges": "Effectiveness depends on quality and size of labeled training data; SMILES validity issues remain unless grammar constraints applied.",
            "uuid": "e5147.6",
            "source_info": {
                "paper_title": "Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "Adversarial AEs / GAN approaches",
            "name_full": "Adversarial autoencoders and generative adversarial networks for molecules",
            "brief_description": "Use of adversarial training (GANs) or adversarial autoencoders to generate molecule sequences that mimic the training distribution, sometimes combined with property objectives.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Adversarial Autoencoder / GAN-based generators (e.g., SeqGAN derivatives)",
            "model_type": "adversarial training (GAN) or adversarial autoencoder combined with sequence decoders",
            "model_size": null,
            "training_data": "SMILES corpora",
            "application_domain": "Drug discovery — realistic-looking molecule generation and objective-optimized generation when combined with RL",
            "generation_method": "Adversarial training between generator and discriminator; some architectures integrate RL or objective functions for property optimization",
            "output_representation": "SMILES",
            "evaluation_metrics": "Realism vs training distribution, property-targeted rewards, validity, diversity",
            "benchmarks_or_datasets": "Not specified in review",
            "results_summary": "Adversarial approaches can produce realistic-like samples and have been adapted for property optimization, but require careful training to be stable and chemically meaningful.",
            "comparison_to_other_methods": "GANs can generate samples closer to training distribution compared to simple VAEs, but are typically harder to train and may produce mode collapse.",
            "limitations_or_challenges": "Training instability, sensitivity to hyperparameters, potential for unrealistic outputs without property constraints or chemical checks.",
            "uuid": "e5147.7",
            "source_info": {
                "paper_title": "Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "SELFIES / DeepSMILES representations",
            "name_full": "SELF-referencIng Embedded Strings (SELFIES) and DeepSMILES (SMILES variants)",
            "brief_description": "Alternative sequence representations to SMILES designed to improve robustness of sequence-based generative models: SELFIES guarantees syntactic validity under mutation while DeepSMILES modifies ring/branch syntax to reduce certain SMILES errors.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "SELFIES / DeepSMILES",
            "model_type": "SMILES-like string representation (grammar- or semantics-constrained)",
            "model_size": null,
            "training_data": "Derived from SMILES corpora",
            "application_domain": "All sequence-based generative tasks in cheminformatics (improving validity of generated molecules)",
            "generation_method": "Replace SMILES with SELFIES or DeepSMILES as generator training/decoding representation to reduce invalid outputs",
            "output_representation": "SELFIES or DeepSMILES (convertible to molecular graphs)",
            "evaluation_metrics": "Validity under random mutations, syntactic validity rate of generated sequences, downstream learning performance",
            "benchmarks_or_datasets": "QM9 cited for evaluation of random-mutation validity comparisons",
            "results_summary": "SELFIES provides superior robustness/validity under random mutations compared to SMILES/DeepSMILES in cited evaluations; DeepSMILES may reduce some errors but can be more grammar-sensitive and in some studies limited learning ability.",
            "comparison_to_other_methods": "SELFIES reduces invalid-output problem of SMILES and is particularly useful for random-mutation robustness; however, representation choice can affect learning dynamics and model performance.",
            "limitations_or_challenges": "Changing representation can alter model learning behavior; DeepSMILES' altered syntax may increase sequence length or grammar sensitivity and thus limit some models' learning.",
            "uuid": "e5147.8",
            "source_info": {
                "paper_title": "Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "RNN rediscovery (GDB-13)",
            "name_full": "RNN-based exploration/rediscovery of GDB-13 chemical space",
            "brief_description": "Use of RNN generative models to sample chemical space and assess how much of a large enumerated database (GDB-13) can be rediscovered by a learned generative model.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "RNN generative model (LSTM/GRU)",
            "model_type": "recurrent autoregressive generator",
            "model_size": null,
            "training_data": "Training on subsets or distributions relevant to GDB-13 or general SMILES corpora (specifics not provided in review)",
            "application_domain": "Benchmarking generative coverage of enumerated chemical space (exploration of chemical space)",
            "generation_method": "Train generative RNN on dataset(s) and sample to measure rediscovery rate of enumerated database entries",
            "output_representation": "SMILES",
            "evaluation_metrics": "Rediscovery fraction of GDB-13, validity, novelty",
            "benchmarks_or_datasets": "GDB-13 (explicitly mentioned as the rediscovery target)",
            "results_summary": "Work explored how much of GDB-13 an RNN-based generative model could rediscover, demonstrating capabilities and limitations of RNN samplers for exhaustive chemical space coverage.",
            "comparison_to_other_methods": "Acts as an empirical benchmark for generative coverage compared to other generative paradigms; specific comparisons not detailed in review.",
            "limitations_or_challenges": "Sampling biases and limitations in covering very large enumerated spaces; validity and diversity trade-offs.",
            "uuid": "e5147.9",
            "source_info": {
                "paper_title": "Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery",
                "publication_date_yy_mm": "2020-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.016651,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery</h1>
<p>Hakime Öztürk ${ }^{\mathrm{a}}$, Arzucan Özgür ${ }^{\mathrm{a}}$, Philippe Schwaller ${ }^{\mathrm{b}}$, Teodoro Laino ${ }^{\mathrm{b}, <em>}$, Elif Ozkirimli ${ }^{\mathrm{c}, \mathrm{d}, </em>}$<br>${ }^{a}$ Department of Computer Engineering, Bogazici University, Istanbul, Turkey<br>${ }^{b}$ IBM Research, Zurich, Switzerland<br>${ }^{c}$ Department of Chemical Engineering, Bogazici University, Istanbul, Turkey<br>${ }^{d}$ Department of Biochemistry, University of Zurich, Winterthurerstrasse 190, CH-8057 Zurich, Switzerland</p>
<h4>Abstract</h4>
<p>Text based representations of chemicals and proteins can be thought of as unstructured languages codified by humans to describe domain specific knowledge. Advances in natural language processing (NLP) methodologies in the processing of spoken languages accelerated the application of NLP to elucidate hidden knowledge in textual representations of these biochemical entities and then use it to construct models to predict molecular properties or to design novel molecules. This review outlines the impact made by these advances on drug discovery and aims to further the dialogue between medicinal chemists and computer scientists.</p>
<p>Teaser. The application of natural language processing methodologies to analyze text based representations of molecular structures opens new doors in deciphering the information rich domain of biochemistry toward the discovery and design of novel drugs.</p>
<p>Keywords: Natural Language Processing, Machine Translation, Molecule Generation, Drug Discovery, Cheminformatics, Bioinformatics, Biochemical Languages, SMILES</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>1. Introduction</h1>
<p>The design and discovery of novel drugs for protein targets is powered by an understanding of the underlying principles of protein-compound interaction. Biochemical methods that measure affinity and biophysical methods that describe the interaction in atomistic level detail have provided valuable information toward a mechanistic explanation for bimolecular recognition [1]. However, more often than not, compounds with drug potential are discovered serendipitously or by phenotypic drug discovery [2] since this highly specific interaction is still difficult to predict [3]. Protein structure based computational strategies such as docking [4], ultra-large library docking for discovering new chemotypes [5], and molecular dynamics simulations [4] or ligand based strategies such as quantitative structure-activity relationship (QSAR) [6, 7], and molecular similarity [8] have been powerful at narrowing down the list of compounds to be tested experimentally. With the increase in available data, machine learning and deep learning architectures are also starting to play a significant role in cheminformatics and drug discovery [9]. These approaches often require extensive computational resources or they are limited by the availability of 3D information. On the other hand, text based representations of biochemical entities are more readily available as evidenced by the 19,588 biomolecular complexes (3D structures) in PDB-Bind [10] (accessed on Nov 13, 2019) compared with 561,356 (manually annotated and reviewed) protein sequences in Uniprot [11] (accessed on Nov 13, 2019) or 97 million compounds in Pubchem [12] (accessed on Nov 13, 2019). The advances in natural language processing (NLP) methodologies make processing of text based representations of biomolecules an area of intense research interest.</p>
<p>The discipline of natural language processing (NLP) comprises a variety of methods that explore a large amount of textual data in order to bring unstructured, latent (or hidden) knowledge to the fore [13]. Advances in this field are beneficial for tasks that use language (textual data) to build insight. The</p>
<p>languages in the domains of bioinformatics and cheminformatics can be investigated under three categories: (i) natural language (mostly English) that is used in documents such as scientific publications, patents, and web pages, (ii) domain specific language, codified by a systematic set of rules extracted from empirical data and describing the human understanding of that domain (e.g. proteins, chemicals, etc), and (iii) structured forms such as tables, ontologies, knowledge graphs or databases [14]. Processing and extracting information from textual data written in natural languages is one of the major application areas of NLP methodologies in the biomedical domain (also known as BioNLP). Information extracted with BioNLP methods is most often shared in structured databases or knowledge graphs [15]. We refer the reader to the comprehensive review on BioNLP by Krallinger et al. [16]. Here, we will be focusing on the application of NLP to domain specific, unstructured biochemical textual representations toward exploration of chemical space in drug discovery efforts.</p>
<p>We can view the textual representation of biomedical/biochemical entities as a domain-specific language. For instance, a genome sequence is an extensive script of four characters (A, T, G, C) constituting a genomic language. In proteins, the composition of 20 different natural amino acids in varying lengths builds the protein sequences. Post-translational modifications expand this 20 letter alphabet and confer different properties to proteins [17]. For chemicals there are several text based alternatives such as chemical formula, IUPAC International Chemical Identifier (InChI) [18] and Simplified Molecular Input Line Entry Specification (SMILES) [19].</p>
<p>Today, the era of "big data" boosts the "learning" aspect of computational approaches substantially, with the ever-growing amounts of information provided by publicly available databases such as PubChem [12], ChEMBL [20], UniProt [11]. These databases are rich in biochemical domain knowledge that is in textual form, thus building an efficient environment in which NLP-based techniques can thrive. Furthermore, advances in computational power allow the design of more complex methodologies, which in turn drive the fields of machine learning (ML) and NLP. However, biological and chemical interpretability and</p>
<p>explainability remain among the major challenges of AI-based approaches. Data management in terms of access, interoperability and reusability are also critical for the development of NLP models that can be shared across disciplines.</p>
<p>With this review, we aim to provide an outline of how the field of NLP has influenced the studies in bioinformatics and cheminformatics and the impact it has had over the last decade. Not only are NLP methodologies facilitating processing and exploitation of biochemical text, they also promise an "understanding" of biochemical language to elucidate the underlying principles of bimolecular recognition. NLP technologies are enhancing the biological and chemical knowledge with the final goal of accelerating drug discovery for improving human health. We highlight the significance of an interdisciplinary approach that integrates computer science and natural sciences.</p>
<h1>1.1. NLP Basics</h1>
<p>Chowdhury [21] describes NLP on three levels: (i) the word level in which the smallest meaningful unit is extracted to define the morphological structure, (ii) the sentence level where grammar and syntactic validity are determined, and (iii) the domain or context level in which the sentences have global meaning. Similarly, our review is organized in three parts in which bio-chemical data is investigated at: (i) word level, (ii) sentence (text) level, and (iii) understanding text and generating meaningful sequences. Table 1 summarizes important NLP concepts related to the processing of biochemical data. We refer to these concepts and explain their applications in the following sections.</p>
<p>All NLP technology relates to specific AI architectures. In Table 2 W-we summarize the main ML and deep learning (DL) architectures that will be mentioned throughout the review.</p>
<h2>2. Biochemical Language Processing</h2>
<p>The language-like properties of text-based representations of chemicals were recognized more than 50 years ago by Garfield [22]. He proposed a "chemicolinguistic" approach to representing chemical nomenclature with the aim of</p>
<p>instructing the computer to draw chemical diagrams. Protein sequence has been an important source of information about protein structure and function since Anfinsen's experiment [23]. Alignment algorithms, such as NeedlemanWunsh [24] and Smith-Waterman [25], rely on sequence information to identify functionally or structurally critical elements of proteins (or genes).</p>
<p>To make predictions about the structure and function of compounds or proteins, the understanding of these sequences is critical for bioinformatics tasks with the final goal of accelerating drug discovery. Much like a linguist who uses the tools of language to bring out hidden knowledge, biochemical sequences can be processed to propose novel solutions, such as predicting interactions between chemicals and proteins or generating new compounds based on the level of understanding. In this section, we will review the applications of some of the NLP-concepts to biochemical data in order to solve bio/cheminformatics problems.</p>
<h1>2.1. Textual Chemical Data</h1>
<p>Information about chemicals can be found in repositories such as PubChem [12], which includes information on around 100 million compounds, or Drugbank [26], which includes information on around 10,000 drugs. The main textual sources used in drug discovery are textual representations of chemicals and proteins. Table 3 lists some sources that store different types of biochemical information.</p>
<p>Chemical structures can be represented in different forms that can be onedimensional (1D), 2D, and 3D. Table 4 depicts different identifiers/representations of the drug ampicillin. While the 2D and 3D representations are also used in ML based approaches [9], here we focus on the 1D form, which is the representation commonly used in NLP.</p>
<p>IUPAC name. The International Union of Pure and Applied Chemistry (IUPAC) scheme (i.e. nomenclature) is used to name compounds following predefined rules such that the names of the compounds are unique and consistent with each other (iupac.org/).</p>
<p>Chemical Formula. The chemical formula is one of the simplest and most widelyknown ways of describing chemicals using letters (i.e. element symbols), numbers, parentheses, and $(-/+)$ signs. This representation gives information about which elements and how many of them are present in the compound.</p>
<p>SMILES. The Simplified Molecular Input Entry Specification (SMILES) is a text-based form of describing molecular structures and reactions [19]. SMILES strings can be obtained by traversing the 2 D graph representation of the compound and therefore SMILES provides more complex information than the chemical formula. Moreover, due to its textual form, SMILES takes $50 \%$ to $70 \%$ less space than other representation methods such as an identical connection table (daylight.com/dayhtml/doc/theory/theory.smiles.html).</p>
<p>SMILES notation is similar to a language with its own set of rules. Just like it is possible to express the same concept with different words in natural languages, the SMILES notation allows molecules to be represented with more than one unique SMILES. Although this may sound like a significant ambiguity, the possibility of using different SMILES to represent the same molecule was successfully adopted as a data augmentation strategy by various groups (Bjerrum [27], Kimber et al. [28], Schwaller et al. [29]).</p>
<p>Canonical SMILES can provide a unique SMILES representation. However, different databases such as PubChem and ChEMBL might use different canonicalization algorithms to generate different unique SMILES. OpenSMILES (opensmiles.org/opensmiles.html) is a new platform that aims to universalize the SMILES notation. In isomeric SMILES, isotopism and stereochemistry information of a molecule is encoded using a variety of symbols ("/", "\", "@", "@@").</p>
<p>DeepSMILES. DeepSMILES is a novel SMILES-like notation that was proposed to address two challenges of the SMILES syntax: (i) unbalanced parentheses and (ii) ring closure pairs [30]. It was initially designed to enhance machine/deeplearning based approaches that utilize SMILES data as input (github.com/ nextmovesoftware/deepsmiles). DeepSMILES was adopted in a drug-target</p>
<p>binding affinity prediction task in which the findings highlighted the efficacy of DeepSMILES over SMILES in terms of identifying undetectable patterns [31]. DeepSMILES was also utilized in a molecule generation task in which it was compared to canonical and randomized SMILES text [32]. Here, the results suggested that DeepSMILES might limit the learning ability of the SMILESbased molecule generation models because its syntax is more grammar sensitive with the ring closure alteration and the use of a single symbol for branching (i.e. " $)$ ") introducing longer sequences.</p>
<p>SELFIES. SELF-referencIng Embedding Strings (SELFIES) is an alternative sequence-based representation that is built upon "semantically constrained graphs" [33]. Each symbol in a SELFIES sequence indicates a recursive Chomsky-2 type grammar, and can thus be used to convert the sequence representation to a unique graph. SELFIES utilize SMILES syntax to extract words that will correspond to semantically valid graphs (github.com/aspuru-guzik-group/ selfies). Krenn et al. [33] compared SELFIES, DeepSMILES and SMILES representations in terms of validity in cases where random character mutations are introduced. The evaluations on the QM9 dataset yielded results in the favor of SELFIES.</p>
<p>InChI. InChI is the IUPAC International Chemical Identifier, which is a nonproprietary and open-source structural representation (inchi-trust.org) [34]. The InChIKey is a character-based representation that is generated by hashing the InChI strings in order to shorten them. InChi representation has several layers (each) separated by the "/" symbol.</p>
<p>The software that generates InChi is publicly available and InChi does not suffer from ambiguity problems. However, its less complex structure makes the SMILES representation easier to use as shown in a molecular generation study [35] and in building meaningful chemical representations with a translationbased system [36]. Interestingly, the translation model was able to translate from InChi to canonical SMILES, whereas it failed to translate from canonical</p>
<p>SMILES to InChi. Winter et al. [36] suggested that the complex syntax of InChi made it difficult for the model to generate a correct sequence.</p>
<p>SMARTS. SMiles ARbitrary Target Specification (SMARTS) is a language that contains specialized symbols and logic operators that enable substructure (pattern) search on SMILES strings [37]. SMARTS can be used in any task that requires pattern matching on a SMILES string such as, querying databases or creating rule dictionaries such as RECAP [38] and BRICS [39] to extract fragments from SMILES (daylight.com/dayhtml/doc/theory/theory.smarts.html).</p>
<p>SMIRKS. SMIRKS notation can be used to describe generic reactions (also known as transforms) that comprise one or more changes in atoms and bonds (https://daylight.com/daycgi_tutorials/smirks_examples.html). These transforms are based on "reactant to product" notation, and thus make use of SMILES and SMARTS languages. SMIRKS is utilized in tasks such as constructing an online transform database [40] and predicting metabolic transformations [41]. A recent study achieves a similar performance to rule-based systems in classifying chemical reactions by learning directly from SMILES text with transforms via neural networks [42].</p>
<h1>2.2. Identification of Words/Tokens</h1>
<p>Similar to words in natural languages, we can assume that the "words" of biochemical sequences convey significant information (e.g. folding, function etc) about the entities. In this regard, each compound/protein is analogous to a sentence, and each compound/protein unit is analogous to a word. Therefore, if we can decipher the grammar of biochemical languages, it would be easier to model bio/cheminformatics problems. However, protein and chemical words are not explicitly known and different approaches are needed to extract syntactically and semantically meaningful biochemical word units from these textual information sources (i.e. sequences). Here, we review some of the most common tokenization approaches used to determine the words of biochemical languages.</p>
<p>$k$-mers ( $n$-grams). One of the simplest approaches in NLP to extract a small language unit is to use $k$-mers, also known as $n$-grams. $k$-mers indicate $k$ consecutive overlapping characters that are extracted from the sequence with a sliding window approach. "LINGO", which is one of the earliest applications of $k$-mers in cheminformatics, is the name of the overlapping 4 -mers that are extracted from SMILES strings [43]. 4-mers of the SMILES of ampicillin, "CC1(C(N2C(S1)C(C2=O)NC(=O)C(C3=CC=CC=C3)N)C(=O)O)C", can be listed as $\left{\right.$ 'CC1(', 'C1(C', '1(C(', ..., 'O)O)', ')O)C' }. From a sequence of length $l$, a total of $(l-n)+1 k$-mers can be extracted. Extracting LINGOs from SMILES is a simple yet powerful idea that has been successfully used to compute molecular similarities, to differentiate between bioisosteric and random molecular pairs [43] and in a drug-target interaction prediction task [44], without requiring 2D or 3D information. The results suggested that a SMILESbased approach to compute the similarity of chemicals is not only as good as a 2D-based similarity measurement, but also faster [44].
$k$-mers were successfully utilized as protein [45] and chemical words [46] in protein family classification tasks. 3-mers to 5-mers were often considered as the words of the protein sequence. Motomura et al. [47] reported that some 5mers could be matched to motifs and protein words are most likely a mixture of different $k$-mers. For the protein function prediction task, Cao et al. [48] decided to choose among the 1000 most frequent words to build the protein vocabulary, whereas Ranjan et al. [49] utilized each $k$-mer type separately and showed that 4 -mers provided the best performance. In the latter work, instead of using the whole protein sequence, the words were extracted from different length protein segments, which are also long $k$-mers (i.e. 100-mer, 120-mer) with 30 aminoacid gaps. The use of segmented protein sequences yielded better results than using the whole protein sequence, and important and conserved subsequences were highlighted. $k$-mers were also used as features, along with position specific score matrix features, in the protein fold prediction problem [50].</p>
<p>Longest Common Subsequences. The identification of the longest common subsequence (LCS) of two sequences is critical for detecting their similarity. When there are multiple sequences, LCSs can point to informative patterns. LCSs extracted from SMILES sequences performed similarly well to 4-mers in chemical similarity calculation [44].</p>
<p>Maximum Common Substructure. Cadeddu et al. [51] investigated organic chemistry as a language in an interesting study that extracts maximum common substructures (MCS) from the 2D structures of pairs of compounds to build a vocabulary of the molecule corpus. Contrary to the common idea of functional groups (e.g. methyl, ethyl etc.) being "words" of the chemical language, the authors argued that MCSs (i.e. fragments) can be described as the words of the chemical language [51]. A recent work investigated the distribution of these words in different molecule subsets [52]. The "words" followed Zipf's Law, which indicates the relationship between the frequency of a word and its rank (based on the frequency) [53], similar to most natural languages. Their results also showed that drug "words" are shorter compared to natural product "words".</p>
<p>Minimum Description Length. Minimum Description Length (MDL) is an unsupervised compression-based word segmentation technique in which words of an unknown language are detected by compressing the text corpus. In a protein classification task, each protein was assigned to the family in which its sequence is compressed the most, according to the MDL-based representation [54]. Ganesan et al. [54] investigated whether the MDL-based words of the proteins show similarities to PROSITE patterns [55] and showed that less conserved residues were compressed less by the algorithm. Ganesan et al. [54] also emphasized that the integration of domain knowledge, such as the consideration of the hydrophilic and hydrophobic aminoacids in the words (i.e. grammar building), might prove effective.</p>
<p>Byte-Pair Encoding. Byte-Pair Encoding (BPE) generates words based on high frequency subsequences starting from frequent characters [56]. A recent study</p>
<p>adopted a linguistic-inspired approach to predict protein-protein interactions (PPIs) [57]. Their model was built upon "words" (i.e. bio-words) of the protein language, in which BPE was utilized to build the bio-word vocabulary. Wang et al. [57] suggested that BPE-segmented words indicate a language-like behavior for the protein sequences and reported improved accuracy results compared to using 3 -mers as words.</p>
<p>Pattern-based words. Subsequences that are conserved throughout evolution are usually associated with protein structure and function. These conserved sequences can be detected as patterns via multiple sequence alignment (MSA) techniques and Hidden Markov Models (HMM). PROSITE [55], a public database that provides information on domains and motifs of proteins, uses regular expressions (i.e. RE or regex) to match these subsequences.</p>
<p>Protein domains have been investigated for their potential of being the words of the protein language. One earlier study suggested that folded domains could be considered as "phrases/clauses" rather than "words" because of the higher semantic complexity between them [58]. Later, domains were described as the words, and domain architectures as sentences of the language [59, 60]. Protein domains were treated as the words of multi-domain proteins in order to evaluate the semantic meaning behind the domains [61]. The study supported prior work by Yu et al. [60] suggesting that domains displayed syntactic and semantic features, but there are only a few multi-domain proteins with more than six domains limiting the use of domains as words to build sentences. Protein domains and motifs have also been utilized as words in different drug discovery tasks such as the prediction of drug-target interaction affinity [62, 63]. These studies showed that motifs and domains together contribute to the prediction as much as the use of the full protein sequence.</p>
<p>SMARTS is a well-known regex-based querying language that is used to identify patterns in a SMILES string. SMARTS has been utilized to build specific rules for small-molecule protonation [64], to design novel ligands based on the fragments connected to the active site of a target [65], and to help generate</p>
<p>products in reaction prediction [66]. MolBlocks, a molecular fragmentation tool, also adopted SMARTS dictionaries to partition a SMILES string into overlapping fragments [37]. Furthermore, MACCS [67] and PubChem [12] Fingerprints (FP) are molecular descriptors that are described as binary vectors based on the absence/presence of substructures that are predefined with SMARTS language. A recent study on protein family clustering uses a ligand-centric representation to describe proteins in which ligands were represented with SMILES-based (i.e. 8-mers) representation, MACCS and Extended Connectivity Fingerprint (ECFP6) [46]. The results indicate that three of the ligand representation approaches provide similar performances for protein family clustering.</p>
<p>To the best of our knowledge, there is no comprehensive evaluation of the different word extraction techniques except a comparison by Wang et al. [57] of the performance of BPE-based words against $k$-mers in a PPI prediction task. Such comparison would provide important insights to the bio/cheminformatics community.</p>
<h1>2.3. Text representation</h1>
<p>The representation of a text (e.g. molecule or protein sequence) aims to capture syntactic, semantic or relational meaning. In the widely used Vector Space Model (VSM), a text is represented by a feature vector of either weighted or un-weighted terms [68]. The terms of this vector may correspond to words, phrases, k-grams, characters, or dimensions in a semantic space such as in the distributed word embedding representation models. The similarity between two texts represented in the vector space model is usually computed using the cosine similarity metric [69], which corresponds to the cosine of the angle between the two vectors.</p>
<p>Similarly to the one-hot encoding scheme [70], in the traditional bag-ofwords [71] and term frequency-inverse document frequency (TF-IDF) [72] text representation models, each word corresponds to a different dimension in the vector space. Therefore, the similarity between two words in the vector space is zero, even if they are synonymous or related to each other. In the distributed</p>
<p>representation models [73] on the other hand, words are represented as dense vectors based on their context. Words that occur in similar contexts have similar vector representations. In this subsection, we review these commonly used text representation models with their applications in cheminformatics.</p>
<p>Bag-of-words representation. In this representation model, a text is represented as a vector of bag-of-words, where the multiplicity of the words is taken into account, but the order of the words in the text is lost [71]. For instance, the SMILES of ampicillin ${ }^{~} \mathrm{CC} 1(\mathrm{C}(\mathrm{N} 2 \mathrm{C}(\mathrm{S} 1) \mathrm{C}(\mathrm{C} 2=\mathrm{O}) \mathrm{NC}(=\mathrm{O}) \mathrm{C}($ $\left.\mathrm{C} 3=\mathrm{CC}=\mathrm{CC}=\mathrm{C} 3) \mathrm{N}\right) \mathrm{C}(=\mathrm{O}) \mathrm{O}) \mathrm{C}$ " can be represented as a bag-of 8 -mers as follows: $\left{{ }^{\circ} \mathrm{CC} 1(\mathrm{C}(\mathrm{N} 2 ",{ }^{\circ} \mathrm{C} 1(\mathrm{C}(\mathrm{N} 2 \mathrm{C}", " 1(\mathrm{C}(\mathrm{N} 2 \mathrm{C}(", "(\mathrm{C}(\mathrm{N} 2 \mathrm{C}\left(\mathrm{S}^{\prime \prime}, \ldots,{ }^{\circ} \mathrm{N}) \mathrm{C}(=\mathrm{O}) \mathrm{O}^{\prime \prime}\right.\right.$ ,") $\left.\mathrm{C}(=\mathrm{O}) \mathrm{O}\right)^{\prime \prime},{ }^{\prime \prime} \mathrm{C}(=\mathrm{O}) \mathrm{O}) \mathrm{C} " }$. We can vectorize it as $S=[1,1,1,1, \ldots, 1,1,1]$ in which each number refers to the frequency of the corresponding 8-mer.</p>
<p>Bag-of-words representation was used in molecular similarity computation, in which the SMILES string and the LINGOs extracted from it were treated as the sentence and words, respectively [43]. The unique LINGOs were considered for each pair and a Tanimoto coefficient was used to measure the similarity [43]. Another approach called SMILES Fingerprint (SMIfp) also adopted bag-ofwords to create representations of molecules for a ligand-based virtual screening task [74]. SMIfp considered 34 unique symbols in SMILES strings to create a frequency-based vector representation, which was utilized to compute molecular similarity. SMIfp provided comparable results to a chemical representation technique that also incorporated polar group and topological information, as well as atom and bond information, in recovering active compounds amongst decoys $[74]$.</p>
<p>TF-IDF. The bag-of-words model, which is based on counting the terms of the sentence/document, might prioritize insignificant but frequent words. To overcome this issue, a weighting scheme can be integrated into the vector representation in order to give more importance to the rare terms that might play a key role in detecting similarity between two documents. One popular weighting approach is to use term frequency-inverse document frequency (TF-IDF) [72].</p>
<p>TF refers to the frequency of a term in the document, and IDF denotes the logarithm of the total number of documents over the number of documents in which the term appears. IDF is therefore an indicator of uniqueness. For instance, the IDF of " $\mathrm{C} 3=\mathrm{CC}=\mathrm{CC}$ " is lower than that of " $(\mathrm{C}(\mathrm{N} 2 \mathrm{C}) \mathrm{S}$ ", which appears in fewer compounds. Therefore, the existence of " $(\mathrm{C}(\mathrm{N} 2 \mathrm{C}) \mathrm{S}$ " in a compound may be more informative.</p>
<p>TF-IDF weigthing was utilized to assign weights to LINGOs that were extracted from SMILES in order to compute molecule similarity using cosine similarity [44]. Molecular similarities were then used as input for drug-target interaction prediction. A similar performance between TF-IDF weighted LINGO and a graph-based chemical similarity measurement was obtained. Cadeddu et al. [51] used TF-IDF weighting on chemical bonds to show that bonds with higher TF-IDF scores have a higher probability of breaking.</p>
<p>One-hot representation. In one-hot representation, for a given vocabulary of a text, each unique word/character is represented with a binary vector that has a 1 in the corresponding position, while the vector positions for the remaining words/characters are filled with 0s [70]. One-hot encoding is fast to build, but might lead to sparse vectors with large dimensions based on the size of the vocabulary (e.g. one million unique words in the vocabulary means one million dimensional binary vectors filled with zeros except one). It is a popular choice, especially in machine learning-based bio/cheminformatic studies to encode different types of information such as SMILES characters [75, 76], atom/bond types $[77,78]$ and molecular properties $[79]$.</p>
<p>Distributed representations. The one-hot encoding builds discrete representations, and thus does not consider the relationships between words. For instance, the cosine similarity of two different words is 0 even if they are semantically similar. However, if the word (i.e. 8-mer) " $(\mathrm{C}(\mathrm{N} 2 \mathrm{C}) \mathrm{S}$ " frequently appears together with the word " $\mathrm{C}(\mathrm{C} 2=\mathrm{O}) \mathrm{N}$ " in SMILES strings, this might suggest that they have related "meanings". Furthermore, two words might have similar semantic</p>
<p>meanings even though they are syntactically apart. This is where distributed vector representations come into play.</p>
<p>The distributed word embeddings models gained popularity with the introduction of Word2Vec [73] and GloVe [80]. The main motivation behind the Word2Vec model is to build real-valued high-dimensional vectors for each word in the vocabulary based on the context in which they appear. There are two main approaches in Word2Vec: (i) Skip-Gram and (ii) Continuous Bag of Words (CBOW). The aim of the Skip-Gram model is to predict context words given the center word, whereas in CBOW the objective is to predict the target word given the context words. Figure 1 depicts the Skip-gram architecture in Word2Vec [73]. For the vocabulary of size $V$, given the target word " $2 \mathrm{C}(\mathrm{S}$ ", the model learns to predict two context words. Both target word and context words are represented as one-hot encoded binary vectors of size $V$. The number of neurons in the hidden layer determines the size of the embedding vectors. The weight matrix between the input layer and the hidden layer stores the embeddings of the vocabulary words. The $i^{t h}$ row of the embedding matrix corresponds to the embedding of the $i^{t h}$ word.</p>
<p>The Word2Vec architecture has inspired a great deal of research in the bio/cheminformatics domains. The Word2Vec algorithm has been successfully applied for determining protein classes [45] and protein-protein interactions (PPI) [57]. Asgari and Mofrad [45] treated 3-mers as the words of the protein sequence and observed that 3-mers with similar biophysical and biochemical properties clustered together when their embeddings were mapped onto the 2D space. Wang et al. [57], on the other hand, utilized BPE-based word segmentation (i.e. bio-words) to determine the words. The authors argued that the improved performance for bio-words in the PPI prediction task might be due to the segmentation-based model providing more distinct words than $k$-mers, which include repetitive segments. Another recent study treated multi-domain proteins as sentences in which each domain was recognized as a word [61]. The Word2Vec algorithm was trained on the domains (i.e. PFAM domain identifiers) of eukaryotic protein sequences to learn semantically interpretable representa-</p>
<p>tions of them. The domain representations were then investigated in terms of the Gene Ontology (GO) annotations that they inherit. The results indicated that semantically similar domains share similar GO terms.</p>
<p>The Word2Vec algorithm was also utilized for representation of chemicals. SMILESVec, a text-based ligand representation technique, utilized Word2Vec to learn embeddings for 8-mers (i.e. chemical words) that are extracted from SMILES strings [46]. SMILESVec was utilized in protein representation such that proteins were represented as the average of the SMILESVec vectors of their interacting ligands. The results indicated comparable performances for ligandbased and sequence based protein representations in protein family/superfamily clustering. Mol2Vec [81], on the other hand, was based on the identifiers of the substructures (i.e. words of the chemical) that were extracted via Extended Connectivity Fingerprint (ECFP) [82]. The results showed a better performance with Mol2Vec than with the simple Morgan Fingerprint in a solubility prediction task, and a comparable performance to graph-based chemical representation [83]. Chakravarti [84] also employed the Word2vec model that was trained on the fragments that are extracted from SMILES strings using a graph traversing algorithm. The results favored the distributed fragment-based ligand representation over fragment-based binary vector representation in a ring system clustering task and showed a comparable performance in the prediction of toxicity against Tetrahymena [84]. Figure 2 illustrates the pipeline of a text-based molecule representation based on $k$-mers.</p>
<p>FP2Vec is another method that utilizes embedding representation for molecules, however instead of the Word2Vec algorithm, it depends on a Convolutional Neural Network (CNN) to build molecule representations to be used in toxicity prediction tasks [85]. CNN architectures have also been utilized for drugtarget binding affinity prediction [86] and drug-drug interaction prediction [76] to build representations for chemicals from raw SMILES strings, as well as for protein fold prediction [87] to learn representations for proteins from aminoacid sequences. SMILES2Vec adopted different DL architectures (GRU, LSTM, CNN+GRU, and CNN+LSTM) to learn molecule embeddings, which were then</p>
<p>used to predict toxicity, affinity and solubility [88]. A CNN+GRU combination was better at the prediction of chemical properties. A recent study compared several DL approaches to investigate the effect of different chemical representations, which were learned through these architectures, on a chemical property prediction problem [89]. The authors also combined DL architectures that were trained on SMILES strings with the MACCS fingerprint, proposing a combined representation for molecules (i.e. CheMixNet). The CheMixNet representation outperformed the other representations that were trained on a single data type such as SMILES2Vec (i.e. SMILES) and Chemception (i.e. 2D graph) [90].</p>
<h1>2.4. Text generation</h1>
<p>Text generation is a primary NLP task, where the aim is to generate grammatically and semantically correct text, with many applications ranging from question answering to machine translation [91]. It is generally formulated as a language modeling task, where a statistical model is trained using a large corpus to predict the distribution of the next word in a given context. In machine translation, the generated text is the translation of an input text in another language.</p>
<p>Medicinal chemistry campaigns use methods such as scaffold hopping [92] or fragment-based drug design [4] to build and test novel molecules but the chemotype diversity and novelty may be limited. It is possible to explore uncharted chemical space with text generation models, which learn a distribution from the available data (i.e. SMILES language) and generate novel molecules that share similar physicochemical properties with the existing molecules [75]. Molecule generation can then be followed by assessing physicochemical properties of the generated compound or its binding potential to a target protein [75]. For a comprehensive review of molecule generation methodologies, including graph-based models, we refer the reader to the review of Elton et al. [93]. Machine translation models have also been recently adapted to text-based molecule generation, which start with one "language" such as that of reactants and generate a novel text in another "language" such as that of products [29]. Below, we present</p>
<p>recent studies on text based molecule generation.
RNN models, which learn a probability distribution from a training set of molecules, are commonly used in molecule generation to propose novel molecules similar to the ones in the training data set. For instance, given the SMILES sequence " $\mathrm{C}(=\mathrm{O}$ ", the model would predict the next character to be ")" with a higher probability than " (". The production of valid SMILES strings, however, is a challenge because of the complicated SMILES syntax that utilizes parentheses to indicate branches and ring numbers. The sequential nature of RNNs, which may miss long range dependencies, is a disadvantage of these models [75]. RNN descendants LSTM and GRU, which model long-term dependencies, are better suited for remembering matching rings and branch closures. Motivated by such a hypothesis, Segler et al. [75] and Ertl et al. [94] successfully pioneered de novo molecule generation using LSTM architecture to generate valid novel SMILES. Segler et al. [75] further modified their model to generate target-specific molecules by integrating a target bioactivity prediction step to filter out inactive molecules and then retraining the LSTM network. In another study, transfer learning was adopted to fine-tune an LSTM-based SMILES generation model so that structurally similar leads were generated for targets with few known ligands [95]. Olivecrona et al. [96] and Popova et al. [97] used reinforcement learning (RL) to bias their model toward compounds with desired properties. Merk et al. $[98,99]$ fine-tuned their LSTM model on a target-focused library of active molecules and synthesized some novel compounds. Arús-Pous et al. [100] explored how much of the GDB-13 database [101] they could rediscover by using an RNN-based generative model.</p>
<p>The variational Auto-encoder (VAE) is another widely adopted text generation architecture [102]. Gómez-Bombarelli et al. [35] adopted this architecture for molecule generation. A traditional auto-encoder encodes the input into the latent space, which is then decoded to reconstruct the input. VAE differs from AE by explicitly defining a probability distribution on the latent space to generate new samples. Gómez-Bombarelli et al. [35] hypothesized that the variational part of the system integrates noise to the encoder, so that the decoder</p>
<p>can be more robust to the large diversity of molecules. However, the authors also reported that the non-context free property of SMILES caused by matching ring numbers and parentheses might often lead the decoder to generate invalid SMILES strings. A grammar variational auto-encoder (GVAE), where the grammar for SMILES is explicitly defined instead of the auto-encoder learning the grammar itself, was proposed to address this issue [103]. This way, the generation is based on the pre-defined grammar rules and the decoding process generates grammar production rules that should also be grammatically valid. Although syntactic validity would be ensured, the molecules may not have semantic validity (chemical validity). Dai et al. [104] built upon the VAE [35] and GVAE [103] architectures and introduced a syntax-directed variational autoencoder (SD-VAE) model for the molecular generation task. The syntax-direct generative mechanism in the decoder contributed to creating both syntactically and semantically valid SMILES sequences. Dai et al. [104] compared the latent representations of molecules generated by VAE, GVAE, and SD-VAE, and showed that SD-VAE provided better discriminative features for druglikeness. Blaschke et al. [105] proposed an adversarial AE for the same task. Conditional VAEs $[106,107]$ were trained to generate molecules conditioned on a desired property. The challenges that SMILES syntax presents inspired the introduction of new syntax such as DeepSMILES [30] and SELFIES [33] (details in Section 2.1).</p>
<p>Generative Adversarial Network (GAN) models generate novel molecules by using two components: the generator network generates novel molecules, and the discriminator network aims to distinguish between the generated molecules and real molecules [108]. In text generation models, the novel molecules are drawn from a distribution, which are then fine-tuned to obtain specific features, whereas adversarial learning utilizes generator and discriminator networks to produce novel molecules [108, 109]. ORGAN [109], a molecular generation methodology, was built upon a sequence generative adversarial network (SeqGAN) from NLP [110]. ORGAN integrated RL in order to generate molecules with desirable properties such as solubility, druglikeness, and synthetizability</p>
<p>through using domain-specific rewards [109].</p>
<p>Machine Translation. Machine translation finds use in cheminformatics in "translation" from one language (e.g. reactants) to another (e.g. products). Machine translation is a challenging task because the syntactic and semantic dependencies of each language differ from one another and this may give rise to ambiguities. Neural Machine Translation (NMT) models benefit from the potential of deep learning architectures to build a statistical model that aims to find the most probable target sequence for an input sequence by learning from a corpus of examples [111, 112]. The main advantage of NMT models is that they provide an end-to-end system that utilizes a single neural network to convert the source sequence into the target sequence. Sutskever et al. [111] refer to their model as a sequence-to-sequence (seq2seq) system that addresses a major limitation of DNNs that can only work with fixed-dimensionality information as input and output. However, in the machine translation task, the length of the input sequences is not fixed, and the length of the output sequences is not known in advance.</p>
<p>The NMT models are based on an encoder-decoder architecture that aims to maximize the probability of generating the target sequence (i.e. most likely correct translation) for the given source sequence. The first encoder-decoder architectures in NMT performed poorly as the sequence length increased mainly because the encoder mapped the source sequence into a single fixed-length vector. However, fixed-size representation may be too small to encode all the information required to translate long sequences [113]. To overcome the issue of the fixed context vector (Figure 3a), a new method was developed, in which every source token was encoded into a memory bank independently (Figure 3b). The decoder could then selectively focus on parts of this memory bank during translation [113, 114]. This technique is known as "attention mechanism" [115].</p>
<p>Inspired by the successes in NMT, the first application of seq2seq models in cheminformatics was for reaction prediction by Nam and Kim [116], who proposed to translate the SMILES strings of reactants and separated reagents</p>
<p>to the corresponding product SMILES. The authors hypothesized that the reaction prediction problem can be re-modelled as a translation system in which both inputs and output are sequences. Their model used GRUs for the encoderdecoder and a Bahdanau [113] attention layer in between. Liu et al. [117] in contrast, performed the opposite task, the single-step retrosynthesis prediction, using a similar encoder-decoder model. When given a product and a reaction class, their model predicted the reactants that would react together to form that product. One major challenge in the retrosynthesis prediction task is the possibility of multiple correct targets, because more than one reactant combination could lead to the same product. Similarly to Nam and Kim [116], Schwaller et al. [118] also adopted a seq2seq model to translate precursors into products, utilizing the SMILES representation for the reaction prediction problem. Their model used a different attention mechanism by Luong et al. [114] and LSTMs in the encoder and decoder. By visualizing the attention weights, an atom-wise mapping between the product and the reactants could be obtained and used to understand the predictions better. Schwaller et al. [118] showed that seq2seq models could compete with graph neural network-based models in the reaction prediction task [119].</p>
<p>A translation model was also employed to learn a data-driven representation of molecules [36]. Winter et al. [36] translated between two textual representations of a chemical, InChi and SMILES, to extract latent representations that can integrate the semantic "meaning" of the molecule. The results indicated a statistically significant improvement with the latent representations in a ligandbased virtual screening task against fingerprint methods such as ECFP (i.e. Morgan algorithm). NMT architectures were also adopted in a protein function prediction task for the first time, in which "words" that were extracted from protein sequences are translated into GO identifiers using RNNs as encoder and decoder [48]. Although exhibiting a comparable performance to the state-of-the-art protein function prediction methods, the authors argued that the performance of the model could be improved by determining more meaningful "words" such as biologically interpretable fragments.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Corresponding author</p>
<p>Email addresses: teo@zurich.ibm.com (Teodoro Laino ), elif.ozkirimli@boun.edu.tr (Elif Ozkirimli ), +41 763497471 (Elif Ozkirimli )&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
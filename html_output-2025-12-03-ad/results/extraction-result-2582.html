<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2582 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2582</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2582</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-66.html">extraction-schema-66</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <p><strong>Paper ID:</strong> paper-677cd63110d9fd4f390aecce1d571e4451a13752</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/677cd63110d9fd4f390aecce1d571e4451a13752" target="_blank">HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The HLTCOE LiveRAG submission utilized the GPT-researcher framework for researching the context of the question, filtering the returned results, and generating the final answer, placing 5th in the LiveRAG automatic evaluation for correctness.</p>
                <p><strong>Paper Abstract:</strong> The HLTCOE LiveRAG submission utilized the GPT-researcher framework for researching the context of the question, filtering the returned results, and generating the final answer. The retrieval system was a ColBERT bi-encoder architecture, which represents a passage with many dense tokens. Retrieval used a local, compressed index of the FineWeb10-BT collection created with PLAID-X, using a model fine-tuned for multilingual retrieval. Query generation from context was done with Qwen2.5-7B-Instruct, while filtering was accomplished with m2-bert-80M-8k-retrieval. Up to nine passages were used as context to generate an answer using Falcon3-10B. This system placed 5th in the LiveRAG automatic evaluation for correctness with a score of 1.07.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2582.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2582.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-Researcher (HLTCOE LiveRAG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-Researcher framework as implemented by HLTCOE for the LiveRAG task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end automated research pipeline used to iteratively generate search queries, retrieve and filter passages from a large web collection, assemble contextual snippets, and generate concise answers with an LLM; implemented as the HLTCOE LiveRAG submission for the TREC LiveRAG challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-Researcher (HLTCOE LiveRAG implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A retrieval-augmented generation (RAG) research-assistant pipeline that: (1) takes a user question as input; (2) issues an initial retrieval against an in-house FineWeb-10BT PLAID-X compressed ColBERT index to obtain top passages; (3) runs an LLM (Qwen2.5-7B-Instruct, 8-bit quantized) to generate up to two additional search queries; (4) retrieves passages for those queries (three passages per query plus original) to produce up to nine passages; (5) chunks passages into overlapping snippets (1000 character chunks with 100 character overlap) and filters snippets by embedding cosine similarity (m2-bert-80M-8k-retrieval) with a threshold of >= 0.35; (6) concatenates accepted snippets into a context and prompts a final answer-generation LLM (Falcon3-10B-Instruct) with a 200-word output limit; and (7) returns the generated answer plus an ordered list of documents. Key capabilities demonstrated: automated query formulation, dense-token retrieval, snippet-level semantic filtering, context assembly, and final abstractive answer generation. The implementation also incorporated engineering optimizations (content-hosting separation, batching, multi-process generation) to meet live throughput constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Research Assistant / RAG pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Information retrieval and open-domain question answering across general web knowledge (multilingual content) — evaluated in an online 'LiveRAG' competition context (TREC-style live question answering).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automate the research process for answering short-form, open-domain user questions under time constraints: formulate effective search queries from a question and retrieved context, retrieve relevant documents from a large web collection (FineWeb-10BT), filter and assemble snippet context, and produce a faithful, concise answer. Example tasks include public-health queries (e.g., risk of bird flu from goose droppings), conceptual/philosophical questions, and diverse factual questions drawn from a 500-question LiveRAG challenge set.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Open-ended, high-dimensional and heterogeneous: search space is a large web collection (FineWeb-10BT) with multilingual documents; retrieval operates at token-dense representations (ColBERT) producing many candidate passages; snippet filtering and answer generation must handle noisy, partially-relevant, and sometimes conflicting information. Complexity factors: variable number of relevant sources per question (0–many), variable snippet counts (observed up to 16 snippets in an example), need for semantic matching across languages, and real-time throughput constraints. Quantitative measures reported: up to 9 passages retrieved per question, typical snippet count varied (histograms provided), and final system processed 500 questions in just over 1 hour.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Pre-existing, large-scale collection (FineWeb-10BT) served locally via PLAID-X-compressed ColBERT index; training data for retrieval components came from MS MARCO (translated for multilingual distillation). No new experimental data was generated; data quality varied (some retrieved snippets irrelevant). Exact corpus size not stated in the paper. Retrieval used top-3 passages per query and up to 9 passages total; snippets were derived by 1000-character chunking with overlaps.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Local GPU-backed inference and retrieval: retrieval service after optimization served queries in ~400 ms using an NVIDIA V100; generation used local LLMs (Qwen2.5-7B-Instruct quantized to 8-bit for query generation, Falcon3-10B-Instruct for answer generation) with a single version of each model used and two parallel processes to finish 500 questions in just over one hour. No end-to-end compute-hour or dollar cost provided. Memory/latency bottlenecks arose from duplicated collection memory in the multi-threaded retrieval service and were mitigated by hosting content separately.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended and under-specified: discrete pipeline stages (query generation, retrieval, filtering, generation) but overall task is open-ended and multi-modal (text across domains); stochastic elements come from LLM outputs and retrieval ranking; evaluation uses automatic correctness/faithfulness metrics provided by the LiveRAG challenge. The problem lacks a single ground-truth for many questions and requires domain generality rather than specialized experimental design.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>LiveRAG automatic evaluation metrics: 'correctness' (numeric score used by the challenge) and 'faithfulness' (proportion/score reflecting how grounded outputs are in retrieved sources); also operational metrics (throughput — ability to answer 500 questions within time limits) and ranking among teams (placement).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Challenge performance: correctness score = 1.070111 (system placed 5th among participating teams); faithfulness score = 0.340711 (14th of 25 teams). Operational success: system completed generation for 500 queries in just over one hour. No per-question accuracy percentages or human baseline scores provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Observed limitations include: (1) relatively low faithfulness (0.340711) implying hallucination or omission of source details in many outputs; (2) inclusion of irrelevant snippets in context (example had some clearly irrelevant snippets) which could pollute generation; (3) occasional failure to surface more specific information contained in snippets into the final answer; (4) snippet ordering heuristic not explicitly relevance-ranked, which may reduce the chance that the most relevant information is used; (5) retrieval latency prior to engineering fixes; (6) diminishing returns from scaling the final LLM (authors observed little qualitative improvement using a larger Llama-70B model compared to Falcon-10B), indicating bottleneck upstream in retrieval/context construction rather than model size.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Factors that improved performance included: dense-token ColBERT retrieval with PLAID-X compression for efficient dense retrieval; multilingual fine-tuning (XLM-R fine-tuned via translate-distill) to handle non-English content; automated query generation (Qwen2.5-7B) to expand retrieval coverage; snippet-level semantic filtering (m2-bert embeddings) to reduce noise; engineering optimizations (separating hosted content to reduce memory duplication, batching retrievals, two-process parallelism) enabling required throughput; and constraining the pipeline (reducing generated queries from four to two and passages per query from four to three) to improve efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>The paper reports an anecdotal comparison between Falcon3-10B-Instruct (used in the challenge) and Llama3.1-70B-Instruct on identical input contexts, finding no significant difference in answer quality; authors conclude that improvements to retrieval and context construction likely have greater impact than swapping to a substantially larger final LLM. No quantitative head-to-head metrics beyond qualitative observation were provided.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>HLTCOE at TREC 2024 NeuCLIR Track <em>(Rating: 2)</em></li>
                <li>Overview of the TREC 2024 NeuCLIR Track <em>(Rating: 2)</em></li>
                <li>Transfer Learning Approaches for Building Cross-Language Dense Retrieval Models <em>(Rating: 2)</em></li>
                <li>Translate-Distill: Learning Cross-Language Dense Retrieval by Translation and Distillation <em>(Rating: 2)</em></li>
                <li>Qwen2.5 Technical Report <em>(Rating: 1)</em></li>
                <li>The Falcon 3 family of Open Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2582",
    "paper_id": "paper-677cd63110d9fd4f390aecce1d571e4451a13752",
    "extraction_schema_id": "extraction-schema-66",
    "extracted_data": [
        {
            "name_short": "GPT-Researcher (HLTCOE LiveRAG)",
            "name_full": "GPT-Researcher framework as implemented by HLTCOE for the LiveRAG task",
            "brief_description": "An end-to-end automated research pipeline used to iteratively generate search queries, retrieve and filter passages from a large web collection, assemble contextual snippets, and generate concise answers with an LLM; implemented as the HLTCOE LiveRAG submission for the TREC LiveRAG challenge.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-Researcher (HLTCOE LiveRAG implementation)",
            "system_description": "A retrieval-augmented generation (RAG) research-assistant pipeline that: (1) takes a user question as input; (2) issues an initial retrieval against an in-house FineWeb-10BT PLAID-X compressed ColBERT index to obtain top passages; (3) runs an LLM (Qwen2.5-7B-Instruct, 8-bit quantized) to generate up to two additional search queries; (4) retrieves passages for those queries (three passages per query plus original) to produce up to nine passages; (5) chunks passages into overlapping snippets (1000 character chunks with 100 character overlap) and filters snippets by embedding cosine similarity (m2-bert-80M-8k-retrieval) with a threshold of &gt;= 0.35; (6) concatenates accepted snippets into a context and prompts a final answer-generation LLM (Falcon3-10B-Instruct) with a 200-word output limit; and (7) returns the generated answer plus an ordered list of documents. Key capabilities demonstrated: automated query formulation, dense-token retrieval, snippet-level semantic filtering, context assembly, and final abstractive answer generation. The implementation also incorporated engineering optimizations (content-hosting separation, batching, multi-process generation) to meet live throughput constraints.",
            "system_type": "Automated Research Assistant / RAG pipeline",
            "problem_domain": "Information retrieval and open-domain question answering across general web knowledge (multilingual content) — evaluated in an online 'LiveRAG' competition context (TREC-style live question answering).",
            "problem_description": "Automate the research process for answering short-form, open-domain user questions under time constraints: formulate effective search queries from a question and retrieved context, retrieve relevant documents from a large web collection (FineWeb-10BT), filter and assemble snippet context, and produce a faithful, concise answer. Example tasks include public-health queries (e.g., risk of bird flu from goose droppings), conceptual/philosophical questions, and diverse factual questions drawn from a 500-question LiveRAG challenge set.",
            "problem_complexity": "Open-ended, high-dimensional and heterogeneous: search space is a large web collection (FineWeb-10BT) with multilingual documents; retrieval operates at token-dense representations (ColBERT) producing many candidate passages; snippet filtering and answer generation must handle noisy, partially-relevant, and sometimes conflicting information. Complexity factors: variable number of relevant sources per question (0–many), variable snippet counts (observed up to 16 snippets in an example), need for semantic matching across languages, and real-time throughput constraints. Quantitative measures reported: up to 9 passages retrieved per question, typical snippet count varied (histograms provided), and final system processed 500 questions in just over 1 hour.",
            "data_availability": "Pre-existing, large-scale collection (FineWeb-10BT) served locally via PLAID-X-compressed ColBERT index; training data for retrieval components came from MS MARCO (translated for multilingual distillation). No new experimental data was generated; data quality varied (some retrieved snippets irrelevant). Exact corpus size not stated in the paper. Retrieval used top-3 passages per query and up to 9 passages total; snippets were derived by 1000-character chunking with overlaps.",
            "computational_requirements": "Local GPU-backed inference and retrieval: retrieval service after optimization served queries in ~400 ms using an NVIDIA V100; generation used local LLMs (Qwen2.5-7B-Instruct quantized to 8-bit for query generation, Falcon3-10B-Instruct for answer generation) with a single version of each model used and two parallel processes to finish 500 questions in just over one hour. No end-to-end compute-hour or dollar cost provided. Memory/latency bottlenecks arose from duplicated collection memory in the multi-threaded retrieval service and were mitigated by hosting content separately.",
            "problem_structure": "Open-ended and under-specified: discrete pipeline stages (query generation, retrieval, filtering, generation) but overall task is open-ended and multi-modal (text across domains); stochastic elements come from LLM outputs and retrieval ranking; evaluation uses automatic correctness/faithfulness metrics provided by the LiveRAG challenge. The problem lacks a single ground-truth for many questions and requires domain generality rather than specialized experimental design.",
            "success_metric": "LiveRAG automatic evaluation metrics: 'correctness' (numeric score used by the challenge) and 'faithfulness' (proportion/score reflecting how grounded outputs are in retrieved sources); also operational metrics (throughput — ability to answer 500 questions within time limits) and ranking among teams (placement).",
            "success_rate": "Challenge performance: correctness score = 1.070111 (system placed 5th among participating teams); faithfulness score = 0.340711 (14th of 25 teams). Operational success: system completed generation for 500 queries in just over one hour. No per-question accuracy percentages or human baseline scores provided in the paper.",
            "failure_modes": "Observed limitations include: (1) relatively low faithfulness (0.340711) implying hallucination or omission of source details in many outputs; (2) inclusion of irrelevant snippets in context (example had some clearly irrelevant snippets) which could pollute generation; (3) occasional failure to surface more specific information contained in snippets into the final answer; (4) snippet ordering heuristic not explicitly relevance-ranked, which may reduce the chance that the most relevant information is used; (5) retrieval latency prior to engineering fixes; (6) diminishing returns from scaling the final LLM (authors observed little qualitative improvement using a larger Llama-70B model compared to Falcon-10B), indicating bottleneck upstream in retrieval/context construction rather than model size.",
            "success_factors": "Factors that improved performance included: dense-token ColBERT retrieval with PLAID-X compression for efficient dense retrieval; multilingual fine-tuning (XLM-R fine-tuned via translate-distill) to handle non-English content; automated query generation (Qwen2.5-7B) to expand retrieval coverage; snippet-level semantic filtering (m2-bert embeddings) to reduce noise; engineering optimizations (separating hosted content to reduce memory duplication, batching retrievals, two-process parallelism) enabling required throughput; and constraining the pipeline (reducing generated queries from four to two and passages per query from four to three) to improve efficiency.",
            "comparative_results": "The paper reports an anecdotal comparison between Falcon3-10B-Instruct (used in the challenge) and Llama3.1-70B-Instruct on identical input contexts, finding no significant difference in answer quality; authors conclude that improvements to retrieval and context construction likely have greater impact than swapping to a substantially larger final LLM. No quantitative head-to-head metrics beyond qualitative observation were provided.",
            "human_baseline": null,
            "uuid": "e2582.0",
            "source_info": {
                "paper_title": "HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "HLTCOE at TREC 2024 NeuCLIR Track",
            "rating": 2
        },
        {
            "paper_title": "Overview of the TREC 2024 NeuCLIR Track",
            "rating": 2
        },
        {
            "paper_title": "Transfer Learning Approaches for Building Cross-Language Dense Retrieval Models",
            "rating": 2
        },
        {
            "paper_title": "Translate-Distill: Learning Cross-Language Dense Retrieval by Translation and Distillation",
            "rating": 2
        },
        {
            "paper_title": "Qwen2.5 Technical Report",
            "rating": 1
        },
        {
            "paper_title": "The Falcon 3 family of Open Models",
            "rating": 1
        }
    ],
    "cost": 0.008643999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval</h1>
<p>Kevin Duh<br>HLTCOE at Johns Hopkins University<br>Baltimore, MD, USA<br>kduh1@jhu.edu</p>
<p>Eugene Yang<br>HLTCOE at Johns Hopkins University<br>Baltimore, MD, USA<br>eugene.yang@jhu.edu</p>
<p>Orion Weller<br>Johns Hopkins University<br>Baltimore, MD, USA<br>oweller2@jhu.edu</p>
<p>Andrew Yates<br>HLTCOE at Johns Hopkins University<br>Baltimore, MD, USA<br>andrew.yates@jhu.edu</p>
<h2>DAwn Lawrie</h2>
<p>HLTCOE at Johns Hopkins University<br>Baltimore, MD, USA<br>lawrie@jhu.edu</p>
<h2>ABSTRACT</h2>
<p>The HLTCOE LiveRAG submission utilized the GPT-researcher framework for researching the context of the question, filtering the returned results, and generating the final answer. The retrieval system was a ColBERT bi-encoder architecture, which represents a passage with many dense tokens. Retrieval used a local, compressed index of the FineWeb10-BT collection created with PLAID-X, using a model fine-tuned for multilingual retrieval. Query generation from context was done with Qwen2.5-7B-Instruct, while filtering was accomplished with m2-bert-80M-8k-retrieval. Up to nine passages were used as context to generate an answer using Falcon3-10B. This system placed 5th in the LiveRAG automatic evaluation for correctness with a score of 1.07 .</p>
<h2>1 INTRODUCTION</h2>
<p>We adopted GPT-Researcher as the main framework for our LiveRAG submission to use an abstractive summarization paradigm to support answer generation. This approach was one of the best performing systems in TREC NeuCLIR's report generation tasks [4]. We used Qwen2.5-7B-Instruct ${ }^{1}$ [7] with 8 bit quantization for query formulation. We used a ColBERT-based search engine [5] for retrieval with a PLAID-X model trained multilingually [9]. The retrieval system returned top-ranked passages. Passages were then chunked into snippets. Passage snippets were filtered based on the cosine similarity between the original question and the snippet using m2-bert-80M-8k-retrieval ${ }^{2}$ [3]. The general architecture appears in Figure 1. Finally, Falcon3-10B-Instruct ${ }^{3}$ [8] was used to generate the final response. The remainder of the paper outlines the detailed architecture of our system as well the prompts we used in the different steps in our pipeline. We describe the steps we took to validate the approach and optimize the performance to be able to perform at the required speed for the competition.</p>
<h2>2 SYSTEM DESIGN</h2>
<p>Our RAG system divided the process into two main stages. In the first stage research was conducted to identify relevant information and in the second stage the response was written. Conducting research included generating queries based on an initial search and then gathering all the responsive data together from the retrieved</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>passages. Writing the response used the gathered context to generate the final output of the system. In Section 2.1 we describe the process for conducting research. Section 2.2 describes the process of generating the response given the research.</p>
<h3>2.1 Conduct Research</h3>
<p>The research stage involves four major steps: (1) issue an initial retrieval to set the context, (2) review the context to determine additional search queries, (3) issue additional search queries, and (4) divide passages into snippets and the snippets based on responsiveness to the original LiveRAG question.</p>
<p>To set the initial context for the research, the top three ranked passages are used based on the query entered by the user, in this case the content of the question field for LiveRAG. Retrieval was done with an in-house index of the FineWeb-10BT collection. A ColBERT architecture search engine [5] was used, which creates a dense vector per token for both the passage and query. We divided documents into non-overlapping passages of 450 tokens each for indexing. For efficiency, the index was compressed using the PLAID-X version ${ }^{4}$ of ColBERT. Since it was possible for FineWeb10BT to contain non-English documents, we chose to use an XLMR model ${ }^{5}$ [1] fine-tuned using translate distill [9] for multilingual retrieval, which is trained with English queries and documents in several languages including English and Spanish. The training dataset is MS MARCO [6]. MS MARCO documents were translated in-house with Sockeye Version 2 [2] to create the training data.</p>
<p>The prompt in Figure 2 was used to determine additional queries that were used for retrieval. We set fields referenced in the prompt as follows:</p>
<ul>
<li>max_iterations - 2</li>
<li>task - the value of the question field</li>
</ul>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Write {max_iterations} google search queries to search online that form an objective opinion from the following task: " ${$ task $}$ " Assume the current date is ${$ date $}$ if required. You are a seasoned research assistant tasked with generating search queries to find relevant information for the following task: " ${$ task $}$ ". Context: {context}
Use this context to inform and refine your search queries. The context provides real-time web information that can help you generate more specific and relevant queries. Consider any current events, recent developments, or specific details mentioned in the context that could enhance the search queries.
You must respond with a list of strings in the following format: ${{$ dynamic_example $} }$. The response should contain ONLY the list.</p>
<p>Figure 2: Query Generation Prompt</p>
<p>Information: "{context}" - Using the above information, answer the following query or task: "{question}" in one or two sentences. Use at most {total_words} words.</p>
<p>Figure 3: Response Generation Prompt</p>
<ul>
<li>date - the date the software is run</li>
<li>context - a concatenation of the top three ranked passages using the question as the search query</li>
<li>dynamic_examples - based on max_iterations and in this case is "query 1", "query 2"
This prompt was issued to the Qwen2.5-7B-Instruct-Turbo model. The top three passages were kept from each of the two queries generated as well as the original query. This created a list of nine passages. Our PLAID-X search engine was used to return passages for each the queries.</li>
</ul>
<p>The final step of conducting research utilized the m2-bert-80M-8k-retrieval model to embed both the original question and the passage in chunks of 1000 characters with 100 character overlap, referred to as snippets herein. The cosine similarity of the two vectors was computed. Any snippet whose similarity with the question was at least 0.35 was maintained for answer generation.</p>
<h3>2.2 Generate Response</h3>
<p>In order to generate the response to the LiveRAG question, the snippets identified during the research phase are used as source material. In particular, the system role is left blank. The user role is the prompt in Figure 3. This prompt has three attributes:</p>
<ul>
<li>context - a concatenation of the text in each of the snippets that was accepted after the final filtering stage</li>
<li>question - the original LiveRAG question.</li>
<li>total_words - 200</li>
</ul>
<p>A local version of Falcon3-10B-Instruct was used to generate the final response as specified in the rules for LiveRAG. The order of snippets in the prompt is not a rank order. Instead snippet ordering is determined by the rank order of the passage to the individual query that retrieved it as well as the lexical ordering of the chunk in the passage. More precisely, generated query $Q_{1}$ retrieves passages $A, B$ and $C$ in rank order. Generated query $Q_{2}$ retrieves passages $D$, $E$, and $F$ in rank order. The original question $Q_{o}$ retrieves passages $G, H$, and $I$. Passages are ordered $A$ to $I$. While passages are of length 450 tokens, they have a variable number of characters since not all tokens are of the same length. Each of the passages are divided into [length of (Passage) $/ 1000$ ] snippets. Some of the snippets are filtered because they are not sufficiently similar to the original query.</p>
<p>In addition to the prompt for answer generation and the generated answer, LiveRAG asked for a list of documents. The documents were ordered based on their retrieval for the individual queries. The three queries were concatenated together. While nine passages were retrieved, there may have been fewer documents if multiple passages from the same document were retrieved by one or more queries. Only the first time the document occurs is included in the final list.</p>
<h2>3 EXPERIMENTS BEFORE LIVE CHALLENGE DAY</h2>
<p>The LiveRAG system was adapted from the system developed for the NeuCLIR Pilot Report Generation task [10]. Prompts for that system were optimized for Claude and GPT-4. Prompts for LiveRAG were adapted to Qwen and Falcon for the Query Generation Prompt and the Response Generation Prompt respectively. Development data from DataMorgana was used to generate sample questions that could be used to spot check the system, both for the ability to retrieve documents that were responsive to the query as well as to check that answers reflected the information found in the documents.</p>
<p>The main changes made to the system during development were with an eye towards improving efficiency. The number of additional generated queries was reduced from four to two. For each query the number of passages was reduced from four to three.</p>
<p>During the dry run, additional human assessment was undertaken. In general the snippets contained information pertaining to the question and in a vast majority of cases, that information was carried forward to the answer. It was noted that on occasion a snippet contained more specific information that could have yielded a more precise answer than was generated.</p>
<p>The main modification that was made as the challenge approached was to ensure that the retrieval system could return responses at a sufficient rate. Since the generation component expects the content of the retrieved passages from the search engine, PLAID-X service needs to host and serve the body text. We identified that the primary latency came from the excessive duplication of the collection in memory due to the multi-thread search service. We modified the service to host the content separately to reduce the latency sufficiently for the live challenge. Each query, after the modification, takes around 400 ms to serve with an NVIDIA V100 GPU.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 4: Histogram of number of unique documents (left), number of snippets (middle), and corresponding final prompt length (right) per input context for response generation</p>
<p>To ensure that the system would respond to all 500 questions in two hours, the question file was divided in half and two processes were used. We used a single retrieval service that batched queries for efficiency and a single version of each of the LLMs for generation. We were able to complete all generation in just over one hour.</p>
<h2>4 AUTOMATED RESULTS</h2>
<p>The HLTCOE system placed fifth among the participating teams in terms of correctness with a score of 1.070111 . The faithfulness score of 0.340711 was 14th among the 25 teams participating.</p>
<p>Appendix A includes Table 1 that illustrates an example output from our system in detail. Note that 2 queries were generated based on an initial search, after which 7 documents were retrieved in total. The passages are then chunked into smaller-sized snippets and filtered according to embedding similarity with the queries. From the example, we observe that many snippets are potentially relevant, even if nothing directly answers the question, e.g. (1), (6), (7); there are also a few clearly irrelevant snippets, e.g. (16). Note that multiple snippets from the same passage may be included in the context. In this case, we have 16 snippets from the 7 documents. In our system, we observe the final response LLM generally takes a large set of snippets as input. Figure 4 shows the variation in numbers of documents and snippets as well as the final prompt length provided in each prompt over the 500 query challenge set.</p>
<p>We also performed some analysis after the live challenge. Table 2 in Appendix A compares two different LLMs for response generation. The Falcon-10B Instruct model was used for the challenge, while a larger Llama-70B-Instruct model is compared. Anecdotally, we do not observe a significant difference in the quality of the responses; this suggests that improving the document retrieval and context construction steps may play a more important role than the size and family of model to affect the final outcome of the generation.</p>
<h2>5 FUTURE WORK</h2>
<p>In the future we are interested in investigating how the order of content impacts the generation and how aggressive the filtering
should be. While the order in which snippets were represented at generation was not strictly based on relevance, it was not clear how best to structure the information in the generation prompt to ensure that the most relevant information would be included in the output. This is worth further investigation. In these experiments, filtering was assessed for each passage independently; however, a model that can reason over more information at once would be able to prioritize information and remove redundant information. This may of interesting implication for RAG tasks.</p>
<h2>6 SUMMARY</h2>
<p>In conclusion, the HLTCOE team developed an effective RAG system that incorporated state-of-the-art components. Our search engine was able to identify content that was useful for addressing the LiveRAG questions. Our generation process was effective at proposing additional queries and assembling the information into a meaningful answer.</p>
<h2>REFERENCES</h2>
<p>[1] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Mylo Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116 (2019).
[2] Tobias Domhan, Michael Denkowski, David Vilar, Xing Niu, Felix Hieber, and Kenneth Hvafield. 2020. The Sockeye 2 Neural Machine Translation Toolkit at AMTA 2020. In Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track). Association for Machine Translation in the Americas, Virtual, 110-115.
[3] Dan Fu, Simran Arora, Jessica Grogan, Isys Johnson, Evan Sabri Eyuboglu, Armin Thomas, Benjamin Spector, Michael Poli, Atri Rudra, and Christopher Ré. 2023. Monarch mixer: A simple sub-quadratic gemm-based architecture. Advances in Neural Information Processing Systems 36 (2023), 77546-77603.
[4] Dawn Lawrie, Sean MacAvaney, James Mayfield, Paul McNamee, Douglas W. Oard, Luca Soldanini, and Eugene Yang. 2025. Overview of the TREC 2024 NeuCLIR Track. In The Thirty-Third Text REtrieval Conference (TREC 2024) Proceedings.
[5] Suraj Nair, Eugene Yang, Dawn Lawrie, Kevin Duh, Paul McNamee, Kenton Murray, James Mayfield, and Douglas W. Oard. 2022. Transfer Learning Approaches for Building Cross-Language Dense Retrieval Models. In Proceedings of the 44th European Conference on Information Retrieval (ECIR). https: //arxiv.org/abs/2201.08471
[6] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv preprint arXiv:1611.09268 (2016).</p>
<p>arXiv:1611.09268 http://arxiv.org/abs/1611.09268
[7] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 Technical Report. arXiv preprint arXiv:2412.15115 (2025).
[8] TII Team. 2024. The Falcon 3 family of Open Models.
[9] Eugene Yang, Dawn Lawrie, James Mayfield, Douglas W. Oard, and Scott Miller. 2024. Translate-Distill: Learning Cross-Language Dense Retrieval by Translation
and Distillation. In Proceedings of the 46th European Conference on Information Retrieval (ECIR). https://arxiv.org/abs/2401.04810
[10] Eugene Yang, Dawn Lawrie, Orion Weller, and James Mayfield. 2025. HLTCOE at TREC 2024 NeuCLIR Track. In The Thirty-Third Text REtrieval Conference (TREC 2024) Proceedings.</p>
<h2>A EXAMPLES</h2>
<p>Table 1 contains example output. Table 2 compares Falcom to Llama generation.</p>
<p>Table 1: Example system output. Generated queries are the result from the LLM call shown in Figure 2. Filtered passages form input context for the LLM call shown in Figure 3.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Input Question</th>
<th style="text-align: center;">I live close to a park with many geese. Can I get bird flu from their droppings?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Generated <br> Queries</td>
<td style="text-align: center;">(1) can i get bird flu from geese droppings in 2025; (2) risks of bird flu transmission from geese to humans</td>
</tr>
<tr>
<td style="text-align: center;">Retrieved Docs</td>
<td style="text-align: center;">7 documents in total from 3 searches (input question plus 2 generated queries)</td>
</tr>
<tr>
<td style="text-align: center;">Filtered Snippets (docid:snippet)</td>
<td style="text-align: center;">(1) e7703b83-95fb-497a-859a-c50d8e37e329: "unaffected themselves, the host birds can spread the infection to susceptible species, especially domesticated chickens, turkeys and geese"; (2) ba608cc9-5d39-420c-98ac-b5a3bed83a60: "Droppings dry, then are pulverized in the air or dissolved in the water and from there get to other birds, wild or domestic."; (3) e7703b83-95fb-497a-859ac50d8e37e329: "The outbreak of any epidemic or the widespread of any disease leads to an inevitable slump in the market."; (4) ba608cc9-5d39-420c-98ac-b5a3bed83a60: "Sometimes, poultry also caught it. But people have never got sick or died of it."; (5) 93b69f5d-b805-4be0-bb73-06be6f33ece4: "Q) Can pets get bird flu? A) Yes."; (6) 795b59f7-6303-40dd-ba97-f3872770569c: "Although this virus can spread quickly amongst birds the risk to the public is very low and well cooked eggs and poultry remain safe to eat."; (7) 61120bc5-bc10-4c6d-825a-629a0fc13d22: "There are many types of bird flu, and most are harmless to humans, however occasionally, in rare cases where bird flu does infect humans."; (8) 93b69f5d-b805-4be0-bb73-06be6f33ece4: "Right now there is a type of bird flu called Highly Pathogenic Avian Influenza (HPAI) type H5N1 (also called H5N1 bird flu) that has made many birds and a few people sick elsewhere in the world."; (9) 795b59f7-6303-40dd-ba97-f3872770569c: "a bird keeper in Sulby found 11 of his geese dead near a pond on his property."; (10) 61120bc5-bc10-4c6d-825a-629a0fc13d22: "The incubation period for bird flu is 2-8 days which is longer than seasonal flu"; (11) bd4fd5de-4e73-4875-8f80-6e4c8fd62606: "STP Pest Control Birds are a nuisance creating problems with noise, bird mess, getting in roof spaces and in some cases aggressive behaviour." (12) 93b69f5d-b805-4be0-bb73-06be6f33ece4: "There is no evidence that properly cooked poultry or eggs can be a source of infection from bird flu." (13) 4f0151b0-6571-4fa2-8a2f-fd9756f0e3b0: "a hawk enthusiast may be willing to let his birds have at it, so long as the pigeons are not located in a high traffic area that could endanger the hawks." (14) bd4fd5de-4e73-4875-8f80-6e4c8fd62606: "Ornithosis Ornithosis is a 'flu-like disease most commonly found in pigeon droppings. Fatalities are rare, but occur as with any other influenza virus."; (15) 93b69f5d-b805-4be0-bb73-06be6f33ece4: "Teach children to always wash their hands after playing outside."; (16) 4f0151b0-6571-4fa2-8a2f-fd9756f0e3b0: "news of the victory over Napoleon at Waterloo was carried by military homing pigeons"</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">Based on the provided sources, there is currently no evidence that properly cooked poultry or eggs can be a source of infection from bird flu. Regarding contact with geese droppings, the sources do not explicitly mention the risk of contracting bird flu from goose droppings in parks or other public places. However, it's generally advised to avoid direct contact with bird droppings due to the potential presence of various bacteria and parasites. To minimize risk, it's recommended to wash hands after being in areas with bird droppings and avoid touching them with bare hands.</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of Answers by Falcon3-10B Instruct (F) vs Llama3.1-70B Instruct (L) when given the same input context</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Q</th>
<th style="text-align: left;">What's the difference between perspectivism and the despair of defiance regarding human autonomy?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">F</td>
<td style="text-align: left;">Perspectivism posits that our understanding of reality is always mediated by individual perspectives, whereas the despair of defiance, as described <br> by thinkers like Nietzsche, suggests a rejection of traditional moral and metaphysical constraints, leading to a state where one acknowledges the <br> lack of absolute truth or meaning, yet defiantly embraces life's chaos and multiplicity.</td>
</tr>
<tr>
<td style="text-align: left;">L</td>
<td style="text-align: left;">Perspectivism acknowledges the multiplicity of perspectives and the fragmented nature of human experience, whereas the despair of defiance <br> regarding human autonomy is a more extreme stance that rejects the idea of human autonomy altogether, often in favor of a nihilistic or <br> deterministic worldview. Perspectivism seeks to understand and navigate the complexities of human experience, whereas the despair of defiance <br> tends to abandon hope for human agency and autonomy.</td>
</tr>
<tr>
<td style="text-align: left;">Q</td>
<td style="text-align: left;">Which tattoo needle arrangements are most suitable for skin preservation?</td>
</tr>
<tr>
<td style="text-align: left;">F</td>
<td style="text-align: left;">Based on the provided information, curved magnum (RM) or curved magnum (CM/M1C) tattoo needle arrangements are most suitable for skin <br> preservation. These needles have a curvature that allows for easier movement on the skin, reducing the risk of the needle edges digging in, which <br> is particularly beneficial for delicate areas with curvature, such as the inner elbow.</td>
</tr>
<tr>
<td style="text-align: left;">L</td>
<td style="text-align: left;">Round curve magnums and curved magnums (RM or CM) are most suitable for skin preservation as they are designed to deflect the skin when <br> moving in and out, reducing the risk of edges digging into the skin and causing damage. These needle arrangements are particularly useful for <br> delicate areas with curvature, such as the inner elbow.</td>
</tr>
<tr>
<td style="text-align: left;">Q</td>
<td style="text-align: left;">What realms wil be the playble nations in the Europa Universalis board game?</td>
</tr>
<tr>
<td style="text-align: left;">F</td>
<td style="text-align: left;">In the Europa Universalis board game, the playable realms will initially focus on European nations, as the game's title and scope suggest a <br> concentration on European history and politics. However, the game may expand to include other regions through future expansions, allowing <br> players to explore and control a broader range of historical territories.</td>
</tr>
<tr>
<td style="text-align: left;">L</td>
<td style="text-align: left;">The playable nations in the Europa Universalis board game will initially be the European nations, with the main map covering Europe and the <br> Mediterranean region. Future expansions may include other parts of the world, such as North and South America, Africa, Asia, and Australia, but <br> the initial focus will be on European nations.</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://huggingface.co/Qwen/Qwen2.5-72B-Instruct
${ }^{2}$ https://huggingface.co/togethercomputer/m2-bert-80M-8k-retrieval
${ }^{3}$ https://huggingface.co/tiiuae/Falcon3-10B-Instruct&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ https://pypi.org/project/PLAID-X/
${ }^{5}$ https://huggingface.co/FacebookAI/xlm-roberta-large&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3164 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3164</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3164</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-74.html">extraction-schema-74</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-1eb1a8c7f88de27af224153f43ecdd41774600f2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1eb1a8c7f88de27af224153f43ecdd41774600f2" target="_blank">PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work presents PromptAgent, an optimization method that autonomously crafts prompts equivalent in quality to those handcrafted by experts, and applies it to 12 tasks spanning three practical domains, showing it significantly outperforms strong Chain-of-Thought and recent prompt optimization baselines.</p>
                <p><strong>Paper Abstract:</strong> Highly effective, task-specific prompts are often heavily engineered by experts to integrate detailed instructions and domain insights based on a deep understanding of both instincts of large language models (LLMs) and the intricacies of the target task. However, automating the generation of such expert-level prompts remains elusive. Existing prompt optimization methods tend to overlook the depth of domain knowledge and struggle to efficiently explore the vast space of expert-level prompts. Addressing this, we present PromptAgent, an optimization method that autonomously crafts prompts equivalent in quality to those handcrafted by experts. At its core, PromptAgent views prompt optimization as a strategic planning problem and employs a principled planning algorithm, rooted in Monte Carlo tree search, to strategically navigate the expert-level prompt space. Inspired by human-like trial-and-error exploration, PromptAgent induces precise expert-level insights and in-depth instructions by reflecting on model errors and generating constructive error feedback. Such a novel framework allows the agent to iteratively examine intermediate prompts (states), refine them based on error feedbacks (actions), simulate future rewards, and search for high-reward paths leading to expert prompts. We apply PromptAgent to 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), as well as domain-specific and general NLP tasks, showing it significantly outperforms strong Chain-of-Thought and recent prompt optimization baselines. Extensive analyses emphasize its capability to craft expert-level, detailed, and domain-insightful prompts with great efficiency and generalizability.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3164",
    "paper_id": "paper-1eb1a8c7f88de27af224153f43ecdd41774600f2",
    "extraction_schema_id": "extraction-schema-74",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00623075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PromptAgent: Strategic Planning with Language Models Enables Expert-LEVEL Prompt Optimization</h1>
<p>Xinyuan Wang ${ }^{1 <em>}$ Chenxi $\mathbf{L i}^{1 </em>}$ Zhen Wang ${ }^{12 * 1}$ Fan Bai ${ }^{5}$ Haotian Luo ${ }^{2}$ Jiayou Zhang ${ }^{2}$ Nebojsa Jojic ${ }^{3}$ Eric Xing ${ }^{44}$ Zhiting $\mathbf{H u}^{1}$<br>${ }^{1}$ UC San Diego ${ }^{4}$ Carnegie Mellon University<br>${ }^{3}$ Microsoft Research ${ }^{5}$ Georgia Institute of Technology<br>${ }^{2}$ Mohamed bin Zayed University of Artificial Intelligence<br>{xiw136, chl078, zhw085, zhh019}@ucsd.edu</p>
<h4>Abstract</h4>
<p>Highly effective, task-specific prompts are often heavily engineered by experts to integrate detailed instructions and domain insights based on a deep understanding of both instincts of large language models (LLMs) and the intricacies of the target task. However, automating the generation of such expert-level prompts remains elusive. Existing prompt optimization methods tend to overlook the depth of domain knowledge and struggle to efficiently explore the vast space of expert-level prompts. Addressing this, we present PromptAgent, an optimization method that autonomously crafts prompts equivalent in quality to those handcrafted by experts. At its core, PromptAgent views prompt optimization as a strategic planning problem and employs a principled planning algorithm, rooted in Monte Carlo tree search, to strategically navigate the expert-level prompt space. Inspired by humanlike trial-and-error exploration, PromptAgent induces precise expert-level insights and in-depth instructions by reflecting on model errors and generating constructive error feedback. Such a novel framework allows the agent to iteratively examine intermediate prompts (states), refine them based on error feedbacks (actions), simulate future rewards, and search for high-reward paths leading to expert prompts. We apply PromptAgent to 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), as well as domain-specific and general NLP tasks, showing it significantly outperforms strong Chain-of-Thought and recent prompt optimization baselines. Extensive analyses emphasize its capability to craft expert-level, detailed, and domain-insightful prompts with great efficiency and generalizability ${ }^{1}$.</p>
<h2>1 INTRODUCTION</h2>
<p>Prompt engineering aims to craft effective prompts for harnessing the full potential of large language models (LLMs). Recent automatic prompt engineering, i.e., prompt optimization, has successfully studied training soft prompts (Lester et al., 2021; Hu et al., 2021; Wang et al., 2022), or searching for optimal combinations of discrete tokens (Shin et al., 2020; Deng et al., 2022; Zhang et al., 2022), by utilizing internal states or gradients of LLMs. For cutting-edge, proprietary API-based LLMs like GPT-4 (OpenAI, 2023b), prompt engineering largely relies on somewhat ad-hoc humanmachine interactions. Human prompting experts thus need a unique blend of domain knowledge and intuition for LLMs to design the most effective prompts. For instance, an ideal prompt from human experts, shown in Figure 1, might integrate nuanced elements like task descriptions, domain knowledge, solution guidance, etc., all of which substantially boost prompt quality and performance.</p>
<p>Automating expert-level prompting engineering on API-based LLMs presents significant challenges, largely due to the intricate nature of expert-level prompts, as illustrated in Figure 1. Although recent prompt optimization approaches have begun to utilize techniques like iterative sampling or</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Expert-level prompt vs. ordinary human-written prompt and prompt from sampling-based methods (i.e., Automatic Prompt Engineer, <em>Zhou et al. (2022)</em>). The task is in the biomedical domain for extracting disease entities (<em>NCBI, Doğan et al. (2014)</em>). The expert prompt provides much richer domain-specific details and structured guidance than the other two, leading to the correct prediction.
evolutionary algorithms, such as Monte Carlo search <em>(Zhou et al., 2022)</em> or Gibbs sampling <em>(Xu et al., 2023)</em>, they mostly employ heuristic methods like text edits or paraphrasing for generating candidate prompts <em>(Zhou et al., 2022; Prasad et al., 2023)</em>. These approaches also often rely on straightforward iteration algorithms and lack a principled strategy to guide the exploration. Consequently, they tend to settle on local variants of prompts from ordinary users and rarely ascend to the excellence and nuances of expert-level prompts. Critically, many of these methods overlook that prompting engineering is essentially a human-in-the-loop application. In this process, humans refine prompts by fixing intermediate errors and integrating necessary domain knowledge through iterative interactions. This iterative refinement process characterizes the merits of how human experts craft superior prompts. Yet, the challenge remains that human exploration, while effective, can be expensive and less efficient at handling multiple errors simultaneously to explore the prompt space, thereby impeding the scalability of expert-level prompting.
To address the above challenges and combine human-like exploration with machine efficiency, we introduce PromptAgent in this paper. Drawing inspiration from human trial-and-error processes, PromptAgent seamlessly incorporates the principled planning approach, specifically Monte Carlo Tree Search (MCTS), to strategically optimize the prompting process. Notably, PromptAgent reformulates prompt optimization as a strategic planning problem to address the complexity of expert-level prompt space. Under this planning framework, it plays trial-and-error iteration to retrieve model errors and leverages the self-reflection ability of LLMs <em>(Jang, 2023; Shinn et al., 2023; Pan et al., 2023)</em> to generate insightful error feedback. This feedback, in turn, plays a critical role in effectively inducing domain knowledge and guiding towards in-depth prompts. Through strategic planning, PromptAgent iteratively leverages insightful error feedback (action) to refine each version of prompts (state). Starting from an initial prompt (state), PromptAgent systematically grows the prompt space in a tree structure and prioritizes high-reward traces to navigate the vast space of expert-level prompts. The principled MCTS planning allows PromptAgent to look ahead and simulate future rewards, which are then backpropagated to update the beliefs about the current prompt so that PromptAgent can explore more promising alternatives later.
We demonstrate that PromptAgent can discover productive expert-level prompts by applying it to 12 tasks spanning three practical and distinct domains: BIG-Bench Hard (BBH) <em>(Suzgun et al., 2022)</em>, as well as domain-specific and general NLP tasks. Starting with an initial human-written prompt and a small set of training samples, PromptAgent not only enhances the performance of the initial human prompt greatly but also significantly surpasses strong Chain-of-Thought (CoT) and recent prompt</p>
<p>optimization baselines. For instance, Figure 2 shows PromptAgent consistently outperforms human and Automatic Prompt Engineer (APE) (Zhou et al., 2022) baselines across GPT-3.5, GPT-4, and PaLM 2, yielding improvements by $9.1 \%, 7.7 \%$ and $6 \%$ over APE, respectively. Extensive qualitative results further highlight the expert-level aspects of optimized prompts, indicating that PromptAgent effectively bridges the domain gap in challenging tasks, offering great exploration efficiency and generalizability. As we anticipate the emergence of even more powerful LLMs that can understand intricate instructions, we believe that expert-level prompting will spearhead the next era of prompt engineering, where PromptAgent stands as a pioneering step in this research direction.</p>
<h1>2 Related Works</h1>
<p>Prompt optimization. Automatically discovering optimal prompts has emerged as a central challenge in the era of LLMs. For open-sourced LLMs, one can leverage their internal states or gradients to either train additional parameters, such as soft prompts (Li \&amp; Liang, 2021; Lester et al., 2021; Hu et al., 2021; Wang et al., 2022), or search for discrete prompts via gradient-based search (Shin et al., 2020; Wen et al., 2023) or reinforcement learning (Deng et al., 2022; Zhang et al., 2022). However, such methods are less feasible for closed-sourced LLMs, which urges people to study gradient-free prompt optimization, typically assuming only APIs and a limited training set are available. Most gradient-free methods follow an iterative process of prompt sampling, i.e., starting from an initial prompt, they iteratively sample prompt candidates and score them to select the best one for the next iteration. Numerous methods emphasize diversifying the prompt candidates-examples include edit-based methods like deleting or swapping phrases (Prasad et al., 2023), back translation (Xu et al., 2022), evolutionary operations (Guo et al., 2023; Fernando et al., 2023), or more relevantly, LLM rewriting based on natural language feedback (Zhou et al., 2022; Pryzant et al., 2023; Yang et al., 2023). There are also explorations into alternate sampling procedures like Monte Carlo search (Zhou et al., 2022), Gibbs sampling (Xu et al., 2023) or Beam search (Pryzant et al., 2023). Nevertheless, PromptAgent fundamentally differs from all the above methods in two ways. First, while primary search algorithms have been investigated (Zhou et al., 2022; Xu et al., 2023; Pryzant et al., 2023), we are the first to introduce strategic planning into prompting optimization research. This innovation provides a structured way to efficiently navigate the intricate space of prompts, with principled capabilities like lookahead and backtrack. Second, most previous methods generate prompt candidates as local variants, such as paraphrasing or LLM sampling, fail to incorporate fine-grained domain insights. Instead, we formulate prompt generation as the state transition and strategically convert error feedback into new states, leading to expert-level prompts.</p>
<p>Augmenting LLMs with self-reflection and planning. Despite their remarkable capabilities, modern LLMs exhibit certain limitations, such as long-term coherence (Malkin et al., 2022), lacking an internal world model (Hao et al., 2023a), the inability to act in the real world, etc. Thus, augmenting LLMs with external modules like reasoning and tools has drawn extensive attention recently (Mialon et al., 2023; Ozturkler et al., 2022; Hao et al., 2023b; Jojic et al., 2023), of which two common strategies are relevant here: self-reflection and planning with LLMs. Self-reflection encourages the LLM to introspect, critique its outputs, and subsequently suggest more refined solutions (Jang, 2023; Pan et al., 2023). This has been leveraged to enhance a variety of applications, from complex computer tasks (Shinn et al., 2023), text generation (Welleck et al., 2022) to reasoning (Paul et al., 2023).</p>
<p>Moreover, planning with LLMs sheds light on evaluating and enhancing these models. At its core, planning is an essential ability for intelligent agents to generate a sequence of actions in achieving specific goals (McCarthy et al., 1963; Bylander, 1994). One line of research is to prompt and evaluate LLMs on planning tasks directly (Liu et al., 2023). For instance, translation-based approaches translate natural language instructions into executable programs (e.g., Planning domain description language) to run classical planning algorithms. Another closer line of research is to augment the strategic reasoning ability of LLMs with planning-based algorithms. For example, Tree of Thoughts (ToT) applies DFS/BFS to augment CoT prompting, while both CoRe (Zhu et al., 2022) and RAP (Hao et al., 2023a) utilize MCTS to navigate richer reasoning paths. Yet, in contrast to existing endeavors in LLM augmentation, PromptAgent is the first novel framework for synergistically marrying the spirits of self-reflection and planning specifically tailored for prompt optimization.</p>
<h2>3 Methodology</h2>
<p>Given a base LLM $\mathcal{B}$ and a target task $\mathcal{T}$, the job at hand for a prompt engineer is to craft an optimized natural language prompt $\mathcal{P}^{\mathcal{T}}$ that maximizes the performance of $\mathcal{B}$ on $\mathcal{T}$. However, the</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: (a) MCTS (Monte Carlo Tree Search) planning for expert-level prompting. The tree structure enables strategic planning for PromptAgent. (b) A simplified state transition example. Given a current state (prompt), the base model (gpt-3.5-turbo) collects errors from the task dataset. The optimizer model (gpt-4) provides error feedback accordingly. The optimized model then updates the prompt according to the feedback and transits to the next state.
gap between novice and expert prompt engineers can be significant, particularly for tasks demanding specialized domain expertise, such as in the biomedical domain. Our primary objective is to autonomously refine the task prompt $\mathcal{P}^{\mathcal{T}}$ to bridge this knowledge gap, minimizing human intervention. Most existing approaches rely on sampling local prompt alternatives iteratively, which is not only resource-intensive but also lacks assurance of yielding an optimal final prompt. In light of this, we introduce PromptAgent, an agent-based framework to produce expert-level task prompts via strategic planning and reflecting with error feedback during the prompting process, striking a proper balance of exploration and performance.</p>
<p>Problem formulation. Following a standard setting in prompt optimization (Zhou et al., 2022), we start with an initial natural language task prompt $\mathcal{P}<em i="i">{0}$ (e.g., "Let's solve this problem step-by-step") and a small set of training samples from target task $\mathcal{T}$ as $(Q, A)=\left{q</em>\right}}, a_{i<em i="i">{i=1}^{N}$, where $q</em>^{} / a_{i}$ are input/output pairs for each sample (e.g., a question and its answer). Given the model input consisting of $\mathcal{P}$ and $q_{i}$, the base LLM $\mathcal{B}$ makes the prediction (typically through a left-to-right generation process) based on $p_{\mathcal{B}}\left(a_{i} \mid q_{i}, \mathcal{P}\right)^{2}$. The goal of prompt optimization is to find the optimal natural language prompt $\mathcal{P<em>}$ that maximizes the performance towards a measure function $\mathcal{R}$ (e.g., accuracy). This can be formally defined as an optimization problem: $\mathcal{P}^{</em>}=\arg \max <em i="i">{\mathcal{P} \in \mathcal{S}} \sum</em>$ denotes the sample space for a natural language prompt, an infinite and intractable space, if not impossible, to comprehensively enumerate. Conventionally, human experts draw upon a blend of heuristics and domain-specific insights to craft such prompts. Although previous optimization methods have attempted to leverage iterative sampling methods for prompt discovery (Zhou et al., 2022), we advance this line of research by proposing a unified framework that seamlessly integrates strategic planning for superior, expert-level prompt optimization. Next, we introduce the formulation of PromptAgent and then present the planning-based prompt optimization.} \mathcal{R}\left(p_{\mathcal{B}}\left(a_{i} \mid q_{i}, \mathcal{P}\right)\right)$, where $\mathcal{S</p>
<h1>3.1 PromptAgent Framework Design</h1>
<p>The goal of PromptAgent is to effectively integrate expert prior knowledge into the task prompt while ensuring an efficient and strategic exploration of the expansive prompt space. In this planning framework, we define the state as each iteration or version of the task prompt, $s_{t}=\mathcal{P}_{t}$. This allows systematic monitoring of the evolution of prompts and directly applying refinements to modify them. Actions, in this context, can be thought of as potential modifications to the current prompt (state), such as word replacements or paraphrasing, as explored in prior works (Jiang et al., 2020; Prasad et al., 2023). However, a more desirable action space should introduce more effective and meaningful revisions that invoke prior expert knowledge, ultimately steering toward expert-level prompts.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>We thus propose error-based actions where each action is generated based on certain errors made by the base model. Specifically, as illustrated in Figure 3 (b), actions are framed as error feedbacks to guide subsequent refinements of the prompt. Such error feedbacks effectively suggest potential directions for correcting model errors, ensuring the revised prompt better instructs the base model to avoid previously observed pitfalls. Note that this approach also resonates with recent findings on the self-reflection capabilities of LLMs (Pryzant et al., 2023; Shinn et al., 2023; Paul et al., 2023), such that an LLM can directly reflect on their errors to yield better prompt modifications.</p>
<p>Given the definition of state and action, PromptAgent formulates the prompt optimization problem as a Markov Decision Process (MDP) by the tuple $(\mathcal{S}, \mathcal{A}, T, r)$. Here, $\mathcal{S}$ denotes the state space, $\mathcal{A}$ is the action space, $T$ defines the transition function $T: \mathcal{S} \times \mathcal{A} \mapsto \mathcal{S}$, and $r$ is the reward function $r: \mathcal{S} \times \mathcal{A} \mapsto \mathbb{R}$. As illustrated in Figure 3 (a), for any given current state $s_{t}$, PromptAgent iteratively generates an action $a_{t}$ based on $a_{t} \sim p_{\mathcal{O}}\left(a \mid s_{t}, m_{1}\right)$, where $m_{1}$ is a meta-prompt employed by an optimizer LLM $\mathcal{O}$ to facilitate the action generation. Specifically, Figure 3 (b) shows the two-step process of action generation: collecting errors of the base model from training samples (Step 1) and reflecting on such errors to draw useful error feedbacks (Step 2). Afterward, PromptAgent obtains a new state based on the transition function $p_{\mathcal{O}}\left(s_{t+1} \mid s_{t}, a_{t}, m_{2}\right)$, where $m_{2}$ is another metaprompt helping the state transition to update the prompt, also operating on $\mathcal{O}$. More specifically, given current error feedback as action $a_{t}, m_{2}$ asks the optimizer to generate a new prompt (state) to leverage any domain knowledge and effectively address model errors, similar to how prompting experts revise their prompts based on error feedbacks.</p>
<p>Finally, the quality of each newly generated state $s_{t}$ after applying action $a_{t}$ is determined by the reward function $r_{t}=r\left(s_{t}, a_{t}\right)$. Drawing parallels with the intricate nature of reward engineering in Reinforcement Learning (RL), crafting rewards could be complex to accommodate domain-specific knowledge or preferences specified for the task of interest. Without losing the generality of our framework across a variety of tasks, we straightforwardly define the reward as the task performance on a held-out set separated from the given training samples. The exact definition of reward, however, will depend on task-specific metrics as described in the implementation details later.</p>
<h1>3.2 Strategic Planning for Prompt Optimization</h1>
<p>The aforementioned reformulation of the prompt optimization enables us to seamlessly integrate PromptAgent with principle planning algorithms, notably the Monte Carlo Tree Search (MCTS). This enables strategically navigating the vast prompt space while balancing the exploration and exploitation in finding high-reward paths of error feedbacks, which leads to the most generalizable expert-level prompts. Specifically, we observe some error feedbacks (actions) may inject instancespecific details into task prompts (states) that are hard to generalize task-wise (exploitation), where we need strategic planning to explore novel error feedbacks for higher rewards (exploration). MCTS operationalizes such strategic planning, as shown in Figure 3 (a), by progressively constructing a tree structure with each node as a state and each edge as the action for transiting states. MCTS expands the tree strategically by maintaining a state-action value function, $Q: \mathcal{S} \times \mathcal{A} \mapsto \mathbb{R}$, which represents the potential future rewards for applying an action $a_{t}$ to a state $s_{t}$. In other words, we rely on this function, $Q\left(s_{t}, a_{t}\right)$, to look ahead and estimate the potential rewards for paths following the current state-action pair. To update this $Q$ function and expand the tree, MCTS iteratively performs four operations: selection, expansion, simulation, and back-propagation. The iteration process ends when a pre-defined number of iterations is reached, and we then select the highest-reward trace for the final prompt. We next explain the four operations in PromptAgent, and the pseudocode of our MCTS-based prompt optimization can be found in Algorithm 1 of the Appendix.</p>
<p>Selection is the first step that selects the most promising nodes at each level to be further expanded and explored. At each iteration, it starts from the root node $s_{0}$, traverses through each tree level, selects a subsequent child node at every level, and stops at a leaf node. When selecting the child node at each level, we leverage the Upper Confidence bounds applied to Trees (UCT) algorithm, which is well-known for balancing the exploitation (choosing high-value nodes) and exploration (choosing less-visited nodes) as follows:</p>
<p>$$
a_{t}^{*}=\underset{a_{t}^{\prime} \in \mathcal{A}\left(s_{t}\right)}{\arg \max }\left(Q\left(s_{t}, a_{t}^{\prime}\right)+c \cdot \sqrt{\frac{\ln \mathcal{N}\left(s_{t}\right)}{\mathcal{N}\left(\operatorname{ch}\left(s_{t}, a_{t}^{\prime}\right)\right)}}\right)
$$</p>
<p>where $A\left(s_{t}\right)$ is the action set for node $s_{t}, \mathcal{N}\left(s_{t}\right)$ is the number of visiting times for node $s_{t}, \operatorname{ch}(s, a)$ represents the child node for $s_{t}$ after applying action $a_{t}^{\prime}$ and $c$ is a constant to adjust the exploration.</p>
<p>As we can see, the first term signifies exploitation by the $Q$ value, and the second term indicates exploration, measuring the uncertainty for less visited nodes. In other words, if a node was less explored and its child node was less visited before, the second term will be higher.</p>
<p>Expansion grows the tree by adding new child nodes to the leaf node reached by the previous selection step. This is done by applying the action generation and state transition (Figure 3 (b)) multiple times, resulting in multiple new actions and states. Note that we may sample multiple training batches to derive diverse error feedbacks (actions). Within new nodes, we then send the highest-reward one to the next simulation step.</p>
<p>Simulation is the lookahead step to simulate the future trajectories for the selected node from the previous expansion step. This step usually comes with a playout policy to reach the terminal state quickly and calculate the future rewards. The choice of playout could be flexible, such as choosing random moves until the terminal. To reduce the computation cost of simulation and simplify the process, we perform the previous expansion step iteratively until the terminal, i.e., we keep generating multiple actions and selecting the highest-reward node among them to proceed to the next tree level.</p>
<p>Back-propagation happens when a terminal state is met during the simulation. The terminal state is usually defined when a pre-defined maximum depth is reached, or an early-stopping criterion is encountered. We then back-propagate the future rewards along the path from the root to the terminal node by updating the $Q$ value function. Specifically, for each state-action pair in the path, $Q\left(s_{t}, a_{t}\right)$ is updated by aggregating the rewards from all future trajectories starting from $s_{t}$ as follows:</p>
<p>$$
Q^{*}\left(s_{t}, a_{t}\right)=\frac{1}{M} \sum_{j=1}^{M}\left(\sum_{s^{\prime} \in S_{s_{t}}^{j}, a^{\prime} \in A_{a_{t}}^{j}} r\left(s^{\prime}, a^{\prime}\right)\right)
$$</p>
<p>where $M$ is the number of future trajectories starting from $s_{t}, S_{s_{t}}^{j}$ and $A_{a_{t}}^{j}$ represent the $j$-th state and action sequences starting from $s_{t}$ and $a_{t}$, respectively.</p>
<p>PromptAgent executes the above four operations with a pre-defined number of iterations to stabilize the $Q$ values and fully grow the tree for exploring the vast prompt space. We finally need to select the best trace and node (i.e., prompt) for the final evaluation. Multiple alternative solutions can be leveraged for this output strategy, e.g., one could opt for the best node in the best path with the highest reward, or directly choose the leaf node with the largest number of visiting times. For simplicity and empirical purposes, we use the first strategy to select the output prompt, which works the best in our experiments.</p>
<h1>4 EXPERIMENTS</h1>
<h3>4.1 EXPERIMENTAL SETUP</h3>
<p>Tasks and Datasets. To comprehensively evaluate the effects of expert-level prompt optimization for a wide range of applications, we curate 12 tasks from three distinct domains for thorough experiments: BIG-Bench Hard (BBH), as well as domain-specific and general NLP tasks. BBH (Suzgun et al., 2022) is a subset of challenging BIG-Bench tasks (Srivastava et al., 2023) that are beyond the capabilities of current LLMs. We select 6 BBH tasks that emphasize a blend of domain knowledge (i.e., Geometric Shapes and Causal Judgment) and complex reasoning abilities (i.e., Penguins in a table, Object Counting, Epistemic Reasoning, and Temporal Sequences). We also select three domainspecific tasks in the biomedical domain, where domain insights are explicitly desired when crafting expert-level prompts. Such tasks include a disease named-entity recognition (NER) task (NCBI, Doğan et al. (2014)), a biomedical sentence similarly task (Biosses, Soğancıoğlu et al. (2017)), and a medical question answering task (Med QA, Jin et al. (2021)). Moreover, to show PromptAgent can also be generally applicable and beneficial for traditional NLP tasks, we further select three well-known NLU tasks, i.e., two text classification tasks (TREC, Voorhees \&amp; Tice (2000) and Subj, Pang \&amp; Lee (2004)), and a natural language inference task (CB, De Marneffe et al. (2019)).</p>
<p>Baselines. We compare our methods with three types of baselines: ordinary human prompts, Chain-of-Thought (CoT) prompts, and recent prompt optimization methods. (1) Human prompts are human-designed instructions representing the generic level of prompt engineering, which usually come from the original datasets. We also have a few-shot (FS) version of human prompts</p>
<p>Table 1: Prompting performance on BBH tasks. ZS: Zero-Shot, FS: Few-Shot. We select six challenging tasks from BBH (Suzgun et al., 2022), requiring domain knowledge (e.g., Geometry) or reasoning (e.g., Causal Judgement). Our method outperforms in 5/6 tasks, with only CoT surpassing in Object Counting. On average, our accuracy exceeds others by at least $9 \%$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Penguins</th>
<th style="text-align: center;">Geometry</th>
<th style="text-align: center;">Epistemic</th>
<th style="text-align: center;">Object Count.</th>
<th style="text-align: center;">Temporal</th>
<th style="text-align: center;">Causal Judge.</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Human (ZS)</td>
<td style="text-align: center;">0.595</td>
<td style="text-align: center;">0.227</td>
<td style="text-align: center;">0.452</td>
<td style="text-align: center;">0.612</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;">0.470</td>
<td style="text-align: center;">0.513</td>
</tr>
<tr>
<td style="text-align: left;">Human (FS)</td>
<td style="text-align: center;">0.595</td>
<td style="text-align: center;">0.315</td>
<td style="text-align: center;">0.556</td>
<td style="text-align: center;">0.534</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">0.505</td>
</tr>
<tr>
<td style="text-align: left;">CoT (ZS)</td>
<td style="text-align: center;">0.747</td>
<td style="text-align: center;">0.320</td>
<td style="text-align: center;">0.532</td>
<td style="text-align: center;">0.542</td>
<td style="text-align: center;">0.734</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.581</td>
</tr>
<tr>
<td style="text-align: left;">CoT</td>
<td style="text-align: center;">0.747</td>
<td style="text-align: center;">0.540</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;">$\mathbf{0 . 9 6 0}$</td>
<td style="text-align: center;">0.626</td>
<td style="text-align: center;">0.650</td>
<td style="text-align: center;">0.707</td>
</tr>
<tr>
<td style="text-align: left;">GPT Agent</td>
<td style="text-align: center;">0.696</td>
<td style="text-align: center;">0.445</td>
<td style="text-align: center;">0.406</td>
<td style="text-align: center;">0.502</td>
<td style="text-align: center;">0.794</td>
<td style="text-align: center;">0.520</td>
<td style="text-align: center;">0.561</td>
</tr>
<tr>
<td style="text-align: left;">APE</td>
<td style="text-align: center;">0.797</td>
<td style="text-align: center;">0.490</td>
<td style="text-align: center;">0.708</td>
<td style="text-align: center;">0.716</td>
<td style="text-align: center;">0.856</td>
<td style="text-align: center;">0.570</td>
<td style="text-align: center;">0.690</td>
</tr>
<tr>
<td style="text-align: left;">PromptAgent</td>
<td style="text-align: center;">$\mathbf{0 . 8 7 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 7 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 0 6}$</td>
<td style="text-align: center;">0.860</td>
<td style="text-align: center;">$\mathbf{0 . 9 3 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 7 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 0 2}$</td>
</tr>
</tbody>
</table>
<p>with teaching examples from Suzgun et al. (2022) for BBH tasks and randomly sampled ones from the training set for others. (2) CoT prompts are considered very effective tricks to boost LLM performance by inducing intermediate reasoning steps, especially for BBH tasks (Suzgun et al., 2022). We directly use the CoT prompts from Suzgun et al. (2022) for BBH tasks and construct CoT prompts by ourselves for other tasks. We also have a zero-shot (ZS) version of CoT, using "Let's think step by step" as the prompt to trigger CoT behavior without few-shot examples (Kojima et al., 2022). (3) Prompt optimization methods include GPT Agent and Automatic Prompt Engineer (APE) (Zhou et al., 2022). GPT Agent represents the recent surge of interest in LLM-powered autonomous agents (Weng, 2023), such as Auto-GPT ${ }^{3}$. Such agents are expected to autonomously perform planning and self-reflection to solve human requests, including optimizing task prompts. We leverage one of the powerful ChatGPT Plugins (OpenAI, 2023a) with GPT-4, AI Agents ${ }^{4}$ for prompt optimization. Specifically, similar to PromptAgent, we sample similar model errors and ask AI Agents plugin to rewrite the prompt based on the errors with a similar iteration number as PromptAgent. Lastly, APE is one of the most recent prompt optimization methods that proposes a Monte Carlo search-based method to iteratively propose and select prompts.</p>
<p>Implementation details. For the datasets with default testing or validation set, we use their original split to obtain our testing set. If there is no official training/testing split, such as BBH tasks, we sample a reasonably large set for stable testing. As stated in Section 3.1, we also split a portion of training samples for calculating the reward. The details of the datasets can be found in Appendix A.1. Unless further specified, we select GPT-3.5 as the default base LLM to be optimized, which is one of the decently powerful modern LLMs. For the optimizer LLM, we need one with a good selfreflection ability and, thus, use GPT-4 as the default optimizer LLM. We set the temperature as 0.0 for base LLM to make predictions and 1.0 in other contexts. When implementing PromptAgent, we set the number of iterations for MCTS as 12, and the exploration weight $c$ in Equation 1 as 2.5. During the expansion step, we generate actions based on model errors by sampling batches from training samples. We sample expand_width batches and generate num_samples new prompts per batch. The maximum depth of each path is depth_limit. To simplify the process of tuning these hyperparameters, we explore three settings: Standard, Wide, and Lite, where Standard and Lite have larger depth, while Wide generates more nodes per expansion step (Specific parameters can be found in Appendix Table 7). The best setting for PromptAgent is selected based on the rewards. Further details are available in Appendix A, including input formatting, data splitting, and the implementation specifics of both the PromptAgent and baseline methods.</p>
<h1>4.2 RESULTS AND ANALYSES</h1>
<p>Comparison with various prompting baselines. Table $1 \&amp; 2$ present a comprehensive comparison of expert-level prompts generated by PromptAgent against human prompts, CoT prompts, and existing prompt optimization methods across 12 tasks spanning three domains. Observing BBH tasks from Table 1, PromptAgent significantly outperforms all baselines overall and achieves $28.9 \%$, $9.5 \%$, and $11.2 \%$ relative improvement over baselines, i.e., human prompts (ZS), CoT, and APE, respectively. It is noteworthy that CoT prompts are especially effective in BBH tasks than human prompts, similar to findings from Suzgun et al. (2022). This is because BBH tasks usually require strictly formatted solutions that can be readily induced by the step-by-step CoT reasoning, which</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Prompt performance on specialized and general NLU tasks. Specialized tasks are three biomedical tasks explicitly asking for domain knowledge for prompting. General NLU tasks are used to demonstrate the generality of our method. Ours significantly outperformed in all tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Domain-specific Tasks</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">General NLU Tasks</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">NCBI (F1)</td>
<td style="text-align: center;">Biosses</td>
<td style="text-align: center;">Med QA</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">Subj</td>
<td style="text-align: center;">TREC</td>
<td style="text-align: center;">CB</td>
<td style="text-align: center;">Avg.</td>
</tr>
<tr>
<td style="text-align: left;">Human (ZS)</td>
<td style="text-align: center;">0.521</td>
<td style="text-align: center;">0.550</td>
<td style="text-align: center;">0.508</td>
<td style="text-align: center;">0.526</td>
<td style="text-align: center;">0.517</td>
<td style="text-align: center;">0.742</td>
<td style="text-align: center;">0.714</td>
<td style="text-align: center;">0.658</td>
</tr>
<tr>
<td style="text-align: left;">Human (FS)</td>
<td style="text-align: center;">0.447</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.492</td>
<td style="text-align: center;">0.521</td>
<td style="text-align: center;">0.740</td>
<td style="text-align: center;">0.742</td>
<td style="text-align: center;">0.429</td>
<td style="text-align: center;">0.637</td>
</tr>
<tr>
<td style="text-align: left;">CoT (ZS)</td>
<td style="text-align: center;">0.384</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.508</td>
<td style="text-align: center;">0.439</td>
<td style="text-align: center;">0.656</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.750</td>
<td style="text-align: center;">0.679</td>
</tr>
<tr>
<td style="text-align: left;">CoT</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.675</td>
<td style="text-align: center;">0.542</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.670</td>
<td style="text-align: center;">0.784</td>
<td style="text-align: center;">0.643</td>
<td style="text-align: center;">0.699</td>
</tr>
<tr>
<td style="text-align: left;">GPT Agent</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.468</td>
<td style="text-align: center;">0.406</td>
<td style="text-align: center;">0.554</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.339</td>
<td style="text-align: center;">0.543</td>
</tr>
<tr>
<td style="text-align: left;">APE</td>
<td style="text-align: center;">0.576</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">0.470</td>
<td style="text-align: center;">0.582</td>
<td style="text-align: center;">0.696</td>
<td style="text-align: center;">0.834</td>
<td style="text-align: center;">0.804</td>
<td style="text-align: center;">0.778</td>
</tr>
<tr>
<td style="text-align: left;">PromptAgent</td>
<td style="text-align: center;">$\mathbf{0 . 6 4 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 5 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 7 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 5 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 0 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 8 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 1 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 6 8}$</td>
</tr>
</tbody>
</table>
<p>also explains why CoT achieves very good performance on Object Counting that can benefit from step-by-step solutions the most. However, PromptAgent still outperforms CoT by a great margin in all tasks (except Object Counting), indicating that our optimized expert-level prompt can lead to bigger improvement over few-shot CoT reasoning (even under the zero-shot prompt setting). Regarding optimization methods, while we appreciate the planning and self-reflection of the GPT Agent, its planning is only used for a single turn of prompt rewriting, but not on a global scale of strategically exploring prompt space. APE, on the other hand, shows a greater scale of searching ability, but its exploration is based on Monte Carlo search, which suffers from inefficient planning and a lack of error-based reflections. Both deficits of GPT Agent and APE suggest the necessity of strategic planning in PromptAgent to fully explore the prompt space and deliver expert-level prompts.</p>
<p>Table 2 presents results on domain-specific and general NLP tasks. The former encompasses a broad spectrum of biomedical tasks, such as information extraction, sentence similarity, and question answering. Crafting prompts for these tasks requires extensive domain knowledge and heavy LLM prompt engineering instincts, where we can observe that straightforward human prompts and CoT prompts do not work very well. Prompt optimization methods like APE with automatic prompt sampling and refining are promising to incorporate domain knowledge without too much human intervention. Notably, PromptAgent surpasses APE significantly by $+7.3 \%$ improvement on average, suggesting PromptAgent can better induce effective domain knowledge to produce expert-level prompts and close the knowledge gap between novice and expert prompt engineers. For general NLP tasks, the efficacy and generality of PromptAgent are further emphasized, outperforming both CoT and APE by margins of $+16.9 \%$ and $+9 \%$, respectively. This implies the nontrivial expert gap, even for general NLP tasks, underscoring the imperative for expert prompts in diverse applications.</p>
<p>Prompt generalization. We next conduct experiments to investigate whether our optimized prompts can be generalized to other base LLMs. This emphasizes the robustness and transferability of expertlevel prompts, which are urgently favorable and underpinning two key facts: (a) the domain insights and nuanced guidance in expert prompts can be seamlessly transferred across powerful LLMs, reinforcing the universal applicability of expert prompts, and (b) we only need to optimize each task once, leading to better computational efficiency. It is crucial to note that the primary goal of PromptAgent is to optimize prompts for state-of-the-art LLMs to achieve expert-level prompting, while less advanced and smaller LLMs, like GPT-2 or LLaMA, may not adeptly grasp the subtleties of these expert-level prompts, potentially causing significant performance drop. Nonetheless, for a holistic assessment, we evaluate two additional base LLMs, one more potent (GPT-4) and one less robust (PaLM 2) than GPT-3.5, within this experimental framework.</p>
<p>Table 3 shows the results when we directly apply the optimized prompts from GPT-3.5 to GPT-4 and PaLM 2 (chat-bison-001) across all 12 tasks. For comparison, we also adopt the same human and APE prompts to these base LLMs as baselines. For certain tasks, such as Penguins, we may employ slightly different prompts than those referenced in Table 1 to make PaLM 2 generate reasonable responses instead of persistent null answers. Observing Table 3, it is worth highlighting that when a stronger base LLM as GPT-4 is deployed, our expert prompts manifest further enhancements, either on par with or outperforming Human and APE prompts in almost all tasks (11/12) (The only exception, Temporal, seems to be a solved task by GPT-4 with almost perfect accuracy). This underscores the untapped potential of expert prompting, especially with the evolution of more sophisticated LLMs in the near future. When transferring expert prompts to a weaker LLM as PaLM</p>
<p>Table 3: Prompt generalization results. While we optimize GPT-3.5 as the default base LLM, its optimized prompts are transferable to other base LLMs like GPT-4 and PaLM 2 (chat-bison-001). GPT-4 sees further enhancement with our prompts, beating baselines in 11/12 tasks. Weaker LLMs like PaLM 2 may have challenges with our advanced prompts but still surpass baselines in 7/12 tasks. Overall, ours can significantly beat baselines with different base LLMs.</p>
<table>
<thead>
<tr>
<th>Tasks</th>
<th>GPT-3.5</th>
<th></th>
<th></th>
<th>GPT-4</th>
<th></th>
<th></th>
<th>PaLM 2</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Human</td>
<td>APE</td>
<td>Ours</td>
<td>Human</td>
<td>APE</td>
<td>Ours</td>
<td>Human</td>
<td>APE</td>
<td>Ours</td>
</tr>
<tr>
<td>Penguins</td>
<td>0.595</td>
<td>0.747</td>
<td>$\mathbf{0 . 7 9 7}$</td>
<td>0.772</td>
<td>0.848</td>
<td>$\mathbf{0 . 9 6 2}$</td>
<td>0.430</td>
<td>0.443</td>
<td>$\mathbf{0 . 4 5 6}$</td>
</tr>
<tr>
<td>Geometry</td>
<td>0.227</td>
<td>0.490</td>
<td>$\mathbf{0 . 6 7 0}$</td>
<td>0.495</td>
<td>0.445</td>
<td>$\mathbf{0 . 6 8 0}$</td>
<td>0.290</td>
<td>0.215</td>
<td>$\mathbf{0 . 3 6 0}$</td>
</tr>
<tr>
<td>Epistemic</td>
<td>0.452</td>
<td>0.708</td>
<td>$\mathbf{0 . 8 0 6}$</td>
<td>0.734</td>
<td>$\mathbf{0 . 8 4 8}$</td>
<td>$\mathbf{0 . 8 4 8}$</td>
<td>0.470</td>
<td>0.392</td>
<td>$\mathbf{0 . 5 8 8}$</td>
</tr>
<tr>
<td>Object Count.</td>
<td>0.612</td>
<td>0.716</td>
<td>$\mathbf{0 . 8 6 0}$</td>
<td>0.830</td>
<td>0.852</td>
<td>$\mathbf{0 . 8 8 8}$</td>
<td>0.290</td>
<td>$\mathbf{0 . 3 7 8}$</td>
<td>0.320</td>
</tr>
<tr>
<td>Temporal</td>
<td>0.720</td>
<td>0.856</td>
<td>$\mathbf{0 . 9 3 4}$</td>
<td>0.980</td>
<td>$\mathbf{0 . 9 9 2}$</td>
<td>0.982</td>
<td>0.540</td>
<td>0.522</td>
<td>$\mathbf{0 . 6 2 0}$</td>
</tr>
<tr>
<td>Causal Judge.</td>
<td>0.470</td>
<td>0.570</td>
<td>$\mathbf{0 . 6 7 0}$</td>
<td>0.740</td>
<td>0.740</td>
<td>$\mathbf{0 . 7 7 0}$</td>
<td>$\mathbf{0 . 4 4 0}$</td>
<td>$\mathbf{0 . 4 4 0}$</td>
<td>0.430</td>
</tr>
<tr>
<td>NCBI (F1)</td>
<td>0.521</td>
<td>0.576</td>
<td>$\mathbf{0 . 6 4 5}$</td>
<td>0.588</td>
<td>0.428</td>
<td>$\mathbf{0 . 6 9 7}$</td>
<td>0.016</td>
<td>0.025</td>
<td>$\mathbf{0 . 1 7 7}$</td>
</tr>
<tr>
<td>Biosses</td>
<td>0.550</td>
<td>0.700</td>
<td>$\mathbf{0 . 7 5 0}$</td>
<td>0.700</td>
<td>0.775</td>
<td>$\mathbf{0 . 8 0 0}$</td>
<td>0.500</td>
<td>0.300</td>
<td>$\mathbf{0 . 6 0 0}$</td>
</tr>
<tr>
<td>Med QA</td>
<td>0.508</td>
<td>0.470</td>
<td>$\mathbf{0 . 5 7 0}$</td>
<td>0.770</td>
<td>0.758</td>
<td>$\mathbf{0 . 7 7 4}$</td>
<td>$\mathbf{0 . 2 8 4}$</td>
<td>0.274</td>
<td>0.276</td>
</tr>
<tr>
<td>Subj</td>
<td>0.517</td>
<td>0.696</td>
<td>$\mathbf{0 . 8 0 6}$</td>
<td>0.867</td>
<td>0.805</td>
<td>$\mathbf{0 . 8 7 9}$</td>
<td>0.496</td>
<td>$\mathbf{0 . 5 3 7}$</td>
<td>0.499</td>
</tr>
<tr>
<td>TREC</td>
<td>0.742</td>
<td>0.834</td>
<td>$\mathbf{0 . 8 8 6}$</td>
<td>0.716</td>
<td>0.764</td>
<td>$\mathbf{0 . 8 7 6}$</td>
<td>0.380</td>
<td>$\mathbf{0 . 4 0 0}$</td>
<td>0.230</td>
</tr>
<tr>
<td>CB</td>
<td>0.714</td>
<td>0.804</td>
<td>$\mathbf{0 . 9 1 4}$</td>
<td>$\mathbf{0 . 9 1 1}$</td>
<td>0.893</td>
<td>$\mathbf{0 . 9 1 1}$</td>
<td>0.571</td>
<td>0.643</td>
<td>$\mathbf{0 . 7 3 2}$</td>
</tr>
<tr>
<td>Average</td>
<td>0.552</td>
<td>0.685</td>
<td>$\mathbf{0 . 7 7 6}$</td>
<td>0.759</td>
<td>0.762</td>
<td>$\mathbf{0 . 8 3 9}$</td>
<td>0.392</td>
<td>0.381</td>
<td>$\mathbf{0 . 4 4 1}$</td>
</tr>
</tbody>
</table>
<p>2, its performance drops dramatically across all tasks unexpectedly. Nonetheless, we still observe PromptAgent exceeds both baselines on 7/12 tasks, with great improvements on domain-specialized tasks, such as $N C B I$, demonstrating the usefulness of domain insights from expert prompts.</p>
<p>Ablation on search strategies. To investigate the effect of strategic planning in PromptAgent systematically, we conduct a thorough ablation study by comparing multiple alternative search strategies to MCTS, i.e., a single Monte Carlo (MC) search, a greedy depth-first search (Greedy), and a Beam search. We use the same action generation and state transition as in PromptAgent and only replace the MCTS planning with each search method. Specifically, MC is a directionless search with a single step of randomly sampling and selecting one action. Greedy provides more structured exploration by consistently choosing the best among multiple samples per step. Beam search also focuses on a structured exploration by keeping multiple promising paths at each level. We keep the same number of overall explored prompts (exploration efficiency; see below for more results) for all three baselines to have a similar exploration space. See more implementation details about search variants in Appendix A.4.</p>
<p>We select a subset of tasks from all three domains to compare all the above search variants due to the computation budget. Table 4 shows that both Greedy and Beam greatly improve the MC baseline, suggesting the necessity of structured iterative exploration in our framework. When maintaining the same exploration efficiency, we observe comparable overall performance for Beam and Greedy. However, neither method strategically explores the prompt space since they operate in a strictly forward direction, lacking the capability to foresee future outcomes and backtrack to past decisions. In contrast, the strategic planning for MCTS allows PromptAgent to navigate complex expert prompt spaces more effectively, which significantly surpasses all search ablations on all tasks and gets a relative $5.6 \%$ overall improvement over the best baseline.</p>
<p>Exploration efficiency analysis. In addition to the superior performance, one of the key advantages of PromptAgent is that it can efficiently explore the prompt space via strategic planning. Explo-</p>
<p>Table 5: Prompt comparison for the NCBI task, including normal human prompt, APE-optimized prompt, and expert-level prompt optimized by PromptAgent. Both baselines mostly describe the task, while our expert prompt is composed of more complex structures and domain-specific insights, achieving superior performance. Bold text denotes domain knowledge usually handcrafted by domain specialists, but here automatically discovered by PromptAgent. We highlight different aspects of expert prompt with colors, including Task Description, Term Clarification, Solution Guidance, Exception Handling, Priority \&amp; Emphasis, Formatting. (Best view with colors)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Approach</th>
<th style="text-align: left;">Optimized Prompt</th>
<th style="text-align: center;">F1 score.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Human</td>
<td style="text-align: left;">Extract the disease or condition from the sentence, if any is mentioned.</td>
<td style="text-align: center;">0.521</td>
</tr>
<tr>
<td style="text-align: left;">APE</td>
<td style="text-align: left;">If any disease or condition is mentioned in the sentence, extract it.</td>
<td style="text-align: center;">0.576</td>
</tr>
<tr>
<td style="text-align: left;">PromptAgent</td>
<td style="text-align: left;">You're tasked with extracting diseases or conditions from the given sen- <br> tence, remember to be cautious and avoid incorporating any associated <br> elements such as inheritance patterns (like autosomal dominant), <br> genes or gene loci (like PAH), proteins, or biological pathways. The <br> task does not entail making assumptions or inferences about the disease <br> names based on other advanced biological terms in the context. Con- <br> sider both specific diseases and broader categories, and remember <br> diseases and conditions can also appear as common abbreviations or <br> variations. Provide the identified diseases or conditions in this format: <br> {entity_1,entity_2,\ldots}. If there are no diseases or conditions present, out- <br> put an empty list in this form: {}. Note that the term 'locus' should be <br> recognized as a genomic location and not a disease name.</td>
<td style="text-align: center;">0.645</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: (a) Exploration efficiency analysis. A proper balance of exploration and exploitation is crucial for search and planning. We compare the number of explored prompts between our method and three strong baselines. Ours achieves the best trade-off of performance and exploration (clustering in the top-left corner). (b) Convergence curves for Epistemic task. We visualize the mean and variance of the training and testing performance along the paths. We can observe that both curves increase at first and become stable after depth 3 , suggesting a stable learning process
ration efficiency is also vital to make the computation cost of the search manageable. We thus analyze the exploration efficiency by comparing PromptAgent with some of our search baselines, including Greedy Search and APE from the previous section. Specifically, the exploration efficiency is measured by the number of prompts explored during the search, i.e., nodes generated during the exploration. We plot its relationship with the task performance in Figure 4a. The Greedy-S and Greedy-L are based on Greedy Search with 34 and 72 explored prompts. The APE explores 150 prompts in each task. The figure shows that points of PromptAgent are clustered around the top left corner, indicating a superior performance with higher accuracy but fewer explored nodes (higher exploration efficiency). Notably, while increasing the number of prompts in Greedy Search may enhance performance (from Greedy-S to Greedy-L), it demands higher exploration cost and still does not surpass PromptAgent. Also, without principled guidance, directionless searches like APE cannot effectively boost performance, even with larger exploration. Nevertheless, to maintain exploration efficiency and superior performance, strategic planning is crucial in PromptAgent and worthy</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: The MCTS state-action transition trajectory of the highest average reward path in NCBI. The initial state is $s_{0}$ with a human-written prompt. At each state transition step, a new prompt is crafted by adjusting the prior state based on error feedback. Highlighted colors indicate similar domain-specific insights. The last state integrates the information from the entire trajectory, elevating the F1 score from 0.521 to 0.645 .</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>of further research investment in future works. The detailed hyperparameter settings of Greedy-S, Greedy-L, and APE are in Appendix A. 4</p>
<p>Convergence analysis. To delve deeper into the learning process of PromptAgent, we examine the evolution of expert prompts throughout the tree planning process. Specifically, we monitor and visualize performance changes with respect to tree depth. As illustrated in Figure 4b for the Epistemic task, we assess the performance across all nodes and aggregate both training (reward) and testing performance at each depth level. The plotted trajectories represent the evolution of average performance on both training (reward) and testing, illustrating a consistent improvement and gradually surpassing all baseline methods. For brevity, convergence plots for other tasks and hyperparameter settings, focusing solely on training trajectories to reduce computational overhead on testing sets, are provided in Appendix C and Appendix A.3. A recurring pattern observed, similar to that in Figure 4b, indicates an upward trend in the initial iterations, suggesting a robust learning dynamic of PromptAgent to iteratively refine and enhance expert prompts.</p>
<p>Qualitative analysis. To provide a more direct illustration of how PromptAgent progressively leverages error feedback (action) to enhance prompts (states), we conduct a qualitative analysis to examine the optimized trace from PromptAgent exploration. Figure 5 displays the initial four states and the corresponding three action-state transitions for the best reward path associated with the NCBI task (Doğan et al., 2014) to extract disease entities. We highlight the domain insights by colors in both actions and states, where consistent coloring signifies analogous insights. Observably, from an initial human-composed prompt as $s_{0}$, PromptAgent discovers various insightful error feedback (action) and effectively merges them into a refined prompt (state) with improved test performance. Over successive transitions, the definition of disease entities becomes increasingly refined, and biomedical-specific details are seamlessly integrated. The accumulation of this iterative process is reflected in the last state, $s_{3}$, which, infused with aggregated insights from its preceding path, manifests as an expert-level prompt, leading to a superior performance.</p>
<p>We further annotate various quality aspects of optimized expert prompts, highlighting important perspectives on how expert prompts advance prompt engineering and provoke advanced task understanding of LLMs. As shown in Table 15 for the NCBI task and Appendix D for all other tasks, in comparison with general human prompts and APE-optimized prompts, PromptAgent prompts are typically more elaborate, offering comprehensive task instruction, which covers various diverse aspects, such as clarifying terminologies, guiding solutions, handling exceptional cases, etc. It is imperative to mention that while future research might explore prompt compression techniques (Jiang et al., 2023; Yin et al., 2023) to condense the expert prompt without sacrificing performance, the</p>
<p>increased complexity of expert-level prompting naturally aligns with the advancement of contemporary state-of-the-art LLMs, enabling more sophisticated understanding of tasks and human requests.</p>
<h1>5 CONCLUSION</h1>
<p>In this paper, we introduce PromptAgent, a novel prompt optimization framework capable of autonomously crafting expert-level prompts for a given task. Expert-level prompting distinguishes itself from traditional prompt engineering by its effectiveness of seamlessly integrating domain insights and closing the knowledge gap for domain experts. To achieve this, central to PromptAgent is the novel perspective of viewing prompt optimization as a strategic planning problem, leveraging the power of MCTS planning to strategically and efficiently traverse the complex prompt space. PromptAgent incorporates domain-specific knowledge from tasks into the newly generated prompts through a trial-and-error manner based on the self-reflection abilities of LLMs. We tested the PromptAgent on 12 diverse tasks spanning three distinct domains. The prompts optimized by PromptAgent consistently exhibited expert-level characteristics, enriched with domain-specific details and guidance. These prompts significantly outperformed both human-written, Chain-ofThought prompting and other optimized method baselines. Further in-depth analyses revealed superior transferability, exploration efficiency, and quality for our expert prompts, paving the way for future prompt engineering to unlock the sophisticated task understanding of state-of-the-art LLMs.</p>
<h2>REFERENCES</h2>
<p>Tom Bylander. The computational complexity of propositional strips planning. Artificial Intelligence, 69(1-2):165-204, 1994.</p>
<p>Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank: Investigating projection in naturally occurring discourse. In proceedings of Sinn und Bedeutung, volume 23, pp. 107-124, 2019.</p>
<p>Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 3369-3391, 2022.</p>
<p>Rezarta Islamaj Doğan, Robert Leaman, and Zhiyong Lu. Ncbi disease corpus: a resource for disease name recognition and concept normalization. Journal of biomedical informatics, 47:110, 2014.</p>
<p>Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rocktäschel. Promptbreeder: Self-referential self-improvement via prompt evolution. arXiv preprint arXiv:2309.16797, 2023.</p>
<p>Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. arXiv preprint arXiv:2309.08532, 2023.</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023a.</p>
<p>Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. arXiv preprint arXiv:2305.11554, 2023b.</p>
<p>Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021.</p>
<p>Eric Jang. Can llms critique and iterate on their own outputs? evjang.com, Mar 2023. URL https://evjang.com/2023/03/26/self-reflection.html.</p>
<p>Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large language models. arXiv preprint arXiv:2310.05736, 2023.</p>
<p>Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438, 2020.</p>
<p>Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021.</p>
<p>Ana Jojic, Zhen Wang, and Nebojsa Jojic. Gpt is becoming a turing machine: Here are some ways to program it. arXiv preprint arXiv:2303.14310, 2023.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199-22213, 2022.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045-3059, 2021.</p>
<p>Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582-4597, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353. URL https://aclanthology.org/2021.acl-long.353.</p>
<p>Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. Llm+ p: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477, 2023.</p>
<p>Nikolay Malkin, Zhen Wang, and Nebojsa Jojic. Coherence boosting: When your pretrained language model is not paying enough attention. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8214-8236, 2022.</p>
<p>John McCarthy et al. Situations, actions, and causal laws. Comtex Scientific, 1963.
Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al. Augmented language models: a survey. arXiv preprint arXiv:2302.07842, 2023.</p>
<p>OpenAI, Sep 2023a. URL https://openai.com/blog/chatgpt-plugins.
OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023b. URL https://api. semanticscholar.org/CorpusID:257532815.</p>
<p>Batu Ozturkler, Nikolay Malkin, Zhen Wang, and Nebojsa Jojic. Thinksum: Probabilistic reasoning over sets using large language models. arXiv preprint arXiv:2210.01293, 2022.</p>
<p>Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. Automatically correcting large language models: Surveying the landscape of diverse selfcorrection strategies. arXiv preprint arXiv:2308.03188, 2023.</p>
<p>Bo Pang and Lillian Lee. A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pp. 271-es, 2004.</p>
<p>Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. Refiner: Reasoning feedback on intermediate representations. arXiv preprint arXiv:2304.01904, 2023.</p>
<p>Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. Grips: Gradient-free, edit-based instruction search for prompting large language models. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 3827-3846, 2023.</p>
<p>Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with" gradient descent" and beam search. arXiv preprint arXiv:2305.03495, 2023.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. $4222-4235,2020$.</p>
<p>Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. arXiv preprint arXiv:2303.11366, 2023.</p>
<p>Gizem Soğancıoğlu, Hakime Öztürk, and Arzucan Özgür. Biosses: a semantic sentence similarity estimation system for the biomedical domain. Bioinformatics, 33(14):i49-i58, 2017.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.</p>
<p>Ellen M Voorhees and Dawn M Tice. Building a question answering test collection. In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pp. 200-207, 2000.</p>
<p>Zhen Wang, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, Huan Sun, and Yoon Kim. Multitask prompt tuning enables parameter-efficient transfer learning. In The Eleventh International Conference on Learning Representations, 2022.</p>
<p>Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. Generating sequences by learning to self-correct. arXiv preprint arXiv:2211.00053, 2022.</p>
<p>Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. arXiv e-prints, pp. arXiv-2302, 2023.</p>
<p>Lilian Weng. Llm-powered autonomous agents. lilianweng.github.io, Jun 2023. URL https: //lilianweng.github.io/posts/2023-06-23-agent/.</p>
<p>Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Wang Yanggang, Haiyu Li, and Zhilin Yang. Gps: Genetic prompt search for efficient few-shot learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 8162-8171, 2022.</p>
<p>Weijia Xu, Andrzej Banburski-Fahey, and Nebojsa Jojic. Reprompting: Automated chain-of-thought prompt inference through gibbs sampling. arXiv preprint arXiv:2305.09993, 2023.</p>
<p>Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.</p>
<p>Fan Yin, Jesse Vig, Philippe Laban, Shafiq Joty, Caiming Xiong, and Chien-Sheng Jason Wu. Did you read the instructions? rethinking the effectiveness of task definitions in instruction learning. arXiv preprint arXiv:2306.01150, 2023.</p>
<p>Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph E Gonzalez. Tempera: Test-time prompt editing via reinforcement learning. In The Eleventh International Conference on Learning Representations, 2022.</p>
<p>Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh International Conference on Learning Representations, 2022.</p>
<p>Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang. Solving math word problem via cooperative reasoning induced language models. arXiv preprint arXiv:2210.16257, 2022.</p>
<h1>Algorithm 1 PromptAgent-MCTS $\left(s_{0}, p_{\theta}, r_{\theta}, p_{\phi}, d, L, \tau, c\right)$</h1>
<h2>Inputs:</h2>
<p>Initial prompt (state) $s_{0}$, state transition function $p_{\theta}$, reward function $r_{\theta}$, action generation function $p_{\phi}$, number of generated actions $d$, depth limit $L$, iteration number $\tau$, exploration weight $c$ (Equation 1)</p>
<h2>Initialize:</h2>
<p>State to action mapping $A: \mathcal{S} \mapsto \mathcal{A}$, children mapping ch : $\mathcal{S} \times \mathcal{A} \mapsto \mathcal{S}$, rewards $r: \mathcal{S} \times \mathcal{A} \mapsto \mathbb{R}$, State-action value function $Q: \mathcal{S} \times \mathcal{A} \mapsto \mathbb{R}$, visit-time counter $\mathcal{N}: \mathcal{S} \mapsto \mathbb{N}$
for $n \leftarrow 0, \ldots, \tau-1$ do
for $t \leftarrow 0, \ldots, L-1$ do
if $A\left(s_{t}\right)$ is not empty then
$\triangleright$ selection</p>
<p>$$
\begin{aligned}
&amp; a_{t} \leftarrow \arg \max <em t="t">{a \in A\left(s</em>\right) \
&amp; s_{t+1} \leftarrow \operatorname{ch}\left(s_{t}, a_{t}\right), r_{t} \leftarrow r\left(s_{t}, a_{t}\right), \mathcal{N}\left(s_{t}\right) \leftarrow \mathcal{N}\left(s_{t}\right)+1
\end{aligned}
$$}\right)}\left(Q\left(s_{t}, a\right)+c \cdot \sqrt{\frac{\ln \mathcal{N}\left(s_{t}\right)}{\mathcal{N}\left(\operatorname{ch}\left(s_{t}, a\right)\right)}</p>
<p>else
for $i \leftarrow 1, \ldots, d$ do
Sample $a_{t}^{i} \sim p_{\phi}\left(a \mid s_{t}\right), s_{t+1}^{i} \sim p_{\theta}\left(s \mid s_{t}, a_{t}^{i}\right)$, and $r_{t}^{i} \leftarrow r_{\theta}\left(s_{t}, a_{t}^{i}\right)$
Update $A\left(s_{t}\right) \leftarrow\left{a_{t}^{i}\right}<em t="t">{i=1}^{d}, \operatorname{ch}\left(s</em>$
end for
$a_{t} \leftarrow \arg \max }, a_{t}^{i}\right) \leftarrow s_{t+1}^{i}$, and $r\left(s_{t}, a_{t}^{i}\right) \leftarrow r_{t}^{i<em t="t">{a</em>\right)$
$s_{t+1} \leftarrow \operatorname{ch}\left(s_{t}, a_{t}\right), r_{t} \leftarrow r\left(s_{t}, a_{t}\right), \mathcal{N}\left(s_{t}\right) \leftarrow \mathcal{N}\left(s_{t}\right)+1$
end if
if $s_{t+1}$ is an early-stopping state then break
end for
$T \leftarrow$ the actual number of steps
for $t \leftarrow T-1, \ldots, 0$ do
Update $Q\left(s_{t}, a_{t}\right)$ with $\left{r_{t}, r_{t+1}, \ldots, r_{L}\right}$ based on Equation 2
end for
end for}^{i} \in A\left(s_{t}\right)} r_{t}^{i}\left(s_{t}, a_{t}^{i</p>
<h1>A MORE EXPERIMENT DETAILS</h1>
<h2>A. 1 INPUT FORMULATION</h2>
<p>The normal model input is composed of the following components:</p>
<p>$$
\text { Prompt }+ \text { Task Prefix }+ \text { Question }+ \text { Task Suffix }+ \text { Answer Format }
$$</p>
<p>"Prompt" is the optimization target. "Task Prefix" (Optional) is the task-specific background intro (For example, a table of background data in the Penguins). "Question" is the main body of the task's question. "Task Suffix" (Optional) includes the options (For example, yes/no, entailment/nonentailment, or A, B, C, D in tasks with multiple choices). "Answer Format" (Optional) is designed for answer caption from the model's response. Examples of the task input are in Appendix B.</p>
<p>The meta formats and prompts, as explained in Section 3.1, are in Appendix A.5.</p>
<h2>A. 2 Data SPLIT</h2>
<p>For datasets with predefined testing sets, we directly use them as our testing set. When these exceed 1,000 examples, we sample 1000 from them. If no default testing set is provided, we shuffle the data and allocate approximately half for testing purposes. We then sample a subset from the remaining data as the training set. From this training set, a held-out subset is sampled for reward calculation with a default size of 150 . If the training set is smaller than 150 or very large, the subset will range between 60 to 200 examples accordingly. The data split details are in Table 6.</p>
<h2>A. 3 MORE IMPLEMENTATION DETAILS</h2>
<p>PromptAgent (Ours). PromptAgent performs MCTS planning within the prompt space, requiring both terminal state conditions and a reward function. A terminal state is achieved when the path length hits depth_limit. The reward function is determined by the base model's performance on the held-out set. For computational efficiency to avoid unnecessary exploration, we also apply an early-stopping method after depth is larger than 2: if the state's reward is less than a min_threshold or larger than a max_threshold, we then reach an early-stopping state. Specifically, min_threshold is the average of the rewards of its parent node and the root node, while max_threshold is the maximum of all the current nodes, which encourages shorter paths. We now further illustrate the details of Algorithm 1.</p>
<ol>
<li>Initialization. The PromptAgent-MCTS algorithm starts with an initial prompt as the root node. For BBH tasks, we directly adopt the task "description" from the original datasets as the initial prompts, except that Object Counting's default description doesn't follow the format of instruction. We crafted the initial prompts for the rest of the tasks according to their task objectives or question-answer formats. The root node will be evaluated to obtain the reward before the first expansion.</li>
<li>MCTS Iterations. The agent will perform 12 MCTS iterations. During the selection step, starting from the root node, the best child node will be added to the path according to its UCT value (Equation 1), and the exploration weight $c$ in UCT is 2.5. During the expansion step, expand_width batches (batch_size is 5) of examples will be sampled from the training set, and each batch will be fed to the base model to collect the errors. If there is no error, this sample-forward loop will iterate until an error is found. The errors will be</li>
</ol>
<p>formatted using error_string (illustrated in Table 8) and inserted into error_feedback (illustrated in Table 8, Meta-prompt 1 in Figure 3) to summarize errors by the optimizer. state_transit prompt (illustrated in Table8, Meta-prompt 2 in Figure 3) contains the expanding node's prompt, the trajectory of prompts (list of prompts from the root of the expanding node on the currently selected path), and the error summarization, which is fed into the optimizer to generate num_samples new prompts (nodes). The new nodes will be evaluated and added as the expanding node's children if they are not terminal nodes. Each expansion will generate expand_width $\times$ num_samples new prompts. The simulation step will recursively expand the last node in the path and pick the one with the highest reward to add to the path. When the last node satisfies the terminal condition or early-stopping condition, the simulation is stopped. During the back-propagation, from the last node to the root, the cumulative rewards (the sum of rewards from the node to the leaf/terminal node) will be appended to the node's cumulative reward list, the average of which will be the node's $Q$ (Equation 2). We have three hyperparameter settings: Standard, Wide, and Lite in Table 7. In the Standard and Lite experiments, both have an expand_width of 3 and num_samples of 1, but their depth_limit are 8 for Standard and 4 for Lite. Wide experiment has expand_width is 3 and num_samples $=2$ to generate more nodes in each expansion step, but with a depth_limit of 6 to limit the total number of explored prompts. We select the best setting for each task based on the final rewards.
3. Output strategy. Each MCTS iteration will output one path from the root node to the leaf node, and there are tens of nodes generated after the searching process. We select the path with the highest average reward, then pick the prompt with the highest reward in the path as the final output prompt. We employ this strategy because the path with the highest average reward represents the best overall search trajectory, and also, the best prompt might not always be the last node on the optimal path, given that it may be a terminal state by reaching the depth limit.</p>
<p>Table 7: Hyperparameter settings for PromptAgent Experiments</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Experiment Name</th>
<th style="text-align: center;">Standard</th>
<th style="text-align: center;">Wide</th>
<th style="text-align: center;">Lite</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">depth_limit</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">expand_width</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: left;">num_samples</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<h1>A. 4 BASELINES IMPLEMENTATION DETAILS</h1>
<p>We illustrate the details for various baselines in our experiments.
Monte Carlo (MC). MC performs one-step sampling multiple times and selects the best one as the optimized prompt. It uses the same prompt sampling method as PromptAgent, but limits the searching depth to one. In the search ablation study, we sampled 72 new prompts in each task.</p>
<p>Beam Search (Beam). Beam also uses the same expand function as PromptAgent. Each node, except the root, will be expanded into 3 new nodes, and the beam width is 3 , meaning that there will be 9 nodes in each depth of the search tree, and the best 3 nodes will be kept for the next expansion. The root will be expanded into 9 new nodes. The search depth is 8 , so there will be 72 nodes or new prompts in total.</p>
<p>Greedy Search (Greedy). Greedy is based on the Beam Search, but the beam width is one, so the algorithm turns into a depth-first greedy search. We conducted 2 experiments, Greedy-S and Greedy-L, in Figure 4a, with the same search depth of 8 but different expand widths. The GreedyS's expand width is 3, and it has 34 prompts in total. The Greedy-L has an expand_width of 9 and 72 nodes in total, which is also referred to as the Greedy baseline in Table 4.</p>
<p>APE (Zhou et al., 2022). We employ the iterative APE with one iteration as our baseline, as suggested by the original paper (Zhou et al., 2022). When generating new prompts, a mini-batch comprising 5 data pieces is sampled as Input-Output examples for APE. Specifically, for Initial Proposal Step, by default, 10 data batches are sampled, with each batch being used to generate 10 new</p>
<p>prompts. This results in a total of 100 candidate prompts during the initial step. (Due to the longer processing time of Med QA, only 25 candidates are generated for it in this phase.) Subsequently, the five prompts with the highest evaluation scores are chosen for the iterative proposal step. For Iterative Proposal Step, similar to the initial phase, 10 batches of data are sampled for each proposed prompt, resulting in a total of 50 candidate prompts in this step. Following this, the prompt with the top evaluation score is chosen as the optimized prompt.</p>
<h1>A. 5 Meta Formats</h1>
<p>In this section, we present the full formats for meta-prompts used in the PromptAgent. "input_format" is the actual input of the base model given a question. "error_string" represents the format of each error example. "error_feedback" includes several error examples and guides the optimizer model to collect the error feedback. "state_transit" guides the optimizer model to make state transitions (generate new prompts), which includes the information of error examples and the sequence of prompts in the selected path, which is the "trajectory_prompts".</p>
<p>Table 8: Meta Formats.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Format Name</th>
<th style="text-align: center;">Meta Format</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">input_format</td>
<td style="text-align: center;">{prompt} <br> {task_prefix} <br> {question} <br> {task_suffix} <br> {answer_format}</td>
</tr>
<tr>
<td style="text-align: center;">error_string</td>
<td style="text-align: center;">$&lt;{$ index $}&gt;$ <br> The model's input is: <br> {question} <br> The model's response is: <br> {response} <br> The correct label is: {label} <br> The model's prediction is {prediction}</td>
</tr>
<tr>
<td style="text-align: center;">error_feedback</td>
<td style="text-align: center;">I'm writing prompts for a language model designed for a task. <br> My current prompt is: <br> {cur_prompt} <br> But this prompt gets the following examples wrong: <br> {error_string} <br> For each wrong example, carefully examine each question and wrong answer step by step, provide comprehensive and different reasons why the prompt leads to the wrong answer. At last, based on all these reasons, summarize and list all the aspects that can improve the prompt.</td>
</tr>
<tr>
<td style="text-align: center;">state_transit</td>
<td style="text-align: center;">I'm writing prompts for a language model designed for a task. <br> My current prompt is: <br> {cur_prompt} <br> But this prompt gets the following examples wrong: <br> {error_string} <br> Based on these errors, the problems with this prompt and the reasons are: <br> {error_feedback} <br> There is a list of former prompts including the current prompt, and each prompt is modified from its former prompts: <br> {trajectory_prompts} <br> Based on the above information, please write {steps_per_gradient} new prompts following these guidelines: <br> 1. The new prompts should solve the current prompt's problems. <br> 2. The new prompts should consider the list of prompts and evolve based on the current prompt. <br> 3. Each new prompt should be wrapped with $&lt;$ START $&gt;$ and $&lt;$ END $&gt;$. <br> The new prompts are:</td>
</tr>
</tbody>
</table>
<h1>B Task Input EXAMPLES</h1>
<p>In this section, we show some input examples in several tasks for the base model. Specifically, our tasks fall into three categories: multi-choice selection, name entity recognition, and direct answer matching. As representative examples, we select Penguins in A Table, NCBI, and Subjective to illustrate the input format.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Input format of Penguins in A Table task.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Input formats of $N C B I$ and Subjective task.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Prompt: Extract the disease or condition from the sentence, if any is mentioned.
${ }^{2}$ F1 score (test): 0.521
Prompt: Identify and extract all diseases or conditions mentioned in the sentence, taking care to distinguish between diseases and any associated factors like genes. Any variations or abbreviations of disease names should also be included.
${ }^{3}$ F1 score (test): 0.609
Prompt: You're tasked with identifying and extracting diseases or conditions as mentioned in the sentence, while carefully excluding any associated factors such as genes, proteins, or pathways. For clarity, the term 'focus' is not part of any disease name but represents a specific location in the genome.
${ }^{4}$ F1 score (test): 0.622
Prompt: You're tasked with extracting diseases or conditions avoid incorporating any associated elements such as inheritance patterns (like autosomal dominant), genes or gene loci (like PAH), proteins, or biological pathways. ... Consider both specific diseases and broader categories, and remember diseases and conditions can also appear as common abbreviations or variations. Provide the identified diseases or conditions in this format (entity_1, entity_2, ...). Note that the term 'focus' should be recognized as a genomic location and not a disease name.
${ }^{5}$ F1 score (test): 0.645&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8936 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8936</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8936</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-8cdb7845b94fada3be213e2abcafa7da6625879f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8cdb7845b94fada3be213e2abcafa7da6625879f" target="_blank">Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies) and proposes the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models are fully open-sourced.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8936.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8936.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Satori-Qwen-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Satori-Qwen-7B (Satori: 7B LLM trained with COAT + RAE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B single LLM (based on Qwen-2.5-Math-7B) trained to internalize autoregressive search via Chain-of-Action-Thought (COAT) and large-scale reinforcement learning with Restart-and-Explore (RAE), enabling self-reflection and iterative self-improvement without external verifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Satori-Qwen-7B (based on Qwen-2.5-Math-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B-parameter LLM built from the Qwen-2.5-Math-7B base; format-tuned on ~10k COAT demonstration trajectories and then trained with large-scale PPO reinforcement learning (RAE), plus iterative distillation (SFT) rounds. Trained on public math instruction datasets (OpenMathInstruct-2, NuminaMathCoT) and synthetic demonstration data from a multi-agent synthesis pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Chain-of-Action-Thought (COAT) + Restart-and-Explore (RAE)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>COAT augments generation with meta-action tokens (<|continue|>, <|reflect|>, <|explore|>) so the model can choose to continue, pause and verify, or explore alternatives; RAE augments RL by restarting rollouts from intermediate (backtracked) states (both correct and incorrect trajectories), appending a <|reflect|> token to trigger refinement, and using reward shaping (rule-based reward, reflection bonus, and a learned Outcome Reward Model) to encourage successful correction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mathematical reasoning benchmarks (GSM8K, MATH500, OlympiadBench, AMC2023, AIME2024) and additional out-of-domain reasoning (FOLIO, BGQA, CRUXEval, StrategyQA, TableBench, MMLUPro-STEM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard math problem-solving and diverse reasoning benchmarks used to evaluate problem solving and general reasoning; main metric is zero-shot pass@1 accuracy using greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>On math benchmarks Satori-Qwen-7B (COAT + RL + RAE) achieves: GSM8K 93.2%, MATH500 85.6%, OlympiadBench 46.6%, AMC2023 67.5%, AIME2024 20.0% (Avg. reported 62.6 for small-scale models); after a second round of iterative self-improvement Satori-Qwen-7B (Round 2) improves to GSM8K 93.9%, OlympiadBench 48.5%, AMC2023 72.5%, AIME2024 23.3% (Avg. 64.4). On out-of-domain benchmarks Satori shows gains (e.g., BGQA 61.8%, CRUXEval 42.5%, MMLUPro-STEM 56.7%, Avg. 60.4; Round 2 Avg. 60.8).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Comparative baselines: Qwen-2.5-Math-7B-Instruct (same base w/o COAT+RAE) reports GSM8K 95.2%, MATH500 83.6%, OlympiadBench 41.6%, AMC2023 62.5%, AIME2024 16.7% (Avg. 59.9). A CoT-trained variant (Qwen-7B-CoT; ablated COAT actions) scored lower than Satori on several benchmarks (e.g., GSM8K 93.1%, MATH500 84.4%, Olympiad 42.7%). Format-tuned only (Satori-Qwen-7B-FT, larger FT data) performs worse than COAT+RL: e.g., Satori-Qwen-7B-FT (300K FT data) GSM8K 92.3%, MATH500 78.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Implemented via special meta-action tokens (<|reflect|>, <|explore|>, <|continue|>) embedded in the tokenization/format; training uses format-tuning on demonstration COAT trajectories, then PPO RL where initial states include backtracked partial trajectories with appended <|reflect|> tokens (RAE); reward shaping includes rule-based correctness reward, reflection bonuses (positive for correcting from incorrect restarts, negative for destroying correct restarts), and a learned Outcome Reward Model (Bradley–Terry preference model) to provide dense preference signals.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: (1) Benchmark gains: Satori (COAT+RL) outperforms same-base instruct and CoT-trained variants across math and many out-of-domain tasks (Tables 1,2,3,5). (2) Self-correction analysis (Table 4): format-tuned only Satori-Qwen-7B-FT shows low positive self-correction (F→T: 20.6% on MATH500), whereas Satori-Qwen-7B (with RL) shows high positive self-correction (F→T: 61.0% on MATH500), indicating RL substantially improves the model's ability to correct incorrect answers via reflection. (3) Test-time scaling: Figures 3–4 show RL training increases response length (more tokens spent reasoning) and test-time accuracy, allocating more compute to harder problems.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported and observed limitations include: (1) Nonzero rate of negative self-corrections (T→F): e.g., Satori still flips correct answers to incorrect ones (T→F ~39.0% on MATH500), so reflection is not universally beneficial. (2) Early RL can find shortcut policies that reduce response length and avoid reflection, temporarily degrading intended behavior. (3) RL can converge to local sub-optima (vicious cycle); authors mitigate via iterative distillation but it remains a training risk. (4) Need for careful reward shaping: reflection bonuses can penalize unnecessary revisions but must be tuned. (5) Some synthetic/demo data and multi-agent synthesis quality affect outcomes. The paper cites broader literature noting that LLM self-correction is sometimes difficult (e.g., Kamoi et al., 2024; Huang et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against classical Chain-of-Thought (CoT) format and large-scale format tuning: COAT+RAE outperforms CoT-trained LLMs (ablations in Table 3), and small-scale FT + large-scale RL outperforms large-scale FT alone (Table 5). The approach is contrasted with two-player frameworks that use external verifiers/reward models; Satori internalizes search within a single LLM. Distillation experiments show that Satori's RL-trained behaviors can be distilled to weaker models more effectively than FT-only data.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablations reported: (1) COAT vs CoT (remove reflect/explore) – COAT yields higher performance (Table 3). (2) Format-tuning scale vs RL: small FT (10k) + RL outperforms large FT (300k) without RL (Table 5). (3) Self-correction ablation: FT-only model has low positive self-correction rates while RL-trained model has much higher positive self-correction (Table 4). Additional ablations on reflection bonus, RAE, and preference bonus are referenced in Appendix E (summarized as supporting importance of these components).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8936.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8936.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COAT+RAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Action-Thought (COAT) reasoning with Restart-and-Explore (RAE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training and reasoning-format methodology that (1) adds meta-action tokens to enable explicit continue/reflect/explore actions during autoregressive decoding and (2) trains with RL using backtracked restart states plus reflection/prefence bonuses to encourage correction and exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used to train Satori (Qwen-2.5-Math-7B base → Satori-Qwen-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied to a 7B LLM base (Qwen-2.5-Math-7B) during post-training: a small format tuning stage (≈10k COAT demonstration trajectories) followed by large-scale PPO RL with RAE and iterative distillation rounds to produce Satori-Qwen-7B.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Chain-of-Action-Thought (COAT) with Restart-and-Explore (RAE)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>COAT: include meta-action tokens (<|continue|>, <|reflect|>, <|explore|>) in generated trajectories so the model can choose reflection or exploration steps; RAE: during RL, sample multiple trajectories, randomly backtrack t ≤ T steps to create intermediate states (both from correct and incorrect trajectories), append <|reflect|> to trigger refinement and store them in positive/negative restart buffers to sample as initial states; reward design includes correctness reward, reflection bonuses (β) to encourage productive corrections on negative restarts and discourage harmful revisions on positive restarts, and a learned Outcome Reward Model (Bradley–Terry) as a fine-grained preference signal.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Math problem-solving and general reasoning benchmarks (same as Satori entry)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks to evaluate stepwise reasoning and benefits of self-reflection: GSM8K, MATH500, OlympiadBench, AMC2023, AIME2024 and various out-of-domain reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>When COAT+RAE is used to train Satori, reported performance (see Satori-Qwen-7B) improves across math and out-of-domain tasks (e.g., MATH500 85.6% vs 83.6% for instruct baseline; F→T self-correction 61.0% vs 20.6% for FT-only). See Satori entry for full numeric breakdown.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Ablated 'CoT-only' variant (remove reflect/explore meta-actions but keep other training settings) yields lower or comparable performance (e.g., Qwen-7B-CoT: GSM8K 93.1%, MATH500 84.4%, Olympiad 42.7%) and fails to match Satori that uses COAT+RAE. Format-tuning-only (no RL) also performs worse (Satori-Qwen-7B-FT 300K: MATH500 78.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Explicit token-level meta-actions integrated into autoregressive output; RAE creates intermediate reflect-invoking contexts by backtracking and appending reflect tokens; RL (PPO) optimizes the policy over these augmented initial-state distributions; reward shaping uses correctness + reflection bonuses + a learned preference reward (ORM).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Empirical ablations: COAT vs CoT (Table 3) shows COAT advantage; format-tune-only vs COAT+RL (Table 5) shows RL+COAT generalizes better and obtains higher accuracy; self-correction analysis (Table 4) shows large increase in positive self-corrections (F→T) after RL+RAE relative to FT-only; Figures 3–4 show RL increases reasoning length and accuracy (test-time scaling).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires initial format-tuning so the model knows meta-action semantics; hyperparameters (reflection bonus β, backtrack horizon T, ORM quality) are important; risk of RL converging to shortcut policies that avoid reflection early in training; reflection can flip correct answers to incorrect ones (nontrivial T→F rates), so reward shaping must balance incentives; relies on synthetic demonstration quality for FT warm-up.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Contrasted with two-player frameworks that use external verifiers or critic models at test-time (which are expensive to deploy): COAT+RAE internalizes search into a single LLM. Compared with prompting-based self-correction and multi-model critique systems (cited in related work), COAT+RAE achieves self-improvement via RL and restart buffers rather than only prompt-engineered retries or external verifiers. Distillation experiments show this RL-internalized capability can be transferred to weaker models.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Reported ablations include: removal of 'reflect' and 'explore' (reduces to CoT) which lowers performance (Table 3); removing RL (FT-only scaling) underperforms COAT+RL (Table 5); ablations on reflection bonus, RAE, and ORM reported in Appendix E show each component contributes positively to final performance (exact numeric values cited in appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>STaR: Bootstrapping reasoning with reasoning <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 1)</em></li>
                <li>Training language models to self-correct via reinforcement learning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8936",
    "paper_id": "paper-8cdb7845b94fada3be213e2abcafa7da6625879f",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "Satori-Qwen-7B",
            "name_full": "Satori-Qwen-7B (Satori: 7B LLM trained with COAT + RAE)",
            "brief_description": "A 7B single LLM (based on Qwen-2.5-Math-7B) trained to internalize autoregressive search via Chain-of-Action-Thought (COAT) and large-scale reinforcement learning with Restart-and-Explore (RAE), enabling self-reflection and iterative self-improvement without external verifiers.",
            "citation_title": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search",
            "mention_or_use": "use",
            "model_name": "Satori-Qwen-7B (based on Qwen-2.5-Math-7B)",
            "model_description": "7B-parameter LLM built from the Qwen-2.5-Math-7B base; format-tuned on ~10k COAT demonstration trajectories and then trained with large-scale PPO reinforcement learning (RAE), plus iterative distillation (SFT) rounds. Trained on public math instruction datasets (OpenMathInstruct-2, NuminaMathCoT) and synthetic demonstration data from a multi-agent synthesis pipeline.",
            "reflection_method_name": "Chain-of-Action-Thought (COAT) + Restart-and-Explore (RAE)",
            "reflection_method_description": "COAT augments generation with meta-action tokens (&lt;|continue|&gt;, &lt;|reflect|&gt;, &lt;|explore|&gt;) so the model can choose to continue, pause and verify, or explore alternatives; RAE augments RL by restarting rollouts from intermediate (backtracked) states (both correct and incorrect trajectories), appending a &lt;|reflect|&gt; token to trigger refinement, and using reward shaping (rule-based reward, reflection bonus, and a learned Outcome Reward Model) to encourage successful correction.",
            "task_name": "Mathematical reasoning benchmarks (GSM8K, MATH500, OlympiadBench, AMC2023, AIME2024) and additional out-of-domain reasoning (FOLIO, BGQA, CRUXEval, StrategyQA, TableBench, MMLUPro-STEM)",
            "task_description": "Standard math problem-solving and diverse reasoning benchmarks used to evaluate problem solving and general reasoning; main metric is zero-shot pass@1 accuracy using greedy decoding.",
            "performance_with_reflection": "On math benchmarks Satori-Qwen-7B (COAT + RL + RAE) achieves: GSM8K 93.2%, MATH500 85.6%, OlympiadBench 46.6%, AMC2023 67.5%, AIME2024 20.0% (Avg. reported 62.6 for small-scale models); after a second round of iterative self-improvement Satori-Qwen-7B (Round 2) improves to GSM8K 93.9%, OlympiadBench 48.5%, AMC2023 72.5%, AIME2024 23.3% (Avg. 64.4). On out-of-domain benchmarks Satori shows gains (e.g., BGQA 61.8%, CRUXEval 42.5%, MMLUPro-STEM 56.7%, Avg. 60.4; Round 2 Avg. 60.8).",
            "performance_without_reflection": "Comparative baselines: Qwen-2.5-Math-7B-Instruct (same base w/o COAT+RAE) reports GSM8K 95.2%, MATH500 83.6%, OlympiadBench 41.6%, AMC2023 62.5%, AIME2024 16.7% (Avg. 59.9). A CoT-trained variant (Qwen-7B-CoT; ablated COAT actions) scored lower than Satori on several benchmarks (e.g., GSM8K 93.1%, MATH500 84.4%, Olympiad 42.7%). Format-tuned only (Satori-Qwen-7B-FT, larger FT data) performs worse than COAT+RL: e.g., Satori-Qwen-7B-FT (300K FT data) GSM8K 92.3%, MATH500 78.2%.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Implemented via special meta-action tokens (&lt;|reflect|&gt;, &lt;|explore|&gt;, &lt;|continue|&gt;) embedded in the tokenization/format; training uses format-tuning on demonstration COAT trajectories, then PPO RL where initial states include backtracked partial trajectories with appended &lt;|reflect|&gt; tokens (RAE); reward shaping includes rule-based correctness reward, reflection bonuses (positive for correcting from incorrect restarts, negative for destroying correct restarts), and a learned Outcome Reward Model (Bradley–Terry preference model) to provide dense preference signals.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative: (1) Benchmark gains: Satori (COAT+RL) outperforms same-base instruct and CoT-trained variants across math and many out-of-domain tasks (Tables 1,2,3,5). (2) Self-correction analysis (Table 4): format-tuned only Satori-Qwen-7B-FT shows low positive self-correction (F→T: 20.6% on MATH500), whereas Satori-Qwen-7B (with RL) shows high positive self-correction (F→T: 61.0% on MATH500), indicating RL substantially improves the model's ability to correct incorrect answers via reflection. (3) Test-time scaling: Figures 3–4 show RL training increases response length (more tokens spent reasoning) and test-time accuracy, allocating more compute to harder problems.",
            "limitations_or_failure_cases": "Reported and observed limitations include: (1) Nonzero rate of negative self-corrections (T→F): e.g., Satori still flips correct answers to incorrect ones (T→F ~39.0% on MATH500), so reflection is not universally beneficial. (2) Early RL can find shortcut policies that reduce response length and avoid reflection, temporarily degrading intended behavior. (3) RL can converge to local sub-optima (vicious cycle); authors mitigate via iterative distillation but it remains a training risk. (4) Need for careful reward shaping: reflection bonuses can penalize unnecessary revisions but must be tuned. (5) Some synthetic/demo data and multi-agent synthesis quality affect outcomes. The paper cites broader literature noting that LLM self-correction is sometimes difficult (e.g., Kamoi et al., 2024; Huang et al.).",
            "comparison_to_other_methods": "Compared against classical Chain-of-Thought (CoT) format and large-scale format tuning: COAT+RAE outperforms CoT-trained LLMs (ablations in Table 3), and small-scale FT + large-scale RL outperforms large-scale FT alone (Table 5). The approach is contrasted with two-player frameworks that use external verifiers/reward models; Satori internalizes search within a single LLM. Distillation experiments show that Satori's RL-trained behaviors can be distilled to weaker models more effectively than FT-only data.",
            "ablation_study_results": "Ablations reported: (1) COAT vs CoT (remove reflect/explore) – COAT yields higher performance (Table 3). (2) Format-tuning scale vs RL: small FT (10k) + RL outperforms large FT (300k) without RL (Table 5). (3) Self-correction ablation: FT-only model has low positive self-correction rates while RL-trained model has much higher positive self-correction (Table 4). Additional ablations on reflection bonus, RAE, and preference bonus are referenced in Appendix E (summarized as supporting importance of these components).",
            "uuid": "e8936.0",
            "source_info": {
                "paper_title": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "COAT+RAE",
            "name_full": "Chain-of-Action-Thought (COAT) reasoning with Restart-and-Explore (RAE)",
            "brief_description": "A training and reasoning-format methodology that (1) adds meta-action tokens to enable explicit continue/reflect/explore actions during autoregressive decoding and (2) trains with RL using backtracked restart states plus reflection/prefence bonuses to encourage correction and exploration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Used to train Satori (Qwen-2.5-Math-7B base → Satori-Qwen-7B)",
            "model_description": "Applied to a 7B LLM base (Qwen-2.5-Math-7B) during post-training: a small format tuning stage (≈10k COAT demonstration trajectories) followed by large-scale PPO RL with RAE and iterative distillation rounds to produce Satori-Qwen-7B.",
            "reflection_method_name": "Chain-of-Action-Thought (COAT) with Restart-and-Explore (RAE)",
            "reflection_method_description": "COAT: include meta-action tokens (&lt;|continue|&gt;, &lt;|reflect|&gt;, &lt;|explore|&gt;) in generated trajectories so the model can choose reflection or exploration steps; RAE: during RL, sample multiple trajectories, randomly backtrack t ≤ T steps to create intermediate states (both from correct and incorrect trajectories), append &lt;|reflect|&gt; to trigger refinement and store them in positive/negative restart buffers to sample as initial states; reward design includes correctness reward, reflection bonuses (β) to encourage productive corrections on negative restarts and discourage harmful revisions on positive restarts, and a learned Outcome Reward Model (Bradley–Terry) as a fine-grained preference signal.",
            "task_name": "Math problem-solving and general reasoning benchmarks (same as Satori entry)",
            "task_description": "Benchmarks to evaluate stepwise reasoning and benefits of self-reflection: GSM8K, MATH500, OlympiadBench, AMC2023, AIME2024 and various out-of-domain reasoning tasks.",
            "performance_with_reflection": "When COAT+RAE is used to train Satori, reported performance (see Satori-Qwen-7B) improves across math and out-of-domain tasks (e.g., MATH500 85.6% vs 83.6% for instruct baseline; F→T self-correction 61.0% vs 20.6% for FT-only). See Satori entry for full numeric breakdown.",
            "performance_without_reflection": "Ablated 'CoT-only' variant (remove reflect/explore meta-actions but keep other training settings) yields lower or comparable performance (e.g., Qwen-7B-CoT: GSM8K 93.1%, MATH500 84.4%, Olympiad 42.7%) and fails to match Satori that uses COAT+RAE. Format-tuning-only (no RL) also performs worse (Satori-Qwen-7B-FT 300K: MATH500 78.2%).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Explicit token-level meta-actions integrated into autoregressive output; RAE creates intermediate reflect-invoking contexts by backtracking and appending reflect tokens; RL (PPO) optimizes the policy over these augmented initial-state distributions; reward shaping uses correctness + reflection bonuses + a learned preference reward (ORM).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Empirical ablations: COAT vs CoT (Table 3) shows COAT advantage; format-tune-only vs COAT+RL (Table 5) shows RL+COAT generalizes better and obtains higher accuracy; self-correction analysis (Table 4) shows large increase in positive self-corrections (F→T) after RL+RAE relative to FT-only; Figures 3–4 show RL increases reasoning length and accuracy (test-time scaling).",
            "limitations_or_failure_cases": "Requires initial format-tuning so the model knows meta-action semantics; hyperparameters (reflection bonus β, backtrack horizon T, ORM quality) are important; risk of RL converging to shortcut policies that avoid reflection early in training; reflection can flip correct answers to incorrect ones (nontrivial T→F rates), so reward shaping must balance incentives; relies on synthetic demonstration quality for FT warm-up.",
            "comparison_to_other_methods": "Contrasted with two-player frameworks that use external verifiers or critic models at test-time (which are expensive to deploy): COAT+RAE internalizes search into a single LLM. Compared with prompting-based self-correction and multi-model critique systems (cited in related work), COAT+RAE achieves self-improvement via RL and restart buffers rather than only prompt-engineered retries or external verifiers. Distillation experiments show this RL-internalized capability can be transferred to weaker models.",
            "ablation_study_results": "Reported ablations include: removal of 'reflect' and 'explore' (reduces to CoT) which lowers performance (Table 3); removing RL (FT-only scaling) underperforms COAT+RL (Table 5); ablations on reflection bonus, RAE, and ORM reported in Appendix E show each component contributes positively to final performance (exact numeric values cited in appendix).",
            "uuid": "e8936.1",
            "source_info": {
                "paper_title": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "STaR: Bootstrapping reasoning with reasoning",
            "rating": 2
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 1
        },
        {
            "paper_title": "Training language models to self-correct via reinforcement learning",
            "rating": 2
        }
    ],
    "cost": 0.0210045,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search</h1>
<p>Maohao Shen ${ }^{+1}$ Guangtao Zeng ${ }^{+2}$ Zhenting Qi ${ }^{+3}$<br>Zhang-Wei Hong ${ }^{1}$ Zhenfang Chen ${ }^{4}$ Wei Lu ${ }^{2}$ Gregory Wornell ${ }^{1}$<br>Subhro Das ${ }^{4}$ David Cox ${ }^{4}$ Chuang Gan ${ }^{45}$</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a twostage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models are fully open-sourced ${ }^{1}$.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>1. Introduction</h2>
<p>Large language models (LLMs) have demonstrated remarkable performance across a wide range of reasoning tasks, including mathematical problems (Cobbe et al., 2021; Hendrycks et al., 2021a), programming (Chen et al., 2021; Zhuo et al., 2024) and logical reasoning (Han et al., 2024; Liu et al., 2020). One of the key techniques enabling these strong reasoning capabilities is Chain-of-Thought (CoT) prompting (Wei et al., 2022), which allows LLMs to address complex tasks by generating a series of intermediate reasoning steps. As a result, many early efforts focus on finetuning LLMs using large-scale, high-quality CoT reasoning chains, either through human annotation (Hendrycks et al., 2021a; Yue et al., 2024) or by distilling synthetic data from more advanced models (Yu et al., 2024; Toshniwal et al., 2024a; Ding et al., 2024). However, human annotation is extremely labor intensive, and distillation often limits the model's reasoning capabilities to certain level.</p>
<p>Apart from scaling up training resources, more recent work has focused on test-time scaling, i.e., allocating additional inference-time compute to search for more accurate solutions. This often involves extensive sampling, either by generating multiple complete solutions (Wang et al., 2023) or by sampling multiple intermediate reasoning steps (Yao et al., 2024; Wan et al., 2024). These methods typically require external feedback to guide the search process, usually through training an auxiliary reward model to rate final solutions or intermediate steps (Sun et al., 2024; Wang et al., 2024a). However, such two-player frameworks incur more model-deployment costs and do not internalize the search capabilities into a single LLM.</p>
<p>Orthogonal to the above work, our study investigates a new direction that enables LLMs with autoregressive search capabilities, i.e., an extended reasoning process with selfreflection and self-exploration of new strategies. Specifically, we introduce the Chain-of-Action-Thought (COAT) mechanism, which enables LLMs to take various metaactions during problem solving. Unlike conventional posttraining consisting of large-scale supervised fine-tuning (SFT) and reinforcement learning from human feedback</p>
<p>(RLHF), we propose a novel two-stage training paradigm: (1) a small-scale format tuning (FT) stage to internalize the COAT reasoning format and (2) a large-scale selfimprovement stage that utilizes reinforcement learning with "Restart and Explore" (RAE) techniques. Our approach leads to the development of Satori, a 7B LLM trained on open-source base models and mathematic data that achieve superior performance on both in-domain and out-of-domain tasks. To summarize, our contributions are threefold,</p>
<ol>
<li>Efficiency: Satori is a single LLM capable of autoregressive search without external guidance (Section 6 and Section A). Moreover, this is achieved with minimal supervision and large-scale self-improvement.</li>
<li>Effectiveness: Satori demonstrates superior performance on in-domain mathematical reasoning tasks and outperforms the instruct model built on the same base model (Section 5.1).</li>
<li>Generalizability: Unlike recent research on math reasoning, Satori exhibits strong transferability to out-ofdomain tasks and demonstrates universal capabilities for self-reflection and self-exploration (Section 5.2).</li>
</ol>
<h2>2. Related Work</h2>
<p>We summarize the literature that is closely aligned with the scope of this paper (refer to Section B for more discussions).</p>
<p>Concurrent Work. Building on the impact of OpenAI's o1 (OpenAI, 2024), significant efforts have been made within the research community to enhance open-source LLMs with advanced reasoning capabilities. The most common approach relies on distilling knowledge from stronger teacher models (Huang et al., 2024a; Zhao et al., 2024; Min et al., 2024). In contrast, Satori addresses this problem from a reinforcement learning (RL) perspective and requires minimal supervision (only 10 K samples in the format tuning stage). The most related concurrent work is DeepSeek's recently released R1 (Guo et al., 2025), which adopts a similar high-level strategy of small-scale cold-start SFT followed by large-scale RL training. Although both works coincide in this high-level idea, our work differs from R1 in key methodologies, including the data synthesis framework and RL algorithms. Additionally, DeepSeek-R1 focuses on training large-scale LLMs (671B), whereas our work provides insights into the development of smaller-scale LLMs (7B) for research purpose. Finally, as an industry-developed model, the technical details of DeepSeek-R1 (Guo et al., 2025) are not fully disclosed, making reproduction difficult, whereas our work is a fully transparent study that aims to open-source training data and training recipes.</p>
<p>Post-training LLMs for Reasoning. Recent advancements have focused on extensive post-training to enhance reasoning. A line of work focus on constructing high-quality instruction-tuning datasets (Hendrycks et al., 2021a; Yue et al., 2024; Yu et al., 2024; Toshniwal et al., 2024a; Ding et al., 2024), but suffers from expensive annotatoin costs. More recent research has focused on self-improvement approaches, where models are trained on data generated by themselves (Zelikman et al., 2022; 2024; Singh et al., 2024; Zhang et al., 2024a). Additionally, reinforcement learning methods, particularly those based on Proximal Policy Optimization (PPO) (Schulman et al., 2017a; Ouyang et al., 2022), have been demonstrated to be more effective, which typically leverage reward models to guide the learning process (Sun et al., 2024; Wang et al., 2024a; Yuan et al., 2024).</p>
<p>Enabling LLMs with Searching Abilities. Promptingbased approaches (Yao et al., 2024; Shinn et al., 2024; Hao et al., 2023; Qi et al., 2024a) guide LLMs to search for solutions via error correction and exploring alternative paths. However, such approaches cannot fundamentally enhance the LLM's reasoning abilities. Moreover, recent work has pointed out the difficulties of LLMs in self-correction (Zhang et al., 2024b; Kamoi et al., 2024). Recent research has pivoted toward training LLMs for selfexploration. Some focused on enabling trajectory-level search-iteratively identify errors in previous complete responses and produce improved responses (Saunders et al., 2022a; Kumar et al., 2024; Qu et al., 2024; Havrilla et al., 2024). Another line of research has explored step-level search, which enables LLMs to identify and correct mistakes in a more fine-grained manner. Some achieve this using another model to provide step-level feedback (Xi et al., 2024; Setlur et al., 2024; Zhang et al., 2024c; Guan et al., 2025; Zhang et al., 2024d), but such two-player frameworks suffer from high costs for model deployment. SoS (Gandhi et al., 2024) is another closely related work that attempts to train a single LLM to perform a tree search as a flattened string. However, the effectiveness of SoS has primarily been shown on simple symbolic tasks, and its ability to generalize to more complex problems remains to be explored.</p>
<h2>3. Preliminaries</h2>
<p>We address mathematical problem-solving by training a language model $\pi_{\theta}$ to generate a solution $\tilde{\boldsymbol{y}}$ that matches the ground truth $\boldsymbol{y}^{<em>}$, given a problem prompt $\boldsymbol{x}$. All sequences $\boldsymbol{x}, \boldsymbol{y}$, and $\boldsymbol{y}^{</em>}$ consist of tokens from a predefined dictionary. Since our approach uses reinforcement learning (RL) to train the model for solving math problems, we outline the key RL concepts below.</p>
<p>Reinforcement Learning (RL). RL (Kaelbling et al., 1996) involves an agent making sequential decisions to max-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A High-level Overview of Satori Training Framework: Format Tuning (FT) + Self-improvement. First, Satori learns COAT reasoning format through imitation learning on small-scale demonstration trajectories. Next, Satori further leverages COAT reasoning format to self-improve via large-scale reinforcement learning.
imize the expected cumulative rewards through interactions with an environment. Here, the language model $\pi_{\theta}$ acts as the agent's policy. Starting from an initial state $z_{0}$, at each step $l$, the agent observes the current state $z_{l}$, receives a reward $r_{l}$, selects an action based on $\pi_{\theta}$, transitions to the next state $z_{l+1}$, and continues until reaching a terminal state. A trajectory is the sequence of states and actions during this interaction. RL optimizes the policy to maximize the expected rewards $\sum_{l=1}^{L} r_{l}$, where $L$ is the trajectory length.</p>
<h2>4. Method</h2>
<p>We start this section by introducing the formulation of reasoning and how reasoning can be formulated as a sequential decision-making problem. Goal: We want to train LLMs to solve problems by reasoning through multiple steps rather than directly predicting the final answer. Given a problem statement $\boldsymbol{x}$, the model generates a sequence of reasoning steps $\left{\boldsymbol{y}<em 2="2">{1}, \boldsymbol{y}</em>}, \ldots, \boldsymbol{y<em L="L">{L}\right}$, where $\boldsymbol{y}</em>}$ provides the final answer. However, not all intermediate steps are help-ful-repeating errors does not improve accuracy. Effective reasoning requires verifying correctness, identifying mistakes, and considering alternative solutions. For instance, given $\boldsymbol{x}=$ " $1+1=$ ?", the model might initially output $\boldsymbol{y<em 2="2">{1}=3$, then recognize the mistake with $\boldsymbol{y}</em>=2$.}$ (e.g., "Wait, let me verify..."), before correcting it to $\boldsymbol{y}_{3</p>
<p>Chain-of-Action-Thought reasoning (COAT). The key challenge is enabling the model to determine when to reflect, continue, or explore alternatives without external intervention. To enable this, we introduce special meta-action tokens that guide the model's reasoning process beyond standard text generation. These tokens serve as hint for the model to determine when to reassess its reasoning before proceeding.</p>
<ul>
<li>Continue Reasoning ( $&lt;$ | continue | &gt;): Encourages the model to build upon its current reasoning trajectory by generating the next intermediate step.</li>
<li>Reflect ( $&lt;$ | reflect | &gt;): Prompts the model to pause and verify the correctness of prior reasoning steps.</li>
<li>Explore Alternative Solution ( $&lt;$ |explore | &gt;): Signals the model to identify critical flaws in its reasoning and explore a new solution.</li>
</ul>
<p>Each reasoning step $\boldsymbol{y}_{l}$ is a sequence of tokens, with the starting token potentially being one of the designated metaaction tokens. We refer to this formulation as Chain-of-Action-Thought reasoning (COAT). In particular, typical Chain-of-Thought reasoning (CoT) (Wei et al., 2022) can be viewed as a special case of COAT, where each reasoning step in CoT is restricted to continuation, without explicitly incorporating other types of meta-actions.</p>
<p>Learning to Reason via RL. We formulate reasoning as a sequential decision-making problem, where reasoning is a process of constructing and refining an answer step by step. Specifically, the model $\pi_{\theta}$ starts with an input context $\boldsymbol{x}$ (initial state $z_{0}$ ), generates a reasoning step $\boldsymbol{y}<em l="l">{l}$ (action), updates the context by appending $\boldsymbol{y}</em>}$ (next state $z_{l+1}=z_{l} \oplus \boldsymbol{y<em L="L">{l}$, where $\oplus$ denotes string concatenation), and repeats this process until it produces a final answer $\boldsymbol{y}</em>^{}$. The reasoning terminates when the model signals completion (e.g., omitting EOS token). The simplest reward function can be $\mathbb{I}\left{\boldsymbol{y}_{L}=\boldsymbol{y<em>}\right}$, evaluates whether the final answer $\boldsymbol{y}_{L}$ matches the ground truth $\boldsymbol{y}^{</em>}$. With this formulation, we could train the model to reason using RL, aiming to generate reasoning steps that maximize the expected reward. However, applying RL to reasoning presents two key challenges:</p>
<ol>
<li>Unawareness of meta-action tokens: The model doesn't understand the purpose of special tokens and fails to recognize that encountering special meta-action tokens may require reflection or proposing alternatives.</li>
<li>Long horizon and sparse rewards: Reasoning requires long-term decision-making with rewards only at the end, which hinders learning effectiveness (Bellemare et al., 2016). The model must take many correct reasoning steps before receiving rewards, and failures force it to restart from the initial state (i.e., the problem statement). This makes learning difficult because training data associated with rewards is scarce, yet rewards are essential for driving RL progress.</li>
</ol>
<p>Overview of Proposed Method. To address the model's initial unawareness of meta-action tokens, we introduce a warm-up "format-tuning" stage: we fine-tune a pre-trained LLM on a small dataset featuring a few demonstrated reasoning trajectories (Section 4.1). This step familiarizes the model with using and reacting to meta-action tokens. Second, to tackle the challenges of long horizons and sparse rewards, we propose a "restart and explore" (RAE) strategy, inspired by Go-explore (Ecoffet et al., 2019). Here, the model restarts from intermediate steps, including those points where previous reasoning attempts failed, allowing it to focus on correcting errors rather than starting from scratch. We also add exploration bonuses to encourage deeper reflection, further increasing opportunities for the model to arrive at correct answers (Section 4.2).</p>
<h3>4.1. Format Tuning Through Imitation Learning</h3>
<p>Training a base LLM $\pi_{\theta}$ to perform COAT reasoning presents a significant challenge: LLMs are typically not pre-trained on COAT reasoning data that incorporates trials and errors, necessitating a post-training stage to inject this capability. To address this, we introduce format tuning (FT), a method designed to train LLMs to emulate expert COAT trajectories through imitation learning. Imitation learning techniques (Hussein et al., 2017) are widely used in the robotics domain, where agents are trained using demonstration trajectories provided by human experts (Ross and Bagnell, 2010; Ross et al., 2011; Ho and Ermon, 2016). However, generating high-quality demonstration trajectories for LLMs is prohibitively expensive for complex tasks. To efficiently construct a demonstration trajectory dataset $\mathcal{D}<em i="1">{\text {syn }}=\left{\left(\boldsymbol{x}^{(i)}, \widehat{\boldsymbol{y}}^{(i)}\right)\right}</em>$, we propose a multi-agent data synthesis framework that leverages three LLMs:}^{N</p>
<ul>
<li>Generator: Given an input problem, a generator $\pi_{\theta}$ generates multiple reasoning paths for a given input problem using classical CoT techniques.</li>
<li>Critic: A critic $\pi_{c}$ evaluates the correctness of the reasoning paths generated by the generator, providing feedback to refine the reasoning and address suboptimal steps.</li>
<li>Reward Model: Additionally, a reward model $\pi_{r}$ assigns scores to the refined reasoning paths and selects the most effective path as the final demonstration trajectory.</li>
</ul>
<p>These three models collaborate to construct high-quality demonstration trajectories (details on the trajectory synthesis are provided in Appendix C). For this work, we adopt the simplest imitation learning approach, behavior cloning, which utilizes supervised fine-tuning to train the LLM policy on the expert COAT demonstration trajectories $\mathcal{D}<em _theta="\theta">{\text {syn }}$. Notably, we observe that even a small number (10K) of COAT demonstration trajectories is sufficient for $\pi</em>$ to effectively follow the COAT reasoning format.</p>
<h3>4.2. Self-improvement via Reinforcement Learning</h3>
<p>After format tuning, the LLM policy $\pi_{\theta}$ adopts the COAT reasoning style but struggles to generalize, particularly in using meta-actions for self-reflection. This limitation arises from the scarcity of demonstrations during format tuning. While collecting more demonstrations could help, it is costly and time-consuming. Instead, we explore whether the model can self-improve its reasoning via RL.</p>
<p>We start with the format-tuned LLM and train it using PPO (Schulman et al., 2017b) algorithm, a widely used RL method. In addition to training on problems $\boldsymbol{x}$ from the dataset $\mathcal{D}$, we also train the model $\pi_{\theta}$ to begin reasoning from partial trajectories generated by the format-tuned LLM. Since reasoning errors typically arise from minor mistakes rather than fundamental flaws, re-exploring from the start is inefficient. Instead, we allow the model to restart from intermediate steps to correct errors and finally achieve correct answers. Inspired by Go-Explore (Ecoffet et al., 2019), we introduce the Restart and Explore (RAE) strategy.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">Restart</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">Explore</span><span class="w"> </span><span class="o">(</span><span class="n">RAE</span><span class="o">)</span>
<span class="n">input</span><span class="w"> </span><span class="n">Dataset</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">D</span><span class="o">}=</span><span class="err">\</span><span class="n">left</span><span class="err">\</span><span class="o">{</span><span class="err">\</span><span class="n">left</span><span class="o">(</span><span class="err">\</span><span class="n">boldsymbol</span><span class="o">{</span><span class="n">x</span><span class="o">}^{(</span><span class="n">i</span><span class="o">)},</span><span class="w"> </span><span class="err">\</span><span class="n">boldsymbol</span><span class="o">{</span><span class="n">y</span><span class="o">}^{</span><span class="err">\</span><span class="n">star</span><span class="o">(</span><span class="n">i</span><span class="o">)}</span><span class="err">\</span><span class="n">right</span><span class="o">)</span><span class="err">\</span><span class="n">right</span><span class="err">\</span><span class="o">}_{</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="o">}^{</span><span class="n">n</span><span class="o">}</span><span class="w"> </span><span class="o">;</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="n">LLM</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="n">pi_</span><span class="o">{</span><span class="err">\</span><span class="n">theta</span><span class="o">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="n">after</span>
<span class="n">format</span><span class="w"> </span><span class="n">tuning</span><span class="o">;</span><span class="w"> </span><span class="n">maximum</span><span class="w"> </span><span class="n">back</span><span class="o">-</span><span class="n">track</span><span class="w"> </span><span class="n">steps</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="n">T</span><span class="err">\</span><span class="o">)</span>
<span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="n">Initialize</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">D</span><span class="o">}_{</span><span class="err">\</span><span class="n">text</span><span class="w"> </span><span class="o">{</span><span class="n">restart</span><span class="w"> </span><span class="o">}}^{+}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">emptyset</span><span class="err">\</span><span class="o">);</span><span class="w"> </span><span class="n">Initialize</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">D</span><span class="o">}_{</span><span class="err">\</span><span class="n">text</span><span class="w"> </span><span class="o">{</span><span class="n">restart</span><span class="w"> </span><span class="o">}}^{-}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">emptyset</span><span class="err">\</span><span class="o">)</span>
<span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="o">,</span><span class="mi">2</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="n">ldots</span><span class="o">,</span><span class="w"> </span><span class="n">n</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="k">do</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="n">Given</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">problem</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="n">boldsymbol</span><span class="o">{</span><span class="n">x</span><span class="o">}^{(</span><span class="n">i</span><span class="o">)}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">,</span><span class="w"> </span><span class="n">sample</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="n">pi_</span><span class="o">{</span><span class="err">\</span><span class="n">theta</span><span class="o">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">collect</span><span class="w"> </span><span class="n">multiple</span>
<span class="w">    </span><span class="n">initial</span><span class="w"> </span><span class="n">trajectories</span><span class="o">.</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="n">Randomly</span><span class="w"> </span><span class="k">select</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">correct</span><span class="w"> </span><span class="n">trajectory</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="n">widehat</span><span class="o">{</span><span class="err">\</span><span class="n">boldsymbol</span><span class="o">{</span><span class="n">y</span><span class="o">}}^{+}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">incorrect</span>
<span class="w">    </span><span class="n">trajectory</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="n">widehat</span><span class="o">{</span><span class="err">\</span><span class="n">boldsymbol</span><span class="o">{</span><span class="n">y</span><span class="o">}}^{-}</span><span class="err">\</span><span class="o">).</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="n">Randomly</span><span class="w"> </span><span class="n">backtrack</span><span class="w"> </span><span class="n">last</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="n">t</span><span class="w"> </span><span class="err">\</span><span class="n">leq</span><span class="w"> </span><span class="n">T</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="n">actions</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="n">widehat</span><span class="o">{</span><span class="err">\</span><span class="n">boldsymbol</span><span class="o">{</span><span class="n">y</span><span class="o">}}^{+}</span><span class="err">\</span><span class="o">)</span><span class="ow">and</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="n">widehat</span><span class="o">{</span><span class="err">\</span><span class="n">boldsymbol</span><span class="o">{</span><span class="n">y</span><span class="o">}}^{-}</span><span class="err">\</span><span class="o">).</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="n">Obtain</span><span class="w"> </span><span class="n">intermediate</span><span class="w"> </span><span class="n">states</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">time</span><span class="o">-</span><span class="n">step</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="n">L</span><span class="o">-</span><span class="n">t</span><span class="o">,</span><span class="w"> </span><span class="n">z_</span><span class="o">{</span><span class="n">L</span><span class="o">-</span><span class="n">t</span><span class="o">}^{+}=</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="n">left</span><span class="o">[</span><span class="err">\</span><span class="n">boldsymbol</span><span class="o">{</span><span class="n">x</span><span class="o">}^{(</span><span class="n">i</span><span class="o">)},</span><span class="w"> </span><span class="err">\</span><span class="n">widehat</span><span class="o">{</span><span class="err">\</span><span class="n">boldsymbol</span><span class="o">{</span><span class="n">y</span><span class="o">}}_{</span><span class="mi">1</span><span class="o">}^{+},</span><span class="w"> </span><span class="err">\</span><span class="n">widehat</span><span class="o">{</span><span class="err">\</span><span class="n">boldsymbol</span><span class="o">{</span><span class="n">y</span><span class="o">}}_{</span><span class="mi">2</span><span class="o">}^{+},</span><span class="w"> </span><span class="err">\</span><span class="n">ldots</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="n">widehat</span><span class="o">{</span><span class="err">\</span><span class="n">boldsymbol</span><span class="o">{</span><span class="n">y</span><span class="o">}}_{</span><span class="n">L</span><span class="o">-</span><span class="n">t</span><span class="o">}^{+}</span><span class="err">\</span><span class="n">right</span><span class="o">]</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="n">z_</span><span class="o">{</span><span class="n">L</span><span class="o">-</span><span class="n">t</span><span class="o">}^{+}=</span><span class="err">\</span><span class="n">left</span><span class="o">[</span><span class="err">\</span><span class="n">boldsymbol</span><span class="o">{</span><span class="n">x</span><span class="o">}^{(</span><span class="n">i</span><span class="o">)},</span><span class="w"> </span><span class="err">\</span><span class="n">widehat</span><span class="o">{</span><span class="err">\</span><span class="n">boldsymbol</span><span class="o">{</span><span class="n">y</span><span class="o">}}_{</span><span class="mi">1</span><span class="o">}^{-},</span><span class="w"> </span><span class="err">\</span><span class="n">widehat</span><span class="o">{</span><span class="err">\</span><span class="n">boldsymbol</span><span class="o">{</span><span class="n">y</span><span class="o">}}_{</span><span class="mi">2</span><span class="o">}^{-},</span><span class="w"> </span><span class="err">\</span><span class="n">ldots</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="n">widehat</span><span class="o">{</span><span class="err">\</span><span class="n">boldsymbol</span><span class="o">{</span><span class="n">y</span><span class="o">}}_{</span><span class="n">L</span><span class="o">-</span><span class="n">t</span><span class="o">}^{-}</span><span class="err">\</span><span class="n">right</span><span class="o">]</span><span class="w"> </span><span class="o">.</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="n">Add</span><span class="w"> </span><span class="s">&quot;reflect&quot;</span><span class="w"> </span><span class="n">special</span><span class="w"> </span><span class="n">token</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">trigger</span><span class="w"> </span><span class="n">self</span><span class="o">-</span><span class="n">reflection</span><span class="w"> </span><span class="n">action</span><span class="o">,</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="n">z_</span><span class="o">{</span><span class="n">L</span><span class="o">-</span><span class="n">t</span><span class="o">}^{+}=</span><span class="err">\</span><span class="n">left</span><span class="o">[</span><span class="err">\</span><span class="n">boldsymbol</span><span class="o">{</span><span class="n">x</span><span class="o">}^{(</span><span class="n">i</span><span class="o">)},</span><span class="w"> </span><span class="err">\</span><span class="n">widehat</span><span class="o">{</span><span class="err">\</span><span class="n">boldsymbol</span><span class="o">{</span><span class="n">y</span><span class="o">}}_{</span><span class="mi">1</span><span class="o">}^{+},</span><span class="w"> </span><span class="err">\</span><span class="n">widehat</span><span class="o">{</span><span class="err">\</span><span class="n">boldsymbol</span><span class="o">{</span><span class="n">y</span><span class="o">}}_{</span><span class="mi">2</span><span class="o">}^{+},</span><span class="w"> </span><span class="err">\</span><span class="n">ldots</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="n">widehat</span><span class="o">{</span><span class="err">\</span><span class="n">boldsymbol</span><span class="o">{</span><span class="n">y</span><span class="o">}}_{</span><span class="n">L</span><span class="o">-</span><span class="n">t</span><span class="o">}^{+},</span><span class="err">\</span><span class="n">langle</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="o">{</span><span class="n">reflect</span><span class="o">}|&gt;</span><span class="err">\</span><span class="n">right</span><span class="o">]</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="n">z_</span><span class="o">{</span><span class="n">L</span><span class="o">-</span><span class="n">t</span><span class="o">}^{-}=</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="n">left</span><span class="o">[</span><span class="err">\</span><span class="n">boldsymbol</span><span class="o">{</span><span class="n">x</span><span class="o">}^{(</span><span class="n">i</span><span class="o">)},</span><span class="w"> </span><span class="err">\</span><span class="n">widehat</span><span class="o">{</span><span class="err">\</span><span class="n">boldsymbol</span><span class="o">{</span><span class="n">y</span><span class="o">}}_{</span><span class="mi">1</span><span class="o">}^{-},</span><span class="w"> </span><span class="err">\</span><span class="n">widehat</span><span class="o">{</span><span class="err">\</span><span class="n">boldsymbol</span><span class="o">{</span><span class="n">y</span><span class="o">}}_{</span><span class="mi">2</span><span class="o">}^{-},</span><span class="w"> </span><span class="err">\</span><span class="n">ldots</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="n">widehat</span><span class="o">{</span><span class="err">\</span><span class="n">boldsymbol</span><span class="o">{</span><span class="n">y</span><span class="o">}}_{</span><span class="n">L</span><span class="o">-</span><span class="n">t</span><span class="o">}^{+},</span><span class="err">\</span><span class="n">langle</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="o">{</span><span class="n">reflect</span><span class="o">}|&gt;</span><span class="err">\</span><span class="n">right</span><span class="o">]</span><span class="w"> </span><span class="o">.</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="n">Update</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">D</span><span class="o">}_{</span><span class="err">\</span><span class="n">text</span><span class="w"> </span><span class="o">{</span><span class="n">restart</span><span class="w"> </span><span class="o">}}^{+}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">D</span><span class="o">}_{</span><span class="err">\</span><span class="n">text</span><span class="w"> </span><span class="o">{</span><span class="n">restart</span><span class="w"> </span><span class="o">}}^{+}</span><span class="w"> </span><span class="err">\</span><span class="n">cup</span><span class="w"> </span><span class="n">z_</span><span class="o">{</span><span class="n">L</span><span class="o">-</span><span class="n">t</span><span class="o">}^{+}</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">D</span><span class="o">}_{</span><span class="err">\</span><span class="n">text</span><span class="w"> </span><span class="o">{</span><span class="n">restart</span><span class="w"> </span><span class="o">}}^{-}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">D</span><span class="o">}_{</span><span class="err">\</span><span class="n">text</span><span class="w"> </span><span class="o">{</span><span class="n">restart</span><span class="w"> </span><span class="o">}}^{-}</span><span class="w"> </span><span class="err">\</span><span class="n">cup</span><span class="w"> </span><span class="n">z_</span><span class="o">{</span><span class="n">L</span><span class="o">-</span><span class="n">t</span><span class="o">}^{-}</span><span class="err">\</span><span class="o">).</span>
<span class="k">end</span>
<span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">D</span><span class="o">}_{</span><span class="err">\</span><span class="n">text</span><span class="w"> </span><span class="o">{</span><span class="n">restart</span><span class="w"> </span><span class="o">}}=</span><span class="err">\</span><span class="n">left</span><span class="err">\</span><span class="o">{</span><span class="err">\</span><span class="n">boldsymbol</span><span class="o">{</span><span class="n">x</span><span class="o">}^{(</span><span class="n">i</span><span class="o">)}</span><span class="err">\</span><span class="n">right</span><span class="err">\</span><span class="o">}_{</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="o">}^{</span><span class="n">n</span><span class="o">}</span><span class="w"> </span><span class="err">\</span><span class="n">cup</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">D</span><span class="o">}_{</span><span class="err">\</span><span class="n">text</span><span class="w"> </span><span class="o">{</span><span class="n">restart</span><span class="w"> </span><span class="o">}}^{+}</span><span class="w"> </span><span class="err">\</span><span class="n">cup</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">D</span><span class="o">}_{</span><span class="err">\</span><span class="n">text</span><span class="w"> </span><span class="o">{</span><span class="n">restart</span><span class="w"> </span><span class="o">}}^{-}</span><span class="err">\</span><span class="o">).</span>
</code></pre></div>

<p>output Augmented initial states dataset $\mathcal{D}_{\text {restart }}$.</p>
<p>Initial States. RAE trains the model to reason not only from the problem statement but also from intermediate steps sampled from past trajectories, both correct and incorrect. This enables deeper exploration without redundant recomputation. As detailed in Algorithm 1, given an input problem $x \in \mathcal{D}$, the format-tuned LLM first generates multiple reasoning trajectories. We then randomly backtrack $T \geq 0$ steps and append a reflect token &lt;| reflect | &gt; to prompt the model to refine its reasoning. To encourage diverse exploration, correct and incorrect trajectories are stored separately in restart buffers ( $\mathcal{D}<em _restart="{restart" _text="\text">{\text {restart }}^{+}$and $\mathcal{D}</em>$). RL training then optimizes reasoning across these buffers along with the}}^{-</p>
<p>original problem dataset, sampling initial states from the merged dataset $\mathcal{D}_{\text {restart }}$.</p>
<p>Intuitively, RAE modifies the initial state distribution by starting new rollouts not only from dataset-sampled prompts but also from random partial trajectories, which encourages a more diverse initial state distribution. This technique is aligned with the insight offered in [Kakade and Langford, 2002], which argues that more uniform state coverage leads to tighter bounds of learned policy. We leave a more rigorous theoretical justification for future work.</p>
<p>Reward Design. RAE gives the model multiple opportunities to refine its reasoning, but effective reflection is key to making use of these chances. In addition to using correctness as rewards, we introduce the following bonuses rewards as hints to guide the model to reach correct answers:</p>
<ul>
<li>Rule-based Reward: Rule-based reward simply evaluates the correctness of the final answer.</li>
</ul>
<p>$$
r_{\text {rule }}\left(\widehat{\boldsymbol{y}}<em _widehat_boldsymbol_y="\widehat{\boldsymbol{y">{L}, \boldsymbol{y}^{\star}\right)=\mathbf{1}</em>-1 \in{-1,0}
$$}}_{L}=\boldsymbol{y}^{\star}</p>
<ul>
<li>Reflection Bonuses: To reinforce self-reflection, we introduce a reflection bonus $r_{\text {bonus }}$. If the model starts from an incorrect reasoning trajectory stored in the negative restart buffer ( $\mathcal{D}<em _restart="{restart" _text="\text">{\text {restart }}^{-}$) and successfully solves the problem, it obtains a reward bonus, encouraging it to correct past mistakes. Conversely, if it starts from a correct trajectory in the positive restart buffer ( $\mathcal{D}</em>$) but fails to solve the problem, it incurs a penalty, discouraging unnecessary revisions when it was already on the right track. Formally, the reflection bonus is defined as:}}^{+</li>
</ul>
<p>$$
r_{\text {bonus }}(z, \widehat{\boldsymbol{y}})= \begin{cases}\beta &amp; \text { if } z \in \mathcal{D}<em L="L">{\text {restart }}^{-} \text {and } \widehat{\boldsymbol{y}}</em>}=\boldsymbol{y}^{\star} \ -\beta &amp; \text { if } z \in \mathcal{D<em L="L">{\text {restart }}^{+} \text {and } \widehat{\boldsymbol{y}}</em>
$$} \neq \boldsymbol{y}^{\star} \ 0 &amp; \text { otherwise }\end{cases</p>
<p>where $\beta$ is a bonus scale hyperparameter.</p>
<ul>
<li>Preference Bonuses: Since correct answers are rare at initial training stage, reward signals are often too sparse for effective RL training. Even with reflection, the model may fail to generate any correct reasoning trajectories, resulting in a sparse reward problem. To mitigate this, we train an Outcome Reward Model (ORM) using a BradleyTerry (BT) preference framework. The ORM rates reasoning trajectories, assigning higher values to correct (preferred) ones. For each problem $\boldsymbol{x} \in \mathcal{D}$, we generate multiple trajectories using $\pi_{\theta}$ and construct a preference dataset by pairing correct and incorrect outputs. A BT model is trained to maximize the score gap between these pairs. The ORM's output, $\sigma\left(r_{\psi}(z, \widehat{\boldsymbol{y}})\right) \in[0,1]$, serves as a fine-grained reward signal, helping the model further refine its reasoning. See Appendix D. 3 for details.
<img alt="img-1.jpeg" src="img-1.jpeg" /></li>
</ul>
<p>Figure 2: Number of Training Samples of Satori-Qwen-7B and Qwen-2.5-Math-7B-Instruct. Satori-Qwen-7B requires significantly less supervision (small-scale FT) and relies more on self-improvement (large-scale RL).</p>
<p>For an initial state $z \in \mathcal{D}_{\text {restart }}$ and a sampled trajectory $\widehat{\boldsymbol{y}}$, the overall reward function $r(z, \widehat{\boldsymbol{y}})$ is defined as:</p>
<p>$$
r(z, \widehat{\boldsymbol{y}})=r_{\text {rule }}\left(\widehat{\boldsymbol{y}}<em _psi="\psi">{L}, \boldsymbol{y}^{\star}\right)+\sigma\left(r</em>)
$$}(z, \widehat{\boldsymbol{y}})\right)+r_{\text {bonus }}(z, \widehat{\boldsymbol{y}</p>
<p>Iterative Self-improvement. RL enables a policy to selfimprove from self-generated trajectories, but it can also lead to a vicious cycle, where the policy converges to a local sub-optimum and cannot further improve. Inspired by [Agarwal et al., 2022; Schmitt et al., 2018], we propose an iterative self-improvement strategy to mitigate this issue. Specifically, after each round of RL training, we distill the knowledge of the current well-optimized policy into the base model through supervised fine-tuning (SFT). Starting from the newly fine-tuned model, we then perform another round of RL training. Intuitively, from an optimization perspective, each round of distillation can be viewed as a parameter reset mechanism that helps the policy escape local optima in the loss landscape, allowing it to continue self-improving (more details are included in Section D.3). In the next section, we provide empirical evidence to validate this approach.</p>
<h2>5. Experiment</h2>
<p>Implementation Details. We employ Qwen-2.5-Math-7B as the base model due to its strong mathematical capabilities. Our training data is sourced from the publicly available math instruction datasets, OpenMathInstruct-2 and NuminaMathCoT. For the multi-agent data synthesis framework, the generator is required to generate high-quality, step-by-step reasoning trajectories. Therefore, we use Qwen-2.5-MathInstruct as the generator. Meanwhile, the critic must have robust instruction-following capabilities, so we choose Llama-3.1-70B-Instruct as the critic. To ensure data quality, we filter out problems with invalid questions or incorrect labels, resulting in approximately 550 k samples. Additional implementation details can be found in Appendix D.</p>
<p>Table 1: Results on Mathematic Benchmarks. Satori-Qwen-7B achieves SOTA performance across five benchmarks, and outperforms Qwen-2.5-Math-7B-Instruct which uses the same base model Qwen-2.5-Math-7B. After round-2 training, Satori-Qwen-7B (Round 2) demonstrates even stronger performance on hard tasks.</p>
<table>
<thead>
<tr>
<th>Scale</th>
<th>Model</th>
<th>GSM8K</th>
<th>MATH500</th>
<th>OlympiadBench</th>
<th>AMC2023</th>
<th>AIME2024</th>
<th>Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td>Large</td>
<td>GPT-4o</td>
<td>$/$</td>
<td>60.3</td>
<td>43.3</td>
<td>$/$</td>
<td>9.3</td>
<td>$/$</td>
</tr>
<tr>
<td></td>
<td>o1-preview</td>
<td>$/$</td>
<td>85.5</td>
<td>$/$</td>
<td>82.5</td>
<td>44.6</td>
<td>$/$</td>
</tr>
<tr>
<td></td>
<td>Llama-3.1-70B-Instruct</td>
<td>94.1</td>
<td>68.0</td>
<td>29.4</td>
<td>42.5</td>
<td>13.3</td>
<td>49.5</td>
</tr>
<tr>
<td></td>
<td>OpenMath2-Llama3.1-70B</td>
<td>94.1</td>
<td>71.8</td>
<td>30.1</td>
<td>45.0</td>
<td>13.3</td>
<td>50.9</td>
</tr>
<tr>
<td></td>
<td>QwQ-32B-Preview</td>
<td>95.5</td>
<td>90.6</td>
<td>61.2</td>
<td>77.5</td>
<td>50.0</td>
<td>75.0</td>
</tr>
<tr>
<td>Small</td>
<td>Llama-3.1-8b-Instruct</td>
<td>84.4</td>
<td>51.9</td>
<td>15.1</td>
<td>22.5</td>
<td>3.3</td>
<td>35.4</td>
</tr>
<tr>
<td></td>
<td>OpenMath2-Llama3.1-8B</td>
<td>90.5</td>
<td>67.8</td>
<td>28.9</td>
<td>37.5</td>
<td>6.7</td>
<td>46.3</td>
</tr>
<tr>
<td></td>
<td>NuminaMath-7B-CoT</td>
<td>78.9</td>
<td>54.6</td>
<td>15.9</td>
<td>20.0</td>
<td>10.0</td>
<td>35.9</td>
</tr>
<tr>
<td></td>
<td>Qwen-2.5-7B-Instruct</td>
<td>91.6</td>
<td>75.5</td>
<td>35.5</td>
<td>52.5</td>
<td>6.7</td>
<td>52.4</td>
</tr>
<tr>
<td></td>
<td>Qwen-2.5-Math-7B-Instruct</td>
<td>95.2</td>
<td>83.6</td>
<td>41.6</td>
<td>62.5</td>
<td>16.7</td>
<td>59.9</td>
</tr>
<tr>
<td></td>
<td>Satori-Qwen-7B</td>
<td>93.2</td>
<td>85.6</td>
<td>46.6</td>
<td>67.5</td>
<td>20.0</td>
<td>62.6</td>
</tr>
<tr>
<td></td>
<td>Satori-Qwen-7B (Round 2)</td>
<td>93.9</td>
<td>83.6</td>
<td>48.5</td>
<td>72.5</td>
<td>23.3</td>
<td>64.4</td>
</tr>
</tbody>
</table>
<p>Table 2: Results on Out-of-domain Benchmarks. Trained only on math datasets, Satori-Qwen-7B exhibits strong transferability across diverse out-of-domain benchmarks and outperforms Qwen-2.5-Math-7B-Instruct by a large margin. Moreover, despite not being trained in other domains, Satori-Qwen-7B achieves performance comparable to or exceeding other small-scale general instruct models.</p>
<table>
<thead>
<tr>
<th>Scale</th>
<th>Model</th>
<th>FOLIO</th>
<th>BGQA</th>
<th>CRUXEval</th>
<th>StrategyQA</th>
<th>TableBench</th>
<th>STEM</th>
<th>Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td>Large</td>
<td>Llama-3.1-70B-Instruct</td>
<td>65.0</td>
<td>58.3</td>
<td>59.6</td>
<td>88.8</td>
<td>34.2</td>
<td>61.7</td>
<td>61.3</td>
</tr>
<tr>
<td></td>
<td>OpenMath2-Llama3.1-70B</td>
<td>68.5</td>
<td>68.7</td>
<td>35.1</td>
<td>95.6</td>
<td>46.8</td>
<td>15.1</td>
<td>55.0</td>
</tr>
<tr>
<td></td>
<td>QwQ-32B-Preview</td>
<td>84.2</td>
<td>71.1</td>
<td>65.2</td>
<td>88.2</td>
<td>51.5</td>
<td>71.3</td>
<td>71.9</td>
</tr>
<tr>
<td>Small</td>
<td>Llama-3.1-8b-Instruct</td>
<td>63.5</td>
<td>50.3</td>
<td>38.5</td>
<td>92.2</td>
<td>32.4</td>
<td>43.4</td>
<td>53.4</td>
</tr>
<tr>
<td></td>
<td>OpenMath2-Llama3.1-8B</td>
<td>57.1</td>
<td>49.0</td>
<td>11.1</td>
<td>84.4</td>
<td>34.2</td>
<td>10.9</td>
<td>41.1</td>
</tr>
<tr>
<td></td>
<td>NuminaMath-7B-CoT</td>
<td>53.2</td>
<td>44.6</td>
<td>28.0</td>
<td>77.8</td>
<td>29.1</td>
<td>11.3</td>
<td>40.7</td>
</tr>
<tr>
<td></td>
<td>Qwen-2.5-7B-Instruct</td>
<td>72.4</td>
<td>53.0</td>
<td>58.1</td>
<td>91.3</td>
<td>43.2</td>
<td>57.1</td>
<td>62.5</td>
</tr>
<tr>
<td></td>
<td>Qwen-2.5-Math-7B-Instruct</td>
<td>68.9</td>
<td>51.3</td>
<td>28.0</td>
<td>85.3</td>
<td>36.2</td>
<td>45.2</td>
<td>52.5</td>
</tr>
<tr>
<td></td>
<td>Satori-Qwen-7B</td>
<td>71.4</td>
<td>61.8</td>
<td>42.5</td>
<td>86.3</td>
<td>43.4</td>
<td>56.7</td>
<td>60.4</td>
</tr>
<tr>
<td></td>
<td>Satori-Qwen-7B (Round 2)</td>
<td>72.9</td>
<td>58.5</td>
<td>41.1</td>
<td>90.4</td>
<td>44.6</td>
<td>57.4</td>
<td>60.8</td>
</tr>
</tbody>
</table>
<p>Benchmark and Evaluation. We conduct the main evaluation of the models using math benchmarks to assess their problem-solving abilities, including GSM8K, MATH500 (a subset of the MATH test set <em>Lightman et al., (2023)</em>), AMC2023, AIME2024, and OlympiadBench. Except for GSM8K, all other datasets feature competition-level problems. The evaluation is performed using greedy decoding without tool integration. The main metric reported is the zero-shot pass@1 accuracy, which measures the percentage of problems correctly solved on the first attempt. We also conduct additional evaluations on a wide range of benchmarks beyond the math domain to evaluate general reasoning capabilities. This includes logical reasoning (FOLIO <em>Han et al., (2024)</em>, BoardgameQA (BGQA) <em>Kazemi et al., (2024)</em>), code reasoning (CRUXEval <em>Gu et al., (2024)</em>), commonsense reasoning (StrategyQA (STGQA) <em>Geva et al., (2021)</em>), tabular reasoning (TableBench <em>Wu et al., (2024a)</em>), and domain-specific reasoning (STEM subsets of MMLUPro <em>Wang et al., (2024b)</em>), including physics, chemistry, computer science, engineering, biology, and economics. For more evaluation details, please refer to Appendix D.4.</p>
<p>Baseline Models. We compare our developed model, Satori-Qwen-7B, with several industry-developed LLMs. The main comparison is between our model and Qwen-2.5-Math-7B-Instruct <em>Yang et al., (2024a)</em>, a math-specialized model built on the same base model (Qwen-2.5-Math-7B) as ours. Additionally, we report the performance of larger models, including o1-preview and QwQ-32B-Preview, which exhibit strong reasoning capabilities and serve as performance upper bounds.</p>
<h3>5.1 Main Results on Math Domain</h3>
<p>We present math benchmark results in Table 1, where Satori-Qwen-7B outperforms all small-scale baseline models. Notably, using Qwen-2.5-Math-7B as the base model, Satori-Qwen-7B achieves superior performance compared to Qwen-2.5-Math-7B-Instruct, despite requiring significantly less supervision (i.e., less SFT data) and relying more on self-improvement (i.e., more RL data) (see Figure 2).</p>
<h3>5.2. Out-of-Domain Transferability</h3>
<p>Although Satori-Qwen-7B is trained only on math domain datasets, we observe that it can extrapolate its reasoning capabilities to other domains. In Table 2, we evaluate SatoriQwen-7B on a diverse set of out-of-domain benchmarks that require reasoning capabilities but are not directly related to math. Similar to the observation on the math domain, Satori demonstrates superior performance on several benchmarks, outperforming Qwen-2.5-Math-7B-Instruct. In particular, on the challenging reasoning benchmark BoardgameQA, Satori-Qwen-7B surpasses all baseline models of the same scale. These results and demo examples in Appendix A suggest that Satori has acquired general reasoning capabilities rather than simply math problem solving skills. In Section 6, we present further analysis to show that this transferability emerges as a result of large-scale reinforcement learning.</p>
<h3>5.3. Results on Iterative Self-improvement</h3>
<p>Finally, we present the results of the second-round training of Satori. As shown in Table 1 and Table 2, compared to Satori-Qwen-7B, Satori-Qwen-7B (Round 2) demonstrates continuous performance gains across most in-domain and out-of-domain benchmarks. This suggests the significant potential of iterative self-improvement to push the limit of LLM's reasoning performance.</p>
<h2>6. Analysis</h2>
<p>In this section, we provide a comprehensive analysis of Satori. First, we demonstrate that Satori effectively leverages self-reflection to seek better solutions and enhance its overall reasoning performance. Next, we observe that Satori exhibits test-scaling behavior through RL training, where it progressively acquires more tokens to improve its reasoning capabilities. Finally, we conduct ablation studies on various components of Satori's training framework. Additional results are provided in Appendix E.</p>
<p>Table 3: COAT Training v.s. CoT Training. Qwen-2.5-Math-7B trained with COAT reasoning format (Satori-Qwen-7B) outperforms the same base model but trained with classical CoT reasoning format (Qwen-7B-CoT)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">MATH500</th>
<th style="text-align: center;">Olym.</th>
<th style="text-align: center;">AMC2023</th>
<th style="text-align: center;">AIME2024</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Qwen-2.5-Math-7B-Instruct</td>
<td style="text-align: center;">95.2</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">16.7</td>
</tr>
<tr>
<td style="text-align: center;">Qwen-7B-CoT (SFT+RL)</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;">Satori-Qwen-7B</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">20.0</td>
</tr>
</tbody>
</table>
<p>COAT Reasoning v.s. CoT Reasoning. We begin by conducting an ablation study to demonstrate the benefits of COAT reasoning compared to the classical CoT reasoning. Specifically, starting from the synthesis of demonstration trajectories in the format tuning stage, we ablate the "reflect" and "explore" actions, retaining only the "continue"
actions. Next, we maintain all other training settings, including the same amount of SFT and RL data and consistent hyper-parameters. This results in a typical CoT LLM (Qwen-7B-CoT) without self-reflection or self-exploration capabilities. As shown in Table 3, the performance of Qwen-7B-CoT is suboptimal compared to Satori-Qwen-7B and fails to surpass Qwen-2.5-Math-7B-Instruct, suggesting the advantages of COAT reasoning over CoT reasoning.</p>
<p>Table 4: Satori's Self-correction Capability. T $\rightarrow$ F: negative self-correction; $\mathrm{F} \rightarrow \mathrm{T}$ : positive self-correction.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">In-Domain</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Out-of-Domain</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MATH500</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">OlympiadBench</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MMLUProSTEM</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{T} \rightarrow \mathrm{F}$</td>
<td style="text-align: center;">$\mathrm{F} \rightarrow \mathrm{T}$</td>
<td style="text-align: center;">$\mathrm{T} \rightarrow \mathrm{F}$</td>
<td style="text-align: center;">$\mathrm{F} \rightarrow \mathrm{T}$</td>
<td style="text-align: center;">$\mathrm{T} \rightarrow \mathrm{F}$</td>
<td style="text-align: center;">$\mathrm{F} \rightarrow \mathrm{T}$</td>
</tr>
<tr>
<td style="text-align: center;">Satori-Qwen-7B-FT</td>
<td style="text-align: center;">$79.4 \%$</td>
<td style="text-align: center;">$20.6 \%$</td>
<td style="text-align: center;">$65.6 \%$</td>
<td style="text-align: center;">$34.4 \%$</td>
<td style="text-align: center;">$59.2 \%$</td>
<td style="text-align: center;">$40.8 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Satori-Qwen-7B</td>
<td style="text-align: center;">$39.0 \%$</td>
<td style="text-align: center;">$61.0 \%$</td>
<td style="text-align: center;">$42.1 \%$</td>
<td style="text-align: center;">$57.9 \%$</td>
<td style="text-align: center;">$46.5 \%$</td>
<td style="text-align: center;">$53.5 \%$</td>
</tr>
</tbody>
</table>
<p>Satori Exhibits Self-correction Capability. We observe that Satori frequently engages in self-reflection during the reasoning process (see demos in Section A), which occurs in two scenarios: (1) it triggers self-reflection at intermediate reasoning steps, and (2) after completing a problem, it initiates a second attempt through self-reflection. We focus on quantitatively evaluating Satori's self-correction capability in the second scenario. Specifically, we extract responses where the final answer before self-reflection differs from the answer after self-reflection. We then quantify the percentage of responses in which Satori's self-correction is positive (i.e., the solution is corrected from incorrect to correct) or negative (i.e., the solution changes from correct to incorrect). The evaluation results on in-domain datasets (MATH500 and Olympiad) and out-of-domain datasets (MMLUPro) are presented in Table 4. First, compared to Satori-Qwen-FT which lacks the RL training stage, Satori-Qwen demonstrates a significantly stronger self-correction capability. Second, we observe that this self-correction capability extends to out-of-domain tasks (MMLUProSTEM). These results suggest that RL plays a crucial role in enhancing the model's true reasoning capabilities.</p>
<h2>RL Enables Satori with Test-time Scaling Behavior.</h2>
<p>Next, we aim to explain how reinforcement learning (RL) incentivizes Satori's autoregressive search capability. First, as shown in Figure 3, we observe that Satori consistently improves policy accuracy and increases the average length of generated tokens with more RL training-time compute. This suggests that Satori learns to allocate more time to reasoning, thereby solving problems more accurately. One interesting observation is that the response length first decreases from 0 to 200 steps and then increases. Upon a closer investigation of the model response, we observe that in the early stage, our model has not yet learned self-reflection capabilities. During this stage, RL optimization may prioritize the model</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Policy Training Acc. \&amp; Response length v.s. RL Traintime Compute. Through RL training, Satori learns to improve its reasoning performance through longer thinking.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Above: Test-time Response Length v.s. Problem Difficulty Level; Below: Test-time Accuracy v.s. Problem Difficulty Level. Compared to FT model (Satori-Qwen-FT), Satori-Qwen uses more test-time compute to tackle more challenging problems.
to find a shot-cut solution without redundant reflection, leading to a temporary reduction in response length. However, in later stage, the model becomes increasingly good at using reflection to self-correct and find a better solution, leading to a longer response length.</p>
<p>Additionally, in Figure 4, we evaluate Satori's test accuracy and response length on MATH datasets across different difficulty levels. Interestingly, through RL training, Satori naturally allocates more test-time compute to tackle more challenging problems, which leads to consistent performance improvements compared to the format-tuned (FT) model.</p>
<p>Large-scale FT v.s. Large-scale RL. We investigate whether scaling up format tuning (FT) can achieve performance gains comparable to RL training. We conduct an ablation study using Qwen-2.5-Math-7B, trained with an equivalent amount of FT data (300K). As shown in Table 5, on the math domain benchmarks, the model trained with large-scale FT (300K) fails to match the performance of the model trained with small-scale FT (10K) and large-scale RL</p>
<p>Table 5: Large-scale FT V.S. Large-scale RL Satori-Qwen (10K FT data + 300K RL data) outperforms same base model Qwen-2.5Math-7B trained with 300K FT data (w/o RL) across all math and out-of-domain benchmarks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">(In-domain)</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">MATH500</th>
<th style="text-align: center;">Olym.</th>
<th style="text-align: center;">AMC2023</th>
<th style="text-align: center;">AIME2024</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Qwen-2.5-Math-7B-Instruct</td>
<td style="text-align: center;">95.2</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">16.7</td>
</tr>
<tr>
<td style="text-align: left;">Satori-Qwen-7B-FT (300K)</td>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">40.9</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">16.7</td>
</tr>
<tr>
<td style="text-align: left;">Satori-Qwen-7B</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">20.0</td>
</tr>
<tr>
<td style="text-align: left;">(Out-of-domain)</td>
<td style="text-align: center;">BGQA</td>
<td style="text-align: center;">CRUX</td>
<td style="text-align: center;">STGQA</td>
<td style="text-align: center;">TableBench</td>
<td style="text-align: center;">STEM</td>
</tr>
<tr>
<td style="text-align: left;">Qwen-2.5-Math-7B-Instruct</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">45.2</td>
</tr>
<tr>
<td style="text-align: left;">Satori-Qwen-7B-FT (300K)</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">47.8</td>
</tr>
<tr>
<td style="text-align: left;">Satori-Qwen-7B</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">56.7</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Format Tuning v.s. Distillation. Distilling from a Stronger model (Satori-Qwen-7B) to weaker base models (Llama8B and Granite-8B) are more effective than directly applying format tuning on weaker base models.
(300K). Additionally, the large-scale FT model performs significantly worse on out-of-domain tasks, demonstrates RL's advantage in generalization.</p>
<p>Distillation Enables Weak-to-Strong Generalization. Finally, we investigate whether distilling a stronger reasoning model can enhance the reasoning performance of weaker base models. Specifically, we use Satori-Qwen-7B to generate 240K synthetic data to train weaker base models, Llama-3.1-8B and Granite-3.1-8B. For comparison, we also synthesize 240K FT data (following Section 4.1) to train the same models. We evaluate the average test accuracy of these models across all math benchmark datasets, with the results presented in Figure 5. The results show that the distilled models outperform the format-tuned models.</p>
<p>This suggests a new, efficient approach to improve the reasoning capabilities of weaker base models: (1) train a strong reasoning model through small-scale FT and large-scale RL (our Satori-Qwen-7B) and (2) distill the strong reasoning capabilities of the model into weaker base models. Since RL only requires answer labels as supervision, this approach introduces minimal costs for data synthesis, i.e., the costs induced by a multi-agent data synthesis framework or even more expensive human annotation.</p>
<h2>7. Concluding Remarks</h2>
<p>The training framework of Satori exhibits significant potential for enhancing LLM reasoning capabilities. The smallscale format tuning stage serves as a warm-up phase, allowing the LLM policy to internalize a specific reasoning format, while large-scale reinforcement learning (RL) plays a crucial role in incentivizing intrinsic reasoning abilities. We believe that this framework can inspire the research community to explore more methods for achieving autoregressive search, such as developing reasoning formats with a broader range of meta-actions, designing more advanced RL algorithms, and extending this approach to general domain.</p>
<h2>Acknowledgment</h2>
<p>This research/project was supported, in part, by the following funding resources: MIT-IBM Watson AI Lab under Agreement No. W1771646, the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD-2021-08-007), and Ministry of Education, Singapore, under its Academic Research Fund (AcRF) Tier 2 Programme (MOE AcRF Tier 2 Award No. : MOE-T2EP20122-0011).</p>
<h2>Impact Statement</h2>
<p>This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.</p>
<h2>References</h2>
<p>K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano et al., "Training verifiers to solve math word problems," arXiv preprint arXiv:2110.14168, 2021. 1, 24, 36
D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt, "Measuring mathematical problem solving with the MATH dataset," in Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. $1,2,24,36$
M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al., "Evaluating large language models trained on code," arXiv preprint arXiv:2107.03374, 2021. 1, 24
T. Y. Zhuo, M. C. Vu, J. Chim, H. Hu, W. Yu, R. Widyasari, I. N. B. Yusuf, H. Zhan, J. He, I. Paul et al., "Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions," arXiv preprint arXiv:2406.15877, 2024. 1, 24
S. Han, H. Schoelkopf, Y. Zhao, Z. Qi, M. Riddell et al., "FOLIO: Natural language reasoning with first-order logic," in Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, Y. AlOnaizan, M. Bansal, and Y.-N. Chen, Eds. Miami, Florida, USA: Association for Computational Linguistics, November 2024, pp. 22 017-22 031. 1, 6, 24, 36
J. Liu, L. Cui, H. Liu, D. Huang, Y. Wang, and Y. Zhang, "LogiQA: A challenge dataset for machine reading comprehension with logical reasoning," in Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020, C. Bessiere, Ed. ijcai.org, 2020, pp. 3622-3628. 1, 24
J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., "Chain-of-thought prompting elicits reasoning in large language models," Advances in neural information processing systems, vol. 35, pp. $24824-24837,2022.1,3,24$
X. Yue, X. Qu, G. Zhang, Y. Fu, W. Huang, H. Sun, Y. Su, and W. Chen, "MAmmoTH: Building math generalist models through hybrid instruction tuning," in The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 1, 2, 24
L. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok, Z. Li, A. Weller, and W. Liu, "MetaMath: Bootstrap your own mathematical questions for large language models," in The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 1, 2, 24
S. Toshniwal, W. Du, I. Moshkov, B. Kisacanin, A. Ayrapetyan, and I. Gitman, "Openmathinstruct-2: Accelerating AI for math with massive open-source instruction data," arXiv preprint arXiv:2410.01560, 2024. 1, 2, 24, 27
Y. Ding, X. Shi, X. Liang, J. Li, Q. Zhu, and M. Zhang, "Unleashing reasoning capability of llms via scalable question synthesis from scratch," arXiv preprint arXiv:2410.18693, 2024. 1, 2, 24
X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, S. Narang, A. Chowdhery, and D. Zhou, "Selfconsistency improves chain of thought reasoning in language models," in The Eleventh International Conference on Learning Representations, 2023. 1
S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and K. Narasimhan, "Tree of thoughts: Deliberate problem solving with large language models," Advances in Neural Information Processing Systems, vol. 36, 2024. 1, 2, 24</p>
<p>Z. Wan, X. Feng, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang, "Alphazero-like tree-search can guide large language model decoding and training," in Forty-first International Conference on Machine Learning, 2024. 1
Z. Sun, L. Yu, Y. Shen, W. Liu, Y. Yang, S. Welleck, and C. Gan, "Easy-to-hard generalization: Scalable alignment beyond human supervision," in The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 1, 2, 24
P. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui, "Math-Shepherd: Verify and reinforce LLMs step-by-step without human annotations," in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), L.-W. Ku, A. Martins, and V. Srikumar, Eds. Bangkok, Thailand: Association for Computational Linguistics, August 2024, pp. 9426-9439. 1, 2, 24</p>
<p>OpenAI, "Learning to reason with llms," 2024, accessed: 2024-12-18. [Online]. Available: https://openai.com/ index/learning-to-reason-with-llms/ 2
Z. Huang, H. Zou, X. Li, Y. Liu, Y. Zheng, E. Chern, S. Xia, Y. Qin, W. Yuan, and P. Liu, "O1 replication journey-part 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson?" arXiv preprint arXiv:2411.16489, 2024. 2, 24
Y. Zhao, H. Yin, B. Zeng, H. Wang, T. Shi, C. Lyu, L. Wang, W. Luo, and K. Zhang, "Marco-o1: Towards open reasoning models for open-ended solutions," arXiv preprint arXiv:2411.14405, 2024. 2, 24
Y. Min, Z. Chen, J. Jiang, J. Chen, J. Deng, Y. Hu, Y. Tang, J. Wang, X. Cheng, H. Song et al., "Imitate, explore, and self-improve: A reproduction report on slow-thinking reasoning systems," arXiv preprint arXiv:2412.09413, 2024. 2, 24
D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi et al., "DeepSeek-R1: Incentivizing reasoning capability in llms via reinforcement learning," arXiv preprint arXiv:2501.12948, 2025. 2
E. Zelikman, Y. Wu, J. Mu, and N. D. Goodman, "STaR: Bootstrapping reasoning with reasoning," in Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 December 9, 2022, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., 2022. 2, 24
E. Zelikman, G. R. Harik, Y. Shao, V. Jayasiri, N. Haber, and N. Goodman, "Quiet-STaR: Language models can teach
themselves to think before speaking," in First Conference on Language Modeling, 2024. 2, 24
A. Singh, J. D. Co-Reyes, R. Agarwal, A. Anand, P. Patil, X. Garcia, P. J. Liu, J. Harrison et al., "Beyond human data: Scaling self-training for problem-solving with language models," Trans. Mach. Learn. Res., vol. 2024, 2024. 2,24
X. Zhang, C. Du, T. Pang, Q. Liu, W. Gao, and M. Lin, "Chain of preference optimization: Improving chain-ofthought reasoning in LLMs," in The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 2, 24
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, "Proximal policy optimization algorithms," arXiv preprint arXiv:1707.06347, 2017. 2, 24, 35
L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe, "Training language models to follow instructions with human feedback," in Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., 2022. 2, 24
L. Yuan, W. Li, H. Chen, G. Cui, N. Ding, K. Zhang, B. Zhou, Z. Liu, and H. Peng, "Free process rewards without process labels," arXiv preprint arXiv:2412.01981, 2024. 2, 24
N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao, "Reflexion: Language agents with verbal reinforcement learning," Advances in Neural Information Processing Systems, vol. 36, 2024. 2, 24
S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, and Z. Hu, "Reasoning with language model is planning with world model," in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, H. Bouamor, J. Pino, and K. Bali, Eds. Association for Computational Linguistics, 2023, pp. 8154-8173. 2, 24
Z. Qi, M. Ma, J. Xu, L. L. Zhang, F. Yang, and M. Yang, "Mutual reasoning makes smaller llms stronger problemsolvers," arXiv preprint arXiv:2408.06195, 2024. 2, 24, 27
Y. Zhang, M. Khalifa, L. Logeswaran, J. Kim, M. Lee, H. Lee, and L. Wang, "Small language models need strong verifiers to self-correct reasoning," in Findings of</p>
<p>the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, L. Ku, A. Martins, and V. Srikumar, Eds. Association for Computational Linguistics, 2024, pp. 1563715653. 2, 24
R. Kamoi, Y. Zhang, N. Zhang, J. Han, and R. Zhang, "When can llms actually correct their own mistakes? a critical survey of self-correction of llms," Transactions of the Association for Computational Linguistics, vol. 12, pp. 1417-1440, 2024. 2, 24
W. Saunders, C. Yeh, J. Wu, S. Bills, L. Ouyang, J. Ward, and J. Leike, "Self-critiquing models for assisting human evaluators," arXiv preprint arXiv:2206.05802, 2022. 2, 24
A. Kumar, V. Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop, R. Roelofs et al., "Training language models to self-correct via reinforcement learning," arXiv preprint arXiv:2409.12917, 2024. 2, 24
Y. Qu, T. Zhang, N. Garg, and A. Kumar, "Recursive introspection: Teaching language model agents how to self-improve," in The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 2, 24
A. Havrilla, S. C. Raparthy, C. Nalmpantis, J. Dwivedi-Yu, M. Zhuravinskyi, E. Hambro, and R. Raileanu, "GLoRe: When, where, and how to improve LLM reasoning via global and local refinements," in Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. 2, 24
Z. Xi, D. Yang, J. Huang, J. Tang, G. Li, Y. Ding, W. He, B. Hong, S. Do, W. Zhan et al., "Enhancing llm reasoning via critique models with test-time and training-time supervision," arXiv preprint arXiv:2411.16579, 2024. 2, 24
A. Setlur, C. Nagpal, A. Fisch, X. Geng, J. Eisenstein, R. Agarwal, A. Agarwal, J. Berant, and A. Kumar, "Rewarding progress: Scaling automated process verifiers for llm reasoning," arXiv preprint arXiv:2410.08146, 2024. 2, 24
L. Zhang, A. Hosseini, H. Bansal, M. Kazemi, A. Kumar, and R. Agarwal, "Generative verifiers: Reward modeling as next-token prediction," in The 4th Workshop on Mathematical Reasoning and AI at NeurIPS'24, 2024. 2, 24
X. Guan, L. L. Zhang, Y. Liu, N. Shang, Y. Sun, Y. Zhu, F. Yang, and M. Yang, "rstar-math: Small llms can master math reasoning with self-evolved deep thinking," arXiv preprint arXiv:2501.04519, 2025. 2, 24
D. Zhang, S. Zhoubian, Z. Hu, Y. Yue, Y. Dong, and J. Tang, "ReST-MCTS*: LLM self-training via process reward guided tree search," in The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 2, 24
K. Gandhi, D. H. J. Lee, G. Grand, M. Liu, W. Cheng, A. Sharma, and N. Goodman, "Stream of search (SoS): Learning to search in language," in First Conference on Language Modeling, 2024. 2, 24
L. P. Kaelbling, M. L. Littman, and A. W. Moore, "Reinforcement learning: A survey," Journal of artificial intelligence research, vol. 4, pp. 237-285, 1996. 2
M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos, "Unifying count-based exploration and intrinsic motivation," Advances in neural information processing systems, vol. 29, 2016. 3
A. Ecoffet, J. Huizinga, J. Lehman, K. O. Stanley, and J. Clune, "Go-explore: a new approach for hardexploration problems," arXiv preprint arXiv:1901.10995, 2019. 4
A. Hussein, M. M. Gaber, E. Elyan, and C. Jayne, "Imitation learning: A survey of learning methods," ACM Computing Surveys (CSUR), vol. 50, no. 2, pp. 1-35, 2017. 4
S. Ross and D. Bagnell, "Efficient reductions for imitation learning," in Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2010, pp. 661668. 4
S. Ross, G. Gordon, and D. Bagnell, "A reduction of imitation learning and structured prediction to no-regret online learning," in Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2011, pp. 627-635. 4
J. Ho and S. Ermon, "Generative adversarial imitation learning," Advances in neural information processing systems, vol. 29, 2016. 4
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, "Proximal policy optimization algorithms," arXiv preprint arXiv:1707.06347, 2017. 4
S. Kakade and J. Langford, "Approximately optimal approximate reinforcement learning," in Proceedings of the nineteenth international conference on machine learning, 2002, pp. 267-274. 5
R. Agarwal, M. Schwarzer, P. S. Castro, A. C. Courville, and M. Bellemare, "Reincarnating reinforcement learning:</p>
<p>Reusing prior computation to accelerate progress," Advances in neural information processing systems, vol. 35, pp. 28 955-28 971, 2022. 5
S. Schmitt, J. J. Hudson, A. Zidek, S. Osindero, C. Doersch, W. M. Czarnecki, J. Z. Leibo, H. Kuttler, A. Zisserman, K. Simonyan et al., "Kickstarting deep reinforcement learning," arXiv preprint arXiv:1803.03835, 2018. 5
H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe, "Let's verify step by step," arXiv preprint arXiv:2305.20050, 2023. 6, 36
M. Kazemi, Q. Yuan, D. Bhatia, N. Kim, X. Xu, V. Imbrasaite, and D. Ramachandran, "Boardgameqa: A dataset for natural language reasoning with contradictory information," Advances in Neural Information Processing Systems, vol. 36, 2024. 6, 36
A. Gu, B. Rozière, H. Leather, A. Solar-Lezama, G. Synnaeve, and S. I. Wang, "Cruxeval: A benchmark for code reasoning, understanding and execution," arXiv preprint arXiv:2401.03065, 2024. 6, 24, 36
M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant, "Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies," Transactions of the Association for Computational Linguistics, vol. 9, pp. 346-361, 2021. 6, 24, 36
X. Wu, J. Yang, L. Chai, G. Zhang, J. Liu, X. Du, D. Liang, D. Shu, X. Cheng, T. Sun et al., "Tablebench: A comprehensive and complex benchmark for table question answering," arXiv preprint arXiv:2408.09174, 2024. 6, 24,36
Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang et al., "Mmlu-pro: A more robust and challenging multi-task language understanding benchmark," arXiv preprint arXiv:2406.01574, 2024. 6, 24, 36
A. Yang, B. Zhang, B. Hui, B. Gao, B. Yu, C. Li, D. Liu, J. Tu, J. Zhou, J. Lin et al., "Qwen2.5-math technical report: Toward mathematical expert model via selfimprovement," arXiv preprint arXiv:2409.12122, 2024. 6
E. Glazer, E. Erdil, T. Besiroglu, D. Chicharro, E. Chen, A. Gunning, C. F. Olsson, J.-S. Denain, A. Ho, E. d. O. Santos et al., "Frontiermath: A benchmark for evaluating advanced mathematical reasoning in ai," arXiv preprint arXiv:2411.04872, 2024. 24
I. Mirzadeh, K. Alizadeh, H. Shahrokhi, O. Tuzel, S. Bengio, and M. Farajtabar, "Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models," arXiv preprint arXiv:2410.05229, 2024. 24
J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le et al., "Program synthesis with large language models," arXiv preprint arXiv:2108.07732, 2021. 24
J. Dai, J. Lu, Y. Feng, D. Huang, G. Zeng, R. Ruan, M. Cheng, H. Tan, and Z. Guo, "Mhpp: Exploring the capabilities and limitations of language models beyond basic code generation," arXiv preprint arXiv:2405.11430, 2024. 24
C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. R. Narasimhan, "SWE-bench: Can language models resolve real-world github issues?" in The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 24
C. Liu, J. Shen, H. Xin, Z. Liu, Y. Yuan, H. Wang, W. Ju, C. Zheng, Y. Yin, L. Li et al., "Fimo: A challenge formal dataset for automated theorem proving," arXiv preprint arXiv:2309.04295, 2023. 24
O. Tafjord, B. Dalvi, and P. Clark, "ProofWriter: Generating implications, proofs, and abductive statements over natural language," in Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, ser. Findings of ACL, C. Zong, F. Xia, W. Li, and R. Navigli, Eds., vol. ACL/IJCNLP 2021. Association for Computational Linguistics, 2021, pp. 36213634. 24
A. Talmor, J. Herzig, N. Lourie, and J. Berant, "Commonsenseqa: A question answering challenge targeting commonsense knowledge," arXiv preprint arXiv:1811.00937, 2018. 24
P. Veličković, A. P. Badia, D. Budden, R. Pascanu, A. Banino, M. Dashevskiy, R. Hadsell, and C. Blundell, "The CLRS algorithmic reasoning benchmark," in International Conference on Machine Learning. PMLR, 2022, pp. 22 084-22 102. 24
L. Markeeva, S. McLeish, B. Ibarz, W. Bounsi, O. Kozlova, A. Vitvitskyi, C. Blundell, T. Goldstein, A. Schwarzschild, and P. Veličković, "The CLRS-Text algorithmic reasoning language benchmark," arXiv preprint arXiv:2406.04229, 2024. 24
Z. Qi, H. Luo, X. Huang, Z. Zhao, Y. Jiang, X. Fan, H. Lakkaraju, and J. Glass, "Quantifying generalization complexity for large language models," arXiv preprint arXiv:2410.01769, 2024. 24
L. Fan, W. Hua, L. Li, H. Ling, and Y. Zhang, "NPHardEval: Dynamic benchmark on reasoning ability of large language models via complexity classes," in Proceedings</p>
<p>of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, L. Ku, A. Martins, and V. Srikumar, Eds. Association for Computational Linguistics, 2024, pp. 4092-4114. 24
P. Lu, L. Qiu, K.-W. Chang, Y. N. Wu, S.-C. Zhu, T. Rajpurohit, P. Clark, and A. Kalyan, "Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning," arXiv preprint arXiv:2209.14610, 2022. 24
X. Wang, Z. Hu, P. Lu, Y. Zhu, J. Zhang, S. Subramaniam, A. R. Loomba, S. Zhang, Y. Sun, and W. Wang, "SciBench: Evaluating college-level scientific problemsolving abilities of large language models," in Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. 24
D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman, "GPQA: A graduate-level google-proof q\&amp;a benchmark," in First Conference on Language Modeling, 2024. 24
D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt, "Measuring massive multitask language understanding," in 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. 24, 36
A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown et al., "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models," Trans. Mach. Learn. Res., vol. 2023, 2023. 24
P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga et al., "Holistic evaluation of language models," Trans. Mach. Learn. Res., vol. 2023, 2023. 24
L. Phan, A. Gatti, Z. Han, N. Li, J. Hu, H. Zhang, S. Shi, M. Choi, A. Agrawal, A. Chopra, A. Khoja, R. Kim, J. Hausenloy et al., "Humanity's last exam," 2025. [Online]. Available: https://arxiv.org/abs/2501.14249 24
W. Saunders, C. Yeh, J. Wu, S. Bills, L. Ouyang, J. Ward, and J. Leike, "Self-critiquing models for assisting human evaluators," arXiv preprint arXiv:2206.05802, 2022. 24
S. Toshniwal, I. Moshkov, S. Narenthiran, D. Gitman, F. Jia, and I. Gitman, "OpenMathInstruct-1: A 1.8 million math instruction tuning dataset," in The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. 24
H. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang, "Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct," arXiv preprint arXiv:2308.09583, 2023. 24
M. Abdin, J. Aneja, H. Behl, S. Bubeck, R. Eldan, S. Gunasekar, M. Harrison, R. J. Hewett, M. Javaheripi, P. Kauffmann et al., "Phi-4 technical report," arXiv preprint arXiv:2412.08905, 2024. 24
I. Shumailov, Z. Shumaylov, Y. Zhao, Y. Gal, N. Papernot, and R. Anderson, "The curse of recursion: Training on generated data makes models forget," arXiv preprint arXiv:2305.17493, 2023. 24
T. Wu, X. Li, and P. Liu, "Progress or regress? selfimprovement reversal in post-training," arXiv preprint arXiv:2407.05013, 2024. 24
M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Podstawski, L. Gianinazzi, J. Gajda, T. Lehmann, H. Niewiadomski, P. Nyczyk et al., "Graph of thoughts: Solving elaborate problems with large language models," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 16, 2024, pp. 17 682-17 690. 24
A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang et al., "Self-refine: Iterative refinement with selffeedback," Advances in Neural Information Processing Systems, vol. 36, 2024. 24
L. Yang, Z. Yu, T. Zhang, S. Cao, M. Xu, W. Zhang, J. E. Gonzalez, and B. CUI, "Buffer of Thoughts: Thoughtaugmented reasoning with large language models," in The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 24
J. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, and D. Zhou, "Large language models cannot self-correct reasoning yet," in The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 24
S. Welleck, X. Lu, P. West, F. Brahman, T. Shen, D. Khashabi, and Y. Choi, "Generating sequences by learning to self-correct," in The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. 24
D. Paul, M. Ismayilzada, M. Peyrard, B. Borges, A. Bosselut, R. West, and B. Faltings, "REFINER: reasoning feedback on intermediate representations," in Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2024 Volume 1: Long Papers, St. Julian's, Malta, March 17-22, 2024, Y. Graham and M. Purver, Eds. Association for Computational Linguistics, 2024, pp. 1100-1126. 24</p>
<p>D. Zhang, J. Wu, J. Lei, T. Che, J. Li, T. Xie, X. Huang, S. Zhang, M. Pavone, Y. Li et al., "Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning," arXiv preprint arXiv:2410.02884, 2024. 24
J. LI, E. Beeching, L. Tunstall, B. Lipkin, R. Soletskyi, S. C. Huang, K. Rasul, L. Yu, A. Jiang, Z. Shen, Z. Qin, B. Dong, L. Zhou, Y. Fleureau, G. Lample, and S. Polu, "NuminaMath," <a href="https: //github.com/project-numina/aimo-progress-prize/blob/ main/report/numina_dataset.pdf">https: //huggingface.co/AI-MO/NuminaMath-CoT</a>, 2024. 27</p>
<p>Qwen, "QwQ: Reflect deeply on the boundaries of the unknown," November 2024. [Online]. Available: https://qwenlm.github.io/blog/qwq-32b-preview/ 27
D. Kocetkov, R. Li, L. B. allal, J. LI, C. Mou, Y. Jernite, M. Mitchell, C. M. Ferrandis, S. Hughes, T. Wolf, D. Bahdanau, L. V. Werra, and H. de Vries, "The Stack: 3 TB of permissively licensed source code," Transactions on Machine Learning Research, 2023. 27
Y. Zheng, R. Zhang, J. Zhang, Y. Ye, and Z. Luo, "LlamaFactory: Unified efficient fine-tuning of 100+ language models," in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Y. Cao, Y. Feng, and D. Xiong, Eds. Bangkok, Thailand: Association for Computational Linguistics, August 2024, pp. 400-410. 35
J. Hu, X. Wu, Z. Zhu, Xianyu, W. Wang, D. Zhang, and Y. Cao, "OpenRLHF: An easy-to-use, scalable and high-performance rlhf framework," arXiv preprint arXiv:2405.11143, 2024. 35
P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei, "Deep reinforcement learning from human preferences," Advances in neural information processing systems, vol. 30, 2017. 35
C. He, R. Luo, Y. Bai, S. Hu, Z. L. Thai, J. Shen, J. Hu, X. Han, Y. Huang, Y. Zhang et al., "Olympiadbench: A challenging benchmark for promoting agi with olympiadlevel bilingual multimodal scientific problems," arXiv preprint arXiv:2402.14008, 2024. 36</p>
<h1>Appendix</h1>
<p>A Satori's Demo Examples ..... 15
B Additional Related Work ..... 24
B. 1 Post-training LLMs for Reasoning ..... 24
B. 2 Enabling LLMs with Searching Abilities ..... 24
C Details about Data Synthesis Framework ..... 25
D Experimental Setup ..... 27
D. 1 Data Processing ..... 27
D.1.1 Prompt Templates ..... 28
D.1.2 Synthetic Data Examples ..... 31
D. 2 Format Tuning ..... 35
D. 3 Reinforcement Learning ..... 35
D. 4 Evaluation Details. ..... 36
E Additional Results ..... 37
E. 1 Ablation on Reflection Bonus ..... 37
E. 2 Ablation on Restart and Explore (RAE) ..... 37
E. 3 Ablation on Preference Bonus ..... 37
E. 4 Offline Restart Buffer v.s. Online Restart Buffer ..... 37</p>
<h2>A. Satori's Demo Examples</h2>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Math Domain Example. Satori verifies the correctness of the intermediate steps and proceeds to the next reasoning step.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Mathematical Reasoning (from AIME2024)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Problems: <br> Every morning Ara gave for a 9-kilometre-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, she walk takes her 4 hours, including $t$ minutes spent in the coffee shop. When she walks $s+2$ kilometers per hour, she walk takes her 2 hours and 16 minutes, including $t$ minutes spent in the coffee shop. Negative Ara walks at $s+2$ kilometers per hour. That the number of minutes she walk takes her, including the $t$ minutes spent in the coffee shop.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Satori's Responses: <br> First, we determine the time Ara spends walking when she walks at a speed of $s$ kilometers per hour. The total time for the walk including the time spent in the coffee shop is 4 hours. Therefore, the time spent walking is:</td>
<td style="text-align: center;">Satori's Responses <br> First, we determine the time Ara spends walking when she walks at a speed of $s+1$ kilometers per hour. The total time for the walk including the time spent in the coffee shop is 4 hours and 24 minutes, which is equivalent to 2.4 hours. Therefore, the time spent walking is:</td>
</tr>
<tr>
<td style="text-align: center;">$4-\frac{1}{16}$ hours</td>
<td style="text-align: center;">$24-\frac{1}{16}$ hours</td>
</tr>
<tr>
<td style="text-align: center;">Since the distance of the walk is 9 kilometers, we can write the equation:</td>
<td style="text-align: center;">Since the distance of the walk is 9 kilometers, we can write the equation:</td>
</tr>
<tr>
<td style="text-align: center;">$s\left(4-\frac{1}{16}\right)=9$</td>
<td style="text-align: center;">$(s+2)\left(4.8-\frac{1}{16}\right)=9$</td>
</tr>
<tr>
<td style="text-align: center;">Next, we determine the time Ara spends walking when she walks at a speed of $s+1$ kilometers per hour. The total time for the walk including the time spent in the coffee shop is 4 hours and 24 minutes, which is equivalent to 2.4 hours. Therefore, the time spent walking is:</td>
<td style="text-align: center;">We now have a system of two equations: $1-s\left(4-\frac{1}{16}\right)=9$ $2\left(s+2\right)\left(2.4-\frac{1}{16}\right)=9$ Let's order these equations step by step. First, expand both equations: $1-4 s-\frac{1}{6 s}=9$ $1-4.8-4.8-\frac{1}{6 s}-\frac{1}{6 s}=9$ Subtract the first equation from the second equation to eliminate $\frac{1}{6 s}$</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; 2.4-\frac{1}{16} \text { hours } \ &amp; \text { Since the distance of the walk is } 9 \text { kilometers, we can write the equation: } \ &amp; (s+2)\left(4.8-\frac{1}{16}\right)=9 \end{aligned}$</td>
<td style="text-align: center;">Solve the quadratic equation using the quadratic formula $s=\frac{-24 s \sqrt{4+48}}{2}$ where $a=3,4=40$, and $c=-225$</td>
</tr>
<tr>
<td style="text-align: center;">We now have a system of two equations: $1-s\left(4-\frac{1}{16}\right)=9$ $2\left(s+2\right)\left(2.4-\frac{1}{16}\right)=9$ Let's order these equations step by step. First, expand both equations: $1-4 s-\frac{1}{6 s}=9$ $1-4.8-4.8-\frac{1}{6 s}-\frac{1}{6 s}=9$ Subtract the first equation from the second equation to eliminate $\frac{1}{6 s}$</td>
<td style="text-align: center;">$4-4 s+\frac{1}{6 s^{2}+48 s-225}{2}=9$ Solve the quadratic equation using the quadratic formula $s=\frac{-24 s \sqrt{4+48}}{2}$ where $a=3,4=40$, and $c=-225$</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; 2.4 s+4.8 \cdot \frac{1}{6 s}-\frac{1}{6 s} \quad \frac{16}{6 s} &amp; =16 s \cdot \frac{16 s}{6 s}=9 \ &amp; 2.4 s+4.8-4 s-\frac{16}{6 s}=0 \end{aligned}$ $\begin{aligned} &amp; -1.6 s+4.8-\frac{16}{6 s}=0 \ &amp; -1.6 s+4.8-\frac{16}{6 s}=0 \ &amp; -1.6 s+4.8-\frac{16}{6 s} \end{aligned}$ $\begin{aligned} &amp; -1.6 s+4.8-\frac{16}{6 s} \quad \frac{16}{6 s}=9 \end{aligned}$ $t=144-48 s$ Substitute $t=144-48 s$ back into the first equation: $s\left(4-\frac{1}{16}\right)=\frac{144-48 s}{6 s}=9$ $s\left(4-\frac{1}{16} \cdot \frac{16}{6 s}\right)=9$ $s\left(4-2.4-\frac{1}{6}\right)=9$ $s\left(1.6+\frac{1}{6}\right)=9$</td>
<td style="text-align: center;">$s\left(4-\frac{1}{16}\right)=9$</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; 2.4 s+4.8 \cdot \frac{1}{6 s}-\frac{16}{6 s} &amp; =16 s \cdot \frac{16 s}{6 s}=9 \ &amp; 2.4 s+4.8-4 s-\frac{16}{6 s}=0 \end{aligned}$ $\begin{aligned} &amp; -1.6 s+4.8-\frac{16}{6 s}=0 \ &amp; -1.6 s+4.8-\frac{16}{6 s}=0 \ &amp; -1.6 s+4.8-\frac{16}{6 s}=0 \end{aligned}$ $\begin{aligned} &amp; 2.4 s+4.8 \cdot \frac{1}{6 s}-\frac{16}{6 s}=9 \end{aligned}$</td>
<td style="text-align: center;">Since $s$ must be positive, we take the positive root: $s=\frac{-24 s \sqrt{4+48}}{2}$ Now, we need to find the time Ara spends walking when she walks at a speed of $s+\frac{1}{2}$ kilometers per hour. The time spent walking is: $\frac{9}{s+\frac{1}{2}}$ <br> Substitute $s=\frac{-24 \sqrt{4+48}}{2}$ <br> $s+\frac{1}{2}=\frac{-24 s \sqrt{4+48}}{2}+\frac{1}{2}=\frac{-9 s \sqrt{4+48}}{2}$ <br> The time spent walking is: $\frac{9}{s \sqrt{4+48}}=\frac{16}{s \sqrt{4+48}}$ <br> Rationalize the denominator: $\frac{16}{s \sqrt{4+48}} \quad \frac{-9-s \sqrt{48}}{2} \quad \frac{16}{s \sqrt{4+48}} \quad \frac{-9}{2 \sqrt{2}}$ $\frac{16-9 s \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9) \sqrt{4+48}}{2 \sqrt{2}$ $\frac{16(9-9)</td>
</tr>
</tbody>
</table>
<h1>Mathematical Reasoning (from MATH)</h1>
<h2>Problem:</h2>
<p>In the diagram, the four points have coordinates $A(0,1), B(1,3), C(5,2)$, and $D(4,0)$. What is the area of quadrilateral $A B C D$ ?</p>
<div class="codehilite"><pre><span></span><code><span class="nt">label</span><span class="o">:</span>
<span class="o">//</span><span class="w"> </span><span class="nt">from</span><span class="w"> </span><span class="nt">axis</span>
<span class="nt">real</span><span class="w"> </span><span class="nt">low</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nt">-1</span><span class="o">;</span><span class="w"> </span><span class="nt">real</span><span class="w"> </span><span class="nt">high</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nt">0</span><span class="o">;</span>
<span class="nt">draw</span><span class="o">(</span><span class="nt">low</span><span class="o">,</span><span class="w"> </span><span class="nt">0</span><span class="o">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="err">high,</span><span class="w"> </span><span class="err">0</span><span class="p">;</span><span class="w"> </span><span class="err">lower</span><span class="p">;</span><span class="w"> </span><span class="err">draw(0),</span><span class="w"> </span><span class="err">low</span><span class="p">}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="err">0,</span><span class="w"> </span><span class="err">high</span><span class="w"> </span><span class="err">-</span><span class="w"> </span><span class="err">1.5</span><span class="p">}</span><span class="o">;</span><span class="w"> </span><span class="nt">down</span><span class="err">}</span><span class="o">;</span>
<span class="nt">label</span><span class="o">(</span><span class="s2">&quot;BAB&quot;</span><span class="o">,</span><span class="w"> </span><span class="p">{</span><span class="err">high,</span><span class="w"> </span><span class="err">0</span><span class="p">;</span><span class="w"> </span><span class="err">0</span><span class="p">}</span><span class="o">;</span><span class="w"> </span><span class="nt">label</span><span class="o">(</span><span class="s2">&quot;BpB&quot;</span><span class="o">,</span><span class="w"> </span><span class="nt">10</span><span class="o">,</span><span class="w"> </span><span class="nt">high</span><span class="w"> </span><span class="nt">-</span><span class="w"> </span><span class="nt">1</span><span class="p">.</span><span class="nc">5</span><span class="o">);</span><span class="w"> </span><span class="nt">0</span><span class="err">}</span><span class="o">;</span>
</code></pre></div>

<p>// draw quadrilateral
pair a = (0, 1); pair b = (1, 2); pair c = (5, 2); pair d = (4, 0);
draw(d = b) = d = b) = b(0,24);
label("BAB", a, b); label("BAB", b, b); label("BAB", c, 5); label("BAB", d, 2);
//pop!</p>
<h2>Satori's Response:</h2>
<p>To find the area of quadrilateral $A B C D$ with vertices $A(0,1), B(1,3), C(5,2)$, and $D(4,0)$, we can use the Shoelace Theorem (also known as Gauss's area formula for polygons). The Shoelace Theorem states that for a polygon with vertices $\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \ldots,\left(x_{n}, y_{n}\right)$, the area $A$ is given by:</p>
<p>$$
A=\frac{1}{2}\left|\sum_{i=1}^{n}\left(x_{i} y_{i+1}-y_{i} x_{i+1}\right)+\left(x_{n} y_{1}-y_{n} x_{1}\right)\right|
$$</p>
<p>First, we list the coordinates of the vertices in order: $A(0,1), B(1,3), C(5,2)$, and $D(4,0)$.
We apply the Shoelace Theorem:</p>
<p>$$
A=\frac{1}{2}\left[0 \cdot 3+1 \cdot 2+5 \cdot 0+4 \cdot 1\right)-(1 \cdot 1+3 \cdot 5+2 \cdot 4+9 \cdot 0)
$$</p>
<p>Calculate the sum of the products of the coordinates:</p>
<p>$$
\begin{aligned}
&amp; \text { Sum of } x_{i} y_{i+1}=0 \cdot 3+1 \cdot 2+5 \cdot 0+4 \cdot 1=0+2+0+4=6 \
&amp; \text { Sum of } y_{i} x_{i+1}=1 \cdot 1+3 \cdot 5+2 \cdot 4+9 \cdot 0=1+15+0+0=24
\end{aligned}
$$</p>
<p>Substitute these sums into the Shoelace Theorem formula:</p>
<p>$$
A=\frac{1}{2}[6-24]-\frac{1}{2}[-18]-\frac{1}{2}+18=9
$$</p>
<p>Therefore, the area of quadrilateral $A B C D$ is:</p>
<p>$$
A=\frac{1}{2}\left[6-24\right]-\frac{1}{2}[-18]-\frac{1}{2}+18=9
$$</p>
<p>Weir: Let's clarify this. The application of the Shoelace Theorem is correct, and the calculations for the sums of the products of the coordinates are accurate, leading to the correct area of $b$. Let's verify the calculation using another method, such as dividing the quadrilateral into two triangles and finding the area of each triangle.
First, we find the sum of triangle $A B D$ :
- vertices $A(0,1), B(1,3), D(4,0)$
Using the formula for the area of a triangle with vertices $\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right)$, and $\left(x_{3}, y_{3}\right)$ :</p>
<p>$$
\text { Area }=\frac{1}{2}\left(x_{1}\left(y_{1}-y_{1}\right)+x_{2}\left(y_{2}-y_{2}\right)+x_{3}\left(y_{3}-y_{3}\right)\right)
$$</p>
<p>For triangle $A B D$ :</p>
<p>$$
\operatorname{Area}_{A B D}=\frac{1}{2}[6(3-0)+1(0-1)+4(1-3)]+\frac{1}{2}[0-1-0]=\frac{1}{2}[-9]+\frac{9}{2}
$$</p>
<p>Next, we find the area of triangle $B C D$ :
- vertices $B(1,3), C(5,2), D(4,0)$
Using the same formula:</p>
<p>$$
\operatorname{Area}_{B C D}=\frac{1}{2}[1(2-0)+5(0-3)+4(3-2)]=\frac{1}{2}[2-15+4]=\frac{1}{2}[-9]+\frac{9}{2}
$$</p>
<p>Finally, we add the areas of the two triangles to get the area of quadrilateral $A B C D$ :</p>
<p>$$
\operatorname{Area}<em A="A" B="B" D="D">{A B C D}=\operatorname{Area}</em>=9
$$}+\operatorname{Area}_{B C D}=\frac{9}{2}+\frac{9}{2</p>
<p>Thus, the area of quadrilateral $A B C D$ is indeed:</p>
<p>Figure 8: Math Domain Example. Satori verifies the correctness of previous solution and initiates a different solution.</p>
<h2>Mathematical Reasoning (from MATH)</h2>
<h2>Problem:</h2>
<p>Given $[\boldsymbol{w}]=5$ and $[\boldsymbol{w}]=8$, find the largest possible value of $[\operatorname{proj}_{\boldsymbol{w}} \boldsymbol{w}]$.</p>
<h2>Satori's Response:</h2>
<p>The formula for the projection of vector $\boldsymbol{w}$ onto vector $\boldsymbol{w}$ is given by:</p>
<p>$$
\operatorname{proj}_{\boldsymbol{w}} \boldsymbol{v}=\frac{\boldsymbol{v} \cdot \boldsymbol{w}}{\boldsymbol{w} \cdot \boldsymbol{w}} \boldsymbol{w}
$$</p>
<p>The magnitude of the projection $[\operatorname{proj}_{\boldsymbol{w}} \boldsymbol{w}]$ is:</p>
<p>$$
[\operatorname{proj}_{\boldsymbol{w}} \boldsymbol{w}]=\frac{1 \cdot \boldsymbol{v} \cdot \boldsymbol{w}}{|\boldsymbol{w} \cdot \boldsymbol{w}|}=\frac{1 \cdot \boldsymbol{w}}{|\boldsymbol{w} \cdot \boldsymbol{w}|}[\boldsymbol{w}]
$$</p>
<p>Since $[\boldsymbol{w}]=8$, we have $\boldsymbol{w} \cdot \boldsymbol{w}=8^{2}=64$. Therefore, the expression for the magnitude of the projection becomes:</p>
<p>$$
[\operatorname{proj}_{\boldsymbol{w}} \boldsymbol{w}]=\frac{1 \cdot \boldsymbol{w}}{|\boldsymbol{w}|}+\frac{9}{2}=\frac{9}{2}
$$</p>
<p>The dot product $\boldsymbol{v} \cdot \boldsymbol{w}$ can be expressed in terms of the magnitudes of $\boldsymbol{v}$ and $\boldsymbol{w}$ and the cosine of the angle $\theta$ between them:</p>
<p>$$
\boldsymbol{v} \cdot \boldsymbol{w}=[\boldsymbol{w}][\boldsymbol{w}] \cos \theta
$$</p>
<p>Given $[\boldsymbol{w}]=5$ and $[\boldsymbol{w}]=8$, we have:</p>
<p>$$
\boldsymbol{v} \cdot \boldsymbol{w}=5 \cdot 8 \cdot \cos \theta=48 \cos \theta
$$</p>
<p>Therefore, the magnitude of the projection is:</p>
<p>$$
[\operatorname{proj}_{\boldsymbol{w}} \boldsymbol{w}]=\frac{18 \cos \theta}{8}=5(\cos \theta)
$$</p>
<p>The maximum value of $(\cos \theta)$ is 1 , which occurs when $\theta=0^{\circ}$ (i.e., when $\boldsymbol{v}$ and $\boldsymbol{w}$ are in the same direction). Thus, the largest possible value of $[\operatorname{proj}_{\boldsymbol{w}} \boldsymbol{w}]$ is:</p>
<p>$$
5 \cdot 1=5
$$</p>
<p>Therefore, the largest possible value of $[\operatorname{proj}<em _boldsymbol_w="\boldsymbol{w">{\boldsymbol{w}} \boldsymbol{w}]$ is $\frac{18}{12}$
Since the mathematics in context will not only result in more concise.
Let's recompute the magnitude of the projection directly using the formula $[\operatorname{proj}</em>] \cos \theta$, so:}} \boldsymbol{w}]=\frac{18 \cdot 12}{12}$. We leave that $\boldsymbol{v} \cdot \boldsymbol{w}=[\boldsymbol{w}][\boldsymbol{w</p>
<p>$$
[\operatorname{proj}_{\boldsymbol{w}} \boldsymbol{w}]=\frac{12 \cdot 12 \cdot 12(\cos \theta)}{[\boldsymbol{w}]^{2}}=[\boldsymbol{w}] \cos \theta
$$</p>
<p>Given $[\boldsymbol{w}]=5$, we have:</p>
<p>$$
[\operatorname{proj}_{\boldsymbol{w}} \boldsymbol{w}]=5(\cos \theta)
$$</p>
<p>The maximum value of $(\cos \theta)$ is 1 , so the maximum value of $[\operatorname{proj}_{\boldsymbol{w}} \boldsymbol{w}]$ is:</p>
<p>$$
5 \cdot 1=5
$$</p>
<p>Therefore, the largest possible value of $[\operatorname{proj}_{\boldsymbol{w}} \boldsymbol{w}]$ is $\frac{12}{12}$</p>
<p>Figure 9: Math Domain Example. Satori verifies the correctness of previous solution and further explores a simpler solution.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Mathematical Reasoning (from OlympiadBench)</th>
<th style="text-align: center;">- For $p=5$ :</th>
<th style="text-align: center;">$\begin{aligned} &amp; 17 \ &amp; 3 \ &amp; 5 \ &amp; 7 \end{aligned}$</th>
<th style="text-align: center;">$\begin{aligned} &amp; 17 \ &amp; 10 \ &amp; 10 \end{aligned}$</th>
<th style="text-align: center;">$&gt;5+1=6$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Problem:</td>
<td style="text-align: center;">- For $p=7$ :</td>
<td style="text-align: center;">$\begin{aligned} &amp; 17 \ &amp; 5 \ &amp; 7 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 17 \ &amp; 10 \end{aligned}$</td>
<td style="text-align: center;">$&gt;2+0=2$</td>
</tr>
<tr>
<td style="text-align: center;">Error's Response:</td>
<td style="text-align: center;">- For $p=7$ :</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">First, we need to find the value of $T+10$</td>
<td style="text-align: center;">- For $p=11$ :</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">17</td>
</tr>
<tr>
<td style="text-align: center;">Next, we need to determine the prime factorization of 17 . The factorial 17 is the product of all positive integers from 1 to 17 .</td>
<td style="text-align: center;">- For $p=13$ :</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">17</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">17</td>
</tr>
<tr>
<td style="text-align: center;">We are interested in the number of positive perfect cubes that are distincts of 17 . A number is a perfect cube if all the exponents in its prime factorization are multiples of 3 .</td>
<td style="text-align: center;">- For $p=13$ :</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">17</td>
</tr>
<tr>
<td style="text-align: center;">Next, let's check that The steps correctly identify the value of $T+10$ and the need to find the prime factorization of 17 to determine the number of perfect cube diviants.</td>
<td style="text-align: center;">- For $p=13$ :</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">17</td>
</tr>
<tr>
<td style="text-align: center;">To find the number of positive perfect cubes that are divisors of 17 , we need to consider the prime factorization of 17 and how the exponents of the prime factors can form a perfect cube.</td>
<td style="text-align: center;">- For $p=17$ :</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">17</td>
</tr>
<tr>
<td style="text-align: center;">First, let's find the prime factorization of 17 . The prime factorization of 17 includes all prime numbers less than or equal to 17 . We need to count the number of these such prime number appears in the factorization.</td>
<td style="text-align: center;">- For $p=17$ :</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">17</td>
</tr>
<tr>
<td style="text-align: center;">For a prime number $p$, the exponent of $p$ in 17 is given by:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">17</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">17</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">17</td>
</tr>
<tr>
<td style="text-align: center;">We will calculate this for each prime number less than or equal to 17 .</td>
<td style="text-align: center;">- For $p=2$ :</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">17</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">17</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">17</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">16</td>
</tr>
</tbody>
</table>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 10: Math Domain Example. 1) Satori verifies the correctness of intermediate steps in early stage. 2) Satori realizes that the pervious solution is actually erroneous and then proposes an alternative correct solution.</p>
<h1>Logical Reasoning (from BoardgameQA)</h1>
<h2>Problem:</h2>
<ol>
<li>Solve the following problem. Your final answer should be "True", "False", or "Causative".</li>
<li>A few players are playing a boardgame. The current state of the game is as follows. The liges has 1 friend that is easy going and 3 friends that are not. The liges is a physiotherapist. The seal has a football with a radius of 22 inches, and is a torches assistant. The seal hides the cards that she has from the bee but does not build a power plant near the given fields of the husky.</li>
<li>And the rules of the game are as follows. Rule1: If the liges works in healthcare, then the liges does not manage to convince the pelikan. Rule2: The seal will bring an oil tank for the pelikan if it (the seal) has a football that the tee is 52.3 x 63.6 x 36.1 inches box. Rule3: Regarding the seal, if it works in education, then we can conclude that it brings an oil tank for the pelikan. Rule4: If you are that something hides the cards that she has from the bee but does not build a power plant near the given fields of the husky, what can you certainly conclude? You can conclude that it does not bring an oil tank for the pelikan. Rule5: For the pelikan, if you have two pieces of evidence (1) the liges manages to convince the pelikan and 2) the seal brings an oil tank for the pelikan, then you can add "pelikan will never bring an oil tank for the dragon" to your conclusions. Rule6: The liges will manage to persuade the pelikan if it (the liges) has fewer than five friends.</li>
<li>Rule7 is preferred over Rule1. Rule2 is preferred over Rule1. Rule6 is preferred over Rule1.</li>
<li>Based on the game state and the rules and preferences, does the pelikan bring an oil tank for the dragon?</li>
</ol>
<h2>Satori's Response:</h2>
<ol>
<li>First, we analyze the given information and rules.</li>
<li>The liges has 1 friend that is easy going and 3 friends that are not.</li>
<li>The liges is a physiotherapist.</li>
<li>The seal has a football with a radius of 22 inches.</li>
<li>The seal hides the cards that she has from the bee but does not build a power plant near the given fields of the husky.</li>
<li>Rule1: If the liges works in healthcare, then the liges does not manage to convince the pelikan.</li>
<li>Rule2: The seal will bring an oil tank for the pelikan if it (the seal) has a football that the tee is 52.3 x 63.6 x 36.1 inches box.</li>
<li>Rule3: Regarding the seal, if it works in education, then we can conclude that it brings an oil tank for the pelikan.</li>
<li>Rule4: If you are that something hides the cards that she has from the bee but does not build a power plant near the given fields of the husky, what can you certainly conclude? You can conclude that it does not bring an oil tank for the pelikan.</li>
<li>Rule5: For the pelikan, if you have two pieces of evidence (1) the liges manages to convince the pelikan and 2) the seal brings an oil tank for the pelikan, then you can add "pelikan will never bring an oil tank for the dragon" to your conclusions.</li>
<li>Rule6: The liges will manage to persuade the pelikan if it (the liges) has fewer than five friends.</li>
</ol>
<p>Note: The message is that the solution starts analyzing the given information and rules without directly addressing the question of whether the pelikan brings an oil tank for the dragon.
Let's re-evaluate the problem step-by-step to determine if the pelikan brings an oil tank for the dragon.
1. Satori Friends and Discussion:
- The liges has 1 friend that is easy going and 3 friends that are not.
- The liges is a physiotherapist.
- According to Rule6: The liges will manage to persuade the pelikan if it (the liges) has fewer than five friends.
- Since the liges has 4 friends ( 1 easy going +2 not), the liges will manage to convince the pelikan.
2. Real's Football and Not:
- The seal has a football with a radius of 22 inches.
- The dimensions of the box are 52.3 x 63.6 x 36.1 inches.
- The volume of the football is given by $V=\frac{4}{3} \pi^{3}$.</p>
<p>$$
\begin{aligned}
V=\frac{4}{3} \pi(22)^{3} &amp; =\frac{4}{3} \pi(10848) \times 44922.67 \text { cubic inches } \
V_{\text {box }} &amp; =\frac{4}{3} \pi \times 63.6 \times 36.1 \times 63257.09 \text { cubic inches }
\end{aligned}
$$</p>
<ul>
<li>Since the volume of the football is less than the volume of the box, the seal's football fits in the box.</li>
<li>According to Rule2: The seal will bring an oil tank for the pelikan if it (the seal) has a football that fits in a 52.3 x 63.6 x 36.1 inches box.</li>
<li>
<p>Therefore, the seal will bring an oil tank for the pelikan.</p>
</li>
<li>
<p>Conclusion Based on Rules:</p>
</li>
<li>From Rule1: If you are that something hides the cards that she has from the bee but does not build a power plant near the given fields of the husky, what can you certainly conclude? You can conclude that it does not bring an oil tank for the pelikan.</li>
<li>The seal hides the cards that she has from the bee but does not build a power plant near the given fields of the husky.</li>
<li>Therefore, the seal does not bring an oil tank for the pelikan.</li>
<li>This creates a contradiction with Rule2, which states that the seal will bring an oil tank for the pelikan.</li>
<li>
<p>Since the rules and the information, the most logical conclusion is that the seal will bring an oil tank for the pelikan, despite the contradiction.</p>
</li>
<li>
<p>Final Conclusion:</p>
</li>
<li>Based on the analysis of the rules and the given information, the seal will bring an oil tank for the pelikan.</li>
</ul>
<p>Therefore, the final answer is "True".
First, The mistake is that the solution starts analyzing the given information and rules without directly addressing the specific question about the pelikan bringing an oil tank for the dragon.
Let's simplify the approach and focus on the direct question. We need to determine if the pelikan brings an oil tank for the dragon based on the given rules and the information about the liges and the seal.
Based on the given information:
- The liges has 4 friends ( 1 easy going and 3 not), so the liges will manage to convince the pelikan (Rule6).
- The seal has a football with a radius of 22 inches, which fits in a 52.3 x 63.6 x 36.1 inches box, so the seal will bring an oil tank for the pelikan (Rule2).
- The seal hides the cards that she has from the bee but does not build a power plant near the given fields of the husky, so according to Rule4, the seal does not bring an oil tank for the pelikan.
- There is a contradiction between Rule2 and Rule1. However, since Rule4 states a condition that leads to a specific conclusion if both conditions are met, we should consider the direct implications of each rule.
- Given the contradiction, we need to re-evaluate the logical flow. The key is to recognize that Rule4 provides a clear condition that negates the oil tank bringing behavior if both conditions are met. Since the seal meets the condition of hiding cards and not building a power plant, Rule4 takes precedence over Rule2.
- Therefore, the pelikan does not bring an oil tank for the dragon.</p>
<p>The final answer is "False".</p>
<p>Figure 11: Out-of-domain Example. 1) Satori identifies the potential mistakes in intermediate steps and initiates another solution. 2) Satori realizes that the pervious solution is still erroneous and then proposes an alternative correct solution.</p>
<h2>Commonssense Reasoning (from StrategyQA)</h2>
<h2>Problem:</h2>
<ol>
<li>You are given the following facts:</li>
<li>Christopher Reeve suffered a serious spinal cord injury that left him a quadriplegic.</li>
<li>Christopher Reeve required a portable ventilator after his injury.</li>
<li>At one point, Christopher Reeve's treatment was costing $\$ 400,000$ yearly.</li>
<li>Based on the facts above, answer the following question. Your final answer should be either "True" or "False".</li>
<li>It is even poor, would Christopher Reeve have lived?</li>
</ol>
<h2>Satori's Response:</h2>
<ol>
<li>The question asks whether Christopher Reeve would have lived if he were poor.</li>
<li>To answer this, we need to consider the impact of his spinal cord injury and the cost of his treatment.</li>
<li>Christopher Reeve's spinal cord injury left him a quadriplegic, which means he was paralyzed from the neck down. This made it difficult for him to move and perform many daily activities.</li>
<li>The fact that he required a portable ventilator indicates that he was dependent on mechanical assistance for breathing.</li>
<li>The cost of his treatment was $\$ 400,000$ yearly, which is a significant amount of money.</li>
<li>Christopher Reeve were poor, he would likely not have been able to afford this level of medical care.</li>
<li>Without the necessary medical treatment, it is highly unlikely that he would have been able to live a normal life.</li>
<li>Table 11.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1</li>
</ol>
<h1>Domain-specific Reasoning (from MMLUPro - Chemistry)</h1>
<h2>Problem:</h2>
<p>Assume all gases are perfect unless stated otherwise. Unless otherwise stated, thermodynamic data are for $208.15 \mathrm{~K}$. A strip of magnesium of mass 15 g is placed in a beaker of dilute hydrochloric acid. Calculate the work done by the system as a result of the reaction. The atmospheric pressure is 1.6 atm and the temperature $25^{\circ} \mathrm{C}$.
$1 \mathrm{~K}-1.5 \mathrm{~K}$
(B) $1 \mathrm{~K} J$ (No work done)
$1 \mathrm{C}-0.5 \mathrm{~K} \mathrm{\zeta}$
$1 \mathrm{D}-0.5 \mathrm{~K} \mathrm{\zeta}$
$1 \mathrm{D}-2.0 \mathrm{~K} \mathrm{\zeta}$
( $\mathrm{F}-2.5 \mathrm{~K}$ ) (Positive work done by the system)
$1 \mathrm{D}-2.0 \mathrm{~K} \mathrm{\zeta}$
$1 \mathrm{D}-1.0 \mathrm{~K} \mathrm{\zeta}$
( $\mathrm{I}-2.0 \mathrm{~K}$ )
( $\mathrm{J}-1.0 \mathrm{~K}$ ) (Positive work done by the system)</p>
<h2>Satori's Response:</h2>
<p>The reaction between magnesium and hydrochloric acid is:</p>
<p>$$
\mathrm{Mg}+\mathrm{2HCl} \rightarrow \mathrm{MgCl}<em 2="2">{2}+\mathrm{H}</em>
$$</p>
<p>The work done by the system as a result of the reaction is given by the formula:</p>
<p>$$
\omega=-P \Delta V
$$</p>
<p>where $P$ is the atmospheric pressure and $\Delta V$ is the change in volume of the system.
First, we need to determine the amount of hydrogen gas produced. The balanced equation shows that 1 mole of magnesium produces 1 mole of hydrogen gas.
The molar mass of magnesium is $24.31 \mathrm{~g} / \mathrm{mol}$. Therefore, the number of moles of magnesium is 15 g in</p>
<p>$$
\text { moles of } \mathrm{Mg}=\frac{15 \mathrm{~g}}{24.31 \mathrm{~g} / \mathrm{mol}}=0.617 \mathrm{~mol}
$$</p>
<p>Since 1 mole of magnesium produces 1 mole of hydrogen gas, the number of moles of hydrogen gas produced is also 0.617 mol .
The volume of hydrogen gas produced at $25^{\circ} \mathrm{C}\left(288.15 \mathrm{~K}\right)$ and 1.0 atm can be calculated using the ideal gas law:</p>
<p>$$
V=\omega H T / P
$$</p>
<p>where $\omega$ is the number of moles of gas, $H$ is the ideal gas constant ( $0.00288 \mathrm{~L} \cdot \mathrm{~atm} /(\mathrm{mol} \cdot \mathrm{K}))$. $T$ is the temperature in Kelvin, and $P$ is the pressure in atmospheres.
Note: Let's clarify this. The steps correctly identify the reaction, the formula for work, and the calculation of the moles of magnesium and hydrogen gas produced, but the volume calculation is unnecessary for determining the work done by the system in this context.
The work done by the system is zero because the reaction occurs in a beaker and the volume change is negligible under the given conditions. The system does not expand or compress against the external pressure.
Therefore, the work done by the system is: $\omega=0.4 \mathrm{~J}$
The correct answer is $\Delta \mathrm{J}$.
Note: The rationale is that the volume change is incorrectly assumed to be negligible, leading to the incorrect conclusion that we work is done.
Let's switch to a new solution. Calculate the volume of hydrogen gas produced using the ideal gas law and then use it to find the work done by the system.
Let's re-evaluate the problem przylicately.
1. Determine the amount of hydrogen gas produced:</p>
<p>The balanced equation is:</p>
<p>$$
\mathrm{Mg}+\mathrm{2HCl} \rightarrow \mathrm{MgCl}<em 2="2">{2}+\mathrm{H}</em>
$$</p>
<p>From the equation, 1 mole of Mg produces 1 mole of $\mathrm{H}_{2}$.
The molar mass of Mg is $24.31 \mathrm{~g} / \mathrm{mol}$. Therefore, the number of moles of Mg in 15 g is:</p>
<p>$$
\text { moles of } \mathrm{Mg}=\frac{15 \mathrm{~g}}{24.31 \mathrm{~g} / \mathrm{mol}}=0.617 \mathrm{~mol}
$$</p>
<p>So, the number of moles of $\mathrm{H}_{2}$ produced is also 0.617 mol .
2. Calculate the volume of hydrogen gas produced:</p>
<p>Using the ideal gas law $V=\omega H T / P$, where $\omega=0.617 \mathrm{~mol}, H=0.00288 \mathrm{~L} \cdot \mathrm{~atm} /(\mathrm{mol} \cdot \mathrm{K}), T=288.15 \mathrm{~K}$, and $P=1.0$ atm:</p>
<p>$$
V=0.617 \mathrm{~mol}+0.00288 \mathrm{~L} \cdot \mathrm{~atm} /(\mathrm{mol} \cdot \mathrm{K})+288.15 \mathrm{~K} / 1.0 \mathrm{~atm}=15.2 \mathrm{~L}
$$</p>
<ol>
<li>Calculate the work done by the system:</li>
</ol>
<p>The work done by the system is given by $\omega=-P \Delta V$. Since the reaction occurs in a beaker and the volume change is significant, we need to consider the external pressure and the change in volume.</p>
<p>$$
\omega=-1.0 \mathrm{~atm}+15.2 \mathrm{~L}=-15.2 \mathrm{~L} \cdot \mathrm{~atm}
$$</p>
<p>Convert L .atm to kJ using the conversion factor $1 \mathrm{~L} \cdot \mathrm{~atm}=101.325 \mathrm{~J}$.</p>
<p>$$
\omega=-15.2 \mathrm{~L} \cdot \mathrm{~atm}=\frac{101.325 \mathrm{~J}}{1 \mathrm{~L} \cdot \mathrm{~atm}}=\frac{1 \mathrm{~kJ}}{3480 \mathrm{~J}}=-1.54 \mathrm{~kJ}
$$</p>
<p>Therefore, the work done by the system is approximately -1.5 kJ .
The correct answer is $\Delta \mathrm{J}$.</p>
<p>Figure 13: Out-of-domain Example. 1) Satori verifies the correctness of intermediate steps in early stage. 2) Satori realizes that the pervious solution is actually erroneous and then proposes an alternative correct solution.</p>
<h1>Domain-specific Reasoning (from MMLUPro - Economics)</h1>
<h2>Problems:</h2>
<p>What does the Harrod-Domar model, taken with the Keynesian theory of savings, imply for the growth rates of poor countries?
(A) Risk and poor countries will experience the same growth rates regardless of savings rates
(B) Poor countries will have fluctuating growth rates independent of savings
(C) The level of technology determines growth rates, not savings
(D) The level of income has no impact on savings
(E) Savings have no correlation with growth rates in any country
(F) Risk countries will have low growth rates
(G) Poor countries will have high growth rates
(H) High levels of savings will always lead to high growth rates
(I) Poor countries will have low growth rates
(J) Poor countries can achieve high growth rates regardless of investment levels</p>
<h2>Ration's Response:</h2>
<p>The Harrod-Domar model is an economic model that relates the growth rate of an economy to its savings rate and its capital-output ratio. The model suggests that the growth rate of an economy is determined by the ratio of savings to capital, and that this ratio is influenced by the capital-output ratio.</p>
<p>Note: Let's clarify this. The Harrod-Domar model indeed relates the growth rate to the savings rate and the capital-output ratio, which is a correct starting point for analyzing the growth rates of poor countries.</p>
<p>The Harrod-Domar model states that the growth rate $g$ of an economy is given by:</p>
<p>$$
g=\frac{1}{k}
$$</p>
<p>where $g$ is the savings rate and $k$ is the capital-output ratio.
Note: The explanations of the Harrod-Domar model is correct but the step does not directly address the implications for poor countries in the context of the Keynesian theory of savings.
Let's switch to a new solution. Work by explaining how the Harrod-Domar model, when combined with the Keynesian theory of savings, suggests that higher savings rates in poor countries can lead to higher growth rates due to increased investment.</p>
<p>The Harrod-Domar model suggests that the growth rate of an economy is determined by the ratio of savings to capital. According to the Keynesian theory of savings, countries with higher savings rates will have more funds available for investment.</p>
<p>In poor countries, when the capital stock is often lower relative to their population, a higher savings rate can lead to a higher capital stock over time. This increase in capital stock can then support higher levels of output and, consequently, higher growth rates.</p>
<p>Note: The explanations does not directly address the options provided in the question, which are specific to the growth rates of poor countries.
Let's switch to a new solution. Analyse each option to determine which one best fits the implications of the Harrod-Domar model and the Keynesian theory of savings for poor countries.
Let's re-evaluate the options given.
(A) Risk and poor countries will experience the same growth rates regardless of savings rates.</p>
<p>This is not true because the Harrod-Domar model suggests that growth rates are determined by the savings rate and the capital-output ratio, which can differ between rich and poor countries.
(B) Poor countries will have fluctuating growth rates independent of savings.</p>
<p>This is not supported by the Harrod-Domar model, which suggests that growth rates are directly related to savings rates.
(C) The level of technology determines growth rates, not savings.</p>
<p>This is not true according to the Harrod-Domar model, which explicitly states that growth rates are determined by the savings rate and the capital-output ratio.
(D) The level of income has no impact on savings.</p>
<p>This is not true according to the Keynesian theory of savings, which suggests that higher income can lead to higher savings.
(E) Savings have no correlation with growth rates in any country.</p>
<p>This is not true according to the Harrod-Domar model, which shows a direct relationship between savings and growth.
(F) Risk countries will have low growth rates.</p>
<p>This is not necessarily true. Risk countries may have higher growth rates if they have high savings rates and a low capital-output ratio.
(G) Poor countries will have high growth rates.</p>
<p>This is not necessarily true. Poor countries may have lower growth rates if they have low savings rates and a high capital-output ratio.
(H) High levels of savings will always lead to high growth rates.</p>
<p>This is not true because the Harrod-Domar model also considers the capital-output ratio. High savings rates alone are not sufficient for high growth rates.
(I) Poor countries will have low growth rates.</p>
<p>This is a possible implication, but it depends on the specific savings and capital-output ratios of poor countries.
(J) Poor countries can achieve high growth rates regardless of investment levels.</p>
<p>This is not true according to the Harrod-Domar model, which suggests that high growth rates require high savings rates and a low capital-output ratio.
Given the analysis, the most accurate statement based on the Harrod-Domar model and the Keynesian theory of savings is that poor countries can achieve high growth rates if they have high savings rates and a low capital-output ratio.
Therefore, the final answer is $[f]$.</p>
<p>Figure 14: Out-of-domain Example. Satori engages in multiple self-reflection processes during intermediate reasoning steps.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>Core contributors of Satori team, contributed equally to this work. Order determined by a dice roll.
${ }^{1}$ MIT ${ }^{2}$ Singapore University of Technology and Design ${ }^{3}$ Harvard ${ }^{4}$ MIT-IBM Watson AI Lab, IBM Research ${ }^{5}$ UMass Amherst. Correspondence to: Maohao Shen $&lt;$ maohao@mit.edu $&gt;$.</li>
</ul>
<p>Proceedings of the $42^{\text {nd }}$ International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s).
${ }^{1}$ https://satori-reasoning.github.io/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
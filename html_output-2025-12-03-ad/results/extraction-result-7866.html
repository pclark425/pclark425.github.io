<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7866 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7866</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7866</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-273350620</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.11594v1.pdf" target="_blank">Black-box Uncertainty Quantification Method for LLM-as-a-Judge</a></p>
                <p><strong>Paper Abstract:</strong> LLM-as-a-Judge is a widely used method for evaluating the performance of Large Language Models (LLMs) across various tasks. We address the challenge of quantifying the uncertainty of LLM-as-a-Judge evaluations. While uncertainty quantification has been well-studied in other domains, applying it effectively to LLMs poses unique challenges due to their complex decision-making capabilities and computational demands. In this paper, we introduce a novel method for quantifying uncertainty designed to enhance the trustworthiness of LLM-as-a-Judge evaluations. The method quantifies uncertainty by analyzing the relationships between generated assessments and possible ratings. By cross-evaluating these relationships and constructing a confusion matrix based on token probabilities, the method derives labels of high or low uncertainty. We evaluate our method across multiple benchmarks, demonstrating a strong correlation between the accuracy of LLM evaluations and the derived uncertainty scores. Our findings suggest that this method can significantly improve the reliability and consistency of LLM-as-a-Judge evaluations.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7866.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7866.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TruthfulQA_Llama-3-8B_lowU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-Judge (Llama-3-8B-Instruct) vs. human annotations on TruthfulQA (low-uncertainty subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reports accuracy of Llama-3-8B-Instruct on TruthfulQA when the paper's confusion-based method labels evaluations as low uncertainty (claims low-uncertainty correlates with higher accuracy versus baseline human-match).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Black-box Uncertainty Quantification Method for LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Truthfulness classification (binary truthful/untruthful)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Llama-3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Instruct-tuned LLaMA-3 family model, ~8B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Dataset-provided human annotations / human raters (TruthfulQA annotations)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>accuracy (LLM matches human rater)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.68</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>coverage of low-uncertainty labels; computational cost of confusion method; potential variability across models/tasks</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Low-uncertainty-labeled predictions have higher accuracy than baseline; low-uncertainty correlates with correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Can produce uncertainty-labelled evaluations where low-uncertainty reliably indicates higher accuracy; potential to reach or exceed human agreement on low-uncertainty cases.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Confusion-based uncertainty: generate n biased assessments then n^2 confusion prompts; threshold α grid-searched per dataset/model; stratified sampling; accuracy measured as match to human rater.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Black-box Uncertainty Quantification Method for LLM-as-a-Judge', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7866.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7866.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TruthfulQA_Llama-3-70B_lowU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-Judge (Llama-3-70B-Instruct) vs. human annotations on TruthfulQA (low-uncertainty subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reports accuracy of Llama-3-70B-Instruct on TruthfulQA for evaluations labeled low uncertainty by confusion-based method.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Black-box Uncertainty Quantification Method for LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Truthfulness classification (binary)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Llama-3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Instruct-tuned LLaMA-3 family model, ~70B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Dataset-provided human annotations / human raters (TruthfulQA annotations)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>accuracy (LLM matches human rater)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.91</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>computational intensity for large models; variable generalizability to non-instruct or non-fine-tuned models</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Low-uncertainty predictions substantially outperform baseline and correspond to higher correctness; larger model yields more low-uncertainty labels and higher low-U accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Larger instruct-tuned models produce a higher share of low-uncertainty evaluations with higher accuracy, approaching or exceeding human agreement in low-U subset.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Same confusion-probe protocol; threshold tuned via grid search; comparisons of high vs low uncertainty accuracy reported per model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Black-box Uncertainty Quantification Method for LLM-as-a-Judge', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7866.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7866.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TruthfulQA_Mixtral8B_lowU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-Judge (Mixtral-8x7B-Instruct-v01) vs. human annotations on TruthfulQA (low-uncertainty subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reports accuracy of Mixtral-8x7B-Instruct-v01 on TruthfulQA when labelled low uncertainty by the confusion-based method.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Black-box Uncertainty Quantification Method for LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Truthfulness classification (binary)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Mixtral-8x7B-Instruct-v01</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Instruct-tuned model, ~8B parameters (Mixtral series)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Dataset-provided human annotations / human raters</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>accuracy (LLM matches human rater)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.81</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>low proportion of low-uncertainty labels for smaller models; computational overhead</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Low-uncertainty predictions show higher accuracy than baseline; smaller models produce fewer low-U labels.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Provides uncertainty signals that correlate with correctness even for smaller models, though coverage is lower.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Confusion-based approach with n assessments and n^2 confusion prompts; threshold chosen per dataset/model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Black-box Uncertainty Quantification Method for LLM-as-a-Judge', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7866.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7866.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reliance_Llama-3-8B_lowU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-Judge (Llama-3-8B-Instruct) vs. human annotations on Reliance Study (low-uncertainty subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reports accuracy of Llama-3-8B on Reliance Study dataset when labelled low uncertainty; measures match to human raters on relevance/naturalness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Black-box Uncertainty Quantification Method for LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Binary evaluation of accuracy/naturalness (Reliance Study)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Reliance Study</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Llama-3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Instruct-tuned LLaMA-3 ~8B</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Dataset-provided human annotations / human raters</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>accuracy (LLM matches human rater)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.5</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>small models classify very few cases as low-uncertainty (<5% in some datasets), limiting coverage</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Low-uncertainty labels correspond to higher alignment with human raters, though proportion of such labels is small for smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>When low-uncertainty, LLM judgments more likely to match humans, enabling targeted human review for high-uncertainty cases.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Stratified sampling by criteria; confusion prompts to compute token log-probabilities; threshold grid search per model/dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Black-box Uncertainty Quantification Method for LLM-as-a-Judge', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7866.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7866.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reliance_Llama-3-70B_lowU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-Judge (Llama-3-70B-Instruct) vs. human annotations on Reliance Study (low-uncertainty subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reports accuracy of Llama-3-70B on Reliance Study for low-uncertainty labeled evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Black-box Uncertainty Quantification Method for LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Binary evaluation (accuracy/naturalness)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Reliance Study</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Llama-3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Instruct-tuned LLaMA-3 ~70B</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Dataset-provided human annotations / human raters</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>accuracy (LLM matches human rater)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.85</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>computational cost for large models; need for fine-tuning/prompt engineering in new tasks</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Larger model produces more low-uncertainty labels and higher low-U accuracy, indicating better alignment with human judgments on those cases.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Higher coverage of reliable (low-U) judgments compared to smaller models; potential to approach human-level agreement in low-U subset.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Confusion-matrix based elicitation; per-dataset threshold optimization; evaluations compared to human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Black-box Uncertainty Quantification Method for LLM-as-a-Judge', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7866.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7866.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reliance_Mixtral8B_lowU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-Judge (Mixtral-8x7B-Instruct-v01) vs. human annotations on Reliance Study (low-uncertainty subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reports low-uncertainty accuracy of Mixtral-8x7B on Reliance Study as measured against human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Black-box Uncertainty Quantification Method for LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Binary evaluation (accuracy/naturalness)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Reliance Study</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Mixtral-8x7B-Instruct-v01</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Instruct-tuned Mixtral ~8B</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Dataset-provided human annotations / human raters</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>accuracy (LLM matches human rater)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>1.0</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>low proportion of low-U labels in small models; risk of overconfidence not directly measured here but noted in related work</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Low-U cases can reach very high accuracy, but coverage may be sparse for smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>High precision for the subset of cases labeled low uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>n assessments -> n^2 confusion prompts; averaging token probabilities per option; threshold α chosen by grid search.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Black-box Uncertainty Quantification Method for LLM-as-a-Judge', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7866.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7866.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FeedbackCollection_Llama-3-8B_lowU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-Judge (Llama-3-8B-Instruct) vs. human ratings on Feedback Collection (low-uncertainty subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reports that on Feedback Collection dataset low-uncertainty predictions from Llama-3-8B achieve perfect accuracy (1.00) though proportion of such cases is low.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Black-box Uncertainty Quantification Method for LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Fine-grained rating (1-5) for feedback quality</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Feedback Collection</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Llama-3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Instruct-tuned LLaMA-3 ~8B</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Dataset-provided human ratings (fine-grained), human raters</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>accuracy (LLM matches human rater)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>1.0</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>very low coverage of low-uncertainty labels (<10%); computational cost; generalizability concerns</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>When the method yields low-uncertainty on this dataset, predictions are fully consistent with human labels, but such cases are rare for smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Enables identification of a small subset of highly-reliable automatic evaluations, useful for scaling with limited human resources.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Threshold tuned per dataset/model; low-U proportion measured and reported; assessments biased to each option then confusion prompts executed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Black-box Uncertainty Quantification Method for LLM-as-a-Judge', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7866.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7866.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FeedbackCollection_Llama-3-70B_lowU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-Judge (Llama-3-70B-Instruct) vs. human ratings on Feedback Collection (low-uncertainty subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Llama-3-70B low-uncertainty evaluations on Feedback Collection achieve perfect accuracy (1.00) per Table 1 despite a small low-U proportion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Black-box Uncertainty Quantification Method for LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Fine-grained rating (1-5)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Feedback Collection</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Llama-3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Instruct-tuned LLaMA-3 ~70B</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Dataset-provided human ratings</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>accuracy (LLM matches human rater)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>1.0</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>high computational cost for larger models; need for prompt engineering to boost coverage</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Large model achieves very high accuracy on low-U cases; larger models produce a higher share of low-U labels than smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Higher coverage of reliable judgments and perfect low-U accuracy on this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Confusion-based protocol; threshold chosen by grid search; stratified sampling across criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Black-box Uncertainty Quantification Method for LLM-as-a-Judge', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7866.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7866.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FeedbackCollection_Mixtral8B_lowU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-Judge (Mixtral-8x7B-Instruct-v01) vs. human ratings on Feedback Collection (low-uncertainty subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mixtral-8x7B low-uncertainty predictions on Feedback Collection attain perfect accuracy (1.00) though low-U coverage is low.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Black-box Uncertainty Quantification Method for LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Fine-grained rating (1-5)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Feedback Collection</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Mixtral-8x7B-Instruct-v01</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Instruct-tuned Mixtral ~8B</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Dataset-provided human ratings</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>accuracy (LLM matches human rater)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>1.0</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>low coverage of low-U cases; computational intensity</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Even smaller instruct models can produce perfectly accurate low-U predictions, but these constitute a small fraction of cases.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Allows selective automation of high-confidence evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>n assessments and n^2 confusion prompts; threshold tuned via grid search.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Black-box Uncertainty Quantification Method for LLM-as-a-Judge', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7866.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7866.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Summarization_Llama-3-8B_lowU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-Judge (Llama-3-8B-Instruct) vs. human raters on Summarization CNN/DM (low-uncertainty subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reports per-model accuracies for summarization rating task; low-uncertainty predictions for Llama-3-8B achieve 0.75 accuracy, compared to human inter-rater agreement of 0.60.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Black-box Uncertainty Quantification Method for LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Summarization quality rating (1-5 scales such as coherence/fluency/relevance)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Summarization CNN/DM</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Llama-3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Instruct-tuned LLaMA-3 ~8B</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Multiple human raters (dataset-provided), inter-rater human agreement reported</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>accuracy (LLM matches human rater); human inter-rater agreement reported</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.75</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>smaller models less reliable; decreased sparsity as option count increases causing more ambiguous probabilities; computational cost</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Low-uncertainty LLM predictions can exceed human inter-rater agreement (0.60) for this model/dataset; low-U substantially reduces deviation from correct ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>When labeled low uncertainty, LLM ratings can match or surpass human inter-rater consistency, suggesting usefulness for partially automated evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Multi-class (1-5) evaluation; confusion prompts per option; threshold grid search; compared low-U, high-U, and baseline accuracies to human agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Black-box Uncertainty Quantification Method for LLM-as-a-Judge', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7866.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7866.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Summarization_Llama-3-70B_lowU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-Judge (Llama-3-70B-Instruct) vs. human raters on Summarization CNN/DM (low-uncertainty subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Llama-3-70B low-uncertainty accuracy on Summarization CNN/DM reported as 0.26 (Table 1) while human inter-rater agreement is 0.60; baseline and high-U values also reported.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Black-box Uncertainty Quantification Method for LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Summarization quality rating (1-5)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Summarization CNN/DM</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Llama-3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Instruct-tuned LLaMA-3 ~70B</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Multiple human raters (dataset-provided), inter-rater agreement available</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>accuracy (LLM matches human rater); human inter-rater agreement reported</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.26</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>inconsistent low-U gains across models (one large model shows low-U accuracy lower than human agreement here), sensitivity to threshold choice</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Low-uncertainty does not always align with higher accuracy across all model-dataset combinations; performance varies by model size and dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Method still provides uncertainty labeling that can highlight cases for human review even when low-U accuracy is not high.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Per-option assessments for 1-5 rating scale, confusion matrix construction, averaging token probabilities to derive uncertainty label; threshold optimization per model/dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Black-box Uncertainty Quantification Method for LLM-as-a-Judge', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7866.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7866.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Summarization_Mixtral8B_lowU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-Judge (Mixtral-8x7B-Instruct-v01) vs. human raters on Summarization CNN/DM (low-uncertainty subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mixtral low-uncertainty accuracy reported as 0.50 on Summarization CNN/DM compared to human agreement 0.60; baseline and high/U values reported.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Black-box Uncertainty Quantification Method for LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Summarization quality rating (1-5)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Summarization CNN/DM</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Mixtral-8x7B-Instruct-v01</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Instruct-tuned Mixtral ~8B</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Multiple human raters (dataset-provided)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>accuracy (LLM matches human rater); human inter-rater agreement reported</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.5</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>smaller models produce fewer low-U labels; low-U accuracy varied across models</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Presence of low-U labels reduces deviation from correct ratings; low-U accuracy can approach human agreement but is model- and dataset-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Can produce higher-precision judgments in the small subset labelled low uncertainty, potentially reducing human workload.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Confusion-based uncertainty for multi-class ratings; threshold grid search; stratified evaluation across criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Black-box Uncertainty Quantification Method for LLM-as-a-Judge', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7866.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e7866.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FeedbackQA_Llama-3-8B_lowU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-Judge (Llama-3-8B-Instruct) vs. human ratings on FeedbackQA (low-uncertainty subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>On FeedbackQA, low-uncertainty Llama-3-8B predictions achieve perfect accuracy (1.00) compared to human inter-rater agreement of 0.48 as reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Black-box Uncertainty Quantification Method for LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Retrieval-based QA rating (excellent to bad) with natural-language explanations</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>FeedbackQA</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Llama-3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Instruct-tuned LLaMA-3 ~8B</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Dataset-provided human ratings (multiple raters), inter-rater agreement reported</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>accuracy (LLM matches human rater); human inter-rater agreement also reported</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>1.0</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>low coverage of low-U labels; potential lack of generalizability beyond instruct-tuned models; computational cost</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Low-U predictions can match or exceed human inter-rater agreement, but these cases may be a small proportion of the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Ability to identify high-confidence judgments that align strongly with human raters, enabling selective automation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Confusion-based uncertainty measured across rated QA pairs; threshold per model/dataset; compared low-U and baseline accuracies to human agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Black-box Uncertainty Quantification Method for LLM-as-a-Judge', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7866.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e7866.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FeedbackQA_Llama-3-70B_lowU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-Judge (Llama-3-70B-Instruct) vs. human ratings on FeedbackQA (low-uncertainty subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Llama-3-70B low-U accuracy on FeedbackQA reported as 0.71 (Table 1) versus human inter-rater agreement 0.48.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Black-box Uncertainty Quantification Method for LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>FeedbackQA rating (excellent to bad)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>FeedbackQA</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Llama-3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Instruct-tuned LLaMA-3 ~70B</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Multiple human raters (dataset-provided), inter-rater agreement reported</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>accuracy (LLM matches human rater); human inter-rater agreement available</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.71</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>method computationally expensive; coverage of low-U cases increases with model size but still limited; threshold sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Large model shows improved low-U accuracy that can exceed human inter-rater agreement on this dataset; variability across models persists.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Higher proportion of reliable (low-U) judgments compared to smaller models; potential to exceed human inter-rater consistency on those subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Confusion-probing with biased assessments; token log-probabilities used to build confusion matrix; averaging across columns yields u_i; threshold α determines low/high uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Black-box Uncertainty Quantification Method for LLM-as-a-Judge', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7866.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e7866.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FeedbackQA_Mixtral8B_lowU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-Judge (Mixtral-8x7B-Instruct-v01) vs. human ratings on FeedbackQA (low-uncertainty subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mixtral low-uncertainty accuracy on FeedbackQA reported as 1.00 (Table 1) compared with human inter-rater agreement 0.48.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Black-box Uncertainty Quantification Method for LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>FeedbackQA rating</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>FeedbackQA</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Mixtral-8x7B-Instruct-v01</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Instruct-tuned Mixtral ~8B</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Multiple human raters (dataset-provided)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>accuracy (LLM matches human rater); human inter-rater agreement reported</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>1.0</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>sparse low-U coverage; reliance on prompt design and threshold; heavy compute</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Perfect low-U accuracy reported for Mixtral on this dataset, but likely low coverage of such cases; highlights trade-off between precision and coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>High precision in small subset of automated evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>As above: generate n assessments, create n^2 confusion prompts, compute token probabilities, average into u_i, label using threshold α tuned per dataset/model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Black-box Uncertainty Quantification Method for LLM-as-a-Judge', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7866.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e7866.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>General_comparison_and_limitations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>General comparisons between LLM-as-a-Judge and human evaluations and reported limitations/failure modes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper-wide statements comparing LLM judge outputs to human judgments: notes both agreements and divergences, lists limitations (computational cost, sensitivity to prompt/threshold, limited generalizability) and failure modes (scattered token probabilities, susceptibility to being convinced under biased assessments, decreased sparsity with many options).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Black-box Uncertainty Quantification Method for LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>General across multiple evaluation tasks (summarization, QA, truthfulness, feedback rating)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Multiple (TruthfulQA, Reliance Study, Summarization CNN/DM, Feedback Collection, FeedbackQA)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Mixtral-8x7B-Instruct-v01; Llama-3-8B-Instruct; Llama-3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Instruct-tuned models of sizes ~8B and ~70B; behavior compared across sizes</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Dataset-provided human raters / human annotations (multiple raters for some datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>accuracy (LLM matches human rater); inter-rater human agreement reported for some datasets</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>computationally intensive O(n^2) inference; variable performance on non-instruct or non-fine-tuned models; low coverage of low-U cases for smaller models; sensitivity to threshold choice; potential overconfidence (cited in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Low-uncertainty labels generally correlate with higher accuracy and can approach or exceed human inter-rater agreement on some datasets; proportion of low-U labels depends on model size and dataset; failure modes include confusion matrices with arbitrary token probability distributions and reduced sparsity as option count grows.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Scalability potential, ability to produce per-evaluation uncertainty labels to prioritize human review, and in low-U cases deliver accuracy comparable to or exceeding inter-human agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Black-box token-probability based confusion prompts with assessment and confusion stages; threshold α tuned via grid search; stratified sampling across criteria; measured baseline, high-U, low-U accuracies and compared to human agreement where available.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Black-box Uncertainty Quantification Method for LLM-as-a-Judge', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks <em>(Rating: 2)</em></li>
                <li>Finding blind spots in evaluator llms with interpretable checklists <em>(Rating: 2)</em></li>
                <li>Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms <em>(Rating: 2)</em></li>
                <li>Large language models are not fair evaluators <em>(Rating: 2)</em></li>
                <li>Prometheus: Inducing evaluation capability in language models <em>(Rating: 1)</em></li>
                <li>Prometheus 2: An open source language model specialized in evaluating other language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7866",
    "paper_id": "paper-273350620",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "TruthfulQA_Llama-3-8B_lowU",
            "name_full": "Comparison of LLM-as-a-Judge (Llama-3-8B-Instruct) vs. human annotations on TruthfulQA (low-uncertainty subset)",
            "brief_description": "Reports accuracy of Llama-3-8B-Instruct on TruthfulQA when the paper's confusion-based method labels evaluations as low uncertainty (claims low-uncertainty correlates with higher accuracy versus baseline human-match).",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
            "evaluation_task": "Truthfulness classification (binary truthful/untruthful)",
            "dataset_name": "TruthfulQA",
            "judge_model_name": "Llama-3-8B-Instruct",
            "judge_model_details": "Instruct-tuned LLaMA-3 family model, ~8B parameters",
            "human_evaluator_type": "Dataset-provided human annotations / human raters (TruthfulQA annotations)",
            "agreement_metric": "accuracy (LLM matches human rater)",
            "agreement_score": 0.68,
            "reported_loss_aspects": "coverage of low-uncertainty labels; computational cost of confusion method; potential variability across models/tasks",
            "qualitative_findings": "Low-uncertainty-labeled predictions have higher accuracy than baseline; low-uncertainty correlates with correctness.",
            "advantages_of_llm_judge": "Can produce uncertainty-labelled evaluations where low-uncertainty reliably indicates higher accuracy; potential to reach or exceed human agreement on low-uncertainty cases.",
            "experimental_setting": "Confusion-based uncertainty: generate n biased assessments then n^2 confusion prompts; threshold α grid-searched per dataset/model; stratified sampling; accuracy measured as match to human rater.",
            "uuid": "e7866.0",
            "source_info": {
                "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "TruthfulQA_Llama-3-70B_lowU",
            "name_full": "Comparison of LLM-as-a-Judge (Llama-3-70B-Instruct) vs. human annotations on TruthfulQA (low-uncertainty subset)",
            "brief_description": "Reports accuracy of Llama-3-70B-Instruct on TruthfulQA for evaluations labeled low uncertainty by confusion-based method.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
            "evaluation_task": "Truthfulness classification (binary)",
            "dataset_name": "TruthfulQA",
            "judge_model_name": "Llama-3-70B-Instruct",
            "judge_model_details": "Instruct-tuned LLaMA-3 family model, ~70B parameters",
            "human_evaluator_type": "Dataset-provided human annotations / human raters (TruthfulQA annotations)",
            "agreement_metric": "accuracy (LLM matches human rater)",
            "agreement_score": 0.91,
            "reported_loss_aspects": "computational intensity for large models; variable generalizability to non-instruct or non-fine-tuned models",
            "qualitative_findings": "Low-uncertainty predictions substantially outperform baseline and correspond to higher correctness; larger model yields more low-uncertainty labels and higher low-U accuracy.",
            "advantages_of_llm_judge": "Larger instruct-tuned models produce a higher share of low-uncertainty evaluations with higher accuracy, approaching or exceeding human agreement in low-U subset.",
            "experimental_setting": "Same confusion-probe protocol; threshold tuned via grid search; comparisons of high vs low uncertainty accuracy reported per model.",
            "uuid": "e7866.1",
            "source_info": {
                "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "TruthfulQA_Mixtral8B_lowU",
            "name_full": "Comparison of LLM-as-a-Judge (Mixtral-8x7B-Instruct-v01) vs. human annotations on TruthfulQA (low-uncertainty subset)",
            "brief_description": "Reports accuracy of Mixtral-8x7B-Instruct-v01 on TruthfulQA when labelled low uncertainty by the confusion-based method.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
            "evaluation_task": "Truthfulness classification (binary)",
            "dataset_name": "TruthfulQA",
            "judge_model_name": "Mixtral-8x7B-Instruct-v01",
            "judge_model_details": "Instruct-tuned model, ~8B parameters (Mixtral series)",
            "human_evaluator_type": "Dataset-provided human annotations / human raters",
            "agreement_metric": "accuracy (LLM matches human rater)",
            "agreement_score": 0.81,
            "reported_loss_aspects": "low proportion of low-uncertainty labels for smaller models; computational overhead",
            "qualitative_findings": "Low-uncertainty predictions show higher accuracy than baseline; smaller models produce fewer low-U labels.",
            "advantages_of_llm_judge": "Provides uncertainty signals that correlate with correctness even for smaller models, though coverage is lower.",
            "experimental_setting": "Confusion-based approach with n assessments and n^2 confusion prompts; threshold chosen per dataset/model.",
            "uuid": "e7866.2",
            "source_info": {
                "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Reliance_Llama-3-8B_lowU",
            "name_full": "Comparison of LLM-as-a-Judge (Llama-3-8B-Instruct) vs. human annotations on Reliance Study (low-uncertainty subset)",
            "brief_description": "Reports accuracy of Llama-3-8B on Reliance Study dataset when labelled low uncertainty; measures match to human raters on relevance/naturalness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
            "evaluation_task": "Binary evaluation of accuracy/naturalness (Reliance Study)",
            "dataset_name": "Reliance Study",
            "judge_model_name": "Llama-3-8B-Instruct",
            "judge_model_details": "Instruct-tuned LLaMA-3 ~8B",
            "human_evaluator_type": "Dataset-provided human annotations / human raters",
            "agreement_metric": "accuracy (LLM matches human rater)",
            "agreement_score": 0.5,
            "reported_loss_aspects": "small models classify very few cases as low-uncertainty (&lt;5% in some datasets), limiting coverage",
            "qualitative_findings": "Low-uncertainty labels correspond to higher alignment with human raters, though proportion of such labels is small for smaller models.",
            "advantages_of_llm_judge": "When low-uncertainty, LLM judgments more likely to match humans, enabling targeted human review for high-uncertainty cases.",
            "experimental_setting": "Stratified sampling by criteria; confusion prompts to compute token log-probabilities; threshold grid search per model/dataset.",
            "uuid": "e7866.3",
            "source_info": {
                "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Reliance_Llama-3-70B_lowU",
            "name_full": "Comparison of LLM-as-a-Judge (Llama-3-70B-Instruct) vs. human annotations on Reliance Study (low-uncertainty subset)",
            "brief_description": "Reports accuracy of Llama-3-70B on Reliance Study for low-uncertainty labeled evaluations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
            "evaluation_task": "Binary evaluation (accuracy/naturalness)",
            "dataset_name": "Reliance Study",
            "judge_model_name": "Llama-3-70B-Instruct",
            "judge_model_details": "Instruct-tuned LLaMA-3 ~70B",
            "human_evaluator_type": "Dataset-provided human annotations / human raters",
            "agreement_metric": "accuracy (LLM matches human rater)",
            "agreement_score": 0.85,
            "reported_loss_aspects": "computational cost for large models; need for fine-tuning/prompt engineering in new tasks",
            "qualitative_findings": "Larger model produces more low-uncertainty labels and higher low-U accuracy, indicating better alignment with human judgments on those cases.",
            "advantages_of_llm_judge": "Higher coverage of reliable (low-U) judgments compared to smaller models; potential to approach human-level agreement in low-U subset.",
            "experimental_setting": "Confusion-matrix based elicitation; per-dataset threshold optimization; evaluations compared to human labels.",
            "uuid": "e7866.4",
            "source_info": {
                "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Reliance_Mixtral8B_lowU",
            "name_full": "Comparison of LLM-as-a-Judge (Mixtral-8x7B-Instruct-v01) vs. human annotations on Reliance Study (low-uncertainty subset)",
            "brief_description": "Reports low-uncertainty accuracy of Mixtral-8x7B on Reliance Study as measured against human annotations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
            "evaluation_task": "Binary evaluation (accuracy/naturalness)",
            "dataset_name": "Reliance Study",
            "judge_model_name": "Mixtral-8x7B-Instruct-v01",
            "judge_model_details": "Instruct-tuned Mixtral ~8B",
            "human_evaluator_type": "Dataset-provided human annotations / human raters",
            "agreement_metric": "accuracy (LLM matches human rater)",
            "agreement_score": 1.0,
            "reported_loss_aspects": "low proportion of low-U labels in small models; risk of overconfidence not directly measured here but noted in related work",
            "qualitative_findings": "Low-U cases can reach very high accuracy, but coverage may be sparse for smaller models.",
            "advantages_of_llm_judge": "High precision for the subset of cases labeled low uncertainty.",
            "experimental_setting": "n assessments -&gt; n^2 confusion prompts; averaging token probabilities per option; threshold α chosen by grid search.",
            "uuid": "e7866.5",
            "source_info": {
                "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "FeedbackCollection_Llama-3-8B_lowU",
            "name_full": "Comparison of LLM-as-a-Judge (Llama-3-8B-Instruct) vs. human ratings on Feedback Collection (low-uncertainty subset)",
            "brief_description": "Reports that on Feedback Collection dataset low-uncertainty predictions from Llama-3-8B achieve perfect accuracy (1.00) though proportion of such cases is low.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
            "evaluation_task": "Fine-grained rating (1-5) for feedback quality",
            "dataset_name": "Feedback Collection",
            "judge_model_name": "Llama-3-8B-Instruct",
            "judge_model_details": "Instruct-tuned LLaMA-3 ~8B",
            "human_evaluator_type": "Dataset-provided human ratings (fine-grained), human raters",
            "agreement_metric": "accuracy (LLM matches human rater)",
            "agreement_score": 1.0,
            "reported_loss_aspects": "very low coverage of low-uncertainty labels (&lt;10%); computational cost; generalizability concerns",
            "qualitative_findings": "When the method yields low-uncertainty on this dataset, predictions are fully consistent with human labels, but such cases are rare for smaller models.",
            "advantages_of_llm_judge": "Enables identification of a small subset of highly-reliable automatic evaluations, useful for scaling with limited human resources.",
            "experimental_setting": "Threshold tuned per dataset/model; low-U proportion measured and reported; assessments biased to each option then confusion prompts executed.",
            "uuid": "e7866.6",
            "source_info": {
                "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "FeedbackCollection_Llama-3-70B_lowU",
            "name_full": "Comparison of LLM-as-a-Judge (Llama-3-70B-Instruct) vs. human ratings on Feedback Collection (low-uncertainty subset)",
            "brief_description": "Llama-3-70B low-uncertainty evaluations on Feedback Collection achieve perfect accuracy (1.00) per Table 1 despite a small low-U proportion.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
            "evaluation_task": "Fine-grained rating (1-5)",
            "dataset_name": "Feedback Collection",
            "judge_model_name": "Llama-3-70B-Instruct",
            "judge_model_details": "Instruct-tuned LLaMA-3 ~70B",
            "human_evaluator_type": "Dataset-provided human ratings",
            "agreement_metric": "accuracy (LLM matches human rater)",
            "agreement_score": 1.0,
            "reported_loss_aspects": "high computational cost for larger models; need for prompt engineering to boost coverage",
            "qualitative_findings": "Large model achieves very high accuracy on low-U cases; larger models produce a higher share of low-U labels than smaller models.",
            "advantages_of_llm_judge": "Higher coverage of reliable judgments and perfect low-U accuracy on this dataset.",
            "experimental_setting": "Confusion-based protocol; threshold chosen by grid search; stratified sampling across criteria.",
            "uuid": "e7866.7",
            "source_info": {
                "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "FeedbackCollection_Mixtral8B_lowU",
            "name_full": "Comparison of LLM-as-a-Judge (Mixtral-8x7B-Instruct-v01) vs. human ratings on Feedback Collection (low-uncertainty subset)",
            "brief_description": "Mixtral-8x7B low-uncertainty predictions on Feedback Collection attain perfect accuracy (1.00) though low-U coverage is low.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
            "evaluation_task": "Fine-grained rating (1-5)",
            "dataset_name": "Feedback Collection",
            "judge_model_name": "Mixtral-8x7B-Instruct-v01",
            "judge_model_details": "Instruct-tuned Mixtral ~8B",
            "human_evaluator_type": "Dataset-provided human ratings",
            "agreement_metric": "accuracy (LLM matches human rater)",
            "agreement_score": 1.0,
            "reported_loss_aspects": "low coverage of low-U cases; computational intensity",
            "qualitative_findings": "Even smaller instruct models can produce perfectly accurate low-U predictions, but these constitute a small fraction of cases.",
            "advantages_of_llm_judge": "Allows selective automation of high-confidence evaluations.",
            "experimental_setting": "n assessments and n^2 confusion prompts; threshold tuned via grid search.",
            "uuid": "e7866.8",
            "source_info": {
                "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Summarization_Llama-3-8B_lowU",
            "name_full": "Comparison of LLM-as-a-Judge (Llama-3-8B-Instruct) vs. human raters on Summarization CNN/DM (low-uncertainty subset)",
            "brief_description": "Reports per-model accuracies for summarization rating task; low-uncertainty predictions for Llama-3-8B achieve 0.75 accuracy, compared to human inter-rater agreement of 0.60.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
            "evaluation_task": "Summarization quality rating (1-5 scales such as coherence/fluency/relevance)",
            "dataset_name": "Summarization CNN/DM",
            "judge_model_name": "Llama-3-8B-Instruct",
            "judge_model_details": "Instruct-tuned LLaMA-3 ~8B",
            "human_evaluator_type": "Multiple human raters (dataset-provided), inter-rater human agreement reported",
            "agreement_metric": "accuracy (LLM matches human rater); human inter-rater agreement reported",
            "agreement_score": 0.75,
            "reported_loss_aspects": "smaller models less reliable; decreased sparsity as option count increases causing more ambiguous probabilities; computational cost",
            "qualitative_findings": "Low-uncertainty LLM predictions can exceed human inter-rater agreement (0.60) for this model/dataset; low-U substantially reduces deviation from correct ratings.",
            "advantages_of_llm_judge": "When labeled low uncertainty, LLM ratings can match or surpass human inter-rater consistency, suggesting usefulness for partially automated evaluation.",
            "experimental_setting": "Multi-class (1-5) evaluation; confusion prompts per option; threshold grid search; compared low-U, high-U, and baseline accuracies to human agreement.",
            "uuid": "e7866.9",
            "source_info": {
                "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Summarization_Llama-3-70B_lowU",
            "name_full": "Comparison of LLM-as-a-Judge (Llama-3-70B-Instruct) vs. human raters on Summarization CNN/DM (low-uncertainty subset)",
            "brief_description": "Llama-3-70B low-uncertainty accuracy on Summarization CNN/DM reported as 0.26 (Table 1) while human inter-rater agreement is 0.60; baseline and high-U values also reported.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
            "evaluation_task": "Summarization quality rating (1-5)",
            "dataset_name": "Summarization CNN/DM",
            "judge_model_name": "Llama-3-70B-Instruct",
            "judge_model_details": "Instruct-tuned LLaMA-3 ~70B",
            "human_evaluator_type": "Multiple human raters (dataset-provided), inter-rater agreement available",
            "agreement_metric": "accuracy (LLM matches human rater); human inter-rater agreement reported",
            "agreement_score": 0.26,
            "reported_loss_aspects": "inconsistent low-U gains across models (one large model shows low-U accuracy lower than human agreement here), sensitivity to threshold choice",
            "qualitative_findings": "Low-uncertainty does not always align with higher accuracy across all model-dataset combinations; performance varies by model size and dataset.",
            "advantages_of_llm_judge": "Method still provides uncertainty labeling that can highlight cases for human review even when low-U accuracy is not high.",
            "experimental_setting": "Per-option assessments for 1-5 rating scale, confusion matrix construction, averaging token probabilities to derive uncertainty label; threshold optimization per model/dataset.",
            "uuid": "e7866.10",
            "source_info": {
                "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Summarization_Mixtral8B_lowU",
            "name_full": "Comparison of LLM-as-a-Judge (Mixtral-8x7B-Instruct-v01) vs. human raters on Summarization CNN/DM (low-uncertainty subset)",
            "brief_description": "Mixtral low-uncertainty accuracy reported as 0.50 on Summarization CNN/DM compared to human agreement 0.60; baseline and high/U values reported.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
            "evaluation_task": "Summarization quality rating (1-5)",
            "dataset_name": "Summarization CNN/DM",
            "judge_model_name": "Mixtral-8x7B-Instruct-v01",
            "judge_model_details": "Instruct-tuned Mixtral ~8B",
            "human_evaluator_type": "Multiple human raters (dataset-provided)",
            "agreement_metric": "accuracy (LLM matches human rater); human inter-rater agreement reported",
            "agreement_score": 0.5,
            "reported_loss_aspects": "smaller models produce fewer low-U labels; low-U accuracy varied across models",
            "qualitative_findings": "Presence of low-U labels reduces deviation from correct ratings; low-U accuracy can approach human agreement but is model- and dataset-dependent.",
            "advantages_of_llm_judge": "Can produce higher-precision judgments in the small subset labelled low uncertainty, potentially reducing human workload.",
            "experimental_setting": "Confusion-based uncertainty for multi-class ratings; threshold grid search; stratified evaluation across criteria.",
            "uuid": "e7866.11",
            "source_info": {
                "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "FeedbackQA_Llama-3-8B_lowU",
            "name_full": "Comparison of LLM-as-a-Judge (Llama-3-8B-Instruct) vs. human ratings on FeedbackQA (low-uncertainty subset)",
            "brief_description": "On FeedbackQA, low-uncertainty Llama-3-8B predictions achieve perfect accuracy (1.00) compared to human inter-rater agreement of 0.48 as reported in Table 1.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
            "evaluation_task": "Retrieval-based QA rating (excellent to bad) with natural-language explanations",
            "dataset_name": "FeedbackQA",
            "judge_model_name": "Llama-3-8B-Instruct",
            "judge_model_details": "Instruct-tuned LLaMA-3 ~8B",
            "human_evaluator_type": "Dataset-provided human ratings (multiple raters), inter-rater agreement reported",
            "agreement_metric": "accuracy (LLM matches human rater); human inter-rater agreement also reported",
            "agreement_score": 1.0,
            "reported_loss_aspects": "low coverage of low-U labels; potential lack of generalizability beyond instruct-tuned models; computational cost",
            "qualitative_findings": "Low-U predictions can match or exceed human inter-rater agreement, but these cases may be a small proportion of the dataset.",
            "advantages_of_llm_judge": "Ability to identify high-confidence judgments that align strongly with human raters, enabling selective automation.",
            "experimental_setting": "Confusion-based uncertainty measured across rated QA pairs; threshold per model/dataset; compared low-U and baseline accuracies to human agreement.",
            "uuid": "e7866.12",
            "source_info": {
                "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "FeedbackQA_Llama-3-70B_lowU",
            "name_full": "Comparison of LLM-as-a-Judge (Llama-3-70B-Instruct) vs. human ratings on FeedbackQA (low-uncertainty subset)",
            "brief_description": "Llama-3-70B low-U accuracy on FeedbackQA reported as 0.71 (Table 1) versus human inter-rater agreement 0.48.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
            "evaluation_task": "FeedbackQA rating (excellent to bad)",
            "dataset_name": "FeedbackQA",
            "judge_model_name": "Llama-3-70B-Instruct",
            "judge_model_details": "Instruct-tuned LLaMA-3 ~70B",
            "human_evaluator_type": "Multiple human raters (dataset-provided), inter-rater agreement reported",
            "agreement_metric": "accuracy (LLM matches human rater); human inter-rater agreement available",
            "agreement_score": 0.71,
            "reported_loss_aspects": "method computationally expensive; coverage of low-U cases increases with model size but still limited; threshold sensitivity",
            "qualitative_findings": "Large model shows improved low-U accuracy that can exceed human inter-rater agreement on this dataset; variability across models persists.",
            "advantages_of_llm_judge": "Higher proportion of reliable (low-U) judgments compared to smaller models; potential to exceed human inter-rater consistency on those subsets.",
            "experimental_setting": "Confusion-probing with biased assessments; token log-probabilities used to build confusion matrix; averaging across columns yields u_i; threshold α determines low/high uncertainty.",
            "uuid": "e7866.13",
            "source_info": {
                "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "FeedbackQA_Mixtral8B_lowU",
            "name_full": "Comparison of LLM-as-a-Judge (Mixtral-8x7B-Instruct-v01) vs. human ratings on FeedbackQA (low-uncertainty subset)",
            "brief_description": "Mixtral low-uncertainty accuracy on FeedbackQA reported as 1.00 (Table 1) compared with human inter-rater agreement 0.48.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
            "evaluation_task": "FeedbackQA rating",
            "dataset_name": "FeedbackQA",
            "judge_model_name": "Mixtral-8x7B-Instruct-v01",
            "judge_model_details": "Instruct-tuned Mixtral ~8B",
            "human_evaluator_type": "Multiple human raters (dataset-provided)",
            "agreement_metric": "accuracy (LLM matches human rater); human inter-rater agreement reported",
            "agreement_score": 1.0,
            "reported_loss_aspects": "sparse low-U coverage; reliance on prompt design and threshold; heavy compute",
            "qualitative_findings": "Perfect low-U accuracy reported for Mixtral on this dataset, but likely low coverage of such cases; highlights trade-off between precision and coverage.",
            "advantages_of_llm_judge": "High precision in small subset of automated evaluations.",
            "experimental_setting": "As above: generate n assessments, create n^2 confusion prompts, compute token probabilities, average into u_i, label using threshold α tuned per dataset/model.",
            "uuid": "e7866.14",
            "source_info": {
                "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "General_comparison_and_limitations",
            "name_full": "General comparisons between LLM-as-a-Judge and human evaluations and reported limitations/failure modes",
            "brief_description": "Paper-wide statements comparing LLM judge outputs to human judgments: notes both agreements and divergences, lists limitations (computational cost, sensitivity to prompt/threshold, limited generalizability) and failure modes (scattered token probabilities, susceptibility to being convinced under biased assessments, decreased sparsity with many options).",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
            "evaluation_task": "General across multiple evaluation tasks (summarization, QA, truthfulness, feedback rating)",
            "dataset_name": "Multiple (TruthfulQA, Reliance Study, Summarization CNN/DM, Feedback Collection, FeedbackQA)",
            "judge_model_name": "Mixtral-8x7B-Instruct-v01; Llama-3-8B-Instruct; Llama-3-70B-Instruct",
            "judge_model_details": "Instruct-tuned models of sizes ~8B and ~70B; behavior compared across sizes",
            "human_evaluator_type": "Dataset-provided human raters / human annotations (multiple raters for some datasets)",
            "agreement_metric": "accuracy (LLM matches human rater); inter-rater human agreement reported for some datasets",
            "agreement_score": null,
            "reported_loss_aspects": "computationally intensive O(n^2) inference; variable performance on non-instruct or non-fine-tuned models; low coverage of low-U cases for smaller models; sensitivity to threshold choice; potential overconfidence (cited in related work)",
            "qualitative_findings": "Low-uncertainty labels generally correlate with higher accuracy and can approach or exceed human inter-rater agreement on some datasets; proportion of low-U labels depends on model size and dataset; failure modes include confusion matrices with arbitrary token probability distributions and reduced sparsity as option count grows.",
            "advantages_of_llm_judge": "Scalability potential, ability to produce per-evaluation uncertainty labels to prioritize human review, and in low-U cases deliver accuracy comparable to or exceeding inter-human agreement.",
            "experimental_setting": "Black-box token-probability based confusion prompts with assessment and confusion stages; threshold α tuned via grid search; stratified sampling across criteria; measured baseline, high-U, low-U accuracies and compared to human agreement where available.",
            "uuid": "e7866.15",
            "source_info": {
                "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks",
            "rating": 2,
            "sanitized_title": "llms_instead_of_human_judges_a_large_scale_empirical_study_across_20_nlp_evaluation_tasks"
        },
        {
            "paper_title": "Finding blind spots in evaluator llms with interpretable checklists",
            "rating": 2,
            "sanitized_title": "finding_blind_spots_in_evaluator_llms_with_interpretable_checklists"
        },
        {
            "paper_title": "Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms",
            "rating": 2,
            "sanitized_title": "can_llms_express_their_uncertainty_an_empirical_evaluation_of_confidence_elicitation_in_llms"
        },
        {
            "paper_title": "Large language models are not fair evaluators",
            "rating": 2,
            "sanitized_title": "large_language_models_are_not_fair_evaluators"
        },
        {
            "paper_title": "Prometheus: Inducing evaluation capability in language models",
            "rating": 1,
            "sanitized_title": "prometheus_inducing_evaluation_capability_in_language_models"
        },
        {
            "paper_title": "Prometheus 2: An open source language model specialized in evaluating other language models",
            "rating": 1,
            "sanitized_title": "prometheus_2_an_open_source_language_model_specialized_in_evaluating_other_language_models"
        }
    ],
    "cost": 0.0194355,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Black-box Uncertainty Quantification Method for LLM-as-a-Judge
15 Oct 2024</p>
<p>Nico Wagner nico.wagner@ 
IBM Research</p>
<p>Michael Desmond mdesmond@us. 
IBM Research</p>
<p>Rahul Nair rahul.nair@ie. 
IBM Research</p>
<p>Zahra Ashktorab zahra.ashktorab1@ 
IBM Research</p>
<p>Elizabeth M Daly elizabeth.daly@ie. 
IBM Research</p>
<p>Qian Pan qian.pan@ 
IBM Research</p>
<p>Martín Santillán Cooper msantillancooper@ 
IBM Research</p>
<p>James M Johnson jmjohnson@us. 
IBM Research</p>
<p>Werner Geyer werner.geyer@us.ibm.com 
IBM Research</p>
<p>Black-box Uncertainty Quantification Method for LLM-as-a-Judge
15 Oct 2024F4BECD1BCD398B601BCAA2788DA7FA6CarXiv:2410.11594v1[cs.LG]
LLM-as-a-Judge is a widely used method for evaluating the performance of Large Language Models (LLMs) across various tasks.We address the challenge of quantifying the uncertainty of LLM-as-a-Judge evaluations.While uncertainty quantification has been well-studied in other domains, applying it effectively to LLMs poses unique challenges due to their complex decision-making capabilities and computational demands.In this paper, we introduce a novel method for quantifying uncertainty designed to enhance the trustworthiness of LLM-as-a-Judge evaluations.The method quantifies uncertainty by analyzing the relationships between generated assessments and possible ratings.By cross-evaluating these relationships and constructing a confusion matrix based on token probabilities, the method derives labels of high or low uncertainty.We evaluate our method across multiple benchmarks, demonstrating a strong correlation between the accuracy of LLM evaluations and the derived uncertainty scores.Our findings suggest that this method can significantly improve the reliability and consistency of LLM-as-a-Judge evaluations.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have become integral to a wide range of tasks, including questionanswering [24], summarization [12], translation [30], concept extraction [5], classification [8], and reasoning [10].The evaluation of the texts they generate has emerged as a significant challenge due to data contamination [1], replicability, and the use of standard metrics or benchmarks [17,7,6] which may not cover all dimensions of use case-specific evaluations.</p>
<p>An emerging method for evaluating generated content involves using other LLMs as evaluators, a method referred to as LLM-as-a-Judge [31].These evaluations can take various forms, including explanations, numeric values, or categorical ratings.This work specifically focuses on LLM-as-a-Judge methods that employ categorical ratings or numerical evaluations to assess generated outputs.</p>
<p>Despite their widespread use, LLM-as-a-Judge methods do not always align with human judgments, leading to instances where the evaluations may be incorrect or misleading [27,2].This divergence highlights the need to assess the trustworthiness of LLM-generated evaluations.Various techniques have been proposed to enhance the performance of LLMs and improve the reliability of their judgments [26].</p>
<p>To improve trustworthiness in LLM-as-a-Judge evaluations and to leverage the strengths of these methods, we introduce a novel approach called confusion-based uncertainty.Our method is designed 38th Conference on Neural Information Processing Systems (NeurIPS 2024).</p>
<p>Figure 1: A biased assessment prompt.The LLM is prompted to asses a response (an input text that is under evaluation) under the assumption that a particular output option (label) is correct.By producing biased assessments, it is possible to determine the LLM's belief in a correct output option subject to assessments that may be contrary to this belief.</p>
<p>to quantify the uncertainty associated with LLM evaluations where evaluation outcomes are discrete, i.e. multiple choice settings, or fixed number of output options.This encompasses a majority of typical evaluation tasks included those involving human evaluations.</p>
<p>The confusion-based uncertainty approach, inspired by chain-of-thought reasoning [28] and confusion matrices [3], first prompts the judge LLM to generate an assessment for each potential output option, biased on the implication that the option is correct.An assessment is an open ended evaluation produced by the LLM prior to making a final judgement.A biased assessment is generated under the implication that a given option is correct (see Figure 1).For each biased assessment, the probability of all output options is recorded using log probabilities.This facilitates an analysis of the relationship between the biased assessments and the LLMs belief in a particular output being correct.A confusion matrix is constructed from these combinations, and levels of uncertainty are derived from the confusion matrix by looking at the distribution of token probabilities for output options, subject to each of the biased assessments.If an option is consistently likely across all potentially biased assessments, it is deemed to have low uncertainty.</p>
<p>The goal of confusion-based uncertainty is to label LLM-as-a-Judge evaluations with high or low uncertainty, offering a clear signal of the evaluations' likely accuracy.We empirically evaluate our confusion-based uncertainty method across diverse benchmarks and models.Our results indicate that low uncertainty ratings correlate with higher accuracy, and the method effectively transfers across datasets and models.</p>
<p>Related Work</p>
<p>The capabilities of large language models have rapidly advanced, leading to their application in increasingly complex tasks [25].However, the growing sophistication of these models also raises new challenges, particularly in evaluating their outputs and understanding their uncertainty [9].As models are increasingly used as evaluators, what is often referred to as LLM-as-a-Judge, it becomes essential to develop methods that allow these models not only to generate responses but also to assess the confidence and reliability of those responses.</p>
<p>A promising line of research addresses these challenges through methods like chain-of-thought reasoning [28] and self-reflection [11].Chain-of-thought reasoning enhances an LLM's ability to arrive at more accurate conclusions by breaking down complex tasks into intermediate logical steps.By guiding the model through a series of smaller, connected reasoning steps, the model's decisionmaking process becomes more transparent and robust.Self-reflection, on the other hand, encourages the model to review and critique its own responses, iterating upon initial outputs to refine and improve its accuracy.Both techniques align with the broader framework of agentic design patterns [21], which foster active engagement by the model in evaluating and refining its generated outputs.These reasoning-based approaches are particularly relevant to the growing body of work on LLM-asa-Judge, where LLMs are used to evaluate data and provide judgments that are comparable to human annotations.Several works have studied LLM-as-a-Judge with evaluations generally focused on correlations between labels generated by LLMs and human annotations.For example, [31,13] and report strong agreements with human annotations, while other studies have reported mixed results [4,2].Some works have proposed using ensembles of smaller models to increase performance [26], while others have used instruction fine-tuning to build customised evaluators [13].</p>
<p>Several works have explored methods for uncertainty estimation in the context of large language models.One approach is calibration-based uncertainty quantification, introduced by [23], which focuses on efficient calibration using a auxiliary model trained over multiple tasks.However, this approach relies on internal model representations to produce features.In contrast, black-box uncertainty quantification methods, which do not require access to the internal workings of the model, have also emerged.Lin etal.[19] investigated prompting strategies that guide LLMs to verbalize their uncertainty, especially when fine-tuned with labeled confidence values.Their work demonstrates how external methods can elicit uncertainty without accessing internal model states.</p>
<p>Xiong et al. [29] evaluated several prompting strategies for eliciting uncertainty in LLMs, including direct assessment, chain-of-thought reasoning, and self-probing.Their findings indicate that models tend to exhibit overconfidence, particularly in general settings.To address this, they proposed strategies such as prompt perturbation, paraphrasing, and entity amplification, which reduced overconfidence and improved uncertainty predictions.They also developed a logistic regression model to predict uncertainty based on these perturbations [22].</p>
<p>Kuhn et al. [15] introduced semantic entropy as a novel approach to capture uncertainty by identifying semantically equivalent prompts.By measuring the variation in responses to these semantically similar prompts, their method provides a more nuanced understanding of model uncertainty.</p>
<p>Confusion-based Uncertainty</p>
<p>In many LLM-as-a-Judge frameworks, the evaluation of generated text is conducted against predefined criteria [14].Each criterion consists of a question and a set of options, among which the LLM must choose.The questions can vary widely, and the options can be defined as numeric values, words, or any other format, with no restriction on the number of words or the nature of the options.</p>
<p>Our proposed technique introduces an uncertainty measure that is calculated independently of the specific decision made by the LLM.This approach aims to enhance the trustworthiness of LLM-as-a-Judge evaluations.The method works in four key steps: generating verbalized assessments, creating prompts for the confusion matrix, constructing the confusion matrix, and setting uncertainty labels.See Figure 2. The LLM is first presented with an evaluation task, and prompted to produce an assessment for each output option, biased on the explicit indication that the option is correct.In the context of the original evaluation task, the LLM is conditioned on each of the biased assessments, and the probability of each option calculated using log probabilities.This information is then encoded in a confusion matrix.Each row of the matrix, representing the probability of a particular option conditioned on each of the biased assessments, is then averaged to produce an uncertainty label.In this figure, α represents the threshold.</p>
<p>Generating Assessments The initial step in our approach involves generating verbalized assessments for each of the n options presented in the criterion.Using the prompt template shown in Figure 3, we apply prompt engineering techniques to guide the LLM toward treating a specific option as correct.This compels the model to generate justifications for why that particular option is the best choice.Each assessment is explicitly linked to one of the available options, ensuring that the reasoning is directly associated with the selected alternative.</p>
<p>The prompts are designed to persuade the LLM that the option in question is correct, leading to a set of n assessments, one for each option.Creating Confusion Prompts After generating assessments, the next step involves creating prompts that will be used to build the confusion matrix.This is done by mixing each assessment with every option, effectively producing a comprehensive set of prompts that cover all possible pairings of assessments and options.The prompt template (see Figure 5) is structured as a conversation between the LLM and the user, where two tasks are presented as separate requests.First, the LLM is asked to generate an assessment for which option is correct without injecting any prompts.Second, the LLM is prompted to choose the correct option based on the assessment.The assessments and options generated in the previous step are inserted as responses from the LLM.</p>
<p>For a criterion with n options, this process results in the creation of n 2 prompts, as each assessment is mixed with every possible option.</p>
<p>Constructing the Confusion Matrix With the n 2 prompts created, the next step is to send these prompts to the LLM to obtain the token probabilities associated with the final decision.The probability of the last token in the response is used to calculate an uncertainty score for the chosen option.These probabilities are organized into a confusion matrix, where each row corresponds to an option from the prompt, and each column corresponds to an assessment generated for a specific option (see Figure 4.A confusion matrix labeled as low uncertainty exhibits high token probabilities concentrated in a single row.In contrast, a matrix labeled as high uncertainty either shows high token probabilities along the diagonal, where the assessments align with the corresponding options, or has high token probabilities scattered arbitrarily across the matrix.Setting Uncertainty Labels The final step in the method involves assigning an uncertainty label, either high or low uncertainty, to the chosen option based on the confusion matrix and predefined threshold.The labeling process follows these rules:</p>
<p>Options Assessments
A 1 A 2 . . . A m         O 1 p 1,1 p 1,2 • • • p 1,m O 2 p 2,1 p 2,2 • • • p 2,m . . . . . . . . . . . . . . . O n p n,1 p n,2 • • • p n,m
• If only one row in the matrix exceeds the uncertainty threshold and this row corresponds to the LLM's initially chosen option, the option is labeled as low uncertainty.</p>
<p>• If more than one row exceeds the uncertainty threshold, the option is labeled as high uncertainty.</p>
<p>• If the option identified with low uncertainty in the confusion matrix does not match the LLM's originally chosen option, the option is labeled as high uncertainty.</p>
<p>• If no row exceeds the uncertainty threshold, the option is labeled as high uncertainty.</p>
<p>This labeling process allows the method to differentiate between evaluations that the LLM is likely confident in and those that may require further scrutiny.The overall goal is to enhance the reliability and trustworthiness of LLM-as-a-Judge evaluations by providing an additional layer of certainty assessment.</p>
<p>Formal Description</p>
<p>Formally, the method can be described as follows.Consider a question q with n possible outcomes o i , where i ∈ {1, 2, . . ., n}.For example, a multiple choice question with four answers, o i can take on values A/B/C/D with n = 4.With each q as context, we consider two prompts, q a an assessment prompt and q c a confusion prompt.The assessment prompt (see Figure 3) generates assessments a i = q a (o i ) ∀i ∈ {1, 2, . . ., n} for each possible discrete outcome.</p>
<p>The confusion prompt (see Figure [5]), considers all combinations of outcomes and assessments for the question, i.e. q c (o i , a j ) denotes a prompt using assessment a j and target label o i .While the assessment prompt generates additional tokens, the confusion prompt is used only to determine the probabilities of the output token(s).The confusion matrix C consists of elements
p ij = p (o i | q c (o i , a j )) , ∀i, j ∈ {1, 2, . . . , n},(1)
where p ij denotes the probability of token o i when the assessment relates to the j-th outcome.This matrix forms the basis for the uncertainty quantification.The main intuition is that if the probability of token o i is high regardless of the assessments, then the model has low uncertainty in its prediction.In contrast, if the token probability follows the assessment, we infer that the model has high uncertainty in its answer.</p>
<p>The uncertainty associated with a specific token can then be estimated by taking the mean token probability across all confusing prompts, i.e.
u i = 1 n j p ij , ∀i ∈ {1, 2, . . . , n}.(2)
For analysis, in this paper we further label the uncertainty of the overall assessment using a threshold α, i.e.
l = low uncertainty if i 1(u i ≥ α) = 1, ∀i ∈ {1, 2, . . . , n}, high uncertainty otherwise.(3)
In other words, the assessment has low uncertainty if the mean token probability exceeds the threshold for exactly a single token.The procedure involves n inferencing calls for the first stage and n 2 inferences for the second stage as it works through all combinations of outcome labels making the overall estimation procedure O(n 2 ).</p>
<p>Threshold</p>
<p>The threshold acts as a crucial parameter in determining the balance between the proportion of low uncertainty and accuracy.Defining an optimal threshold depends on the specific requirements of the use case.For applications such as content filtering or large-scale feedback collection, where there are a large number of evaluations but limited human resources to assess the output, prioritizing a higher volume of low-uncertainty responses may necessitate a more lenient threshold, even if it results in only a modest accuracy gain.Conversely, for tasks demanding highly reliable outcomes, such as the evaluation of medical diagnoses or legal decision-making, where accuracy is critical and errors carry significant consequences, a stricter threshold is essential to ensure the chosen options are highly reliable.</p>
<p>An interesting observation arises when the threshold is reduced below 0.5, accuracy tends to increase, suggesting that as the average token probability for incorrect options decreases, the model performance improves.This suggests that valuable information can be derived not only from options marked as having low uncertainty but also from those with higher uncertainty.The behavior of token probabilities across both low and high uncertainty options provides insights into the decision-making process of the LLM, suggesting that thresholds should be dynamically tuned based on the specific performance trade-offs desired for the task at hand.</p>
<p>Our analysis reveals that threshold tuning significantly impacts the relationship between accuracy and uncertainty, forming a parabolic effect.In the threshold grid search for the Feedback Collection dataset (see Figure 6), we observe that as the threshold increases beyond 0.5, accuracy improves, but the proportion of low-uncertainty predictions decreases.Conversely, when the threshold is below 0.5, lowering the threshold leads to an increase in accuracy but a decrease in the proportion of low-uncertainty labels.This inverse relationship between accuracy and uncertainty highlights that while stricter thresholds (above 0.5) favor higher accuracy at the expense of fewer low-uncertainty predictions, lenient thresholds (below 0.5) enhance accuracy but reduce the certainty of the predictions.This parabolic behavior is consistent across datasets, emphasizing the threshold's pivotal role in determining model performance.In (a), the focus is on the effect of options labeled as low uncertainty, while in (b), the focus shifts to the effect of options labeled as high uncertainty.The results highlight how the threshold choice influences both accuracy and the proportion of selected options.</p>
<p>Experiments</p>
<p>Benchmark Datasets The proposed uncertainty method was evaluated on five benchmark datasets: TruthfulQA [18], Reliance Study, Summarization CNN/DM [20], Feedback Collection [14], and FeedbackQA [16].TruthfulQA contains question-answer pairs and involves a binary classification task to determine whether the answer is truthful, with human annotations available for verification.The Reliance Study dataset originates from a study designed to measure reliance on Large Language Models (LLMs) across various tasks.It uses binary classification to evaluate the accuracy and naturalness of LLM outputs in settings such as conversations and customer-agent interactions, focusing on criteria like relevance and naturalness.The Summarization CNN/DM dataset, which consists of model-generated summaries of news articles, is evaluated on a scale from 1 to 5 based on criteria such as coherence, fluency, and relevance.Feedback Collection is designed to induce fine-grained evaluation capabilities in language models, using a rating scale from 1 to 5. Lastly, FeedbackQA is a retrieval-based QA dataset that includes interactive user feedback, where each question-answer pair is rated from excellent to bad, accompanied by natural language explanations detailing the strengths and weaknesses of the responses.</p>
<p>Models In this study, we utilize instruct models exclusively, as LLM-as-a-Judge requires agentlike capabilities, where models must reliably follow explicit evaluation instructions.The instruct models chosen for this experiment include Mixtral-8x7B-Instruct-v01, Llama-3-8B-Instruct, and Llama-3-70B-Instruct. To investigate the impact of model architecture and size on performance, we selected models of varying sizes 8B and 70B parameters.This approach allows us to assess whether performance improvements in LLM-as-a-Judge tasks are primarily driven by the scale of the model or the underlying instruct-tuned structure.</p>
<p>Implementation Details</p>
<p>The experiments were conducted on stratified samples from each dataset, with the sample size determined by the number of evaluation criteria.For instance, if a dataset included four distinct criteria, the total sample size was multiplied by four to ensure that each criterion was equally represented.This stratification ensures a balanced evaluation across all criteria.To determine the optimal threshold for distinguishing high and low uncertainty in the LLM-as-a-Judge predictions, we employed a grid search, systematically exploring different threshold values to identify the best-performing configuration.</p>
<p>Baseline The evaluation metric for this work is based on accuracy, specifically measuring whether the option selected by the LLM matches the option chosen by the human rater.The baseline for the method is to achieve higher accuracy for cases labeled as low uncertainty and lower accuracy for those labeled as high uncertainty.In datasets such as FeedbackQA and Summarization CNN/DM, which include ratings from multiple human raters, inter-rater accuracy can also be computed.The aim is to maximize alignment between the LLM's predictions and the human ratings, with the ultimate goal of approaching the inter-rater accuracy in these datasets.</p>
<p>Results</p>
<p>Uncertainty labeling correlates with accuracy Our uncertainty labeling method demonstrates a clear advantage in aligning low uncertainty labels with higher accuracy, as shown in Figure 7. Across all datasets and models, the markers for low uncertainty are consistently above the baseline, indicating that these labels correspond to more accurate evaluations compared to high uncertainty labels.This result confirms that the uncertainty labels generated by our method are effective in predicting the likelihood of accurate outputs in LLM-as-a-Judge scenarios.Variability in low uncertainty proportions A key finding is the variance in the ratio of high to low uncertainty labels depending on the dataset and model, as depicted in Figure 8. Notably, the Llama-3-70B-Instruct model consistently produces a higher proportion of low uncertainty labels, outperforming both Llama-3-8B-Instruct and Mixtral-8x7B-Instruct-v01. In datasets such as the Reliance Study, Summarization CNN/DM, and FeedbackQA, the smaller models classify less than 5% of cases as low uncertainty, whereas Llama-3-70B-Instruct exceeds 15%.This suggests that both model size and structure significantly impact the model's ability to assign more reliable uncertainty labels.</p>
<p>Low uncertainty consistently leads to higher accuracy Even in cases where the proportion of low uncertainty labels is small, they still correspond to high accuracy.For example, in the Feedback Collection dataset, the proportion of low uncertainty labels is below 10% for all models (see Figure 8), yet these labels consistently achieve 100% accuracy (see Table 1).This trend further supports the strong predictive power of low uncertainty labels in LLM evaluations.</p>
<p>Smaller deviation in multi-classification ratings In multi-classification datasets, such as Summarization CNN/DM, which require the evaluation of generated summaries, a smaller deviation is observed between the correct ratings and the LLM's selected ratings when low uncertainty labels are present.This indicates that LLMs not only tend to choose the correct options but also exhibit greater precision in their ratings under conditions characterized by a low proportion of uncertainty.</p>
<p>Low uncertainty labels approach human agreement In datasets with human-generated ratings such as Summarization CNN/DM and FeedbackQA, the accuracy of LLM predictions labeled as low uncertainty meets or even exceeds the level of agreement observed between human raters (see Table 1).This suggests that low uncertainty labels can be as reliable as human evaluations, further establishing the effectiveness of our method in aligning LLM outputs with human judgment.The "High Uncertainty" and "Low Uncertainty" values represent the accuracy when considering only options labeled under each respective category.Baseline" reflects the LLM's accuracy without factoring in uncertainty labels, while "Human Agreement" indicates the consistency in accuracy between multiple human raters.The listed models refer to their instruct versions.The threshold was optimized for each specific dataset and model.
Dataset Model Llama-3-8B Llama-3-70B Mixtral-8x7B</p>
<p>Interpreting Uncertainty</p>
<p>The interpretation of uncertainty in LLMs remains a significant challenge [9].Our method attempts to confuse the LLM by convincing it of statements without knowing whether they are true, and then considering the LLM's beliefs under these biased conditions.We define two key scenarios for interpreting low uncertainty.The first scenario is when the LLM selects an option consistently, regardless of conflicting assessments, indicating that even when alternative assessments are presented, they fail to sway the model's decision 9a.The second scenario occurs when the LLM cannot be convinced to generate an assessment for a different option, even if such an assessment is prompted.These two types of low uncertainty can be observed in the structure of the confusion matrices.This behavior is also reflected in the sparsity of the confusion matrices.In cases of high uncertainty, only the token probabilities of the matching assessments and options are high.In contrast, for low uncertainty, only one row exhibits high token probabilities, demonstrating strong model confidence in its chosen option, as shown in Figure 9a.In contrast, high uncertainty can also manifest when the token probabilities are arbitrarily distributed across the matrix, as shown in Figure 10.</p>
<p>Another intuitive observation is that as the number of options increases, the sparsity of the matrix decreases, as seen in Figure 11.This implies that the token probabilities for non-matching assessments and options increase, but this effect is also applicable to token probabilities in cases of low uncertainty.</p>
<p>Discussion</p>
<p>In this work, we introduced a method for quantifying uncertainty in LLM-as-a-Judge evaluations.</p>
<p>Through empirical analysis, we found that the uncertainty labels correlate with accuracy, indicating the effectiveness of the method.Although our primary focus was on evaluating this method within the context of LLM-based evaluations, the potential for broader applications is significant.Despite these promising results, the current approach presents certain limitations.The method is computationally intensive, especially when using large models such as Llama-3-70B-Instruct, which may not be feasible for all applications.Additionally, the performance of the method may vary when applied to models or tasks that have not been fine-tuned for evaluation purposes.Generalizability across diverse tasks and domains also requires further investigation.</p>
<p>To address these challenges, one potential solution is to reduce inference time by consolidating all assessments into a single prompt, querying the model for its chosen option only once.This strategy could lower computational overhead without compromising accuracy.Overall, while the current method yields strong results, further optimization and refinement have the potential to enhance efficiency and effectiveness.</p>
<p>To further improve the results and increase the proportion of low uncertainty labels, prompt engineering emerges as a key factor.We recommend adapting the prompt structure specifically to the task and the model in use.Fine-tuning and tailoring prompts could significantly enhance performance, with the potential to surpass the results obtained in this study.Future research could explore the method's effectiveness with various prompt designs to further optimize its performance.</p>
<p>Figure 2 :
2
Figure 2: Method Overview.The method is divided into four stages, resulting in an uncertainty label.The LLM is first presented with an evaluation task, and prompted to produce an assessment for each output option, biased on the explicit indication that the option is correct.In the context of the original evaluation task, the LLM is conditioned on each of the biased assessments, and the probability of each option calculated using log probabilities.This information is then encoded in a confusion matrix.Each row of the matrix, representing the probability of a particular option conditioned on each of the biased assessments, is then averaged to produce an uncertainty label.In this figure, α represents the threshold.</p>
<p>Figure 3 :
3
Figure 3: Persuasion prompt generating an assessment for each option.</p>
<p>Figure 4 :
4
Figure 4: Structure of the confusion matrix.Each row represents an option and each column corresponds to an assessment, with the matrix values being the token probabilities for each optionassessment combination.</p>
<p>Figure 5 :
5
Figure 5: Confusion prompt forcing a final answer for each option and assessment from the previous step leading to n 2 prompts being used to obtain token log probabilities for each option and assessment combination.</p>
<p>Figure 6 :
6
Figure 6: Relationship between threshold, accuracy, and proportion through grid search optimization.Grid search optimization of the threshold for the Llama-3-70B-Instruct model on the Feedback Collection dataset.The figure illustrates how varying the threshold impacts performance.In (a), the focus is on the effect of options labeled as low uncertainty, while in (b), the focus shifts to the effect of options labeled as high uncertainty.The results highlight how the threshold choice influences both accuracy and the proportion of selected options.</p>
<p>Figure 7 :
7
Figure 7: Accuracy comparison of options labeled low uncertainty versus high uncertainty.Each marker represents the performance of a specific model on a particular dataset.Markers above the dashed line indicate that the model has surpassed the baseline for that dataset.</p>
<p>Figure 8 :
8
Figure 8: Ratio of Options Labeled Low Uncertainty.The y-axis represents the percentage of options labeled as low uncertainty, while the x-axis denotes the datasets evaluated across three different models, as indicated in the legend.The ratio of options labeled as high uncertainty is calculated as 1 minus the ratio of options labeled low uncertainty.</p>
<p>Figure 9 :
9
Figure 9: Examples of confusion matrices for high and low uncertainty.</p>
<p>Figure 10 :
10
Figure 10: Example of a confusion matrix with arbitrarily distributed token probabilities.</p>
<p>Figure 11 :
11
Figure 11: Examples of sparsity across different numbers of options.This figure presents confusion matrices, all labeled as high uncertainty.It illustrates that as the number of options increases, the sparsity within the matrices decreases, highlighting the relationship between option quantity and token probability distribution.</p>
<p>Table 1 :
1
Accuracy across various datasets and models for different uncertainty categories.
TruthfulQAHigh Uncertainty0.590.500.60Baseline0.620.780.66Low Uncertainty0.680.910.81Reliance StudyHigh Uncertainty0.490.510.63Baseline0.490.630.64Low Uncertainty0.500.851.00Feedback CollectionHigh Uncertainty0.360.370.30Baseline0.390.400.32Low Uncertainty1.001.001.00Summarization CNN/DMHigh Uncertainty0.270.240.15Baseline0.280.250.16Low Uncertainty0.750.260.50Human Agreement0.600.600.60FeedbackQAHigh Uncertainty0.460.670.28Baseline0.470.690.29Low Uncertainty1.000.711.00Human Agreement0.480.480.48</p>
<p>Leak, cheat, repeat: Data contamination and evaluation malpractices in closed-source LLMs. Simone Balloccu, Patrícia Schmidtová, Mateusz Lango, Ondrej Dusek, Proceedings of the 18th Conference of the European Chapter. Long Papers. Yvette Graham, Matthew Purver, the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational LinguisticsMarch 20241</p>
<p>Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks. Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, arXiv:2406.184032024arXiv preprint</p>
<p>Confusion matrix -Wikipedia, the free encyclopedia. 2024. -October-202415Wikipedia contributors</p>
<p>Sumanth Doddapaneni, Mohammed Safi Ur Rahman Khan, Sshubam Verma, Mitesh M Khapra, arXiv:2406.13439Finding blind spots in evaluator llms with interpretable checklists. 2024arXiv preprint</p>
<p>Data-efficient concept extraction from pre-trained language models for commonsense explanation generation. Yanbo Fang, Yongfeng Zhang, 2022EMNLP 2022</p>
<p>Bigbench: Towards an industry standard benchmark for big data analytics. Ahmad Ghazal, Tilmann Rabl, Minqing Hu, Francois Raab, Meikel Poess, Alain Crolotte, Hans-Arno Jacobsen, Proceedings of the 2013 ACM SIGMOD international conference on Management of data. the 2013 ACM SIGMOD international conference on Management of data2013</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Universal language model fine-tuning for text classification. Jeremy Howard, Sebastian Ruder, arXiv:1801.061462018arXiv preprint</p>
<p>Mengting Hu, Zhen Zhang, Shiwan Zhao, Minlie Huang, Bingzhe Wu, arXiv:2306.04459Uncertainty in natural language processing: Sources, quantification, and applications. 2023arXiv preprint</p>
<p>Jie Huang, Kevin Chen, -Chuan Chang, arXiv:2212.10403Towards reasoning in large language models: A survey. 2022arXiv preprint</p>
<p>Towards mitigating llm hallucination via self reflection. Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, Pascale Fung, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>A comprehensive survey on process-oriented automatic text summarization with exploration of llm-based methods. Hanlei Jin, Yang Zhang, Dan Meng, Jun Wang, Jinghua Tan, arXiv:2403.029012024arXiv preprint</p>
<p>Prometheus: Inducing evaluation capability in language models. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. 2023</p>
<p>Prometheus 2: An open source language model specialized in evaluating other language models. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, Minjoon Seo, arXiv:2405.015352024arXiv preprint</p>
<p>Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. Lorenz Kuhn, Yarin Gal, Sebastian Farquhar, arXiv:2302.096642023arXiv preprint</p>
<p>Using interactive feedback to improve the accuracy and explainability of question answering systems postdeployment. Zichao Li, Prakhar Sharma, Xing Han Lu, Jackie Ck Cheung, Siva Reddy, arXiv:2204.030252022arXiv preprint</p>
<p>Holistic evaluation of language models. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, arXiv:2211.091102022arXiv preprint</p>
<p>Stephanie Lin, Jacob Hilton, Owain Evans, Truthfulqa, arXiv:2109.07958Measuring how models mimic human falsehoods. 2021arXiv preprint</p>
<p>Teaching models to express their uncertainty in words. Stephanie Lin, Jacob Hilton, Owain Evans, arXiv:2205.143342022arXiv preprint</p>
<p>Abstractive text summarization using sequence-to-sequence rnns and beyond. Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, arXiv:1602.060232016arXiv preprint</p>
<p>Agentic design patterns part 1. Andrew Ng, 2024</p>
<p>Tejaswini Pedapati, Amit Dhurandhar, Soumya Ghosh, arXiv:2406.04370Soham Dan, and Prasanna Sattigeri. Large language model confidence estimation via black-box access. 2024arXiv preprint</p>
<p>Thermometer: Towards universal calibration for large language models. Maohao Shen, Subhro Das, Kristjan Greenewald, Prasanna Sattigeri, Gregory Wornell, Soumya Ghosh, arXiv:2403.088192024arXiv preprint</p>
<p>Towards expert-level medical question answering with large language models. Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, arXiv:2305.096172023arXiv preprint</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.046152022arXiv preprint</p>
<p>Replacing judges with juries: Evaluating llm generations with a panel of diverse models. Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, Patrick Lewis, arXiv:2404.187962024arXiv preprint</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, arXiv:2305.179262023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi, arXiv:2306.130632023arXiv preprint</p>
<p>Prompting large language model for machine translation: A case study. Biao Zhang, Barry Haddow, Alexandra Birch, International Conference on Machine Learning. PMLR2023</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 362024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6804 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6804</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6804</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-258762525</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.10601v2.pdf" target="_blank">Tree of Thoughts: Deliberate Problem Solving with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6804.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6804.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that augments autoregressive LMs with deliberate search over intermediate 'thought' units using LM-based thought generation and LM-based state evaluation combined with tree search (BFS/DFS) to enable lookahead and backtracking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (primary experiments); GPT-3.5-turbo (appendix experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied on large transformer LMs (GPT-4 in main experiments) by treating the LM as both a generator of candidate high-level text 'thoughts' and as a value/vote-based evaluator; integrates these LM capabilities with classical search algorithms (BFS/DFS).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer based LM used as generator+evaluator, combined with classical search (ToT: BFS/DFS)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Tree-of-Thoughts: generate multiple candidate intermediate 'thoughts' per state, evaluate states via LM-based value or voting prompts, and explore/prune using BFS or DFS with lookahead and backtracking</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Game of 24 (4nums.com subset), Creative Writing (synthetic sentences), Mini Crosswords (GooBix); additional zero-shot tests on GSM8K and StrategyQA (appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Game of 24: arithmetic puzzle using 4 numbers to reach 24 (scraped from 4nums.com); Creative Writing: produce coherent 4-paragraph passage ending with 4 given sentences; Mini Crosswords: 5x5 crosswords from GooBix with 10 clues (5 horizontal, 5 vertical); GSM8K/StrategyQA: standard reasoning/NLI datasets (zero-shot ToT extension in appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Mathematical arithmetic search (Game of 24), open-ended planning/coherent generation (Creative Writing), constrained combinatorial search (Mini Crosswords), general multi-step reasoning (GSM8K/StrategyQA)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task-specific metrics: success rate (Game of 24), human/GPT-4 coherence score (1-10) and human pairwise preference (Creative Writing), letter/word/game accuracy (Mini Crosswords)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Game of 24: ToT (breadth b=1) 45% success; ToT (b=5) 74% success (GPT-4, temperature 0.7). Creative Writing (GPT-4): ToT average GPT-4 coherence score 7.56/10 vs IO 6.19 and CoT 6.93; humans preferred ToT over CoT in 41/100 pairs vs CoT over ToT 21/100. Mini Crosswords (GPT-4): ToT word-level success ≈ 60% and solved 4/20 games; oracle best-state ToT solved 7/20 games. Appendix: zero-shot ToT slightly improves GSM8K and StrategyQA over CoT; GPT-3.5+ToT shows gains on some tasks but substantially worse than GPT-4+ToT on hard tasks (Game of 24: GPT-3.5+ToT 19% vs GPT-4+ToT 74%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Large improvements versus token-level baselines on hard planning/search tasks: e.g., Game of 24 GPT-4 CoT 4.0% vs ToT b=5 74% (+70 percentage points). Creative Writing: ToT > CoT and IO in automatic and human evaluations. Mini Crosswords: ToT yields major gains in letter/word/game accuracy compared to IO/CoT (word-level ~60% vs <16%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Deliberate tree search over LM-generated intermediate thoughts with LM-based evaluation substantially improves performance on tasks requiring planning, lookahead, and backtracking compared to IO, CoT, and CoT self-consistency; ToT is modular and can combine different generation/evaluation/search strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Higher computational and API cost (ToT can require 5–100× more tokens than CoT), imperfect LM-based state-evaluators can prune correct branches (causing missed solutions), not necessary for many tasks where GPT-4 already performs well, performance sensitive to base LM strength (GPT-3.5 weaker), current heuristics/simple BFS/DFS are basic and can be further improved, requires prompt engineering and careful configuration of breadth/depth/pruning thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6804.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6804.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits step-by-step intermediate natural-language reasoning chains from an LM before producing a final answer, sampled as a continuous token sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based LM prompted to produce intermediate reasoning steps (chains of thought) in a left-to-right autoregressive manner.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (autoregressive) + chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Chain-of-Thought prompting: induce an explanatory sequence of intermediate steps sampled sequentially in the autoregressive decode</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Game of 24; Creative Writing; Mini Crosswords (used as baselines in the experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same task set as ToT experiments (see ToT entry).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Mathematical reasoning (Game of 24), planning/writing, crossword solving (as baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (Game of 24), coherence score (Creative Writing), letter/word/game accuracy (Mini Crosswords)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Game of 24 (GPT-4, CoT): 4.0% success (100 samples); CoT self-consistency (majority over 100 CoT samples): 9.0%; CoT (best of 100) 49% (oracle best of k). Creative Writing (zero-shot CoT) average GPT-4 score 6.93 vs ToT 7.56. Mini Crosswords CoT word-level <16% (perform poorly).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CoT outperforms IO on some standard reasoning tasks but fails badly on the introduced search/planning problems; CoT-SC (self-consistency) improves over single CoT but still far below ToT on Game of 24.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT often fails early in left-to-right decoding on combinatorial tasks (≈60% of CoT samples fail at the first step in Game of 24), and lacks local branching and global planning/backtracking capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>No local exploration within a chain, no lookahead/backtracking, left-to-right decoding leads to early mistakes that cannot be revisited; self-consistency helps but cannot replace explicit search.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6804.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6804.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble method that samples multiple chain-of-thought traces and returns the most frequent final answer (voting over multiple CoT samples).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same base LM as CoT, sampling multiple independent CoT traces then aggregating outputs by majority vote.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer autoregressive sampling + ensemble majority vote over sampled CoT outputs</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Self-consistency: sample k i.i.d. CoT chains and aggregate final answers by majority vote</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Game of 24 (used as a baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Game of 24 arithmetic puzzles; self-consistency applied to aggregates of CoT samples.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Mathematical arithmetic search (evaluation of ensemble CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Game of 24 (GPT-4, CoT-SC k=100): 9.0% success; CoT (best of 100) 49% (oracle), IO (best of 100) 33%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Provides modest gains over single-sample CoT (4.0% -> 9.0% in Game of 24) but remains far below ToT (74%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Ensembling CoT traces helps but does not substitute for explicit search and branching over intermediate steps; self-consistency is limited when initial steps are frequently incorrect.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on repeated sampling and majority heuristic; applicable primarily when output space allows voting; does not provide local branching or backtracking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6804.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6804.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Input-Output Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard prompting approach that conditions the LM on task instruction and examples (few-shot) to directly produce an output without explicit intermediate reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (primary baseline); GPT-3.5 in appendix</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer LM prompted with examples/instructions to map input to output directly.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer autoregressive decoding with direct input-output prompts</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Direct IO prompting (few-shot or zero-shot), no explicit chain-of-thought</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Game of 24, Creative Writing, Mini Crosswords</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same task suite used as baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Mathematical arithmetic, creative generation, crossword solving</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate or coherence score depending on task</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Game of 24 (GPT-4 IO): 7.3% success; IO best-of-100: 33%. Creative Writing (GPT-4 IO) average score 6.19. Mini Crosswords IO letter-level ≈38.7% (word-level <16%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Works better than naive single-sample CoT in some settings but worse than ToT and iterative-refine on the introduced hard search tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>IO prompting is insufficient for tasks that require planning/search/backtracking; creative tasks can be improved by refinement or ToT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>No intermediate deliberative steps, poor at combinatorial planning and backtracking tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6804.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6804.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iterative-refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative-refine (self-refinement with feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that conditions the LM on previous outputs and feedback (here using ground-truth correctness signals in experiments) to iteratively refine an answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (applied in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer LM that takes its previous generation and a feedback instruction to produce refined outputs iteratively.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer autoregressive with iterative conditioning on previous generations and feedback</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Iterative self-refinement: LM re-generates conditioned on prior outputs and corrective signals</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Game of 24, Creative Writing</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Used as an alternative thought-generation/refinement strategy: apply multiple refine iterations to improve final outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Mathematical correctness improvement (Game of 24) and natural-language coherency (Creative Writing)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (Game of 24), coherence score (Creative Writing)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Game of 24 IO+Refine (k≤10) achieved 27% success (uses ground-truth correctness during refinement). Creative Writing: iterative-refine improved IO coherency from 6.19 -> 7.67 and improved ToT from 7.56 -> 7.91.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Iterative-refine improves IO substantially and can complement ToT; on some natural language tasks it is highly effective.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Refinement using feedback can raise performance but in Game of 24 it required ground-truth correctness signals (oracle-like), and still lags behind ToT when ToT uses search.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>In the Game of 24 experiments it used ground-truth signal to detect incorrect equations (not always realistic); can be computationally expensive; may converge slowly or get stuck without good feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6804.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6804.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The primary large language model used in the experiments; a state-of-the-art transformer-based conversational LM accessed via Chat Completions API (used at temperature 0.7).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer autoregressive language model (OpenAI) used in chat-completion mode for both generation and LM-based evaluation/voting prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer autoregressive</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Used with IO prompting, Chain-of-Thought, Self-consistency, Tree-of-Thoughts (generation + evaluation), and iterative-refine in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>See ToT entry: Game of 24, Creative Writing, Mini Crosswords, GSM8K/StrategyQA (appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See ToT entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Arithmetic puzzles, creative writing, combinatorial crossword solving, standard reasoning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>See per-method metrics above</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4 combined with Tree of Thoughts achieved substantial improvements on tasks requiring search/planning; GPT-4's abilities enable both generation and deliberative self-evaluation used by ToT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper does not report parameter count or full training data; performance depends on prompt design and search hyperparameters; using GPT-4 for ToT is computationally and financially more expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6804.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6804.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller/earlier OpenAI conversational LM tested in appendix to measure how ToT performs with weaker base LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based LM (smaller/earlier generation than GPT-4) used in appendix experiments to run ToT and baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer autoregressive</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Applied with IO, CoT, and ToT in appendix experiments</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Game of 24 (appendix), Creative Writing (appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Subset experiments showing ToT>CoT>IO still holds but absolute performance lower than GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Arithmetic and creative writing</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate / coherence score</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Game of 24 (GPT-3.5+ToT): 19% success (worse than GPT-4+ToT 74%). Creative Writing GPT-3.5+ToT outperformed GPT-4+IO and was similar to GPT-4+CoT in that task.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>ToT improves GPT-3.5 performance versus CoT/IO on some tasks, but overall weaker than GPT-4+ToT on hard search tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ToT benefits extend to weaker models but absolute gains depend strongly on base LM generation quality; generation (not evaluation) is often the bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Weaker generation quality limits ToT effectiveness on combinatorial tasks; reported results show large gap between GPT-3.5 and GPT-4 for Game of 24.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6804.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6804.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-eval decoding (related)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-eval guided decoding / self-eval decoding (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related approaches that evaluate LM-generated candidates with LM self-evaluation and use those evaluations to guide decoding or search (mentioned in related work as similar in spirit to ToT).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Methods that sample candidate outputs (leaves) and use LM-based self-evaluation prompts to score/evaluate candidates and guide search/selection.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer + LM-based candidate evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>LM-based self-evaluation applied to guide decoding/search (e.g., sample leaves and self-eval to prioritize/select).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as a related technique that also uses LM self-evaluation to guide search; differs in representation (e.g., PAL codes) and in task applicability per the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper notes differences (e.g., PAL/coded thoughts may limit applicability to creative writing) and that those methods lack the modular ToT formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Decomposition enhances reasoning via self-evaluation guided decoding <em>(Rating: 2)</em></li>
                <li>Pal: Programaided language models <em>(Rating: 1)</em></li>
                <li>Reasoning with language model is planning with world model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6804",
    "paper_id": "paper-258762525",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "ToT",
            "name_full": "Tree of Thoughts",
            "brief_description": "A framework that augments autoregressive LMs with deliberate search over intermediate 'thought' units using LM-based thought generation and LM-based state evaluation combined with tree search (BFS/DFS) to enable lookahead and backtracking.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (primary experiments); GPT-3.5-turbo (appendix experiments)",
            "model_description": "Applied on large transformer LMs (GPT-4 in main experiments) by treating the LM as both a generator of candidate high-level text 'thoughts' and as a value/vote-based evaluator; integrates these LM capabilities with classical search algorithms (BFS/DFS).",
            "model_size": null,
            "architecture_type": "Transformer based LM used as generator+evaluator, combined with classical search (ToT: BFS/DFS)",
            "training_data": null,
            "reasoning_method": "Tree-of-Thoughts: generate multiple candidate intermediate 'thoughts' per state, evaluate states via LM-based value or voting prompts, and explore/prune using BFS or DFS with lookahead and backtracking",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Game of 24 (4nums.com subset), Creative Writing (synthetic sentences), Mini Crosswords (GooBix); additional zero-shot tests on GSM8K and StrategyQA (appendix)",
            "benchmark_description": "Game of 24: arithmetic puzzle using 4 numbers to reach 24 (scraped from 4nums.com); Creative Writing: produce coherent 4-paragraph passage ending with 4 given sentences; Mini Crosswords: 5x5 crosswords from GooBix with 10 clues (5 horizontal, 5 vertical); GSM8K/StrategyQA: standard reasoning/NLI datasets (zero-shot ToT extension in appendix).",
            "task_type": "Mathematical arithmetic search (Game of 24), open-ended planning/coherent generation (Creative Writing), constrained combinatorial search (Mini Crosswords), general multi-step reasoning (GSM8K/StrategyQA)",
            "performance_metric": "Task-specific metrics: success rate (Game of 24), human/GPT-4 coherence score (1-10) and human pairwise preference (Creative Writing), letter/word/game accuracy (Mini Crosswords)",
            "performance_value": "Game of 24: ToT (breadth b=1) 45% success; ToT (b=5) 74% success (GPT-4, temperature 0.7). Creative Writing (GPT-4): ToT average GPT-4 coherence score 7.56/10 vs IO 6.19 and CoT 6.93; humans preferred ToT over CoT in 41/100 pairs vs CoT over ToT 21/100. Mini Crosswords (GPT-4): ToT word-level success ≈ 60% and solved 4/20 games; oracle best-state ToT solved 7/20 games. Appendix: zero-shot ToT slightly improves GSM8K and StrategyQA over CoT; GPT-3.5+ToT shows gains on some tasks but substantially worse than GPT-4+ToT on hard tasks (Game of 24: GPT-3.5+ToT 19% vs GPT-4+ToT 74%).",
            "comparison_with_baseline": "Large improvements versus token-level baselines on hard planning/search tasks: e.g., Game of 24 GPT-4 CoT 4.0% vs ToT b=5 74% (+70 percentage points). Creative Writing: ToT &gt; CoT and IO in automatic and human evaluations. Mini Crosswords: ToT yields major gains in letter/word/game accuracy compared to IO/CoT (word-level ~60% vs &lt;16%).",
            "key_findings": "Deliberate tree search over LM-generated intermediate thoughts with LM-based evaluation substantially improves performance on tasks requiring planning, lookahead, and backtracking compared to IO, CoT, and CoT self-consistency; ToT is modular and can combine different generation/evaluation/search strategies.",
            "limitations": "Higher computational and API cost (ToT can require 5–100× more tokens than CoT), imperfect LM-based state-evaluators can prune correct branches (causing missed solutions), not necessary for many tasks where GPT-4 already performs well, performance sensitive to base LM strength (GPT-3.5 weaker), current heuristics/simple BFS/DFS are basic and can be further improved, requires prompt engineering and careful configuration of breadth/depth/pruning thresholds.",
            "uuid": "e6804.0",
            "source_info": {
                "paper_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits step-by-step intermediate natural-language reasoning chains from an LM before producing a final answer, sampled as a continuous token sequence.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "GPT-4 (used as baseline)",
            "model_description": "Transformer-based LM prompted to produce intermediate reasoning steps (chains of thought) in a left-to-right autoregressive manner.",
            "model_size": null,
            "architecture_type": "Transformer (autoregressive) + chain-of-thought prompting",
            "training_data": null,
            "reasoning_method": "Chain-of-Thought prompting: induce an explanatory sequence of intermediate steps sampled sequentially in the autoregressive decode",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Game of 24; Creative Writing; Mini Crosswords (used as baselines in the experiments)",
            "benchmark_description": "Same task set as ToT experiments (see ToT entry).",
            "task_type": "Mathematical reasoning (Game of 24), planning/writing, crossword solving (as baselines)",
            "performance_metric": "Success rate (Game of 24), coherence score (Creative Writing), letter/word/game accuracy (Mini Crosswords)",
            "performance_value": "Game of 24 (GPT-4, CoT): 4.0% success (100 samples); CoT self-consistency (majority over 100 CoT samples): 9.0%; CoT (best of 100) 49% (oracle best of k). Creative Writing (zero-shot CoT) average GPT-4 score 6.93 vs ToT 7.56. Mini Crosswords CoT word-level &lt;16% (perform poorly).",
            "comparison_with_baseline": "CoT outperforms IO on some standard reasoning tasks but fails badly on the introduced search/planning problems; CoT-SC (self-consistency) improves over single CoT but still far below ToT on Game of 24.",
            "key_findings": "CoT often fails early in left-to-right decoding on combinatorial tasks (≈60% of CoT samples fail at the first step in Game of 24), and lacks local branching and global planning/backtracking capabilities.",
            "limitations": "No local exploration within a chain, no lookahead/backtracking, left-to-right decoding leads to early mistakes that cannot be revisited; self-consistency helps but cannot replace explicit search.",
            "uuid": "e6804.1",
            "source_info": {
                "paper_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "CoT-SC",
            "name_full": "Chain-of-Thought Self-Consistency",
            "brief_description": "An ensemble method that samples multiple chain-of-thought traces and returns the most frequent final answer (voting over multiple CoT samples).",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Same base LM as CoT, sampling multiple independent CoT traces then aggregating outputs by majority vote.",
            "model_size": null,
            "architecture_type": "Transformer autoregressive sampling + ensemble majority vote over sampled CoT outputs",
            "training_data": null,
            "reasoning_method": "Self-consistency: sample k i.i.d. CoT chains and aggregate final answers by majority vote",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Game of 24 (used as a baseline)",
            "benchmark_description": "Game of 24 arithmetic puzzles; self-consistency applied to aggregates of CoT samples.",
            "task_type": "Mathematical arithmetic search (evaluation of ensemble CoT)",
            "performance_metric": "Success rate",
            "performance_value": "Game of 24 (GPT-4, CoT-SC k=100): 9.0% success; CoT (best of 100) 49% (oracle), IO (best of 100) 33%.",
            "comparison_with_baseline": "Provides modest gains over single-sample CoT (4.0% -&gt; 9.0% in Game of 24) but remains far below ToT (74%).",
            "key_findings": "Ensembling CoT traces helps but does not substitute for explicit search and branching over intermediate steps; self-consistency is limited when initial steps are frequently incorrect.",
            "limitations": "Relies on repeated sampling and majority heuristic; applicable primarily when output space allows voting; does not provide local branching or backtracking.",
            "uuid": "e6804.2",
            "source_info": {
                "paper_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "IO",
            "name_full": "Input-Output Prompting",
            "brief_description": "Standard prompting approach that conditions the LM on task instruction and examples (few-shot) to directly produce an output without explicit intermediate reasoning steps.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 (primary baseline); GPT-3.5 in appendix",
            "model_description": "Transformer LM prompted with examples/instructions to map input to output directly.",
            "model_size": null,
            "architecture_type": "Transformer autoregressive decoding with direct input-output prompts",
            "training_data": null,
            "reasoning_method": "Direct IO prompting (few-shot or zero-shot), no explicit chain-of-thought",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Game of 24, Creative Writing, Mini Crosswords",
            "benchmark_description": "Same task suite used as baselines.",
            "task_type": "Mathematical arithmetic, creative generation, crossword solving",
            "performance_metric": "Success rate or coherence score depending on task",
            "performance_value": "Game of 24 (GPT-4 IO): 7.3% success; IO best-of-100: 33%. Creative Writing (GPT-4 IO) average score 6.19. Mini Crosswords IO letter-level ≈38.7% (word-level &lt;16%).",
            "comparison_with_baseline": "Works better than naive single-sample CoT in some settings but worse than ToT and iterative-refine on the introduced hard search tasks.",
            "key_findings": "IO prompting is insufficient for tasks that require planning/search/backtracking; creative tasks can be improved by refinement or ToT.",
            "limitations": "No intermediate deliberative steps, poor at combinatorial planning and backtracking tasks.",
            "uuid": "e6804.3",
            "source_info": {
                "paper_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Iterative-refine",
            "name_full": "Iterative-refine (self-refinement with feedback)",
            "brief_description": "A method that conditions the LM on previous outputs and feedback (here using ground-truth correctness signals in experiments) to iteratively refine an answer.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 (applied in experiments)",
            "model_description": "Transformer LM that takes its previous generation and a feedback instruction to produce refined outputs iteratively.",
            "model_size": null,
            "architecture_type": "Transformer autoregressive with iterative conditioning on previous generations and feedback",
            "training_data": null,
            "reasoning_method": "Iterative self-refinement: LM re-generates conditioned on prior outputs and corrective signals",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Game of 24, Creative Writing",
            "benchmark_description": "Used as an alternative thought-generation/refinement strategy: apply multiple refine iterations to improve final outputs.",
            "task_type": "Mathematical correctness improvement (Game of 24) and natural-language coherency (Creative Writing)",
            "performance_metric": "Success rate (Game of 24), coherence score (Creative Writing)",
            "performance_value": "Game of 24 IO+Refine (k≤10) achieved 27% success (uses ground-truth correctness during refinement). Creative Writing: iterative-refine improved IO coherency from 6.19 -&gt; 7.67 and improved ToT from 7.56 -&gt; 7.91.",
            "comparison_with_baseline": "Iterative-refine improves IO substantially and can complement ToT; on some natural language tasks it is highly effective.",
            "key_findings": "Refinement using feedback can raise performance but in Game of 24 it required ground-truth correctness signals (oracle-like), and still lags behind ToT when ToT uses search.",
            "limitations": "In the Game of 24 experiments it used ground-truth signal to detect incorrect equations (not always realistic); can be computationally expensive; may converge slowly or get stuck without good feedback.",
            "uuid": "e6804.4",
            "source_info": {
                "paper_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "The primary large language model used in the experiments; a state-of-the-art transformer-based conversational LM accessed via Chat Completions API (used at temperature 0.7).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large transformer autoregressive language model (OpenAI) used in chat-completion mode for both generation and LM-based evaluation/voting prompts.",
            "model_size": null,
            "architecture_type": "Transformer autoregressive",
            "training_data": null,
            "reasoning_method": "Used with IO prompting, Chain-of-Thought, Self-consistency, Tree-of-Thoughts (generation + evaluation), and iterative-refine in experiments.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "See ToT entry: Game of 24, Creative Writing, Mini Crosswords, GSM8K/StrategyQA (appendix)",
            "benchmark_description": "See ToT entry.",
            "task_type": "Arithmetic puzzles, creative writing, combinatorial crossword solving, standard reasoning benchmarks",
            "performance_metric": "See per-method metrics above",
            "performance_value": null,
            "comparison_with_baseline": null,
            "key_findings": "GPT-4 combined with Tree of Thoughts achieved substantial improvements on tasks requiring search/planning; GPT-4's abilities enable both generation and deliberative self-evaluation used by ToT.",
            "limitations": "Paper does not report parameter count or full training data; performance depends on prompt design and search hyperparameters; using GPT-4 for ToT is computationally and financially more expensive.",
            "uuid": "e6804.5",
            "source_info": {
                "paper_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5-turbo (OpenAI)",
            "brief_description": "A smaller/earlier OpenAI conversational LM tested in appendix to measure how ToT performs with weaker base LMs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "Transformer-based LM (smaller/earlier generation than GPT-4) used in appendix experiments to run ToT and baselines.",
            "model_size": null,
            "architecture_type": "Transformer autoregressive",
            "training_data": null,
            "reasoning_method": "Applied with IO, CoT, and ToT in appendix experiments",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Game of 24 (appendix), Creative Writing (appendix)",
            "benchmark_description": "Subset experiments showing ToT&gt;CoT&gt;IO still holds but absolute performance lower than GPT-4.",
            "task_type": "Arithmetic and creative writing",
            "performance_metric": "Success rate / coherence score",
            "performance_value": "Game of 24 (GPT-3.5+ToT): 19% success (worse than GPT-4+ToT 74%). Creative Writing GPT-3.5+ToT outperformed GPT-4+IO and was similar to GPT-4+CoT in that task.",
            "comparison_with_baseline": "ToT improves GPT-3.5 performance versus CoT/IO on some tasks, but overall weaker than GPT-4+ToT on hard search tasks.",
            "key_findings": "ToT benefits extend to weaker models but absolute gains depend strongly on base LM generation quality; generation (not evaluation) is often the bottleneck.",
            "limitations": "Weaker generation quality limits ToT effectiveness on combinatorial tasks; reported results show large gap between GPT-3.5 and GPT-4 for Game of 24.",
            "uuid": "e6804.6",
            "source_info": {
                "paper_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Self-eval decoding (related)",
            "name_full": "Self-eval guided decoding / self-eval decoding (related work)",
            "brief_description": "Related approaches that evaluate LM-generated candidates with LM self-evaluation and use those evaluations to guide decoding or search (mentioned in related work as similar in spirit to ToT).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": "Methods that sample candidate outputs (leaves) and use LM-based self-evaluation prompts to score/evaluate candidates and guide search/selection.",
            "model_size": null,
            "architecture_type": "Transformer + LM-based candidate evaluation",
            "training_data": null,
            "reasoning_method": "LM-based self-evaluation applied to guide decoding/search (e.g., sample leaves and self-eval to prioritize/select).",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": null,
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": null,
            "key_findings": "Mentioned as a related technique that also uses LM self-evaluation to guide search; differs in representation (e.g., PAL codes) and in task applicability per the authors.",
            "limitations": "Paper notes differences (e.g., PAL/coded thoughts may limit applicability to creative writing) and that those methods lack the modular ToT formulation.",
            "uuid": "e6804.7",
            "source_info": {
                "paper_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Decomposition enhances reasoning via self-evaluation guided decoding",
            "rating": 2,
            "sanitized_title": "decomposition_enhances_reasoning_via_selfevaluation_guided_decoding"
        },
        {
            "paper_title": "Pal: Programaided language models",
            "rating": 1,
            "sanitized_title": "pal_programaided_language_models"
        },
        {
            "paper_title": "Reasoning with language model is planning with world model",
            "rating": 1,
            "sanitized_title": "reasoning_with_language_model_is_planning_with_world_model"
        }
    ],
    "cost": 0.01754975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models
3 Dec 2023</p>
<p>Shunyu Yao 
Dian Yu 
Google Deepmind 
Jeffrey Zhao 
Thomas L Griffiths 
Yuan Cao 
Karthik Narasimhan </p>
<p>Princeton University</p>
<p>Izhak Shafran Google DeepMind</p>
<p>Princeton University</p>
<p>Princeton University</p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models
3 Dec 2023EA4B626EA3CE180EBA53E1B2754D8441arXiv:2305.10601v2[cs.CL]
Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference.This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role.To surmount these challenges, we introduce a new framework for language model inference, "Tree of Thoughts" (ToT), which generalizes over the popular "Chain of Thought" approach to prompting language models, and enables exploration over coherent units of text ("thoughts") that serve as intermediate steps toward problem solving.ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices.Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords.For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%.Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.37th Conference on Neural Information Processing Systems (NeurIPS 2023).</p>
<p>Introduction</p>
<p>Originally designed to generate text, scaled-up versions of language models (LMs) such as GPT [25,26,1,23] and PaLM [5] have been shown to be increasingly capable of performing an ever wider range of tasks requiring mathematical, symbolic, commonsense, and knowledge reasoning.It is perhaps surprising that underlying all this progress is still the original autoregressive mechanism for generating text, which makes token-level decisions one by one and in a left-to-right fashion.Is such a simple mechanism sufficient for a LM to be built toward a general problem solver?If not, what problems would challenge the current paradigm, and what should be alternative mechanisms?</p>
<p>The literature on human cognition provides some clues to answer these questions.Research on "dual process" models suggests that people have two modes in which they engage with decisions -a fast, automatic, unconscious mode ("System 1") and a slow, deliberate, conscious mode ("System 2") [30,31,16,15].These two modes have previously been connected to a variety of mathematical models used in machine learning.For example, research on reinforcement learning in humans and other animals has explored the circumstances under which they engage in associative "model free" learning or more deliberative "model based" planning [7].The simple associative token-level choices of LMs are also reminiscent of "System 1", and thus might benefit from augmentation by a more deliberate "System 2" planning process that (1) maintains and explores diverse alternatives for current Figure 1: Schematic illustrating various approaches to problem solving with LLMs.Each rectangle box represents a thought, which is a coherent language sequence that serves as an intermediate step toward problem solving.See concrete examples of how thoughts are generated, evaluated, and searched in Figures 2,4,6.choices instead of just picking one, and (2) evaluates its current status and actively looks ahead or backtracks to make more global decisions.</p>
<p>To design such a planning process, we return to the origins of artificial intelligence (and cognitive science), drawing inspiration from the planning processes explored by Newell, Shaw, and Simon starting in the 1950s [21,22].Newell and colleagues characterized problem solving [21] as search through a combinatorial problem space, represented as a tree.We thus propose the Tree of Thoughts (ToT) framework for general problem solving with language models.As Figure 1 illustrates, while existing methods (detailed below) sample continuous language sequences for problem solving, ToT actively maintains a tree of thoughts, where each thought is a coherent language sequence that serves as an intermediate step toward problem solving (Table 1).Such a high-level semantic unit allows the LM to self-evaluate the progress different intermediate thoughts make towards solving the problem through a deliberate reasoning process that is also instantiated in language (Figures 2,4,6).This implementation of search heuristics via LM self-evaluation and deliberation is novel, as previous search heuristics are either programmed or learned.Finally, we combine this language-based capability to generate and evaluate diverse thoughts with search algorithms, such as breadth-first search (BFS) or depth-first search (DFS), which allow systematic exploration of the tree of thoughts with lookahead and backtracking.Empirically, we propose three new problems that challenge existing LM inference methods even with the state-of-the-art language model, GPT-4 [23]: Game of 24, Creative Writing, and Crosswords (Table 1).These tasks require deductive, mathematical, commonsense, lexical reasoning abilities, and a way to incorporate systematic planning or search.We show ToT obtains superior results on all three tasks by being general and flexible enough to support different levels of thoughts, different ways to generate and evaluate thoughts, and different search algorithms that adapt to the nature of different problems.We also analyze how such choices affect model performances via systematic ablations and discuss future directions to better train and use LMs.</p>
<p>Background</p>
<p>We first formalize some existing methods that use large language models for problem-solving, which our approach is inspired by and later compared with.We use p θ to denote a pre-trained LM with parameters θ, and lowercase letters x, y, z, s, • • • to denote a language sequence, i.e.
x = (x[1], • • • , x[n]) where each x[i] is a token, so that p θ (x) = n i=1 p θ (x[i]|x[1...i]
).We use uppercase letters S, • • • to denote a collection of language sequences.Input-output (IO) prompting is the most common way to turn a problem input x into output y with LM: y ∼ p θ (y|prompt IO (x)), where prompt IO (x) wraps input x with task instructions and/or few-shot input-output examples.For simplicity, let us denote p prompt θ (output | input) = p θ (output | prompt(input)), so that IO prompting can be formulated as y ∼ p IO θ (y|x).</p>
<p>Chain-of-thought (CoT) prompting [38] was proposed to address cases where the mapping of input x to output y is non-trivial (e.g. when x is a math question and y is the final numerical answer).</p>
<p>The key idea is to introduce a chain of thoughts z 1 , • • • , z n to bridge x and y, where each z i is a coherent language sequence that serves as a meaningful intermediate step toward problem solving (e.g.z i could be an intermediate equation for math QA).To solve problems with CoT, each thought
z i ∼ p CoT θ (z i | x, z 1•••i−1 ) is sampled sequentially, then the output y ∼ p CoT θ (y|x, z 1•••n ). In practice, [z 1•••n , y] ∼ p CoT θ (z 1•••n , y|x
) is sampled as a continuous language sequence, and the decomposition of thoughts (e.g. is each z i a phrase, a sentence, or a paragraph) is left ambiguous.</p>
<p>Self-consistency with CoT (CoT-SC) [36] is an ensemble approach that samples k i.i.d.chains of thought: [z
(i) 1•••n , y (i) ] ∼ p CoT θ (z 1•••n , y|x) (i = 1 • • • k),
then returns the most frequent output: arg max y #{i | y (i) = y}.CoT-SC improves upon CoT, because there are generally different thought processes for the same problem (e.g.different ways to prove the same theorem), and the output decision can be more faithful by exploring a richer set of thoughts.However, within each chain there is no local exploration of different thought steps, and the "most frequent" heuristic only applies when the output space is limited (e.g.multi-choice QA).</p>
<p>Tree of Thoughts: Deliberate Problem Solving with LM</p>
<p>A genuine problem-solving process involves the repeated use of available information to initiate exploration, which discloses, in turn, more information until a way to attain the solution is finally discovered.--Newellet al. [21] Research on human problem-solving suggests that people search through a combinatorial problemspace -a tree where the nodes represent partial solutions, and the branches correspond to operators that modify them [21,22].Which branch to take is determined by heuristics that help to navigate the problem-space and guide the problem-solver towards a solution.This perspective highlights two key shortcomings of existing approaches that use LMs to solve general problems: 1) Locally, they do not explore different continuations within a thought process -the branches of the tree.2) Globally, they do not incorporate any type of planning, lookahead, or backtracking to help evaluate these different options -the kind of heuristic-guided search that seems characteristic of human problem-solving.</p>
<p>To address these shortcomings, we introduce Tree of Thoughts (ToT), a paradigm that allows LMs to explore multiple reasoning paths over thoughts (Figure 1(c)).ToT frames any problem as a search over a tree, where each node is a state s = [x, z 1•••i ] representing a partial solution with the input and the sequence of thoughts so far.A specific instantiation of ToT involves answering four questions: 1.How to decompose the intermediate process into thought steps; 2. How to generate potential thoughts from each state; 3. How to heuristically evaluate states; 4. What search algorithm to use.</p>
<ol>
<li>Thought decomposition.While CoT samples thoughts coherently without explicit decomposition, ToT leverages problem properties to design and decompose intermediate thought steps.As Table 1 shows, depending on different problems, a thought could be a couple of words (Crosswords), a line of equation (Game of 24), or a whole paragraph of writing plan (Creative Writing).In general, a thought should be "small" enough so that LMs can generate promising and diverse samples (e.g.generating a whole book is usually too "big" to be coherent), yet "big" enough so that LMs can evaluate its prospect toward problem solving (e.g.generating one token is usually too "small" to evaluate).</li>
</ol>
<p>Thought generator
G(p θ , s, k). Given a tree state s = [x, z 1•••i ],
we consider two strategies to generate k candidates for the next thought step:</p>
<p>(a) Sample i.i.d.thoughts from a CoT prompt (Creative Writing, Figure 4):
z (j) ∼ p CoT θ (z i+1 |s) = p CoT θ (z i+1 |x, z 1•••i ) (j = 1 • • • k).
This works better when the thought space is rich (e.g. each thought is a paragraph), and i.i.d.samples lead to diversity; (b) Propose thoughts sequentially using a "propose prompt" (Game of 24, Figure 2; Crosswords, Figure 6):
[z (1) , • • • , z (k) ] ∼ p propose θ (z (1•••k) i+1
| s).This works better when the thought space is more constrained (e.g. each thought is just a word or a line), so proposing different thoughts in the same context avoids duplication.3. State evaluator V (p θ , S).Given a frontier of different states, the state evaluator evaluates the progress they make towards solving the problem, serving as a heuristic for the search algorithm to determine which states to keep exploring and in which order.While heuristics are a standard approach to solving search problems, they are typically either programmed (e.g.DeepBlue [3]) or learned (e.g.AlphaGo [29]).We propose a third alternative, by using the LM to deliberately reason about states.When applicable, such a deliberate heuristic can be more flexible than programmed rules, and more sample-efficient than learned models.Similar to the thought generator, we consider two strategies to evaluate states either independently or together:</p>
<p>(a) Value each state independently: V (p θ , S)(s) ∼ p value θ (v|s) ∀s ∈ S, where a value prompt reasons about the state s to generate a scalar value v (e.g.1-10) or a classification (e.g.sure/likely/impossible) that could be heuristically turned into a value.The basis of such evaluative reasoning can vary across problems and thought steps.In this work, we explore evaluation via few lookahead simulations (e.g.quickly confirm that 5, 5, 14 can reach 24 via 5 + 5 + 14, or "hot l" can mean "inn" via filling "e" in " ") plus commonsense (e.g. 1 2 3 are too small to reach 24, or no word can start with "tzxc").While the former might promote "good" states, the latter could help eliminate "bad" states.Such valuations do not need to be perfect, and only need to be approximately helpful for decision making.(b) Vote across states:
V (p θ , S)(s) = 1[s = s * ],
where a "good" state s * ∼ p vote θ (s * |S) is voted out based on deliberately comparing different states in S in a vote prompt.When problem success is harder to directly value (e.g.passage coherency), it is natural to to instead compare different partial solutions and vote for the most promising one.This is similar in spirit to a "step-wise" self-consistency strategy, i.e. cast "which state to explore" as a multi-choice QA, and use LM samples to vote for it.For both strategies, we could prompt the LM multiple times to aggregate the value or vote results to trade time/resource/cost for more faithful/robust heuristics.
Algorithm 1 ToT-BFS(x, p θ , G, k, V, T, b) Require: Input x, LM p θ , thought generator G() &amp; size limit k, states evaluator V (), step limit T , breadth limit b. S 0 ← {x} for t = 1, • • • , T do S ′ t ← {[s, z] | s ∈ S t−1 , z t ∈ G(p θ , s, k)} V t ← V (p θ , S ′ t ) S t ← arg max S⊂S ′ t ,|S|=b s∈S V t (s) end for return G(p θ , arg max s∈S T V T (s), 1) Algorithm 2 ToT-DFS(s, t, p θ , G, k, V, T, v th ) Require: Current state s, step t, LM p θ , thought
generator G() and size limit k, states evaluator V (),
step limit T , threshold v th if t &gt; T then record output G(p θ , s, 1) end if for s ′ ∈ G(p θ , s, k) do ▷ sorted candidates if V (p θ , {s ′ })(s) &gt; v thres then ▷ pruning DFS(s ′ , t + 1) end if end for
4. Search algorithm.Finally, within the ToT framework, one can plug and play different search algorithms depending on the tree structure.We explore two relatively simple search algorithms and leave more advanced ones (e.g.A* [11], MCTS [2]) for future work:</p>
<p>(a) Breadth-first search (BFS) (Algorithm 1) maintains a set of the b most promising states per step.This is used for Game of 24 and Creative Writing where the tree depth is limit (T ≤ 3), and initial thought steps can be evaluated and pruned to a small set (b ≤ 5).(b) Depth-first search (DFS) (Algorithm 2) explores the most promising state first, until the final output is reached (t &gt; T ), or the state evaluator deems it impossible to solve the problem from the current s (V (p θ , {s})(s) ≤ v th for a value threshold v th ).In the latter case, the subtree from s is pruned to trade exploration for exploitation.In both cases, DFS backtracks to the parent state of s to continue exploration.Conceptually, ToT has several benefits as a method for general problem-solving with LMs: (1) Generality.IO, CoT, CoT-SC, and self-refinement can be seen as special cases of ToT (i.e.trees of limited depth and breadth; Figure 1).( 2) Modularity.The base LM, as well as the thought decomposition, generation, evaluation, and search procedures can all be varied independently.(3) Adaptability.Different problem properties, LM capabilities, and resource constraints can be accommodated.( 4) Convenience.No extra training is needed, just a pre-trained LM is sufficient.The next section will show how these conceptual benefits translate to strong empirical performance in different problems.</p>
<p>Experiments</p>
<p>We propose three tasks that are hard even when sampling from the state-of-the-art language model, GPT-4 [23], using standard IO prompting or chain-of-thought (CoT) prompting.We show how deliberate search in trees of thoughts (ToT) produces better results, and more importantly, interesting and promising new ways to use language models to solve problems requiring search or planning.Unless otherwise stated, we perform experiments using a Chat Completion mode GPT-41 with a sampling temperature of 0.7.</p>
<p>Game of 24</p>
<p>Game of 24 is a mathematical reasoning challenge, where the goal is to use 4 numbers and basic arithmetic operations (+-*/) to obtain 24.For example, given input "4 9 10 13", a solution output could be "(10 -4) * (13 -9) = 24".Task Setup.We scrape data from 4nums.com, which has 1,362 games that are sorted from easy to hard by human solving time, and use a subset of relatively hard games indexed 901-1,000 for testing.For each task, we consider the output as success if it is a valid equation that equals 24 and uses the input numbers each exactly once.We report the success rate across 100 games as the metric.</p>
<p>Baselines.We use a standard input-output (IO) prompt with 5 in-context examples.For chain-ofthought (CoT) prompting, we augment each input-output pair with 3 intermediate equations, each operating on two remaining numbers.For example, given input "4 9 10 13", the thoughts could be "13 -9 = 4 (left: 4 4 10); 10 -4 = 6 (left: 4 6); 4 * 6 = 24 (left: 24)".For each game, we sample IO and CoT prompting for 100 times for average performance.We also consider a CoT self-consistency baseline, which takes the majority output from 100 CoT samples, and an iterative-refine approach on top of an IO sample for at most 10 iterations.At each iteration, the LM is conditioned on all previous history to "reflect on your mistakes and generate a refined answer" if the output is incorrect.Note that it uses groundtruth feedback signals about equation correctness.</p>
<p>ToT Setup.To frame Game of 24 into ToT, it is natural to decompose the thoughts into 3 steps, each an intermediate equation.As shown in Figure 2(a), at each tree node, we exact the remaining numbers and prompt the LM to propose some possible next steps.The same "propose prompt" is used for all 3 thought steps, though it only has one example with 4 input numbers.We perform a breadth-first search (BFS) in ToT, where at each step we keep the best b = 5 candidates.To perform deliberate BFS in ToT, as shown in Figure 2(b), we prompt LM to evaluate each thought candidate as "sure/maybe/impossible" with regard to reaching 24.The aim is to promote correct partial solutions that can be verdicted within few lookahead trials, and eliminate impossible partial solutions based on "too big/small" commonsense, and keep the rest "maybe".We sample values 3 times for each thought.Results.As shown in Table 2, IO, CoT, and CoT-SC prompting methods perform badly on the task, achieving only 7.3%, 4.0%, and 9.0% success rates.In contrast, ToT with a breadth of b = 1 already achieves a success rate of 45%, while b = 5 achieves 74%.We also consider an oracle setup for IO/CoT, by calculating the success rate using best of k samples (1 ≤ k ≤ 100).To compare IO/CoT (best of k) with ToT, we consider calculating the tree nodes visited per task in ToT across b = 1 • • • 5, and map the 5 success rates in Figure 3(a), treating IO/CoT (best of k) as visiting k nodes in a bandit.Not surprisingly, CoT scales better than IO, and best of 100 CoT samples achieve a success rate of 49%, but still much worse than exploring more nodes in ToT (b &gt; 1).</p>
<p>Method</p>
<p>Error analysis.Figure 3(b) breaks down at which step CoT and ToT samples fail the task, i.e. the thought (in CoT) or all b thoughts (in ToT) are invalid or impossible to reach 24.Notably, around 60% of CoT samples already failed the task after generating the first step, or equivalently, the first three words (e.g."4 + 9").This highlights the issues with direct left-to-right decoding.</p>
<p>Creative writing</p>
<p>Next, we invent a creative writing task where the input is 4 random sentences and the output should be a coherent passage with 4 paragraphs that end in the 4 input sentences respectively.Such a task is open-ended and exploratory, and challenges creative thinking as well as high-level planning.</p>
<p>Task setup.We sample random sentences from randomwordgenerator.com to form 100 inputs, and there is no groundtruth passage for each input constraint.As we find that GPT-4 can follow the input constraints most of the time, we focus on evaluating passage coherency in two ways: using a GPT-4 zero-shot prompt to provide a 1-10 scalar score, or using human judgments to compare pairs of outputs from different methods.For the former, we sample 5 scores and average them for each task output, and we find these 5 scores usually consistent, with a standard deviation of around 0.56 on average across outputs.For the latter, we employ a subset of the authors in a blind study to compare the coherency of CoT vs. ToT generated passage pairs, where the order of passages is random flipped over 100 inputs.</p>
<p>Baselines.Given the creative nature of the task, both IO and CoT prompts are zero-shot.While the former prompts the LM to directly generate a coherent passage given input constraints, the latter prompts the LM to first make a brief plan then write the passage, i.e. the plan serves as the intermediate thought step.We generate 10 IO and CoT samples per task.We also consider an iterative-refine (k ≤ 5) method on top of a random IO sample for each task, where the LM is conditioned on input constraints and the last generated passage to decide if the passage is already "perfectly coherent", and if not generate a refined one.</p>
<p>ToT setup.We build a ToT with depth 2 (and only 1 intermediate thought step) -the LM first generates k = 5 plans and votes for the best one (Figure 4), then similarly generate k = 5 passages based on the best plan then vote for the best one.Here the breadth limit b = 1, as only one choice is kept per step.A simple zero-shot vote prompt ("analyze choices below, then conclude which is most promising for the instruction") is used to sample 5 votes at both steps.</p>
<p>Results. Figure 5(a) shows average GPT-4 scores across 100 tasks, where ToT (7.56) is deemed to generate more coherent passages than IO (6.19) and CoT (6.93) on average.While such an automatic metric might be noisy, Figure 5(b) confirms the finding by showing that humans prefer ToT over CoT in 41 out of 100 passage pairs, while only prefer CoT over ToT in 21 (other 38 pairs are found "similarly coherent").Lastly, iterative-refine is more effective on this natural language task, where  it improves IO coherency score from 6.19 to 7.67, and ToT coherency score from 7.56 to 7.91.We believe it could be thought of as a third approach to thought generation in the ToT framework, where new thoughts can arise from refining old thoughts instead of i.i.d. or sequentially generated.</p>
<p>Mini crosswords</p>
<p>In Game of 24 and Creative Writing, ToT is relatively shallow -at most 3 thought steps are needed to reach the final output.Here we explore 5 × 5 mini crosswords as a harder search problem involving natural language.Again, the goal is not just to solve the task, as more general crosswords can be readily solved with specialized NLP pipelines [34] that leverages large-scale retrieval instead of LM.Rather, we aim to explore the limit of LM as a general problem solver that explores its own thoughts and guides its own exploration with deliberate reasoning as heuristics.</p>
<p>Task setup.We scrape data from GooBix, which contains 156 games of 5 × 5 mini crosswords.As we observe adjacent games contain similar clues, we use 20 games with indices 1, 6, • • • , 91, 96 for testing, and games 136, 141, 146, 151, 156 for prompting.For each task, the input describes the 5 horizontal clues and 5 vertical clues, and the output should be a board of 5 × 5 = 25 letters to solve the crosswords.For evaluation, we consider three levels of success: the portion of correct letters (25 per game), words (10 per game), and games.</p>
<p>Baselines.We provide 5 example input-output pairs in the IO prompt, and in the CoT prompt additionally include intermediate words in the order h1..5 then v1..5.We run each prompt for 10 samples and average the results.</p>
<p>ToT setup.We leverage a depth-first search (Algorithm 2) that keeps exploring the most promising subsequent word clue until the state is no longer promising, then backtrack to the parent state to explore alternative thoughts.To make search tractable, subsequent thoughts are constrained not to change any filled words or letters, so that the ToT has at most 10 intermediate steps.For thought generation, at each state we translate all existing thoughts (e.g."h2.motor; h1.tasks" for the state in Figure 6(a)) into letter constraints for remaining clues (e.g."v1.To heap: tm ;...") and prompt a proposal prompt 5 times to come up with candidates for where and what to fill in the next word.Importantly, we also prompt the LM to give a confidence level for different thoughts, and aggregate these across proposals to obtain a sorted list of next thoughts to explore (Figure 6(a)).For state evaluations, we similarly translate each state into letter constraints for remaining clues, then evaluate for each clue if it is possible to fill given the constraints.If any remaining clue is deemed "impossible" to fill in (e.g."v1.To heap: tm s "), then the exploration of the state's subtree is pruned and DFS backtracks to its parent to explore the next promising thought.We limit DFS search steps to 100, and simply render the deepest explored state (the first explored one if multiple) into the final output.</p>
<p>Results.As shown in Table 3, IO and CoT prompting methods perform poorly with a word-level success rate less than 16%, while ToT significantly improves all metrics, achieving a word-level success rate of 60% and solving 4 out of 20 games.Such an improvement is not surprising, given IO and CoT lack mechanisms to try different clues, make changes to decisions, or backtrack.</p>
<p>Oracle and ablation studies.When outputting from the oracle best DFS state (instead of the heuristically determined best state) per task, ToT performance is even higher and actually solves 7/20 games (Table 3, "+best state"), indicating our simple output heuristics can be readily improved.Interestingly, sometimes when the crosswords game is actually solved, the state evaluator might still deem some words as "impossible" and prune -possibly because 5 × 5 crosswords by design have some rare or obselete words that GPT-4 cannot recognize 2 .Given the state evaluation as a pruning heuristic is imperfect, we also explore ablating the pruning, and find the performance generally worse (Table 3, "-prune").However, it could actually find the correct solution for 4/20 games (though only outputting 1 via heuristic), 3 of which are games ToT+pruning cannot solve within 100 steps.Thus, better heuristics for DFS pruning are critical for problem solving in this case.Lastly, we confirm the importance of backtracking by running an ablation that keeps filling the most promising clue for at most 20 steps, allowing overwrites.This is similar to a "greedy" BFS search with breadth limit of b = 1, and performs poorly with a word level success of only 20% (Table 3, "-backtrack").</p>
<p>Related Work</p>
<p>Planning and decision making.Smart planning and decision making are critical to achieving predefined goals.As they are trained on vast amount of world knowledge and human examples, LMs are known to have already absorbed rich commonsense that makes it possible to propose reasonable plans conditioned on problem setting and environmental states [12,42,37,13,35,41,40].Our proposed ToT approach extends existing planning formulations by considering multiple potentially feasible plans simultaneously at each problem-solving step, and proceeding with the most promising ones.The integration between thought sampling and value feedback organically integrates planning and decision-making mechanisms, enabling effective search inside a solution tree.On the other hand, traditional decision-making procedures usually require training dedicated reward and policy models as in reinforcement learning (for example CHAI [33]), whereas we use the LM itself to provide the value estimates for decision making.RAP [9] is a concurrent work that treats language model reasoning as planning with its internal world model, and proposes a MCTS-based method similar to ToT.However, its tasks are simpler than ours, and its framework lacks the modularity to incorporate different tree search algorithms.</p>
<p>Self-reflection.Using LLMs to assess the viability of their own predictions is becoming an increasingly important procedure in problem solving.[28,20,24] introduced the "self-reflection" mechanism, in which LMs provide feedback to their generation candidates.[4] improves LMs code generation accuracy by injecting feedback messages generated by the LM itself based on its code execution results.Similarly, [17] also introduces "critic" or review steps over the actions and states, deciding the next action to take in solving computer operation tasks.Another recent work very relevant to ours is "self-eval guided decoding" [39].Similar to our method, self-eval decoding also follows a tree-search procedure with leaves sampled from stochastic beam search decoding, which are then evaluated by LLM itself with carefully prepared self-eval prompts.Their approach however, uses the PAL formulation [8] which represents thoughts as codes, which makes it difficult to tackle challenging tasks like creative writing which we consider in this paper.Our Tree-of-Thought formulation is thus more versatile and handles challenging tasks on which GPT-4 only achieves very low accuracy with standard prompts.</p>
<p>Program-guided LLM generation.Our proposal is also related to recent advancements that organize LM's behavior with systematic procedures [14,44,6,43] or symbolic program guidance.For example, Schlag et al. [27] embeds LMs in an algorithmic search procedure to help solve problems like question answering step-by-step, in which the search trees are expanded by relevant paragraphs that might provide answers.This approach however differs from ours in that trees are expanded by sampling external paragraphs instead of the LM's own thoughts, and there is no reflection or voting steps.Another approach, LLM+P [18], goes one step further and delegates the actual planning process to a classical planner.</p>
<p>Classical search methods.Last but not least, our approach can be treated as a modern rendition of classical search methods for problem solving.For example it can be considered as a heuristic search algorithm like A<em> [10], in which the heuristic at each search node is provided by the LM's selfassessment.From this perspective, our method is also related to NeuroLogic A</em>esque decoding [19], which is inspired by A* search but introduces look-ahead heuristics that are efficient for LMs to improve the beam-search or top-k sampling decoding.This method however is constrained to sentence generation tasks, whereas our framework are designed for complex, multi-step problem solving guarded by value feedback.</p>
<p>Discussion</p>
<p>Limitations and future directions.Deliberate search such as ToT might not be necessary for many existing tasks that GPT-4 already excels at (see Appendix B.1), and as an initial step this work only explores three relatively simple tasks that challenges GPT-4 (see Appendix B.2 for some GPT-3.5 experiment results) and calls of better search and planning abilities incorporated with LMs.However, as we begin to deploy LMs for more real-world decision making applications (e.g.coding, data analysis, robotics, etc.), more complex tasks could emerge and present new opportunities to study these research questions.Also, search methods like ToT requires more resources (e.g.GPT-4 API cost) than sampling methods in order to improve task performances, but the modular flexibility of ToT allows users to customize such performance-cost tradeoffs, and ongoing open-source efforts [32] should readily reduce such costs in the near future.More details about cost and efficiency are in Appendix B.3.Lastly, this work focuses on using an off-the-shelf LM, and fine-tuning LMs using a ToT-style high-level counterfactual decision making (e.g.deliberating over potential choices for the next paragraph, instead of predicting the next token) might present opportunities to enhance the problem-solving capabilities of LMs.</p>
<p>Conclusion.The associative "System 1" of LMs can be beneficially augmented by a "System 2" based on searching a tree of possible paths to the solution to a problem.The Tree of Thoughts framework provides a way to translate classical insights about problem-solving into actionable methods for contemporary LMs.At the same time, LMs address a weakness of these classical methods, providing a way to solve complex problems that are not easily formalized, such as creative writing.We see this intersection of LMs with classical approaches to AI as an exciting direction.</p>
<p>Broader Impact</p>
<p>ToT is a framework that empowers LMs to more autonomously and intelligently make decisions and solve problems.While current tasks are limited to reasoning and search problems, future applications involving interaction with external environments or humans could bring potential danger, e.g.facilitating harmful uses of LMs.On the other hand, ToT also improves the interpretability of model decisions and the opportunity for human alignment, as the resulting representations are readable, high-level language reasoning instead of implicit, low-level token values.</p>
<p>Figure 2 :
2
Figure 2: ToT in a game of 24.The LM is prompted for (a) thought generation and (b) valuation.</p>
<p>Figure 3 :
3
Figure 3: Game of 24 (a) scale analysis &amp; (b) error analysis.</p>
<p>Figure 4 :Figure 5 :
45
Figure 4: A step of deliberate search in a randomly picked Creative Writing task.Given the input, the LM samples 5 different plans, then votes 5 times to decide which plan is best.The majority choice is used to consequently write the output passage with the same sample-vote procedure.</p>
<p>Figure 6 :
6
Figure 6: In Mini Crosswords, (a) how thoughts are proposed and aggregated in a priority queue for depth-first search (DFS), and (b) how a state is evaluated based on the possibility of filling in each remaining word clue, and pruned if any remaining clue is deemed not possible to fill by the LM.Then DFS backtracks to the parent state and explore the next promising thought for clue.</p>
<p>Table 1 :
1
Task overview.Input, output, thought examples are in blue.
Game of 24Creative Writing5x5 CrosswordsInput4 numbers (4 9 10 13)4 random sentences10 clues (h1. presented;..)OutputAn equation to reach 24A passage of 4 paragraphs5x5 letters: SHOWN;(13-9)<em>(10-4)=24ending in the 4 sentencesWIRRA; AVAIL; ...Thoughts3 intermediate equationsA short writing planWords to fill in for clues:(13-9=4 (left 4,4,10); 10-(1. Introduce a book that(h1. shown; v5. naled; ...)4=6 (left 4,6); 4</em>6=24)connects...)#ToT steps 315-10 (variable)</p>
<p>Table 2 :
2
Game of 24 Results.
SuccessIO prompt7.3%CoT prompt4.0%CoT-SC (k=100)9.0%ToT (ours) (b=1)45%ToT (ours) (b=5)74%IO + Refine (k=10) 27%IO (best of 100)33%CoT (best of 100)49%</p>
<p>Table 3 :
3
Mini Crosswords results.
MethodSuccess Rate (%)Letter Word GameIO38.7 140CoT40.6 15.6 1ToT (ours) 786020+best state 82.4 67.5 35-prune65.4 41.5 5-backtrack 54.6 205
Experiments were done between May 5-16,
.
For example, "agend" is an obsolete form of "agendum", but GPT-4 deems it a typo for "agenda". External retrieval or web interaction could augment LM for problem solving under knowledge uncertainty.
AcknowledgementsSY and KN acknowledge support from an Oracle Collaborative Research award and the National Science Foundation under Grant No. 2239363.Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.SY is also supported by the Harold W. Dodds Fellowship from Princeton.A Code, Prompts, TrajectoriesAll code is available at https://github.com/princeton-nlp/tree-of-thought-llm.All prompts are available at https://github.com/princeton-nlp/tree-of-thought-llm/tree/master/src/tot/prompts.Trajectories are available at https://github.com/princeton-nlp/tree-of-thought-llm/tree/master/logs.B Additional Experiment ResultsGiven the motivation of exploring and extending the capability frontier of language models, our experiments in the main paper have focused on a setup with the state-of-the-art language model (GPT-4), and three hard tasks invented to challenge it.Here, we report additional experiments with weaker LLM or easier tasks, and discuss cost and efficiency.While more common NLP tasks might be too easy for GPT-4 and do not require ToT (which is why we considered harder new tasks), we believe applying ToT to new tasks could be straightforward.For example, we implemented a simple and generic zero-shot ToT-BFS similar to creative writing (sample 5 problem solving strategies then vote for the best one; then sample 5 solutions based on the best strategy then vote for the best one) for GSM8K and StrategyQA with few extra lines of code:# define the answer format of new tasks gsm8k_format = '"the answer is n" where n is a number' strategyqa_format = 'either "the answer is yes" or "the answer is no"' # define zero-shot io prompting standard_prompt = 'Answer the following question with {format}: {input}' # define thought format for zero-shot cot and zero-shot tot cot_prompt = '''Answer the following question: {input} Make a strategy then write.Your output should be of the following format:Strategy: Your strategy about how to answer the question.Answer: Your answer to the question.It should end with {format}.''' # define zero-shot voting used for zero-shot tot vote_prompt = '''Given an instruction and several choices, decide which choice is most promising.Analyze each choice in detail, then conclude in the last line "The best choice is {s}", where s the integer id of the choice.'''We evaluated on a subset of 100 random GSM8K test and StrategyQA dev questions.As shown in Table4and as expected, ToT improves over CoT on both tasks (but only slightly, given GPT-4 + CoT is already very good on such tasks, and StrategyQA's bottleneck is external knowledge, not reasoning).Considering computational costs, it is more suitable to try smaller LLMs + ToT for traditional NLP tasks, or GPT-4 + ToT for hard tasks that challenge GPT-4 + CoT's reasoning.B.2 Extension to new LMs (GPT-3.5)To understand how ToT works with other LLMs, we also ran GPT-3.5-turbo for Creative Writing (Table6) and Game of 24 (Table5).On both tasks, "ToT &gt; CoT &gt; IO" remains true for GPT-3.5.On Creative Writing, we find GPT-3.5+ToToutperform GPT-4+IO, and similar to GPT-4+CoT, which suggests ToT could also work well on weaker language models.On Game of 24 (we changed 1-shot proposal prompt to 3-shot to make it work), GPT-3.5+ToT's19% is far worse than GPT-4+ToT's 74%.To further understand the importance of generation vs. evaluation, we ran GPT-4 generation + GPT-3.5 evaluation (64%) and GPT-3.5 generation + GPT-4 evaluation (31%).This suggests the game's bottleneck is thought generation, and different generation/evaluation language models might attain decent results while reducing costs.B.3 Cost and efficiencyRunning ToT requires significantly more computations than IO or CoT prompting.For example, in Game of 24 (Table7So completing Game of 24 and Creative Writing's main ToT experiments cost around 0.74 × 100 + 0.32 × 100 = 106 dollars.Crosswords' DFS experiments should be also within 100 dollars.In general, cost and efficiency of ToT highly depend on the prompts and search algorithms used, and could require 5-100 times more generated tokens than CoT.Some actionable insights:• We recommend using ToT on tasks requiring deliberate reasoning, on which CoT struggles.• Flexibility of ToT allows some performance-cost tradeoff, e.g., change beam size or vote number in BFS, few-shot vs. zero-shot prompting, GPT-3.5 vs. GPT-4, etc.One could configure the setup based on some resource constraints or performance goal.• There is much space for improving efficiency, e.g., BFS could early stop when solution is found, or trim down beam size to when some thoughts are "impossible".• We believe that more computation is indeed required in order for the model to achieve stronger intelligence, and this should not become a blocking issue as in the long run, (opensource) LMs will become much cheaper and more efficient.It is also a great direction how to better train/finetune LMs for thought generation and/or evaluation.
Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>A survey of monte carlo tree search methods. C Browne, E J Powley, D Whitehouse, S M M Lucas, P I Cowling, P Rohlfshagen, S Tavener, D P Liebana, S Samothrakis, S Colton, IEEE Transactions on Computational Intelligence and AI in Games. 42012</p>
<p>Deep blue. M Campbell, A J HoaneJr, F.-H Hsu, Artificial intelligence. 1341-22002</p>
<p>Teaching large language models to self-debug. X Chen, M Lin, N Schärli, D Zhou, 2023</p>
<p>A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>A Creswell, M Shanahan, arXiv:2208.14271Faithful reasoning using large language models. 2022arXiv preprint</p>
<p>Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control. N D Daw, Y Niv, P Dayan, Nature neuroscience. 8122005</p>
<p>L Gao, A Madaan, S Zhou, U Alon, P Liu, Y Yang, J Callan, G Neubig, Pal: Programaided language models. 2023</p>
<p>S Hao, Y Gu, H Ma, J J Hong, Z Wang, D Z Wang, Z Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023arXiv preprint</p>
<p>A formal basis for the heuristic determination of minimum cost paths. P E Hart, N J Nilsson, B Raphael, 10.1109/TSSC.1968.300136IEEE Transactions on Systems Science and Cybernetics. 19684</p>
<p>A formal basis for the heuristic determination of minimum cost paths. P E Hart, N J Nilsson, B Raphael, IEEE transactions on Systems Science and Cybernetics. 19684</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, 2022</p>
<p>Inner monologue: Embodied reasoning through planning with language models. W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, arXiv:2207.056082022arXiv preprint</p>
<p>Maieutic prompting: Logically consistent reasoning with recursive explanations. J Jung, L Qin, S Welleck, F Brahman, C Bhagavatula, R L Bras, Y Choi, arXiv:2205.118222022arXiv preprint</p>
<p>Thinking, fast and slow. D Kahneman, 2011Macmillan</p>
<p>Representativeness revisited: Attribute substitution in intuitive judgment. Heuristics and biases: The psychology of intuitive judgment. D Kahneman, S Frederick, 20024974</p>
<p>Language models can solve computer tasks. G Kim, P Baldi, S Mcaleer, 2023</p>
<p>B Liu, Y Jiang, X Zhang, Q Liu, S Zhang, J Biswas, P Stone, Llm+p: Empowering large language models with optimal planning proficiency. 2023</p>
<p>Neurologic a*esque decoding: Constrained text generation with lookahead heuristics. X Lu, S Welleck, P West, L Jiang, J Kasai, D Khashabi, R L Bras, L Qin, Y Yu, R Zellers, N A Smith, Y Choi, North American Chapter. the Association for Computational Linguistics2021</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, S Welleck, B P Majumder, S Gupta, A Yazdanbakhsh, P Clark, 2023</p>
<p>Report on a general problem solving program. A Newell, J C Shaw, H A Simon, IFIP congress. Pittsburgh, PA195925664</p>
<p>Human problem solving. A Newell, H A Simon, 1972Prentice-Hall</p>
<p>. ArXiv, abs/2303.08774OpenAI. Gpt-4 technical report. 2023</p>
<p>D Paul, M Ismayilzada, M Peyrard, B Borges, A Bosselut, R West, B Faltings, Refiner: Reasoning feedback on intermediate representations. 2023</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, OpenAI blog. 2018</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>Large language model programs. I Schlag, S Sukhbaatar, A Celikyilmaz, W Yih, J Weston, J Schmidhuber, X Li, 2023</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. N Shinn, B Labash, A Gopinath, 2023</p>
<p>Mastering the game of go without human knowledge. D Silver, J Schrittwieser, K Simonyan, I Antonoglou, A Huang, A Guez, T Hubert, L Baker, M Lai, A Bolton, nature. 55076762017</p>
<p>The empirical case for two systems of reasoning. S A Sloman, Psychological bulletin. 119131996</p>
<p>Who is rational? Studies of individual differences in reasoning. K E Stanovich, 1999Psychology Press</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Chai: A chatbot ai for task-oriented dialogue with offline reinforcement learning. S Verma, J Fu, S Yang, S Levine, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language Technologies2022</p>
<p>E Wallace, N Tomlin, A Xu, K Yang, E Pathak, M Ginsberg, D Klein, arXiv:2205.09665Automated crossword solving. 2022arXiv preprint</p>
<p>Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. L Wang, W Xu, Y Lan, Z Hu, Y Lan, R K W Lee, E.-P Lim, 2023</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q Le, E Chi, D Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. Z Wang, S Cai, A Liu, X Ma, Y Liang, 2023</p>
<p>J Wei, X Wang, D Schuurmans, M Bosma, E Chi, Q Le, D Zhou, arXiv:2201.11903Chain of thought prompting elicits reasoning in large language models. 2022arXiv preprint</p>
<p>Decomposition enhances reasoning via self-evaluation guided decoding. Y Xie, K Kawaguchi, Y Zhao, X Zhao, M.-Y Kan, J He, Q Xie, 2023</p>
<p>Foundation models for decision making: Problems, methods, and opportunities. S Yang, O Nachum, Y Du, J Wei, P Abbeel, D Schuurmans, 2023</p>
<p>S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, arXiv:2210.03629ReAct: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Planning with large language models for code generation. S Zhang, Z Chen, Y Shen, M Ding, J B Tenenbaum, C Gan, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Least-to-most prompting enables complex reasoning in large language models. D Zhou, N Schärli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, C Cui, O Bousquet, Q Le, arXiv:2205.106252022arXiv preprint</p>
<p>X Zhu, J Wang, L Zhang, Y Zhang, R Gan, J Zhang, Y Yang, arXiv:2210.16257Solving math word problem via cooperative reasoning induced language models. 2022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
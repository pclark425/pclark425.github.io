<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7199 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7199</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7199</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-764b371693860195c2c92991cad92419720e002c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/764b371693860195c2c92991cad92419720e002c" target="_blank">Generating English from Abstract Meaning Representations</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Natural Language Generation</p>
                <p><strong>Paper TL;DR:</strong> A method that learns to linearize tokens of AMR graphs into an English-like order is introduced, which reduces the amount of distortion in PBMT and increases generation quality.</p>
                <p><strong>Paper Abstract:</strong> We present a method for generating English sentences from Abstract Meaning Representation (AMR) graphs, exploiting a parallel corpus of AMRs and English sentences. We treat AMR-to-English generation as phrase-based machine translation (PBMT). We introduce a method that learns to linearize tokens of AMR graphs into an English-like order. Our linearization reduces the amount of distortion in PBMT and increases generation quality. We report a Bleu score of 26.8 on the standard AMR/English test set.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7199.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7199.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMR-DFS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pre-order Depth-First Search AMR Linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline sequential tokenization of AMR graphs obtained by a straightforward pre-order depth-first traversal that emits concept and role tokens in traversal order to produce a source string for a phrase-based MT system.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Pre-order DFS linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode the AMR graph as a flat token sequence by performing a pre-order depth-first traversal: emit the concept at a node, then emit each outgoing role and recursively traverse its subtree in traversal order; tokens include concept labels, role labels, and variable placeholders as in the AMR corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based, lossy</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>pre-order depth-first traversal (DFS)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR 1.0 (LDC 2014T12)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-English generation (graph-to-text generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phrase-Based Machine Translation (Moses PBMT) with 5-gram language model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Moses PBMT: phrase extraction (max phrase length 9, yielding ~1.2M phrase pairs), MERT tuning, decoding with stack size 1000; 5-gram language model trained on 1.7B Gigaword tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (single-reference, case-insensitive, 1..4-grams)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Dev BLEU 17.7; Test BLEU 16.6 (without AMR cleaning or specialized realization components)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Serves as a simple baseline but yields many alignment crossings and poor locality, increasing PBMT distortion and harming translation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>High number of alignment crossings and poor locality between semantically-related tokens; does not reorder to resemble English, causing lower BLEU; representation is lossy when downstream cleaning is applied and some AMR details must be reinserted later.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Worse than the Majority and Classifier linearization methods: lower BLEU and many more alignment crossings (used as the baseline for reductions reported in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating English from Abstract Meaning Representations', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7199.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7199.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMR-Majority</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Majority-order AMR Linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned, canonical sequential encoding that memorizes the most frequent sibling-edge ordering for each set of AMR role labels and falls back to the original AMR ordering when unseen, producing token sequences more English-like than raw DFS.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Majority-method linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each set of sibling outgoing edges (role labels) observed in training, store the most common permutation; at linearization time, emit concept and siblings in that stored order; fallback: use original annotated AMR order with :instance-of first.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based, lossy, canonical</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>memorized majority ordering of sibling role-sets (lookup-based permutation), fallback to annotated AMR order</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR 1.0 (LDC 2014T12)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-English generation (graph-to-text generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phrase-Based Machine Translation (Moses PBMT) with 5-gram language model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same PBMT setup as above: Moses, phrase extraction to ~1.2M phrase pairs, 5-gram Gigaword LM (1.7B tokens), MERT tuning, decoding stack size 1000.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (single-reference, case-insensitive, 1..4-grams); alignment crossings (raw counts)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>With AMR cleaning and name/number/date realizers: Dev BLEU 26.5; Test BLEU 25.6. Alignment crossings after linearization reported as 33772 (with adjacent crossings 4850) as shown in Table 2 (numbers given in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Substantially reduced alignment crossings compared to pre-order DFS and improved locality, yielding large BLEU gains (≈+9 BLEU on test relative to raw DFS baseline with no cleaning).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on memorized observed orderings—may not generalize to unseen role-sets; still lossy because subsequent cleaning removes variables/sense-tags and some roles; requires sufficient training examples to learn reliable majority orders.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Outperforms Pre-order DFS (higher BLEU, fewer crossings) but is outperformed by the Classifier Method in BLEU and adjacent-crossing reduction according to reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating English from Abstract Meaning Representations', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7199.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7199.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMR-Classifier</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Classifier-Based AMR Linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned permutation function for sibling edges implemented via three maximum-entropy binary classifiers that decide whether to drop :instance-of, whether each edge appears before :instance-of, and pairwise ordering between edges, producing an English-like linearization that reduces alignment crossings and improves generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Classifier-method linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Linearization produced by a learned function decomposed into three binary classifiers: (1) whether to drop the :instance-of edge; (2) for each edge whether it should appear before :instance-of; (3) pairwise ordering decisions between edges; final order obtained by computing pairwise probabilities and recursively selecting edges by a left-leaning score.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based, lossy, canonical</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>data-driven permutation via three binary maximum-entropy classifiers (features include role labels, concepts, and pairwise role combinations); left-leaning score for final ordering</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR 1.0 (LDC 2014T12)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-English generation (graph-to-text generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phrase-Based Machine Translation (Moses PBMT) with 5-gram language model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same PBMT pipeline: Moses phrase-based MT; ~1.2M phrase pairs; 5-gram LM on 1.7B Gigaword tokens; MERT tuning; decoding stack size 1000. Classifiers trained with Zhang (2004) max-entropy toolkit.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (single-reference, case-insensitive, 1..4-grams); alignment crossings (raw counts); concept-dropping accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>With AMR cleaning and name/number/date realizers: Dev BLEU 27.2; Test BLEU 26.9. Alignment crossings reported as 35603 (adjacent crossings 4015) in Table 2. Concept dropping: 97% of concepts dropped by the classifier are indeed unaligned; classifier correctly drops 87% of the unaligned concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Reduces alignment crossings and improves locality more than the Majority method (notably fewer adjacent crossings), producing the best end-to-end BLEU in the study (≈+1.3 BLEU over Majority in dev and +1.3 on test in reported experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Still a lossy sequentialization that may discard AMR details (intentional concept dropping allowed); requires annotated alignments to train ordering classifiers; complexity of pairwise classifiers and recursive ordering can be more computationally involved than a simple lookup; downstream generation still required specialized components to realize names/numbers/dates and to reinsert omitted details.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Outperforms both Pre-order DFS and the Majority Method in BLEU and in reducing adjacent alignment crossings; also outperforms prior tree-transducer approach (Flanigan et al., 2016) by ~4.9 BLEU on test in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating English from Abstract Meaning Representations', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7199.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7199.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMR-Cleaning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMR String Cleaning and Specialized Realizers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Post-processing applied to linearized AMR strings: removal of variables, quotes, sense tags, and certain roles/concepts, plus the use of specialized realizers for names, dates, and numbers to produce cleaner input strings for PBMT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Cleaned linearized AMR strings (with name/number/date realizers)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>After linearization, remove variable identifiers, quotation marks, PropBank sense-tags, *-quantity/*-entity concepts, and roles such as :op*, :snt*, :arg0-2, :name, :quant, :unit, :value, :year, :domain-of; additionally, replace or extend training data with specialized realizers for names, dates, and numbers (added to training corpus) to improve generation.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based, lossy (intentional information removal and normalization)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>post-linearization filtering/normalization plus augmentation of training data with specialized realizations</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR 1.0 (LDC 2014T12)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-English generation (graph-to-text generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phrase-Based Machine Translation (Moses PBMT) with 5-gram language model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Moses PBMT described above; name/number/date components are added to the training data as specialized realizers before PBMT training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (single-reference, case-insensitive, 1..4-grams)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Applying cleaning increases BLEU substantially: Pre-order DFS + cleaning: Dev BLEU 21.6 (from 17.7), Test BLEU 21.0 (from 16.6). Adding specialized name/number/date realizers further increases to Dev BLEU 23.5 and Test BLEU 22.5 (intermediate results reported in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Strong positive effect: cleaning and specialized realizers materially improved PBMT quality by reducing noisy tokens and providing concrete realizations for otherwise abstract AMR concepts, yielding several BLEU points of improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lossy by design — removes or abstracts away AMR information (variables, sense-tags, some roles) which means structural information is discarded; requires external heuristics or realizers for names/dates/numbers; may not be suitable when preservation of full AMR details is required.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>A necessary preprocessing step for the PBMT pipeline in this work; essential to bring baseline DFS performance closer to that of advanced linearization methods but still insufficient alone to reach the best results achieved by classifier-based ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating English from Abstract Meaning Representations', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generation from abstract meaning representation using tree transducers <em>(Rating: 2)</em></li>
                <li>Aligning English strings with Abstract Meaning Representation graphs <em>(Rating: 2)</em></li>
                <li>Abstract Meaning Representation for sembanking <em>(Rating: 2)</em></li>
                <li>Source-side classifier preordering for machine translation <em>(Rating: 1)</em></li>
                <li>Moses: Open source toolkit for statistical machine translation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7199",
    "paper_id": "paper-764b371693860195c2c92991cad92419720e002c",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "AMR-DFS",
            "name_full": "Pre-order Depth-First Search AMR Linearization",
            "brief_description": "A baseline sequential tokenization of AMR graphs obtained by a straightforward pre-order depth-first traversal that emits concept and role tokens in traversal order to produce a source string for a phrase-based MT system.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Pre-order DFS linearization",
            "representation_description": "Encode the AMR graph as a flat token sequence by performing a pre-order depth-first traversal: emit the concept at a node, then emit each outgoing role and recursively traverse its subtree in traversal order; tokens include concept labels, role labels, and variable placeholders as in the AMR corpus.",
            "representation_type": "sequential, token-based, lossy",
            "encoding_method": "pre-order depth-first traversal (DFS)",
            "canonicalization": true,
            "average_token_length": null,
            "dataset_name": "AMR 1.0 (LDC 2014T12)",
            "task_name": "AMR-to-English generation (graph-to-text generation)",
            "model_name": "Phrase-Based Machine Translation (Moses PBMT) with 5-gram language model",
            "model_description": "Moses PBMT: phrase extraction (max phrase length 9, yielding ~1.2M phrase pairs), MERT tuning, decoding with stack size 1000; 5-gram language model trained on 1.7B Gigaword tokens.",
            "performance_metric": "BLEU (single-reference, case-insensitive, 1..4-grams)",
            "performance_value": "Dev BLEU 17.7; Test BLEU 16.6 (without AMR cleaning or specialized realization components)",
            "impact_on_training": "Serves as a simple baseline but yields many alignment crossings and poor locality, increasing PBMT distortion and harming translation quality.",
            "limitations": "High number of alignment crossings and poor locality between semantically-related tokens; does not reorder to resemble English, causing lower BLEU; representation is lossy when downstream cleaning is applied and some AMR details must be reinserted later.",
            "comparison_with_other": "Worse than the Majority and Classifier linearization methods: lower BLEU and many more alignment crossings (used as the baseline for reductions reported in the paper).",
            "uuid": "e7199.0",
            "source_info": {
                "paper_title": "Generating English from Abstract Meaning Representations",
                "publication_date_yy_mm": "2016-09"
            }
        },
        {
            "name_short": "AMR-Majority",
            "name_full": "Majority-order AMR Linearization",
            "brief_description": "A learned, canonical sequential encoding that memorizes the most frequent sibling-edge ordering for each set of AMR role labels and falls back to the original AMR ordering when unseen, producing token sequences more English-like than raw DFS.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Majority-method linearization",
            "representation_description": "For each set of sibling outgoing edges (role labels) observed in training, store the most common permutation; at linearization time, emit concept and siblings in that stored order; fallback: use original annotated AMR order with :instance-of first.",
            "representation_type": "sequential, token-based, lossy, canonical",
            "encoding_method": "memorized majority ordering of sibling role-sets (lookup-based permutation), fallback to annotated AMR order",
            "canonicalization": true,
            "average_token_length": null,
            "dataset_name": "AMR 1.0 (LDC 2014T12)",
            "task_name": "AMR-to-English generation (graph-to-text generation)",
            "model_name": "Phrase-Based Machine Translation (Moses PBMT) with 5-gram language model",
            "model_description": "Same PBMT setup as above: Moses, phrase extraction to ~1.2M phrase pairs, 5-gram Gigaword LM (1.7B tokens), MERT tuning, decoding stack size 1000.",
            "performance_metric": "BLEU (single-reference, case-insensitive, 1..4-grams); alignment crossings (raw counts)",
            "performance_value": "With AMR cleaning and name/number/date realizers: Dev BLEU 26.5; Test BLEU 25.6. Alignment crossings after linearization reported as 33772 (with adjacent crossings 4850) as shown in Table 2 (numbers given in the paper).",
            "impact_on_training": "Substantially reduced alignment crossings compared to pre-order DFS and improved locality, yielding large BLEU gains (≈+9 BLEU on test relative to raw DFS baseline with no cleaning).",
            "limitations": "Relies on memorized observed orderings—may not generalize to unseen role-sets; still lossy because subsequent cleaning removes variables/sense-tags and some roles; requires sufficient training examples to learn reliable majority orders.",
            "comparison_with_other": "Outperforms Pre-order DFS (higher BLEU, fewer crossings) but is outperformed by the Classifier Method in BLEU and adjacent-crossing reduction according to reported experiments.",
            "uuid": "e7199.1",
            "source_info": {
                "paper_title": "Generating English from Abstract Meaning Representations",
                "publication_date_yy_mm": "2016-09"
            }
        },
        {
            "name_short": "AMR-Classifier",
            "name_full": "Classifier-Based AMR Linearization",
            "brief_description": "A learned permutation function for sibling edges implemented via three maximum-entropy binary classifiers that decide whether to drop :instance-of, whether each edge appears before :instance-of, and pairwise ordering between edges, producing an English-like linearization that reduces alignment crossings and improves generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Classifier-method linearization",
            "representation_description": "Linearization produced by a learned function decomposed into three binary classifiers: (1) whether to drop the :instance-of edge; (2) for each edge whether it should appear before :instance-of; (3) pairwise ordering decisions between edges; final order obtained by computing pairwise probabilities and recursively selecting edges by a left-leaning score.",
            "representation_type": "sequential, token-based, lossy, canonical",
            "encoding_method": "data-driven permutation via three binary maximum-entropy classifiers (features include role labels, concepts, and pairwise role combinations); left-leaning score for final ordering",
            "canonicalization": true,
            "average_token_length": null,
            "dataset_name": "AMR 1.0 (LDC 2014T12)",
            "task_name": "AMR-to-English generation (graph-to-text generation)",
            "model_name": "Phrase-Based Machine Translation (Moses PBMT) with 5-gram language model",
            "model_description": "Same PBMT pipeline: Moses phrase-based MT; ~1.2M phrase pairs; 5-gram LM on 1.7B Gigaword tokens; MERT tuning; decoding stack size 1000. Classifiers trained with Zhang (2004) max-entropy toolkit.",
            "performance_metric": "BLEU (single-reference, case-insensitive, 1..4-grams); alignment crossings (raw counts); concept-dropping accuracy",
            "performance_value": "With AMR cleaning and name/number/date realizers: Dev BLEU 27.2; Test BLEU 26.9. Alignment crossings reported as 35603 (adjacent crossings 4015) in Table 2. Concept dropping: 97% of concepts dropped by the classifier are indeed unaligned; classifier correctly drops 87% of the unaligned concepts.",
            "impact_on_training": "Reduces alignment crossings and improves locality more than the Majority method (notably fewer adjacent crossings), producing the best end-to-end BLEU in the study (≈+1.3 BLEU over Majority in dev and +1.3 on test in reported experiments).",
            "limitations": "Still a lossy sequentialization that may discard AMR details (intentional concept dropping allowed); requires annotated alignments to train ordering classifiers; complexity of pairwise classifiers and recursive ordering can be more computationally involved than a simple lookup; downstream generation still required specialized components to realize names/numbers/dates and to reinsert omitted details.",
            "comparison_with_other": "Outperforms both Pre-order DFS and the Majority Method in BLEU and in reducing adjacent alignment crossings; also outperforms prior tree-transducer approach (Flanigan et al., 2016) by ~4.9 BLEU on test in this paper's experiments.",
            "uuid": "e7199.2",
            "source_info": {
                "paper_title": "Generating English from Abstract Meaning Representations",
                "publication_date_yy_mm": "2016-09"
            }
        },
        {
            "name_short": "AMR-Cleaning",
            "name_full": "AMR String Cleaning and Specialized Realizers",
            "brief_description": "Post-processing applied to linearized AMR strings: removal of variables, quotes, sense tags, and certain roles/concepts, plus the use of specialized realizers for names, dates, and numbers to produce cleaner input strings for PBMT.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Cleaned linearized AMR strings (with name/number/date realizers)",
            "representation_description": "After linearization, remove variable identifiers, quotation marks, PropBank sense-tags, *-quantity/*-entity concepts, and roles such as :op*, :snt*, :arg0-2, :name, :quant, :unit, :value, :year, :domain-of; additionally, replace or extend training data with specialized realizers for names, dates, and numbers (added to training corpus) to improve generation.",
            "representation_type": "sequential, token-based, lossy (intentional information removal and normalization)",
            "encoding_method": "post-linearization filtering/normalization plus augmentation of training data with specialized realizations",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "AMR 1.0 (LDC 2014T12)",
            "task_name": "AMR-to-English generation (graph-to-text generation)",
            "model_name": "Phrase-Based Machine Translation (Moses PBMT) with 5-gram language model",
            "model_description": "Moses PBMT described above; name/number/date components are added to the training data as specialized realizers before PBMT training.",
            "performance_metric": "BLEU (single-reference, case-insensitive, 1..4-grams)",
            "performance_value": "Applying cleaning increases BLEU substantially: Pre-order DFS + cleaning: Dev BLEU 21.6 (from 17.7), Test BLEU 21.0 (from 16.6). Adding specialized name/number/date realizers further increases to Dev BLEU 23.5 and Test BLEU 22.5 (intermediate results reported in Table 3).",
            "impact_on_training": "Strong positive effect: cleaning and specialized realizers materially improved PBMT quality by reducing noisy tokens and providing concrete realizations for otherwise abstract AMR concepts, yielding several BLEU points of improvement.",
            "limitations": "Lossy by design — removes or abstracts away AMR information (variables, sense-tags, some roles) which means structural information is discarded; requires external heuristics or realizers for names/dates/numbers; may not be suitable when preservation of full AMR details is required.",
            "comparison_with_other": "A necessary preprocessing step for the PBMT pipeline in this work; essential to bring baseline DFS performance closer to that of advanced linearization methods but still insufficient alone to reach the best results achieved by classifier-based ordering.",
            "uuid": "e7199.3",
            "source_info": {
                "paper_title": "Generating English from Abstract Meaning Representations",
                "publication_date_yy_mm": "2016-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generation from abstract meaning representation using tree transducers",
            "rating": 2
        },
        {
            "paper_title": "Aligning English strings with Abstract Meaning Representation graphs",
            "rating": 2
        },
        {
            "paper_title": "Abstract Meaning Representation for sembanking",
            "rating": 2
        },
        {
            "paper_title": "Source-side classifier preordering for machine translation",
            "rating": 1
        },
        {
            "paper_title": "Moses: Open source toolkit for statistical machine translation",
            "rating": 1
        }
    ],
    "cost": 0.010583,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Generating English from Abstract Meaning Representations</h1>
<p>Nima Pourdamghani , Kevin Knight , Ulf Hermjakob<br>Information Sciences Institute<br>Department of Computer Science<br>University of Southern California<br>{damghani,knight,ulf}@isi.edu</p>
<h4>Abstract</h4>
<p>We present a method for generating English sentences from Abstract Meaning Representation (AMR) graphs, exploiting a parallel corpus of AMRs and English sentences. We treat AMR-to-English generation as phrase-based machine translation (PBMT). We introduce a method that learns to linearize tokens of AMR graphs into an English-like order. Our linearization reduces the amount of distortion in PBMT and increases generation quality. We report a Bleu score of 26.8 on the standard AMR/English test set.</p>
<h2>1 Introduction</h2>
<p>Banarescu et al. (2013) introduce Abstract Meaning Representation (AMR) graphs to represent sentence level semantics. Human annotators have created a dataset of more than 10,000 AMR/English string pairs.</p>
<p>AMRs are directed acyclic graphs, where leaves are labeled with concepts, internal nodes are labeled with variables representing instances of those concepts, and edges are labeled with roles that relate pairs of concepts. For instance, the sentence The boy wants to go is represented as:</p>
<div class="codehilite"><pre><span></span><code><span class="p">(</span><span class="n">w</span><span class="w"> </span><span class="err">:</span><span class="n">instance</span><span class="o">-</span><span class="k">of</span><span class="w"> </span><span class="n">want</span><span class="o">-</span><span class="mi">01</span>
<span class="w">    </span><span class="err">:</span><span class="n">arg0</span><span class="w"> </span><span class="p">(</span><span class="n">b</span><span class="w"> </span><span class="err">:</span><span class="n">instance</span><span class="o">-</span><span class="k">of</span><span class="w"> </span><span class="n">boy</span><span class="p">)</span>
<span class="w">    </span><span class="err">:</span><span class="n">arg1</span><span class="w"> </span><span class="p">(</span><span class="n">g</span><span class="w"> </span><span class="err">:</span><span class="n">instance</span><span class="o">-</span><span class="k">of</span><span class="w"> </span><span class="k">go</span><span class="o">-</span><span class="mi">01</span>
<span class="w">        </span><span class="err">:</span><span class="n">arg0</span><span class="w"> </span><span class="n">b</span><span class="p">))</span>
</code></pre></div>

<p>Colons discriminate roles from concepts. In this paper, :instance-of is our way of writing the slash $(/)$ found in the AMR corpus.</p>
<p>Because AMR and English are highly cognate, the AMR-to-English generation problem might seem similar to previous natural language generation (NLG) problems such as bag generation (Brown et al., 1990), restoring order to unordered dependency trees (Guo et al., 2011) or generation from logical form (Corston-Oliver et al., 2002). However, AMR's deeper logic provides a serious challenge for English realization. AMR also abstracts away details of time, number, and voice, which must be inserted.</p>
<p>Langkilde and Knight (1998) introduced Nitrogen, which used a precursor of AMR for generating English. Recently, Flanigan et al. (2016) presented the first trained AMR-to-English generator. They generate spanning trees from AMR graphs and apply tree-to-string transducers to the trees to generate English.</p>
<p>We attack AMR-to-English generation using the tools of phrase-based machine translation (PBMT). PBMT has already been applied to natural language generation from simple semantic structures (Mairesse et al., 2010), but deep semantic representations such as AMR are more challenging to deal with. PBMT expects strings for its source and target languages, so we cannot work with AMR graphs as input. Therefore, we develop a method that learns to linearize AMR graphs into AMR strings. Our linearization strives to put AMR tokens roughly into English word order, making the transformation to English easier.</p>
<p>It may seem surprising that we ignore much of the structure of AMR, but we follow string-based statistical MT, which ignored much of the structure of</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: AMR-to-English generation pipeline.
language but nonetheless provided a strong baseline.
Figure 1 shows our pipeline for generating English from AMR. Our contributions are:</p>
<ol>
<li>We present a strong baseline method for AMR-to-English generation.</li>
<li>We introduce a method that learns to linearize AMR tokens into an order resembling English.</li>
<li>We obtain a Bleu score of 26.8 on the standard AMR/English test set, which is 4.9 points higher than previous work.</li>
</ol>
<h2>2 Method</h2>
<p>Given a set of AMR/English pairs, divided into train, development, and test sets, we follow these steps:
Construct token-level alignments: We use the method proposed in (Pourdamghani et al., 2014) to construct alignments between AMR and English tokens in the training set.
Extend training data: We use special realization components for names, dates, and numbers found in the dev/test sets, adding their results to the training corpus.
Linearize AMR graphs: We learn to convert AMR graphs into AMR strings in a way that linearized AMR tokens have an English-like order (Section 3). Clean AMR strings: We remove variables, quote marks, and sense tags from linearized AMRs. We also remove <em>-quantity and </em>-entity concepts, plus these roles: :op<em>, :snt</em>, :arg0, :arg1, :arg2, :name, :quant, :unit, :value, :year, :domain-of.
Phrase-Based Machine Translation: We use Moses (Koehn et al., 2007) to train and tune a PBMT system on string/string training data. We then use this system to produce English realizations from linearized development and test AMRs.</p>
<h2>3 Linearization</h2>
<p>When we linearize AMR, we would like-at a minimum-for semantically-related tokens to stay close together. A straightforward, pre-order depth first search (DFS) accomplishes this (Pourdamghani et al., 2014). For instance, linearizing</p>
<div class="codehilite"><pre><span></span><code><span class="p">(</span><span class="n">w</span><span class="w"> </span><span class="err">:</span><span class="n">instance</span><span class="o">-</span><span class="k">of</span><span class="w"> </span><span class="n">want</span><span class="o">-</span><span class="mi">01</span>
<span class="w">    </span><span class="err">:</span><span class="n">arg0</span><span class="w"> </span><span class="p">(</span><span class="n">b</span><span class="w"> </span><span class="err">:</span><span class="n">instance</span><span class="o">-</span><span class="k">of</span><span class="w"> </span><span class="n">boy</span><span class="p">)</span>
<span class="w">    </span><span class="err">:</span><span class="n">arg1</span><span class="w"> </span><span class="p">(</span><span class="n">g</span><span class="w"> </span><span class="err">:</span><span class="n">instance</span><span class="o">-</span><span class="k">of</span><span class="w"> </span><span class="k">go</span><span class="o">-</span><span class="mi">01</span>
<span class="w">        </span><span class="err">:</span><span class="n">arg0</span><span class="w"> </span><span class="n">b</span><span class="p">))</span>
</code></pre></div>

<p>yields " $w$ :instance-of want-01 :arg0 $b$ :instance-of boy :arg1 $g$ :instance-of go-01 :arg0 $b$ ".</p>
<p>Of course, we are free to visit AMR sister nodes in any order. For instance, if we visit sisters in order (:arg0, :instance-of, :arg1), we get this string instead: " $w$ :arg0 $b$ :instance-of boy :instance-of want-01 :arg1 $g$ :instance-of go-01 :arg0 $b$ ", which more resembles English word order.</p>
<p>We therefore induce an ordering function that takes any set of edge labels as input and produces a permutation of those labels. We call this the linearization function.</p>
<p>The input to this function is a sequence consisting of the concept under the :instance-of edge (e.g., want-01) followed by the other edges sorted alphabetically (e.g., :arg0 :arg1). The output is a permutation of the input (e.g., $(2,1,3)$ ).</p>
<p>Because :instance-of concepts often have no equivalent in English, e.g.:</p>
<div class="codehilite"><pre><span></span><code><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">:</span><span class="n">instance</span><span class="o">-</span><span class="kr">of</span><span class="w"> </span><span class="n">name</span>
<span class="w">    </span><span class="o">:</span><span class="n">op1</span><span class="w"> </span><span class="s">&quot;Pierre&quot;</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Pierre</span><span class="w"> </span><span class="n">Vinken</span>
<span class="w">    </span><span class="o">:</span><span class="n">op2</span><span class="w"> </span><span class="s">&quot;Vinken&quot;</span><span class="p">)</span>
</code></pre></div>

<p>we additionally allow the first component of the output to be " -1 ", indicating deletion.</p>
<p>Our linearization function therefore has the following form:</p>
<p>$$
p:\left{c, r_{1}, r_{2}, \ldots, r_{k-1}\right} \rightarrow\left(\pi_{1}, \pi_{2}, \ldots, \pi_{k}\right)
$$</p>
<p>where $c$ is a concept token, $r_{i}$ are role tokens, $\pi_{i&gt;1} \in$ ${1,2, \ldots, k}$ and $\pi_{1} \in{-1,1,2, \ldots, k}$.</p>
<p>Here are sample input/output pairs for the linearization function:</p>
<div class="codehilite"><pre><span></span><code><span class="p">(</span><span class="n">want</span><span class="o">-</span><span class="mo">01</span><span class="p">,</span><span class="w"> </span><span class="o">:</span><span class="n">arg0</span><span class="p">,</span><span class="w"> </span><span class="o">:</span><span class="n">arg1</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span>
<span class="p">(</span><span class="n">name</span><span class="p">,</span><span class="w"> </span><span class="o">:</span><span class="n">op1</span><span class="p">,</span><span class="w"> </span><span class="o">:</span><span class="n">op2</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span>
<span class="p">(</span><span class="kr">and</span><span class="p">,</span><span class="w"> </span><span class="o">:</span><span class="n">op1</span><span class="p">,</span><span class="w"> </span><span class="o">:</span><span class="n">op2</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span>
<span class="p">(</span><span class="n">area</span><span class="o">-</span><span class="n">quantity</span><span class="p">,</span><span class="w"> </span><span class="o">:</span><span class="n">quant</span><span class="p">,</span><span class="w"> </span><span class="o">:</span><span class="n">unit</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span>
<span class="p">(</span><span class="n">win</span><span class="o">-</span><span class="mo">01</span><span class="p">,</span><span class="w"> </span><span class="o">:</span><span class="n">arg0</span><span class="p">,</span><span class="w"> </span><span class="o">:</span><span class="n">arg1</span><span class="p">,</span><span class="w"> </span><span class="o">:</span><span class="n">time</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">)</span>
</code></pre></div>

<p>Our overall objective is to minimize the number of crossings in the alignment links after linearization. We use our token-aligned AMR/English data to produce training examples for the function (1). We assign each outgoing AMR edge a position equal to the median of the alignment points of all tokens in its subtree, including the edge itself. We assign -1 to an edge if none of its subtree tokens are aligned. Then we extract all sets of sibling edges in the AMR graph, and sort them based on these numbers. We use these sorted sets to create training instances.</p>
<p>We now describe three linearization methods.</p>
<h3>3.1 Pre-order DFS</h3>
<p>This baseline method linearizes AMR by simple preorder traversal, ignoring the data just described.</p>
<h3>3.2 Majority Method</h3>
<p>The majority method memorizes the most common order for each role set in the data. If no match is found, we use the ordering given in the original, human-annotated AMR, with the :instance-of edge first.</p>
<h3>3.3 Classifier Method</h3>
<p>The classifier method breaks the problem into learning three binary classifiers over inputs of the form $\left(c, r_{1}, r_{2}, \ldots, r_{k-1}\right):$</p>
<ol>
<li>
<p>Should the :instance-of edge be dropped?</p>
</li>
<li>
<p>Features: $k, c,\left(c, r_{i}\right)$, whether $c$ is a Propbank frameset, and whether $c$ is a "special keyword" as defined by Banarescu et al. (2013).</p>
</li>
<li>
<p>Should edge $r_{i}$ appear before :instance-of?</p>
</li>
<li>
<p>Features: $r_{i},\left(c, r_{i}\right),\left(r_{i}, r_{j}\right)$ for all $j \neq i$</p>
</li>
<li>
<p>Should edge $r_{i}$ appear before $r_{j}$ ?</p>
</li>
<li>
<p>Features: $\left(c, r_{i}, r_{j}\right)$</p>
</li>
</ol>
<p>We use the toolkit of Zhang (2004) to learn a maximum entropy classifier for each task.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">AMR/English pairs</th>
<th style="text-align: right;">English word tokens</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Train</td>
<td style="text-align: right;">10,313</td>
<td style="text-align: right;">218,021</td>
</tr>
<tr>
<td style="text-align: left;">Dev</td>
<td style="text-align: right;">1,368</td>
<td style="text-align: right;">29,848</td>
</tr>
<tr>
<td style="text-align: left;">Test</td>
<td style="text-align: right;">1,371</td>
<td style="text-align: right;">30,263</td>
</tr>
</tbody>
</table>
<p>Table 1: Data for AMR-to-English generation.</p>
<p>After training, for a given input query, we consult the first classifier on whether or not to drop the :instance-of edge.</p>
<p>If we drop this edge, we consider the rest of the edges as one group; otherwise, we divide them into two groups each appearing on one side of the :instance-of edge, using the second classifier.</p>
<p>Next, we order the edges within each group. Let $\mathrm{P}\left(r_{i}&lt;r_{j}\right)$ be the probability-according to the third classifier-that $r_{i}$ precedes $r_{j}$. For each edge $r_{i}$, we assign it a "left-leaning" score, which is the product of all $\mathrm{P}\left(r_{i}&lt;r_{j}\right)$, for all $j \neq i$. We remove the edge with the highest left-leaning score. We then recursively process the remaining edges in the group.</p>
<p>We were inspired by Lerner and Petrov (2013) to break the problem down this way. Because their dependencies are ordered, while our AMRs edges are not, we defined a different set of features and classifiers.</p>
<h2>4 Experiments</h2>
<p>We use AMR/English data from the AMR 1.0 corpus, ${ }^{1}$ along with the provided train/development/test split (Table 1).</p>
<p>We implement the method of Pourdamghani et al. (2014) to construct alignments for the training set. We train the linearization function introduced in Section 3 on the aligned training set and use it to re-linearize that training set, maintaining the alignment links. This gives us aligned string-to-string training data for PBMT. We use the same trained linearization function to linearize development and test AMRs.</p>
<p>To measure the quality of linearization, we make calculations on the development set, using alignments to references (these alignments are used only for this experiment, and not for decoding).</p>
<p>A good linearization function should: (a) reduce the number of crossings in the alignment links, and (b) correctly identify concepts to be dropped.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Crossings</th>
<th style="text-align: center;">Adj. crossings</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Pre-order DFS</td>
<td style="text-align: center;">46671</td>
<td style="text-align: center;">7409</td>
</tr>
<tr>
<td style="text-align: left;">Majority Method</td>
<td style="text-align: center;">$33772(72 \%)$</td>
<td style="text-align: center;">$4850(65 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Classifier Method</td>
<td style="text-align: center;">$35603(76 \%)$</td>
<td style="text-align: center;">$4015(54 \%)$</td>
</tr>
</tbody>
</table>
<p>Table 2: Total alignment crossings, and crossings between adjacent links after linearizing development AMRs with different methods. Numbers in parentheses show the reduction compared to Pre-order DFS.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Dev Bleu</th>
<th style="text-align: center;">Test Bleu</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1: Pre-order DFS</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">16.6</td>
</tr>
<tr>
<td style="text-align: left;">1a: $1+$ clean AMRs</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">21.0</td>
</tr>
<tr>
<td style="text-align: left;">1b: 1a + name/number/date</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">22.5</td>
</tr>
<tr>
<td style="text-align: left;">2: Majority Method</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">25.6</td>
</tr>
<tr>
<td style="text-align: left;">3: Classifier Method</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">26.9</td>
</tr>
<tr>
<td style="text-align: left;">Flanigan et al. (2016)</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">22.0</td>
</tr>
</tbody>
</table>
<p>Table 3: Results for AMR-to-English generation on development and test data. Experiments 2 and 3 include cleaning AMRs and name/number/date translations. Bleu scores are singlereference, case insensitive, ${1 . .4}$-grams.</p>
<p>Table 2 shows the total number of crossings and number of crossings between adjacent alignment links after linearizing development AMRs with the three methods introduced in Section 3. Both advanced methods highly reduce the number of crossings. The Classifier Method reduces the number of adjacent crossings much more than the Majority Method, helping to enhance locality. End-to-end experiments (Table 3) show that the Classifier Method outperforms the Majority Method in improving Bleu score.</p>
<p>With respect to concept dropping, $97 \%$ of the concepts dropped by the Classifier Method are in fact not aligned, and the method correctly drops $87 \%$ of the unaligned concepts.</p>
<p>Next, we use the Moses (Koehn et al., 2007) system for our PBMT implementation. Phrase extraction, limited to maximum phrase length 9 , yields 1.2 m phrase pairs. We use a 5 -gram language model trained on 1.7 b tokens of Gigaword English. We use MERT for tuning, and we decode linearized AMRs into English with a maximum stack size of 1000.</p>
<p>Table 3 shows our results. We find that better linearization methods lead to better Bleu scores. The Majority Method outperforms Pre-order DFS by 3.1 Bleu on test data, and the Classifier Method adds another 1.2 Bleu. We also find that steps of cleaning
and specialized name/number/date generators significantly improve Bleu. Compared to (Flanigan et al., 2016) our best system achives 4.5 Bleu points improvement on dev and 4.9 points improvement on test data.</p>
<p>Here is a small-sized input/output example from the automatic AMR-to-English generation system:</p>
<h2>Input AMR:</h2>
<div class="codehilite"><pre><span></span><code>(s / state-01
    :arg0 (p / person
        :name (n / name :opl &quot;fan&quot;))
    :arg1 (c / concern-01
        :arg1 (c3 / commission)
        :arg2 (t / term
            :mod (i / invest-01
                :arg2 (c2 / country
                    :name (n3/name :op1 &quot;taiwan&quot;))
            :time (f / future)))
    :manner (p2 / primary)))
</code></pre></div>

<p>Linearized, Cleaned AMR: fan state commission :manner primary concern invest taiwan :time future term
System Output: fans who have stated that the commission is primarily concerned with the terms of the investment in taiwan in the future .
Gold English: fan stated the commission is primarily concerned with the term of future investment in taiwan .</p>
<h2>5 Conclusion</h2>
<p>We introduce a method for learning to generate English from AMR. We use phrase-based machine translation technology and carry out experiments to compare different AMR linearization methods. We show that our method outperforms prior work by a large margin. We consider our results to form a strong baseline for future work.</p>
<h2>References</h2>
<p>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract Meaning Representation for sembanking. In Proc. ACL Linguistic Annotation Workshop (LAW).
Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek, John D. Laf-</p>
<p>ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A statistical approach to machine translation. Computational linguistics, 16(2):79-85.
Simon Corston-Oliver, Michael Gamon, Eric Ringger, and Robert Moore. 2002. An overview of Amalgam: A machine-learned generation module. In Proc. INLG.
Jeffrey Flanigan, Chris Dyer, Noah A. Smith, and Jaime Carbonell. 2016. Generation from abstract meaning representation using tree transducers. In Proc. NAACL.
Yuqing Guo, Haifeng Wang, and Josef Van Genabith. 2011. Dependency-based n-gram models for general purpose sentence realisation. Natural Language Engineering, 17(4):455-483.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. ACL Poster and Demonstration Sessions.
Irene Langkilde and Kevin Knight. 1998. Generation that exploits corpus-based statistical knowledge. In Proc. ACL.
Uri Lerner and Slav Petrov. 2013. Source-side classifier preordering for machine translation. In Proc. EMNLP.
François Mairesse, Milica Gašić, Filip Jurčíček, Simon Keizer, Blaise Thomson, Kai Yu, and Steve Young. 2010. Phrase-based statistical language generation using graphical models and active learning. In Proc. ACL.
Nima Pourdamghani, Yang Gao, Ulf Hermjakob, and Kevin Knight. 2014. Aligning English strings with Abstract Meaning Representation graphs. In Proc. EMNLP.
Le Zhang. 2004. Maximum entropy modeling toolkit for Python and C++. http://bit.ly/1DGnb2p.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ LDC Catalog number 2014T12.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9650 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9650</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9650</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-272827292</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.13712v1.pdf" target="_blank">Good Idea or Not, Representation of LLM Could Tell</a></p>
                <p><strong>Paper Abstract:</strong> In the ever-expanding landscape of academic research, the proliferation of ideas presents a significant challenge for researchers: discerning valuable ideas from the less impactful ones. The ability to efficiently evaluate the potential of these ideas is crucial for the advancement of science and paper review. In this work, we focus on idea assessment, which aims to leverage the knowledge of large language models to assess the merit of scientific ideas. First, we investigate existing text evaluation research and define the problem of quantitative evaluation of ideas. Second, we curate and release a benchmark dataset from nearly four thousand manuscript papers with full texts, meticulously designed to train and evaluate the performance of different approaches to this task. Third, we establish a framework for quantifying the value of ideas by employing representations in a specific layer of large language models. Experimental results show that the scores predicted by our method are relatively consistent with those of humans. Our findings suggest that the representations of large language models hold more potential in quantifying the value of ideas than their generative outputs, demonstrating a promising avenue for automating the idea assessment process.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9650.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9650.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Representation-based Evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM Representation-based Idea Evaluator (Rep -> MLP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that extracts hidden token representations from a selected layer of an LLM (e.g., LLaMA-2-7b-base or Baichuan-2-7b-base), optionally concatenates section-level last-token vectors, and trains a small MLP (one hidden layer) with MSE to predict human-assigned scalar idea scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>LLaMA-2-7b-base (primary), Baichuan-2-7b-base (secondary)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>7-billion-parameter LLaMA-2 and Baichuan-2 7B variants used as backbone encoders to extract layerwise token representations; models are pretrained LLMs (paper does not provide further training-corpus details beyond standard releases).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science (ICLR submissions)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Map fixed LLM representations (selected layer and selected tokens) to scalar scores using a downstream MLP regressor trained with human-average scores; evaluate agreement with humans via Spearman rank correlation on held-out test splits.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Primarily overall quality (mean reviewer score); dataset also contains correctness, technical novelty, empirical novelty (but experiments focus on overall quality).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>ICLR23 full-text manuscript benchmark: 3,795 ICLR 2023 manuscripts parsed with GROBID; two partitions used: ICLR23-all (3,795 papers) and ICLR23-low-std (subset of papers with low inter-reviewer variance, ~190 papers).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Representation-based evaluator outperformed all baselines in the paper's experiments. Key quantitative findings include: abstract-input Spearman corr using last-token: 0.2783 (5% train) and 0.3366 (30% train); full-text section-token strategy reached 0.3258 (5% train) and 0.3821 (30% train) (Table 4). The paper reports its method achieves up to ~30% higher Spearman correlation than the next-best baseline (SciBERT) in some settings, >80% of predicted scores are within ±2 of human averages (86.8% reported), and in one setting (ICLR23-all, 30% training ratio) the model's correlation exceeded that of randomly choosing an individual human reviewer (interpreted as exceeding a human baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Scope limited to computer science (ICLR submissions); primary experiments target composite 'overall quality' score rather than separate dimensions (correctness/novelty); representation usefulness varies by layer and token strategy; model-scale effects (beyond 7B) not explored; human-score variance reduces learnability (high variance/controversial papers harder to learn).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Representation-based scores align more closely with human average ratings than prompt-based generative numeric outputs from LLMs; in some splits the evaluator matches or exceeds consistency of individual human reviewers, but the authors stress it should assist rather than replace human judgement.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Filter training data by reviewer-consistency (consistency sorting) to learn rating standards; select representations from middle-to-late layers (empirically best performance from layers in the last one-third); for long texts use last-token or section-last-token concatenation (avoid naive concatenation of many mid tokens or arbitrary segments); use a simple MLP regressor trained with MSE and report rank correlation (Spearman) to human averages; small labelled sets (even 5%) can yield positive performance due to pretrained knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Good Idea or Not, Representation of LLM Could Tell', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9650.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9650.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM Generation Baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompted / Fine-tuned LLM Generative Scoring (GPT-3.5-turbo, LLaMA-2, Baichuan-2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline approach where LLMs are prompted or fine-tuned to directly generate a numeric overall-quality score (1–10) from a paper abstract; includes zero-shot GPT-3.5 and fine-tuned LLaMA-2 / Baichuan-2 variants (LoRA or full SFT).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-3.5-turbo (zero/ k-shot), LLaMA-2-7b (LoRA-SFT and Full-SFT), Baichuan-2-7b (LoRA-SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>GPT-3.5 used in zero- and k-shot prompting; LLaMA-2 and Baichuan-2 7B variants fine-tuned on the score prediction task using LoRA or full parameter updates (hyperparameters reported in Appendix B).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science (ICLR paper abstracts)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Prompt LLMs (or fine-tune them) to output a single numeric score per abstract; compare generated scores to human average via Spearman correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Overall quality score (1–10) produced directly as the model output.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Same ICLR23 benchmark (abstract-level inputs for generation experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Generative baselines underperformed relative to representation-based methods. Fine-tuned LLaMA-2 variants performed worse than Baichuan-2 in experiments; LLaMA-2-Full-SFT lacked significant predictive ability (p-value > 0.05). GPT-3.5 in zero-/k-shot often produced frequent/default scores and did not produce reliable numeric predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Generative outputs are insensitive to numeric scoring and prone to producing frequent or round values; small labelled datasets can cause overfitting in fine-tuning; generative modes can hallucinate and inject subjective text rather than calibrated numeric predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Generative LLM outputs are less consistent with human average ratings than the representation->MLP approach; they are generally not competent for stable quantitative scoring in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>For numeric scoring tasks prefer representation extraction and downstream regressors over direct generative scoring; if generation is used, careful prompt engineering and larger training data are needed, but may still remain suboptimal for precise numeric alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Good Idea or Not, Representation of LLM Could Tell', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9650.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9650.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Consistency Sorting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training-set Consistency Sorting by Low Human-score Variance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data selection strategy that ranks papers by inter-reviewer variance per criterion and uses low-variance papers for training to let the model learn stable human rating standards.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>LLM-agnostic (applies to representation and generation approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Not an LLM but a preprocessing/data-partitioning step applied to the dataset before training any evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science (ICLR submissions)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Sort papers by standard deviation of human reviewer scores for the chosen criterion; allocate top low-std fraction (e.g., top 5% or 30%) to training and remainder to testing.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Human score variance (std) per paper used as sorting metric; criterion tested primarily overall quality.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>ICLR23-all and ICLR23-low-std partitions; ICLR23-low-std explicitly constructed to contain papers with low std.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Using low-variance training splits improved learnability: experiments use top 5% and top 30% low-std for training; ICLR23-low-std set has inter-human correlation close to 1 and models trained on these produce higher Spearman correlations on test comparisons. Even small training sizes from low-variance subsets (5%) produced positive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Excluding high-variance (controversial) papers limits the evaluator's exposure to genuinely hard/ambiguous cases and may reduce generalizability; for large-scale deployment one must handle controversial papers differently.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Consistency sorting attempts to align training data with stable human standards rather than learning from noisy/controversial labels; improves model-to-human agreement relative to using full noisy sets.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Apply consistency sorting to construct training splits when training judges of subjective criteria; report how training-set consistency affects evaluation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Good Idea or Not, Representation of LLM Could Tell', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9650.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9650.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Layer Selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Layer-wise Selection of LLM Representations (middle/back-layer focus)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical procedure to identify which LLM layer's representations yield the best downstream evaluator performance; findings indicate representations from middle-to-late (last one-third) layers perform best for idea/evaluation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>LLaMA-2-7b-base and SciBERT (evaluated layerwise)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Layer indices across the entire depth of the LLM are evaluated; middle and rear layers (not the final generation-oriented layer) often give higher discriminative signal.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science (ICLR papers)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>For each candidate layer, extract token-level vectors, apply token selection, train MLP evaluator and measure Spearman correlation on test set; pick layer with highest correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Spearman correlation between predicted and human average scores.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>ICLR23 datasets (both partitions); experiments sweep all layers (figures 3,5–8 report detailed per-layer results).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Across models, middle-to-late layers outperform early and very last layers; very last layer (generation-focused) typically not optimal for discriminative/regression tasks. Authors recommend selecting layers in the last one-third of model depth.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Layer-performance curves vary by model and training ratio; layer selection must be validated per model/dataset. Exhaustive layer search increases evaluation cost.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Layer selection is an engineering step specific to representation-based automated evaluation and has no direct analogue in human review, but it materially affects correlation to human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Perform a layer sweep and prioritize layers in the last third of the model; avoid assuming final output layer is optimal for discriminative scoring tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Good Idea or Not, Representation of LLM Could Tell', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9650.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9650.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Token Selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Token/Section Selection Strategies for Long Document Representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Token selection alternatives examined for long full-text manuscripts: (a) last-token representation of entire input, (b) last-token of each equidistant segment, (c) last-token of each paper section (section tokens), and (d) middle + last token combinations; concatenated section last-tokens often best for full-texts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>LLaMA-2-7b-base (primary experiments); also applied to BERT/SciBERT with CLS token strategies</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>For autoregressive LLMs the last token aggregates attention; for encoder-only models [CLS] token used and long sections split to <=512 tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science (ICLR manuscripts)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compare Spearman correlation of evaluators trained on different token-selection strategies using abstract and full-text inputs (Table 4 reports results).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Spearman correlation with human averages.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>ICLR23-all (full-text) and abstract-only experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Abstract-level: last-token outperforms middle+last. Full-text: section-based last-token concatenation outperforms segment-based last-token concatenation (segment tokens often truncate sentences); reported Spearman correlations (5%/30% train) — abstract last-token: 0.2783 / 0.3366; middle+last: 0.2162 / 0.2772; full-text segment tokens: 0.1306 / 0.2597; section tokens: 0.3258 / 0.3821.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Segment-based splitting can create incomplete-sentence fragments that dilute semantics; last-token-only for very long texts may lose localized semantic details; memory constraints for long inputs require careful batching (vLLM suggested).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Token-selection choices correspond to how much of a paper a human reviewer would read and integrate; section-based aggregation approximates per-section human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use last-token for short inputs (abstracts); use section-last-token concatenation for full-texts when sections can be reliably parsed; avoid indiscriminate concatenation of many mid tokens or arbitrary segments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Good Idea or Not, Representation of LLM Could Tell', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9650.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9650.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spearman Rank Correlation (evaluation) and MSE Loss (training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Spearman correlation used as primary evaluation metric to measure rank-order agreement between predicted and human-average scores; Mean Squared Error (MSE) is used as the training objective for the MLP regressor.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>LLM-agnostic (applies to representation-based and generative approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Not an LLM; metrics applied to outputs from evaluators built on various LLM backbones.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science (ICLR submissions)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute Spearman rho between vector of human average scores [s1..sn] and predicted scores [ŝ1..ŝn] on test set (Equation 5). Train MLP with MSE: L = (1/n) Σ(ŝi - si)^2 (Equation 4).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Spearman correlation for alignment; distributional comparisons and absolute error distributions also reported (percentage within ±2 points).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>ICLR23 datasets; reported per training ratio and per-layer sweeps.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Spearman used to show representation-based method achieves substantially higher rank agreement than generative baselines and many classical encoders; the paper reports many per-layer and per-training-ratio Spearman values (figures/tables). Absolute-error analysis: 86.8% of model predictions on ICLR23-low-std have absolute error < 2.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Spearman is rank-based and invariant under affine transforms, so absolute calibration differences are not captured; authors therefore also report score distributions and absolute errors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Spearman quantifies similarity to human consensus rankings, not to individual reviewer idiosyncrasies; used to compare model vs. human inter-reviewer agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Report Spearman for rank alignment plus absolute error/distributional statistics to capture calibration; use cross-validation or multiple random initializations (authors averaged 3 runs).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Good Idea or Not, Representation of LLM Could Tell', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9650.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9650.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICLR23 Benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ICLR 2023 Full-text Manuscript Benchmark (ICLR23-all, ICLR23-low-std)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curated dataset of 3,795 ICLR 2023 manuscripts (PDFs parsed with GROBID) annotated with reviewer metadata including overall quality, correctness, technical novelty, and empirical novelty; a low-std subset (ICLR23-low-std) selects papers with low inter-reviewer variance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>dataset (LLM-agnostic)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Not an LLM; dataset of full-text papers and reviewer scores used to train/evaluate representation and generation-based evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science (Machine Learning research)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Used as ground-truth labels (human average scores) for training regressors and for evaluation via Spearman correlations and error distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Human-assigned reviewer scores per paper for overall quality, correctness, technical novelty, empirical novelty (experiments focus on overall quality).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>ICLR23-all (3,795 papers) and ICLR23-low-std (~190 papers) with table of descriptive statistics (Table 1 shows means and stds).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ICLR23-low-std: mean overall quality 5.52 ± 0.61 (190 papers); ICLR23-all: mean 5.41 ± 1.06 (3,795 papers). Low-std subset yields higher model-to-human Spearman correlations in training/test.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Dataset restricted to one conference/discipline (CS/ML) and one year; reviewer variance and domain imbalance (some domains like Theory, Neuroscience, Infrastructure differ more) may bias models; potential data-leakage concerns for newer LLM pretraining led authors to choose LLaMA-2 rather than LLaMA-3.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Provides human-reviewer consensus labels to compare automated evaluators against community reviewer judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>When building such benchmarks ensure section parsing (GROBID) and provide low-variance splits for model calibration; make dataset public to support reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Good Idea or Not, Representation of LLM Could Tell', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9650.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9650.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RePE (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RePE (Representation-based Evaluation) / Representation engineering (Zou et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior representation-based method benchmarked by the authors; in this paper RePE is evaluated as a baseline and uses PCA-based embedding evaluation per the original RePE recommendations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Representation engineering: A topdown approach to ai transparency.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>RePE (method applied to LLM representations / embedding spaces)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Representation-engineering approach from Zou et al., 2023; in this paper it is applied as a comparative baseline with PCA for embedding evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General AI / representation analysis applied to idea evaluation (ICLR corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>PCA embedding evaluation operating on representations to estimate utility; compared to the proposed MLP-on-representation approach via Spearman correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Rank correlation to human average scores (Spearman).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>ICLR23 datasets (same benchmark used for fair comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>RePE (and SciBERT) were among the stronger baselines, but the proposed method in this paper still outperformed RePE in reported experiments; middle-layer representations often gave best RePE results as well.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>RePE's PCA-based approach is one embedding-evaluation recipe; may be less effective than a supervised MLP regressor tuned for the specific scoring task.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Acts as a representation-based comparative baseline showing supervised downstream training can improve alignment with human scores.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use RePE-style analysis to inspect representation utility across layers before building supervised evaluators; consider supervised heads for task-specific calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Good Idea or Not, Representation of LLM Could Tell', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Representation engineering: A topdown approach to ai transparency. <em>(Rating: 2)</em></li>
                <li>Reviewergpt? an exploratory study on using large language models for paper reviewing. <em>(Rating: 2)</em></li>
                <li>Can large language models provide useful feedback on research papers? <em>(Rating: 2)</em></li>
                <li>Can we automate scientific reviewing. <em>(Rating: 2)</em></li>
                <li>Litllm: A toolkit for scientific literature review. <em>(Rating: 1)</em></li>
                <li>SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models. <em>(Rating: 1)</em></li>
                <li>Predicting article quality scores with machine learning: The uk research excellence framework. <em>(Rating: 1)</em></li>
                <li>Transformer feed-forward layers are key-value memories. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9650",
    "paper_id": "paper-272827292",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "Representation-based Evaluator",
            "name_full": "LLM Representation-based Idea Evaluator (Rep -&gt; MLP)",
            "brief_description": "A framework that extracts hidden token representations from a selected layer of an LLM (e.g., LLaMA-2-7b-base or Baichuan-2-7b-base), optionally concatenates section-level last-token vectors, and trains a small MLP (one hidden layer) with MSE to predict human-assigned scalar idea scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "LLaMA-2-7b-base (primary), Baichuan-2-7b-base (secondary)",
            "llm_description": "7-billion-parameter LLaMA-2 and Baichuan-2 7B variants used as backbone encoders to extract layerwise token representations; models are pretrained LLMs (paper does not provide further training-corpus details beyond standard releases).",
            "scientific_domain": "Computer Science (ICLR submissions)",
            "evaluation_method": "Map fixed LLM representations (selected layer and selected tokens) to scalar scores using a downstream MLP regressor trained with human-average scores; evaluate agreement with humans via Spearman rank correlation on held-out test splits.",
            "evaluation_criteria": "Primarily overall quality (mean reviewer score); dataset also contains correctness, technical novelty, empirical novelty (but experiments focus on overall quality).",
            "benchmark_or_dataset": "ICLR23 full-text manuscript benchmark: 3,795 ICLR 2023 manuscripts parsed with GROBID; two partitions used: ICLR23-all (3,795 papers) and ICLR23-low-std (subset of papers with low inter-reviewer variance, ~190 papers).",
            "results_summary": "Representation-based evaluator outperformed all baselines in the paper's experiments. Key quantitative findings include: abstract-input Spearman corr using last-token: 0.2783 (5% train) and 0.3366 (30% train); full-text section-token strategy reached 0.3258 (5% train) and 0.3821 (30% train) (Table 4). The paper reports its method achieves up to ~30% higher Spearman correlation than the next-best baseline (SciBERT) in some settings, &gt;80% of predicted scores are within ±2 of human averages (86.8% reported), and in one setting (ICLR23-all, 30% training ratio) the model's correlation exceeded that of randomly choosing an individual human reviewer (interpreted as exceeding a human baseline).",
            "limitations_or_challenges": "Scope limited to computer science (ICLR submissions); primary experiments target composite 'overall quality' score rather than separate dimensions (correctness/novelty); representation usefulness varies by layer and token strategy; model-scale effects (beyond 7B) not explored; human-score variance reduces learnability (high variance/controversial papers harder to learn).",
            "comparison_to_human_or_traditional": "Representation-based scores align more closely with human average ratings than prompt-based generative numeric outputs from LLMs; in some splits the evaluator matches or exceeds consistency of individual human reviewers, but the authors stress it should assist rather than replace human judgement.",
            "recommendations_or_best_practices": "Filter training data by reviewer-consistency (consistency sorting) to learn rating standards; select representations from middle-to-late layers (empirically best performance from layers in the last one-third); for long texts use last-token or section-last-token concatenation (avoid naive concatenation of many mid tokens or arbitrary segments); use a simple MLP regressor trained with MSE and report rank correlation (Spearman) to human averages; small labelled sets (even 5%) can yield positive performance due to pretrained knowledge.",
            "uuid": "e9650.0",
            "source_info": {
                "paper_title": "Good Idea or Not, Representation of LLM Could Tell",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LLM Generation Baseline",
            "name_full": "Prompted / Fine-tuned LLM Generative Scoring (GPT-3.5-turbo, LLaMA-2, Baichuan-2)",
            "brief_description": "Baseline approach where LLMs are prompted or fine-tuned to directly generate a numeric overall-quality score (1–10) from a paper abstract; includes zero-shot GPT-3.5 and fine-tuned LLaMA-2 / Baichuan-2 variants (LoRA or full SFT).",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-3.5-turbo (zero/ k-shot), LLaMA-2-7b (LoRA-SFT and Full-SFT), Baichuan-2-7b (LoRA-SFT)",
            "llm_description": "GPT-3.5 used in zero- and k-shot prompting; LLaMA-2 and Baichuan-2 7B variants fine-tuned on the score prediction task using LoRA or full parameter updates (hyperparameters reported in Appendix B).",
            "scientific_domain": "Computer Science (ICLR paper abstracts)",
            "evaluation_method": "Prompt LLMs (or fine-tune them) to output a single numeric score per abstract; compare generated scores to human average via Spearman correlation.",
            "evaluation_criteria": "Overall quality score (1–10) produced directly as the model output.",
            "benchmark_or_dataset": "Same ICLR23 benchmark (abstract-level inputs for generation experiments).",
            "results_summary": "Generative baselines underperformed relative to representation-based methods. Fine-tuned LLaMA-2 variants performed worse than Baichuan-2 in experiments; LLaMA-2-Full-SFT lacked significant predictive ability (p-value &gt; 0.05). GPT-3.5 in zero-/k-shot often produced frequent/default scores and did not produce reliable numeric predictions.",
            "limitations_or_challenges": "Generative outputs are insensitive to numeric scoring and prone to producing frequent or round values; small labelled datasets can cause overfitting in fine-tuning; generative modes can hallucinate and inject subjective text rather than calibrated numeric predictions.",
            "comparison_to_human_or_traditional": "Generative LLM outputs are less consistent with human average ratings than the representation-&gt;MLP approach; they are generally not competent for stable quantitative scoring in this setup.",
            "recommendations_or_best_practices": "For numeric scoring tasks prefer representation extraction and downstream regressors over direct generative scoring; if generation is used, careful prompt engineering and larger training data are needed, but may still remain suboptimal for precise numeric alignment.",
            "uuid": "e9650.1",
            "source_info": {
                "paper_title": "Good Idea or Not, Representation of LLM Could Tell",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Consistency Sorting",
            "name_full": "Training-set Consistency Sorting by Low Human-score Variance",
            "brief_description": "A data selection strategy that ranks papers by inter-reviewer variance per criterion and uses low-variance papers for training to let the model learn stable human rating standards.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "LLM-agnostic (applies to representation and generation approaches)",
            "llm_description": "Not an LLM but a preprocessing/data-partitioning step applied to the dataset before training any evaluator.",
            "scientific_domain": "Computer Science (ICLR submissions)",
            "evaluation_method": "Sort papers by standard deviation of human reviewer scores for the chosen criterion; allocate top low-std fraction (e.g., top 5% or 30%) to training and remainder to testing.",
            "evaluation_criteria": "Human score variance (std) per paper used as sorting metric; criterion tested primarily overall quality.",
            "benchmark_or_dataset": "ICLR23-all and ICLR23-low-std partitions; ICLR23-low-std explicitly constructed to contain papers with low std.",
            "results_summary": "Using low-variance training splits improved learnability: experiments use top 5% and top 30% low-std for training; ICLR23-low-std set has inter-human correlation close to 1 and models trained on these produce higher Spearman correlations on test comparisons. Even small training sizes from low-variance subsets (5%) produced positive performance.",
            "limitations_or_challenges": "Excluding high-variance (controversial) papers limits the evaluator's exposure to genuinely hard/ambiguous cases and may reduce generalizability; for large-scale deployment one must handle controversial papers differently.",
            "comparison_to_human_or_traditional": "Consistency sorting attempts to align training data with stable human standards rather than learning from noisy/controversial labels; improves model-to-human agreement relative to using full noisy sets.",
            "recommendations_or_best_practices": "Apply consistency sorting to construct training splits when training judges of subjective criteria; report how training-set consistency affects evaluation metrics.",
            "uuid": "e9650.2",
            "source_info": {
                "paper_title": "Good Idea or Not, Representation of LLM Could Tell",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Layer Selection",
            "name_full": "Layer-wise Selection of LLM Representations (middle/back-layer focus)",
            "brief_description": "Empirical procedure to identify which LLM layer's representations yield the best downstream evaluator performance; findings indicate representations from middle-to-late (last one-third) layers perform best for idea/evaluation tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "LLaMA-2-7b-base and SciBERT (evaluated layerwise)",
            "llm_description": "Layer indices across the entire depth of the LLM are evaluated; middle and rear layers (not the final generation-oriented layer) often give higher discriminative signal.",
            "scientific_domain": "Computer Science (ICLR papers)",
            "evaluation_method": "For each candidate layer, extract token-level vectors, apply token selection, train MLP evaluator and measure Spearman correlation on test set; pick layer with highest correlation.",
            "evaluation_criteria": "Spearman correlation between predicted and human average scores.",
            "benchmark_or_dataset": "ICLR23 datasets (both partitions); experiments sweep all layers (figures 3,5–8 report detailed per-layer results).",
            "results_summary": "Across models, middle-to-late layers outperform early and very last layers; very last layer (generation-focused) typically not optimal for discriminative/regression tasks. Authors recommend selecting layers in the last one-third of model depth.",
            "limitations_or_challenges": "Layer-performance curves vary by model and training ratio; layer selection must be validated per model/dataset. Exhaustive layer search increases evaluation cost.",
            "comparison_to_human_or_traditional": "Layer selection is an engineering step specific to representation-based automated evaluation and has no direct analogue in human review, but it materially affects correlation to human judgments.",
            "recommendations_or_best_practices": "Perform a layer sweep and prioritize layers in the last third of the model; avoid assuming final output layer is optimal for discriminative scoring tasks.",
            "uuid": "e9650.3",
            "source_info": {
                "paper_title": "Good Idea or Not, Representation of LLM Could Tell",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Token Selection",
            "name_full": "Token/Section Selection Strategies for Long Document Representations",
            "brief_description": "Token selection alternatives examined for long full-text manuscripts: (a) last-token representation of entire input, (b) last-token of each equidistant segment, (c) last-token of each paper section (section tokens), and (d) middle + last token combinations; concatenated section last-tokens often best for full-texts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "LLaMA-2-7b-base (primary experiments); also applied to BERT/SciBERT with CLS token strategies",
            "llm_description": "For autoregressive LLMs the last token aggregates attention; for encoder-only models [CLS] token used and long sections split to &lt;=512 tokens.",
            "scientific_domain": "Computer Science (ICLR manuscripts)",
            "evaluation_method": "Compare Spearman correlation of evaluators trained on different token-selection strategies using abstract and full-text inputs (Table 4 reports results).",
            "evaluation_criteria": "Spearman correlation with human averages.",
            "benchmark_or_dataset": "ICLR23-all (full-text) and abstract-only experiments.",
            "results_summary": "Abstract-level: last-token outperforms middle+last. Full-text: section-based last-token concatenation outperforms segment-based last-token concatenation (segment tokens often truncate sentences); reported Spearman correlations (5%/30% train) — abstract last-token: 0.2783 / 0.3366; middle+last: 0.2162 / 0.2772; full-text segment tokens: 0.1306 / 0.2597; section tokens: 0.3258 / 0.3821.",
            "limitations_or_challenges": "Segment-based splitting can create incomplete-sentence fragments that dilute semantics; last-token-only for very long texts may lose localized semantic details; memory constraints for long inputs require careful batching (vLLM suggested).",
            "comparison_to_human_or_traditional": "Token-selection choices correspond to how much of a paper a human reviewer would read and integrate; section-based aggregation approximates per-section human evaluation.",
            "recommendations_or_best_practices": "Use last-token for short inputs (abstracts); use section-last-token concatenation for full-texts when sections can be reliably parsed; avoid indiscriminate concatenation of many mid tokens or arbitrary segments.",
            "uuid": "e9650.4",
            "source_info": {
                "paper_title": "Good Idea or Not, Representation of LLM Could Tell",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Evaluation Metrics",
            "name_full": "Spearman Rank Correlation (evaluation) and MSE Loss (training)",
            "brief_description": "Spearman correlation used as primary evaluation metric to measure rank-order agreement between predicted and human-average scores; Mean Squared Error (MSE) is used as the training objective for the MLP regressor.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "LLM-agnostic (applies to representation-based and generative approaches)",
            "llm_description": "Not an LLM; metrics applied to outputs from evaluators built on various LLM backbones.",
            "scientific_domain": "Computer Science (ICLR submissions)",
            "evaluation_method": "Compute Spearman rho between vector of human average scores [s1..sn] and predicted scores [ŝ1..ŝn] on test set (Equation 5). Train MLP with MSE: L = (1/n) Σ(ŝi - si)^2 (Equation 4).",
            "evaluation_criteria": "Spearman correlation for alignment; distributional comparisons and absolute error distributions also reported (percentage within ±2 points).",
            "benchmark_or_dataset": "ICLR23 datasets; reported per training ratio and per-layer sweeps.",
            "results_summary": "Spearman used to show representation-based method achieves substantially higher rank agreement than generative baselines and many classical encoders; the paper reports many per-layer and per-training-ratio Spearman values (figures/tables). Absolute-error analysis: 86.8% of model predictions on ICLR23-low-std have absolute error &lt; 2.",
            "limitations_or_challenges": "Spearman is rank-based and invariant under affine transforms, so absolute calibration differences are not captured; authors therefore also report score distributions and absolute errors.",
            "comparison_to_human_or_traditional": "Spearman quantifies similarity to human consensus rankings, not to individual reviewer idiosyncrasies; used to compare model vs. human inter-reviewer agreement.",
            "recommendations_or_best_practices": "Report Spearman for rank alignment plus absolute error/distributional statistics to capture calibration; use cross-validation or multiple random initializations (authors averaged 3 runs).",
            "uuid": "e9650.5",
            "source_info": {
                "paper_title": "Good Idea or Not, Representation of LLM Could Tell",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "ICLR23 Benchmark",
            "name_full": "ICLR 2023 Full-text Manuscript Benchmark (ICLR23-all, ICLR23-low-std)",
            "brief_description": "A curated dataset of 3,795 ICLR 2023 manuscripts (PDFs parsed with GROBID) annotated with reviewer metadata including overall quality, correctness, technical novelty, and empirical novelty; a low-std subset (ICLR23-low-std) selects papers with low inter-reviewer variance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "dataset (LLM-agnostic)",
            "llm_description": "Not an LLM; dataset of full-text papers and reviewer scores used to train/evaluate representation and generation-based evaluators.",
            "scientific_domain": "Computer Science (Machine Learning research)",
            "evaluation_method": "Used as ground-truth labels (human average scores) for training regressors and for evaluation via Spearman correlations and error distributions.",
            "evaluation_criteria": "Human-assigned reviewer scores per paper for overall quality, correctness, technical novelty, empirical novelty (experiments focus on overall quality).",
            "benchmark_or_dataset": "ICLR23-all (3,795 papers) and ICLR23-low-std (~190 papers) with table of descriptive statistics (Table 1 shows means and stds).",
            "results_summary": "ICLR23-low-std: mean overall quality 5.52 ± 0.61 (190 papers); ICLR23-all: mean 5.41 ± 1.06 (3,795 papers). Low-std subset yields higher model-to-human Spearman correlations in training/test.",
            "limitations_or_challenges": "Dataset restricted to one conference/discipline (CS/ML) and one year; reviewer variance and domain imbalance (some domains like Theory, Neuroscience, Infrastructure differ more) may bias models; potential data-leakage concerns for newer LLM pretraining led authors to choose LLaMA-2 rather than LLaMA-3.",
            "comparison_to_human_or_traditional": "Provides human-reviewer consensus labels to compare automated evaluators against community reviewer judgments.",
            "recommendations_or_best_practices": "When building such benchmarks ensure section parsing (GROBID) and provide low-variance splits for model calibration; make dataset public to support reproducibility.",
            "uuid": "e9650.6",
            "source_info": {
                "paper_title": "Good Idea or Not, Representation of LLM Could Tell",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "RePE (baseline)",
            "name_full": "RePE (Representation-based Evaluation) / Representation engineering (Zou et al., 2023)",
            "brief_description": "Prior representation-based method benchmarked by the authors; in this paper RePE is evaluated as a baseline and uses PCA-based embedding evaluation per the original RePE recommendations.",
            "citation_title": "Representation engineering: A topdown approach to ai transparency.",
            "mention_or_use": "use",
            "llm_name": "RePE (method applied to LLM representations / embedding spaces)",
            "llm_description": "Representation-engineering approach from Zou et al., 2023; in this paper it is applied as a comparative baseline with PCA for embedding evaluation.",
            "scientific_domain": "General AI / representation analysis applied to idea evaluation (ICLR corpus)",
            "evaluation_method": "PCA embedding evaluation operating on representations to estimate utility; compared to the proposed MLP-on-representation approach via Spearman correlation.",
            "evaluation_criteria": "Rank correlation to human average scores (Spearman).",
            "benchmark_or_dataset": "ICLR23 datasets (same benchmark used for fair comparison).",
            "results_summary": "RePE (and SciBERT) were among the stronger baselines, but the proposed method in this paper still outperformed RePE in reported experiments; middle-layer representations often gave best RePE results as well.",
            "limitations_or_challenges": "RePE's PCA-based approach is one embedding-evaluation recipe; may be less effective than a supervised MLP regressor tuned for the specific scoring task.",
            "comparison_to_human_or_traditional": "Acts as a representation-based comparative baseline showing supervised downstream training can improve alignment with human scores.",
            "recommendations_or_best_practices": "Use RePE-style analysis to inspect representation utility across layers before building supervised evaluators; consider supervised heads for task-specific calibration.",
            "uuid": "e9650.7",
            "source_info": {
                "paper_title": "Good Idea or Not, Representation of LLM Could Tell",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Representation engineering: A topdown approach to ai transparency.",
            "rating": 2,
            "sanitized_title": "representation_engineering_a_topdown_approach_to_ai_transparency"
        },
        {
            "paper_title": "Reviewergpt? an exploratory study on using large language models for paper reviewing.",
            "rating": 2,
            "sanitized_title": "reviewergpt_an_exploratory_study_on_using_large_language_models_for_paper_reviewing"
        },
        {
            "paper_title": "Can large language models provide useful feedback on research papers?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_provide_useful_feedback_on_research_papers"
        },
        {
            "paper_title": "Can we automate scientific reviewing.",
            "rating": 2,
            "sanitized_title": "can_we_automate_scientific_reviewing"
        },
        {
            "paper_title": "Litllm: A toolkit for scientific literature review.",
            "rating": 1,
            "sanitized_title": "litllm_a_toolkit_for_scientific_literature_review"
        },
        {
            "paper_title": "SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models.",
            "rating": 1,
            "sanitized_title": "selfcheckgpt_zeroresource_blackbox_hallucination_detection_for_generative_large_language_models"
        },
        {
            "paper_title": "Predicting article quality scores with machine learning: The uk research excellence framework.",
            "rating": 1,
            "sanitized_title": "predicting_article_quality_scores_with_machine_learning_the_uk_research_excellence_framework"
        },
        {
            "paper_title": "Transformer feed-forward layers are key-value memories.",
            "rating": 1,
            "sanitized_title": "transformer_feedforward_layers_are_keyvalue_memories"
        }
    ],
    "cost": 0.0176895,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Good Idea or Not, Representation of LLM Could Tell
7 Sep 2024</p>
<p>Yi Xu 
Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Bo Xue 
Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Shuqian Sheng 
Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Cheng Deng 
Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Jiaxin Ding 
Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Zanwei Shen 
Luoyi Fu 
Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Xinbing Wang 
Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Chenghu Zhou 
Shanghai Jiao Tong University
ShanghaiChina</p>
<p>IGSNRR
Chinese Academy of Sciences
BeijingChina</p>
<p>Good Idea or Not, Representation of LLM Could Tell
7 Sep 2024C6ECAECE2D9003DF70D4182DD49E90E2arXiv:2409.13712v1[cs.CL]Scores Human-rated Score: 750 LLM Representation Score: 722 Title Ensemble Homomorphic Encrypted Data Classification Scores Human-rated Score: 150 LLM Representation Score: 161 Title Comparative Analysis between Vision Transformers and CNNs from Neuroscience Scores Human-rated Score: 250 LLM Representation Score: 480 (Over Estimated) Title Neural Causal Models for Counterfactual Identification and Estimation
In the ever-expanding landscape of academic research, the proliferation of ideas presents a significant challenge for researchers: discerning valuable ideas from the less impactful ones.The ability to efficiently evaluate the potential of these ideas is crucial for the advancement of science and paper review.In this work, we focus on idea assessment, which aims to leverage the knowledge of large language models to assess the merit of scientific ideas.First, we investigate existing text evaluation research and define the problem of quantitative evaluation of ideas.Second, we curate and release a benchmark dataset from nearly four thousand manuscript papers with full texts, meticulously designed to train and evaluate the performance of different approaches to this task.Third, we establish a framework for quantifying the value of ideas by employing representations in a specific layer of large language models.Experimental results show that the scores predicted by our method are relatively consistent with those of humans.Our findings suggest that the representations of large language models hold more potential in quantifying the value of ideas than their generative outputs, demonstrating a promising avenue for automating the idea assessment process.</p>
<p>Introduction</p>
<p>The rapid pace of scientific research in various disciplines has given rise to an overwhelming number of academic papers (Tabah, 1999;Bornmann and Mutz, 2015;Xu et al., 2023).Typically, ideas are commonly conveyed through these papers, which reviewers must carefully scrutinize to grasp the ideas authors present.However, amidst the vast volume of paper submissions, the review process becomes slow, labor-intensive, and less precise (Checco et al., 2021), making it a challenge to identify valuable ideas.Fortunately, with the advent of large language models (LLMs), we are presented with an unprecedented opportunity to revolutionize how we assess the merit of scientific ideas, and this work explores their use as a knowledge tool for such evaluation.In order to make idea more concrete, we take unpublished manuscripts or the latest papers as the research object.</p>
<p>While the generative capabilities of LLMs have been widely recognized, their potential as idea (or paper) evaluative instruments has remained relatively underexplored.Recent studies (Yuan et al., 2022;Liang et al., 2023;Liu and Shah, 2023;Agarwal et al., 2024) have begun to harness LLMs for the automatic generation of paper reviews, aiming for outputs that are both informative and emulate human feedback.These efforts primarily utilize the text generation capability of LLM, which are highly dependent on the scale of model parameters.For instance, Liu and Shah (2023) proves that GPT-4 surpasses other open-source LLMs, such as LLaMA (Touvron et al., 2023), in generating reviews.Meanwhile, crafting intricate prompts containing specific commands and inquiries is essential for LLMs to produce meaningful reviews.Nonetheless, LLM-generated reviews can still reflect models' subjective biases and occasionally produce hallucinated contents (Zhang et al., 2023;Manakul et al., 2023).There is currently no research that quantitatively evaluate ideas in an objective way.</p>
<p>According to Geva et al. (2021); Zou et al. (2023), the representations of different layers in LLM contain different semantic information, and in some tasks, the performance of the last layer is not the best.Based on this point, we suppose that the representations of LLMs encapsulate a detailed and nuanced comprehension of text, which can be leveraged to construct a systematic and objective framework for the assessment of ideas.Our research thus focuses on the quantitative evaluation of ideas through LLMs, an approach we argue is more objective than generative techniques.It is worth not-ing that LLMs' generative method is not inherently adept at processing numerical data (digits), which is also why we turn to their representations as a means to quantify the value of ideas.We delve into the latent knowledge embedded within the representations of LLMs for this purpose.Specifically, we first define the problem of quantitative idea evaluation, and construct a benchmark dataset comprised of nearly 4k scientific manuscripts with full texts in the discipline of computer science.This dataset serves as the bedrock for training and testing various LLM-based approaches to idea assessment.In the subsequent phase of our research, we develop a framework for quantifying the value of ideas.This framework leverages the representations produced by LLMs, which encode semantic features of the text in a high-dimensional space.Finally, by training a downstream evaluator using these representations, our framework can identify patterns and signals within these representations that align with the value of scientific ideas as determined by the distribution of expert consensus.</p>
<p>We perform extensive experiments on our benchmark dataset.The results reveal that the representations generated by LLMs are inherently indicative of the potential value of scientific ideas, especially using the representations in the middle and rear layers, which demonstrate a high degree of consistency with human judgements.Meanwhile, we also found that high consistency can be achieved with only a small amount of data, thanks to the pretrained knowledge in LLMs.The contributions of our paper are summarized as follows:</p>
<p>• We conduct a thorough investigation of current methods for paper (idea) assessment and pioneer the study of quantitative evaluation of scientific ideas.</p>
<p>• We meticulously curate a new benchmark dataset, complete with human-assigned scores on overall quality, novelty, and correctness.We are making it publicly available.This dataset is comprised of the full texts of 3,795 papers in PDF format.</p>
<p>• We propose a new framework that quantifies the value of ideas by utilizing the deep representations produced by LLMs.</p>
<p>• We conduct extensive experiments to verify the performance of our method.The results indicate that utilizing the representations from LLMs aligns more closely with human judgments than using generative textual outputs of LLMs when assessing the quality of ideas.</p>
<p>2 Related Work Yuan et al. (2022) have explored the use of various NLP techniques to produce decisive and comprehensive reviews of academic papers from multiple perspectives.The work of ReviewerGPT (Liu and Shah, 2023) studies the application of LLMs as review assistants, focusing on tasks such as error identification, checklist verification, and comparative paper analysis.Another innovative approach by researchers (Liang et al., 2023) involves an automated system that employs GPT-4 to create review comments and suggestions for revisions, which are then benchmarked against feedback from human reviewers.What's more, LitLLM (Agarwal et al., 2024) equips Retrieval Augmented Generation (RAG) module to address the hallucination problem in review generation.</p>
<p>In addition to the methods mentioned above, some researches rely on external data to assess the quality of a paper.For instance, Thelwall et al. (2023) have developed a framework that predicts article quality scores using a range of bibliometric and metadata indicators, including citation counts, journal impact factors, and institutional rankings.Similarly, KQI (Wang et al., 2023) leverages the structure of citation networks to quantify the knowledge contribution of a paper.This kind of approaches, however, pertains to post-publication evaluation.Unlike our approach, which is based solely on the text of the paper itself, it does not require information about the paper's acceptance or publication status.</p>
<p>Idea Assessment</p>
<p>In this work, we focus on quantitative evaluation of ideas.Let D = {d 1 , d 2 , ..., d n } be a dataset consisting of n scientific manuscripts (papers), each representing a distinct scientific idea.The direct scoring of an idea involves mapping each manuscript d i to a quantitative score s i based on a predefined criterion c ∈ C. The criterion set C serves as the basis for the assessment and is essential for guiding the evaluation process.It can encompass various aspects of potential impacts of an idea, such as novelty, correctness, excitement, soundness, or alignment with current research trends.</p>
<p>We first define a function A c : D → R such that for any manuscript d i ∈ D and criterion c ∈ C, A c (d i ) produces a scalar value s i ∈ R which quantifies the value of the idea d i with respect to c.The quantitative evaluation of idea d i with respect to criterion c can be expressed as:
s i = A c (d i ),(1)
where the function A c is the evaluator that assesses the idea based on the text of the manuscript d i and the specified criterion c.</p>
<p>In our work, we utilize the representations of LLM to quantify the value of an idea.Let M be an LLM that transforms textual data into a highdimensional representation space.We define an encoding function Rep : D → R l×m such that for any manuscript d i ∈ D, there is a hidden representation Rep(d i ) ∈ R l×m , where l is the number of tokens in d i and m is the dimension of the representation space in LLM M .Now, we revise the evaluator function A c : R l×m → R that maps the representation to a scalar value s i :
s i = A c (Rep(d i )).
(2)</p>
<p>The evaluator, A c , is designed to be flexible and adaptable to different criteria and can be trained using annotated data that provide ground truth measures of the ideas' impact with respect to the chosen criterion.To the best of our knowledge, we are the first to quantify the value of an idea.</p>
<p>Dataset</p>
<p>To ensure that the idea evaluator is well-calibrated, the benchmark idea dataset D should be representative of the scientific community and contain cuttingedge knowledge in academia.To this end, we have compiled a collection of 3,795 manuscripts that are available in PDF format from the International Conference on Learning Representations (ICLR) 2023.For the extraction of full texts from these PDFs, we employed GROBID (Lopez, 2009), a sophisticated tool for parsing academic PDF documents.</p>
<p>Additionally, the metadata of these papers includes comprehensive evaluation criteria from official reviewers, encompassing scores for overall quality, correctness, technical and empirical novelty, providing a rich ground truth for training and validation.It is possible that an idea is interesting but the paper score of a criterion such as correctness is low because there are flaws in the experiments.</p>
<p>Therefore, in our work, we mainly investigate the criterion overall quality and take it as the overall score of an idea.Considering the different consistencies of human-rated data, which can have impacts on different evaluation models, we choose papers with highly consistent human-rated scores from the original dataset ICLR23-all and construct dataset ICLR23-low-std, where the standard deviation (std) of overall quality scores for each paper is relatively lower.The statistics are listed in Table 1.</p>
<p>Methodology</p>
<p>The purpose of our method is to train an evaluator A c to score ideas, which consists of four steps: consistency sorting, layer selection, token selection, and evaluator training.Figure 1 shows the pipeline of quantitative evaluation of ideas using the representation of LLMs.It should be highlighted that the steps of layer and token selection only exist in training process, which are determined during the inference process.</p>
<p>Consistency Sorting</p>
<p>In our scenario, we anticipate that models can learn the rating standard from human-rated data.Specifically, the human-assigned scores for each paper in the training set should exhibit a high level of consistency; that is, the more uniform the scores for each paper are (reflected by lower variance), the more suitable the data is for model training.Therefore, our method employs a consistency-based sorting mechanism to construct the training and testing sets.We commence by ordering the papers according to the variance in human scores for a given criterion c.Subsequently, based on a predetermined threshold for training set partitioning, papers that demonstrate high consistency (low variance) are allocated to the training set, while the remainder are designated for the testing set.This mechanism facilitates a more straightforward learning process for models to grasp the human rating standards.Conversely, a high degree of variance in humanassigned scores suggests that the paper (or idea) is controversial, rendering the learning of standards from such data as potentially futile.</p>
<p>Layer Selection</p>
<p>As claimed by Geva et al. (2021), lower layers of LLMs tend to capture shallow data patterns, while upper layers contain more semantic knowledge.This hierarchical processing of information within LLMs suggests that the utility of representations may vary across layers.Further to this, RepE (Zou et al., 2023) explores the relationship between layer depth and performance in utility estimation tasks, finding that middle-layer representations often yield the highest accuracy.</p>
<p>Inspired by these findings, our approach involves identifying the optimal layer within an LLM that provides the most effective representations for constructing an accurate evaluator.We hypothesize that a specific intermediate layer may offer the ideal balance between capturing fundamental linguistic features and the nuanced semantic understanding necessary for assessing the quality of scientific ideas.Our experiments are thus tailored to pinpoint this layer by analyzing the performance of given data across all layers.Then, we leverage its representations to enhance the performance of our idea evaluation framework.</p>
<p>Token Selection</p>
<p>Considering that a manuscript d i is composed of l sequential tokens, the semantic information of these token representations varies significantly.Due to the fact that most LLMs are auto-regressive models, the last token aggregates the attention information of all previous tokens (Zou et al., 2023).With a slight abuse of notation, by default, we use the last token representation Rep(d i , −1) ∈ R m to symbolize the entirety of the manuscript d i .</p>
<p>Nevertheless, when dealing with lengthy input texts, such as full-text manuscript d i , there are two issues with the default approach.For one thing, memory optimization mechanism such as vLLM (Kwon et al., 2023) should be adopted to prevent GPU from running out of memory.For another thing, the representation of the last token may become diluted or overly abstracted owing to the extensive accumulation of attention, potentially leading to a loss of specific semantic details pertinent to the overall idea assessment.To address these issues, we explore alternative strategies for token selection that aim to maintain the richness of semantic information while ensuring computational feasibility.</p>
<p>We consider a manuscript d i to be a composition of distinct sections.We select the last token representations from each section, and concatenate these to form a composite representation.The approach allows us to capture the essence of each section.Formally, if a manuscript d i is divided into r sections, and Rep(d i,j , −1) represents the last token of the j th section, then the combined representation Rep(d i ) is given by:
Rep(d i ) = r j=1 Rep(d i,j , −1),(3)
where denotes the concatenation operation, and Rep(d i ) is in R r×m .Similarly, we can take into account the full length of the manuscript and divide it into equidistant segments based on a predefined length to obtain Rep(d i ).By experimenting with these strategies, we aim to refine our approach to token selection and optimize the representation of manuscript for more accurate idea assessment.</p>
<p>Evaluator Training</p>
<p>In this part, we use the pre-processed Rep(d i ) to train an idea evaluator A c .Let s i be the average score given by humans for manuscript d i , reflecting its overall quality according to the criterion c.The average score serves as the ground truth in our training process.The evaluator A c is instantiated as an Multilayer Perceptron (MLP) with one hidden layer.The MLP is tasked with learning the mapping from the representation space to the scalar scores, which takes as input the representation Rep(d i ) for each manuscript d i and outputs a predicted score ŝi .To optimize all parameters of the MLP, we employ the Mean Squared Error (MSE) loss function:
L = 1 n n i=1 (ŝ i − s i ) 2 . (4)
By minimizing L, the MLP learns to approximate the human-assigned scores as closely as possible.Through this training process, we aim to calibrate the evaluator A c such that it can reliably predict the value of new or unseen ideas based on their textual representations.</p>
<p>Experiments</p>
<p>This section presents a series of experiments to verify the performance of LLMs in the task of quantitative evaluation of ideas.Our main focus is on the mean value of the criteria overall quality, which is used as the training objective for the idea evaluator.Through our released benchmark and the experimental methodology, we answer the following four research questions (RQs):</p>
<p>• RQ1: To what extent do the representations from LLMs correlate with human judgements in the evaluation of scientific ideas?Additionally, is the LLM generation method suitable for this task?</p>
<p>• RQ2: What is the impact of choosing different layers and tokens for LLM representations on the performance of idea evaluation?</p>
<p>• RQ3: How significantly does the consistency of human judgements influence the performance of LLM representations in this context?</p>
<p>• RQ4: How does the size of training set impact the correlation between A c evaluations and human judgments in idea assessment?</p>
<p>Baselines</p>
<p>We categorize the baselines into three distinct groups: LLM Generation, LLM Representation, and Human Evaluation.The first category involves LLMs generating numerical scores in response to textual descriptions of ideas.This category includes models such as GPT-3.5-turbo,LLaMa-2-7b-base (Touvron et al., 2023), and Baichuan-2-7bbase (Yang et al., 2023), which are fine-tuned using techniques like LoRA (Hu et al., 2021) or with full parameter updates.The prompts we choose are presented in Appendix A. In the LLM Representation category, we evaluate models like BERT (Kenton and Toutanova, 2019), SciBERT (Beltagy et al., 2019), and RePE (Zou et al., 2023).For BERT and SciBERT, we also apply our proposed framework to quantify the value of ideas, with the primary distinction being in the token selection strategy.Specifically, we used the [CLS] token as the representation of an idea, and if the length of a section exceeds 512 tokens, we will divide it into equidistant subsections to apply the token selection strategy for BERT-like models.Moreover, we also analyze the performance of human evaluators through randomly selecting one score from the human-rated list against other scores.</p>
<p>Training Settings and Evaluation Details</p>
<p>In our implementation, our method employs LLaMA-2-7b-base as the foundational model.In order to make our experiments more solid and validate our framework is model-agnostic, we also use Baichuan-2-7b-base as the base model, the results of which are provided in Appendix C. We use the grid search to find appropriate sets of hyperparameters for baselines and our proposed method.</p>
<p>For the configuration of the MLP evaluator, we choose a batch size of 32, a hidden layer dimension of 1024, a learning rate of 0.001, a dropout rate of 0.2, and employ the Adam optimizer.We limit the training to 20 epochs.More detailed settings are documented in Appendix B. Each experiment is executed three times with random initializations, and the mean results are reported.We use the results of the training set for model selection.All experiments are first conducted to evaluate the efficacy of our framework using the abstracts of papers for all research questions.We also explore the effects of using the full texts of papers as the training inputs for the token selection in RQ2.</p>
<p>To gauge the alignment of scores generated by  various methods with human-assigned scores, we report a widely-used metric called Spearman Correlation (Spearman, 1961).The correlation corr with human is defined as:
corr = ρ([s 1 , s 2 , ..., s n ], [ŝ 1 , ŝ2 , ..., ŝn ]), (5)
where ρ is the Spearman Correlation function, s i is the average score given by humans, and ŝi is the A c predicted score for d i in the testing set.Since Spearman correlation is invariant under affine transformations, we also provide score distribution and the absolute error between human-rated scores and our LLM representation scores in Section 6.4 and Appendix E.</p>
<p>Comparative Experiments (RQ1)</p>
<p>According to the principle of consistency sorting in Section 5.1, we construct training sets using the top 5% and top 30% ratios from ICLR23-low-std and ICLR23-all datasets respectively to preliminarily exclude the influence of dataset proportion on the conclusion, and take the rest of each dataset as the testing set.Table 2 shows the Spearman correlations with humans of different methods on these two datasets.We also provide indexes for the layers with the highest correlation.It can be observed that our proposed method achieves the best performance in all settings, where the performance on ICLR23-all is at most 30% higher than the second best method SciBERT.As expected, the correlation of ICLR23-low-std among human scores is close to 1, which is attributed to our data partitioning strategy.It should be noted that the correlation of our method on ICLR23-all dataset exceed the result of humans, when the training ratio is 30%, proving the feasibility of our method and its potential ability to be applied to real-world review scenarios.Moreover, in terms of different layers' performance, the middle and back layers of most models may achieve better results.</p>
<p>For the LLM Generation baselines, the finetuned LLaMA-2 is worse than Baichuan-2, especially for the LLaMA-2-Full-SFT, fine-tuned with full parameters, lacking the capability of effective evaluation since its pvalue &gt; 0.05.Due to the inability of GPT-3.5 being fine-tuned, we adopt the zero-shot setting, which is only for sketchy reference.We also try k-shot setting for GPT-3.5, but it only generates the most frequent scores from the given examples.Overall, the LLM Generation methods are not competent for the quantitative evaluation of ideas.By analyzing the generated results, we believe there are two possible reasons.One is that the amount of training data is relatively small, and models are prone to overfitting.Apart from that, LLM generation is not sensitive to digital numbers, and the semantic knowledge is hidden in its representations, which should be guided through appropriate means.</p>
<p>Furthermore, our experiment studies the degree of consistency between the predicted score and that assigned by the human reviewer whose score most closely match the predicted one.As depicted in Table 3, the ICLR23-all dataset exhibits a higher consistency with the closest human-rated scores compared to the ICLR23-low-std dataset.This suggests that, despite the higher variance in human scores of the ICLR23-all dataset, our proposed method is adept at mirroring the evaluation of the most similar human reviewer.</p>
<p>Score Distribution (RQ1)</p>
<p>We also examine the difference (absolute error) between human-rated scores and the predicted scores on ICLR23-low-std dataset.The results are shown in the pie chart of Figure 2. We can see that 86.8% paper scores generated by our method are close to the human-rated scores, where the differences between them are lower than 2. Additionally, the distributions are shown in the right part of Figure</p>
<p>The distribution of scores predicted by our idea evaluator is normal distribution as expected while the human reviewers tend to give more higher or lower scores.More analysis can be found in Appendix E.</p>
<p>Influence of Layer Selection (RQ2)</p>
<p>We analyzed the representational efficacy across various layers of LLM and SciBERT.As illustrated in Figure 3, it is evident that for both LLM and SciBERT, the representations from the middle to later layers outperform those from other layers.Obviously, the very last layers do not typically yield the best performance.This may be attributed to the specific semantic information encapsulated in different layers.The last layer is inherently born to facilitate generation tasks, rather than tasks that require more discriminative capabilities, like classification or regression.
-3 0 -2 8 -2 6 -2 4 -2 2 -2 0 -1 8 -1 6 -1 4 -1 2 -1 0 -8 -6 -4 -2
Ours Layer Given the nuanced role of layer-specific representations in the context of assessing the merit of scientific ideas, we propose a layer selection of representations for the task at hand.Specifically, we advocate for the utilization of representations from the layers situated in the last one-third of the model's depth.Such choice is informed by the empirical evidence suggesting that these layers strike a balance between retaining rich semantic content and providing the necessary abstraction for discriminative tasks.</p>
<p>Influence of Token Selection (RQ2)</p>
<p>In this part, we use the ICLR23-all dataset to investigate the influence of token selection.In terms of the paper abstract inputs, we first test the correlation results of using the last token.Subsequently, we expand our scope to include both the middle and last tokens (middle + last token) of the input text.The findings, as presented in Table 4, indicate that solely relying on the last token yields superior results compared to combining it with the middle token.The latter approach appears to introduce a surplus of redundant information that may hinder the downstream performance of evaluator.</p>
<p>When dealing with the full text of papers, we implement two token selection strategies outlined in Section 5.3: the amalgamation of last tokens from equidistant segments and the aggregation of last tokens from distinct paper sections.The ex-- 3 1  -2 9  -2 7  -2 5  -2 3  -2 1  -1 9  -1 7  -1 5  -1 3  -1
.5 -3 1 -2 9 -2 7 -2 5 -2 3 -2 1 -1 9 -1 7 -1 5 -1 3 -1 1 -9 -7 -5 -3 -1
ICLR23-all 0.01 0.05 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9</p>
<p>Training Ratio 0.12 0.09 0.09 0.10 0.14 0.18 0.18 0.20 0.20 0.21 0.18 0.20 0.18 0.16 0.17 0.19 0.12 0.17 0.12 0.20 0.15 0.13 0.12 0.10 0.15 0.12 0.17 0.17  perimental outcomes suggest that merging last tokens from all segments does not effectively capture the necessary semantic content.This is largely due to the fact that many segments are incomplete sentences, arbitrarily truncated from the original texts.On the other hand, the section-based strategy successfully compiles comprehensive information from each complete section.Overall, we recommend using the representations of the last tokens or section-based strategy to train the idea evaluator.</p>
<p>Analysis of Training Set (RQ3 &amp; RQ4)</p>
<p>Figure 4 shows the Spearman correlations varying with layers under different training ratios on ICLR23 datasets.Due to space limitations, we only present the results of odd numbered layers.See Appendix C for more results.The manuscript papers are sorted in ascending order of variance in their corresponding human-rated scores to partition datasets according to the consistency sorting in Section 5.1.For ICLR23-low-std dataset, the human-rated scores in the dataset are highly consistent, and it is observed that the Spearman correlation tends to improve in tandem with increases in the training set size.Notably, when the proportion of data used for training surpasses the 50% threshold, the correlation between the scores predicted by our idea evaluator and those assigned by human experts becomes moderate, exceeding 0.4.Furthermore, our analysis reveals that even a relatively small subset of the data (with a training ratio of 5%) is capable of yielding positive performance.</p>
<p>As to ICLR23-all dataset, the outcomes indicate that an increase in the volume of training data does not necessarily correspond to a higher alignment with human evaluations in the testing set.The phenomenon can be attributed to the diminishing consistency of human scores as the dataset expands; that is, the variance in human-assigned scores grows with the size of the dataset.It becomes evident that while a larger training set generally provides more information, it also introduces a greater diversity of human judgment, which may not always be conducive to improving the ability of evaluator to mimic human scoring behavior.</p>
<p>Conclusion and Future Work</p>
<p>The study focuses on the quantitative evaluation of scientific ideas.We have reviewed existing methodologies for paper and idea evaluation and have broken new ground by focusing on the quantitative aspect of idea evaluation.Specifically, we first introduce a comprehensive benchmark dataset, accessible to the research community.Then, we develop a new framework that leverages the token representations of specific layers in LLM to quantify the value of ideas.Through rigorous experiments, we demonstrate that LLM representations correlate more strongly with human judgments compared to generative text outputs.Additionally, in our benchmark, the predicted scores of more than 80% papers are close to human-rated scores.In the future, we will broaden the scope of our research to encompass diverse disciplines with balanced data ratios, including the exact and social sciences, to further validate and refine our evaluative framework.</p>
<p>Limitations Discipline</p>
<p>The scope of our research is confined to the field of computer science, which may restrict the broader applicability of our framework.The generalization performance of our model across different scientific disciplines remains an open question.Future research endeavors should aim to adapt and validate the framework in diverse fields, ranging from the exact sciences to the humanities.</p>
<p>Criterion</p>
<p>Our experiments have primarily focused on the overall quality score of manuscript papers, which is a composite yet somewhat abstract.Important aspects such as the correctness of the presented work and its novelty are equally critical in determining a paper's impact and significance.In forthcoming studies, we plan to dissect these individual criteria, developing a more granular approach to idea evaluation.</p>
<p>Model Scale</p>
<p>The impact of model scale on performance is an aspect that has not been extensively explored in our research.The performance of LLMs is often closely tied to the number of parameters they contain; thus, models with different sizes may yield different results in the task of idea evaluation.Larger models may have the capacity to encode more nuanced representations of text, potentially leading to more accurate assessments of scientific ideas.Conversely, they may also introduce complexities that do not necessarily translate to better performance, such as overfitting or increased computational costs.The trade-offs between model size, accuracy, and efficiency are still an area ripe for exploration.</p>
<p>Ethics Statement</p>
<p>The dataset used in our study consists of publicly available academic papers.We have ensured that all data was collected and handled in a manner that respects the privacy and intellectual property rights of the authors.No personal data was used, and all information is attributed to its original source.</p>
<p>We are committed to transparency in our research process.To this end, we have made our benchmark dataset publicly available and have provided detailed descriptions of our methodologies and experimental setups to facilitate reproducibility by other researchers.</p>
<p>We recognize the importance of the human element in the evaluation of scientific ideas.Our framework is designed to assist, rather than replace, human judgment.We believe that the most effective use of our model is as a tool to support and enhance the work of human reviewers, not to supplant them.</p>
<p>A Prompts for LLM Generation</p>
<p>For the baselines involving LLM generation, we design a prompt to elicit the evaluation of a manuscript paper's overall quality based on its abstract.The prompt, which demonstrated optimal performance when applied to GPT-3.5-turbo, is structured as follows:</p>
<p>Evaluate the quality (overall quality score) of the following manuscript paper based on its abstract.</p>
<p>title: {title} abstract: {abstract}</p>
<p>The score should be between 1 and 10, with 1 being the lowest and 10 being the highest.Just output your score, no more other words.</p>
<p>It is important to note that while this prompt is most effective for GPT-3.5-turbo,its influence on the performance of other fine-tuned models, such as LLaMA-2 and Baichuan-2, is less pronounced.These models have been trained to adapt to the distribution of scores in the training set, which mitigates the impact of the prompt's phrasing on their generative capabilities.</p>
<p>B Hyper Parameters</p>
<p>We first declare that the reason for using LLaMA-2 instead of LLaMA-3 or other updated models is because we are concerned that new models may be pretrained using papers from ICLR23, resulting in a data leakage problem.</p>
<p>Parameter</p>
<p>Value learning rate 2e-5 epoch 3 weight decay 0 warmup ratio 0.03 bf16 True We detail the hyper-parameters for the LLM generation baseline, LLaMA-2-Full-SFT, in Table 5.Additionally, the hyper-parameters for models trained with the LoRA technique, specifically LLaMA-2-LoRA-SFT and Baichuan-2-LoRA-SFT, are outlined in Table 6.</p>
<p>Parameter</p>
<p>LLaMA For the representation-based evaluation method RePE (Zou et al., 2023), we employ Principal Component Analysis (PCA) as the embedding evaluation mechanism, in line with the recommendations provided in the original paper.</p>
<p>C Expanded Results</p>
<p>Building upon the experimental setup detailed in Section 6.2, we examine the performance across all layers of the foundational model, LLaMA-2-7b-base.The detailed Spearman correlation results, which consider the full spectrum of layers under various training ratios, are illustrated in Figure 5 for the ICLR23-low-std dataset and in Figure 6 for the ICLR23-all dataset.</p>
<p>In our pursuit to validate the robustness of our findings, we conducted parallel experiments using Baichuan-2-7b-base as an alternative base model.The corresponding Spearman correlation results are depicted in Figure 7 and Figure 8.The patterns observed with Baichuan-2-7b-base are found to be in harmony with those from the LLaMA-2-7b-base model, lending credence to the consistency and reliability of our conclusions.</p>
<p>The experiments across different models not only reinforces the validity of our initial observations but also suggests that the underlying phenomena we have identified are model-agnostic to a certain extent.Such findings are indicative of the potential generalizability of our framework, hinting at its applicability across a variety of LLMs.Future work may delve deeper into the comparative analysis of additional models, further expanding our understanding of the relationship between the base model and the efficacy of idea evaluation.</p>
<p>D Case Study</p>
<p>We list four cases to show the performance and drawbacks of our method in Table 7.All these cases are selected from the domain of reinforcement learning.The first two cases are correctly predicted, and the human-rated scores and LLM representation scores are very close.As to the third case, although our method gives an overestimated score, the final score is not enough to make it acceptable.The fourth case is underestimated.One possible reason is that in such cases, our method may lack more contextual information to make a decision, such as tables and figures in papers, which is also something we need to consider in the future.</p>
<p>E Domain Analysis</p>
<p>To see how our method rates ideas on popular topics or less trendy domains, we analyze the score distributions and differences between human-rated scores and LLM representation scores in 14 domains divided by ICLR-2023 program committee.The results are shown in Table 8.On the whole, the differences in mean scores of most domains are less than 10%.However, there are three domains (Theory, Neuroscience and Cognitive Science, Infrastructure) where the mean values of human-rated scores are relatively higher than average, and the differences exceed 10%.We believe these three domains are distinguished from other domains since others frequently focus on Learning and Optimization, which makes our evaluator overfitting on these data.Therefore, it is necessary to train a domainspecific evaluator, while also maintaining a balance in the content of the dataset.We will address these issues in our future work.</p>
<p>F Frequently Asked Questions Numerical Processing Limitations in LLMs</p>
<p>A critical issue faced by LLMs is their inherent difficulty in processing numerical data, such as digits.This limitation stems from the finite-sized vocabulary and tokenization strategies used by these models, affecting both encoder and decoder architectures.This impacts the ability to perform tasks that require precise numerical understanding.Therefore, we leverage the deep, contextual representations within LLMs to quantify the value of scientific ideas.These representations encapsulate rich semantic and contextual information that extends beyond the superficial token sequences.</p>
<p>Our approach diverges significantly from the strategy of merely adding task-specific heads to the model.Instead, our approach involves a strategic selection of layers and tokens.By leveraging the hierarchical processing capabilities of LLMs, we can harness the most relevant and informative features for idea evaluation.This approach contrasts with using the entire weight set of the LLM (adding head to LLM), which might not be as efficient or effective for capturing the specific attributes necessary for assessing the value of scientific ideas.</p>
<p>Framework</p>
<p>The current design of our framework shows considerable promise in the automated assessment of scientific ideas, yet there are avenues for further enhancing the evaluator's performance.The quantitative assessment of scientific ideas is inherently complex, involving a blend of objective metrics and subjective judgments.Our method, leveraging the representations of large language models, demonstrates the potential to approximate human judgment to a significant degree, which will provide human reviewers with an objective score, rather than replacing them to give subjective comments from multiple perspectives.</p>
<p>Scores</p>
<p>Human-rated Score: 7.33 LLM Representation Score: 5.21 (Under Estimated)</p>
<p>Table 7: Case study of the comparision between human-rated scores and LLM representation scores
-3 1 -3 0 -2 9 -2 8 -2 7 -2 6 -2 5 -2 4 -2 3 -2 2 -2 1 -2 0 -1 9 -1 8 -1 7 -1 6 -1 5 -1 4 -1 3 -1 2 -1 1 -1 0 -9 -8 -7 -6 -5 -4 -3 -2 -1
0.01 0.05 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9</p>
<p>Training Ratio -0.05 -0.08 -0.06 -0.04 -0.04 -0.02 -0.04 -0.04 -0.03 0.01 0.07 0.11 0.11 0.11 0.10 0.11 0.12 0.12 0.09 0.09 0.04 0.01 -0.00 -0.03 -0.02 -0.04 -0.02 -0.04 -0.05 -0.02 -0.02 0.17 0.20 0.21 0.25 0.22 0.27 0.25 0.25 0.26 0.25 0.33 0.34 0.33 0.32 0.31 0.33 0.33 0.33 0.33 0.32 0.31 0.30 0.29 0.30 0.30 0.28 0.28 0.30 0.30 0.27 0.32  -  0.12 0.18 0.20 0.19 0.15 0.19 0.13 0.14 0.12 0.12 0.10 0.12 0.15 0.15 0.12 0.13 0.17 0.17 0.17 0.18 0.20 0.17 0.18 0.17 -3 1 Training Ratio 0.09 0.09 0.09 0.14 0.15 0.16 0.12 0.12 0.11 0.14 0.14 0.15 0.16 0.16 0.16 0.17 0.18 0.17 0.19 0.19 0.18 0.18 0.18 0.17 0.16 0.16 0.16 0.17 0.17 0.17 0.16 0.11 0.14 0.14 0.18 0.19 0.21 0.20 0.20 0.22 0.24 0.23 0.24 0.25 0.26 0.26 0.27 0.26 0.25 0.24 0.26 0.27 0.27 0.26 0.27 0.26 0.26 0.24 0.25 0.26 0.27 0.26 0.14 0.17 0.18 0.19 0.17 0.19 0.17 0.08 0.09 0.16 0.16 0.15 0.21 0.17 0.15 0.18 0.17 0.14 0.15 0.18 0.16 0.15 0.17 0.18 0.17 0.16 0.16 0.18 0.15 0.15 0.16 0.17 0.17 0.19 0.19 0.17
3 1 -3 0 -2 9 -2 8 -2 7 -2 6 -2 5 -2 4 -2 3 -2 2 -2 1 -2 0 -1 9 -1 8 -1 7 -1 6 -1 5 -1 4 -1 3 -1 2 -1 1 -1 0 -9 -8 -7 -6 -5 -4 -3 -2 -10-3 0 -2 9 -2 8 -2 7 -2 6 -2 5 -2 4 -2 3 -2 2 -2 1 -2 0 -1 9 -1 8 -1 7 -1 6 -1 5 -1 4 -1 3 -1 2 -1 1 -1 0 -9 -8 -7 -6 -5 -4 -3 -2 -10
Figure 1 :
1
Figure 1: Pipeline of quantitative evaluation of ideas using the representations of LLMs.</p>
<p>Figure 2 :
2
Figure 2: The difference (absolute error) between human-rated scores and the predicted scores (left subfigure).The distributions of human-rated scores and the predicted (right subfigure).</p>
<p>Figure 4 :
4
Figure 4: Spearman correlations varying with layers under different training ratios on ICLR23 datasets.</p>
<p>Figure 5 :
5
Figure 5: Spearman correlations varying with layers under different training ratios of LLaMA-2-7b-base on ICLR23-low-std dataset.</p>
<p>0.16 0.16 0.20 0.21 0.19 0.21 0.22 0.23 0.22 0.23 0.23 0.22 0.22 0.23 0.24 0.23 0.23 0.24 0.23 0.25 0.24 0.25 0.24 0.25 0.18 0.21 0.20 0.20 0.18 0.21 0.19 0.20 0.21 0.22 0.24 0.23 0.23 0.20 0.25 0.25 0.25 0.25 0.26 0.26 0.25 0.23 0.27 0.26 0.25 0.25 0.26 0.27 0.24 0.27 0.28 0.15 0.19 0.21 0.20 0.22 0.23 0.20 0.18 0.14 0.18 0.16 0.18 0.18 0.20 0.18 0.19 0.19 0.19 0.20 0.21 0.20 0.20 0.20 0.22 0.20 0.21 0.22 0.22 0.21 0.21 0.22</p>
<p>Figure 6 :
6
Figure 6: Spearman correlations varying with layers under different training ratios of LLaMA-2-7b-base on ICLR23-all dataset.</p>
<p>Figure 7 :
7
Figure 7: Spearman correlations varying with layers under different training ratios of Baichuan-2-7b-base on ICLR23-low-std dataset.</p>
<p>Figure 8 :
8
Figure 8: Spearman correlations varying with layers under different training ratios of Baichuan-2-7b-base on ICLR23-all dataset.</p>
<p>Table 1 :
1
Statistics of benchmarks.
ICLR23-low-stdICLR23-all# paper19013795overall quality5.52 ± 0.615.41 ± 1.06correctness3.09 ± 0.443.09 ± 0.49technical novelty2.59 ± 0.432.59 ± 0.48empirical novelty2.56 ± 0.412.56 ± 0.47</p>
<p>Table 2 :
2
Spearman correlations with humans of different methods on ICLR23 datasets.N/A in corr column means its corresponding pvalue &gt; 0.05.There is no need for LLM Generation baselines to select layers.The human performance is evaluated by randomly selecting one score from the human-rated list against other reviews.</p>
<p>Table 3 :
3
Spearman correlations with the closest humanrated score.</p>
<p>0.05 -0.06 -0.04 -0.04 -0.03 0.07 0.11 0.10 0.12 0.09 0.04 -0.00 -0.02 -0.02 -0.05 -0.02 0.17 0.21 0.22 0.25 0.26 0.33 0.33 0.31 0.33 0.33 0.31 0.29 0.30 0.28 0.30 0.32 0.16 0.21 0.25 0.25 0.23 0.30 0.31 0.26 0.32 0.34 0.34 0.34 0.31 0.31 0.32 0.34 0.18 0.24 0.25 0.24 0.25 0.35 0.31 0.31 0.31 0.33 0.33 0.31 0.33 0.32 0.33 0.34 0.19 0.24 0.26 0.25 0.28 0.31 0.31 0.30 0.31 0.37 0.36 0.39 0.35 0.35 0.35 0.36 0.20 0.25 0.26 0.24 0.20 0.32 0.34 0.27 0.35 0.35 0.38 0.39 0.38 0.36 0.36 0.36 0.20 0.24 0.22 0.22 0.27 0.27 0.25 0.23 0.34 0.34 0.35 0.36 0.41 0.39 0.40 0.37 0.24 0.28 0.25 0.24 0.26 0.34 0.35 0.34 0.36 0.40 0.41 0.41 0.42 0.43 0.42 0.44
Training Ratio0.01 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.050.32 0.33 0.25 0.21 0.28 0.36 0.42 0.48 0.55 0.56 0.55 0.52 0.55 0.51 0.48 0.47 0.25 0.29 0.28 0.31 0.28 0.33 0.39 0.36 0.39 0.43 0.43 0.42 0.43 0.43 0.45 0.43 0.23 0.27 0.28 0.26 0.28 0.35 0.37 0.36 0.42 0.42 0.41 0.40 0.43 0.43 0.39 0.440.0 0.1 0.2 0.3 0.4 01 -9 -7 -5 -3 -1 ICLR23-low-std
-</p>
<p>Table 4 :
4
Spearman correlations of different token selection strategies.
Token Selection StrategyTraining Ratio 5% 30%Abstractlast token0.27830.3366middle + last token0.21620.2772Full-textsegment tokens0.13060.2597section tokens0.32580.3821</p>
<p>Table 5 :
5
Hyper-parameters of LLaMA-2-Full-SFT.</p>
<p>0.16 0.22 0.21 0.23 0.25 0.26 0.25 0.24 0.23 0.24 0.30 0.31 0.31 0.30 0.26 0.26 0.32 0.34 0.34 0.33 0.34 0.33 0.34 0.33 0.31 0.34 0.31 0.31 0.32 0.32 0.34 0.18 0.23 0.24 0.25 0.25 0.24 0.24 0.26 0.25 0.30 0.35 0.31 0.31 0.31 0.31 0.31 0.31 0.32 0.33 0.33 0.33 0.33 0.31 0.32 0.33 0.31 0.32 0.33 0.33 0.35 0.34 0.19 0.24 0.24 0.24 0.26 0.22 0.25 0.28 0.28 0.33 0.31 0.34 0.31 0.31 0.30 0.33 0.31 0.37 0.37 0.36 0.36 0.37 0.39 0.35 0.35 0.37 0.35 0.32 0.35 0.37 0.36 0.20 0.25 0.25 0.24 0.26 0.25 0.24 0.22 0.20 0.21 0.32 0.31 0.34 0.27 0.27 0.35 0.35 0.39 0.35 0.35 0.38 0.40 0.39 0.37 0.38 0.39 0.36 0.37 0.36 0.34 0.36 0.20 0.24 0.24 0.22 0.22 0.21 0.22 0.21 0.27 0.27 0.27 0.31 0.25 0.28 0.23 0.31 0.34 0.35 0.34 0.33 0.35 0.35 0.36 0.35 0.41 0.39 0.39 0.40 0.40 0.32 0.37 0.24 0.30 0.28 0.27 0.25 0.27 0.24 0.31 0.26 0.27 0.34 0.29 0.35 0.32 0.</p>
<p>.12 0.09 0.09 0.11 0.09 0.11 0.10 0.12 0.14 0.15 0.18 0.18 0.18 0.18 0.20 0.21 0.20 0.20 0.21 0.20 0.18 0.18 0.20 0.18 0.18 0.15 0.16 0.16 0.17 0.14 0.19 0.12 0.16 0.17 0.19 0.20 0.20 0.20 0.18 0.19 0.18 0.22 0.23 0.24 0.23 0.22 0.22 0.24 0.26 0.27 0.25 0.27 0.26 0.26 0.25 0.27 0.26 0.26 0.27 0.25 0.26 0.28 0.16 0.18 0.20 0.20 0.19 0.19 0.20 0.18 0.23 0.24 0.26 0.25 0.24 0.22 0.25 0.24 0.25 0.24 0.27 0.26 0.25 0.27 0.26 0.25 0.25 0.27 0.26 0.26 0.26 0.26 0.25 0.18 0.19 0.21 0.22 0.20 0.22 0.17 0.15 0.16 0.21 0.26 0.22 0.26 0.23 0.23 0.25 0.22 0.25 0.27 0.29 0.28 0.30 0.26 0.30 0.27 0.29 0.31 0.26 0.30 0.27 0.28 0.19 0.21 0.21 0.24 0.20 0.20 0.21 0.22 0.23 0.23 0.24 0.24 0.26 0.24 0.29 0.27 0.26 0.27 0.29 0.30 0.29 0.30 0.29 0.29 0.32 0.32 0.32 0.34 0.30 0.32 0.33 0.16 0.20 0.23 0.24 0.22 0.22 0.21 0.22 0.24 0.25 0.26 0.27 0.28 0.27 0.26 0.26 0.28 0.30 0.31 0.30 0.30 0.30 0.30 0.30 0.31 0.29 0.29 0.30 0.28 0.29 0.31 0.16 0.18 0.19 0.21 0.19 0.20 0.19 0.21 0.22 0.23 0.21 0.20 0.22 0.21 0.24 0.23 0.23 0.25 0.24 0.23 0.25 0.23 0.23 0.22 0.25 0.24 0.23 0.23 0.26 0.26 0.27
Training Ratio.01 0.05 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.9 0.8
00.15 0.18 0.20 0.18 0.17 0.15 0.17</p>
<p>Table 8 :
8
The detailed distribution of LLM representation scores and human-rated scores in 14 domains.
Min MaxHuman Min Ours Min Human Max Ours Max1.6667 1.7936 8.0000 8.04281.5000 1.0039 8.0000 7.23201.5000 2.2600 8.0000 8.20552.3333 1.7422 8.0000 7.49291.0000 2.3488 8.0000 7.62882.0000 3.5655 8.0000 8.31561.5000 1.5492 8.0000 7.70182.0000 2.9334 8.0000 7.25511.5000 1.5517 8.0000 7.29601.6667 2.3116 8.0000 7.34443.0000 2.7473 8.0000 6.69932.5000 3.3245 8.0000 7.99183.5000 3.0230 8.0000 7.18371.0000 0.34500 5.7500 5.3299MeanHuman Mean Ours Mean Diff5.5131 ± 1.3967 5.3385 ± 1.0125 3.17%5.4062 ± 1.4132 5.1867 ± 1.0791 4.06%5.4297 ± 1.5289 5.2762 ± 1.1101 2.83%5.4955 ± 1.2828 5.1640 ± 1.0825 6.03%5.6285 ± 1.4025 5.1934 ± 1.0932 7.73%6.1155 ± 1.2607 5.3296 ± 1.0076 12.85%5.6245 ± 1.3514 5.4084 ± 1.0636 3.84%5.5672 ± 1.2967 5.2882 ± 0.9917 5.01%5.3518 ± 1.3639 5.0908 ± 0.9983 4.88%5.2392 ± 1.6701 5.0426 ± 1.1133 3.75%6.0667 ± 1.3181 5.1115 ± 0.9040 15.75%5.6442 ± 1.4135 5.2455 ± 0.9943 7.06%5.9648 ± 1.2578 5.2065 ± 1.0139 12.71%3.5500 ± 1.6763 3.6157 ± 1.8071 1.85%Num52716823211124986729576962349185DomainDeep Learning and representational learningSocial Aspects of Machine LearningReinforcement LearningGeneral Machine LearningApplicationsTheoryGenerative modelsUnsupervised and Self-supervised learningOptimizationMachine Learning for SciencesNeuroscience and Cognitive ScienceProbabilistic MethodsInfrastructureNone</p>
<p>Shubham Agarwal, H Issam, Laurent Laradji, Christopher Charlin, Pal, arXiv:2402.01788Litllm: A toolkit for scientific literature review. 2024arXiv preprint</p>
<p>Scibert: A pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Growth rates of modern science: A bibliometric analysis based on the number of publications and cited references. Lutz Bornmann, Rüdiger Mutz, Journal of the association for information science and technology. 66112015</p>
<p>Ai-assisted peer review. Alessandro Checco, Lorenzo Bracciale, Pierpaolo Loreti, Stephen Pinfield, Giuseppe Bianchi, Humanities and Social Sciences Communications. 812021</p>
<p>Transformer feed-forward layers are key-value memories. Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Lora: Low-rank adaptation of large language models. J Edward, Phillip Hu, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2021</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Proceedings of NAACL-HLT. Jacob Devlin, Ming-Wei Chang, Kenton , Lee Kristina, Toutanova , NAACL-HLT2019</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the 29th Symposium on Operating Systems Principles. the 29th Symposium on Operating Systems Principles2023</p>
<p>Can large language models provide useful feedback on research papers? a large-scale empirical analysis. Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Daniel Smith, Yian Yin, arXiv:2310.017832023arXiv preprint</p>
<p>Ryan Liu, Nihar B Shah, arXiv:2306.00622Reviewergpt? an exploratory study on using large language models for paper reviewing. 2023arXiv preprint</p>
<p>Grobid: Combining automatic bibliographic data recognition and term extraction for scholarship publications. Patrice Lopez, Research and Advanced Technology for Digital Libraries: 13th European Conference. Corfu, GreeceSpringer2009. 2009. September 27-October 2, 200913</p>
<p>SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models. Potsawee Manakul, Adian Liusie, Mark Gales, 10.18653/v1/2023.emnlp-main.557Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>The proof and measurement of association between two things. Charles Spearman, 1961</p>
<p>Albert N Tabah, Literature dynamics: Studies on growth, diffusion, and epidemics. Annual review of information science and technology (ARIST). 199934</p>
<p>Predicting article quality scores with machine learning: The uk research excellence framework. Mike Thelwall, Kayvan Kousha, Paul Wilson, Meiko Makita, Mahshid Abdoli, Emma Stuart, Jonathan Levitt, Petr Knoth, Matteo Cancellieri, Quantitative Science Studies. 422023</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Quantifying knowledge from the perspective of information structurization. Xinbing Wang, Huquan Kang, Luoyi Fu, Ling Yao, Jiaxin Ding, Jianghao Wang, Xiaoying Gan, Chenghu Zhou, John E Hopcroft, Plos one. 181e02793142023</p>
<p>Exploring and verbalizing academic ideas by concept co-occurrence. Yi Xu, Shuqian Sheng, Bo Xue, Luoyi Fu, Xinbing Wang, Chenghu Zhou, 10.18653/v1/2023.acl-long.727Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chenxu Chao Yin, Lv, Dian Da Pan, Dong Wang, Yan, arXiv:2309.10305Open large-scale language models. 20232arXiv preprint</p>
<p>Can we automate scientific reviewing. Weizhe Yuan, Pengfei Liu, Graham Neubig, Journal of Artificial Intelligence Research. 752022</p>
<p>Siren's song in the ai ocean: A survey on hallucination in large language models. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, arXiv:2309.012192023arXiv preprint</p>
<p>Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, arXiv:2310.01405Representation engineering: A topdown approach to ai transparency. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
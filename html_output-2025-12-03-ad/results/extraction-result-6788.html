<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6788 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6788</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6788</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-253180818</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2212.08686v2.pdf" target="_blank">Evaluating Step-by-Step Reasoning through Symbolic Verification</a></p>
                <p><strong>Paper Abstract:</strong> Pre-trained language models (LMs) have shown remarkable reasoning performance using explanations or chain-of-thoughts (CoT)) for in-context learning. On the other hand, these reasoning tasks are usually presumed to be more approachable for symbolic programming. To understand the mechanism of reasoning of LMs, we curate synthetic datasets containing equivalent (natural, symbolic) data pairs, where symbolic examples contain first-order logic rules and predicates from non-parametric knowledge bases (KBs), supporting automated verification of intermediate reasoning results. Then we revisit neuro-symbolic approaches and propose to learn from demonstrations containing logic rules and corresponding examples to iteratively reason over KBs, recovering Prolog's backward chaining algorithm and supporting automated verification of LMs' outputs. Comprehensive experiments are included to systematically compare LMLP with CoT in deductive reasoning settings, showing that LMLP enjoys more than $25\%$ higher accuracy than CoT on length generalization benchmarks even with smaller model sizes.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6788.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6788.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LMLP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Models as Logic Programmers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic in‑context learning method that treats a pre-trained LM as a planner (P_θ) and a sentence embedding model as a translator (T_ϕ) to iteratively generate natural-language proof steps which are projected into a symbolic KB for verification; the loop enforces chain‑rule constraints and terminates when a target is reached.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 / GPT2-Large; LLaMA2-7B; Mistral-7B (used as planning LMs in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer language models used as the Planning LM (P_θ) to generate free-form proof steps; a separate masked/sentence embedding model (Translation LM T_ϕ implemented with Sentence-BERT / Sentence-RoBERTa-Large) maps generated sentences to KB (subject, relation, object) predicates via cosine similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-2 family (GPT2-large used; ~774M for GPT2-large), LLaMA2-7B (7B), Mistral-7B (~7B); translation LM: Sentence-RoBERTa-Large (size not specified in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (autoregressive LM) + neuro-symbolic loop with external non-parametric KB and sentence-embedding translator</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on large web/text corpora (standard LM pretraining; specific corpora not specified); no additional fine-tuning reported for LMLP beyond using in-context examples and external KBs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Iterative in-context planning: retrieve a logic rule and a grounded example from R, concatenate with the query as prompt, generate candidate natural-language steps via Planning LM, translate each step to the closest KB predicate via a Translation LM embedding similarity, enforce chain-rule transition constraints, append chosen predicate to prompt and repeat until target reached (symbolic verification of each intermediate step).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Uses an explicit non-parametric knowledge base (KB) of facts F and first-order logic rules R as the grounding and symbolic verifier; uses a Translation LM (Sentence-BERT / Sentence-RoBERTa) to map LM outputs to KB predicates and constrain the search to KB facts. Not a theorem prover or SAT solver but a symbolic KB for provenance verification.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>CLUTRR-LP; Countries-LP (derived from CLUTRR and Countries datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>CLUTRR-LP: synthetic family-relationship stories paired with equivalent symbolic KBs and FOL rules, designed to test multi-hop relational/deductive reasoning and length generalization (test stories longer than training). Countries-LP: link-prediction style tasks (LocatedIn, NeighborOf) with three task tiers (S1,S2,S3) of increasing reasoning complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Relational multi-hop deductive reasoning / proof generation over KBs; link prediction (Countries tasks S1–S3); length generalization (extrapolating to longer proof chains than seen in training).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Human-evaluated proof plausibility / proven accuracy (success rate); reported averages over human judgments and success rates for automatic checks when possible.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example reported averages (human-eval success rates): LMLP achieved ~32.8% (GPT-2 backbone), ~49.3% (Mistral-7B backbone), ~47.9% (LLaMA2-7B backbone) average on Countries-LP / CLUTRR-LP aggregated settings reported in tables (values reported as decimals in paper). LMLP also attains much higher success rates as prompt ensembling (K) increases (e.g., K up to 10 yields many tasks nearly 100% in Countries-LP examples).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>LMLP outperforms Chain-of-Thought (CoT) prompting across length-generalization tests: reported improvements of more than ~25 percentage points over CoT on length generalization benchmarks (e.g., CoT averages ~12.96% (GPT-2), ~21.28% (Mistral), ~20.63% (LLaMA2) vs LMLP's ~32.75%, ~49.32%, ~47.91% respectively in the paper's reported averages). LMLP also outperforms other baselines (no prompt, rule-only, entity-based prompts, and Language Planner) in most settings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Confining LM-generated intermediate steps to a symbolic KB via a translator + verification loop (LMLP) produces more reliable multi-step deductive reasoning and better length generalization than free-form CoT; prompt design (rule+grounded example), prompt ensembling (K), and multiple in-context examples (N) substantially improve LMLP performance; larger planning LMs improve performance but LMLP works even with smaller backbones due to symbolic constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Evaluations use synthetic datasets and human evaluation of proof plausibility; method relies on an external KB and quality of mapping from LM outputs to KB predicates (translation errors); sensitive to in-context exemplar choice and prompt format; experiments did not include very large proprietary models (e.g., PaLM) due to resource/access limits; results may not transfer to more complex, real-world reasoning tasks or to settings without an available structured KB.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Step-by-Step Reasoning through Symbolic Verification', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6788.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6788.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that supplies few-shot examples with intermediate reasoning steps (natural-language explanations/scratchpads) so an LM generates multi-step explanations before producing answers, used here as a baseline for multi-hop relational reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied with multiple planning LMs in experiments: GPT-2 family, Mistral-7B, LLaMA2-7B (same backbones used for CoT baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer LMs prompted with (input, explanation, output) exemplars so the model imitates provided natural-language reasoning paths to answer new queries; no explicit symbolic KB grounding in standard CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Varied across experiments: GPT-2 variants and 7B-class models (Mistral-7B, LLaMA2-7B) were used as backbones for CoT evaluation in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer with chain-of-thought in-context prompting (no explicit neuro-symbolic integration).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on general web/text corpora; CoT uses in-context exemplars (natural-language explanations) extracted from training stories or neuro-symbolic proof extraction, not further fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Chain-of-thought (provide natural-language intermediate reasoning steps as part of in-context examples), generate full reasoning path and final answer in a single generation pass.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>CLUTRR-LP; Countries-LP</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same synthetic multi-hop relational reasoning benchmarks as used to evaluate LMLP; CoT uses natural-language explanations derived from stories.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Relational multi-hop reasoning / proof generation via natural-language explanation; link prediction for Countries-LP.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Human-evaluated proof plausibility / accuracy of final predicted relation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported average human-eval success rates (paper tables): CoT achieved approximately ~12.96% (GPT-2 backbone average), ~21.28% (Mistral-7B), ~20.63% (LLaMA2-7B) in comparable experimental settings (decay as reasoning length increases).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CoT underperforms LMLP on length-generalization and controlled KB-based deductive tasks; CoT's performance degrades sharply with increasing reasoning chain length while LMLP's symbolic verification preserves higher accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>While CoT can produce correct proofs on some examples, it tends to generate the entire proof in one pass and is more prone to errors/hallucination as chain length increases; natural-language explanations are less verifiable than symbolic provenance, which weakens robustness in strict deductive settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Prone to hallucination and poor compositional generalization (especially to longer chains than seen in training); sensitive to exemplar choice, ordering, and prompt format; explanations are entangled with natural language making automated verification difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Step-by-Step Reasoning through Symbolic Verification', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6788.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6788.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Language Planner (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Planner (Huang et al., 2022) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline retrieval-style method that finds the most similar training example (task) in R and uses it as an in-context prompt to the planning LM; included as a baseline to compare exemplar retrieval strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models as zero-shot planners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used with GPT-2 planning LM in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Retrieval augmented prompting strategy that locates the most similar historical example and supplies it as the prompt for the LM to imitate; no symbolic projection/verification step.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Depends on planning LM (in experiments GPT-2 variants were used).</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Retriever + Transformer planner (no KB projection step)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Uses the training set examples (R) to retrieve prompts; no additional training reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Example-retrieval then generate proof steps in natural language (single-pass imitation of retrieved example's structure).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>CLUTRR-LP; Countries-LP (used as a baseline comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same synthetic relational reasoning benchmarks; Planner retrieves similar exemplar tasks from R to form prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Relational multi-hop reasoning (treated via retrieval of exemplar proofs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Human-evaluated proof plausibility / success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported to perform worse than LMLP and often worse than simpler baselines; specific examples in the paper show Planner failing when it retrieves examples with similar entities but different relations (numerical averages for Planner are lower than LMLP across reported tables; see paper Table 2/3 for exact values).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Language Planner baseline underperforms LMLP; authors hypothesize Planner's retrieval of examples with same entities (rather than same relation) causes inappropriate guidance for relational reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieving the single most similar training example by task similarity (entity overlap) is not sufficient for correct relational reasoning; example selection must match the relation-pattern not merely entities.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Retriever can pick examples with entity overlap but different relations, leading to incorrect proofs; lacks symbolic projection/verification to the KB so hallucinations are unchecked.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Step-by-Step Reasoning through Symbolic Verification', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models <em>(Rating: 2)</em></li>
                <li>CLUTRR: A diagnostic benchmark for inductive reasoning from text <em>(Rating: 2)</em></li>
                <li>On approximate reasoning capabilities of low-rank vector spaces <em>(Rating: 1)</em></li>
                <li>End-to-end differentiable proving <em>(Rating: 2)</em></li>
                <li>Language models as zero-shot planners <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6788",
    "paper_id": "paper-253180818",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "LMLP",
            "name_full": "Language Models as Logic Programmers",
            "brief_description": "A neuro-symbolic in‑context learning method that treats a pre-trained LM as a planner (P_θ) and a sentence embedding model as a translator (T_ϕ) to iteratively generate natural-language proof steps which are projected into a symbolic KB for verification; the loop enforces chain‑rule constraints and terminates when a target is reached.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 / GPT2-Large; LLaMA2-7B; Mistral-7B (used as planning LMs in experiments)",
            "model_description": "Autoregressive transformer language models used as the Planning LM (P_θ) to generate free-form proof steps; a separate masked/sentence embedding model (Translation LM T_ϕ implemented with Sentence-BERT / Sentence-RoBERTa-Large) maps generated sentences to KB (subject, relation, object) predicates via cosine similarity.",
            "model_size": "GPT-2 family (GPT2-large used; ~774M for GPT2-large), LLaMA2-7B (7B), Mistral-7B (~7B); translation LM: Sentence-RoBERTa-Large (size not specified in paper).",
            "architecture_type": "Transformer (autoregressive LM) + neuro-symbolic loop with external non-parametric KB and sentence-embedding translator",
            "training_data": "Pretrained on large web/text corpora (standard LM pretraining; specific corpora not specified); no additional fine-tuning reported for LMLP beyond using in-context examples and external KBs.",
            "reasoning_method": "Iterative in-context planning: retrieve a logic rule and a grounded example from R, concatenate with the query as prompt, generate candidate natural-language steps via Planning LM, translate each step to the closest KB predicate via a Translation LM embedding similarity, enforce chain-rule transition constraints, append chosen predicate to prompt and repeat until target reached (symbolic verification of each intermediate step).",
            "external_tool_used": true,
            "external_tool_description": "Uses an explicit non-parametric knowledge base (KB) of facts F and first-order logic rules R as the grounding and symbolic verifier; uses a Translation LM (Sentence-BERT / Sentence-RoBERTa) to map LM outputs to KB predicates and constrain the search to KB facts. Not a theorem prover or SAT solver but a symbolic KB for provenance verification.",
            "benchmark_name": "CLUTRR-LP; Countries-LP (derived from CLUTRR and Countries datasets)",
            "benchmark_description": "CLUTRR-LP: synthetic family-relationship stories paired with equivalent symbolic KBs and FOL rules, designed to test multi-hop relational/deductive reasoning and length generalization (test stories longer than training). Countries-LP: link-prediction style tasks (LocatedIn, NeighborOf) with three task tiers (S1,S2,S3) of increasing reasoning complexity.",
            "task_type": "Relational multi-hop deductive reasoning / proof generation over KBs; link prediction (Countries tasks S1–S3); length generalization (extrapolating to longer proof chains than seen in training).",
            "performance_metric": "Human-evaluated proof plausibility / proven accuracy (success rate); reported averages over human judgments and success rates for automatic checks when possible.",
            "performance_value": "Example reported averages (human-eval success rates): LMLP achieved ~32.8% (GPT-2 backbone), ~49.3% (Mistral-7B backbone), ~47.9% (LLaMA2-7B backbone) average on Countries-LP / CLUTRR-LP aggregated settings reported in tables (values reported as decimals in paper). LMLP also attains much higher success rates as prompt ensembling (K) increases (e.g., K up to 10 yields many tasks nearly 100% in Countries-LP examples).",
            "comparison_with_baseline": "LMLP outperforms Chain-of-Thought (CoT) prompting across length-generalization tests: reported improvements of more than ~25 percentage points over CoT on length generalization benchmarks (e.g., CoT averages ~12.96% (GPT-2), ~21.28% (Mistral), ~20.63% (LLaMA2) vs LMLP's ~32.75%, ~49.32%, ~47.91% respectively in the paper's reported averages). LMLP also outperforms other baselines (no prompt, rule-only, entity-based prompts, and Language Planner) in most settings.",
            "key_findings": "Confining LM-generated intermediate steps to a symbolic KB via a translator + verification loop (LMLP) produces more reliable multi-step deductive reasoning and better length generalization than free-form CoT; prompt design (rule+grounded example), prompt ensembling (K), and multiple in-context examples (N) substantially improve LMLP performance; larger planning LMs improve performance but LMLP works even with smaller backbones due to symbolic constraints.",
            "limitations": "Evaluations use synthetic datasets and human evaluation of proof plausibility; method relies on an external KB and quality of mapping from LM outputs to KB predicates (translation errors); sensitive to in-context exemplar choice and prompt format; experiments did not include very large proprietary models (e.g., PaLM) due to resource/access limits; results may not transfer to more complex, real-world reasoning tasks or to settings without an available structured KB.",
            "uuid": "e6788.0",
            "source_info": {
                "paper_title": "Evaluating Step-by-Step Reasoning through Symbolic Verification",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought Prompting",
            "brief_description": "A prompting technique that supplies few-shot examples with intermediate reasoning steps (natural-language explanations/scratchpads) so an LM generates multi-step explanations before producing answers, used here as a baseline for multi-hop relational reasoning.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "Applied with multiple planning LMs in experiments: GPT-2 family, Mistral-7B, LLaMA2-7B (same backbones used for CoT baselines).",
            "model_description": "Autoregressive transformer LMs prompted with (input, explanation, output) exemplars so the model imitates provided natural-language reasoning paths to answer new queries; no explicit symbolic KB grounding in standard CoT.",
            "model_size": "Varied across experiments: GPT-2 variants and 7B-class models (Mistral-7B, LLaMA2-7B) were used as backbones for CoT evaluation in the paper.",
            "architecture_type": "Transformer with chain-of-thought in-context prompting (no explicit neuro-symbolic integration).",
            "training_data": "Pretrained on general web/text corpora; CoT uses in-context exemplars (natural-language explanations) extracted from training stories or neuro-symbolic proof extraction, not further fine-tuned.",
            "reasoning_method": "Chain-of-thought (provide natural-language intermediate reasoning steps as part of in-context examples), generate full reasoning path and final answer in a single generation pass.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "CLUTRR-LP; Countries-LP",
            "benchmark_description": "Same synthetic multi-hop relational reasoning benchmarks as used to evaluate LMLP; CoT uses natural-language explanations derived from stories.",
            "task_type": "Relational multi-hop reasoning / proof generation via natural-language explanation; link prediction for Countries-LP.",
            "performance_metric": "Human-evaluated proof plausibility / accuracy of final predicted relation.",
            "performance_value": "Reported average human-eval success rates (paper tables): CoT achieved approximately ~12.96% (GPT-2 backbone average), ~21.28% (Mistral-7B), ~20.63% (LLaMA2-7B) in comparable experimental settings (decay as reasoning length increases).",
            "comparison_with_baseline": "CoT underperforms LMLP on length-generalization and controlled KB-based deductive tasks; CoT's performance degrades sharply with increasing reasoning chain length while LMLP's symbolic verification preserves higher accuracy.",
            "key_findings": "While CoT can produce correct proofs on some examples, it tends to generate the entire proof in one pass and is more prone to errors/hallucination as chain length increases; natural-language explanations are less verifiable than symbolic provenance, which weakens robustness in strict deductive settings.",
            "limitations": "Prone to hallucination and poor compositional generalization (especially to longer chains than seen in training); sensitive to exemplar choice, ordering, and prompt format; explanations are entangled with natural language making automated verification difficult.",
            "uuid": "e6788.1",
            "source_info": {
                "paper_title": "Evaluating Step-by-Step Reasoning through Symbolic Verification",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Language Planner (baseline)",
            "name_full": "Language Planner (Huang et al., 2022) baseline",
            "brief_description": "A baseline retrieval-style method that finds the most similar training example (task) in R and uses it as an in-context prompt to the planning LM; included as a baseline to compare exemplar retrieval strategies.",
            "citation_title": "Language models as zero-shot planners",
            "mention_or_use": "use",
            "model_name": "Used with GPT-2 planning LM in experiments",
            "model_description": "Retrieval augmented prompting strategy that locates the most similar historical example and supplies it as the prompt for the LM to imitate; no symbolic projection/verification step.",
            "model_size": "Depends on planning LM (in experiments GPT-2 variants were used).",
            "architecture_type": "Retriever + Transformer planner (no KB projection step)",
            "training_data": "Uses the training set examples (R) to retrieve prompts; no additional training reported.",
            "reasoning_method": "Example-retrieval then generate proof steps in natural language (single-pass imitation of retrieved example's structure).",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "CLUTRR-LP; Countries-LP (used as a baseline comparison)",
            "benchmark_description": "Same synthetic relational reasoning benchmarks; Planner retrieves similar exemplar tasks from R to form prompts.",
            "task_type": "Relational multi-hop reasoning (treated via retrieval of exemplar proofs).",
            "performance_metric": "Human-evaluated proof plausibility / success rate",
            "performance_value": "Reported to perform worse than LMLP and often worse than simpler baselines; specific examples in the paper show Planner failing when it retrieves examples with similar entities but different relations (numerical averages for Planner are lower than LMLP across reported tables; see paper Table 2/3 for exact values).",
            "comparison_with_baseline": "Language Planner baseline underperforms LMLP; authors hypothesize Planner's retrieval of examples with same entities (rather than same relation) causes inappropriate guidance for relational reasoning.",
            "key_findings": "Retrieving the single most similar training example by task similarity (entity overlap) is not sufficient for correct relational reasoning; example selection must match the relation-pattern not merely entities.",
            "limitations": "Retriever can pick examples with entity overlap but different relations, leading to incorrect proofs; lacks symbolic projection/verification to the KB so hallucinations are unchecked.",
            "uuid": "e6788.2",
            "source_info": {
                "paper_title": "Evaluating Step-by-Step Reasoning through Symbolic Verification",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models",
            "rating": 2,
            "sanitized_title": "leasttomost_prompting_enables_complex_reasoning_in_large_language_models"
        },
        {
            "paper_title": "CLUTRR: A diagnostic benchmark for inductive reasoning from text",
            "rating": 2,
            "sanitized_title": "clutrr_a_diagnostic_benchmark_for_inductive_reasoning_from_text"
        },
        {
            "paper_title": "On approximate reasoning capabilities of low-rank vector spaces",
            "rating": 1,
            "sanitized_title": "on_approximate_reasoning_capabilities_of_lowrank_vector_spaces"
        },
        {
            "paper_title": "End-to-end differentiable proving",
            "rating": 2,
            "sanitized_title": "endtoend_differentiable_proving"
        },
        {
            "paper_title": "Language models as zero-shot planners",
            "rating": 2,
            "sanitized_title": "language_models_as_zeroshot_planners"
        }
    ],
    "cost": 0.015170499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating Step-by-Step Reasoning through Symbolic Verification
28 Mar 2024</p>
<p>Yi-Fan Zhang 
Hanlin Zhang 
Carnegie Mellon University
3 AWS AIAmazon</p>
<p>Erran Li 
Li 
Eric Xing 
Carnegie Mellon University
3 AWS AIAmazon</p>
<p>Petuum Inc</p>
<p>Mbzuai 
Evaluating Step-by-Step Reasoning through Symbolic Verification
28 Mar 2024F1FC58EA98128CAB76D1D1F009B8F2F9arXiv:2212.08686v2[cs.CL]
Pre-trained language models (LMs) have shown remarkable reasoning performance using explanations or chain-of-thoughts (CoT)) for in-context learning.On the other hand, these reasoning tasks are usually presumed to be more approachable for symbolic programming.To understand the mechanism of reasoning of LMs, we curate synthetic datasets containing equivalent (natural, symbolic) data pairs, where symbolic examples contain firstorder logic rules and predicates from nonparametric knowledge bases (KBs), supporting automated verification of intermediate reasoning results.Then we revisit neuro-symbolic approaches and propose to learn from demonstrations containing logic rules and corresponding examples to iteratively reason over KBs, recovering Prolog's backward chaining algorithm and supporting automated verification of LMs' outputs.Comprehensive experiments are included to systematically compare LMLP with CoT in deductive reasoning settings, showing that LMLP enjoys more than 25% higher accuracy than CoT on length generalization benchmarks even with smaller model sizes.</p>
<p>Introduction</p>
<p>There are emerging interests in leveraging LMs to enable planning (Li et al., 2022;Huang et al., 2022), heuristic search (Dahlgren et al., 2021) and symbolic inference (Wei et al., 2022b;Zelikman et al., 2022;Zhang et al., 2022).Among them, chain of thought prompting or scratchpads (Wei et al., 2022b;Nye et al., 2021) shows that taking (input, explanation, output) as in-context examples for LMs can lead to significant performance gain in reasoning tasks.However, like many fine-tuning approaches, it can be difficult for these models to generalize compositionally (Zhou et al., 2022a), meaning they may struggle to apply their knowledge to solve new problems that * Work done outside of Amazon.involve novel combinations of information (Lake and Baroni, 2018;Bahdanau et al., 2018;Keysers et al., 2019).One notable case is that LMs would suffer from catastrophic performance degradation when tested on sequences longer than training ones (Figure 1).As a solution, least-to-most prompting (Zhou et al., 2022a) takes inspiration from symbolic programs and proposes to tackle the challenge by modularizing the prompt on the reduced problem.The divide-and-conquer strategy is useful to improve the reasoning ability of language models, but it also presents additional challenges: what are the appropriate representations for factual knowledge and in-context samples that can ensure the correctness of each individual reasoning step?How do natural language explanations compare to symbolic provenance, which is easily verifiable, when used as prompts for reasoning?</p>
<p>Our goal is to evaluate the natural and symbolic paradigms closely in order to answer these questions.To enable fine-grained comparison and gain insight into in-context learning for reasoning tasks, we study relational reasoning over both natural language and knowledge bases (KBs).KBs are particularly useful for this purpose because they are constructed using clear pipelines and strong su-pervision, which makes them reliable and easy to control.This allows us to verify and evaluate reasoning paths and provenances without the need for human-provided rationales or explanations (Camburu et al., 2018;Zhou et al., 2020;Wei et al., 2022b;Nye et al., 2021;Zelikman et al., 2022).We study language models as logic programmers (LMLP) to enable few-shot learning from symbolic demonstrations and simultaneous planning in an explainable and scalable way.LMLP uses logic rule templates, examples, and pre-trained knowledge to iteratively perform in-context learning and answer relational queries.</p>
<p>Specifically, given a goal query as the in-context example that can be interpreted as a question answering (QA) task, LMLP searches or retrieves a related task example with a corresponding logic rule (Figure 2).Then the context and task description are concatenated as the input prompt for an autoregressive planning LM.At each step of generation, we use a masked translation LM to compare the similarity between the generated natural language sentences and encoded it into (subject, relation, object) predicates in the KB.In this way, each generated sentence is transformed into the most similar predicate and the reasoning path is confined within the KB.The process is iterated until a predefined maximum iteration or the target of interest is reached (Figure 2) and the generated reasoning path is evaluated manually.</p>
<p>To evaluate the reasoning capability of CoT and LMLP, we curate two datasets and design a series of experiments, aiming to compare two recent in-context learning paradigms and explore both symbolic and naturalistic scenarios.Specifically, we adopt synthetic datasets containing (natural, symbolic) data pairs.The symbolic part contains predicates and first-order logic (FOL) rules, which are well-suited for investigating the role of symbolic representations for few-shot reasoning.The natural part of our study includes a story written in natural language that describes a set of entities and relations, as well as the reasoning paths that connect them.These reasoning paths can be seen as explanations for the relationships and events described in the story.Moreover, we create experimental settings that are unfavorable for LMLP since (i) we use GPT-2 and Sentence-BERT as its backbones, which is known to be of much smaller scale compared to CoT which is usu-  ally based on GPT-3 (Brown et al., 2020) or PaLM (Chowdhery et al., 2022a); (ii) LMs are pre-trained over natural language sentences as opposed to KBs, which creates substantial gaps in semantics and representations, thus posing a grounding challenge where LMs are known to be ineffective (Bisk et al., 2020).</p>
<p>Controlled experiments on relational reasoning have shown that (i) CoT prompting struggles to solve the compositionality challenge (Sinha et al., 2019), while with explicit verification, LMLP can work more reliably as reasoning length increases by taking symbolic inputs that explicitly separate logic and control (Kowalski, 1979).(ii) While it is commonly believed that large pre-trained language models (LMs) are not grounded in contexts that require rich experiences, experimental results suggest that in-context learning, which maps the conceptual structure of a space learned from text onto a new structured space, is sufficient to solve some challenging reasoning tasks over knowledge bases (KBs).(iii) LMs struggle to effectively solve relational reasoning tasks without proper demonstrations containing the target relation and correct input-label mappings.This is supported by evidence in in-context examples, which are poorly understood and have many intricate design choices (Zhao et al., 2021;Liu et al., 2021;Min et al., 2022).</p>
<p>Related Works</p>
<p>In-context learning concerns feeding input texts describing a task with some examples to the black-box model for learning the task (Brown et al., 2020).Many works show that there are intricate design choices like prompt formats (Jiang et al., 2020;Liu et al., 2021;Zhao et al., 2021;Min et al., 2022), example choices and their ordering (Zhao et al., 2021;Lu et al., 2021b), pretraining data distribution (Xie et al., 2021;Shin et al., 2022;Chan et al., 2022) and model architectures (Chan et al., 2022) to improve the LMs' powerful and versatile in-context learning ability.Recent work focuses on bootstrapping LM with natural language explanations, intermediate steps, or rationales for reasoning (Camburu et al., 2018;Zhou et al., 2020;Nye et al., 2021;Wei et al., 2022b;Nye et al., 2021;Zelikman et al., 2022).Recent works showcase both some positive (Clark et al., 2021) and negative results (Kassner et al., 2020;Helwe et al., 2021;Talmor et al., 2020) in adapting LMs for symbolic or logical reasoning.The length generalization challenge is echoed in a few recent works (Zhang et al., 2022;Anil et al., 2022;Liu et al., 2022;Zhou et al., 2022b;Press et al., 2022).Though there are some encouraging progress (Clark et al., 2021;Wei et al., 2022b;Chowdhery et al., 2022a;Zelikman et al., 2022), they require a significant amount of computation for re-training and human annotations about reasoning paths or explanations (Wei et al., 2022b;Nye et al., 2021).Moreover, their entangled nature with natural language makes them hard to make robust inferences over symbolic factual knowledge.However, our goal is fundamentally different from theirs in investigating the role of symbolic representations on few-shot reasoning using in-context learning.LMLP that bootstraps the reasoning process from the LMs in a few-shot manner (Figure 2) is in contrast to popular methods that need expensive human annotations and retraining (Camburu et al., 2018;Zhou et al., 2020;Wei et al., 2022b;Zelikman et al., 2022) or uncontrollable using only pre-trained knowledge (Kojima et al., 2022).Moreover, related works typically finetune the model using rationales or explanations (Camburu et al., 2018;Zhou et al., 2020) or focus on natural language based reasoning such as commonsense reasoning, arithmetic reasoning, open domain question answering (Wei et al., 2022b), concept grounding (Patel and Pavlick, 2021) etc. Synthetic ontology datasets are constructed in (Saparov and He, 2022) to understand the failure modes of CoT reasoning, but they are in natural language forms instead of investigating the reasoning done over interpretable symbolic structures as we do.Huang et al. (2022) uses a mechanism for constraining the LLM output to feasible action sequences, which we adopt in this work.LMLP can be conceptually understood as a realization of recency biases (Press et al., 2021), which has been shown effective in scratchpad-based reasoning (Liu et al., 2022).Therefore, all the above works are different from our goal of exploring the representations of prompts in-context learning.</p>
<p>Retrieval-augmented Generation.Our study is also related to retrieval-augmented generation (Lewis et al., 2020) like kNN-LM (Khandelwal et al., 2019), DPR (Karpukhin et al., 2020), RALM (Guu et al., 2020), and RETRO (Borgeaud et al., 2022), which integrates parametric models with non-parametric KBs to address key LM challenges like knowledge staleness (Roberts et al., 2020) and hallucination (Shuster et al., 2021), reasoning (Shao et al., 2023).We explore more controllable environments where the evaluation of intermediate reasoning can be automated, demonstrating that this verification process helps filter out incorrect reasoning paths.This, in turn, enhances reasoning performance by assessing how effectively language models can reason when instances of hallucination are minimized.</p>
<p>Methodology Overview</p>
<p>We consider the reasoning task with an SRL query as the question and some background knowledge as the context.</p>
<p>The relational information in the query and context can be expressed either using natural language or a (subject, relation, object)  predicate/triplet.There is a KB with facts F and (FOL) rules R to support the above QA.There are two equivalent ways for representing the problem, symbolic or natural language, which leads to the designs below.</p>
<p>Datasets construction.To ensure that the natural and symbolic data are equivalent, we keep the ground truth facts the same in natural language stories and knowledge bases.We construct natural language story datasets following the method described in (Sinha et al., 2019).As shown in Table 1, we seek to curate new symbolic datasets from the original ones into (i) A query subset containing predicates needed for proving.(ii) A set of facts F containing all the available facts/predicates, which composes a KB, and (iii) A set of rules R containing examples (A task and its proofs) extracted from the training subset using backward chaining based neuro-symbolic reasoners (Rocktäschel and Riedel, 2017).See appendix B.1 for more details.</p>
<p>Task.Given a query Task: Joseph's sister is Katherine, which consists of two entities Joseph, Katherine and a target relation sisiter .Our task is to find a proof path from Joseph to Katherine where the relationship sisiter can be correctly inferred.On a high level, we need to leverage an abstract logic rule and its grounded example:
Sister(A,C) ← Brother(A,B) ∧ Sister(B,C)
Sister(George, Nancy) ← Brother(George, Dale) ∧ Sister(Dale, Nancy) to derive the answer for the query Sister(Joseph, Katherine) (Figure 3(a)).</p>
<p>Language Models as Logic Programmers achieves this goal using in-context learning.At first, examples and logic rules r in R are selected.For example, in Figure 2, LMLP samples one logic rule and its grounded example, which is concatenated with the query q Task: Joseph's sister is Katherine as a prompt r ′ = [r, q].The prompt is fed into a Planning LM P θ , which is an autoregressive LM such as GPT-3 for proof generation.Multiple sentences x are generated using temperature sampling from P θ (r ′ ).However, these sentences are in free-form language and often not in the (subject, relation, object) predicate format.In LMLP, the generated output is converted to the most similar fact in KB F using the cosine similarity of the embedding from a Translation LM T ϕ , implemented as a sentence-specific Masked LM.Specifically, T ϕ embed the output sentence from P θ : T ϕ (x) and all predicates f from F: T ϕ (f ), calculating their cosine similarity.The most similar f to x is chosen as the conversion results f ′ .By translating the output space of P θ into an external KB this way, LMLP is expected to produce a more plausible provenance to explain the reasoning process of a final prediction.Given frozen P θ and T ϕ , we then repeatedly generate proofs by prompting P θ using r ′ = [r ′ , f ′ ], projecting the generated sentences to the KB by the T ϕ , attaching the output to the prompt (Figure 2).The model terminates when the predefined maximum number of iterations or the target entity of interest is reached.To improve coherency, we enforce the chain rule transition constraints: the tail entity of the previous predicate should be the same as the head entity of the next predicate for each output step.Specifically, during the translation phase, we only select the predicates satisfying the requirement to compare similarity with T ϕ (x).The faithfulness of the reasoning path is governed by post-hoc human evaluations.The overall algorithm is described in Algorithm 1 in Appendix B. Using the prompt supported by the KBs, we bootstrap the reasoning process from the LMs in a few-shot manner (Figure 2).</p>
<p>Chain-of-Thought prompting.CoT (Wei et al., 2022b) solves complicated multi-step reasoning tasks by providing explanations, which is also intuitive for our multi-hop SRL tasks since we can take intermediate reasoning paths as explanations.</p>
<p>Figure 3(b) shows an example of applying CoT to solve an SRL task from the CLUTRR dataset (Sinha et al., 2019): given an in-context sample in the form of (input, explanation, output) .LMs are expected to imitate the reasoning process of the given explanation to generalize to a new query.The explanation of each question is generated just the same as the rule set R, which is extracted from the training set using a neuro-symbolic reasoners and converted to natural language forms.Specifically, the in-context exemplar adapts LMs to another sample containing multiple relations and a query for the relation between two entities "What is the relation between Theodore and Frances?", CoT first generates a reasoning path from Frances to Theodore, namely "France's grandson is Charles, . . ., Chris's brother is Theodore.",and finally answers the query: "The relation of Frances between Theodore is grandson".With such a prompt, LMs are expected to generate both the reasoning paths and the resulting queried relation.For a fair comparison with LMLP, human judgments on the reasoning path are included to calculate the accuracy.Note that the explanation in CoT is extracted from the story in the question, which contains much clearer information than the logic rules for LMLP.</p>
<p>Experiments</p>
<p>We now describe the experimental setups, empirically evaluate LMLP and compare it with existing methods.See Appendix C for full details of data preprocessing and performance evaluation.</p>
<p>Settings.We curate two datasets for evaluating the in-context learning capability of LMs for reasoning: CLUTRR-LP and Countries-LP, which are based on CLUTRR (Sinha et al., 2019) and Countries (Bouchard et al., 2015) datasets respectively.CLUTRR (Sinha et al., 2019) contains a group of KBs, where each node denotes a family member and edges are family relations.The target of CLUTRR dataset is to infer a two-family members' relationship that is not explicitly mentioned.The training set of CLUTRR consists of graphs that the target relation can be inferred by traversing a limited number of edges while the relation in the test set needs more traversing steps for inference, which allows controlled studies on compositionality.Another intriguing property of CLUTRR is that there are ground truth one-to-one correspondances between KBs and natural language stories, which exactly suits our needs.Countries (Bouchard et al., 2015) concerns link prediction, where countries, regions, and sub-regions are entities and relations containing LocatedIn and NeighborOf.Countries has three tasks, R1,R2, and R3, each requiring reasoning skills of increasing complexity (Rocktäschel and Riedel, 2017).Implementation details.For LMLP, we implement the planning LM P θ as GPT-2 (Radford et al., 2019), the translation LM T ϕ as Sentence BERT (Sent-BERT) (Reimers and Gurevych, 2019) based on Hugging Face Transformers (Wolf et al., 2019).The default model for Translation LM is Sentence-RoBERTa-Large and for Planning LM is GPT2-Large (Radford et al., 2019) pretrained on large corpora by default.For CoT, we follow the original paper (Wei et al., 2022b) to sample in-context samples.</p>
<p>Since prompt formats lead to significant performance variations (Liu et al., 2021), we propose to explore two simple design choices for LMLP and find that they can further boost the reasoning capacity.(i) Multiple examples for prompting.Denote N the number of examples we used in one proof task.Table 9 shows two examples with N = 1 and N = 2 are supplied respectively.The intuition is that, getting more examples in the prompt can make LMs better recognize the proof task and thus produce more reliable reasoning paths.See the experimental section for empirical verification.(ii) Prompts Ensembling.Table 10 shows the results of different prompts for the same task.We can see the influence of prompts on the generated proof path.The first few proof steps are largely similar to the provided example.If the provided example supplies a wrong direction, the proof is likely to be wrong.To study and exploit the benefit brought by different prompts, during experiments, we propose to use K prompts alternatively for one task, where one task is marked to be successfully proved if any of these K prompts gets the right result.Namely, a larger K means that we have a higher probability of picking a good prompt.The default hyper-parameters N, K are set to one.</p>
<p>Evaluation metrics In Table 2 and Table 3, where LMLP is compared to various baselines, the correctness of the proven reasoning path is evaluated manually.For each reasoning path, we ask annotators to answer "Yes" or "No" to whether the generated proof path is plausible to human commonsense and the target relation can be induced from it.We include 5 participants to reduce randomness and observe that their answers are almost the same.Because of resource Limitations, for other simple ablation studies of LMLP, the metric is proven accuracy or success rate.For example, for query "Task: palau locatedIn oceania", we begin with entity "palau" and select facts from the F. If the chosen triplet ends with entity "oceania", the proven path is correct, e.g., "micronesia locatedIn oceania" in Table 1.For LMLP, if there is no chosen triplet ends with entity "oceania", the prediction is incorrect.</p>
<p>Comparisons of LMLP and CoT</p>
<p>The goal of this part is to systematically compare LMLP with CoT both quantitatively and qualitatively on SRL tasks to better understand the reasoning of LMs using in-context learning.</p>
<p>In Figure 1  can be that, although CoT decomposes complex multi-hop relation reasoning tasks into a multi-step reasoning process and then predict the final results, the proof path is all generated by LMs at once.The decomposition of LMLP to multi-hop reasoning tasks is more thorough, where the generation of a proof path is divided into multi-steps and each step will be projected into the KB, which is a much stronger inductive bias.Therefore, the decomposed tasks in each step are easier to solve and the knowledge in the KB can be well exploited.See appendix for results on Countries-LP.</p>
<p>Analysis of LMLP</p>
<p>Given the above observations that LMLP outperforms CoT by a large margin, we systematically analyze LMLP with extensive experiments below.</p>
<p>Ablation Studies on prompting strategies.As illustrated in Table 2, No Prompt means that we only feed the target directly and generate each step, prompts in the Only Rule baseline is one proof example with entities replaced by some symbols.We also compare LMLP to Language Planner (Huang et al., 2022), which first finds the most similar target in the R and uses such an example as the prompt.LMLP-reverse swaps the position of the abstract logic rule and its grounded example in the prompt of LMLP.For example, in Figure 2, the in-context prompt of LMLP-reverse will place Sister(George, Nancy) ← Brother(George, Dale) ∧ Sister(Dale, Nancy) before its abstract logic rule
Sister(A,C) ← Brother(A,B) ∧ Sister(B,C)
. Examples for all baselines are shown in Appendix Table 9.</p>
<p>Table 2 shows that directly applying Language Planner for relational reasoning does not work and using only facts or no prompt attain inferior performance.The possible reason for the inferior performance of Planner can be that it finds the example from R with the most similar task as the prompt, which usually retrieves rules with the same entities of the goal task.However, for reasoning tasks over KBs, relation contains much more information of the task than the entity.As shown in Table 9, for the task "Patricia's uncle is Donald", Planner finds the example with task "David's nephew is Don", whose following proofs do not make sense for the relation "uncle".LMLP in contrast finds an example whose task has the same relation as the goal predicate, which is more informative.</p>
<p>LMLP can be robust to large search space.We may wonder if the superior results of LMLP are an artifact for datasets with a small search space.To control the confounding, we progressively inject 5, 000 random noisy facts/predicates into the facts set F. With more noisy facts, at each decoding step, it will be more difficult for LMLP to choose the correct proof path as the search space is enlarged.shows the results when we vary the number of noisy facts, where the noisy rate is 0.5 means that we add 5000 * 0.5 random facts to the F during evaluation and noisy rate 0 means F only contains query-relevant facts.We see that enlarging the search space generally decreases the performance.However, even though when all the noisy facts are injected into F, i.e. more than 95% facts are noisy, the performance is still favorable (more than 38% success rate), showing that LMLP can produce robust reasoning performance.</p>
<p>Effects of model size.Figure 4(c) shows the impact of the size of the planning LM model: larger GPT models generally attain better performance; using GPT2-large and LlaMA2-7B (Touvron et al., 2023) can dramatically improve model performance, which aligns with the findings that reasoning performance can emerge in larger models (Wei et al., 2022a;Saparov and He, 2022).</p>
<p>Prompts ensembling boosts the reasoning capability.  of them can solve the task.We show the evaluation results on CLUTRR-LP in Table 6 and the proposed method can generate realistic and correct proof paths.A large K can further boost performance, which also verifies the importance of prompt ensembling: Table 5 shows the performance on Countries-LP where almost all the query samples can be proved correctly with a large K.</p>
<p>One interesting phenomenon is that LMLP can generate a much longer proof path even though the proof path length in the rule set R is less than 3.This manifests a potential improvement with respect to the significant weakness in systematic generalization of fine-tuning or re-training of LMs (Sinha et al., 2019).The R of CLUTRR-LP contains only examples whose proof paths are less than five.However, during testing, our model can produce proof paths much longer than five steps and  perform well on all query sets.Prompting using multiple examples boosts the reasoning capability.N denotes the number of in-context examples used in one proof task.Results show that a larger N can generally produce performance gains (Figure 4(a)).However, longer prompts require larger GPU memories, so there is a trade-off between memory and performance.</p>
<p>Analysis of Demonstrations of ICL</p>
<p>Besides results in Appendix Table 7, we conduct qualitative analysis of demonstrations of in-context learning.</p>
<p>Failure cases analysis of baselines.Since the generated sentences are closely related to the prompt, Table 11 in Appendix shows that if we randomly choose prompts, the generated proof path has relations similar to the prompt, but is wrong for the given task.For entity-based prompts, since the task has the same start entity as the in-context exemplar, the generated steps 1 in this setting are very similar, leading to many wrong proof paths.Language Planner, without chain rule constraint, the generated triplets are chaos, e.g., in Example 1, the generated proof does even not contain the subject "Jon" and thus exactly wrong.Although the proposed LMLP attains a high success rate, there are also some failure cases.As shown in Appendix Table 10, an appropriate prompt needs to be chosen for the right proof paths.</p>
<p>Takeaways.Similar to previous work (Liu et al., 2021;Min et al., 2022), we find that in-context learning performance varies greatly with choices of exemplars (Table 6).One of the key findings in (Min et al., 2022) is that even without any labeled data, LMs can achieve k-shot performance by simply prompting with demonstrations containing unlabeled inputs.Our findings are generally in-line is in line with the importance of input-label formats highlighted in the work.However, we show in Table 8 and 9 that the correct mapping of ruleexample pairs is important since giving only rules with symbols like X, Y, Z rather than concrete entities like China makes LMLP fail catastrophically.</p>
<p>Concluding Remarks</p>
<p>In this study, we systematically examine incontext learning of language models (LMs) from a symbolic reasoning perspective, demonstrating that LMs can be prompted with logical demonstrations to generate plausible explanations for reasoning tasks over knowledge bases (KBs).Our evaluation results show that constraining outputs of LMs and ensuring intermediate reasoning correctness are important for reasoning performance, providing new insights into in-context learning and a mechanism to reduce incorrect reasoning through symbolic verification.</p>
<p>Limitations</p>
<p>Like previous works, we study reasoning empirically without theoretical justifications and focus specifically on synthetic data.Therefore, our results serve as a proof of concept on investigating how ensuring and reducing hallucination can improve overall reasoning, and might not transfer to more complex reasoning tasks.Moreover, due to access and computation restrictions, we are not able to conduct experiments with the latest LMs like PaLM (Chowdhery et al., 2022b).</p>
<p>Appendix A Extended Related Work</p>
<p>Neuro-Symbolic Reasoning.ILP (Muggleton and De Raedt, 1994) and its neural version (Yang and Song, 2020) are unable to reason about disjoint relations in confront of missing links when KBs are noisy like in FreeBase, which means ILP only synthesizes rules based on existing relations.Methods like Neural-LP (Yang et al., 2017) and RNNLogic (Qu et al., 2020) require enumeration of all possible rules given a max rule length T. Thus the complexity of these models grows exponentially as maximum rule length increases, which is a significant disadvantage for systematicity problems.For deductive reasoning, NTP (Rocktäschel and Riedel, 2017) and its improved versions (Minervini et al., 2018(Minervini et al., , 2020) ) require hand-crafted templates to imitate backward chaining for deductive reasoning.This belies the considerable user burden of authoring the templates which then fundamentally biases the tool toward a specific subset of programs that the author has in mind.Moreover, the performance and efficiency of NTP is far from satisfactory: the performance usually lags far behind its neural counterparts like knowledge graph embedding methods (Lin et al., 2015); during both training and inference, NTPs need to compute all possible proof trees needed for proving a query, relying on the continuous unification of the query with all the rules and facts in the KB.The search space of existing works is exponentially large, which makes them hard to scale up in general (Minervini et al., 2018;Chaudhuri et al., 2021).</p>
<p>LMs for Theorem Proving.Most works focus on proving formal mathematical theorems: GPT-f (Polu and Sutskever, 2020) shows promising results by generative language modeling over mathematical formulas.Systematicity of LMs when training on proofs is evaluated in (Gontier et al., 2020) but shows negative results in generalizing to unseen proof steps in extrapolation and complex language.Three synthetic tasks inspired by three reasoning primitives of deduction, induction, and abduction are demonstrated in (Wu et al., 2021).The above works provide insights into understanding LMs' reasoning capabilities.Though they share similar problem structures like compositionality with ours, they fundamentally require large-scale pretraining and fine-tuning due to the mismatch between Wikipedia pre-training corpora and mathe-matical formulas.Such a re-training requirement not only results in computational inefficiency but lacking in compositional generalization to longer proof steps unseen during training (Gontier et al., 2020).</p>
<p>Symbolic Reasoning with LMs.Large LMs pre-trained on open-domain text corpora have achieved impressive advances in natural language generation and understanding tasks (Kenton and Toutanova, 2019;Brown et al., 2020).By selfsupervised imitation on human-generated texts, LMs contain rich factual knowledge (Petroni et al., 2019;Bouraoui et al., 2020;Roberts et al., 2020) and linguistic structures (Manning et al., 2020), serving as a versatile inference regime for various downstream tasks (Brown et al., 2020;Lu et al., 2021a).Among them, GPT-3 stands out by its fewshot generalization to unseen cases without further fine-tuning given in-context samples as demonstrations (Brown et al., 2020).Constraint decoding is shown to be effective in incorporating logical constraints into natural language generation (Lu et al., 2022).However, it is a common belief that LMs have not yet enjoyed a comparable success in tasks that require extensive planning and grounding (Glenberg and Kaschak, 2002;Bender and Koller, 2020;Bisk et al., 2020) as well as symbolic reasoning (Kassner et al., 2020;Helwe et al., 2021;Razeghi et al., 2022).</p>
<p>B Algorithm Description</p>
<p>Algorithm 1 describes the procedure or LMLP.It can also be illustrated in Figure 3(a).</p>
<p>B.1 Data Generation.</p>
<p>CLUTRR-LP.CLUTRR has 9 subsets with difference story length, named l 2 , l 3 , . . ., l 10 .Following (Minervini et al., 2020), we convert l 2 , l 3 , l 4 to the R and use l 5 , . . ., l 10 to the query sets.As illustrated in Table .1, data samples in CLUTRR consist of a story and a target, where the target contains two entities and the relation that is needed to be inferred, the story contains available triplets.Each sample in the l 2 , l 3 , l 4 will be converted to the format "Task: . . ., Step i: . . ." and added to the R. Note that all examples in the R have a story length of less than five, which enables us to test the systematic generalization ability of LMLP.For CLUTRR, the story triplets in the R are not useful for test target proving, because they are all from different relation graphs.For example, story triplets Algorithm 1 Generate proof path from Pre-Trained Language Models.</p>
<p>Require: Planning LM P θ , Translation LM T ϕ , Query set Q that contains all query triplets, F that contains all available facts, R that contains all the available logic rules or proof examples.for q = (s, p, o) ∈ Q do // s, p, o denote subject entity, predicate (relation) and object entity respectively.Find r ∈ R, whose task relation is p.Construct prompt r ′ = [r, q].// [r, q] means the concatenation of two strings.while Max step is not reached do Sample 10 sentences {x i } 10
i=1 from P θ (f ′ ). Set F ′ ∈ F whose first entity are s. if |F ′ | == 0 then
Break // No available facts in the F start with entity s. for x ∈ {x i } 10 i=1 do score i = max ∀r∈F ′ cosine(T ϕ (x), T ϕ (r));// Cosine similarities of s to facts in F ′ .idx = arg max ∀r∈F ′ cosine(T ϕ (x), T ϕ (r));// Select r ∈ F ′ with the highest similarity to x.
x ′ = F ′ [idx]
Choose the highest score rule x * as the next proof step and append it to the prompt
f ′ = [f ′ , x * ]. if o ′ == o then
Break // The object entity converges to the target entity o.</p>
<p>in the l 2 , l 3 , l 4 contain "(William's brother is Steve)" while one test story on l 5 contains "(William's uncle is Steve)".During the evaluation, if the model chooses "(William's brother is Steve)", the proof path will be wrong.However, the similarity of these two triplets is high, the model is then easy to make errors and these noisy facts increase proof difficulties.We hence evaluate our methods in two settings considering the number of noisy facts.</p>
<p>The simplest setting (Test Facts Setting) is that, when queries are from l i , i ∈ [5, . . ., 10], the F only contains facts in l i .In this case, the F 5∼10 have 251,222,275,279,285,304 facts respectively.The most difficult setting is termed All Facts Setting.We first extract facts in the F with length l 2 , l 3 , l 4 and get totally 5, 210 facts.When queries are from l i , i ∈ [5, . . ., 10], the F contains triplets in l i , l 2 , l 3 , l 4 , where the additional 5, 210 facts are not useful for the proof path and are noisy facts.</p>
<p>The All Facts Setting is set as our default setting and experimental results of the Test Facts Setting are mainly in the Appendix.For CoT, the F is needless and the construction of prompt examples is slightly different from the procedure above.Specifically, as shown in Figure 3(b), for each target in the training samples, we need to preserve the story and extract a proof path for the target.</p>
<p>Countries-LP.</p>
<p>Training samples in Countries are triplets that describe the neighbor of relation or located in relation of two regions/subregions/countries and can thus be directly used as F. Be-cause the three tasks (S1, S2, S3) (Minervini et al., 2020) have different training sets and thus have different F. Test samples in Countries are also triplets with specific entities and relations, hence the query set is just the test set of the original Countries dataset.One main difficulty in applying the proposed method to Countries is the lack of off-theshelf proof paths (R).The CTP (Minervini et al., 2020) model is trained and used for proving each triplet in the training set.CTP returns the scores of the possible proof path and the proof with the maximum score is iteratively searched as added into the R.After that, 924, 906, 705 available examples are found for S1, S2, S3 tasks respectively.</p>
<p>C Additional Experimental Setups and Results</p>
<p>Human evaluation results on the Countries-LP benchmark.Because Countries-LPis a simple reasoning benchmark, which has only two predicates, namely LocatedIN , and NeighborOf .Table. 3 shows that simple baselines with entity-based prompts or no prompt can attain comparable or higher results than LMLPon the simplest task S1.However, LMLPattains better reasoning capability on harder tasks.Besides, All baselines outperform COT by a large margin, which verifies the importance of generating a reasoning path step by step than proofing the query predicate at once.Step 3: C's daughter is A Task: James's granddaughter is Juanita</p>
<p>Step 1: James's daughter is David</p>
<p>Step 2: David's brother is Joshua</p>
<p>Step 3: Joshua's daughter is Juanita Task: James's niece is Mary</p>
<p>Step 1: James's daughter is Mary Task: B's granddaughter is D</p>
<p>Step 1: B's grandson is C</p>
<p>Step 2: C's brother is A</p>
<p>Step 3: A's sister is D Task: James's granddaughter is Andrea</p>
<p>Step 1: James's grandson is Thomas</p>
<p>Step 2: Thomas's brother is Don</p>
<p>Step 3: Don's sister is Andrea Task: James's nephew is Don</p>
<p>Step 1: James's grandson is Thomas</p>
<p>Step 2: Thomas's brother is Don</p>
<p>Language Planner</p>
<p>Task: James's father is Dale</p>
<p>Step 1: James's sister is Charlotte</p>
<p>Step 2:Charlotte's father is Dale Task: Jon's grandfather is Daniel</p>
<p>Step 1: James's grandmother is Mary</p>
<p>Step 2: Dorothy's mother is Mary</p>
<p>Step 3: Mary's husband is Daniel Task: Lynn's nephew is Michael</p>
<p>Step 1: Lynn's brother is Hugh</p>
<p>Step 2:Hugh's son is Johnson</p>
<p>Step 3:Johnson's brother is Michael Task: Bonnie's nephew is Michael</p>
<p>Step 1: Ross's brother is Michael LMLP (N=1)</p>
<p>Task: B's father is C</p>
<p>Step 1: B's son is D</p>
<p>Step 2: D's mother is A</p>
<p>Step 3: A's father is C Task: Hugh's father is James</p>
<p>Step 1: Hugh's son is Bobby</p>
<p>Step 2: Bobby's mother is David</p>
<p>Step 3: David's father is James Task: Irene's father is Milton</p>
<p>Step 1: Irene's grandfather is Jose</p>
<p>Step 2: Jose's mother is Mary</p>
<p>Step 3: Mary's husband is Milton Task: Task: D's nephew is A</p>
<p>Step 1: D's brother is B</p>
<p>Step 2: B's son is C</p>
<p>Step 3: C's brother is A Task: Francisco's nephew is Clarence</p>
<p>Step 1: Francisco's brother is Joshua</p>
<p>Step 2: Joshua's son is Joseph</p>
<p>Step 3: Joseph's brother is Clarence Task: Melanie's nephew is Charles</p>
<p>Step 1: Melanie's husband is William</p>
<p>Step 2: William's nephew is Kyle</p>
<p>Step 3: Kyle's father is Charles</p>
<p>Figure 1 :
1
Figure 1: Deductive reasoning performance (human evaluation accuracy) comparisons on the CLUTRR-LP given training data with story length 2, 3, 4.</p>
<p>PROMPT</p>
<p>Task: A's sister is C Step 1: A's brother is B Step 2: B's sister is C Task: George's sister is Nancy Step 1: George's brother is Dale Step 2: Dale's sister is Nancy Task: Joseph's sister is Katherine Step 1: Joseph's mother is Mary PROMPT Task: A's sister is C Step 1: A's brother is B Step 2: B's sister is C Task: George's sister is Nancy Step 1: George's brother is Dale Step 2: Dale's sister is Nancy Task: Joseph's sister is Katherine OUTPUT Joseph's mother is Mary OUTPUT Mary's daughter is Katherine</p>
<p>Figure 2 :
2
Figure 2: Illustration of a deductive reasoning example and iterative prompting of LMLP.LMLP retrieves a firstorder logic rule and an associated grounded example to answer the question.It stops when predefined maximum iterations or the target entity of interest is reached.The reasoning path explains the sister concept.</p>
<p>Figure 3: Schematic overview of (a) LMLP and (b) CoT.</p>
<p>Figure</p>
<p>Figure 4(b)  shows the results when we vary the number of noisy facts, where the noisy rate is 0.5 means that we add 5000 * 0.5 random facts to the F during evaluation and noisy rate 0 means F only contains query-relevant facts.We see that enlarging the search space generally decreases the performance.However, even though when all the noisy facts are injected into F, i.e. more than 95% facts are noisy, the performance is still favorable (more than 38% success rate), showing that LMLP can produce robust reasoning performance.Effects of model size.Figure4(c) shows the impact of the size of the planning LM model: larger GPT models generally attain better performance; using GPT2-large and LlaMA2-7B(Touvron et al., 2023) can dramatically improve model performance, which aligns with the findings that reasoning performance can emerge in larger models(Wei et al., 2022a;Saparov and He, 2022).Prompts ensembling boosts the reasoning capability.For each test example, we sample K in-context examples and count as correct if any one</p>
<p>Figure 4 :
4
Figure 4: (a) Effect of the number of templates for LMLP on CLUTRR-LP.(b) The effects of noisy facts for LMLP on CLUTRR-LP.Ablation on the scaling of (c) Planning LMs.</p>
<p>and Table.4, we compare LMLP to CoT and the reported performances are all human evaluation results.Qualitatively, CoT can get positive results on some query examples, for example, in Table 12, we showcase two examples where CoT can generate a correct proof path and predict the target relation at the same time.However, compared to LMLP, CoT achieves inferior results in all query sets with test reasoning length 5, 6, 7, 8, 9, 10 with different LLMs for text generation.In addition, as the reasoning length increases, the performance of CoT shows a clear downward trend.Table 12 shows two negative examples where the story contains sophisticated relations and the model cannot get the right reasoning path or just generate a wrong relation.In contrast, LMLP can consistently achieves a high human evaluation score (Table 2), which again verifies the systematic generalization capability of LMLP.Table 7 in the appendix shows examples with the same task but processed by the two methods respectively, where CoT cannot get deduce a right relation path from Margaret to Charles but LMLP can extract a simple yet right relation path.The reason why LMLP is better than CoT</p>
<p>Table 2 :
2
Numerical results and ablation on the length of test samples on CLUTRR-LP.
Test Story LengthBaseline Planner CoTAblation No Prompt Only Rule Random Entity-based LMLP-reverse LMLP Ours50.09730.1730.15140.16220.29190.20000.37300.329760.1810 0.13650.12380.15240.20950.14290.30480.247670.2258 0.10320.20000.21290.23230.17420.37420.258180.1037 0.15060.22220.20000.31110.23700.35560.355690.1048 0.09140.19350.21770.16130.18550.35480.2984100.12300.1230.28690.21310.39340.27050.52460.4754Average0.1393 0.12960.19630.19310.26660.20170.38120.3275TasksBaseline Planner CoTAblation No Prompt Only Rule Random Entity-based LMLP-reverse LMLP OursS10.7500 0.33330.85420.77080.60420.89580.83330.7917S20.7917 0.37500.66670.45830.67500.75000.83330.6250S30.7500 0.25000.72920.70830.64580.66670.75000.8333Average 0.7639 0.31940.75000.64580.64170.77080.80550.7500</p>
<p>Table 3 :
3
(Minervini et al., 2020) in various settings of Countries-LP.S1, S2, S3(Minervini et al., 2020)are three different tasks with different F (see the experimental setting for details).
Test Story LengthGPT-2 CoT LMLPMistral-7B-v0.1 CoT LMLPLLaMA2-7B CoT LMLP50.1730 0.3297 0.3083 0.5032 0.2721 0.482360.1365 0.2476 0.2762 0.5182 0.2543 0.487270.1032 0.2581 0.2314 0.4732 0.2364 0.471580.1506 0.3556 0.2247 0.5181 0.2102 0.532390.0914 0.2984 0.1143 0.4723 0.1345 0.4021100.1230 0.4754 0.1220 0.4741 0.1305 0.4992Average0.1296 0.3275 0.2128 0.4932 0.2063 0.4791Table 4: Numerical results considering different back-bone models.</p>
<p>For each test example, we sample K in-context examples and count as correct if any one
K=1K=3K=5K=10A Long ExampleTask: A locatedIn CS1 0.7083 0.9583 1.0000 1.0000Step 1: A neighborOf BStep 2: B locatedIn CTask: uruguay locatedIn south_americaS2 0.5000 0.8750 0.9583 1.0000Step 1: uruguay neighborOf argentinaStep 2: argentina locatedIn south_americaTask: sudan locatedIn africaStep 1: sudan neighborOf central african republicStep 2: central african republic neighborOf chadS3 0.7500 0.9167 0.9167 1.0000Step 3: chad neighborOf south sudan Step 4: south sudan neighborOf dr congoStep 5: dr congo neighborOf republic of the congoStep 6: republic of the congo locatedIn middle africaStep 7: middle africa locatedIn africa</p>
<p>Table 5 :
5
(Minervini et al., 2020)ries-LP.S1, S2, S3(Minervini et al., 2020)are three different tasks with different F (see the experimental setting for details).</p>
<p>Table 6 :
6
Ablation of LMLP on CLUTRR-LP.
Test Reasoning LengthK=1K=3K=5K=10Avg5 Hops0.3946 0.6865 0.7838 1.0000 0.71626 Hops0.5048 0.7143 0.7619 1.0000 0.74527 Hops0.4323 0.8065 0.8774 1.0000 0.77908 Hops0.5037 0.8000 0.8593 1.0000 0.79079 Hops0.3710 0.6452 0.7500 1.0000 0.691510 Hops0.5328 0.8279 0.8525 0.9180 0.7828</p>
<p>Wilhelmina took her uncle Hugh to the grocery store.Francisco and his brother Wesley were wrestling.Wilhelmina, Francisco's daughter, was cheering on the competition.What is the relation between Hugh and Wesley?Answer: Wesley's brother is Francisco, Francisco's daughter is Wilhelmina, Wilhelmina's uncle is Hugh.The relation of Hugh between Wesley is brother.Question: Constance went shoe shopping with her sister Ellen.Elsie had a daughter named Constance.Elsie had picked her daughter Margaret out the cutest new dress to wear on her birthday.Charles and his sister Kathleen have been best friends ever since childhood.Nadia and her father, James, went to the marina.James's daughter, Mabel, had purchased a boat, and they were eager to see it.Mabel bought her mother, Ellen, a puppy for her birthday.James hung his son Charles's finger paintings on the refrigerator.The paintings were right next to the paintings of Nadia, Charles's sister.Kathleen wasn't old enough to make any paintings for her father, James.What is the relation between Margaret and Charles?Answer: charles' mother is nancy, nancy's daughter is elizabeth, elizabeth's husband is john, john's wife is mary, mary's brother is george, david's nephews are william, william's nephews are robert.
CoT PromptingLMLPQuestion: Task: A's aunt is BStep 1: A's sister is CStep 2: C's mother is DStep 3: D's sister is BTask: Carrie's aunt is LynnStep 1: Carrie's sister is NatashaStep 2: Natasha's mother is CindyStep 3: Cindy's sister is LynnTask: Charles's aunt is MargaretStep 1: Charles's sister is NadiaStep 2: Nadia's aunt is Margaret</p>
<p>Table 7 :
7
Qualitative comparison of CoT and LMLP over the same example on CLUTRR-LP.
MethodPromptOutputFacts w/o Provenance Task: Kazakhstan locatedIn AsiaStep 1: Kazakhstan locatedIn CentralAsia Step 2: CentralAsia locatedIn AsiaTask: Kazakhstan locatedIn CentralAsiaStep 1: Kazakhstan neighborOf ChinaFacts w/ ProvenanceStep 1: Kazakhstan neighborOf Uzbekistan Step 2: Uzbekistan locatedIn CentralAsiaStep 2: China neighborOf India Step 3: India locatedIn SouthernAsiaTask: Kazakhstan locatedIn AsiaStep 4: SouthernAsia locatedIn AsiaStep 1: Kazakhstan neighborOf kyrgyzstanTask: X is located in ZStep 2: Kyrgyzstan neighborOf UzbekistanStep 1: X is located in YStep 3: Uzbekistan neighborOf AfghanistanStep 2: Y is located in MStep 4: Afghanistan neighborOf chinaStep 3: M is located in ZStep 5: China neighborOf IndiaTask: Kazakhstan locatedIn AsiaStep 6: India neighborOf NepalStep 7: Nepal locatedIn AsiaTask: X locatedIn YStep 1: X neighborOf M Step 2: M neighborOf N Step 3: N locatedIn YStep 1: Kazakhstan locatedIn CentralAsia Step 2: CentralAsia locatedIn AsiaLMLP (N=1)Task: Kazakhstan locatedIn Asia Task: X locatedIn YStep 1: X neighborOf M Step 2: M locatedIn N Step 3: N locatedIn YStep 1: Kazakhstan locatedIn CentralAsia Step 2: CentralAsia locatedIn AsiaTask: Kazakhstan locatedIn AsiaStep 1: Kazakhstan neighborOf kyrgyzstanStep 2: Kyrgyzstan neighborOf UzbekistanTask: X locatedIn YStep 3: Uzbekistan neighborOf AfghanistanStep 1: X neighborOf ZStep 4: Afghanistan neighborOf TajikistanStep 2: Z locatedIn YStep 5: Tajikistan neighborOf ChinaTask: Kazakhstan locatedIn AsiaStep 6: China neighborOf IndiaStep 7: India neighborOf NepalStep 8: Nepal locatedIn Asia</p>
<p>Table 8 :
8
(Rocktäschel and Riedel, 2017) formats using LMLP in the Countries dataset(Rocktäschel and Riedel, 2017).
MethodPromptOutputNo PromptTask: Richard's father is DonaldStep 1: Richard's grandmother is Elizabeth Step 2: Elizabeth's son is DonaldTask: C's father is BStep 1: C's brother is ARandom PromptStep 2: A's father is B Task: Janet's father is George Step 1: Janet's brother is JohnStep 1: Pat's uncle is Samuel Step 2: Samuel's father is JoseStep 2: John's father is GeorgeTask: Pat's grandfather is JoseTask: B's father is CRule OnlyStep 1: B's mother is D Step 2: D's son is A Step 3: A's father is CStep 1: Richard's grandmother is Elizabeth Step 2: Elizabeth's son is DonaldTask: Richard's father is DonaldTask: C's brother is AStep 1: C's brother is BEntity-based PromptStep 2: B's brother is A Task: Milton's brother is Glen Step 1: Milton's brother is MichaelStep 1: Milton's daughter is Margaretta Step 2: Margaretta's grandmother is AntoniaStep 2: Michael's brother is GlenTask: Milton's mother is AntoniaTask: David's nephew is DonStep 1: David's mother is FrancesLanguage PlannerStep 2: Frances's daughter is LynnStep 1: Richard's father is DonaldStep 3: Lynn's son is DonTask: Patricia's uncle is DonaldTask: B's father is CStep 1: B's mother is DStep 2: D's daughter is ALMLP (N=1)Step 3: A's father is C Task: Bobby's father is Hugh Step 1: Bobby's mother is DavidStep 1: Richard's mother is Maria Step 2: Maria's husband is DonaldStep 2: David's daughter is MarieStep 3: Marie's father is HughTask: Richard's father is DonaldTask: E's father is BStep 1: E's daughter is AStep 2: A's sister is DStep 3: D's brother is CStep 4: C's grandfather is BTask: Louis's father is HaroldStep 1: Louis's daughter is JenniferStep 2: Jennifer's sister is NettieLMLP (N=2)Step 3: Nettie's brother is Samuel Step 4: Samuel's grandfather is Harold Task: D's father is CStep 1: Richard's grandmother is Elizabeth Step 2: Elizabeth's son is DonaldStep 1: D's husband is AStep 2: A's son is BStep 3: B's grandfather is CTask: Lena's father is JamesStep 1: Lena's husband is WesleyStep 2: Wesley's son is TheodoreStep 3: Theodore's grandfather is JamesTask: Richard's father is Donald</p>
<p>Table 9 :
9
Failure cases analysis -the results of different prompts for the same task.
WrongRightTask: A's nephew is ETask: C's nephew is A Step 1: C's son is D Step 2: D's aunt is B Step 3: B's son is ATask: A's nephew is B Step 1: A's brother is C Step 2: C's son is BTask: C's nephew is A Step 1: C's son is D Step 3: B's son is A Step 2: D's aunt is BStep 1: A's son is D Step 2: D's grandmother is B Step 3: B's son is C Step 4: C's son is ETask: Lena's nephew is Richard Step 1: Lena's son is Theodore Step 2: Theodore's aunt is Cindy Step 3: Cindy's son is RichardTask: Theresa's nephew is John Step 1: Theresa's brother is George Step 2: George's son is John Task: Donald's nephew is GilbertTask: Lena's nephew is Richard Step 1: Lena's son is Theodore Step 2: Theodore's aunt is Cindy Step 3: Cindy's son is RichardTask: Antonio's nephew is Allan Step 1: Antonio's son is Bobby Step 2: Bobby's grandmother is Shantel Step 3: Shantel's son is Robert Step 4: Robert's son is AllanTask: Donald's nephew is Gilbert Step 1: Donald's son is Edgar Step 2: Edgar's mother is Marge Step 3: Marge's son is JohnStep 1: Donald's brother is James Step 2: James's grandson is Johnson Step 3: Johnson's father is Hugh Step 4: Hugh's brother is WesleyTask: Donald's nephew is Gilbert Step 1: Donald's son is Edgar Step 2: Edgar's mother is MargeTask: Donald's nephew is Gilbert Step 1: Donald's son is Jeremy Step 2: Jeremy's grandfather is Jose Step 3: Jose's son is SamuelStep 4: Samuel's son is Gilbert</p>
<p>Table 10 :
10
The comparison of different demonstrations on results.
MethodExample 1Example 2Task: D's mother is BStep 1: D's father is EStep 2: E's daughter is CTask: A's grandson is BStep 3: C's brother is AStep 1: A's granddaughter is CStep 4: A's mother is BStep 2:C's brother is BTask: Allan's mother is MarieTask: Clarence's grandson is JamesRandom PromptStep 1: Allan's father is RobertStep 1: Clarence's granddaughter is CharlotteStep 2: Robert's daughter is MichelleStep 2: Charlotte's brother is JamesStep 3: Michelle's brother is RonaldTask: Samuel's nephew is CharlesStep 4: Ronald's mother is MarieStep 1: Samuel's aunt is MarieTask: Jose's granddaughter is MargarettaStep 2: Marie's grandfather is CharlesStep 1: Jose's father is WilliamStep 2: William's niece is MargarettaTask: B's granddaughter is AStep 1: B's daughter is DStep 2: D's brother is CEntity-based Prompt</p>
<p>Table 11 :
11
Failure cases analysis of different baselines.Examples for each method are all from the same setting.</p>
<p>RightQuestion: Nettie's brother Paul took her to the fair when she was little.Paul also brought his brother Samuel.Question: Constance went shoe shopping with her sister Ellen.Elsie had a daughter named Constance.Elsie had picked her daughter Margaret out the cutest new dress to wear on her birthday.Charles and his sister Kathleen have been best friends ever since childhood.Nadia and her father, James, went to the marina.James's daughter, Mabel, had purchased a boat, and they were eager to see it.Mabel bought her mother, Ellen, a puppy for her birthday.James hung his son Charles's finger paintings on the refrigerator.The paintings were right next to the paintings of Nadia, Charles's sister.Kathleen was n't old enough to make any paintings for her father, James.What is the relation between Margaret and Charles?Answer: charles' mother is nancy, nancy's daughter is elizabeth, elizabeth's husband is john, john's wife is mary, mary's brother is george, george's nephews are david, david's nephews are william, william's nephews are robert.Question: Nicholas bought his brother Wayne a present.It was to congratulate him on becoming a father to his new son, Lorraine.What is the relation between Nicholas and Lorraine?Answer: Lorraine's father is Wayne, Wayne's brother is Nicholas.The relation of Nicholas between Lorraine is uncle.Question: William wanted to have a family cookout so he invited his brother James.James wanted to invite his other brother Cesar.Darryl's mother, Patrice, waited impatiently for him at the diner.Dan played basketball with his brother Eric.Darryl took his brother Eric to the baseball game with Nora's father Cesar.Cesar took his son Eric to go get nachos during the game.Patrice fixed her husband Cesar dinner and then they watched a movie they rented.Cesar rushed to the hospital to find out that his wife and already given birth to a boy and had named him Dan.Cesar was so excited to surprise his son, Eric, with the tickets to the playoffs.What is the relation between William and Nora?Answer: nora has two brothers, one of them being william.william is the only one who knows about nora's pregnancy.he is also the only one who knew about nora's pregnancy.
Exploring length generalization in large language models. Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, Behnam Neyshabur, 2022NeurIPS</p>
<p>Systematic generalization: What is required and can it be learned. Dzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, 2018Thien Huu Nguyen, Harm de Vries, and Aaron Courville. In ICLR</p>
<p>Climbing towards nlu: On meaning, form, and understanding in the age of data. M Emily, Alexander Bender, Koller, ACL. 2020</p>
<p>Aleksandr Nisnevich, et al. 2020. Experience grounds language. Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, EMNLP. </p>
<p>Improving language models by retrieving from trillions of tokens. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, International conference on machine learning. PMLR2022</p>
<p>On approximate reasoning capabilities of low-rank vector spaces. Guillaume Bouchard, Sameer Singh, Theo Trouillon, AAAI. 2015</p>
<p>Inducing relational knowledge from bert. Zied Bouraoui, Jose Camacho-Collados, Steven Schockaert, AAAI. 2020</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, 2020NeurIPS</p>
<p>e-snli: Natural language inference with natural language explanations. Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, Phil Blunsom, 2018NeurIPS</p>
<p>Data distributional properties drive emergent fewshot learning in transformers. C Y Stephanie, Adam Chan, Santoro, Jane X Andrew K Lampinen, Aaditya Wang, Pierre H Singh, Jay Richemond, Felix Mcclelland, Hill, arXiv:2205.050552022arXiv preprint</p>
<p>. Swarat Chaudhuri, Kevin Ellis, Oleksandr Polozov, Rishabh Singh, Armando Solar-Lezama, Yisong Yue, 2021Neurosymbolic Programming. Now Publishers</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, arXiv:2204.02311Sebastian Gehrmann, et al. 2022a. Palm: Scaling language modeling with pathways. arXiv preprint</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, arXiv:2204.02311Sebastian Gehrmann, et al. 2022b. Palm: Scaling language modeling with pathways. arXiv preprint</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, IJCAI. 2021</p>
<p>Perception, memory, and inference: The trinity of machine learning. Adam Dahlgren, Johanna Björklund, Frank Drewes, Is Neuro-Symbolic SOTA still a myth for Natural Language Inference? The first workshop. 2021</p>
<p>Grounding language in action. M Arthur, Michael P Glenberg, Kaschak, Psychonomic bulletin &amp; review. 2002</p>
<p>Measuring systematic generalization in neural proof generation with transformers. Nicolas Gontier, Koustuv Sinha, Siva Reddy, Chris Pal, 2020NeurIPS</p>
<p>Retrieval augmented language model pre-training. Kelvin Guu, Kenton Lee, Zora Tung, International conference on machine learning. PMLR2020Panupong Pasupat, and Mingwei Chang</p>
<p>Reasoning with transformer-based models: Deep learning, but shallow reasoning. Chadi Helwe, Chloé Clavel, Fabian M Suchanek, AKBC. 2021</p>
<p>Language models as zero-shot planners. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, arXiv:2201.07207Extracting actionable knowledge for embodied agents. 2022arXiv preprint</p>
<p>Zhengbao Jiang, Frank F Xu, How can we know what language models know? TACL. Jun Araki, and Graham Neubig. 2020</p>
<p>Dense passage retrieval for open-domain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-Tau Yih, arXiv:2004.049062020arXiv preprint</p>
<p>Are pretrained language models symbolic reasoners over knowledge?. Nora Kassner, Benno Krojer, Hinrich Schütze, arXiv:2006.104132020arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. NAACL-HLT. Jacob Devlin, Ming-Wei Chang, Kenton , Lee Kristina, Toutanova , 2019</p>
<p>Measuring compositional generalization: A comprehensive method on realistic data. Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, ICLR. 2019</p>
<p>Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis, arXiv:1911.00172Generalization through memorization: Nearest neighbor language models. 2019arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, 2022</p>
<p>Algorithm= logic+ control. Robert Kowalski, Communications of the ACM. 1979</p>
<p>Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. Brenden Lake, Marco Baroni, ICML. 2018</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Pre-trained language models for interactive decision-making. Shuang Li, Xavier Puig, Yilun Du, Clinton Wang, Ekin Akyurek, Antonio Torralba, Jacob Andreas, Igor Mordatch, arXiv:2202.017712022arXiv preprint</p>
<p>Learning entity and relation embeddings for knowledge graph completion. Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, Xuan Zhu, AAAI. 2015</p>
<p>Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, Cyril Zhang, arXiv:2210.10749Transformers learn shortcuts to automata. 2022arXiv preprint</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, arXiv:2101.06804What makes good in-context examples for gpt-3?. 2021arXiv preprint</p>
<p>Neurologic a* esque decoding: Constrained text generation with lookahead heuristics. Kevin Lu, Aditya Grover, Pieter Abbeel, Igor Mordatch ; Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Le Ronan, Lianhui Bras, Youngjae Qin, Rowan Yu, Zellers, arXiv:2103.05247Pretrained transformers as universal computation engines. NAACL2021a. 2022arXiv preprint</p>
<p>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, arXiv:2104.087862021barXiv preprint</p>
<p>Emergent linguistic structure in artificial neural networks trained by self-supervision. Kevin Christopher D Manning, John Clark, Urvashi Hewitt, Omer Khandelwal, Levy, 2020PNAS</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, arXiv:2202.12837Rethinking the role of demonstrations: What makes in-context learning work?. 2022arXiv preprint</p>
<p>Pasquale Minervini, Matko Bosnjak, Tim Rocktäschel, Sebastian Riedel, arXiv:1807.08204Towards neural theorem proving at scale. 2018arXiv preprint</p>
<p>Learning reasoning strategies in end-to-end differentiable proving. Pasquale Minervini, Sebastian Riedel, Pontus Stenetorp, Edward Grefenstette, Tim Rocktäschel, ICML. 2020</p>
<p>Inductive logic programming: Theory and methods. Stephen Muggleton, Luc De, Raedt , The Journal of Logic Programming. 1994</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, arXiv:2112.00114Show your work: Scratchpads for intermediate computation with language models. 2021arXiv preprint</p>
<p>Mapping language models to grounded conceptual spaces. Roma Patel, Ellie Pavlick, 2021In ICLR</p>
<p>Language models as knowledge bases. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, EMNLP-IJCNLP. 2019</p>
<p>Generative language modeling for automated theorem proving. Stanislas Polu, Ilya Sutskever, arXiv:2009.033932020arXiv preprint</p>
<p>Train short, test long: Attention with linear biases enables input length extrapolation. Noah A Smith, Mike Lewis, arXiv:2108.124092021Ofir PressarXiv preprint</p>
<p>Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, Mike Lewis, arXiv:2210.03350Measuring and narrowing the compositionality gap in language models. 2022arXiv preprint</p>
<p>Rnnlogic: Learning logic rules for reasoning on knowledge graphs. Meng Qu, Junkun Chen, Louis-Pascal Xhonneux, Yoshua Bengio, Jian Tang, ICLR. 2020</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019OpenAI blog</p>
<p>Yasaman Razeghi, Robert L Logan, I V , Matt Gardner, Sameer Singh, arXiv:2202.07206Impact of pretraining term frequencies on few-shot reasoning. 2022arXiv preprint</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. Nils Reimers, Iryna Gurevych, EMNLP-IJCNLP. 2019</p>
<p>How much knowledge can you pack into the parameters of a language model. Adam Roberts, Colin Raffel, Noam Shazeer, EMNLP. 2020</p>
<p>End-toend differentiable proving. Tim Rocktäschel, Sebastian Riedel, 2017NeurIPS</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, arXiv:2210.012402022arXiv preprint</p>
<p>Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu Chen, arXiv:2305.152942023arXiv preprint</p>
<p>Seongjin Shin, Sang-Woo Lee, Hwijeen Ahn, Sungdong Kim, Hyoungseok Kim, Boseop Kim, Kyunghyun Cho, Gichang Lee, Woomyoung Park, Jung-Woo Ha, arXiv:2204.13509On the effect of pretraining corpora on in-context learning by a large-scale language model. 2022arXiv preprint</p>
<p>Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, Jason Weston, arXiv:2104.07567Retrieval augmentation reduces hallucination in conversation. 2021arXiv preprint</p>
<p>Clutrr: A diagnostic benchmark for inductive reasoning from text. Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, William L Hamilton, 2019EMNLP</p>
<p>Alon Talmor, Yanai Elazar, Yoav Goldberg, Jonathan Berant, olmpics-on what language model pre-training captures. TACL2020</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Emergent abilities of large language models. TMLR2022a</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903Chain of thought prompting elicits reasoning in large language models. 2022barXiv preprint</p>
<p>Huggingface's transformers: State-ofthe-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, arXiv:1910.037712019arXiv preprint</p>
<p>Lime: Learning inductive bias for primitives of mathematical reasoning. Yuhuai Wu, Markus N Rabe, Wenda Li, Jimmy Ba, Roger B Grosse, Christian Szegedy, ICML. 2021</p>
<p>An explanation of in-context learning as implicit bayesian inference. Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma, 2021ICLR</p>
<p>Differentiable learning of logical rules for knowledge base reasoning. Fan Yang, Zhilin Yang, William W Cohen, 2017NeurIPS</p>
<p>Learn to explain efficiently via neural logic inductive learning. Yuan Yang, Le Song, 2020In ICLR</p>
<p>Eric Zelikman, Yuhuai Wu, Noah D Goodman, arXiv:2203.14465Star: Bootstrapping reasoning with reasoning. 2022arXiv preprint</p>
<p>Improved logical reasoning of language models via differentiable symbolic programming. Hanlin Zhang, Ziyang Li, Jiani Huang, First Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward at ICML 2022. 2022Mayur Naik, and Eric Xing</p>
<p>Calibrate before use: Improving few-shot performance of language models. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, ICML. 2021</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, arXiv:2205.10625Quoc Le, and Ed Chi. 2022a. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint</p>
<p>Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, Hanie Sedghi, arXiv:2211.09066Teaching algorithmic reasoning via incontext learning. 2022barXiv preprint</p>
<p>Towards interpretable natural language understanding with explanations as latent variables. Wangchunshu Zhou, Jinyi Hu, Hanlin Zhang, Xiaodan Liang, Maosong Sun, Chenyan Xiong, Jian Tang, 2020NeurIPS</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8820 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8820</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8820</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-273346560</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.10743v2.pdf" target="_blank">From Anchors to Answers: A Novel Node Tokenizer for Integrating Graph Structure into Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Enabling large language models (LLMs) to effectively process and reason with graph-structured data remains a significant challenge despite their remarkable success in natural language tasks. Current approaches either convert graph structures into verbose textual descriptions, consuming substantial computational resources, or employ complex graph neural networks as tokenizers, which introduce significant training overhead. To bridge this gap, we present NT-LLM, a novel framework with an anchor-based positional encoding scheme for graph representation. Our approach strategically selects reference nodes as anchors and encodes each node's position relative to these anchors, capturing essential topological information without the computational burden of existing methods. Notably, we identify and address a fundamental issue: the inherent misalignment between discrete hop-based distances in graphs and continuous distances in embedding spaces. By implementing a rank-preserving objective for positional encoding pretraining, NT-LLM achieves superior performance across diverse graph tasks ranging from basic structural analysis to complex reasoning scenarios. Our comprehensive evaluation demonstrates that this lightweight yet powerful approach effectively enhances LLMs'ability to understand and reason with graph-structured information, offering an efficient solution for graph-based applications of language models.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8820.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8820.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NT-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Node Tokenizer for Large Language Models (anchor-based positional encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An anchor-based graph node tokenizer that encodes each node by its vector of shortest-hop distances to a selected set of anchor nodes, projects that discrete distance encoding into a Euclidean embedding space via a rank-preserving pretrained MLP, and injects the resulting positional embeddings into frozen LLMs via prompt-tuning and LoRA for downstream graph tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Anchor-based positional node encoding (node tokenizer)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Select a set of anchor nodes by a greedy coverage algorithm; encode each node v as d_v = (dist(v,a1),...,dist(v,aK)) where dist is shortest-path hop count; approximate pairwise node distances by min_k d_u[k]+d_v[k]; learn a mapping phi (implemented as a 3-layer MLP) that projects the discrete anchor-distance vector into a continuous Euclidean embedding space via a rank-preserving pretraining objective (binary-cross-entropy on pairwise ordering of distances). The pretrained embeddings are converted into soft prompt tokens by a small adapter for prompt tuning; LoRA is used to adapt the LLM weights efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs, citation networks, molecular graphs, knowledge/explanation graphs (evaluated on Cora, OGBN-arxiv, OGBL-ddi, OGBG-molhiv, ExplaGraphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Greedy anchor selection with coverage ratio rho and coverage radius c (default c=1, rho=0.7); compute K-dimensional hop-distance vectors per node; approximate pairwise distances by min_k (d_u[k]+d_v[k]); project distance vectors to Euclidean space via a small MLP phi pretrained with a rank-preserving objective (BCE on pairwise distance orderings); map projected embeddings into LLM soft-prompt tokens via a learnable adapter; fine-tune LLM with prompt-tuning and LoRA (only small adapter and low-rank matrices trained).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification (Cora, OGBN-arxiv), edge prediction / link prediction (OGBL-ddi), graph property prediction (OGBG-molhiv), knowledge-graph question answering / explanation (ExplaGraphs), and general graph reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 3 reported NT-LLM (LoRA / tuned) results: Cora accuracy 0.9478; OGBN-arxiv accuracy 0.7525; OGBL-ddi Hits@20 0.5904; OGBG-molhiv ROC-AUC 0.7531; ExplaGraphs accuracy 0.9332. Ablation (Table 5) shows drops: w/o PE (no positional encoding) Cora 0.8070, arxiv 0.6971, ddi 0.3592, molhiv 0.6554, ExplaGraphs 0.8224; w/o Pre (no distance-to-Euclidean pretrain) Cora 0.8195, arxiv 0.6538, ddi 0.3791, molhiv 0.6419, ExplaGraphs 0.7671. Tokenizer efficiency (Table 6): NT-LLM trainable parameters: Cora 0.3M (<1min training), OGBN-arxiv 0.7M (10min), OGBL-ddi 0.5M (1min); comparable GAT/GraphFormer tokenizers have 1.4M–49.2M params and much larger training times.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>NT-LLM consistently outperforms pure LLM baselines, classic GNNs (GCN, GAT, GraphSAGE), graph transformers (GraphFormers, Heterformer) and GNN-LLM hybrid methods (GraphGPT, GraphTranslator, G-Retriever, GRAG) on reported benchmarks. Anchor selection ablation (Table 4) shows NT-LLM's greedy coverage selection yields higher accuracy than Degree, Random, Closeness, Eigenvector, PageRank, Betweenness, and HPLC strategies (e.g., Cora: Ours 0.9478 vs HPLC 0.9174). The paper also reports NT-LLM gives >60% improvement over LLM-only methods on OGBL-ddi (qualitative claim in text).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Lightweight tokenizer (single small MLP) with far fewer trainable parameters and lower training time than GNN tokenizers; preserves structural/topological information via anchor-distance encoding; avoids verbose graph-to-text prompts (reducing inference token cost); compatible with frozen large LLMs via prompt tuning and LoRA; empirically improves accuracy across multiple graph tasks; explicit theoretical error bound when nodes are covered by anchors.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Relies on hop-distance (unweighted, unit-edge) assumptions — the paper assumes unit edge lengths and leaves weighted graphs for future work; anchor coverage trade-offs (more anchors required for small coverage radius or high coverage ratio, increasing compute); requires a rank-preserving pretraining stage to resolve mismatch between discrete hop distances and Euclidean embedding distances; when nodes are not covered by anchors the distance estimate can be arbitrarily bad with probability (1 - coverage_ratio)^2.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>If neither of two nodes is covered by any anchor, the estimated pairwise distance error is unbounded (probability (1 - rho)^2); significant performance degradation if projection pretraining is omitted (ablation w/o Pre shows large drops); performance/hyperparameter sensitivity: small coverage radius and large coverage ratio greatly increase anchor count and computational cost; method currently not evaluated on weighted graphs (limitation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Anchors to Answers: A Novel Node Tokenizer for Integrating Graph Structure into Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8820.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8820.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph Textual Conversion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-to-text / descriptive prompt conversion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Category of methods that serialize or describe graph structure as natural-language textual prompts by converting local contexts or subgraphs into descriptive sentences so that a vanilla LLM can process the graph as text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph textual conversion (descriptive serialization / promptification)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert graph topology (local node neighborhood, paths, relations) into natural-language descriptions or long textual prompts that enumerate nodes, edges, attributes, and structural relations; supply these textual descriptions directly to LLMs for downstream predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs, knowledge graphs, local subgraphs / neighborhoods (paper refers to general graphs where nodes/edges may have textual attributes).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Serialize local context of a target node (e.g., neighbors, edge relations, short paths) into explicit textual descriptions; build prompts that include concatenated textual attributes and structural descriptions; then run LLM zero-shot or fine-tuned prompt/LoRA workflows on those prompts. (No single explicit traversal algorithm provided in paper; described generically as translation of local context into textual descriptions.)</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification, link prediction, graph-conditioned text generation, knowledge-graph question answering / QA, explanation generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No direct numeric metrics provided in this paper for graph-to-text baselines beyond qualitative statements. The paper reports that purely textual LLM methods performed worse than graph-aware approaches on structure-only datasets (e.g., OGBL-ddi) and are substantially more token-expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared qualitatively to node-tokenizer approaches: textual conversion consumes many more tokens (higher inference cost) and may lose or inadequately encode topology; node-tokenizers (including NT-LLM) are presented as more efficient alternatives. No detailed numerical head-to-head in tables beyond general statements.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Leverages LLMs' pretrained language and reasoning capabilities directly without requiring GNNs or specialized tokenizers; easy to implement (serialize graph as readable prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Produces verbose prompts that greatly increase token length and LLM inference cost; scalability issues for large graphs; potential loss or distortion of fine-grained topological information during conversion.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Poor scalability to large graphs due to token length; weak on structure-only datasets (no textual attributes) where topology matters (e.g., LLM-only approaches perform poorly on OGBL-ddi as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Anchors to Answers: A Novel Node Tokenizer for Integrating Graph Structure into Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8820.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8820.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph Node Tokenizer (GNN-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GNN-based graph node tokenizer (embedding projection into LLM token space)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approach that uses graph neural networks to produce structure-aware node embeddings which are then projected into the LLM's token/embedding space so that the LLM ingests compact node tokens rather than verbose text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GNN-based node tokenizer (embedding-to-token projection)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Train a GNN (e.g., GCN, GAT, GraphSAGE or graph transformer) to produce node representations; map those continuous node embeddings to soft tokens in the LLM embedding space (via projection layers) so nodes are represented compactly as input tokens for downstream LLM processing.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs including text-attributed graphs; used in hybrid GNN-LLM systems.</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Run message-passing GNN to compute node embeddings; apply a projection layer to map GNN output into LLM input embedding dimension; prepend/insert these as soft tokens or otherwise inject into the LLM's input for prompt tuning or downstream generation.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification, link prediction, graph-conditioned generation, retrieval-augmented generation for QA, explanation generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Indirectly reported via baselines: GNN baselines (GCN, GAT, GraphSAGE) achieve e.g., Cora accuracies 0.8147–0.8352; GraphFormers reported Cora 0.8910. Efficiency Table 6 shows GAT tokenizers have 1.4M–2.6M trainable params and training time from minutes to hours; GraphFormers reported much larger (3.6M–49.2M params).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to textual-conversion: more compact and less token-intensive for LLM inference. Compared to NT-LLM: more expressive but substantially heavier to train and scale (higher trainable parameters and longer training times). NT-LLM presents similar or better task performance while being substantially more parameter- and time-efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Produces compact node tokens that reduce LLM inference token cost; leverages powerful graph representation learning to capture topology.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Training overhead is substantial (needs expressive GNN architecture and training); scaling GNN tokenizers to match LLMs' capacity introduces heavy compute; larger number of trainable parameters and longer training times (see Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Scalability and training-cost limitations when attempting to match LLM-scale expressivity; potential mismatch if projection alignment to LLM embedding space is inadequate (motivates alternatives like NT-LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Anchors to Answers: A Novel Node Tokenizer for Integrating Graph Structure into Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8820.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8820.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GNN-LLM hybrid / Retrieval-augmented hybrids</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GNN-LLM hybrid and Retrieval-Augmented Generation (RAG) methods (e.g., GraphGPT, GraphTranslator, G-Retriever, GRAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hybrid pipelines that combine GNNs (or retrieval modules over graphs) with LLMs: GNNs provide structure-aware embeddings or retrieval indices, which are fused or aligned with LLMs for downstream open-ended generation, QA, or classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GNN-LLM hybrid / Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Use GNNs to compute structure-aware representations and (a) feed them into LLMs as embeddings/tokens, (b) use them to retrieve relevant graph substructures or textual passages for LLM conditioning, or (c) align a graph model with an LLM via translator modules to support graph-conditioned generation and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs, textual graphs, knowledge graphs, text-attributed graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Train or use pretrained GNNs to embed nodes/subgraphs; optionally index representations for retrieval; supply retrieved textual or embedding context to LLMs; sometimes use an alignment/translation module (GraphTranslator) to map graph model outputs into LLM inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-conditioned text generation, knowledge graph question answering, node/edge prediction, explanation generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports that NT-LLM (fine-tuned with LoRA) surpasses these hybrid methods on the evaluated benchmarks (no per-method numerical breakdown in the paper's main tables, but hybrids are listed among baselines and NT-LLM is reported superior overall).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Presented as stronger than pure LLM-only methods but heavier than NT-LLM in training/complexity; NT-LLM claims to outperform them while being lighter-weight because it avoids full GNN training and heavy alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Combines strengths of explicit graph modeling (GNNs) with LLM generative/reasoning abilities; retrieval augmentation can provide faithful grounding documents/subgraphs to the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Complex system design and training (model fusion, alignment); higher computational/training cost; potential latency from retrieval pipelines or GNN inference.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When aligned modules are not well trained, or when GNN capacity is insufficient, hybrids may underperform; additional engineering complexity and compute can hinder deployment at LLM scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Anchors to Answers: A Novel Node Tokenizer for Integrating Graph Structure into Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Talk like a Graph: Encoding Graphs for Large Language Models <em>(Rating: 2)</em></li>
                <li>GraphGPT: Graph instruction tuning for large language models <em>(Rating: 2)</em></li>
                <li>GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks <em>(Rating: 2)</em></li>
                <li>G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering <em>(Rating: 2)</em></li>
                <li>GRAG: Graph Retrieval-Augmented Generation <em>(Rating: 2)</em></li>
                <li>WalkLM: a uniform language model fine-tuning framework for attributed graph embedding <em>(Rating: 1)</em></li>
                <li>GRENADE: Graph-Centric Language Model for Self-Supervised Representation Learning on Text-Attributed Graphs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8820",
    "paper_id": "paper-273346560",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "NT-LLM",
            "name_full": "Node Tokenizer for Large Language Models (anchor-based positional encoding)",
            "brief_description": "An anchor-based graph node tokenizer that encodes each node by its vector of shortest-hop distances to a selected set of anchor nodes, projects that discrete distance encoding into a Euclidean embedding space via a rank-preserving pretrained MLP, and injects the resulting positional embeddings into frozen LLMs via prompt-tuning and LoRA for downstream graph tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Anchor-based positional node encoding (node tokenizer)",
            "representation_description": "Select a set of anchor nodes by a greedy coverage algorithm; encode each node v as d_v = (dist(v,a1),...,dist(v,aK)) where dist is shortest-path hop count; approximate pairwise node distances by min_k d_u[k]+d_v[k]; learn a mapping phi (implemented as a 3-layer MLP) that projects the discrete anchor-distance vector into a continuous Euclidean embedding space via a rank-preserving pretraining objective (binary-cross-entropy on pairwise ordering of distances). The pretrained embeddings are converted into soft prompt tokens by a small adapter for prompt tuning; LoRA is used to adapt the LLM weights efficiently.",
            "graph_type": "Text-attributed graphs, citation networks, molecular graphs, knowledge/explanation graphs (evaluated on Cora, OGBN-arxiv, OGBL-ddi, OGBG-molhiv, ExplaGraphs)",
            "conversion_method": "Greedy anchor selection with coverage ratio rho and coverage radius c (default c=1, rho=0.7); compute K-dimensional hop-distance vectors per node; approximate pairwise distances by min_k (d_u[k]+d_v[k]); project distance vectors to Euclidean space via a small MLP phi pretrained with a rank-preserving objective (BCE on pairwise distance orderings); map projected embeddings into LLM soft-prompt tokens via a learnable adapter; fine-tune LLM with prompt-tuning and LoRA (only small adapter and low-rank matrices trained).",
            "downstream_task": "Node classification (Cora, OGBN-arxiv), edge prediction / link prediction (OGBL-ddi), graph property prediction (OGBG-molhiv), knowledge-graph question answering / explanation (ExplaGraphs), and general graph reasoning tasks.",
            "performance_metrics": "Table 3 reported NT-LLM (LoRA / tuned) results: Cora accuracy 0.9478; OGBN-arxiv accuracy 0.7525; OGBL-ddi Hits@20 0.5904; OGBG-molhiv ROC-AUC 0.7531; ExplaGraphs accuracy 0.9332. Ablation (Table 5) shows drops: w/o PE (no positional encoding) Cora 0.8070, arxiv 0.6971, ddi 0.3592, molhiv 0.6554, ExplaGraphs 0.8224; w/o Pre (no distance-to-Euclidean pretrain) Cora 0.8195, arxiv 0.6538, ddi 0.3791, molhiv 0.6419, ExplaGraphs 0.7671. Tokenizer efficiency (Table 6): NT-LLM trainable parameters: Cora 0.3M (&lt;1min training), OGBN-arxiv 0.7M (10min), OGBL-ddi 0.5M (1min); comparable GAT/GraphFormer tokenizers have 1.4M–49.2M params and much larger training times.",
            "comparison_to_others": "NT-LLM consistently outperforms pure LLM baselines, classic GNNs (GCN, GAT, GraphSAGE), graph transformers (GraphFormers, Heterformer) and GNN-LLM hybrid methods (GraphGPT, GraphTranslator, G-Retriever, GRAG) on reported benchmarks. Anchor selection ablation (Table 4) shows NT-LLM's greedy coverage selection yields higher accuracy than Degree, Random, Closeness, Eigenvector, PageRank, Betweenness, and HPLC strategies (e.g., Cora: Ours 0.9478 vs HPLC 0.9174). The paper also reports NT-LLM gives &gt;60% improvement over LLM-only methods on OGBL-ddi (qualitative claim in text).",
            "advantages": "Lightweight tokenizer (single small MLP) with far fewer trainable parameters and lower training time than GNN tokenizers; preserves structural/topological information via anchor-distance encoding; avoids verbose graph-to-text prompts (reducing inference token cost); compatible with frozen large LLMs via prompt tuning and LoRA; empirically improves accuracy across multiple graph tasks; explicit theoretical error bound when nodes are covered by anchors.",
            "disadvantages": "Relies on hop-distance (unweighted, unit-edge) assumptions — the paper assumes unit edge lengths and leaves weighted graphs for future work; anchor coverage trade-offs (more anchors required for small coverage radius or high coverage ratio, increasing compute); requires a rank-preserving pretraining stage to resolve mismatch between discrete hop distances and Euclidean embedding distances; when nodes are not covered by anchors the distance estimate can be arbitrarily bad with probability (1 - coverage_ratio)^2.",
            "failure_cases": "If neither of two nodes is covered by any anchor, the estimated pairwise distance error is unbounded (probability (1 - rho)^2); significant performance degradation if projection pretraining is omitted (ablation w/o Pre shows large drops); performance/hyperparameter sensitivity: small coverage radius and large coverage ratio greatly increase anchor count and computational cost; method currently not evaluated on weighted graphs (limitation).",
            "uuid": "e8820.0",
            "source_info": {
                "paper_title": "From Anchors to Answers: A Novel Node Tokenizer for Integrating Graph Structure into Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Graph Textual Conversion",
            "name_full": "Graph-to-text / descriptive prompt conversion",
            "brief_description": "Category of methods that serialize or describe graph structure as natural-language textual prompts by converting local contexts or subgraphs into descriptive sentences so that a vanilla LLM can process the graph as text.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Graph textual conversion (descriptive serialization / promptification)",
            "representation_description": "Convert graph topology (local node neighborhood, paths, relations) into natural-language descriptions or long textual prompts that enumerate nodes, edges, attributes, and structural relations; supply these textual descriptions directly to LLMs for downstream predictions.",
            "graph_type": "Text-attributed graphs, knowledge graphs, local subgraphs / neighborhoods (paper refers to general graphs where nodes/edges may have textual attributes).",
            "conversion_method": "Serialize local context of a target node (e.g., neighbors, edge relations, short paths) into explicit textual descriptions; build prompts that include concatenated textual attributes and structural descriptions; then run LLM zero-shot or fine-tuned prompt/LoRA workflows on those prompts. (No single explicit traversal algorithm provided in paper; described generically as translation of local context into textual descriptions.)",
            "downstream_task": "Node classification, link prediction, graph-conditioned text generation, knowledge-graph question answering / QA, explanation generation.",
            "performance_metrics": "No direct numeric metrics provided in this paper for graph-to-text baselines beyond qualitative statements. The paper reports that purely textual LLM methods performed worse than graph-aware approaches on structure-only datasets (e.g., OGBL-ddi) and are substantially more token-expensive.",
            "comparison_to_others": "Compared qualitatively to node-tokenizer approaches: textual conversion consumes many more tokens (higher inference cost) and may lose or inadequately encode topology; node-tokenizers (including NT-LLM) are presented as more efficient alternatives. No detailed numerical head-to-head in tables beyond general statements.",
            "advantages": "Leverages LLMs' pretrained language and reasoning capabilities directly without requiring GNNs or specialized tokenizers; easy to implement (serialize graph as readable prompts).",
            "disadvantages": "Produces verbose prompts that greatly increase token length and LLM inference cost; scalability issues for large graphs; potential loss or distortion of fine-grained topological information during conversion.",
            "failure_cases": "Poor scalability to large graphs due to token length; weak on structure-only datasets (no textual attributes) where topology matters (e.g., LLM-only approaches perform poorly on OGBL-ddi as reported).",
            "uuid": "e8820.1",
            "source_info": {
                "paper_title": "From Anchors to Answers: A Novel Node Tokenizer for Integrating Graph Structure into Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Graph Node Tokenizer (GNN-based)",
            "name_full": "GNN-based graph node tokenizer (embedding projection into LLM token space)",
            "brief_description": "Approach that uses graph neural networks to produce structure-aware node embeddings which are then projected into the LLM's token/embedding space so that the LLM ingests compact node tokens rather than verbose text.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "GNN-based node tokenizer (embedding-to-token projection)",
            "representation_description": "Train a GNN (e.g., GCN, GAT, GraphSAGE or graph transformer) to produce node representations; map those continuous node embeddings to soft tokens in the LLM embedding space (via projection layers) so nodes are represented compactly as input tokens for downstream LLM processing.",
            "graph_type": "General graphs including text-attributed graphs; used in hybrid GNN-LLM systems.",
            "conversion_method": "Run message-passing GNN to compute node embeddings; apply a projection layer to map GNN output into LLM input embedding dimension; prepend/insert these as soft tokens or otherwise inject into the LLM's input for prompt tuning or downstream generation.",
            "downstream_task": "Node classification, link prediction, graph-conditioned generation, retrieval-augmented generation for QA, explanation generation.",
            "performance_metrics": "Indirectly reported via baselines: GNN baselines (GCN, GAT, GraphSAGE) achieve e.g., Cora accuracies 0.8147–0.8352; GraphFormers reported Cora 0.8910. Efficiency Table 6 shows GAT tokenizers have 1.4M–2.6M trainable params and training time from minutes to hours; GraphFormers reported much larger (3.6M–49.2M params).",
            "comparison_to_others": "Compared to textual-conversion: more compact and less token-intensive for LLM inference. Compared to NT-LLM: more expressive but substantially heavier to train and scale (higher trainable parameters and longer training times). NT-LLM presents similar or better task performance while being substantially more parameter- and time-efficient.",
            "advantages": "Produces compact node tokens that reduce LLM inference token cost; leverages powerful graph representation learning to capture topology.",
            "disadvantages": "Training overhead is substantial (needs expressive GNN architecture and training); scaling GNN tokenizers to match LLMs' capacity introduces heavy compute; larger number of trainable parameters and longer training times (see Table 6).",
            "failure_cases": "Scalability and training-cost limitations when attempting to match LLM-scale expressivity; potential mismatch if projection alignment to LLM embedding space is inadequate (motivates alternatives like NT-LLM).",
            "uuid": "e8820.2",
            "source_info": {
                "paper_title": "From Anchors to Answers: A Novel Node Tokenizer for Integrating Graph Structure into Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GNN-LLM hybrid / Retrieval-augmented hybrids",
            "name_full": "GNN-LLM hybrid and Retrieval-Augmented Generation (RAG) methods (e.g., GraphGPT, GraphTranslator, G-Retriever, GRAG)",
            "brief_description": "Hybrid pipelines that combine GNNs (or retrieval modules over graphs) with LLMs: GNNs provide structure-aware embeddings or retrieval indices, which are fused or aligned with LLMs for downstream open-ended generation, QA, or classification.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "GNN-LLM hybrid / Retrieval-Augmented Generation (RAG)",
            "representation_description": "Use GNNs to compute structure-aware representations and (a) feed them into LLMs as embeddings/tokens, (b) use them to retrieve relevant graph substructures or textual passages for LLM conditioning, or (c) align a graph model with an LLM via translator modules to support graph-conditioned generation and reasoning.",
            "graph_type": "General graphs, textual graphs, knowledge graphs, text-attributed graphs.",
            "conversion_method": "Train or use pretrained GNNs to embed nodes/subgraphs; optionally index representations for retrieval; supply retrieved textual or embedding context to LLMs; sometimes use an alignment/translation module (GraphTranslator) to map graph model outputs into LLM inputs.",
            "downstream_task": "Graph-conditioned text generation, knowledge graph question answering, node/edge prediction, explanation generation.",
            "performance_metrics": "Paper reports that NT-LLM (fine-tuned with LoRA) surpasses these hybrid methods on the evaluated benchmarks (no per-method numerical breakdown in the paper's main tables, but hybrids are listed among baselines and NT-LLM is reported superior overall).",
            "comparison_to_others": "Presented as stronger than pure LLM-only methods but heavier than NT-LLM in training/complexity; NT-LLM claims to outperform them while being lighter-weight because it avoids full GNN training and heavy alignment.",
            "advantages": "Combines strengths of explicit graph modeling (GNNs) with LLM generative/reasoning abilities; retrieval augmentation can provide faithful grounding documents/subgraphs to the LLM.",
            "disadvantages": "Complex system design and training (model fusion, alignment); higher computational/training cost; potential latency from retrieval pipelines or GNN inference.",
            "failure_cases": "When aligned modules are not well trained, or when GNN capacity is insufficient, hybrids may underperform; additional engineering complexity and compute can hinder deployment at LLM scale.",
            "uuid": "e8820.3",
            "source_info": {
                "paper_title": "From Anchors to Answers: A Novel Node Tokenizer for Integrating Graph Structure into Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Talk like a Graph: Encoding Graphs for Large Language Models",
            "rating": 2,
            "sanitized_title": "talk_like_a_graph_encoding_graphs_for_large_language_models"
        },
        {
            "paper_title": "GraphGPT: Graph instruction tuning for large language models",
            "rating": 2,
            "sanitized_title": "graphgpt_graph_instruction_tuning_for_large_language_models"
        },
        {
            "paper_title": "GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks",
            "rating": 2,
            "sanitized_title": "graphtranslator_aligning_graph_model_to_large_language_model_for_openended_tasks"
        },
        {
            "paper_title": "G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering",
            "rating": 2,
            "sanitized_title": "gretriever_retrievalaugmented_generation_for_textual_graph_understanding_and_question_answering"
        },
        {
            "paper_title": "GRAG: Graph Retrieval-Augmented Generation",
            "rating": 2,
            "sanitized_title": "grag_graph_retrievalaugmented_generation"
        },
        {
            "paper_title": "WalkLM: a uniform language model fine-tuning framework for attributed graph embedding",
            "rating": 1,
            "sanitized_title": "walklm_a_uniform_language_model_finetuning_framework_for_attributed_graph_embedding"
        },
        {
            "paper_title": "GRENADE: Graph-Centric Language Model for Self-Supervised Representation Learning on Text-Attributed Graphs",
            "rating": 1,
            "sanitized_title": "grenade_graphcentric_language_model_for_selfsupervised_representation_learning_on_textattributed_graphs"
        }
    ],
    "cost": 0.01597925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>From Anchors to Answers: A Novel Node Tokenizer for Integrating Graph Structure into Large Language Models
31 Aug 2025</p>
<p>Yanbiao Ji jiyanbiao@sjtu.edu.cn 
Chang Liu 
Xin Chen xchen@se.cuhk.edu.hk 
Dan Luo 
Mei Li mei-li@sjtu.edu.cn 
Yue Ding dingyue@sjtu.edu.cn 
Wenqing Lin 
Hongtao Lu htlu@sjtu.edu.cn </p>
<p>Shanghai Jiao Tong University Shanghai
China</p>
<p>Shanghai Jiao Tong University Shanghai
China</p>
<p>The Chinese University of Hong Kong Hong Kong
China</p>
<p>Lehigh University Bethlehem
PAUSA</p>
<p>Shanghai Jiao Tong University Shanghai
China</p>
<p>Shanghai Jiao Tong University Shanghai
China</p>
<p>ShenzhenChina</p>
<p>Shanghai Jiao Tong University Shanghai
China</p>
<p>From Anchors to Answers: A Novel Node Tokenizer for Integrating Graph Structure into Large Language Models
31 Aug 2025E391DEE7BB83D774FD9C5EE0D529B71D10.1145/3746252.3761167arXiv:2410.10743v2[cs.AI]Large Language ModelsPositional EncodingKnowledge Graphs
Enabling large language models (LLMs) to effectively process and reason with graph-structured data remains a significant challenge despite their remarkable success in natural language tasks.Current approaches either convert graph structures into verbose textual descriptions, consuming substantial computational resources, or employ complex graph neural networks as tokenizers, which introduce significant training overhead.To bridge this gap, we present NT-LLM, a novel framework with an anchor-based positional encoding scheme for graph representation.Our approach strategically selects reference nodes as anchors and encodes each node's position relative to these anchors, capturing essential topological information without the computational burden of existing methods.Notably, we identify and address a fundamental issue: the inherent misalignment between discrete hop-based distances in graphs and continuous distances in embedding spaces.By implementing a rankpreserving objective for positional encoding pretraining, NT-LLM achieves superior performance across diverse graph tasks ranging from basic structural analysis to complex reasoning scenarios.Our comprehensive evaluation demonstrates that this lightweight yet powerful approach effectively enhances LLMs' ability to understand and reason with graph-structured information, offering an efficient solution for graph-based applications of language models.CCS Concepts• Information systems → Data mining.</p>
<p>Introduction</p>
<p>In recent years, Large Language Models (LLMs), such as LLaMA [59] and GPT [45], have revolutionized artificial intelligence.They have demonstrated powerful capabilities in solving various natural language processing (NLP) tasks, including question answering [36,53], text generation [2,50], and document understanding [27,68].While LLMs have primarily been applied to text data, an increasing number of applications now involve text data intertwined with structured information represented as graphs.For instance, in social networks, nodes represent entities, while edges capture the relationships between them.Both nodes and edges can also be associated with textual descriptions that detail their attributes.Since LLMs are primarily designed to model text in a sequential format, applying them to graph-related tasks presents new challenges, particularly in encoding the structural information of graphs [15,35].</p>
<p>While many studies [28,40,65] have attempted to combine language modeling and graph representation learning with mediumsized transformer models such as BERT [12] and RoBERTa [41], efficient graph reasoning with LLMs of billions of parameters remains challenging.To leverage the strength of LLMs for graph structure understanding, existing efforts can be categorized into two groups [25,52]: (1) Graph Textual Conversion, which translates a graph's structure into a descriptive textual representation [9,26,56].These studies typically convert the local context of a target node into textual descriptions that incorporate relevant structural information, and then utilize large language models to predict properties such as node labels and the presence of links.The underlying assumption is that the powerful capabilities of LLMs can generalize to interpret graph-structured knowledge through textual input.</p>
<p>However, such descriptions typically require a large number of tokens to describe the graph structure, greatly increasing the cost of LLM inference.(2) Graph Node Tokenizer, which generates node embeddings for each node and then projects these embeddings into LLM token space [38,57,58].With the utilization of powerful Graph Neural Networks (GNNs) as graph node tokenizers, these methods effectively reduce the inference cost by representing the graph structure with compact node tokens.However, the graph representation learning process often brings heavy training overhead.Achieving scalability comparable to LLMs requires an expressive GNN (e.g., with elaborate graph convolution paradigms) of similar scale, which introduces additional computational overhead.</p>
<p>To enable effective and efficient LLM reasoning on graphs, a graph encoding paradigm that preserves rich graph structural information without introducing heavy training or inference overhead is needed.This naturally aligns with the motivation of graph positional encoding, which introduces extra embeddings containing structural information to disambiguate nodes and enhance graph representation learning during the training of GNNs and graph transformers [7,14,49].In this paper, we introduce an anchor-based graph positional encoding scheme for graph node tokenization, and investigate its integration with LLMs across various graph-related tasks.The core of our method is the strategic selection of key nodes, referred to as anchors, which serve as reference points for encoding the graph topology.Each node is then represented based on its relative distance to these anchors, effectively capturing the structural information of the graph.Furthermore, we identify the issue of misalignment between the non-Euclidean graph space (hop-based discrete distance) and the Euclidean embedding space (continuous Euclidean distance).A rank-preserving pretraining objective is proposed to project the positional embedding into Euclidean space.We then apply task-specific tuning procedures using prompt tuning and LoRA techniques to facilitate better structural understanding of LLMs for downstream tasks.Extensive empirical studies demonstrate that NT-LLM substantially improves LLM performance across a diverse range of graph-related tasks, from basic graph analysis to complex reasoning.Our main contributions are as follows:</p>
<p>• We introduce a position-anchored graph encoding approach for LLMs that efficiently preserves crucial structural information while reducing the computational complexity associated with commonly used graph encoding methods.Several approaches have been developed to encode positional information in graphs.Laplacian eigenmaps [6,7] utilize the eigenvectors of the graph Laplacian matrix for this purpose.In contrast, random walk encodings [8,48,69] capture structural information by simulating random walks on the graph.This method encodes the co-occurrence probabilities of nodes during these walks, thereby embedding nodes with similar neighborhoods closer in the embedding space.Rx'ecently, researchers have introduced Distance Encoding [13,33,49], which incorporates structural information by encoding the shortest path distances between nodes.Furthermore, Random Feature methods [1,14] have been developed to approximate positional encodings using learnable or predefined random feature maps.To provide a comprehensive overview of these approaches, Table 1 presents a detailed comparison of various graph positional encoding methods.</p>
<p>LLMs in Graph-Related Tasks</p>
<p>The rapid advancement in LLMs have led to their successful application across various domains, leveraging their powerful sequence modeling capabilities [37,39,62].In recent years, there has been a growing interest in applying LLMs to graph-related tasks, aiming to harness their ability to capture long-range dependencies and perform complex reasoning.Initial efforts focused on directly feeding textual descriptions of graphs into LLMs to tackle tasks such as node classification and link prediction [18,24].While these methods demonstrated the potential of LLMs in understanding graph data, they faced significant scalability challenges due to the complexity of constructing comprehensive prompts and the loss of crucial structural information during the graph-to-text conversion process.To address these limitations, subsequent research has explored the integration of Graph Neural Networks (GNNs) with LLMs to better leverage the strengths of both paradigms [20,23,57].One common approach involves using GNNs to generate structure-aware embeddings, which are then fed into LLMs for downstream tasks [57,58].More advanced techniques have delved into model fusion training [70], model alignment [34,63], and the development of LLM agents specifically designed to handle graph data [10,43].</p>
<p>Preliminary</p>
<p>Textual Graphs.A textual graph is a graph in which nodes and edges are associated with textual attributes.Formally, it is defined as G = (V, E, {T  }  ∈ V , {T  }  ∈ E ), where V and E represent the sets of nodes and edges, respectively.Here, T  and T  denote the textual attributes corresponding to each node and edge, which are usually represented by natural language descriptions. 1ext Encoding via Language Models.Language Models (LMs) have proven to be highly effective at encoding textual attributes
✓ ✓ ✓ ✓ ✓ ✓ Global Position × × ✓ ✓ × ✓ Euclidean Space ✓ ✓ × ✓ ✓ ✓ Time Complexity 𝑂 ( | V | 3 ) 𝑂 ( | E | ) 𝑂 ( | V | 2 𝑙𝑜𝑔 2 ( | V | ) ) 𝑂 ( | E |𝑙𝑜𝑔 ( | V | ) + | V |𝑙𝑜𝑔 2 ( | V | ) ) - 𝑂 ( | V | 2 + | V | | E | )
in graphs, producing embeddings that capture rich semantic information.For a given textual attribute   associated with a node or edge , an LM encodes this attribute into an embedding vector as follows:
x 𝑖 = LM(𝑇 𝑖 ) ∈ R 𝑘 .(1)
Prompt Tuning for LLMs.LLMs are trained on vast corpora of textual data, demonstrating emergent capabilities that facilitate advanced semantic understanding and exceptional task generalization.Formally, an LLM parameterized by  takes as input a sequence of tokens X = {x 1 , x 2 , . . ., x  } along with a task prompt P, and generates an output sequence Y = {y 1 , y 2 , . . ., y  }.The probability distribution of the output sequence, conditioned on the concatenated input sequence and prompt [P; X], is expressed as:
𝑝 𝜃 (Y| [P; X]) = 𝑟 𝑖=1 𝑝 𝜃 (y 𝑖 |y &lt;𝑖 , [P; X]),(2)
where y &lt; represents the prefix of sequence y up to position  − 1, and   (y  |y &lt; , [P; X]) denotes the probability of generating token y  given the preceding tokens y &lt; and the input [P; X].</p>
<p>Prompt tuning [32] is an efficient technique for adapting LLMs to specific tasks without modifying the model's parameters.This technique keeps the pretrained LLM frozen, and optimizes a small set of continuous prompt embeddings {e  }  =1 , where  is the number of prompt tokens.These prompts are generally initialized either randomly or using the embeddings of specific tokens, and are subsequently optimized throughout the training process.Formally, the prompt embeddings can be represented as:
E = [e 1 , e 2 , ..., e 𝑛 ] 𝑇 ,(3)
where the dimension of the embedding space is , and E ∈ R × .The prompt embeddings can be generated by a small trainable mapping network Φ:
E = Φ(X),(4)
where X represents the input embeddings to be transformed.This allows for more flexible and expressive prompt representations.The generation process with prompt tuning can be represented as follows:
𝑝 𝜃,Φ (Y| [P; X]) = 𝑟 𝑖=1 𝑝 𝜃,Φ (y 𝑖 |y &lt;𝑖 , [P; X]),(5)
where  represents the frozen parameters of the pretrained LLM, Φ is the learnable prompt mapping network, P is the prompt, X is the input sequence, and Y = {y 1 , y 2 , ..., y  } is the output sequence.</p>
<p>Methodology</p>
<p>We propose NT-LLM, which can seamlessly integrate graph-structure knowledge with LLMs through two key components: Graph Node Tokenizer and Task-Specific LLM Tuning.The node tokenizer leverages carefully selected anchor nodes to encode the spatial position of each node, and positional embedding pretraining to preserve geometric relationships between nodes.The task-specific LLM tuning integrates our node position embedding with prompt tuning and low-rank adaptation, which allows LLMs to effectively leverage both textual and graph-based information.Figure 1 illustrates the overall framework of NT-LLM.</p>
<p>Graph Node Tokenizer</p>
<p>In large language models, it is straightforward to inject information about the relative or absolute position of tokens in a sequence via their index.However, this approach is not feasible for graphs due to two key differences.First, graphs do not have an inherent linear ordering of nodes, unlike sequences, where tokens follow a clear order.Nodes in a graph are interconnected in a complex, multidimensional structure, where relationships are defined by edges, and there is no natural start or end.Second, the neighborhood of each node can vary significantly in size and shape, which makes the concept of a relative or absolute "position" less meaningful.To address this challenge, we propose a novel graph node tokenizer, which consists of three key steps: anchor node identification, node encoding, and Euclidean projection.</p>
<p>Anchor Node Identification.</p>
<p>Prior works [11,66] have demonstrated that using anchor nodes can well capture the position of a given node with respect to all other nodes in a graph.In particular, the position of a node can be described in terms of its relative distance (e.g., shortest path distance) to these anchor nodes.For efficient identification of anchor nodes, we implement a greedy anchor selection algorithm with a coverage ratio threshold.The details of this greedy selection procedure are shown in Algorithm 1.Given a coverage ratio  and coverage radius , we start with an empty set A of anchor nodes and an empty set   of covered nodes (Line 1).Here, we define that a node  is covered by a node  only if  is in the -hop subgraph of node ; otherwise,  is considered uncovered by .Then, we iteratively select a new anchor node that covers the maximum set of uncovered nodes in its -hop subgraph   () (Line 3) and add these covered nodes to   (Line 8) until the size of   is no less than  * |V | (Line 2).The identified anchor nodes enable us to provide a unique node description for other nodes in terms of their relative distance, capturing both global and local structures within the graph.</p>
<p>Node Encoding.</p>
<p>Given the identified anchor nodes A = { 1 ,  2 , . . .,   }, we encode the position of each node  with respect to these anchors:  A ← A ∪ {ℎ }
d𝑣 = (𝑑 1 , 𝑑 2 , . . . , 𝑑 𝐾 ),(6)𝑑 𝑖 = dist(𝑣, 𝑎 𝑖 ), ∀𝑖 ∈ {1, . . . , 𝐾 },(7)</p>
<p>8:</p>
<p>←   ∪   (ℎ ) 9: end while 10: return A where dist(,   ) denotes the number of hops in the shortest path between node  and anchor node   .</p>
<p>Utilizing relative distance, we can approximate the shortest distance between any two nodes  and  in the graph defined as:
d (𝑢, 𝑣) := min 𝑘 ∈ {1,...,𝐾 } d𝑢 [𝑘] + d𝑣 [𝑘] ,(8)
where d [] means the -th element of d .This approximation estimates the distance by identifying the anchor node that provides the minimal combined distance between  and .Note that our approximated shortest path distance may not be the actual shortest path distance.However, d (, ) actually serves as an upper bound for the true shortest path distance between  and .More formally, the error between the estimated distance and real distance is bounded by the parameters  and : Lemma 4.1.Given any two nodes ,  from a graph, the error of the estimated shortest path distance can be bounded by 2 with a probability no smaller than 1 − (1 − ) 2 , where  is the coverage radius and  is the coverage ratio.The error bound holds when either  or  are covered by some anchor nodes.When neither  nor  is covered, this error is unbounded.The probability for this case is (1 − ) 2 .Therefore, the probability that the error of our estimated distance is bounded is Mismatch between Shortest Path Distance and Euclidean Distance.In LLMs, positional embeddings reflect the linear order of tokens, where proximity in the sequence corresponds to closeness in the embedding space, adhering to Euclidean-like assumptions.This enables the model to capture local relationships: tokens near each other in the input sequence are also close in the learned embedding space, preserving context and meaning.However, as demonstrated in Figure 2, when nodes 2 and 4 are set as anchor nodes, the shortest Non-Eucilidean Space path distances between nodes 2 and 3, as well as between nodes 1 and 2, are both 1 in the graph's non-Euclidean space.In contrast, the corresponding Euclidean distances would be 1 and √ 2, respectively.This discrepancy in relative distances between node pairs leads to a mismatch between shortest path and Euclidean distances.
1 − (1 − 𝐶𝑅) 2 . □4
To address this issue, we propose a pretraining approach that maps the distance encoding from non-Euclidean to Euclidean space, aiming to preserve geometric relationships between nodes.The necessity of this mapping is further justified through ablation studies in Section 5.6.The pretraining process involves a learnable function  : R  → R  that projects the anchor-based encoding into Euclidean space:
e 𝑣 = 𝜙 ( d𝑣 ) ∈ R 𝑁(9)
where e  represents the transformed node embedding for node .</p>
<p>To preserve geometric relationships among nodes in the embedding space, we propose a rank-preserving training objective based on maximum likelihood estimation.The objective is to maximize the posterior probability  (Φ| &gt;), where Φ denotes the parameters of the mapping function , and &gt; represents the desired order of distances.Assuming independence for the ordering of each pair of distances, we formulate the likelihood function as: (10) where d (, ) denotes the estimated distance between nodes  and , and d (, ) represents the Euclidean distance between their corresponding mapped embeddings e  and e  .We can model the probability of one distance being greater than another using the logistic function :
𝑝 (&gt; |Φ) = (𝑢,𝑣),(𝑖,𝑗 ) ∈ E 𝑝 d𝜙 (𝑢, 𝑣) &gt; d𝜙 (𝑖, 𝑗)|Φ I( d (𝑢,𝑣) &gt; d (𝑖,𝑗 ) ) • 1 − 𝑝 d𝜙 (𝑢, 𝑣) &gt; d𝜙 (𝑖, 𝑗)|Φ I( d (𝑢,𝑣) ≤ d (𝑖,𝑗 ) )𝑝 d𝜙 (𝑢, 𝑣) &gt; d𝜙 (𝑖, 𝑗)|Φ := 𝜎 ( x𝑢,𝑣,𝑖,𝑗 (Φ)),(11)
This objective function encourages the ranking of distances between nodes in the embedding space to align with the ranking of their corresponding shortest path distances in the graph.</p>
<p>To facilitate practical implementation, we reformulate the objective as a binary cross-entropy (BCE) loss:
min Φ L = ∑︁ (𝑢,𝑣),(𝑖,𝑗 ) ∈ E BCE 𝜎 ∥e 𝑢 − e 𝑣 ∥ 2 − ∥e 𝑖 − e 𝑗 ∥ 2 , 𝑦 ,(13)
where  captures the relative ordering of distances:
𝑦 = I( d (𝑢, 𝑣) &gt; d (𝑖, 𝑗)) = 1, if d (𝑢, 𝑣) &gt; d (𝑖, 𝑗), 0, otherwise.(14)
This pretraining approach ensures that the positional embeddings derived from graph structures are compatible with the Euclidean assumptions of LLM architectures while preserving the essential spatial relationships between nodes.Anchor Selection.In each iteration, the algorithm selects an anchor and updates the coverage for remaining nodes.The worst-case time complexity for this part is  (|V | 2 ).This is because:</p>
<ol>
<li>Selecting an anchor requires examining all uncovered nodes in each candidate's c-hop neighborhood ( (|V |) in the worst case).</li>
</ol>
<p>2.</p>
<p>After selecting an anchor, the algorithm must update the uncovered node counts for all other nodes' c-hop neighborhoods that overlap with the newly covered area ( (|V |) nodes to update, each potentially affecting  (|V |) other neighborhoods).</p>
<p>The total time complexity is thus
𝑂 (|V | • |E | + |V | 2 .</p>
<p>Task-Specific LLM Tuning</p>
<p>We now focus on adapting LLMs to leverage graph-based knowledge for specific downstream tasks.Our approach integrates prompt tuning with Low-Rank Adaptation (LoRA) for efficient and effective task-specific fine-tuning.</p>
<p>Prompt</p>
<p>Tuning.We employ prompt tuning to incorporate pretrained graph-based knowledge into the LLM.This technique introduces a small, trainable adapter layer that transforms our pretrained anchor-based node embeddings to soft prompts.These soft prompts serve as a learned prefix to the input, guiding the model's attention and output generation.The generation process, including our prompt tuning adapter, can be formally expressed as:
𝑝 𝜃,Φ (𝑌 |𝐺, 𝑞) = 𝑟 𝑖=1 𝑝 (𝑦 𝑖 |𝑦 &lt;𝑖 , [e 𝐺 ; e 𝑇 ; e 𝑞 ]),(15)
where  denotes the frozen LLM parameters, Φ represents the trainable parameters of the prompt tuning adapters, e  is the pretrained positional encoding derived from the graph structure, e  is the textual embeddings, and e  represents the question designed for corresponding graph tasks.The prompt tuning adapter is a neural that maps the input embeddings to a sequence of continuous prompt tokens.These tokens are prepended to the input sequence before being processed by the LLM.</p>
<p>Low-Rank Adaptation (LoRA).</p>
<p>To further enhance the LLMs' adaptability to graph-structure data, we implement Low-Rank Adaptation (LoRA) [21] in conjunction with prompt tuning.LoRA modifies the weight update mechanism of the LLM by introducing low-rank decomposition, allowing for efficient fine-tuning of the model.For each weight matrix  ∈ R  1 ×2 in the LLM, we introduce a low-rank update:
W ′ = W + BA,(16)
where B ∈ R  1 × and A ∈ R  × 2 are low-rank matrices with rank  ≪ min( 1 ,  2 ).This decomposition significantly reduces the number of trainable parameters, as  is typically much smaller than  1 and  2 .</p>
<p>During the training process, only A and B are updated while the original weights W remain frozen.The update rule for the LoRA parameters can be expressed as:
A 𝑡 +1 = A 𝑡 − 𝜂∇ A L (𝜃, A 𝑡 , B 𝑡 ),(17)B 𝑡 +1 = B 𝑡 − 𝜂∇ B L (𝜃, A 𝑡 , B 𝑡 ), (18)
where  is the learning rate, L is the task-specific loss function, and  denotes the training iteration.</p>
<p>The combination of prompt tuning and LoRA in our approach enables the model to effectively incorporate graph-structural knowledge while adapting to various downstream tasks.</p>
<p>Experiments</p>
<p>We conduct extensive experiments to demonstrate the effectiveness of our NT-LLM by investigating the following research questions: • RQ1: Can NT-LLM outperform state-of-the-art methods in various graph-related tasks?• RQ2: What does node position encoding learn?Does it capture the spatial information as intended?• RQ3: How do different anchor selection strategies influence the performance of NT-LLM?• RQ4: What influence do different design choices have on NT-LLM?</p>
<p>• RQ5: How does our tokenizer compare in efficiency to conventional message-passing GNNs and graph transformers?</p>
<p>Experimental Settings</p>
<p>5.1.1Datasets.We evaluate our approach on diverse graph-based tasks using benchmark datasets from Cora [55], the Open Graph Benchmark (OGB) [22], and ExplaGraphs [54].Our experiments cover node classification with Cora and OGBN-arxiv, edge prediction using OGBL-ddi, and graph property prediction employing OGBG-molhiv 2 .Additionally, we assess knowledge graph question answering tasks using the ExplaGraphs dataset.These datasets encompass a wide range of graph structures and task complexities, allowing for a comprehensive evaluation of our method.Table 2 presents key statistics for each dataset, while Figure 3 illustrates their characteristics in detail.• GNN-based methods: We incorporate widely-adopted GNN architectures, including Graph Convolutional Networks (GCN) [31], Graph Attention Networks (GAT) [60], and GraphSAGE [19].Besides, we also evaluate two graph transformer models: Graph-Formers [65] and Heterformer [28].</p>
<p>Table 3: Main results on benchmark datasets.The best performance is highlighted in bold and the second best is underlined.Δ prompt and Δ LoRA represent the improvements over LLM prompt tuning and LoRA baselines, respectively.* indicates the statistically significant improvements (i.e., two-sided t-test with p&lt;0.05) over the compared baseline.</p>
<p>Method</p>
<p>Cora OGBN-arxiv OGBL-ddi OGBG-molhiv ExplaGraphs (Accuracy↑) (Accuracy↑) (Hits@20↑) (ROC-AUC↑) (Accuracy↑)</p>
<p>GCN [31] 0.8147 0.7360 0.3707 0.7606 -GAT [60] 0.8352 0.7366 0.4133 0.7520 -GraphSAGE [19] 0.8265 0.7295 0.5390 0.7558 -GraphFormers [65] 0.8910 0.7431 0.5538 0.7414 -Heterformer [28] 0 • LLM-only methods: We consider approaches that process graph information directly as textual sequences using LLMs.This category includes implementations utilizing zero-shot inference, prompt tuning [32], and Low-Rank Adaptation (LoRA) [21].• GNN-LLM hybrid methods: We compare our approach with state-of-the-art methods that integrate GNNs and LLMs.Specifically, we include GraphGPT [57] and GraphTranslator [67], which focus on text-attributed graph representation learning with language models.Additionally, we compare our method with G-Retriever [20] and GRAG [23], which are Graph Retrieval-Augmented Generation (RAG) methods that combine GNNs and LLMs for graph-based text generation tasks.</p>
<p>Implementation Details</p>
<p>We implement all models and experiments using PyTorch [47], PyTorch Geometric [16], and the HuggingFace Transformers [64] libraries.All experiments are conducted on two NVIDIA RTX 6000 Ada GPUs, each with 48GB memory.</p>
<p>Text and LLM Components.</p>
<p>For encoding textual attributes, we employ SentenceBERT [51].The LLM component of all experiments is based on the pretrained LLaMA3-8B [59].We use LLaMA3-8B in zero-shot (no fine-tuning), as well as in prompt-tuning and LoRA-based fine-tuning settings.During LLM fine-tuning with LoRA, we set the low-rank dimension to 8 and the scaling factor to 16. Optimization uses AdamW [42] with a learning rate of 1e-4 and weight decay of 0.05.Fine-tuning runs for a maximum of 10 epochs with an early stopping patience of 3. The batch size is set to 32 for OGBN-arxiv and OGBL-ddi, and to 2 for OGBG-molhiv and ExplaGraphs, according to dataset size.</p>
<p>GNN-based Methods.</p>
<p>Our baseline and hybrid GNN models use a 4-layer architecture with hidden dimensions of 256, ReLU activation, and a dropout rate of 0.5.Graph transformer baselines utilize nested GAT architecture combined with transformer layers, where each node uses 5 uniformly sampled neighbors as context.</p>
<p>Training runs using the AdamW optimizer for 500 epochs with an early stopping patience of 10, learning rate of 1e-3 and weight decay of 5e-4.</p>
<p>NT-LLM Implementation.</p>
<p>In the node tokenizing stage, we set the anchor identification parameters as  = 1 and  = 0.7, and map node encodings via a 3-layer MLP.In the LLM fine-tuning stage, we following the settings in 5.2.1.</p>
<p>GNN-LLM Hybrid</p>
<p>Baselines.For GNN-LLM hybrid methods, we combine a 4-layer GAT with LLaMA3-8B, following the architecture and hyperparameter settings as described in their papers.</p>
<p>Main Results (RQ1)</p>
<p>Table 3 compares the performance of our proposed NT-LLM method against baselines on five benchmark datasets on the corresponding task, respectively. 4We have the following key findings:</p>
<p>• NT-LLM consistently outperforms all baseline methods across various tasks and datasets.This observation justifies the superiority of NT-LLM and demonstrates its effectiveness and broad applicability in graph learning.• NT-LLM effectively addresses the challenge of enabling LLMs to understand graph structures.In other words, NT-LLM leverages the strengths of LLMs in understanding textual attributes while benefiting from our proposed node position encoding to capture the graph topology.First, NT-LLM outperforms pure LLM and GNN baselines on all datasets.This observation demonstrates that understanding textual attributes and topology  are equally important for graph learning tasks.Second, when fine-tuning NT-LLM with LoRA (fine-tuned NT-LLM), its performance surpasses LLM-GNN hybrid approaches.This suggests that NT-LLM is more effective at enabling LLMs to understand graph structures compared to intermediate solutions, i.e., LLM-GNN hybrid approaches.• The superiority of NT-LLM in graph understanding comes from our proposed node position encoding.In particular, the OGBL-ddi dataset lacks textual attributes.As we can see, LLM methods perform worse than GNN baseline methods, which highlights their limitations in capturing topological information from graph data.Unlike LLM methods, our proposed NT-LLM, despite not using GNNs, outperforms all baselines with over 60% improvement compared to LLM methods, demonstrating its ability to effectively encode graph structure.</p>
<p>In conclusion, NT-LLM shows superior performance and adaptability across various graph-related tasks and datasets.The improvements over state-of-the-art baselines, even in the absence of textual attributes, highlight the effectiveness of our proposed method in capturing both textual and structural information.</p>
<p>Understanding Node Position Encoding (RQ2)</p>
<p>To understand what node position encoding learns, in this section, we provide visualization for the learned node position embedding on the Cora dataset to gain further insights.We select this dataset because, in Cora, nodes from the same class tend to be naturally closer in the graph structure.This property allows us to directly evaluate the quality of the node position embeddings by observing how well they align with the class labels.Figure 4 illustrates the embeddings before and after the transformation in positional embedding pretraining, shown against class labels.Prior to the transformation, nodes belonging to the same class can be separated distantly in the embedding space.However, after applying the transformation, these nodes are effectively projected into the same region, highlighting the efficacy of our pretraining approach in capturing the underlying semantic relationships among nodes.For instance, the green dots, which are dispersed before the transformation, become densely clustered afterward.</p>
<p>Anchor Selection Strategies Impact (RQ3)</p>
<p>Since anchor nodes offer a comprehensive view of the graph structure, different strategies for identifying anchor nodes may impact NT-LLM's ability to comprehend the graph.In this section, we conduct an extensive evaluation of various anchor selection strategies, on three datasets, i.e., Cora, OGBN-arxiv and OGBL-ddi, using a fixed seed and the NT-LLM architecture.Subsequently, the positional embeddings are pretrained following the same procedure outlined in Section 4.1.3.  4 presents the experimental results.Our method achieves the best performance among all evaluated strategies, surpassing traditional centrality-based approaches (such as Degree and PageRank [46]), random selection, and the landmark-based HPLC [29].</p>
<p>To provide a clearer insight into the advantage of our anchor selection strategy, we compare the anchor nodes selected by different strategies on Cora dataset in Figure 5.The anchor nodes selected by our method are more evenly distributed across the graph structure.In contrast, methods such as Degree, HPLC, Closeness, PageRank, and Eigenvector focus on selecting "important" nodes but fail to provide broad coverage, particularly of nodes located at considerable distances from the graph's central area.inputting all embeddings into the LLM.</p>
<p>Table 5 presents the results of the ablation study, which evaluates the impact of removing individual components from the proposed method.The observed performance drop across all datasets confirms the importance and complementary nature of each component within the method.In particular, we observe that node position encoding pretraining is critical for NT-LLM.The variant without pretraining (w/o Pre) experiences a significant performance drop when the pretraining module is removed, supporting our argument in Section 4.1.3.This is due to the mismatch between shortest path and Euclidean distances, which distorts actual spatial relationships.Therefore, positional embedding pretraining is an indispensable component of NT-LLM.5.6.2Impact of Hyperparameters.We investigate the impact of two key hyperparameters in NT-LLM: the coverage radius  and the coverage ratio .Figure 6 presents the relationships between these hyperparameters, model accuracy and the number of anchor nodes.The results demonstrate that smaller values of  and larger values of  generally lead to a better performance.This trend aligns with the error bound established in Lemma 4.1.Notably, we observed that the number of anchor nodes increases exponentially as  decreases and  increases.This relationship underscores the importance of carefully selecting these hyperparameters to balance computational complexity and model performance.</p>
<p>Tokenizer Efficiency (RQ5)</p>
<p>The only trainable component in our proposed graph tokenizer is a simple MLP, making it intuitively much more efficient than conventional message-passing GNNs or graph transformers.To validate this, we compare the efficiency of various graph tokenizers, including our own, across multiple datasets.The results are summarized in Table 6.The consistently lower number of trainable parameters and training time demonstrate the efficiency of our tokenizer.</p>
<p>Conlusion</p>
<p>In the paper, we propose NT-LLM, an anchor-based graph positional encoding approach that enables efficient graph tokenization for LLMs.Our method preserves crucial structural information through anchor nodes selection without requiring extensive textual descriptions or complex GNNs.Evaluations across diverse benchmarks demonstrate significant improvements across diverse tasks from node classification to complex reasoning, confirming the effectiveness and efficiency of our proposed NT-LLM method.</p>
<p>Figure 1 :
1
Figure1: Overview of our proposed NT-LLM approach.It consists of two steps: (1) Graph Tokenizer: We select key nodes as anchors with a greedy algorithm and compute relative distances between nodes and these anchors to encode the graph structure.The relative distances are then projected into a continuous Euclidean space while preserving the partial ordering of node distances.(2) Task Tuning: We integrate the pretrained embeddings with a large language model using LoRA for task-specific fine-tuning of the LLM, enhancing the performance of downstream graph understanding tasks.</p>
<p>PROOF.Given node pair ,  from graph and a set of anchor nodes A = { 1 ,  2 , . . .,   }, assume  is covered by an anchor node, denoted as  * , then the shortest path distance between them  (,  * ) ≤ .Without loss of generality, we assume  (,  * ) &lt;  ( * , ).Note that the following error bound still holds if  (,  * ) &gt;  ( * , ).The error of the estimated shortest path distance between ,  is bounded by  (, ) = d (, ) −  (, ) =  ∈ A  (, ) +  (, ) −  (, ) ≤  (,  * ) +  ( * , ) −  (, ) ≤  (,  * ) +  ( * , ) − | (,  * ) −  ( * , )| = 2 (,  * ) ≤ 2</p>
<p>Figure 2 :
2
Figure 2: A toy example illustrating the discrepancy between relative distance encoding in non-Euclidean graph space and the required Euclidean space for LLM positional embeddings.</p>
<p>where x,,, (Φ) denotes the difference between the Euclidean distances of the two pairs of mapped embeddings.By maximizing the log-posterior, which is equivalent to minimizing the negative log-likelihood function, we derive the rankpreserving training objective: min Φ L = − ∑︁ (,),(, ) ∈ E I( d (, ) &gt; d (, )) ln  ( x,,, (Φ)) + I( d (, ) ≤ d (, )) ln(1 −  ( x,,, (Φ)))</p>
<ol>
<li>1 . 4
14
Time Complexity Analysis.The time complexity of the greedy algorithm for anchor node selection can be analyzed in two parts: Initialization.Each node performs a BFS to construct its c-hop neighborhood, requiring  (|V | • |E |) time, where |V | is the number of nodes and |E | is the number of edges in the graph.The c-hop neighborhoods are stored for each node.</li>
</ol>
<p>Figure 3 :
3
Figure 3: Illustration of dataset characteristics and LLM-based processing workflow for diverse graph-related tasks employed in our experimental setup.</p>
<p>Figure 4 :
4
Figure 4: Embeddings visualized before and after the transformation in pretraining on Cora dataset.The colors represent the ground truth labels of nodes.</p>
<p>Figure 5 :
5
Figure 5: Distribution of anchor nodes (marked in red) selected by different strategies on the Cora dataset.Our method achieves a more even distribution, effectively covering the peripheral regions of the graph.</p>
<p>Figure 6 :
6
Figure 6: Effects of coverage radius () and coverage ratio () on model accuracy and the number of anchor nodes for the Cora and OGBN-arxiv datasets.The top row shows the impact on model accuracy, while the bottom row illustrates the changes in the number of anchor nodes as  and  vary.</p>
<p>•</p>
<p>We identify and address the issue of misalignment between the non-Euclidean graph space and the Euclidean embedding space, which hinders the effectiveness of graph positional embedding in graph reasoning with LLMs.
• We conduct an extensive empirical evaluation on multiple graphbenchmarks, covering a wide range of task complexities andgraph types. Our results provide insights into the performanceand generalizability of NT-LLM, highlighting its potential foradoption in various graph learning scenarios.</p>
<p>, 5, 30, 44, 61].However, standard GNN architectures often struggle to differentiate among nodes with similar local structures but different positions within the global graph topology.Graph positional encoding addresses this limitation by enhancing node representations with positional information, allowing the capture of important structural features.
2 Related Work2.1 Graph Positional Encoding
[4aph Neural Networks (GNNs) have significantly advanced graph representation learning by enabling the extraction of meaningful embeddings from graph-structured data through message-passing mechanisms[4</p>
<p>Table 1 :
1
Comparative analysis of graph positional encoding techniques, including our proposed method.
Laplacian Eigenmap [7] DeepWalk [48]PGNN [66]HPLC [29]RFP [14]OursEncoding Schemeeigenvectorsrandom walkdistancedistance, eigenvectorsrandom featuredistanceLocal Structure</p>
<p>.1.3Euclidean Projection.While anchor-based encoding enables the representation of spatial positions for nodes in a graph, it is not directly applicable for positional embeddings in LLMs.This is because shortest path distances in graph space do not correspond to distances in Euclidean space, potentially distorting actual spatial relationships.Next, we first elaborate on this argument and then present our solution.</p>
<p>Table 2 :
2
Dataset statistics and evaluation metrics.For OGBGmolhiv and ExplaGraphs, #Nodes and #Edges counts represent averages across all graphs in the dataset.
Dataset#Nodes #Edges #GraphsMetricCora2,70810,5561AccuracyOGBN-arxiv169,343 1,166,2431AccuracyOGBL-ddi4,2671,334,8891Hits@20OGBG-molhiv25.527.541,127ROC-AUCExplaGraphs5.174.252,766Accuracy5.1.2 Baselines. We evaluate our proposed method against variousbaselines, including both traditional graph learning approaches andLLM-based methods:</p>
<p>Table 4 :
4
Comparison of anchor selection strategies across three datasets.The highest performance for each dataset is shown in bold.
StrategyCoraOGBN-arxiv OGBL-ddiDegree0.91720.73120.5731Random0.88910.67830.5019Closeness [3]0.89310.63920.4852Eigenvector [6]0.84240.61050.4736PageRank [46]0.87030.66410.5127Betweenness [17] 0.85390.64280.4967HPLC [29]0.91740.74110.5613Ours0.94780.75250.5904Table</p>
<p>Table 5 :
5
Performance comparison of NT-LLM variants across four datasets.Best results for each dataset are in bold.Impact of Model Components.NT-LLM has specific design features, including the node position encoding, its corresponding pretraining task, and two different strategies for LLMs to leverage node position encoding, i.e., prompt tuning and low-rank adaptation.We evaluate the performance of each variant of our model on five datasets as follows:• w/o PE: The NT-LLM without positional encoding, using raw node features as input to the LLM.• w/o Pre: The NT-LLM without the distance transformation pretraining module, using concrete anchor-based distances as node position embeddings.• w/o PT: The NT-LLM without the prompt tuning module, directly
Variant Cora arxiv ddi molhiv ExplaGraphsNT-LLM 0.9478 0.7525 0.5904 0.75310.9332w/o PE 0.8070 0.6971 0.3592 0.65540.8224w/o Pre 0.8195 0.6538 0.3791 0.64190.7671w/o PT 0.7864 0.5904 0.3460 0.58340.70245.6 Ablation Studies (RQ4)In this section, we conduct extensive ablation studies to investigatethe effectiveness of each component in NT-LLM, and justify ourmodel design choices.5.6.1</p>
<p>Table 6 :
6
Comparison of different tokenization methods based on the number of trainable parameters and training time.
DatasetTokenizerTrainable Parameters Training TimeGAT1.4M2minCoraGraphFormer3.6M10minNT-LLM0.3M&lt;1minGAT21.7M4hOGBN-arxivGraphFormers49.2M6hNT-LLM0.7M10minGAT2.6M3minOGBL-ddiGraphFormers6.1M13minNT-LLM0.5M1min
In this work, we assume that the distance between two adjacent nodes is fixed at 1. The study of weighted graphs, where edge distances may vary, is left for future work.
For OGBG-molhiv, we use the SMILES strings representing molecules as textual attributes, which are not directly provided by OGB.
Cora, being a similar citation network to OGBN-arxiv, was omitted from Figure3to avoid redundancy.
GNNs are unable to perform complex graph reasoning tasks in the ExplaGraphs dataset, thus the corresponding cells are marked with -.
From Anchors to Answers: A Novel Node Tokenizer for Integrating Graph Structure into Large Language Models.In Proceedings of the 34th ACM International Conference on Information and Knowledge Management (CIKM '25), November 10-14, 2025, Seoul, Republic of Korea.ACM, New York, NY, USA, 11 pages.
The Surprising Power of Graph Neural Networks with Random Node Initialization. Ralph Abboud, Martin İsmail İlkan Ceylan, Thomas Grohe, Lukasiewicz, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21. International Joint Conferences on Artificial Intelligence Organization. the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21. International Joint Conferences on Artificial Intelligence Organization2021</p>
<p>LLM Based Generation of Item-Description for Recommendation System. Arkadeep Acharya, Brijraj Singh, Naoyuki Onoe, Proceedings of the 17th ACM Conference on Recommender Systems, RecSys 2023. the 17th ACM Conference on Recommender Systems, RecSys 2023Singapore, SingaporeACM2023. September 18-22, 2023</p>
<p>A mathematical model for group structures. Alex Bavelas, Human organization. 71948. 1948</p>
<p>Directional Graph Networks. Dominique Beaini, Saro Passaro, Vincent Létourneau, Will Hamilton, Gabriele Corso, Pietro Lió, Proceedings of the 38th International Conference on Machine Learning (Proceedings of Machine Learning Research. Marina Meila, Tong Zhang, the 38th International Conference on Machine Learning ( Machine Learning ResearchPMLR2021139</p>
<p>Directional Graph Networks. Dominique Beaini, Saro Passaro, Vincent Létourneau, Will Hamilton, Gabriele Corso, Pietro Lió, Proceedings of the 38th International Conference on Machine Learning (Proceedings of Machine Learning Research. Marina Meila, Tong Zhang, the 38th International Conference on Machine Learning ( Machine Learning ResearchPMLR2021139</p>
<p>Laplacian eigenmaps and spectral techniques for embedding and clustering (NIPS'01). Mikhail Belkin, Partha Niyogi, 2001MIT Press</p>
<p>Laplacian Eigenmaps for Dimensionality Reduction and Data Representation. Mikhail Belkin, Partha Niyogi, Neural Computation. 2003. 2003</p>
<p>InfiniteWalk: Deep Network Embeddings as Laplacian Embeddings with a Nonlinearity. Sudhanshu Chanpuriya, Cameron Musco, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2020. 2020</p>
<p>LLaGA: Large Language and Graph Assistant. Runjin Chen, Tong Zhao, Ajay Kumar Jaiswal, Neil Shah, Zhangyang Wang, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine LearningPMLR2024</p>
<p>Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over Structured Environments. Sitao Cheng, Ziyuan Zhuang, Findings of the Association for Computational Linguistics ACL 2024. Association for Computational Linguistics2024</p>
<p>GraphPOPE: Retaining Structural Graph Information Using Position-aware Node Embeddings. Joran Jeroen Den Boef, Paul Cornelisse, Groth, DL4KG@ISWC. 2021</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Recurrent Distance Filtering for Graph Representation Learning. Yuhui Ding, Antonio Orvieto, Bobby He, Thomas Hofmann, Proceedings of the 41st International Conference on Machine Learning (Proceedings of Machine Learning Research). the 41st International Conference on Machine Learning ( Machine Learning Research)PMLR2024</p>
<p>Graph positional encoding via random feature propagation. Moshe Eliasof, Fabrizio Frasca, Beatrice Bevilacqua, Eran Treister, Ga Chechik, Haggai Maron, Proceedings of the 40th International Conference on Machine Learning (ICML'23). the 40th International Conference on Machine Learning (ICML'23)2023</p>
<p>Talk like a Graph: Encoding Graphs for Large Language Models. Bahare Fatemi, Jonathan Halcrow, Bryan Perozzi, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Fast Graph Representation Learning with PyTorch Geometric. Matthias Fey, ArXiv abs/1903.02428Jan Eric Lenssen. 2019. 2019</p>
<p>Centrality in social networks conceptual clarification. C Linton, Freeman, Social Networks. 11978. 1978</p>
<p>GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking. Jiayan Guo, Lun Du, Hengyu Liu, CoRR abs/2305.150662023. 2023</p>
<p>Inductive representation learning on large graphs. William L Hamilton, Rex Ying, Jure Leskovec, Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS'17). the 31st International Conference on Neural Information Processing Systems (NIPS'17)2017</p>
<p>G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering. Xiaoxin He, Yijun Tian, Yifei Sun, V Nitesh, Thomas Chawla, Yann Laurent, Xavier Lecun, Bryan Bresson, Hooi, arXiv:2402.07630[cs.LG2024</p>
<p>LoRA: Low-Rank Adaptation of Large Language Models. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. 2022. April 25-29, 2022OpenReview.net</p>
<p>Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec, arXiv:2005.00687[cs.LGOpen Graph Benchmark: Datasets for Machine Learning on Graphs. 2021</p>
<p>Yuntong Hu, Zhihan Lei, Zheng Zhang, Bo Pan, Chen Ling, Liang Zhao, arXiv abs/2405.16506GRAG: Graph Retrieval-Augmented Generation. 2024. 2024</p>
<p>Beyond Text: A Deep Dive into Large Language Models' Ability on Understanding Graph Data. Yuntong Hu, Zheng Zhang, Liang Zhao, NeurIPS 2023 Workshop: New Frontiers in Graph Learning. 2023</p>
<p>Large Language Models for Graphs: Progresses and Directions. Chao Huang, Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh Chawla, Companion Proceedings of the ACM on Web Conference 2024. 2024</p>
<p>Can GNN be Good Adapter for LLMs. Xuanwen Huang, Kaiqiao Han, Yang Yang, Dezheng Bao, Quanjin Tao, Ziwei Chai, Qi Zhu, Proceedings of the ACM Web Conference 2024 (WWW '24). the ACM Web Conference 2024 (WWW '24)Association for Computing Machinery2024</p>
<p>Tender Document Analyzer with the Combination of Supervised Learning and LLM-based Improver. Tomoki Ito, Shun Nakagawa, Companion Proceedings of the ACM on Web Conference 2024, WWW 2024. Singapore, SingaporeACM2024. May 13-17, 2024</p>
<p>Heterformer: Transformerbased Deep Node Representation Learning on Heterogeneous Text-Rich Networks. Jin Bowen, Yu Zhang, Qi Zhu, Jiawei Han, 10.1145/3580305.3599376Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data MiningLong Beach, CA, USA; New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Hierarchical Position Embedding of Graphs with Landmarks and Clustering for Link Prediction. Minsang Kim, Seung Baek, Proceedings of the ACM Web Conference 2024. the ACM Web Conference 2024202424</p>
<p>Semi-Supervised Classification with Graph Convolutional Networks. Thomas N Kipf, Max Welling, 5th International Conference on Learning Representations, ICLR 2017. Conference Track Proceedings. OpenReview.net. Toulon, France2017. April 24-26, 2017</p>
<p>Semi-Supervised Classification with Graph Convolutional Networks. Thomas N Kipf, Max Welling, 5th International Conference on Learning Representations. Conference Track Proceedings. Toulon, France2017. 2017. April 24-26, 2017</p>
<p>The Power of Scale for Parameter-Efficient Prompt Tuning. Brian Lester, Rami Al-Rfou, Noah Constant, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Marie-Francine Moens, Xuanjing Huang, Lucia Specia, Scott , Wen-Tau Yih, the 2021 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2021</p>
<p>Distance encoding: design provably more powerful neural networks for graph representation learning. Pan Li, Yanbang Wang, Hongwei Wang, Jure Leskovec, Proceedings of the 34th International Conference on Neural Information Processing Systems (NIPS '20). the 34th International Conference on Neural Information Processing Systems (NIPS '20)2020</p>
<p>GRENADE: Graph-Centric Language Model for Self-Supervised Representation Learning on Text-Attributed Graphs. Yichuan Li, Kaize Ding, Kyumin Lee, Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>GLBench: A Comprehensive Benchmark for Graph with Large Language Models. Yuhan Li, Peisong Wang, Xiao Zhu, Aochuan Chen, Haiyun Jiang, Deng Cai, Wai Kin ; Chan, Jia Li, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems. Ulrich Paquet, M Jakub, Cheng Tomczak, Zhang, Vancouver, BC, Canada; Lester Mackey, Danielle Belgrave, Angela FanAmir Globersons2024. 2024. December 10 -15, 20242024</p>
<p>FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering. Zhenyu Li, Sunqi Fan, Yu Gu, Xiuxing Li, Zhichao Duan, Bowen Dong, Ning Liu, Jianyong Wang, Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence. Vancouver, CanadaAAAI Press2024. February 20-27, 20242014</p>
<p>Data-efficient Fine-tuning for LLM-based Recommendation. Xinyu Lin, Wenjie Wang, Yongqi Li, Shuo Yang, Fuli Feng, Yinwei Wei, Tat-Seng Chua, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information RetrievalWashington DC, USA202424</p>
<p>Guangyi Liu, Yongqi Zhang, Yong Li, Quanming Yao, arXiv:2406.01145[cs.CLDual Reasoning: A GNN-LLM Collaborative Framework for Knowledge Graph Question Answering. 2025</p>
<p>Improved Baselines with Visual Instruction Tuning. Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee, 10.1109/CVPR52733.2024.024842024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2024</p>
<p>Graph Foundation Models: Concepts, Opportunities and Challenges. Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S Yu, Chuan Shi, 10.1109/TPAMI.2025.3548729IEEE Transactions on Pattern Analysis and Machine Intelligence. 472025. 2025</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692[cs.CL]RoBERTa: A Robustly Optimized BERT Pretraining Approach. 2019</p>
<p>Decoupled Weight Decay Regularization. Ilya Loshchilov, Frank Hutter, International Conference on Learning Representations. 2017</p>
<p>Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning. Linhao Luo, Yuan-Fang Li, Reza Haf, Shirui Pan, The Twelfth International Conference on Learning Representations, ICLR 2024. Vienna, Austria2024. May 7-11, 2024</p>
<p>Path integral based convolution and pooling for graph neural networks. Zheng Ma, Junyu Xuan, Yu Guang Wang, Ming Li, Pietro Liò, 2020NIPS '20</p>
<p>The PageRank Citation Ranking : Bringing Order to the Web. Lawrence Page, Sergey Brin, Rajeev Motwani, Terry Winograd, The Web Conference. 1999</p>
<p>PyTorch: an imperative style, high-performance deep learning library. Adam Paszke, 2019</p>
<p>DeepWalk: online learning of social representations. Bryan Perozzi, Rami Al-Rfou, Steven S Skiena, Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. the 20th ACM SIGKDD international conference on Knowledge discovery and data mining2014. 2014</p>
<p>Querying shortest path distance with bounded errors in large graphs. Miao Qiao, Hong Cheng, Jeffrey Xu, Yu , Proceedings of the 23rd International Conference on Scientific and Statistical Database Management (SSDBM'11). the 23rd International Conference on Scientific and Statistical Database Management (SSDBM'11)2011</p>
<p>Char-acterMeet: Supporting Creative Writers' Entire Story Character Construction Processes Through Conversation with LLM-Powered Chatbot Avatars. Shan Hua Xuan Qin, Ze Jin, Mingming Gao, Pan Fan, Hui, Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI 2024. the CHI Conference on Human Factors in Computing Systems, CHI 2024Honolulu, HI, USAACM2024. May 11-16, 2024</p>
<p>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. Nils Reimers, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>A Survey of Large Language Models for Graphs. Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh Chawla, Chao Huang, 10.1145/3637528.3671460Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data MiningBarcelona, Spain; New York, NY, USAAssociation for Computing Machinery2024</p>
<p>Leveraging Large Language Models for Multiple Choice Question Answering. Joshua Robinson, David Wingate, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Expla-Graphs: An Explanation Graph Generation Task for Structured Commonsense Reasoning. Swarnadeep Saha, Prateek Yadav, Lisa Bauer, Mohit Bansal, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2021</p>
<p>Collective Classification in Network Data. Prithviraj Sen, Mark Galileo, Mustafa Namata, Lise Bilgic, Brian Getoor, Tina Gallagher, Eliassi-Rad, AI Magazine. 292008. 2008</p>
<p>WalkLM: a uniform language model fine-tuning framework for attributed graph embedding. Yanchao Tan, Zihao Zhou, Hang Lv, Weiming Liu, Carl Yang, Proceedings of the 37th International Conference on Neural Information Processing Systems (NIPS '23). the 37th International Conference on Neural Information Processing Systems (NIPS '23)2024</p>
<p>Graphgpt: Graph instruction tuning for large language models. Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, Chao Huang, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>Higpt: Heterogeneous graph language model. Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Long Xia, Dawei Yin, Chao Huang, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>Hugo Touvron, Louis Martin, arXiv:2407.21783[cs.AIThe Llama 3 Herd of Models. 2024</p>
<p>Graph Attention Networks. Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio, 6th International Conference on Learning Representations, ICLR 2018. Conference Track Proceedings. Vancouver, BC, Canada2018. April 30 -May 3, 2018</p>
<p>Powerful Graph Convolutional Networks with Adaptive Propagation Mechanism for Homophily and Heterophily. Tao Wang, Di Jin, Rui Wang, Dongxiao He, Yuxiao Huang, Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event. AAAI Press2022. February 22 -March 1, 2022</p>
<p>How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey Macmillan, Noah A Smith, Iz Beltagy, Hannaneh Hajishirzi, Advances in Neural Information Processing Systems. T Oh, A Naumann, K Globerson, M Saenko, S Hardt, Levine, Curran Associates, Inc202336</p>
<p>Augmenting Low-Resource Text Classification with Graph-Grounded Pre-training and Prompting. Zhihao Wen, Yuan Fang, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '23). the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '23)Association for Computing Machinery2023</p>
<p>HuggingFace's Transformers: State-of-the-art Natural Language Processing. Thomas Wolf, Lysandre Debut, CoRR abs/1910.037712019. 2019</p>
<p>GraphFormers: GNN-nested transformers for representation learning on textual graph. Junhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo Li, Defu Lian, Sanjay Agrawal, Amit Singh, Guangzhong Sun, Xing Xie, Proceedings of the 35th International Conference on Neural Information Processing Systems (NIPS '21). the 35th International Conference on Neural Information Processing Systems (NIPS '21)2021</p>
<p>Position-aware Graph Neural Networks. Jiaxuan You, Rex Ying, Jure Leskovec, International Conference on Machine Learning. 2019</p>
<p>GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks. Mengmei Zhang, Mingwei Sun, Peng Wang, Shen Fan, Yanhu Mo, Xiaoxiao Xu, Hong Liu, Cheng Yang, Chuan Shi, The Web Conference. 2024. 2024</p>
<p>DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in Understanding Financial Documents. Yilun Zhao, Yitao Long, Hongjun Liu, Ryo Kamoi, Linyong Nan, Lyuhao Chen, Yixin Liu, Xiangru Tang, Rui Zhang, Arman Cohan, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024. August 11-16, 20241ACL 2024</p>
<p>Scalable graph embedding for asymmetric proximity. Chang Zhou, Yuqiong Liu, Xiaofei Liu, Zhongyi Liu, Jun Gao, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI'17). the Thirty-First AAAI Conference on Artificial Intelligence (AAAI'17)2017</p>
<p>Efficient Tuning and Inference for Large Language Models on Textual Graphs. Yun Zhu, Yaoke Wang, Haizhou Shi, Siliang Tang, Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence. the Thirty-Third International Joint Conference on Artificial Intelligence202424</p>            </div>
        </div>

    </div>
</body>
</html>
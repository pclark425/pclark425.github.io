<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3088 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3088</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3088</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-267658066</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.09193v2.pdf" target="_blank">(Ir)rationality and cognitive biases in large language models</a></p>
                <p><strong>Paper Abstract:</strong> Do large language models (LLMs) display rational reasoning? LLMs have been shown to contain human biases due to the data they have been trained on; whether this is reflected in rational reasoning remains less clear. In this paper, we answer this question by evaluating seven language models using tasks from the cognitive psychology literature. We find that, like humans, LLMs display irrationality in these tasks. However, the way this irrationality is displayed does not reflect that shown by humans. When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases. On top of this, the LLMs reveal an additional layer of irrationality in the significant inconsistency of the responses. Aside from the experimental results, this paper seeks to make a methodological contribution by showing how we can assess and compare different capabilities of these types of models, in this case with respect to rational reasoning.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3088.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3088.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's state-of-the-art large language model evaluated in this paper; produced the highest proportion of correct answers with correct logical reasoning across the cognitive tasks tested.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI transformer-based large language model (GPT family) accessed via OpenAI API for zero-shot evaluation on cognitive psychology tasks; no model parameter changes were made in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot prompting (classic task prompts)', 'classic vs facilitated prompt variants (task framing)', 'repeated-run sampling (10 independent runs per task to measure consistency)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>All GPT-4 evaluations used zero-shot prompting with the classic cognitive task wording (and facilitated variants where applicable). Each task was prompted 10 times to assess response consistency; no chain-of-thought or few-shot examples were provided in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single/similar style — GPT-4 was evaluated only in zero-shot mode with classic and facilitated prompt variants; the paper did not apply distinct internal prompting strategies (e.g., chain-of-thought) to GPT-4, so observed behavior arises from a single prompting style plus task framing.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Battery of 12 cognitive psychology tasks (e.g., Monty Hall, Wason selection task, Linda/conjunction, AIDS/Bayesian, birth-order, Monty Hall facilitated, Wason facilitated, Monty Hall facilitated, mathematical and non-mathematical variants)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Twelve tasks from cognitive psychology (Kahneman & Tversky, Wason, Eddy, Friedman and facilitated versions) designed to probe rational vs biased (human-like) reasoning; each task was asked 10 times per model in zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Zero-shot/classic+facilitated (aggregated): 69.2% of runs produced correct answers with correct logical reasoning (paper's primary metric). Combined 'human-like' responses (correct or incorrect that match studied human biases) were 73.3% for GPT-4. The paper reports higher success on non-mathematical tasks than mathematical ones but does not break GPT-4 performance by task-type numerically beyond the aggregated numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>No direct ablation contrasting distinct internal reasoning prompts for GPT-4 (e.g., chain-of-thought vs none) was performed. The paper compares classic vs facilitated task framings and finds facilitated versions rarely improved performance across models; GPT-4 remained the strongest performer under the single (zero-shot) prompting regime.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4 achieved the highest rate of correct answers with correct reasoning (69.2%), produced the most human-like overall response distribution, but still showed inconsistency across runs; improvements from facilitated prompt variants were limited (AIDS task an exception).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Although GPT-4 often produced correct reasoning, the paper notes inconsistency: the same model sometimes produced basic mathematical errors or illogical reasoning in other runs. No evidence in this study that applying diverse explicit reasoning methods (e.g., chain-of-thought) was used to improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(Ir)rationality and cognitive biases in large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3088.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3088.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-3.5 evaluated as a baseline in the study; produced substantially fewer runs with correct logical reasoning than GPT-4 and showed the highest proportion of responses classified as human-like incorrect among some models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI transformer-based model (GPT-3.5) accessed via OpenAI API for zero-shot evaluation; default parameters maintained.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot prompting (classic and facilitated task wording)', 'repeated-run sampling (10 runs per task)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Evaluated only in zero-shot mode using the original and facilitated formulations of each cognitive task, sampled multiple times to measure variability; no chain-of-thought or few-shot in-context examples were provided for the experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single/similar style — GPT-3.5 was only tested under zero-shot prompting and task-framing variants, so it did not demonstrate use of diverse internal reasoning strategies in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Same 12 cognitive psychology tasks</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks probing probabilistic, logical and decision biases (e.g., conjunction fallacy, Wason selection, Monty Hall, birth-order, AIDS/Bayesian reasoning), asked repeatedly in zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Zero-shot/classic+facilitated (aggregated): 29.2% of runs produced correct answers with correct logical reasoning. 'Human-like' incorrect responses occurred in 21.7% of runs; combined human-like (correct+human-like incorrect) about 50.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Paper did not apply multiple explicit reasoning strategies to GPT-3.5; comparison limited to classic vs facilitated task framings and to cross-model comparisons. Facilitated variants did not consistently improve GPT-3.5 performance except in some tasks (AIDS task exception across models).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3.5 shows a substantial share of human-like responses relative to many other models (when combined with correct human-like answers ~50.8%), but lower absolute rates of correct logical reasoning than GPT-4; responses are inconsistent across repeated runs.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>The majority of incorrect GPT-3.5 responses were non-human-like (illogical or plainly incorrect), indicating failures are not primarily due to human cognitive biases but other reasoning errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(Ir)rationality and cognitive biases in large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3088.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3088.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bard (LaMDA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google Bard (LaMDA-powered)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's conversational model (powered by LaMDA) evaluated in the study; displayed intermediate performance with notable failures on mathematical tasks where correct answers often came with illogical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Bard (LaMDA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Google's LaMDA-powered Bard chatbot accessed through its online interface in default settings; evaluated in zero-shot on the cognitive task battery without parameter changes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot prompting (classic and facilitated variants)', 'repeated-run sampling (10 runs per task)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Bard was prompted in zero-shot using the same classic and facilitated task wordings. The paper additionally reports analysis by task-type (mathematical vs non-mathematical) showing differing reasoning quality.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single/similar style — Bard was tested only in the paper's zero-shot setup and task framings; no deliberate variety of reasoning prompting strategies was applied.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Same 12 cognitive psychology tasks (including mathematical and non-mathematical variants)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Cognitive tasks probing probabilistic and logical reasoning and specific biases (e.g., Monty Hall, Wason selection, AIDS/Bayesian).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Zero-shot/classic+facilitated (aggregated): 35.8% of runs produced correct answers with correct logical reasoning. For mathematical tasks, Bard produced correct final answers more often with illogical reasoning (39%) than with logical reasoning (20%), per the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>No experiment in the paper applied multiple internal reasoning strategies for Bard; comparison is across task framings (classic vs facilitated) and across task types (mathematical vs non-mathematical), with non-mathematical tasks showing higher correct+logical proportions generally.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Bard's performance suffers especially on mathematical tasks where correct answers are frequently paired with illogical or incorrect internal reasoning; facilitated prompts rarely improved overall performance beyond specific cases (AIDS).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Despite being intermediate overall, Bard sometimes produced correct answers with illogical reasoning more often than logically reasoned correct answers for math tasks — a negative result for reliability of reasoning style.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(Ir)rationality and cognitive biases in large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3088.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3088.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anthropic Claude 2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Anthropic's Claude 2 conversational model evaluated on the cognitive task battery; produced the second-highest rate of correct answers with correct reasoning after GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic's Claude 2 chatbot accessed via its online interface using default settings for zero-shot evaluation on the cognitive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot prompting (classic and facilitated versions)', 'repeated-run sampling (10 runs per task)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>All evaluations used zero-shot prompts with either the classic or facilitated task wording; each task was run ten times to sample output variability.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single/similar style — Claude 2 was only exercised under zero-shot prompting and task framing variants; the study did not apply multiple distinct prompting strategies to it.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Same 12 cognitive psychology tasks</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks designed to probe rational vs biased reasoning; included mathematical and non-mathematical items as well as facilitated reformulations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Zero-shot/classic+facilitated (aggregated): 55.0% of runs produced correct answers with correct logical reasoning (per aggregated results). Combined human-like responses (correct+human-like incorrect) approximately 67.5% (R 55.0% + H 12.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>No internal method ablations performed; comparison limited to classic vs facilitated task framings and cross-model performance. Facilitated versions largely did not improve performance except for the AIDS task (across most models except Llama variants).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Claude 2 is the second-best of the evaluated models on correct+logical reasoning (55%), showing more human-like response patterns than most Llama 2 variants but less than GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Claude 2 did not identify most test problems as known puzzles (paper reports only Monty Hall frequently identified across models), and like others showed inconsistencies across repeated runs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(Ir)rationality and cognitive biases in large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3088.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3088.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta Llama 2 (7 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The 7B parameter Llama 2 model from Meta evaluated via its chatbot interface; performed poorly overall and frequently produced incorrect non-human-like responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2 (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Meta's Llama 2 7B parameter conversational model accessed through its online interface with the model's default system prompt enabled initially (the paper removed that prompt in later runs because it caused refusal behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot prompting (classic and facilitated task wording)', 'use and removal of default system prompt (safety/system message)', 'repeated-run sampling (10 runs per task)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Llama 2 7B was tested in zero-shot; the default system prompt instructing safe/respectful behavior was present initially and led to refusals, so the authors removed it for many runs to obtain responses. Each task was prompted 10 times to sample variability.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single/similar style — testing used only zero-shot prompts with and without the default system prompt; no other reasoning prompting strategies were applied.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Same 12 cognitive psychology tasks</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Classic and facilitated cognitive tests probing biases and probabilistic/logical reasoning; Llama 2 7B responses were often incorrect and non-human-like.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Zero-shot/classic+facilitated (aggregated): only 2.5% of runs produced correct answers with correct logical reasoning. The model produced incorrect responses in ~77.5% of runs (text-stated aggregated incorrect rate). Many incorrect responses were non-human-like (illogical).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Paper compared runs with and without the default system prompt; inclusion of the safety/system prompt led to many refusals rather than improved reasoning. Facilitated task variants did not generally rescue performance for Llama 2 7B.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Llama 2 7B performed worst among tested models, with a high rate of incorrect non-human-like answers and extreme inconsistency; default system prompts caused many refusals, and removing them changed availability of outputs but not overall reasoning quality.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Removing the safety/system prompt enabled responses but did not meaningfully improve correct logical reasoning; facilitated versions of tasks did not improve Llama 2 7B performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(Ir)rationality and cognitive biases in large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3088.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3088.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta Llama 2 (13 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The 13B parameter Llama 2 model evaluated in the study; low proportion of human-like responses and poor correctness with logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2 (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Meta's Llama 2 13B parameter conversational model accessed through its online interface with a default system prompt; the authors removed the system prompt for many runs to obtain answers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot prompting (classic and facilitated task wording)', 'default system prompt handling (removed in many runs)', 'repeated-run sampling (10 runs per task)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Evaluated in zero-shot; the default safety/system prompt initially caused refusals so it was removed for many runs. Tasks were sampled multiple times to assess variability.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single/similar style — only zero-shot prompting and task framing variants were used; no alternative internal reasoning prompts were applied.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Same 12 cognitive psychology tasks</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Classic and facilitated versions of cognitive reasoning tasks probing heuristics and biases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Zero-shot/classic+facilitated (aggregated): 5.0% of runs produced correct answers with correct logical reasoning. Combined human-like responses (correct+human-like incorrect) only 8.3% overall (very low).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Comparison primarily across the presence/absence of the default system prompt and classic vs facilitated task framings; neither substantially improved reasoning correctness for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Llama 2 13B produced very few human-like responses and had low correct logical reasoning rates; removing the system prompt was necessary to obtain responses but did not meaningfully raise performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>The facilitated task variants and prompt adjustments did not yield improved reasoning; the model performed poorly on mathematical tasks (no correct math-task runs reported).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(Ir)rationality and cognitive biases in large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3088.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3088.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta Llama 2 (70 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The 70B parameter Llama 2 model evaluated; showed somewhat better correct+logical rates than smaller Llama variants but often refused to answer (system prompt behavior) and remained inconsistent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2 (70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Meta's Llama 2 70B parameter model accessed via its online chatbot interface; the default system prompt appeared embedded and led to many refusals to answer in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot prompting (classic and facilitated task wording)', 'default system prompt behaviour (embedded; many refusals observed)', 'repeated-run sampling (10 runs per task)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Evaluations used zero-shot classic and facilitated prompts. The 70B model frequently refused to answer (41.7% refusal rate reported); when it did answer it was sampled across multiple runs.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single/similar style — tested only in zero-shot with task framings; no additional prompting strategies were applied.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Same 12 cognitive psychology tasks</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks probing biases and rational reasoning; the model refused to answer many prompts due to safety/system instructions embedded in the model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Zero-shot/classic+facilitated (aggregated): 15.0% of runs produced correct answers with correct logical reasoning. Refusal/no-answer rate was high: 41.7% of runs resulted in no answer or refusal. On mathematical tasks, 70B produced only one correct response across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Paper compared runs in the presence of embedded system prompt behavior (70B) to the 7B/13B variants where the prompt was removed; the 70B behaved similarly to the prompted smaller models and often refused, indicating system prompt effects are important but removal did not appear to be possible for 70B in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Llama 2 70B performed better than smaller Llama variants on correct+logical metric but suffered from a high refusal rate (embedded safety prompt), producing inconsistent outputs and near-zero success on mathematical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Despite being much larger, the 70B model did not reliably outperform the smaller Llama variants when considering refusals and inconsistencies; removal of system prompt (not possible for 70B in this study) seemed to be required to elicit outputs but embedding of safety prompts may be baked in.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(Ir)rationality and cognitive biases in large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Using cognitive psychology to understand GPT-3 <em>(Rating: 2)</em></li>
                <li>Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT <em>(Rating: 2)</em></li>
                <li>Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias <em>(Rating: 2)</em></li>
                <li>The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs <em>(Rating: 2)</em></li>
                <li>Language models show human-like content effects on reasoning tasks <em>(Rating: 2)</em></li>
                <li>Sparks of Artificial General Intelligence: Early experiments with GPT-4 <em>(Rating: 1)</em></li>
                <li>Emergent analogical reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Inductive reasoning in humans and large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3088",
    "paper_id": "paper-267658066",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "OpenAI's state-of-the-art large language model evaluated in this paper; produced the highest proportion of correct answers with correct logical reasoning across the cognitive tasks tested.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI transformer-based large language model (GPT family) accessed via OpenAI API for zero-shot evaluation on cognitive psychology tasks; no model parameter changes were made in this study.",
            "model_size": null,
            "reasoning_methods": [
                "zero-shot prompting (classic task prompts)",
                "classic vs facilitated prompt variants (task framing)",
                "repeated-run sampling (10 independent runs per task to measure consistency)"
            ],
            "reasoning_methods_description": "All GPT-4 evaluations used zero-shot prompting with the classic cognitive task wording (and facilitated variants where applicable). Each task was prompted 10 times to assess response consistency; no chain-of-thought or few-shot examples were provided in these experiments.",
            "diversity_of_methods": "single/similar style — GPT-4 was evaluated only in zero-shot mode with classic and facilitated prompt variants; the paper did not apply distinct internal prompting strategies (e.g., chain-of-thought) to GPT-4, so observed behavior arises from a single prompting style plus task framing.",
            "reasoning_task_name": "Battery of 12 cognitive psychology tasks (e.g., Monty Hall, Wason selection task, Linda/conjunction, AIDS/Bayesian, birth-order, Monty Hall facilitated, Wason facilitated, Monty Hall facilitated, mathematical and non-mathematical variants)",
            "reasoning_task_description": "Twelve tasks from cognitive psychology (Kahneman & Tversky, Wason, Eddy, Friedman and facilitated versions) designed to probe rational vs biased (human-like) reasoning; each task was asked 10 times per model in zero-shot.",
            "performance_by_method": "Zero-shot/classic+facilitated (aggregated): 69.2% of runs produced correct answers with correct logical reasoning (paper's primary metric). Combined 'human-like' responses (correct or incorrect that match studied human biases) were 73.3% for GPT-4. The paper reports higher success on non-mathematical tasks than mathematical ones but does not break GPT-4 performance by task-type numerically beyond the aggregated numbers.",
            "comparison_of_methods": "No direct ablation contrasting distinct internal reasoning prompts for GPT-4 (e.g., chain-of-thought vs none) was performed. The paper compares classic vs facilitated task framings and finds facilitated versions rarely improved performance across models; GPT-4 remained the strongest performer under the single (zero-shot) prompting regime.",
            "key_findings": "GPT-4 achieved the highest rate of correct answers with correct reasoning (69.2%), produced the most human-like overall response distribution, but still showed inconsistency across runs; improvements from facilitated prompt variants were limited (AIDS task an exception).",
            "counter_examples_or_negative_results": "Although GPT-4 often produced correct reasoning, the paper notes inconsistency: the same model sometimes produced basic mathematical errors or illogical reasoning in other runs. No evidence in this study that applying diverse explicit reasoning methods (e.g., chain-of-thought) was used to improve performance.",
            "uuid": "e3088.0",
            "source_info": {
                "paper_title": "(Ir)rationality and cognitive biases in large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "Generative Pre-trained Transformer 3.5",
            "brief_description": "OpenAI's GPT-3.5 evaluated as a baseline in the study; produced substantially fewer runs with correct logical reasoning than GPT-4 and showed the highest proportion of responses classified as human-like incorrect among some models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "OpenAI transformer-based model (GPT-3.5) accessed via OpenAI API for zero-shot evaluation; default parameters maintained.",
            "model_size": null,
            "reasoning_methods": [
                "zero-shot prompting (classic and facilitated task wording)",
                "repeated-run sampling (10 runs per task)"
            ],
            "reasoning_methods_description": "Evaluated only in zero-shot mode using the original and facilitated formulations of each cognitive task, sampled multiple times to measure variability; no chain-of-thought or few-shot in-context examples were provided for the experiments in this paper.",
            "diversity_of_methods": "single/similar style — GPT-3.5 was only tested under zero-shot prompting and task-framing variants, so it did not demonstrate use of diverse internal reasoning strategies in this work.",
            "reasoning_task_name": "Same 12 cognitive psychology tasks",
            "reasoning_task_description": "Tasks probing probabilistic, logical and decision biases (e.g., conjunction fallacy, Wason selection, Monty Hall, birth-order, AIDS/Bayesian reasoning), asked repeatedly in zero-shot.",
            "performance_by_method": "Zero-shot/classic+facilitated (aggregated): 29.2% of runs produced correct answers with correct logical reasoning. 'Human-like' incorrect responses occurred in 21.7% of runs; combined human-like (correct+human-like incorrect) about 50.8%.",
            "comparison_of_methods": "Paper did not apply multiple explicit reasoning strategies to GPT-3.5; comparison limited to classic vs facilitated task framings and to cross-model comparisons. Facilitated variants did not consistently improve GPT-3.5 performance except in some tasks (AIDS task exception across models).",
            "key_findings": "GPT-3.5 shows a substantial share of human-like responses relative to many other models (when combined with correct human-like answers ~50.8%), but lower absolute rates of correct logical reasoning than GPT-4; responses are inconsistent across repeated runs.",
            "counter_examples_or_negative_results": "The majority of incorrect GPT-3.5 responses were non-human-like (illogical or plainly incorrect), indicating failures are not primarily due to human cognitive biases but other reasoning errors.",
            "uuid": "e3088.1",
            "source_info": {
                "paper_title": "(Ir)rationality and cognitive biases in large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Bard (LaMDA)",
            "name_full": "Google Bard (LaMDA-powered)",
            "brief_description": "Google's conversational model (powered by LaMDA) evaluated in the study; displayed intermediate performance with notable failures on mathematical tasks where correct answers often came with illogical reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Bard (LaMDA)",
            "model_description": "Google's LaMDA-powered Bard chatbot accessed through its online interface in default settings; evaluated in zero-shot on the cognitive task battery without parameter changes.",
            "model_size": null,
            "reasoning_methods": [
                "zero-shot prompting (classic and facilitated variants)",
                "repeated-run sampling (10 runs per task)"
            ],
            "reasoning_methods_description": "Bard was prompted in zero-shot using the same classic and facilitated task wordings. The paper additionally reports analysis by task-type (mathematical vs non-mathematical) showing differing reasoning quality.",
            "diversity_of_methods": "single/similar style — Bard was tested only in the paper's zero-shot setup and task framings; no deliberate variety of reasoning prompting strategies was applied.",
            "reasoning_task_name": "Same 12 cognitive psychology tasks (including mathematical and non-mathematical variants)",
            "reasoning_task_description": "Cognitive tasks probing probabilistic and logical reasoning and specific biases (e.g., Monty Hall, Wason selection, AIDS/Bayesian).",
            "performance_by_method": "Zero-shot/classic+facilitated (aggregated): 35.8% of runs produced correct answers with correct logical reasoning. For mathematical tasks, Bard produced correct final answers more often with illogical reasoning (39%) than with logical reasoning (20%), per the paper.",
            "comparison_of_methods": "No experiment in the paper applied multiple internal reasoning strategies for Bard; comparison is across task framings (classic vs facilitated) and across task types (mathematical vs non-mathematical), with non-mathematical tasks showing higher correct+logical proportions generally.",
            "key_findings": "Bard's performance suffers especially on mathematical tasks where correct answers are frequently paired with illogical or incorrect internal reasoning; facilitated prompts rarely improved overall performance beyond specific cases (AIDS).",
            "counter_examples_or_negative_results": "Despite being intermediate overall, Bard sometimes produced correct answers with illogical reasoning more often than logically reasoned correct answers for math tasks — a negative result for reliability of reasoning style.",
            "uuid": "e3088.2",
            "source_info": {
                "paper_title": "(Ir)rationality and cognitive biases in large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Claude 2",
            "name_full": "Anthropic Claude 2",
            "brief_description": "Anthropic's Claude 2 conversational model evaluated on the cognitive task battery; produced the second-highest rate of correct answers with correct reasoning after GPT-4.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Claude 2",
            "model_description": "Anthropic's Claude 2 chatbot accessed via its online interface using default settings for zero-shot evaluation on the cognitive tasks.",
            "model_size": null,
            "reasoning_methods": [
                "zero-shot prompting (classic and facilitated versions)",
                "repeated-run sampling (10 runs per task)"
            ],
            "reasoning_methods_description": "All evaluations used zero-shot prompts with either the classic or facilitated task wording; each task was run ten times to sample output variability.",
            "diversity_of_methods": "single/similar style — Claude 2 was only exercised under zero-shot prompting and task framing variants; the study did not apply multiple distinct prompting strategies to it.",
            "reasoning_task_name": "Same 12 cognitive psychology tasks",
            "reasoning_task_description": "Tasks designed to probe rational vs biased reasoning; included mathematical and non-mathematical items as well as facilitated reformulations.",
            "performance_by_method": "Zero-shot/classic+facilitated (aggregated): 55.0% of runs produced correct answers with correct logical reasoning (per aggregated results). Combined human-like responses (correct+human-like incorrect) approximately 67.5% (R 55.0% + H 12.5%).",
            "comparison_of_methods": "No internal method ablations performed; comparison limited to classic vs facilitated task framings and cross-model performance. Facilitated versions largely did not improve performance except for the AIDS task (across most models except Llama variants).",
            "key_findings": "Claude 2 is the second-best of the evaluated models on correct+logical reasoning (55%), showing more human-like response patterns than most Llama 2 variants but less than GPT-4.",
            "counter_examples_or_negative_results": "Claude 2 did not identify most test problems as known puzzles (paper reports only Monty Hall frequently identified across models), and like others showed inconsistencies across repeated runs.",
            "uuid": "e3088.3",
            "source_info": {
                "paper_title": "(Ir)rationality and cognitive biases in large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Llama2-7B",
            "name_full": "Meta Llama 2 (7 billion parameters)",
            "brief_description": "The 7B parameter Llama 2 model from Meta evaluated via its chatbot interface; performed poorly overall and frequently produced incorrect non-human-like responses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 2 (7B)",
            "model_description": "Meta's Llama 2 7B parameter conversational model accessed through its online interface with the model's default system prompt enabled initially (the paper removed that prompt in later runs because it caused refusal behavior).",
            "model_size": "7B",
            "reasoning_methods": [
                "zero-shot prompting (classic and facilitated task wording)",
                "use and removal of default system prompt (safety/system message)",
                "repeated-run sampling (10 runs per task)"
            ],
            "reasoning_methods_description": "Llama 2 7B was tested in zero-shot; the default system prompt instructing safe/respectful behavior was present initially and led to refusals, so the authors removed it for many runs to obtain responses. Each task was prompted 10 times to sample variability.",
            "diversity_of_methods": "single/similar style — testing used only zero-shot prompts with and without the default system prompt; no other reasoning prompting strategies were applied.",
            "reasoning_task_name": "Same 12 cognitive psychology tasks",
            "reasoning_task_description": "Classic and facilitated cognitive tests probing biases and probabilistic/logical reasoning; Llama 2 7B responses were often incorrect and non-human-like.",
            "performance_by_method": "Zero-shot/classic+facilitated (aggregated): only 2.5% of runs produced correct answers with correct logical reasoning. The model produced incorrect responses in ~77.5% of runs (text-stated aggregated incorrect rate). Many incorrect responses were non-human-like (illogical).",
            "comparison_of_methods": "Paper compared runs with and without the default system prompt; inclusion of the safety/system prompt led to many refusals rather than improved reasoning. Facilitated task variants did not generally rescue performance for Llama 2 7B.",
            "key_findings": "Llama 2 7B performed worst among tested models, with a high rate of incorrect non-human-like answers and extreme inconsistency; default system prompts caused many refusals, and removing them changed availability of outputs but not overall reasoning quality.",
            "counter_examples_or_negative_results": "Removing the safety/system prompt enabled responses but did not meaningfully improve correct logical reasoning; facilitated versions of tasks did not improve Llama 2 7B performance.",
            "uuid": "e3088.4",
            "source_info": {
                "paper_title": "(Ir)rationality and cognitive biases in large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Llama2-13B",
            "name_full": "Meta Llama 2 (13 billion parameters)",
            "brief_description": "The 13B parameter Llama 2 model evaluated in the study; low proportion of human-like responses and poor correctness with logical reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 2 (13B)",
            "model_description": "Meta's Llama 2 13B parameter conversational model accessed through its online interface with a default system prompt; the authors removed the system prompt for many runs to obtain answers.",
            "model_size": "13B",
            "reasoning_methods": [
                "zero-shot prompting (classic and facilitated task wording)",
                "default system prompt handling (removed in many runs)",
                "repeated-run sampling (10 runs per task)"
            ],
            "reasoning_methods_description": "Evaluated in zero-shot; the default safety/system prompt initially caused refusals so it was removed for many runs. Tasks were sampled multiple times to assess variability.",
            "diversity_of_methods": "single/similar style — only zero-shot prompting and task framing variants were used; no alternative internal reasoning prompts were applied.",
            "reasoning_task_name": "Same 12 cognitive psychology tasks",
            "reasoning_task_description": "Classic and facilitated versions of cognitive reasoning tasks probing heuristics and biases.",
            "performance_by_method": "Zero-shot/classic+facilitated (aggregated): 5.0% of runs produced correct answers with correct logical reasoning. Combined human-like responses (correct+human-like incorrect) only 8.3% overall (very low).",
            "comparison_of_methods": "Comparison primarily across the presence/absence of the default system prompt and classic vs facilitated task framings; neither substantially improved reasoning correctness for this model.",
            "key_findings": "Llama 2 13B produced very few human-like responses and had low correct logical reasoning rates; removing the system prompt was necessary to obtain responses but did not meaningfully raise performance.",
            "counter_examples_or_negative_results": "The facilitated task variants and prompt adjustments did not yield improved reasoning; the model performed poorly on mathematical tasks (no correct math-task runs reported).",
            "uuid": "e3088.5",
            "source_info": {
                "paper_title": "(Ir)rationality and cognitive biases in large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Llama2-70B",
            "name_full": "Meta Llama 2 (70 billion parameters)",
            "brief_description": "The 70B parameter Llama 2 model evaluated; showed somewhat better correct+logical rates than smaller Llama variants but often refused to answer (system prompt behavior) and remained inconsistent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 2 (70B)",
            "model_description": "Meta's Llama 2 70B parameter model accessed via its online chatbot interface; the default system prompt appeared embedded and led to many refusals to answer in the study.",
            "model_size": "70B",
            "reasoning_methods": [
                "zero-shot prompting (classic and facilitated task wording)",
                "default system prompt behaviour (embedded; many refusals observed)",
                "repeated-run sampling (10 runs per task)"
            ],
            "reasoning_methods_description": "Evaluations used zero-shot classic and facilitated prompts. The 70B model frequently refused to answer (41.7% refusal rate reported); when it did answer it was sampled across multiple runs.",
            "diversity_of_methods": "single/similar style — tested only in zero-shot with task framings; no additional prompting strategies were applied.",
            "reasoning_task_name": "Same 12 cognitive psychology tasks",
            "reasoning_task_description": "Tasks probing biases and rational reasoning; the model refused to answer many prompts due to safety/system instructions embedded in the model.",
            "performance_by_method": "Zero-shot/classic+facilitated (aggregated): 15.0% of runs produced correct answers with correct logical reasoning. Refusal/no-answer rate was high: 41.7% of runs resulted in no answer or refusal. On mathematical tasks, 70B produced only one correct response across runs.",
            "comparison_of_methods": "Paper compared runs in the presence of embedded system prompt behavior (70B) to the 7B/13B variants where the prompt was removed; the 70B behaved similarly to the prompted smaller models and often refused, indicating system prompt effects are important but removal did not appear to be possible for 70B in the study.",
            "key_findings": "Llama 2 70B performed better than smaller Llama variants on correct+logical metric but suffered from a high refusal rate (embedded safety prompt), producing inconsistent outputs and near-zero success on mathematical tasks.",
            "counter_examples_or_negative_results": "Despite being much larger, the 70B model did not reliably outperform the smaller Llama variants when considering refusals and inconsistencies; removal of system prompt (not possible for 70B in this study) seemed to be required to elicit outputs but embedding of safety prompts may be baked in.",
            "uuid": "e3088.6",
            "source_info": {
                "paper_title": "(Ir)rationality and cognitive biases in large language models",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Using cognitive psychology to understand GPT-3",
            "rating": 2,
            "sanitized_title": "using_cognitive_psychology_to_understand_gpt3"
        },
        {
            "paper_title": "Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT",
            "rating": 2,
            "sanitized_title": "humanlike_intuitive_behavior_and_reasoning_biases_emerged_in_large_language_models_but_disappeared_in_chatgpt"
        },
        {
            "paper_title": "Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias",
            "rating": 2,
            "sanitized_title": "instructed_to_bias_instructiontuned_language_models_exhibit_emergent_cognitive_bias"
        },
        {
            "paper_title": "The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs",
            "rating": 2,
            "sanitized_title": "the_goldilocks_of_pragmatic_understanding_finetuning_strategy_matters_for_implicature_resolution_by_llms"
        },
        {
            "paper_title": "Language models show human-like content effects on reasoning tasks",
            "rating": 2,
            "sanitized_title": "language_models_show_humanlike_content_effects_on_reasoning_tasks"
        },
        {
            "paper_title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4",
            "rating": 1,
            "sanitized_title": "sparks_of_artificial_general_intelligence_early_experiments_with_gpt4"
        },
        {
            "paper_title": "Emergent analogical reasoning in large language models",
            "rating": 1,
            "sanitized_title": "emergent_analogical_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Inductive reasoning in humans and large language models",
            "rating": 1,
            "sanitized_title": "inductive_reasoning_in_humans_and_large_language_models"
        }
    ],
    "cost": 0.0164195,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>(Ir)rationality and Cognitive Biases in Large Language Models
15 Feb 2024</p>
<p>Olivia Macmillan-Scott olivia.macmillan-scott.16@ucl.ac.uk 
Mirco Musolesi m.musolesi@ucl.ac.uk </p>
<p>University College London</p>
<p>University College London University of Bologna</p>
<p>(Ir)rationality and Cognitive Biases in Large Language Models
15 Feb 2024E3B9A053ED9032BD792C0876DDF1133AarXiv:2402.09193v2[cs.CL]
Do large language models (LLMs) display rational reasoning?LLMs have been shown to contain human biases due to the data they have been trained on; whether this is reflected in rational reasoning remains less clear.In this paper, we answer this question by evaluating seven language models using tasks from the cognitive psychology literature.We find that, like humans, LLMs display irrationality in these tasks.However, the way this irrationality is displayed does not reflect that shown by humans.When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases.On top of this, the LLMs reveal an additional layer of irrationality in the significant inconsistency of the responses.Aside from the experimental results, this paper seeks to make a methodological contribution by showing how we can assess and compare different capabilities of these types of models, in this case with respect to rational reasoning.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have quickly become integrated into everyday activities, and their increasing capabilities mean this will only become more pervasive.Given this notion, it is important for us to develop methodologies to evaluate the behaviour of LLMs.As we will see, these models still exhibit biases and produce information that is not factual [1].However, there is extensive variation in the responses given by different models to the same prompts.In this paper, we take a comparative approach based in cognitive psychology to evaluate the rationality and cognitive biases present in a series of LLMs; the aim of this paper is to provide a method to evaluate and compare the behaviour and capabilities of different models, here with a focus on rational and irrational reasoning.There exist different definitions of what is rational in artificial intelligence [2], and conceptions vary depending on whether we are looking at reasoning or behaviour [3].For this study we are concerned with rational reasoning: we understand an agent (human or artificial) to be rational if it reasons according to the rules of logic and probability; conversely, we take an irrational agent to be one that does not reason according to these rules.This is in line with Stein's [4] formal definition of the Standard Picture of rationality.</p>
<p>In this paper, we evaluate seven LLMs using cognitive tests proposed by Kahneman and Tversky [5][6][7] and others [8][9][10], as well as some facilitated versions formulated by Bruckmaier et al. [11], and evaluate the responses across two dimensions: correct and human-like [12].These tasks were initially designed to illustrate cognitive biases and heuristics in human reasoning, showing that humans often do not reason rationally [13]; in this case, we use them to evaluate the rationality of LLMs.The 'holy grail' would be to develop a set of benchmarks that can be used to test the rationality of a model; this is a complex problem which requires a consensus what is deemed rational and irrational.</p>
<p>In using methods designed to evaluate human reasoning, it is important to acknowledge the performance vs. competence debate [14].This line of argument encourages species-fair comparisons between humans and machines, meaning that we should design tests specific to either humans or machines, as otherwise apparent failures may not reflect underlying capabilities but only superficial differences.Lampinen [15] discusses this problem when it comes to language models in particular, highlighting that different approaches must be taking to evaluate cognitive and foundation models.However, if we take the purpose of LLMs to be to produce human-like language, perhaps the best approach is precisely to evaluate their output with tasks designed to evaluate humans.This is the approach we have taken in this paper -in order to identify whether LLMs reason rationally, or whether they exhibit biases that can be assimilated to those present in human decision-making, the most appropriate approach is therefore to use tasks that were initially designed for humans.</p>
<p>Building on this debate and looking at LLMs being evaluated using human tests, Hagendorff et al. [16] have proposed the creation of a new field of research called machine psychology, which would treat LLMs as participants in psychological experiments.The approach employed in this paper precisely applies tests from psychology that were originally designed for humans, in this case to evaluate rational and irrational reasoning displayed but such models.Further to this, some have even discussed the potential of using LLMs as participants in cognitive experiments instead of humans [17], although some see this proposal as too optimistic [18], and others warn against excessive anthropomorphism [19].One argument against the use of such models in cognitive experiments is that LLMs may be effective at approximating average human judgements, but are not good at capturing the variation in human behaviour [20].One potential avenue to address this issue is current work on language models impersonating different roles [21], in this way capturing some of the variation in human behaviour.Binz and Schulz [22] show that after finetuning LLMs on data from psychological experiments, they can become accurate cognitive models, which they claim begins paving the way for the potential of using these models to study human behaviour.Park et al. [23] combine large language models with computational interactive agents to simulate human behaviour, both individual and within social settings.</p>
<p>Given the data that they are trained on, LLMs naturally contain human-like biases [24][25][26].Schramowski et al. [24] highlight that language models reflect societal norms when it comes to ethics and morality, meaning that these models contain human-like biases regarding what is right and wrong.Similarly, Durt et al. [26] discuss the clichés and biases exhibited by LLMs, emphasising that the presence of these biases is not due to the models' mental capacities but due to the data they are trained on.Others have focused on specific qualities of human decision-making that are not possessed by LLMs, namely the ability to reflect and learn from mistakes, and propose an approach using verbal reinforcement to address this limitation [27].As these studies show, LLMs display human-like biases which do not arise from the models' ability to reason, but from the data they are trained on.Therefore, the question is whether LLMs also display biases that relate to reasoning: do LLMs simulate human cognitive biases?There are cases where is may be beneficial for AI systems to replicate human cognitive biases, in particular for applications that require human-AI collaboration [28].</p>
<p>To answer this question, we use tasks from the cognitive psychology literature designed to test human cognitive biases, and apply these to a series of LLMs to evaluate whether they display rational or irrational reasoning.The capabilities of these models are quickly advancing, therefore the aim of this paper is to provide a methodological contribution showing how we can assess and compare LLMs.A number of studies have taken a similar approach, however they do not generally compare across different model types [12,16,[29][30][31][32][33][34][35], or those that do are not evaluating rational reasoning [36].Some find that LLMs outperform humans on reasoning tasks [16,37], others find that these models replicate human biases [30,38], and finally some studies have shown that LLMs perform much worse than humans on certain tasks [36].Binz and Schulz [12] take a similar approach to that presented in this paper, where they treat GPT-3 as a participant in a psychological experiment to assess its decision-making, information search, deliberation and causal reasoning abilities.They assess the responses across two dimensions, looking at whether GPT-3's output is correct and/or human-like; we follow this approach in this paper as it allows us to distinguish between answers that are incorrect due to a human-like bias or are incorrect in a different way.While they find that GPT-3 performs as well or even better than human subjects, they also find that small changes to the wording of tasks can dramatically decrease the performance, likely due to GPT-3 having encountered these tasks in training.Hagendorff et al. [16] similarly use the Cognitive Reflection Test (CRT) and semantic illusions on a series of OpenAI's Generative Pre-trained Transformer (GPT) models.They classify the responses as correct, intuitive (but incorrect), and atypical -as models increase in size, the majority of responses go from being atypical, to intuitive, to overwhelmingly correct for GPT-4, which no longer displays human cognitive errors.Other studies that find the reasoning of LLMs to outperform that of humans includes Chen et al.'s [33] assessment of the economic rationality of GPT, and Webb et al.'s [34] comparison of GPT-3 and human performance on analogical tasks.</p>
<p>As mentioned, some studies have found that LLMs replicate cognitive biases present in human reasoning, and so in some instances display irrational thinking in the same way that humans do.Itzhak et al. [38] focus on the effects of fine-tuning; they show that instruction tuning and reinforcement learning from human feedback, while improving the performance of LLMs, can also cause these models to express cognitive biases that were not present or less expressed before these fine-tuning methods were applied.While said study [38] investigate three cognitive biases that lead to irrational reasoning, namely the decoy effect, certainty effect and belief bias, Dasgupta et al. [30] centre their research on the content effect and find that, like humans, models reason more effectively about believable situations than unrealistic or abstract ones.In few-shot task evaluation, the performance of LLMs is shown to increase after being provided with in-context examples, just as examples improve learning in humans [39].Others have found LLMs to perform worse than human subjects on certain cognitive tasks, Ruis et al. [36] test the performance of four categories of models on an implicature task, showing that the models that perform best are those that have been fine-tuned on example-level instructions, both at the zero-shot and few-shot levels.However, they still find that models perform close to random, particularly in zero-shot evaluation.Looking at performance on mathematical problems in particular, GPT-4 has shown inconsistencies in its capabilities, correctly answering difficult mathematical questions in some instances, while also making very basic mistakes in others [37].As we will see below, we find this to be the case in our analysis across the language models evaluated.The inconsistency in performance is not only present in tasks involving mathematical calculations, but is apparent across the battery of tasks.This paper forms part of the existing area of research on the evaluation of LLMs.It differs from existing work by focusing on rational and irrational reasoning, and comparing the performance of different models.As we have seen, past studies have applied cognitive psychology to study LLMs.While they often focus on seeing whether LLMs replicate different aspects of human behaviour and reasoning, such as cognitive biases, we are interested in whether the way LLMs display rational or irrational reasoning.Much of the existing work focuses on a single model, or different versions of the same model.In this case, we compare across model types and propose a way to evaluate the performance of LLMs, which may ultimately lead to the development of a set of benchmarks to test the rationality of a model.</p>
<p>Methods</p>
<p>Language Models</p>
<p>We evaluate the rational reasoning of seven LLMs using a series of tasks from the cognitive psychology literature.The models that we assess are OpenAI's GPT-3.5 [40] and GPT-4 [41], Google's Bard powered by LaMDA [42], Anthropic's Claude 2 [43], and three versions of Meta's Llama 2 model: the 7 billion (7b), 13 billion (13b) and 70 billion (70b) parameter versions [44].We use the OpenAI API to prompt GPT-3.5 and GPT-4, and all other models are accessed through their online chatbot interfaces.The code for the former is available on GitHub, and information on how models were accessed is detailed in Appendix 1.</p>
<p>We did not change any parameter settings in order to evaluate the models on these cognitive tasks.However, for Llama 2, the 7b and 13b parameter models had the default prompt shown in Figure 1.After running an initial set of the tasks on these Llama 2 models, we removed the default prompt as it generally meant that the models refused to provide a response due to ethical concerns.Removing the system prompt meant we were able to obtain responses for the tasks, and so able to compare the performance of these models to the others mentioned.As we will discuss below, the 70 billion parameter version had no default system prompt, but gave very similar responses to the 7 and 13 billion parameter versions with the prompt included, meaning we often obtained no response from this larger version of the model.</p>
<p>System prompt -Llama 2 7b and 13b</p>
<p>You are a helpful, respectful and honest assistant.Always answer as helpfully as possible, while being safe.Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.Please ensure that your responses are socially unbiased and positive in nature.If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.If you don't know the answer to a question, please don't share false information.1: List of tasks and the cognitive biases they were designed to exemplify.</p>
<p>Description of Tasks</p>
<p>The tasks used to evaluate these models are taken primarily from Kahneman and Tversky's work [5,6,13,7], who designed a series of tasks to highlight biases and heuristics in human reasoning.Additional tasks [8][9][10] and facilitated versions [11] are also included.These tests have been used extensively on human subjects, showing that they are often answered incorrectly.Based primarily on work by Gigerenzer [45,46], a series of facilitated versions of these tasks were developed, emphasising the impact of context and presentation of the problem.Following on from this, Bruckmaier et al. [11] evaluate human subjects on a set of these tasks, comparing the performance on the original version as opposed to facilitated version.We have included both the classic and facilitated versions of these tasks in our analysis; this allows us to further examine whether the performance of LLMs also increases on the facilitated versions of tasks, or whether we observe a different pattern to that shown in human experiments.Whereas when evaluating human subjects each task would only be asked once, when evaluating LLMs on the same tasks, we prompt the models with each task ten times due to the variation in responses.</p>
<p>In total, we study the performance of seven language models on twelve cognitive tasks, listed in Table 1 (full task details are included in Appendix 2).Nine of them are from the set of tasks originally designed by Kahneman and Tversky [5][6][7], Wason [8], Eddy [9] and Friedman [10], and three which are facilitated versions of these tasks [11].For the birth sequence problem [5], two version are included: one with an ordered sequence and one with a random sequence.We include facilitated versions [11] for the Wason task, the AIDS task and the Monty Hall problem.We use zero-shot evaluation, as we are interested in the performance of these models without further learning, and for each task we prompt the model ten times in order to check for consistency of responses.</p>
<p>Categorisation of Responses</p>
<p>Each response to the task is categorised across two dimensions: correct and human-like [12], as detailed in Table 2.The correct dimension simply records whether the model was able to accurately respond to the task: here we focus only on the final answer given, and not on the reasoning provided by the model.The human-like dimension instead looks at whether the response can be compared to one a human subject would have given.A response that is correct and non-human-like would be one where the final response is correct, but the reasoning stated contains factual inaccuracies, incorrect calculations, or violates rules of logic and probability.Instead, a response that is incorrect and human-like corresponds to the answers typically given by humans to these tasks, so is one where the answer given is in accordance with one of the biases studied in the cognitive psychology literature.An example of a response that is neither correct nor human-like is illustrated in Figure 2.</p>
<p>Llama 2 7b -Monty Hall problem</p>
<p>In this scenario, it doesn't matter whether the candidate switches their choice or not.Here's why: When the candidate initially chose door 1, there were two doors left (2 and 3).The quizmaster then opened door 2, revealing a goat.At this point, the candidate has two options:</p>
<p>• Keep their original choice of door 1, in which case they will lose the game.</p>
<p>• Switch to door 3, in which case they will win the game (the car is behind one of the two unopened doors).In either case, the outcome is the same.Whether the candidate switches or not, they will either win the game or lose.Therefore, it doesn't matter whether they switch or not.</p>
<p>Results</p>
<p>The cognitive tasks used in this paper were designed to show that human reasoning is not always rational, and that humans often employ heuristics or display cognitive biases [5,6].Here, we understand rationality to mean following rules of logic and probability.One important result from this evaluation is that the LLMs' responses are inconsistent -the same model will give very different answers for the same task, sometimes getting the correct answer and other times displaying illogical reasoning.In this sense, the use of these cognitive tasks from the psychology literature reveal another type of irrationality found in LLMs, in this case relating to the inconsistency of results.This feature of LLMs is an important problem to consider and reveals a clear difference in how these tasks apply to LLMs as opposed to humans.Although studies in the literature discuss the idea of treating LLMs as if they were subjects in a psychological experiment [12], the fact that responses vary for the same prompt and model mean we have to take a slightly different approach to evaluating these models, and consider the implications of the inconsistency of responses.Figure 3: Aggregated results across all tasks for each model.The LLMs were prompted with twelve tasks from cognitive psychology, and their responses were categorised over two dimensions: correct and human-like (in this graph, responses categorised as incorrect and non-human-like are distinguished from those that were incorrect but displayed correct reasoning).For each task, the LLMs were prompted ten times.</p>
<p>Results across all tasks are aggregated in Table 3 and Figure 3.The model that displayed the best overall performance was OpenAI's GPT-4, which achieved the highest proportion of answers that were correct and where the results was achieved through correct reasoning (cateogorised as correct and human-like in the above categorisation).GPT-4 gave the correct response and correct reasoning in 69.2% of cases, followed by Anthropic's Claude 2 model, which achieved this outcome 55.0% of the time.Conversely, the model with the highest proportion of incorrect responses (both human-like and non-human-like) was Meta's Llama 2 model with 7 billion parameters, which gave incorrect responses in 77.5% of cases.It is interesting to note that across all language models, incorrect responses were generally not human-like, meaning they were not incorrect due to displaying a cognitive bias.Instead, these responses generally displayed illogical reasoning, and even on occasion provided correct reasoning but then gave an incorrect final answer.An example of the latter is illustrated in Figure 4: this example shows Bard's response to the facilitated version of the Wason task, where the correct response is that both Letter 3 and Letter 4 should be turned over.The model correctly reaches this conclusion in the explanation, but both at the start and end of the response only states that Letter 4 needs to be turned over.This type of response, where the reasoning is correct but the final answer is not, was observed across all model families to varying degrees.</p>
<p>The result that most incorrect responses were not incorrect due to having fallen for a cognitive bias highlight that these models do not fail at these tasks in the same way that humans do.As we have seen, many studies have shown that LLMs simulate human biases and societal norms [24][25][26].However, when it comes to reasoning, the effect is less clear.The model that displayed the highest proportion of human-like biases in its responses was GPT-3.5, where this only occurred in 21.7% of cases.If we include human-like correct responses for GPT-3.5, this brings the proportion to 50.8% of cases.Again, the model that displayed the most human-like responses (both correct and incorrect) was GPT-4 (73.3%); the lowest was Llama 2 with 13 billion parameters, only giving human-like responses in 8.3% of cases.The comparison between correct and human-like responses given by each model is summarised in Figure 5.</p>
<p>In some occasions, the LLMs did not answer the question, or explicitly refused to respond to the task.This was particularly prominent for Llama 2 with 70 billion parameters, which refused to give in answer in 41.7% of cases -an example is given in Figure 6.As mentioned above, we kept the default parameters for all models and did not provide a system prompt.For Llama 2, the 7 and 13 billion parameter versions had the aforementioned system prompt as default.For the 70 billion parameter Google Bard -Wason task (facilitated)</p>
<p>The answer is d) Letter 4.You only need to turn over Letter 4 to check compliance with the rule.</p>
<p>• Letter 1 has a 90-cent stamp on the front, so it is compliant with the rule, regardless of where it is going.• Letter 2 has Italy marked on the back, so you know it is not going to the USA.</p>
<p>Therefore, you do not need to turn it over.• Letter 3 has a 50-cent stamp on the front, which is not enough for a letter going to the USA.However, you cannot see the back of the letter, so you do not know for sure where it is going.Therefore, you need to turn it over to check.• Letter 4 has USA marked on the back, so you know it is going to the USA.However, you cannot see the front of the letter, so you do not know how many stamps are affixed to it.Therefore, you need to turn it over to check.</p>
<p>So the answer is Letter 4, because you need to turn it over to see if it has a 90-cent stamp on the front.version, this system prompt was no longer included.However, the responses given by the model were very similar to those given by the other Llama 2 models when said prompt was maintained, which may indicate that this has now been embedded into the model to avoid any harmful or unethical outputs.</p>
<p>As part of the tasks, we included three facilitated versions of classic cognitive tests [11], as well as two versions of the Births Order problem: one of these gives a random order, and the other appears less random.Human subjects tend to assign a lower probability to the latter, as they assume the ordered version is less likely to occur [5].A comparison of the models' results for the classic and facilitated tasks is shown in Figure 7.Given that the facilitated versions of these tasks are more often answered correctly by humans [11], we hypothesised that the same result would be observed for LLMs.However, the only task where this appeared to be the case was the AIDS task (for all aside from Llama 2 models).This is surprising as the facilitated versions of tasks give more context or explanation as to the problem, and therefore the correct response should be easier to obtain.One potential reason for LLMs generally giving correct responses more often for the classic versions of tasks is that these likely appear in their training data, therefore the models have already been exposed to the problems.</p>
<p>The question of whether these models have already seen the cognitive tasks in training can be partially answered by looking at cases where the LLM identifies the problem they are being posed (see Table 4).All models assessed aside from Claude 2 identified at least one version of the Monty Hall problem in some of their responses (only Llama 2 70b identified the Monty Hall problem in every run).Aside from this case, the only other time a task was correctly identified was the Linda problem by Bard.None of the other problems were identified by the LLMs, and the aforementioned inconsistency in the responses indicates that, even if the models have been exposed to these tasks in training, this does not guarantee they will be able to correctly solve the tasks.</p>
<p>Previous literature has identified that LLMs often make basic mistakes in seemingly simple calculations [37].Given this finding, we decided to compare the performance of the models on tasks that contained mathematical calculations and those that did not -these results are illustrated in Figure 8.</p>
<p>In this case, we only look as answers that were categorised as correct and human-like, that is to say that the final answer was correct, and the reasoning presented was also logical.Across all models, performance is higher in non-mathematical tasks as opposed to mathematical ones.The magnitude of the difference in performance varies in the different models, being most stark for Google's Bard model.Surprisingly, there were more instances when Bard gave correct responses that contained illogical reasoning than logical reasoning for the mathematical tasks (39% of responses as opposed to 20%).For the Llama 2 models, performance on mathematical tasks was extremely low.The 7 and 13  Graph also depicts the proportion of responses which did not contain an answer or where there was a refusal to provide an answer.Correct responses include those those with correct (logical) reasoning, as well as those with incorrect (illogical) reasoning that reached the correct answer.Human-like responses include those that are correct with logical reasoning, and those that are incorrect but are achieved through a studied human cognitive bias.</p>
<p>Llama 2 70b -Linda problem I must point out that the question contains harmful gender stereotypes.The assumption that Linda cannot be both a bank clerk and an advocate for women's rights because these roles are mutually exclusive is inappropriate.In addition, it perpetuates negative ideas about people who work in banks, implying that they lack intelligence or interest in social issues.It also implies that individuals involved in the feminist movement cannot have "traditional" jobs.Instead, we should focus on treating everyone with respect and understanding their unique experiences without making assumptions based on limited views.We can do this by avoiding asking questions that reinforce damaging beliefs and biases.Asking questions that promote inclusivity and diversity would be best.billion parameter models did not give correct responses to any of the tasks containing calculations, whereas the 70 billion parameter version only did so in one instance.</p>
<p>Discussion</p>
<p>This paper set out to evaluate LLMs using tasks from the cognitive psychology literature in order to assess whether these models display rational reasoning, or whether they display irrational reasoning akin to that observed in humans.Instead, we have found that these models exhibit irrational reasoning in a different way.Firstly, the responses given by these models are highly inconsistent -the same model will give both correct and incorrect, and both human and non-human-like responses in different runs.Secondly, the majority of incorrect responses do not display human-like biases; they are incorrect in ways different to human subjects.A series of issues can be identified when looking at the explanations given by LLMs, particularly when it comes to mathematical calculations, but also  4: Proportion of task runs that each task was identified by the given model.No other tasks were identified by any of the LLMs.inconsistent logic.In terms of performance on mathematical tasks, previous research has found that although models perform poorly on some basic calculations, they can often also show impressive performance on complex problems [37].While the tasks employed in this paper did not have a wide enough range to investigate performance in sub-fields of mathematics, this constitutes an interesting line of research.
Task Comparison
To ensure we could accurately compare the results to responses given by human subjects, we did not alter the prompts from the classic formulation of the problems.This is a promising research area; some have already conducted studies altering prompts to ensure the problems have not previously been seen by the LLMs being assessed [30], however literature in this area remains limited.Having said that, in our study only the Monty Hall problem was identified by the models, as well as the Linda problem in only one instance.Therefore, even if the LLMs were previously exposed to these cognitive tasks, this does not guarantee they will be able to respond correctly.</p>
<p>When conducting the experiments, we left the default parameters for the LLMs, as these appear to be the preferred option by LLM designers and the majority of users will likely keep them.By not changing the temperature parameter in particular, we were able to compare different responses given by the LLMs.Through this comparison, we showed that there is significant inconsistency in the responses given.Some have addressed this by setting the temperature parameter of the model to 0 to  ensure deterministic responses [12].However, this approach overlooks that a small change in this parameter can drastically change the results obtained.Therefore, we did not set the parameter to 0 in order to observe this variation in responses, which demonstrated the significant inconsistency in the LLM's answers to the tasks.</p>
<p>The only change we made to the default parameters was to remove the default prompts for the 7 and 13 billion versions of the Llama 2 models.Including the prompt led to the LLMs refusing to provide a response in the majority of cases, whereas without it we were able to obtain and analyse results.The 70b version of the model appears to have this prompt embedded, which led to many of the tasks not being answered.Röttger et al. [47] claim that in some cases we may have gone too far in trying to prevent models from responding to unsafe prompts and refer to exaggerated safety, showing that LLMs refuse to comply with safe requests when they include language that is sensitive or may be included in unsafe prompts.</p>
<p>Conclusion</p>
<p>Despite the widespread adoption of LLMs, researchers are still developing methods to assess and evaluate their capabilities.In this paper, we treat these models as if they are participants in cognitive experiments, an approach that has been taken in a growing body of literature.In so doing, we analyse the reasoning they display.We have shown that the seven models assessed in this paper show a different type of irrationality to that displayed by humans; this irrationality is observed across two dimensions.First, the responses given by the LLMs often display incorrect reasoning that differs from cognitive biases observed in humans.This may mean errors in calculations, or violations to rules of logic and probability, or simple factual inaccuracies.Second, the inconsistency of responses reveals another form of irrationality -there is significant variation in the responses given by a single model for the same task.This has implications for potential uses of these models in critical applications and scenarios, such as diplomacy [48,49] or medicine [50].Therefore, the work presented here can serve as a starting point for dealing with safety aspects of LLMs with respect to rational reasoning.This paper provides a methodological contribution to show how the rational reasoning abilities of these types of models can be assessed and compared.The proposed methodology has potential wider applications in studying cognitive abilities of LLMs.These tasks were originally designed for human reasoning, and given that LLMs attempt to simulate human-like language, using these tasks allows us to evaluate whether this is the case.</p>
<p>AIDS task</p>
<p>The probability that someone is infected with HIV is 0.01%.The test recognizes HIV virus with 100% probability if it is present.So, the test is positive.The probability of getting a positive test result when you don't really have the virus is only 0.01%.The test result for your friend is positive.The probability that your friend is infected with the HIV virus is therefore: ___ % [11] adapted from [9] AIDS task (facilitated)</p>
<p>This task involves an assessment of the results of the AIDS test.It is known that HIV can cause AIDS.Now imagine the following: A friend of yours gave blood at the hospital.It will then be checked to see if HIV is present in the blood.The test result is positive.How likely is it that your friend is actually infected with the HIV?To answer this question, you will need the following information: Out of 10,000 people, 1 person is infected with HIV.If the person is infected with the HIV, the test detects HIV.So the test is positive.Only 1 of the 9,999 people who are not infected with HIV have a positive test.</p>
<p>The test result for your friend is positive.How many people who have received a positive test result are actually infected with HIV? ___ from ___.</p>
<p>[11]</p>
<p>Hospital problem</p>
<p>In hospital A about 100 children are born per month.In hospital B about 10 children are born per month.The probability of the birth of a boy or a girl is about 50 percent each.Which of the following statements is right, which is wrong?</p>
<p>The probability that once in a month more than 60 percent of boys will be born is. . .(a) . . .larger in hospital A (b) . . .larger in hospital B (c) . . .equally big in both hospitals [11], adapted from [5,6] Monty Hall problem A candidate on a quiz show can choose one of three doors.Behind one of the doors is the main prize, a car.Behind the other two doors, there are two goats.The rules of the game are now as follows: The quizmaster knows behind which of the doors the car and the goats are.After the candidate has chosen one of the doors, it remains locked for the time being.</p>
<p>The quizmaster then opens one of the other two doors.He always opens a door with a goat behind it.Imagine that the candidate chooses door 1.Instead of opening this door, the quizmaster opens another door, behind which there is a goat.He now offers the candidate the option of switching his choice to the last unopened door.Should the candidate switch to the door or not?[10,11]</p>
<p>Figure 1 :
1
Figure 1: Default system prompt for Llama 2 7b and 13b.</p>
<p>Figure 2 :
2
Figure 2: Example response to the Monty Hall problem by Llama 2 7b (emphasis added).</p>
<p>Figure 4 :
4
Figure 4: Example response to the Wason task (facilitated) by Bard (emphasis added).</p>
<p>Figure 5 :
5
Figure5: Proportion of correct vs human-like responses across all tasks for each language model.Graph also depicts the proportion of responses which did not contain an answer or where there was a refusal to provide an answer.Correct responses include those those with correct (logical) reasoning, as well as those with incorrect (illogical) reasoning that reached the correct answer.Human-like responses include those that are correct with logical reasoning, and those that are incorrect but are achieved through a studied human cognitive bias.</p>
<p>Figure 6 :
6
Figure 6: Example response to the Linda problem by Llama 2 70b.</p>
<p>Figure 7 :
7
Figure 7: Result comparison for tasks that had two versions.For the Wason task, AIDS task and Monty Hall problem, the second set of results corresponds to the facilitated version.For the birth order problem, the second set of results corresponds to the version with a random order.For all four tasks, the second set of results (shown on the right) correspond to the task that human participants more often get right.Aside from the AIDS task, none of the tasks mimic this pattern.</p>
<p>Figure 8 :
8
Figure 8: Proportion of correct and human-like responses (includes only responses with logical reasoning) in mathematical vs. non-mathematical tasks.</p>
<p>the following rule: If there is a vowel on one side of the card, there is an even number on the other side.You see four cards now: cards must in any case be turned over to check the rule?(In other words: which cards could violate the rule above?) are working for the post office.You are responsible for checking whether the right stamp is affixed to a letter.The following rule applies: If a letter is sent to the USA, at least one 90-cent stamp must be affixed to it.There are four letters in front of you, of which you can see either the front or the back.(a) Letter 1: 90-cent stamp on the front (b) Letter 2: Italy marked on the back (c) Letter 3: 50-cents stamp on the front (d) Letter 4: USA marked on the back Which of the letters do you have to turn over in any case if you want to check compliance with this rule?</p>
<p>Table 2 :
2
Categorisation of responses.
CorrectIncorrectHuman-likeCorrect (logical) reasoningStudied biasOther responseNon-human-likeIncorrect (illogical) reasoning(including correct reasoning butincorrect response)CorrectCorrectIncorrectIncorrectIncorrectNo answer(R)(IR)(H)(NH)(CR)GPT-3.50.2920.0420.2170.4080.0330.008GPT-40.6920.1170.0420.1420.0080.000Bard0.3580.2330.0830.1920.1330.000Claude 20.5500.1000.1250.1080.1080.008Llama 2 7b0.0250.1920.1670.6080.0000.008Llama 2 13b0.0500.1920.0330.7000.0000.025Llama 2 70b0.1500.0500.0000.3330.0500.417</p>
<p>Table 3 :
3
Aggregated results.R: reasoned, IR: incorrect reasoning, H: human-like, NH: non-humanlike, CR: correct reasoning.Both Incorrect (NH) and Incorrect (CR) belong to the incorrect &amp; non-human-like categorisation.</p>
<p>Appendix 1 LLM Prompting 1.1 GPT-3.5 and GPT-4To prompt the OpenAI models, GPT-3.5 and GPT-4, we used the OpenAI API.The code for replication can be found in the following GitHub repository: https://github.com/oliviams/LLM_Rationality.All other modelsFor all models aside from the OpenAI ones we accessed them through their online chatbot interfaces.All default parameter settings were kept, aside from the following default prompt that is included for the 7 and 13 billion parameter versions of Llama 2: System prompt -Llama 2 7b and 13b You are a helpful, respectful and honest assistant.Always answer as helpfully as possible, while being safe.Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.Please ensure that your responses are socially unbiased and positive in nature.If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.If you don't know the answer to a question, please don't share false information.In order to ensure that the previous prompts were not affecting the following responses, the models were reinitialised at each instance rather than adding the task prompt to the current chat.The models were accessed via the following links within the dates 29 th August -22 nd September 2023: In how many of these three constellations should the candidate switch from his first choice to the remaining unopened door?In ___ of 3 constellations.What should the candidate therefore do?Stay or switch?[11]Linda problem Linda is 31 years old, single, very intelligent, and speaks her mind openly.She studied philosophy.During her studies, she dealt extensively with questions of equality and social justice and participated in anti-nuclear demonstrations.Now order the following statements about Linda according to how likely they are.Which statement is more likely?(a) Linda is a bank clerk.(b) Linda is active in the feminist movement.(c) Linda is a bank clerk and is active in the feminist movement.[Birth sequence problem (random)All families with six children in a city were surveyed.In seventy-two families, the exact order of births of boys (B) and girls (G) was GBGBBG.What is your estimate of the number of families surveyed in which the exact order of births was BGBBBB?[5]Birth sequence problem (ordered)All families with six children in a city were surveyed.In seventy-two families, the exact order of births of boys (B) and girls (G) was GBGBBG.What is your estimate of the number of families surveyed in which the exact order of births was BBBGGG?[5]High school problemThere are two programs in a high school.In many rounds of the game, will there be more results of type I or of type II?[5]Table5: Task Description
Language Model Behavior: A Comprehensive Survey. Tyler A Chang, Benjamin K Bergen, Computational Linguistics. 2023</p>
<p>Rationality and Intelligence: A Brief Update. Stuart Russell, Fundamental Issues of Artificial Intelligence. Vincent Müller, Springer2016</p>
<p>Olivia Macmillan, - Scott, Mirco Musolesi, arXiv preprint: 2311.17165Ir)rationality in AI: State of the Art, Research Challenges and Open Questions. 2023</p>
<p>Edward Stein, Without Good Reason: The Rationality Debate in Philosophy and Cognitive Science. Clarendon Press1996</p>
<p>Subjective probability: A judgment of representativeness. Daniel Kahneman, Amos Tversky, Cognitive Psychology. 331972</p>
<p>Judgment under Uncertainty: Heuristics and Biases. Amos Tversky, Daniel Kahneman, Science. 18541571974</p>
<p>Extensional versus intuitive reasoning: The conjunction fallacy in probability judgment. Amos Tversky, Daniel Kahneman, Psychological Review. 9041983</p>
<p>C Peter, Wason, New Horizons in Psychology. B Foss, Penguin Books1966</p>
<p>Probabilistic reasoning in clinical medicine: Problems and opportunities. David M Eddy, Judgment under Uncertainty: Heuristics and Biases. Paul Daniel Kahneman, Amos Slovic, Tversky, Cambridge University Press1982</p>
<p>Monty Hall's Three Doors: Construction and Deconstruction of a Choice Anomaly. Daniel Friedman, The American Economic Review. 8841998</p>
<p>Tversky and Kahneman's Cognitive Illusions: Who Can Solve Them, and Why?. Georg Bruckmaier, Stefan Krauss, Karin Binder, Sven Hilbert, Martin Brunner, Frontiers in Psychology. 122021</p>
<p>Using cognitive psychology to understand GPT-3. Marcel Binz, Eric Schulz, Proceedings of the National Academy of Sciences. 1206e22185231202023</p>
<p>The psychology of preferences. Daniel Kahneman, Amos Tversky, Scientific American. 24611982</p>
<p>Performance vs. competence in human-machine comparisons. Proceedings of the National Academy of Sciences. 117432020Chaz Firestone</p>
<p>Can language models handle recursively nested grammatical structures? a case study on comparing models and humans. Andrew K Lampinen, arXiv preprint: 2210.153032023</p>
<p>Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT. Thilo Hagendorff, Sarah Fabi, Michal Kosinski, Nature Computational Science. 3102023</p>
<p>Can AI language models replace human participants?. Danica Dillion, Niket Tandon, Yuling Gu, Kurt Gray, Trends in Cognitive Sciences. 2772023</p>
<p>AI language models cannot replace human research participants. Jacqueline Harding, N G William D'alessandro, Robert Laskowski, Long, 2023AI &amp; Society</p>
<p>Machine behaviour. Iyad Rahwan, Manuel Cebrian, Nick Obradovich, Josh Bongard, Jean-François Bonnefon, Cynthia Breazeal, Jacob Crandall, Nicholas Christakis, Iain Couzin, Matthew Jackson, Nature. 5682019</p>
<p>Whose Opinions Do Language Models Reflect?. Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, Tatsunori Hashimoto, arXiv preprint: 2303.175482023</p>
<p>Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz, Zeynep Akata, arXiv preprint: 2305.14930Context Impersonation Reveals Large Language Models' Strengths and Biases. 2023</p>
<p>Turning large language models into cognitive models. Marcel Binz, Eric Schulz, arXiv preprint: 2306.039172023</p>
<p>Generative Agents: Interactive Simulacra of Human Behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (UIST '23). the 36th Annual ACM Symposium on User Interface Software and Technology (UIST '23)New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Large pre-trained language models contain human-like biases of what is right and wrong to do. Patrick Schramowski, Cigdem Turan, Nico Andersen, Constantin A Rothkopf, Kristian Kersting, Nature Machine Intelligence. 432022</p>
<p>Large language models show human-like content biases in transmission chain experiments. Alberto Acerbi, Joseph M Stubbersfield, Proceedings of the National Academy of Sciences. 12044e23137901202023</p>
<p>Large Language Models and the Patterns of Human Language Use: An Alternative View of the Relation of AI to Understanding and Sentience. Christoph Durt, Tom Froese, Thomas Fuchs, 2023Preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, arXiv preprint: 2303.113662023</p>
<p>Aditya Gulati, Miguel Angel Lozano, Bruno Lepri, Nuria Oliver, arXiv preprint: 2210.01122BIASeD: Bringing Irrationality into Automated System Design. 2023</p>
<p>Sotiris Lamprinidis, arXiv preprint: 2307.11787LLM Cognitive Judgements Differ From Human. 2023</p>
<p>Language models show human-like content effects on reasoning tasks. Ishita Dasgupta, Andrew K Lampinen, C Y Stephanie, Hannah R Chan, Antonia Sheahan, Dharshan Creswell, James L Kumaran, Felix Mcclelland, Hill, arXiv preprint: 2207.070512023</p>
<p>Bart Holterman, Kees Van Deemter, arXiv preprint: 2305.14020Does ChatGPT have Theory of Mind. 2023</p>
<p>Exploring the Intersection of Rationality, Reality, and Theory of Mind in AI Reasoning: An Analysis of GPT-4's Responses to Paradoxes and ToM Tests. Lucas Freund, 2023Preprint</p>
<p>Yiting Chen, Tracy Xiao Liu, You Shan, Songfa Zhong, arXiv preprint: 2305.12763The Emergence of Economic Rationality of GPT. 2023</p>
<p>Emergent analogical reasoning in large language models. Taylor Webb, Keith J Holyoak, Hongjing Lu, Nature Human Behaviour. 72023</p>
<p>Inductive reasoning in humans and large language models. Simon Jerome Han, Keith J Ransom, Andrew Perfors, Charles Kemp, Cognitive Systems Research. 831011552024</p>
<p>The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs. Laura Ruis, Akbir Khan, Stella Biderman, Sara Hooker, Tim Rocktäschel, Edward Grefenstette, Proceedings of the 37th Conference on Neural Information Processing Systems. the 37th Conference on Neural Information Processing Systems2023</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, arXiv preprint: 2303.12712Sparks of Artificial General Intelligence: Early experiments with GPT-4. 2023</p>
<p>Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias. Itay Itzhak, Gabriel Stanovsky, Nir Rosenfeld, Yonatan Belinkov, arXiv preprint: 2308.002252023</p>
<p>Can language models learn from explanations in context?. Andrew Lampinen, Ishita Dasgupta, Stephanie Chan, Kory Mathewson, Mh Tessler, Antonia Creswell, James Mcclelland, Jane Wang, Felix Hill, Findings of the Association for Computational Linguistics: EMNLP-22. Yoav Goldberg, Zornitsa Kozareva, Yue Zhang, Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish202033</p>
<p>GPT-4 Technical Report. 2023OpenAIOpenAITechnical report</p>
<p>. Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Yaguang Du, Hongrae Li, Huaixiu Lee, Amin Steven Zheng, Marcelo Ghafouri, Yanping Menegali, Maxim Huang, Dmitry Krikun, James Lepikhin, Dehao Qin, Yuanzhong Chen, Zhifeng Xu, Adam Chen, Maarten Roberts, Vincent Bosma, Yanqi Zhao, Chung-Ching Zhou, Igor Chang, Will Krivokon, Marc Rusch, Pranesh Pickett, Laichee Srinivasan, Kathleen Man, Meier-Hellstern, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak2022Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben HutchinsonQuoc Le. LaMDA: Language Models for Dialog Applications. arXiv preprint: 2201.08239</p>
<p>Model Card and Evaluations for Claude Models. Anthropic, 2023AnthropicTechnical report</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, 2023Aurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv preprint: 2307.09288</p>
<p>The bounded rationality of probabilistic mental models. Gerd Gigerenzer, Rationality: Psychological and philosophical perspectives. K I Manktelow, D E Over, Taylor &amp; Frances/Routledge1993</p>
<p>Reasoning the fast and frugal way: models of bounded rationality. Gerd Gigerenzer, Daniel Goldstein, Psychological Review. 1031996</p>
<p>Paul Röttger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, Dirk Hovy, arXiv preprint: 2308.01263XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models. 2023</p>
<p>Escalation Risks from Language Models in Military and Diplomatic Decision-Making. Juan-Pablo Rivera, Gabriel Mukobi, Anka Reuel, Max Lamparth, Chandler Smith, Jacquelyn Schneider, arXiv preprint: 2401.034082024</p>
<p>Andrew Moore, How AI Could Revolutionize Diplomacy. Foreign Policy. 2023. February 9th, 2024</p>
<p>Large language models in medicine. Arun James Thirunavukarasu, Darren Shu, Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, Daniel Shu, Wei Ting, Nature Medicine. 292023</p>            </div>
        </div>

    </div>
</body>
</html>
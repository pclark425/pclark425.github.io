<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5628 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5628</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5628</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-117.html">extraction-schema-117</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to estimate or predict the probability or likelihood of specific future real-world scientific discoveries, including details on methods, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-265034268</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.02692v1.pdf" target="_blank">ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Multimodal Large Language Models (MLLMs) have shown impressive abilities in interacting with visual content with myriad potential downstream tasks. However, even though a list of benchmarks has been proposed, the capabilities and limitations of MLLMs are still not comprehensively understood, due to a lack of a standardized and holistic evaluation framework. To this end, we present the first Comprehensive Evaluation Framework (ChEF) that can holistically profile each MLLM and fairly compare different MLLMs. First, we structure ChEF as four modular components, i.e., Scenario as scalable multimodal datasets, Instruction as flexible instruction retrieving formulae, Inferencer as reliable question answering strategies, and Metric as indicative task-specific score functions. Based on them, ChEF facilitates versatile evaluations in a standardized framework, and new evaluations can be built by designing new Recipes (systematic selection of these four components). Notably, current MLLM benchmarks can be readily summarized as recipes of ChEF. Second, we introduce 6 new recipes to quantify competent MLLMs' desired capabilities (or called desiderata, i.e., calibration, in-context learning, instruction following, language performance, hallucination, and robustness) as reliable agents that can perform real-world multimodal interactions. Third, we conduct a large-scale evaluation of 9 prominent MLLMs on 9 scenarios and 6 desiderata. Our evaluation summarized over 20 valuable observations concerning the generalizability of MLLMs across various scenarios and the composite capability of MLLMs required for multimodal interactions. We will publicly release all the detailed implementations for further analysis, as well as an easy-to-use modular toolkit for the integration of new recipes and models, so that ChEF can be a growing evaluation framework for the MLLM community.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5628",
    "paper_id": "paper-265034268",
    "extraction_schema_id": "extraction-schema-117",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00723275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CHEF: A COMPREHENSIVE EVALUATION FRAME-WORK FOR STANDARDIZED ASSESSMENT OF MULTI-MODAL LARGE LANGUAGE MODELS
5 Nov 2023</p>
<p>Zhelun Shi shizhelun@pjlab.org.cn 
Shanghai Artificial Intelligence Laboratory</p>
<p>Beihang University</p>
<p>Equal Contribution</p>
<p>Zhipin Wang 
Beihang University</p>
<p>Equal Contribution</p>
<p>Hongxing Fan 
Beihang University</p>
<p>Equal Contribution</p>
<p>Zhenfei Yin 
Shanghai Artificial Intelligence Laboratory</p>
<p>The University of Sydney</p>
<p>Lu Sheng lsheng@buaa.edu.cn 
Beihang University</p>
<p>Yu Qiao 
Shanghai Artificial Intelligence Laboratory</p>
<p>Jing Shao shaojing@pjlab.org.cn 
Shanghai Artificial Intelligence Laboratory</p>
<p>CHEF: A COMPREHENSIVE EVALUATION FRAME-WORK FOR STANDARDIZED ASSESSMENT OF MULTI-MODAL LARGE LANGUAGE MODELS
5 Nov 2023F640EBFBA50F63F97591109E9D0177BEarXiv:2311.02692v1[cs.CV]
Multimodal Large Language Models (MLLMs) have shown impressive abilities in interacting with visual content with myriad potential downstream tasks.However, even though a list of benchmarks has been proposed, the capabilities and limitations of MLLMs are still not comprehensively understood, due to a lack of a standardized and holistic evaluation framework.To this end, we present the first Comprehensive Evaluation Framework (ChEF) that can holistically profile each MLLM and fairly compare different MLLMs.First, we structure ChEF as four modular components, i.e., Scenario as scalable multimodal datasets, Instruction as flexible instruction retrieving formulae, Inferencer as reliable questionanswering strategies, and Metric as indicative task-specific score functions.Based on them, ChEF facilitates versatile evaluations in a standardized framework, and new evaluations can be built by designing new Recipes (systematic selection of these four components).Notably, current MLLM benchmarks can be readily summarized as recipes of ChEF.Second, we introduce 6 new recipes to quantify competent MLLMs' desired capabilities (or called desiderata, i.e., calibration, incontext learning, instruction following, language performance, hallucination, and robustness) as reliable agents that can perform real-world multimodal interactions.Third, we conduct a large-scale evaluation of 9 prominent MLLMs on 9 scenarios and 6 desiderata.Our evaluation summarized over 20 valuable observations concerning the generalizability of MLLMs across various scenarios and the composite capability of MLLMs required for multimodal interactions.Codes and data are now available at https://openlamm.github.io</p>
<p>INTRODUCTION</p>
<p>By applying the powerful Large Language Models (LLMs) (OpenAI, 2023b;Chiang et al., 2023;Touvron et al., 2023) as a universal task interface, recent works on Multimodal Large Language Models (MLLMs) (Liu et al., 2023a;Zhu et al., 2023;Dai et al., 2023) have shown impressive abilities to interact with visual contents through question-answering dialogues and are expected to address more complex multimodal tasks that can harness LLMs' generalization ability to myriad downstream scenarios.Yet the capabilities and limitations of MLLMs are still not well understood, and we observe a lack of a standardized framework that can comprehensively evaluate different MLLMs.Recent benchmarks often focus on building a multimodal evaluation dataset for MLLMs (Li et al., 2023b;Liu et al., 2023c;Fu et al., 2023) or only evaluate one or a few factors of MLLMs (Shao et al., 2023;Li et al., 2023d;Yu et al., 2023;Bitton et al., 2023), or attempt to establish a framework but lack scalability and have limits in their comprehensiveness (Yin et al., Acc. is the accuracy.Acc.* is the accuracy from GPT-based metric.∩ means overlap with ChEF.ICL, Lang.Perf., Instruct.Follow.are shorts for in-context learning, language performance, and instruction following, respectively.2023; Xu et al., 2023) 1 .This makes a thorough assessment of each model and reliable comparisons among various models challenging.</p>
<p>To address these issues, we believe that a comprehensive evaluation framework, which is specially designed for MLLMs, should encompass scalable datasets about multimodal tasks that can be handled by MLLMs.For each model, we should evaluate the performance in a broad set of perspectives (i.e.capabilities more than multimodal perception and reasoning, such as robustness, in-context learning, and etc.) that are vital to profile the intrinsic properties of MLLMs, especially as the agents that can perform real-world multimodal interaction.Moreover, meaningful comparisons among MLLMs require standardization in the evaluation process so that each model can be conveniently adapted.To this end, as shown in Figure 1(a), we present ChEF, a Comprehensive Evaluation Framework for reliable and indicative assessment of MLLMs, which is highly scalable and can be flexibly modified to adapt to the evaluation of any new model or task.It is modularly designed with four components, i.e., Scenario, Instruction, Inferencer, and Metric.</p>
<p>(1) Scenarios are a set of datasets concerning representative multimodal tasks that are suitable for MLLMs.Scenarios are scalable by design, allowing the inclusion of any related dataset if necessary.We have included several prominent single-task datasets, such as CIFAR-10 ( Krizhevsky &amp; Hinton, 2009) for image classification, VOC2012 (Everingham et al., 2012) for object detection, Sci-enceQA (Lu et al., 2022) for multimodal question-answering.Recent multi-task benchmark datasets proposed for evaluating MLLMs, such as MMBench (Fu et al., 2023) and SEEDBench (Li et al., 2023b), are also accessible as Scenarios.</p>
<p>(2) Instruction focuses on how to pose questions and set instruction examples to the MLLMs.We integrate various standard queries and query pools adaptive to each MLLM, and multimodal incontext example (ICE) retrieving strategies for in-context learning (ICL) (Wu et al., 2023;Brown et al., 2020).Both are tailored to specific Scenarios.To the best of our knowledge, we are the first to incorporate ICL into the evaluation framework.The design of Instruction makes it flexible to evaluate diverse Scenarios within the same framework.</p>
<p>(3) Inferencer pertains to how an MLLM answers questions.In a single-turn question-answering (QA), in addition to the standard textual outputs (Direct) that may be hard to compare with the Preprint ground-truth answers, we can employ the Perplexity (PPL) (Klein et al., 2017) to select the most probable candidate answers, or Chain-of-Thought (CoT) (Zhang et al., 2023) prompting to increase the reliability of the prediction.The Inferencer also allows Multi-Turn, in which PPL, CoT, and Direct outputs can be applied in turns, and makes the evaluation result reliable.(4) Metrics are a set of score functions designed to evaluate the performance of each MLLM.For example, we include task-specific metrics such as accuracy for classification or multi-choice QA, mAP for detection, BLEU for captioning, and etc.More metrics can be included when evaluating the MLLMs from new perspectives, such as Expected Calibration Error (ECE) (Naeini et al., 2015) if we would like to know how the model is aware of its uncertainty in prediction, GPT-based metric (Chiang &amp; Lee, 2023) if we would like the outputs to be readable as natural language.The inclusion of appropriate and newly defined metrics ensures that the evaluation results are more indicative.With a systematic selection of Scenarios, Instructions, Inferencers, and Metrics, ChEF facilitates versatile evaluations in a standardized framework.Users can easily build new evaluations according to new Recipes (i.e.specific choices of the four components).For example, current MLLM benchmarks (Fu et al., 2023;Li et al., 2023b;Liu et al., 2023c;Bitton et al., 2023;Yu et al., 2023;Xu et al., 2023;Yin et al., 2023) can be summarized as different Recipes, as shown in Figure 1(b), and thus can be readily absorbed into ChEF.We will extensively discuss the design principles in Section 2.1.Moreover, we view ChEF as a growing framework, where each component can be evolved according to the emerging techniques or applications.We will continuously update the ChEF framework with a wider range of accessible models and evaluation tasks.</p>
<p>Based on ChEF, it becomes rather convenient to set up new evaluations to quantify the desired capabilities (or called desiderata) that a competent MLLM model should possess, as a reliable agent that can perform real-world multimodal interactions.These desiderata include:</p>
<p>• Calibration: Does MLLM express accurate uncertainty and confidence?• In-context Learning: Does MLLM learn from instruction examples?• Instruction Following: Does MLLM adhere to instructions?• Language Performance: Does MLLM describe visual content in readable language?• Hallucination: Does MLLM avoid mentioning objects that do not exist in the images?• Robustness: Is MLLM robust to corruptions in the multimodal inputs?Each desideratum is evaluated by constructing the evaluation pipeline from a ChEF Recipe.We will introduce the Recipes for the desiderata in Section 2.3.</p>
<p>Overall, we comprehensively evaluated 9 MLLMs across 9 Scenarios and 6 desiderata.Our evaluation yields the following 3 key findings:</p>
<p>(1) Recent MLLMs cannot perform well across all Scenarios.There is a significant tug-of-war issue (Hadsell et al., 2020) between different tasks.There are also several critical tasks that can not be addressed by recent MLLMs.</p>
<p>(2) Recent MLLMs are struggling with in-context learning, instruction following, and robustness, thus they may fall short of real-world multimodal interactions.</p>
<p>(3) There is a strong correlation between the desiderata and visual performance.Evaluating the desiderata reveals the intrinsic property on Scenarios that used to evaluate a composite performance.</p>
<p>CHEF: A COMPREHENSIVE EVALUATION FRAMEWORK</p>
<p>In this section, we first list the design principles of ChEF in Section 2.1, and then depict how to conduct an evaluation process based on a Recipe of selecting the four modules in ChEF (Section 2.2).Furthermore, we introduce the Recipes of six desired capabilities (or called desiderata) that a competent MLLM should have, as shown in Section 2.3.</p>
<p>DESIGN PRINCIPLES</p>
<p>ChEF is a comprehensive evaluation framework aiming at providing a fair and holistic assessment of MLLMs' performance across diverse multimodal tasks.To accomplish this objective, our design principles encompass the following key aspects: (1) Modular.We decouple the evaluation framework into four modular components2 : Scenario, Instruction, Inferencer, and Metric, so as to enable fast modification of each component and ensure consistent evaluation results across different benchmark datasets.</p>
<p>(2) Scalable.We implement easy-to-use interfaces to streamline the integration of new Scenarios into the framework and have included almost all recent benchmark datasets into the Scenario.</p>
<p>(3) Flexible.We design ChEF to accommodate the varying input formats supported by different MLLMs, including Queries and in-context learning examples (ICE).Based on these Instructions, MLLMs can generate outputs that are suitable for specific Scenarios.</p>
<p>(4) Reliable.We include three more reliable Inferencers, such as CoT and PPL, as well as their multi-round combination (Multi-Turn), in addition to standard free-form outputs (Direct).These Inferencers make the evaluation more reliable, and better tailored to reflect the precise perception or reasoning abilities that the Scenarios tend to assess.</p>
<p>(5) Indicative.We utilize a list of task-specific metrics ranging from metrics for vision tasks to the GPT-based metric for language proficiency.Each MLLM's textual outputs are adapted to these metrics, so as to indicatively measure that the MLLMs can actually perform the target tasks.</p>
<p>EXEMPLAR RECIPES AND THEIR EVALUATION PROCESSES</p>
<p>For an illustration of how each component functions and the overall evaluation is processed, we provide two examples of Recipes in Figure 2.</p>
<p>(1) Image Captioning on Flicker30k.In Figure 2(a), the Scenario is Flickr30k and the task is image captioning.The Instruction does not only include the standard query "Generate caption of this image", but also Top-k ICE to guide the generation of captions.These examples are retrieved according to image similarity.The Inferencer applies single-round PPL to measure how each of the four answers (as the answer pool) is consistent with the input image, in the form of probability.The negative answers are retrieved based on text similarity.Using PPL instead of free-form outputs constrains the scope of the captions and thus can be measured more reliably.Finally, to be compatible with PPL, the Metric applies accuracy to determine the correctness of the prediction.</p>
<p>(2) Object Detection on VOC2012.Object detection is another typical vision task.In Figure 2(b), we apply VOC2012 as the Scenario.The Instruction has no ICE, but just a standard query.The Inferencer is PPL that is conducted in two rounds.In the first round, ask the MLLMs "What is in the image?", and in the second round, ask the MLLMs the bounding box of the predicated object.Figure 4: The exemplar of desiderata.The distinguished design of each desideratum is marked in red.For calibration evaluation, the prediction confidence is calculated to determine the gap between confidence and accuracy.Instruction following is evaluated through verbalizer manipulation.</p>
<p>In-context learning is evaluated by providing ICE in the instruction.Robustness is assessed by introducing noise to both the image and text inputs.Language performance is evaluated by instructing the model to generate chain-of-thought content.Hallucination is solely evaluated on MSCOCO, and evaluated by querying whether a specific object is present in the image.</p>
<p>Note that the answer pools of the bounding boxes are generated by random scaling and translating the ground-truth bounding boxes.The Metric is accuracy as we transform the detection task into a multi-choice question-answering paradigm.</p>
<p>DESIDERATA</p>
<p>As shown in Figure 3, we implement six more evaluations based on the desiderata that a competent MLLM model should have, i.e., calibration, in-context learning, instruction following, language performance, robustness, and hallucination.Each dimension is assessed using a specially designed Recipe.To fulfill consistent evaluations among different dimensions of the desiderata, the Scenarios are almost MMBench (Liu et al., 2023c) and ScienceQA (Lu et al., 2022), except that hallucination is evaluated on MSCOCO (Lin et al., 2014).The Inferencers share a similar strategy.Hallucination applies PPL in a single round, while the rest desiderata use the same Multi-Turn that is composed of CoT and PPL, to increase the reliability of the prediction.In the following part, we introduce the rest components in each Recipe.</p>
<p>(1) Calibration.It evaluates how the uncertainty about each MLLM's prediction is aligned with its accuracy, as highlighted by HELM (Liang et al., 2022).As shown in Figure 4, its instruction is a standard query.Moreover, calibration is measured using the Expected Calibration Error (ECE) (Naeini et al., 2015;Guo et al., 2017), which calculates the difference between the model's predicted probability and the fraction of times the model is correct.</p>
<p>Preprint</p>
<p>(2) In-context Learning.It evaluates the crucial in-context learning (ICL) ability of an MLLM.To evaluate this desideratum, the Instruction is set to include randomly retrieved in-context examples (ICE).Note that ICE can include images.To assess the ICL ability, we introduce the Relative ICL Accuracy for Multi-Choice QA (RIAM), which measures the relative accuracy improvement beyond random guessing, written as RIAM = (acc ICL − acc 0-shot )/(acc 0-shot − acc rand ),</p>
<p>where acc ICL denotes the average accuracy among the results based on the instructions with different shots of in-context examples.acc 0-shot means zero-shot prediction without ICE.acc rand means the accuracy by random guessing.</p>
<p>(3) Instruction Following.It evaluates how exactly the MLLM relies on the given instructions.The Instruction is set as standard query, which is retrieved from the three categories of instructions as the way used in verbalizer manipulation, i.e., natural, neutral, and unnatural (Li et al., 2023c).The Metric applied here is the Match Ratio (MR), which calculates the percentage of textual outputs that are matched with the outputs indicated by the verbalizer instructions.</p>
<p>(4) Language Performance.It evaluates the quality of the generated sentences.Since the applied Inferencer does not generate free-form output, we evaluate the language performance of the outputs corresponding to the chain-of-thought.Knowing that GPT-based metrics have shown to be well correlated with human evaluation (Zheng et al., 2023;Liu et al., 2023b;Wang et al., 2023a), we use GPT-4 to evaluate the language performance of the CoT outputs based on the ground-truth sentences (i.e.questions and answers) in the question-answering tasks.Moreover, we choose the average results of multiple rounds of evaluations to eliminate the flickering of the GPT-based evaluations.</p>
<p>(5) Robustness.It measures how robust an MLLM is to corruptions in the multimodal inputs.The image corruptions include noise, blur, weather, digital (Hendrycks &amp; Dietterich, 2019) and common data augmentation techniques.The textual corruptions include sentence-, word-and character-level corruptions (Chen et al., 2023b), as well as switching choices for multi-choice question-answering.</p>
<p>The Metric in this desideratum is Relative Robustness for Multi-Choice (RRM), written as RRM = (acc crp − acc rand )/(acc − acc rand ),</p>
<p>where acc crp denotes the accuracy after corruption, acc is the accuracy before corruption.acc rand means the accuracy by random guessing.</p>
<p>(6) Hallucination.It evaluates how an MLLM avoids mentioning visual objects that do not exist in the images.The Scenario is MSCOCO.We follow the Polling-based Object Probing Evaluation (POPE) (Li et al., 2023d) in this desideratum.It transforms hallucination evaluation into a set of binary classification tasks.Essentially, the MLLMs are posed Yes-or-No questions about the existence of some particular objects in the images, such as "Is there a car in the image?"Notably, PPL is applied to as a more reliable Inferencer.The Metric applied here is accuracy.</p>
<p>EXPERIMENTS</p>
<p>3.1 EVALUATION SETUP A wide range of recently introduced MLLMs are evaluated in ChEF, including LLaVA (Liu et al., 2023a), LAMM (Yin et al., 2023), MiniGPT4 (Zhu et al., 2023), mPLUG-Owl (mPLUG) (Ye et al., 2023), Otter (Li et al., 2023a), InstructBLIP (Dai et al., 2023), LLaMA-Adapter-v2 (LAv2) (Gao et al., 2023), as well as models specifically designed for grounding tasks, such as Shikra (Chen et al., 2023a) and Kosmos-2 (Peng et al., 2023).These MLLMs are evaluated across various singletask Scenarios, including CIFAR-10 (CIFAR) (Krizhevsky &amp; Hinton, 2009) for classification, Omnibenchmark (Omni) (Zhang et al., 2022b) for fine-grained classification, VOC2012 (VOC) (Everingham et al., 2012) for object detection, FSC147 (FSC) (Ranjan et al., 2021) for object counting, Flickr30k (Flickr) (Young et al., 2014) for image captioning and ScienceQA (SQA) (Lu et al., 2022) for multimodal question-answering.We also evaluate the MLLMs on several multi-task datasets including MME (Fu et al., 2023), MMbench (MM) (Liu et al., 2023c) 3 , and Seedbench (SEED) (Li et al., 2023b).1.As the default Recipes incorporate PPL, which can be regarded as a multi-choice question-answering paradigm, we also provide the accuracy of random choice for each Scenario.There are some observations as follows:</p>
<p>(1) InstructBLIP attains superior performance across most Scenarios.It is worth noting that both Shikra and InstructBLIP showcase exceptional performance on the multi-task datasets, including MME, MMBench, and SEEDBench, while the performance of other models displays inconsistencies.The visual performance of these MLLMs exhibits strong trade-offs across different tasks.</p>
<p>(2) All the MLLMs struggle in the object counting task (i.e.FSC), primarily due to the complexities associated with the precise identification of numerous objects within an image.</p>
<p>(3) There is a capability gap between detection and other tasks.Shikra and Kosmos-2 demonstrate remarkable detection capabilities, owing to their specialized training on detection datasets.However, Kosmos-2 exhibits limited aptitude in other Scenarios, especially on MMBench and ScienceQA.Despite its ability to perform perception and reasoning tasks, Kosmos-2 struggles to comprehend the meaning of options {A, B, C, D} provided in the question, resulting in difficulty in aligning the answers to options.As a consequence, it exhibits lower performance on discriminative tasks.</p>
<p>The unified evaluation of these models on diverse Scenarios in the ChEF enables us to conduct a fair comparison, discerning the optimal architectures and methodologies for specific Scenarios.</p>
<p>RESULTS OF DESIDERATA</p>
<p>The scores of all the desiderata on MLLMs are shown in Figure 5 with the corresponding accuracy of MMBench which we consider as the most representative assessment of MLLMs' visual performance.The six dimensions of desiderata are deemed essential for an MLLM to function as an interactive AI agent, emphasizing human-like interactions.However, the poor performance on these dimensions shows that current MLLMs fall short of being an AI agent capable of interacting with humans.</p>
<p>Preprint (1) Most MLLMs exhibit good calibration, indicating their ability to accurately convey uncertainty.This is primarily due to the relatively low accuracy of these models and their lack of confidence in the responses, which results in such consistency.</p>
<p>(2) Most MLLMs achieve satisfactory language performance, except for Kosmos-2, which provides few reasoning processes in its chain-of-thought responses.</p>
<p>(3) InstructBLIP and Shikra surpass other models on hallucination and meanwhile achieve superior visual performance on MMBench, emphasizing the crucial role of hallucination.</p>
<p>(4) Most MLLMs exhibit poor performance in ICL.Notably, Otter, which is specifically trained on in-context instruction tuning data, though performs the best ICL among the 9 MLLMs, also struggles in ICL primarily due to its limited proficiency in visual tasks.</p>
<p>(5) Instruction following and robustness pose challenges for most MLLMs in effectively handling Instructions that deviate from their priors and their susceptibility to noisy multimodal inputs.Alternatively, employing the PPL can substantially mitigate these fluctuations with a much smaller variance, accompanied by a noteworthy gain in accuracy for all MLLMs.Similar observations can be also found in Figure 6(b).We further leverage CoT, which mandates the model to provide its reasoning process.Although the accuracy has a slight gain, it does not bolster the stability.Nevertheless, the optimal combination of accuracy and stability emerges when employing both the CoT and PPL in a Multi-Turn Inferencer.</p>
<p>CHEF PROVIDES STABLE ASSESSMENT</p>
<p>Preprint</p>
<p>Based on these interesting discoveries, we believe that ChEF, in conjunction with the meticulously derived and recommended Recipes for diverse Scenarios, can deliver a trustworthy and indicative assessment of MLLMs.We also conduct numerous experiments to carefully select appropriate Recipes for reliable evaluations across the six dimensions of desiderata5 .</p>
<p>CORRELATION BETWEEN VISUAL PERFORMANCE AND DESIDERATA</p>
<p>To investigate the relationship between visual performance and the desiderata, we display the Pearson correlation matrix in Figure 7(a).(1) Calibration is an independent dimension, primarily assessing a model's proficiency in expressing uncertainty, without direct correlations to other dimensions.</p>
<p>(2) ICL demonstrates correlation with others, as their evaluations involve specific instructional aspects.MLLMs with enhanced ICL ability are better equipped to provide relevant responses to unseen cases.</p>
<p>(3) Instruction following demonstrates a significant correlation with language performance, robustness, and accuracy.As language performance assesses the content of an MLLM's reasoning process, which is obtained through instructional guidance, MLLMs with stronger instruction following capabilities are more likely to adhere to the "step by step" instruction and generate a comprehensive reasoning process.</p>
<p>(4) Hallucination is strongly correlated with the performance on MMBench.The choice distribution of three models, as shown in Figure 7(b), reveals that LLaVA and LAMM prefer option D to C, while Shikra tends to favor option A over D. These MLLMs display lower accuracy on options they are inclined to answer and perform better on options that they resist.The distinct prior to options, which is caused by the hallucination issue, leads to poor performance.</p>
<p>It can be concluded that the evaluation of Scenarios that involve discriminative questions evaluates a composite performance, i.e., visual performance, and additional dimensions of abilities, such as the comprehension of options.The evaluation of desiderata unveils intrinsic properties beyond visual performance.</p>
<p>CONCLUSION</p>
<p>In this work, we introduce ChEF, a comprehensive evaluation framework for holistically profiling and comparing MLLMs.ChEF's modular design (i.e.Scenario, Instruction, Inferencer, and Metric) enables versatile evaluations in a standardized framework.Based on ChEF, any evaluation, including current MLLM benchmarks, can be summarized as Recipes of ChEF.We further introduce recipes to assess MLLMs' six dimensions of desiderata and conduct large-scale experiments to test the generalizability of MLLMs across various scenarios and their composite capability for multimodal interactions.</p>
<p>Limitations.As one of the pioneering works in this domain, our study has certain limitations.Firstly, ChEF is still in its nascent stage, currently supporting only a limited number of Scenarios and models.For instance, Scenarios evaluating safety and biases have not been incorporated yet.</p>
<p>As we move forward, we aim to include a wider array of Scenarios and other models to further enrich and expand the framework's applicability and comprehensiveness.Secondly, there remains a discernible performance variance among models when confronted with different queries.While our provided Recipes have significantly mitigated these disparities, such variations are inevitable.Fur-Preprint ther research is needed to more accurately assess and optimize model performances across diverse queries to achieve more consistent evaluation outcomes.Furthermore, the utilization of the GPT API for evaluation remains an area where the effectiveness has not been conclusively determined.We will continue to stay updated with the latest advancements in the field and leverage the scalability of ChEF to optimize and update accordingly.</p>
<p>Preprint</p>
<p>Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier.From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions.</p>
<p>A.2 BENCHMARKS FOR LARGE LANGUAGE MODELS</p>
<p>In recent years, significant efforts have been made to comprehensively evaluate large language models from diverse perspectives (Liang et al., 2022;Wang et al., 2023d;Bommasani et al., 2021;Gehrmann et al., 2021;2022;Brown et al., 2020;Gao et al., 2021;von Werra et al., 2022;Srivastava et al., 2022).Gao et al. (2021) provides a unified framework to test autoregressive language models on a large number of different evaluation tasks.Liang et al. (2022) measures seven metrics that reflect a range of societal considerations, including accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency, in order to improve the transparency of language models.Li et al. (2023c) propose to evaluate the instruction following ability from the aspect of how well models can follow instructions that may not align with their priors.Recent studies evaluating the quality of natural language generation (Zheng et al., 2023;Liu et al., 2023b;Wang et al., 2023a) have indicated that GPT-based metrics typically exhibit superior performance compared to traditional reference-based and reference-free baseline metrics in terms of their correlation with human quality judgments.These evaluation metrics effectively assess the capabilities of LLMs from multiple dimensions.However, in the evaluation of MLLMs, there is currently a lack of frameworks and relevant metrics.These frameworks and metrics are of significant importance in assessing MLLMs.</p>
<p>A.3 BENCHMARKS FOR MULTIMODAL LARGE LANGUAGE MODELS</p>
<p>MLLMs have demonstrated remarkable capabilities (Liu et al., 2023a;Zhu et al., 2023;Dai et al., 2023) and are poised to address increasingly complex multimodal tasks.Various benchmarks have emerged to evaluate MLLMs.Some works focus on evaluating MLLMs using existing conventional multimodal datasets (Wang et al., 2023c) or only evaluate one or a few factors of MLLMs (Shao et al., 2023;Li et al., 2023d;Yu et al., 2023;Bitton et al., 2023), which may not provide a comprehensive evaluation suitable for these models.Recent benchmarks (Li et al., 2023b;Liu et al., 2023c;Fu et al., 2023) 2023) have attempted to establish evaluation frameworks, yet they have been characterized by limitations in terms of scalability and comprehensiveness.In response to these challenges, ChEF offers a standardized framework for conducting versatile evaluations and facilitates seamless integration of new models and tasks.</p>
<p>B CHEF (COMPREHENSIVE EVALUATION FRAMEWORK) MODULES</p>
<p>ChEF is a comprehensive evaluation framework aiming at providing a fair and holistic assessment of MLLMs' performance across diverse multimodal tasks.To accomplish this objective, our design principles encompass the following key aspects: Modular, Scalable, Flexible, Reliable, and Indicative.Based on these principles, we carefully design and implement ChEF with four components i.e., Scenario, Instruction, Inferencer, and Metric.In this section, we will introduce the details of each module.</p>
<p>B.1 SCENARIO</p>
<p>The Scenario pertains to the datasets and tasks utilized for evaluating the proficiency of MLLMs in visual and multimodal tasks.Following the principles, the Scenario is designed to be scalable.Any Scenario can be easily integrated into ChEF by defining the required Instruction and Metric with the provided interfaces.Due to the substantial similarities among datasets within the same visual task, we categorize them based on task divisions.Within each task, the Scenarios can share similar implementations for the given interfaces.</p>
<p>To facilitate the active participation of the open-source community in expanding the scope of Scenarios, we incorporate several prominent datasets from highly regarded visual tasks as exemplary Scenarios.These datasets include CIFAR-10 ( Krizhevsky &amp; Hinton, 2009) for classification, Flickr30k (Young et al., 2014) for image captioning, ScienceQA (Lu et al., 2022) for multimodal question-answering, etc.Furthermore, we seamlessly integrate multi-task datasets, including MMbench (Liu et al., 2023c), SeedBench (Li et al., 2023b), and MME (Fu et al., 2023), into the framework of ChEF.We warmly welcome the integration of additional Scenarios into ChEF by simply implementing the requirements with the provided interfaces.</p>
<p>B.2 INSTRUCTION</p>
<p>The Instruction component plays a pivotal role in facilitating the model's comprehension of the underlying semantics within the Scenario and generating pertinent responses.Within ChEF, a standard query is initially incorporated for each Scenario, such as "The photo of" for classification, providing the model with a basis for answer generation.Nevertheless, it is noteworthy that divergent models may interpret the same query dissimilarly, leading to variations in evaluation.</p>
<p>To ensure the universal compatibility of the Instruction module, in line with the design principle of flexibility, we undertake measures to devise the query pool, encompassing frequently employed queries that exhibit similar intents.This designation allows for the seamless integration of new queries, thereby ensuring the requisite adaptability for different MLLMs.The standard query and query pool are collectively referred to as Query.</p>
<p>Moreover, we firmly believe that leveraging the In-context Example (ICE) as the Instruction presents a more comprehensive and generalized approach, empowering models to grasp the intricacies of the assigned task and generate responses in the desired format and content.The ICE is retrieved from the dataset based on various criteria commonly employed in the field of NLP, including Random ICE, Fixed ICE, and Top-K ICE (Wu et al., 2023;Liu et al., 2022;Su et al., 2023).</p>
<p>(1) Random ICE is retrieved at random, without considering their relevance or importance.An example is shown in Figure 8.</p>
<p>(2) Fixed ICE is predetermined based on prior knowledge or experiments.These ICE can serve as instructional cues to encourage the model to replicate and generate outputs in a format consistent with the provided examples, as shown in Figure 9 ( The designation and implementation of the Query and ICE significantly contribute to the flexibility of evaluation.</p>
<p>B.3 INFERENCER</p>
<p>The Inferencer plays a vital role in determining the model's response to questions.Within ChEF, it incorporates a fundamental auto-regressive generation method.However, due to the free-form and long-term nature of its output, evaluating the quality of the generated text becomes subjective and unreliable (Yin et al., 2023;Li et al., 2023b).To address this concern, we design the following Inferencers to support reliable evaluation:</p>
<p>(1) Direct: This is an auto-regressive generation method employed without sampling.The output of the MLLMs is determined through greedy search, ensuring consistent output across multiple inference instances for enhanced reliability.</p>
<p>(2) Chain-of-Thought (CoT): This answering approach includes a special query, "Let's think step by step", which prompts the model to provide responses in a sequential manner.It prompts the model to provide its reasoning process, ensuring that the model's answers are well-thought-out and dependable.</p>
<p>framework, we integrate well-established metrics such as BLEU for captioning, accuracy for classification, and mAP for detection, which are commonly used in traditional computer vision tasks.</p>
<p>Additionally, when employing the PPL as Inferencer in evaluation pipelines, we rely on accuracy as the primary Metric since the generated text is confined to an answer pool.This methodology enables the harmonization of evaluation across various Scenarios, as accuracy is adopted as the shared assessment criterion.</p>
<p>C DESIDERATA</p>
<p>Based on ChEF, it becomes rather convenient to set up new evaluations to quantify the desired capabilities (or called desiderata) that a competent MLLM model should possess, as a reliable agent that can perform real-world multimodal interactions.The desiderata include calibration, incontext learning, instruction following, language performance, hallucination, and robustness.In this section, we will introduce the details of each desideratum.</p>
<p>C.1 CALIBRATION</p>
<p>Calibration aims to evaluate the model's performance to be simultaneously accurate and to provide appropriate uncertainty in its outputs, as emphasized in the work by HELM (Liang et al., 2022).This is particularly significant in risk scenarios We evaluate calibration by Expected Calibration Error (ECE) (Naeini et al., 2015;Guo et al., 2017).Formally, let y be the ground truth, and ŷ be the model's prediction with associated confidence p.The ECE examines the difference between the model's predicted confidence p and the probability the model is correctly given p, as shown in equation 3.</p>
<p>ECE = E[|p − E(y = ŷ|p)|]</p>
<p>(3) To estimate the expected accuracy E(y = ŷ|p) from finite samples, we compute the ECE by binning the model's predictions into m bins following prior work (Guo et al., 2017;Liang et al., 2022).We choose uniform-mass bins for better approximation with k = 10, where an equal number of samples fall into each bin.Let B m be a set of indices i of samples falling in m-th bin, then the average confidence and accuracy of B m are defined as
conf (B m ) = 1 |B m | i∈Bm pi (4) acc (B m ) = 1 |B m | i∈Bm 1(ŷ i = y i )(5)
Therefore, we can approximates equation 3 by equation 6.
ECE = k m=1 |B m | n |conf (B m ) − acc (B m )|(6)
The difference between conf and acc for a given bin represents the calibration gap (visualized in Figure 14 ).The lower the ECE, the better the calibration of the model, indicating that the predicted confidence p more accurately represents the true probability.</p>
<p>C.2 IN-CONTEXT LEARNING</p>
<p>In-context Learning (ICL) aims to evaluate MLLMs' ability to perform new tasks without any gradient-based training (Wu et al., 2023;Brown et al., 2020).This ability is capable of generalizing to unseen cases, which opens up many new technological possibilities that were previously considered unique to humans.While in the field of NLP, LLMs have demonstrated their ability for ICL.However, within the domain of MLLMs, this potential remains unexplored.Most MLLMs lack the ability for ICL (Li et al., 2023a).Therefore, considering the ICL ability is crucial when evaluating multimodal large language models.</p>
<p>ICL adds a small number of ICE before Query as the Instruction and has demonstrated its ability to enhance the performance of LLMs in few-shot scenarios.Given that multimodal tasks typically</p>
<p>Preprint</p>
<p>(2) Neutral."Smith|Johnson|Williams|Jones|Brown" and "foo|dog|hip|oh|cat".</p>
<p>(3) Unnatural.The choices are mapped to their respective next choices as the new verbalizer for each given question (e.g., "D|A|B|C" corresponding to "A|B|C|D").</p>
<p>We calculate the Match Ratio (MR) to determine the percentage of samples that adhere to the verbalizer manipulation instructions, mapping their original answers to corresponding new answers.This calculation helps mitigate the influence of the model's accuracy in answering questions and highlights its proficiency in following verbalizer manipulation instructions.A higher MR indicates a superior ability of the model to follow instructions.</p>
<p>C.4 LANGUAGE PERFORMANCE</p>
<p>Please act as an impartial judge and conduct a comprehensive assessment of a multimodal AI assistant's performance in the field of Visual Question Answering (VQA).Each data sample to be evaluated follows the following format: Your task is to evaluate the quality of natural language generation from AI assistant considering factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response.</p>
<p>Please first provide a comprehensive explanation of your evaluation and an overall score ranging from 0 to 10 based on explanation, where a higher score indicates better overall performance.Please output in the following format:</p>
<p>[Explanation] {Evaluation Explanation} [Overall Score] {An integer ranging from 0 to 10 representing the final evaluation score} Please ensure that your evaluation score comprehensively captures the AI assistant's performance avoiding any potential bias.Assuming that the visual information mentioned by the AI assistant is contained in the image, you only need to evaluate the quality of the generated text.Your assessments will contribute to enhancing the assistant's effectiveness in visual question answering.</p>
<p>GPT-4 System Message</p>
<p>Figure 13: System message for GPT-4 to evaluate language performance of MLLMs.The System Message includes the evaluation task description, the format of the evaluation input template, the evaluation criteria, and the format of the evaluation output template.The phrases enclosed in "[]" represent domain names, which remain constant during the testing process.The phrases enclosed in "{}" represent the meanings of the domain names, which is a placeholder to be replaced with the specific content corresponding to the domain name during testing.</p>
<p>Evaluating the quality of natural language generation is a challenging task, often requiring scoring based on various aspects such as coherence, consistency, fluency, and more.Recent studies (Zheng et al., 2023;Liu et al., 2023b;Wang et al., 2023a) have indicated that GPT-based metrics typically exhibit superior performance compared to traditional reference-based and reference-free baseline metrics in terms of their correlation with human quality judgments.Thus, we employ GPT to score the chain-of-thought text generated by the model in the multimodal question-answering Scenarios, aiming to evaluate the model's language performance.</p>
<p>In contrast to NLP, where GPT can evaluate the quality of natural language generation without references (Zheng et al., 2023;Liu et al., 2023b;Wang et al., 2023a), the evaluation process in the visual Scenarios presents a distinct challenge as GPT lacks access to visual information.Therefore, we implement specific adaptations for evaluating GPT's performance in multimodal tasks as follows:</p>
<p>(1) Reference-Based Evaluation: We provide GPT with ground-truth sentences (i.e.answers and questions) as the reference during the evaluation, which ensures faithfulness of the chain-of-thought.</p>
<p>(2) Visual Information Assumption: GPT is prompted to assume that all visual information mentioned in the test model's responses is contained in the image.This measure prevents GPT from misjudging descriptions of images in the chain-of-thought as language hallucinations (which may Preprint not be explicitly stated in the given question).This helps avoid unwarranted reductions in the language performance score.</p>
<p>(3) Selective Sampling of Correct Conclusions: We selectively extract samples in which the MLLMs' conclusions are correct.This reduces the impact of conclusion accuracy on the evaluation of language generation quality, as mentioned in Section E.4.</p>
<p>(4) Efficient and Scalable Evaluation: For more efficient and scalable evaluation, instead of pairwise comparisons, we individually assess each MLLM's response, which is called Single Answer Grading.This method exhibits high agreement with human experts in NLP tasks as demonstrated in (Zheng et al., 2023).</p>
<p>(5) Multiple Evidence Calibration: (Wang et al., 2023b) To make the GPT score more reliable and interpretable, we prompt the GPT to generate an explanation as evaluation evidence before generating the final overall score.Thanks to the properties of autoregressive models, this method allows GPT to calibrate scores based on evaluation evidence.To further reduce the systematic error of GPT evaluation, we conduct Multiple Evidence Calibration, sampling multiple GPT responses for each evaluation query, and taking the average score of all responses as the final evaluation score.</p>
<p>To apply the adaptations below, we modify the system message for GPT-4.Figure 13 shows the system message for GPT-4 to evaluate the language performance of MLLMs.</p>
<p>C.5 ROBUSTNESS Robustness aims at evaluating the capability of MLLMs to maintain accurate performance and meaningful outputs in the face of diverse challenges and variations in input data.This includes addressing data corruption and perturbations, which ensures the model's reliability in real-world applications.</p>
<p>To evaluate the robustness of our model, we carefully select mild image and text corruptions, drawing inspiration from recent work (Liang et al., 2022;Qiu et al., 2022;Chen et al., 2023b;Schiappa et al., 2022).</p>
<p>Table 3: Text corruption methods are categorized into five types.In the robustness experiments, the corruption for each text is formed by sequentially combining methods each with random severity level from the following five categories: Basic, Sentence, Word, Character, and Choice.Each category's method is selected based on the corresponding combination strategy: Random denotes the random selection of one method from all methods within that category, while Sequential implies the consecutive execution of all methods within that category.Severity represents the number of adjustable severity levels for the corruption method.For image corruptions, we incorporate five corruption categories: noise, blur, weather, digital (sourced from ImageNet-C (Hendrycks &amp; Dietterich, 2019)), and others (fundamental data augmentation techniques).For text corruption, we introduce five categories like (Chen et al., 2023b): basic, sentence, word, character (sourced from (Wang et al., 2021)) and choice.The choice category specifically represents additional corruption introduced for multi-choice question-answering Scenarios.All the corruption methods are shown in Table 2 and Table 3.These corruption methods we employ do not distort the core information of the images and text.For instance, the Center Crop for images retains at least 90% of the image content.Text perturbations solely target the questions, and in the options section, only Circular Option and Reverse Option (circular shifting and reverse order on options respectively) are applied, ensuring that the original meaning of the questions and correct answers remain unchanged.</p>
<p>To simulate real-world complexity, we construct composite corruption sequences with random severity levels for both image and text within each sample.Specifically, corruption methods from various categories are composited in a specific order.For each category, the corruption method to apply is selected based on a composite strategy.We employ two strategies: Random, where one corruption method from the category is chosen randomly, and Sequential, where all methods from the category are applied sequentially.This approach enables us to assess the model's robustness in a scalable manner, rather than evaluating the model for each instance of every separate corruption.By applying image corruption and text corruption at the same time, we can evaluate the model's performance in handling joint corruption across visual and textual domains.</p>
<p>To assess the model's robustness more accurately, we introduce the Relative Robustness for Multichoice (RRM).Similar to the RIAM described in Section C.2, we eliminate the bias introduced by random choice.The RRM primarily calculates the relative accuracy change of the model beyond random guessing accuracy before and after corruptions.</p>
<p>C.6 HALLUCINATION</p>
<p>Hallucination refers to the generated content that is nonsensical or unfaithful to the provided source content (Ji et al., 2023).Similar to LLMs, MLLMs also encounter the challenge of hallucination.Since objects are the core elements that contribute to the visual semantics of an image, we study the object hallucination problem, which refers to the generated descriptions containing objects that are inconsistent with the given image (Biten et al., 2022).As a result, we utilize the Polling-based</p>
<p>Preprint</p>
<p>Object Probing Evaluation (POPE) pipeline (Li et al., 2023d) on MSCOCO (Lin et al., 2014).The fundamental concept behind this approach is to transform the evaluation of hallucination into a series of binary classification tasks.This is achieved by presenting MLLMs with straightforward Yes-or-No questions regarding the presence of specific objects within the images (e.g., "Is there a car in the image?").Each image is prompted with six such Yes-or-No questions.To generate the probing objects, POPE considers three polling strategies by sampling the objects randomly, from popular objects, and among those frequently co-occurring objects, respectively.Additionally, we employ PPL to enhance the reliability of our evaluation.Similar to POPE, we also adopt Metrics including accuracy, precision, recall, F1-Score, and the ratio of "Yes" responses.In Table 4, we show the details of all the evaluated MLLMs in ChEF.In order to ensure that the evaluated MLLMs are relatively up-to-date, we attempt to align the results of the choice extraction success rate in Step-1 with MMBench (Liu et al., 2023c), which is a recently proposed multimudal benchmark.We align the results with all the open-sourced evaluated MLLMs in MMBench, as shown in Table 5. Due to differences in evaluation settings, such as input queries, inference strategies, and metrics, the evaluated results on MMBench in ChEF may differ slightly from those in MMBench.</p>
<p>D EXPERIMENTS: DETAILS OF EVALUATION SETUP D.1 DETAILS OF THE EVALUATED MODELS</p>
<p>D.2 DEFAULT RECIPES FOR SCENARIOS</p>
<p>In ChEF, we provide default Recipes for each Scenario.In Table 6, we show the details of the default Recipes for each Scenario.Among the Scenarios, the Omnibenchmark is meticulously labeled using a hierarchical chain of categories, facilitated by the Bamboo tree methodology (Zhang et al., 2022a).</p>
<p>For Instruction, we employ standard queries as nearly all MLLMs lack the ability for in-context learning.</p>
<p>For Inferencer, we adopt PPL for most Scenarios.For ScienceQA and MMBench, we employ Multi-Turn, with the first turn using the CoT, followed by the PPL in the second turn.For finegrained classification tasks, we utilize the Multi-Turn, where each turn is a PPL, to hierarchically inquire about categories.For detection tasks, the first turn employs PPL to inquire about categories, while the second turn utilizes PPL to inquire about bounding boxes.The answer pool for CIFAR-10  encompasses the ten predefined classes, while for FSC147, it involves the ground truth values with an additional range of ±2.The answer pool for Omnibenchmark is randomly retrieved from the category tree in Bamboo (Zhang et al., 2022a).In the case of Flickr30k, the answer pool is determined by retrieving the top-k negative candidates from the test data based on BERT similarity (Reimers &amp; Gurevych, 2019).The answer pool for VOC2012 is randomly generated by scaling and translating the ground-truth bounding boxes.The answer pool for multimodal question-answering tasks is the options {A, B, C, D}.</p>
<p>In the Metric, a single accuracy measure is utilized to assess all Scenarios uniformly.For certain specialized Scenarios, we adopt specific approaches to calculate accuracy.For Omnibenchmark, weighted accuracy is employed, which entails a weighted accuracy calculation based on the granularity of the predicted classification.MMBench provides two evaluation settings (i.e., VanillaEval and CircularEval), where the CircularEval is used to assess the MLLMs' consistency in responses for the same question when the order of options is changed.We conduct evaluations in both settings, as shown in Table 7. Across all MLLMs, a significant decline is observed, indicating MLLMs' poor performance in consistency.The utilization of CircularEval assesses a composite capability with both visual performance and consistency.To disentangle these two dimensions of capability, we employ the VanillaEval for the default Recipe and incorporate hallucination and robustness within the desiderata to evaluate the dimensions associated with consistency.</p>
<p>D.3 RECIPES FOR DESIDERATA</p>
<p>We employ specialized Recipes to assess the six dimensions of desiderata, as shown in Table 8.All the six dimensions of desiderata except language performance and hallucination are evaluated on MMBench and ScienceQA.Language performance is evaluated on 250 samples random retrieved from ScienceQA and MMBench.Following POPE (Li et al., 2023d), hallucination is specifically assessed on the MSCOCO dataset (Lin et al., 2014).</p>
<p>In terms of the Instruction, Random ICE is employed as the Instruction for ICL evaluation, while standard queries are utilized for the other dimensions.For most MLLMs that lack support for multiimage input, the Random ICE consists solely of text, while for MLLMs that do support multi-image input, such as Otter (Li et al., 2023a), the Random ICE is adapted to incorporate images.For instruction following evaluation, we concatenate instructions from different groups of verbalizer manipulation at the end of the standard query.</p>
<p>For the Inferencer, we employ Multi-Turn with the first turn using the CoT, followed by PPL.</p>
<p>The Metric we use for each dimension is discussed in Section C.</p>
<p>E EMPIRICAL EXPERIMENTS</p>
<p>Preprint</p>
<p>The calibration results are presented in Table 9.To illustrate the differences in calibration performance, we also provide reliability diagrams for LLaVA and Otter on ScienceQA in Figure 14.In reliability diagrams, predictions are sorted based on the MLLMs' confidence scores, and an equal number of predictions are grouped into 10 bins.By calculating the average confidence and accuracy within each bin, we can compare and evaluate the gap between confidence and accuracy intuitively.</p>
<p>The observations are as follows:</p>
<p>(1) Higher accuracy does not imply better calibration.In ScienceQA, LLaVA demonstrates an average accuracy with the lowest ECE, showing a relatively better calibration.In contrast, Otter achieves higher accuracy with the highest ECE, showing a relatively worse calibration.Reliability diagrams provide a more intuitive and detailed illustration.We can observe that the confidence and actual accuracy in the first 9 bins exhibited a clear correlation, indicating that the predicted confidence of the first 90% of LLaVA is relatively well calibrated.However, the reliability diagram of Otter shows a larger gap between confidence and accuracy, suggesting that Otter's predicted confidence is relatively poorly calibrated.</p>
<p>(2) Higher confidence does not imply higher accuracy and better calibration.In the reliability diagrams, both MLLMs have a substantial gap between confidence and accuracy in the last bin, which contains samples with top 10% confidence.Both MLLMs exhibit overconfidence in these samples, which reminds us to avoid considering higher confidence as evidence for higher accuracy.Additionally, it can be observed that the gap between accuracy and confidence does not decrease with increasing confidence, indicating that confidence cannot effectively represent reliability.</p>
<p>(3) InstructBLIP achieves the highest accuracy in both visual tasks, while simultaneously exhibiting remarkably low ECE, indicating exceptional calibration.Conversely, other models demonstrate a certain trade-off between the two dimensions.It implies that InstructBLIP can yield superior calibration, so as to provide precise answers to questions while accurately conveying its uncertainty.(1) It can be observed from Figure 15(a) that most of the MLLMs exhibited a decline in performance compared to the zero-shot setting, except for Otter and Kosmos-2.This can be attributed to Otter's training on in-context instruction tuning data, thus enhancing its ICL capabilities.In contrast, the observed improvement in Kosmos-2's performance is due to its struggles to comprehend the meaning of options {A, B, C, D} provided in the question, resulting in difficulty in aligning the answers to options.The number of ICE does not present a significant impact on the results.From an overall perspective, the majority of MLLMs do not demonstrate capabilities in ICL.</p>
<p>E.2 IN-CONTEXT LEARNING</p>
<p>(2) Otter demonstrates a slight enhancement when deploying ICE with images compared to the ICE without image, as shown in Figure 15(b).However, its performance attenuates in the absence of images, failing to manifest its ICL capabilities.This suggests that integrating ICE with an image is a judicious design choice within MLLMs.Contrarily, neither mPLUG-Owl nor MiniGPT-4 shows improvement in their capabilities regardless of the presence or absence of images in the ICE.</p>
<p>(3) It can be observed from Figure 16 that different retrievers have different results, and the Topk method exhibits slightly inferior performance compared to the others.This potential decline in performance might be attributed to the fact that the MLLMs might regard the given answer in a similar ICE as the correct answer for the Query, thereby influencing the model's prediction.Figure 18: Analysis of language performance.(a) Complete results of language performance.This desiderata only evaluates the natural language generation quality of thought chains in which the model provides correct conclusions to prevent conclusion accuracy from dominating the language performance score.(b) Accuracy distribution in language performance.We divide the evaluation samples into four intervals based on GPT scores and calculate the conclusion accuracy within each interval.</p>
<p>E.3 INSTRUCTION FOLLOWING</p>
<p>interactive scenarios, MLLMs should offer some form of reasoning alongside their answers.Merely achieving higher accuracy does not necessarily guarantee enhanced interactivity.Therefore, the significance of language performance is highlighted.This finding further emphasizes the imperative need to evaluate language performance in MLLMs.</p>
<p>(2) As shown in Figure 18(b), it can be observed that samples with lower language performance scores (0-3) provided by GPT are predominantly incorrect answers by the MLLMs, indicating that the low scores are largely influenced by the accuracy of the answers rather than language performance.Conversely, in the three bins with scores &gt; 3, the accuracy significantly improved.It is worth noting that despite most models providing correct answers, there are substantial differences in language performance.Therefore, it is appropriate to evaluate the language performance of only the correct samples to mitigate the impact of answer accuracy on the evaluation.</p>
<p>(3) Figure 19, as a typical example, illustrates the difference in language performance between LLaVA and Shikra, when they both provide correct answers.Regarding Shikra, GPT-4 noted that its generated-chain of-thought, while yielding the correct answer, lacks relevance to the given options.This inconsistency could potentially cause confusion, resulting in a lower score of 6.In the case of LLaVA, its generated chain-of-thought showcases a logical process that adeptly employs visual information for reasoned deductive reasoning.GPT successfully acknowledges the strengths of LLaVA's chain-of-thought, providing a comprehensive explanation for its impressive score of 9.</p>
<p>The deduction of one point may be attributed to a limited presence of divergent associations and generalizations.</p>
<p>E.5 ROBUSTNESS</p>
<p>The robustness experiment is conducted on ScienceQA and MMBench, results are presented in Table 11.The acc random on ScienceQA is 35.80.The acc random on MMBench is 27.57.In order to evaluate the influence of different corruptions, we conduct experiments on ScienceQA and MM-Bench using different corruption types, as shown in Figure 20.These corruptions encompass both image corruption and text corruption.The observations are as follows:</p>
<p>(1) The experimental results indicate that current MLLMs, when subjected to image and text corruption, do not exhibit significant decreases in accuracy in absolute terms.However, it is the portion of accuracy beyond random guessing that truly reflects the model's capabilities, and it shows significant drops after perturbation.Given the prevalence of perturbations in daily environments, evaluating a model's robustness becomes pivotal.</p>
<p>(2) Image corruptions have a relatively minor effect on model performance, possibly owing to the robustness of the pre-trained vision encoder.In contrast, text corruptions show a significant impact on performance, potentially due to the heightened sensitivity of the MLLMs' text encoder when incorporating the visual tokens.Due to the modular design of ChEF, we have the flexibility to employ different Recipes for evaluating the same Scenario and finally identify the most reasonable Recipe that can provide reliable and indicative assessments through experiments.Besides the reliability of evaluating the visual performance, we also try to ensure the stability and reliability of evaluating the desiderata.We conduct experiments to investigate the inherent randomness within them.This entailed scrutinizing the consistency of random factors, such as the utilization of random retrieved ICE for Instruction in ICL evaluation.Additionally, the evaluation of language performance, which is based on GPT assessment, inherently incorporates stochastic elements.</p>
<p>Preprint</p>
<p>To evaluate the stability of random ICE as Instruction, we conduct experiments on CIFAR10, Flickr30k, ScienceQA, and MMBench, employing a diverse set of random seeds.To emphasize deviations from the mean value, we first calculate the average of results from the five different seed sets.Then, for each seed, we determine the deviation by subtracting this average and taking the absolute value of the difference.This approach highlights the variation in results for each seed compared to the average.As illustrated in Figure 21(a), the deviation for most model results is at around 1.0, indicating notable stability.</p>
<p>To mitigate systematic errors in GPT evaluation, we employed Multiple Evidence Calibration.In this approach, we prompt GPT-4 to provide evaluation explanations as evidence for deriving the final score, as described in Section C.4.As illustrated in Figure 21(b), our prompts effectively ensure the stability of GPT's scores across multiple samplings, where the maximum deviation is controlled under 1.0.This implies that GPT can maintain a consistent scale across multiple evaluations.Furthermore, the use of five sampled responses is deemed sufficient for GPT to furnish reliable and meaningful language performance scores.</p>
<p>These results indicate that the Recipe we provide for evaluating the desiderata is indicative and reliable.</p>
<p>PreprintFigure 1 :
1
Figure 1: (a) ChEF Overview.(b) Current MLLM benchmarks can be readily absorbed into ChEF.Acc. is the accuracy.Acc.* is the accuracy from GPT-based metric.∩ means overlap with ChEF.ICL, Lang.Perf., Instruct.Follow.are shorts for in-context learning, language performance, and instruction following, respectively.</p>
<p>PreprintFigure 2 :
2
Figure 2: Two examples of Recipes in ChEF.A Recipe consists of {Scenario, Instruction, Inferencer, Metric}.The Recipe of (a) is {Flickr30k, ICE, PPL, Accuracy}, while (b) is {VOC2012, Query, Multi-Turn, Accuracy}.</p>
<p>Figure 3 :
3
Figure 3: Recipes for evaluating six dimensions of desiderata. 1) All six dimensions are assessed on MMBench and ScienceQA, except for Hallucination, which is evaluated solely on MSCOCO; 2) All use standard query as Instruction, except ICL uses random ICE; 3) All employ Multi-Turn from CoT to PPL as Inferencer, except Hallucination with a single PPL; 4) The Metric for each dimension is specifically designed for the respective evaluation.</p>
<p>Figure 5 :
5
Figure 5: Results of desiderata.The dashline is the accuracy evaluated on MMBench.The score for each dimension is computed by normalizing the results from the specific metric to a range of 0-100.Calibration score is represented by 1-ECE.Instruction following score is the average MR across different verbalizer settings.In-context learning score is the average RIAM across various shot numbers.Language performance score is normalized from the results of the GPT-based metric.Robustness score is normalized from RMM and hallucination score directly represents accuracy.</p>
<p>Figure 6 :
6
Figure 6: Results of various Inferencers across different queries on CIFAR10 and ScienceQA.Black lines within each boxplot represent the median.Boxplots display the accuracy distribution.</p>
<p>Figure 7 :
7
Figure 7: (a) Pearson correlation matrix of desiderata and accuracy on MMBench.Cooler colors indicate higher correlations.(b) Choice distribution with accuracy on MM-Bench.GT indicates the actual choice distribution.</p>
<p>Question:Figure 8 :
8
Figure 8: An example of Random ICE.The Random ICE are randomly retrieved from the dataset, without considering their relevance or importance.</p>
<p>Question:Figure 9 :
9
Figure 9: An example of Fixed ICE.The Fixed ICE is predetermined based on prior knowledge or experiment.</p>
<p>Figure 10 :
10
Figure 10: An example of Top-k Text ICE.The Top-k Text ICE is retrieved from the dataset based on text similarity.</p>
<p>Figure 14 :
14
Figure 14: Reliability diagrams for LLaVA and Otter on ScienceQA.The red excess parts represent the degree of insufficient confidence of the model, and the blue excess parts represent the degree of overconfidence of the model.</p>
<p>Figure 15 :
15
Figure 15: Results of in-context learning.(a) Average results of in-context learning on ScienceQA and MMBench utilizing various ICE numbers.(b) Results of in-context learning on MMBench for Otter, mPUG-Owl, and MiniGPT-4, utilizing various ICE numbers with and without images respectively.</p>
<p>Figure 16 :
16
Figure 16: Experimental results of evaluation with ICE as Instruction under different retriever settings.The retriever methodologies employed encompass Random, Fixed, Top-k Text, and Top-k Image.</p>
<p>Figure 20 :Figure 21 :
2021
Figure 20: Results of robustness under different settings.The accuracy in this figure represents the weighted average results on ScienceQA and MMBench.The origin represents the original accuracy; Image Crp. represents the accuracy after image corruption; Text Crp.represents the accuracy after text corruption; I.&amp;T.Crp.represents the accuracy after both image and text corruption; the dotted line represents the accuracy of random guessing.</p>
<p>Figure 22 :
22
Figure 22: Examples of in-context learning evaluation on GPT-4V.The input instruction is ICE with image, as defined in Section C.2.The left subfigure is an example of GPT-4V giving the correct answer, while the right subfigure is an example of GPT-4V giving the incorrect answer.</p>
<p>Figure 23 :Figure 24 :
2324
Figure 23: Examples of in-context learning evaluation on Bard.The input instruction is ICE without image, as defined in Section C.2.The left subfigure is an example of Bard giving the correct answer, while the right subfigure is an example of Bard giving the incorrect answer.</p>
<p>Figure 25 :
25
Figure 25: Examples of robustness evaluation on GPT-4V.The left subfigure is an example of GPT-4V giving the correct answer, while the right subfigure is an example of GPT-4V giving the incorrect answer.</p>
<p>Table 1 :
1
Visual performance of MLLMs on different Scenarios.In SQA and MM, as options {A, B, C, D} are explicitly provided in the questions, models are required to output their answers in the form of options.Similarly, MME also requires models to provide "yes" or "no" outputs.These Scenarios can be considered as a discriminative (discrim.)question type.Conversely, the other Scenario, we conduct various experiments with diverse Recipes, from which, the Recipe behaving most reliably (i.e.stable to Instruction variations) is selected as the default setting 4 to evaluate the visual performance of all MLLMs, as shown in Table
Scenarios are characterized by generative (gen.) types, as they require responses without predefinedoptions in questions. The abbreviations for Scenarios and MLLMs are defined in section 3.1. ForOmnibenchmark (Omni  † ), weighted accuracy is employed, which entails a weighted accuracy cal-culation based on the granularity of classification. The entries that are both bold and underlinedindicate the best performance.ScenarioCIFARFlickrVOCOmni  †FSCSQAMMSEEDMMEQuestion Typegen.gen.gen.gen.gen.discrim.discrim.gen.discrim.LLaVA89.4080.8026.0126.6224.1146.5543.1346.4550.17LAMM80.7072.5029.5822.5419.3352.7544.4747.0355.82MiniGPT-480.8071.5026.5130.6022.5247.054.3446.4857.12mPLUG79.6779.2028.5030.7020.9248.4449.5742.8171.59Otter81.3471.3027.1526.4120.0050.2253.9136.4063.78LAv270.1779.5031.6032.0021.2654.3457.0635.4169.90InstructBLIP84.2779.4027.6530.7525.0455.1865.7350.8172.0Shikra68.7194.7055.2322.8922.4345.2163.2649.7970.28Kosmos-288.8785.7054.5521.3421.9334.6032.8246.3852.95Random Choice10.025.0025.0010.9420.0035.8027.5724.2750.003.2 STANDARD PERFORMANCE OF VISUAL ABILITYFor each</p>
<p>(Peng et al., 2023))Association for Computational Linguistics, pp.67-78, 2014.Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.Mm-vet: Evaluating large multimodal models for integrated capabilities.CoRR, abs/2308.02490,2023.Yuanhan Zhang, Qinghong Sun, Yichun Zhou, Zexin He, Zhenfei Yin, Kun Wang, Lu Sheng, Yu Qiao, Jing Shao, and Ziwei Liu.Bamboo: Building mega-scale vision dataset continually with human-machine synergy.CoRR, abs/2203.07845,2022a.Yuanhan Zhang, Zhenfei Yin, Jing Shao, and Ziwei Liu.Benchmarking omni-vision representation through the lens of visual realms.In ECCV, pp.594-611, 2022b.Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola.Multimodal chain-of-thought reasoning in language models.CoRR, abs/2302.00923,2023.Multimodal Large Language Models . . . . . . . . . . . . . . . . .........A.2 Benchmarks for Large Language Models . . . . . . . . . . . . . . .........A.3 Benchmarks for Multimodal Large Language Models . . . . . . . .........Scenario . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .........B.2 Instruction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .........B.3 Inferencer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .........B.4 Metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .........Ye et al., 2023) leverages the capabilities of pre-trained LLMs, a visual knowledge module, and a connected visual abstractor module to effectively align images with text.LAMM (Yin et al., 2023) extend the research of MLLMs to point clouds and propose a training framework optimized for modalities' extension.Otter(Li et al., 2023a)utilizes multimodal context instruction tuning data, demonstrating an improved ability to follow instructions and in in-context learning.LLaMA-Adapter-v2(Gao et al., 2023)propose an early fusion strategy to solve the interference between image-text alignment and instruction following learning targets.Shikra(Chen et al., 2023a)and Kosmos-2(Peng et al., 2023)integrate grounding data during the training phase, enabling the model to develop grounding capabilities.In order to comprehensively assess the capabilities of these MLLMs, we present the first Comprehensive Evaluation Framework (ChEF) that can holistically profile each MLLM and fairly compare different MLLMs.
Preprint A Related Works A.1 B ChEF (Comprehensive Evaluation Framework) Modules A RELATED WORKS A.1 MULTIMODAL LARGE LANGUAGE MODELS Due to the success of large Language models (LLMs) like GPTs (Radford et al., 2019; Brown et al., 2020; Ouyang et al., 2022), LLAMA (Touvron et al., 2023) and Vicuna (Chiang et al., 2023), Mul-timodal Large Language Models (MLLMs) have recently experienced substantial development. In-structBLIP (Dai et al., 2023), LLaVA (Liu et al., 2023a), and MiniGPT-4 (Zhu et al., 2023) are based on open-source LLMs using vision-language instruction tuning get promising results. mPLUG-B.1 Preprint Owl (10592,2023.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.Judging llm-as-a-judge with mt-bench and chatbot arena.CoRR, abs/2306.05685,2023.Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.Minigpt-4: Enhancing vision-language understanding with advanced large language models.CoRR, abs/2304.C Desiderata C.1 Calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .C.2 In-context Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .C.3 Instruction Following . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .C.4 Language Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .C.5 Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .C.6 Hallucination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .D Experiments: Details of Evaluation Setup D.1 Details of the Evaluated Models . . . . . . . . . . . . . . . . . . . . . . . . . . .D.2 Default Recipes for Scenarios . . . . . . . . . . . . . . . . . . . . . . . . . . . .D.3 Recipes for Desiderata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .E Empirical Experiments on Desiderata E.1 Calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .E.2 In-context Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .E.3 Instruction Following . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .E.4 Language Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .E.5 Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .E.6 Hallucination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .F ChEF Provides Reliable Assessments of Desiderata G Evaluation on GPT-4V(ision) and Bard G.1 Evaluation Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .G.2 Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</p>
<p>often focus on building a multimodal evaluation dataset for MLLMs.These benchmarks have been designed to transform open-ended predictions into predefined categorical choices.For instance, MME transforms free-form responses into binary True/False questions, whileLi et al.</p>
<p>Liu et al. (2023c) (2023c)employ multi-choice questions.However, the efficacy of these benchmarks is contingent upon the quality of the dataset construction and may suffer from scalability issues.More recently, efforts such as Yin et al. (2023);Xu et al. (</p>
<p>Table 2 :
2
Image corruption methods are categorized into five types.In the robustness experiments, the corruption for each image is formed by sequentially combining methods each with random severity level from the following five categories: Noise, Blur, Weather, Digital, and Other.Each category's method is selected based on the corresponding combination strategy: Random denotes the random selection of one method from all methods within that category, while Sequential implies the consecutive execution of all methods within that category.Severity represents the number of adjustable severity levels for the corruption method.
CategoryMethodSeverity Compose StrategyGaussian Noise5NoiseShot Noise Impulse Noise5 5RandomSpeckle Noise5Defocus Blur5Frosted Glass Blur5BlurMotion Blur5RandomZoom Blur5Gaussian Blur5Snow5Frost5WeatherFog5RandomBrightness5Spatter5Contrast5Elastic5DigitalPixelate5RandomJPEG Compression5Saturate5Center Crop5OtherResize5SequentialRotate5</p>
<p>Table 4 :
4
Details of the evaluated MLLMs.mPLUG stands for mPLUG-Owl and LAv2 stands for LLaMA-Adapter-v2.
MLLMVisual ModelLanguage Model Overall ParameterLLaVACLIP ViT-L/14MPT 7B7BLAMMCLIP ViT-L/14Vicuna 13B13BMiniGPT-4EVA-GVicuna 7B8BmPLUGCLIP ViT-L/14LLaMA 7B7BOtterCLIP ViT-L/14LLaMA 7B9BLAv2CLIP ViT-L/14LLaMA 7B7BInstructBLIPEVA-GVicuna 7B8BShikraCLIP ViT-L/14LLaMA 7B7BKosmos-2CLIP ViT-L/14Decoder 1.3B1.6B</p>
<p>Table 5 :
5
Success rate in choice extraction on MMBench.The results represent the success rate in choice extraction of Step-1, which is defined in MMBench.MMBench released the evaluation code for three models.The results in ChEF are aligned with those in MMBench.
MMBench ChEFLLaVA14.8514.78MiniGPT-455.5852.52InstructBLIP91.291.52</p>
<p>Table 6 :
6
Details of default Recipes.Acc. is accuracy.CoT → PPL means Multi-Turn with CoT in the first turn and PPL in the second.
ScenarioInstructionInferencerMetricCIFAR10Standard QueryPPLAcc.Omnibenchmark Standard Query Multi-Turn PPL WeightedACCFlickr30kStandard QueryPPLAcc.VOC2012Standard Query Multi-Turn PPLAcc.FSC147Standard QueryPPLAcc.ScienceQAStandard QueryCoT → PPLAcc.MMBenchStandard QueryCoT → PPLAcc.MMEStandard QueryPPLAcc.SEEDBenchStandard QueryPPLAcc.</p>
<p>Table 7 :
7
Results of VanillaEval and CircularEval on MMBench.The results reveal a substantial decrease in accuracy when switching from VanillaEval to CircularEval.
VanillaEval CircularEvalLLaVA43.1310.24LAMM44.4714.21MiniGPT-454.3426.46mPLUG49.5712.24Otter53.9126.27LAv257.0624.01InstructBLIP65.7346.8Shikra63.2643.08Kosmos-232.821.2</p>
<p>Table 8 :
8
Details of Recipes for six dimensions of desiderata.ICL is in-context learning.Ins.Follow. is instruction following and Lang.Perf. is language performance.
DesiderataScenarioInstructionInferencerMetricCalibrationMMBench + ScienceQA Standard Query CoT → PPLECEICLMMBench + ScienceQARandom ICECoT → PPLRIAMIns. Follow.MMBench + ScienceQA Standard Query CoT → PPLMRLang. Perf.ScienceQAStandard Query CoT → PPL GPT-based MetricRobustnessMMBench + ScienceQA Standard Query CoT → PPLMRRHallucinationMSCOCOStandard QueryPPLAcc</p>
<p>Table 9 :
9
Results of calibration.Acc.stands for accuracy and ECE is the Expected Calibration Error.The overall score is calculated through 1 -weighted average ECE, representing the reliability of the model's prediction probability.The entries that are both bold and underlined indicate the best performance.
MLLMScenarioScienceQA Acc. ↑ ECE% ↓ Acc. ↑ ECE% ↓ MMBenchOverallLLaVA46.557.2644.1314.6690.01LAMM52.7520.7944.4728.5276.36MiniGPT-447.0015.2854.3415.2484.73mPLUG48.4415.7249.5715.4784.15Otter50.2221.1053.9110.5282.80LAv254.348.1757.0614.1989.61InstructBLIP55.1810.5765.736.2591.25Shikra45.2114.5763.266.6588.35Kosmos-234.6010.6332.8211.1389.19</p>
<p>Table 10 :
10
Results of instruction following.The abbreviations we use are: Acc for original accuracy; Acc vm for the weighted average accuracy for different instructions of verbalizer manipulation; MR for the weighted average match ratio for different instructions of verbalizer manipulation, as defined in Section C; Avg. for an average of results on ScienceQA and MMBench.The entries that are both bold and underlined indicate the best performance.Accvm ↑ MR% ↑ Acc↑ Accvm ↑ MR% ↑ Acc ↑ Accvm ↑ MR% ↑
ScenarioScienceQAMMBenchAvg.MLLM Acc ↑ LLaVA 46.5541.1046.2344.1335.0239.6045.6638.8643.79LAMM52.7541.4142.4144.4734.1134.7249.7038.7239.58MiniGPT-447.0036.9543.0154.3441.8143.7849.7038.7443.29mPLUG48.4439.9340.2849.5735.3933.4348.8638.2537.76Otter50.2238.6538.3053.9133.2936.9051.5836.6737.78LAv254.3441.7144.4057.0627.3828.8355.3436.4338.66InstructBLIP55.1838.2345.0765.7337.5943.4659.0738.0044.47Shikra45.2135.8037.8963.2631.5832.9151.8634.2436.05Kosmos-234.6035.3617.7032.8232.1714.1933.9434.1816.41
Table10reports the results of instruction following on ScienceQA and MMBench.We also report the original accuracy Acc and the weighted average accuracy Acc vm of different verbalizer manipulation instructions.To further explore the instruction following, we show the results of different verbalizer manipulations in Figure17.We also provide the results in Figure17, that follow the ranking of different groups of instructions in alignment with prior knowledge (natural &gt; neutral &gt; unnatural), where MR also decreases sequentially.The observations are as follows:</p>
<p>Table 11 :
11
Results of robustness.Acc represents the original accuracy without corruptions; Acc crp represents the accuracy after image and text corruptions; RRM% is Relative Robustness for multichoice; Avg. is the weighted average results on ScienceQA and MMBench; As Kosmos-2 † degenerates into random guessing, the results are meaningless.The entries that are both bold and underlined indicate the best performance.
ScenarioScienceQAMMBenchAvg.MLLMAcc ↑ Acccrp ↑ RRM% ↑ Acc ↑ Acccrp ↑ RRM% ↑AccAcccrp ↑ RRM% ↑LLaVA46.5543.4571.1344.1335.8550.0345.6640.6563.36LAMM52.7543.8947.7544.4740.3375.5249.7042.5857.98MiniGPT-447.0042.2957.9554.3444.8764.6149.7043.2460.40mPLUG48.4442.8755.9349.5736.9642.6848.8640.6951.05Otter50.2244.1958.1653.9142.1855.4751.5843.4557.17LAv254.3449.3172.8957.0643.0552.5055.3447.0165.38InstructBLIP55.1849.7271.8365.7356.0452.5059.0752.0572.85Shikra45.2139.1735.8163.2652.0768.6551.8643.9247.01Kosmos-2 †34.6035.4826.6732.8228.4015.8733.9432.8722.69͘͞Ǥ͘͝Ǥ ǤƬǤǤ͚͙͛͘͘͘͘͘͜Ǧ͚͜ Ǧ͚</p>
<p>Table 12 :
12
Results of Hallucination.Acc represents the accuracy of prediction; Precision represents how many of the predicted positive samples are true positive samples; Recall represents how many of all true positive samples are correctly identified; and Yes% represents the probability that the model outputs a yes answer.The entries that are both bold and underlined indicate the best performance.
DatasetMLLMAccPrecision Recall F1 Score Yes%LLaVA51.5551.5510068.03100LAMM53.8454.1252.9169.1995.53MiniGPT-480.9389.6771.2079.3840.92mPLUG55.8153.8599.8069.9595.53MSCOCO-RandomOtter82.2789.1174.7381.2943.23LAv275.4069.549379.5868.93InstructBLIP 90.2493.5587.0690.1947.97Shikra87.1887.0088.3387.6652.33Kosmos-251.5551.5510068.03100LLaVA505010066.67100LAMM505099.9366.6599.93MiniGPT-474.375.472.1373.7347.83mPLUG49.9749.9899.866.699.83MSCOCO-PopularOtter73.5773.0374.7373.8751.17LAv259.1055.429369.4583.90InstructBLIP 83.3781.0787.0783.9653.7Shikra83.380.2588.3384.1055.03Kosmos-2505010066.67100LLaVA505010066.67100LAMM50.1350.0699.6066.6499.47MiniGPT-472.1772.5171.4071.9549.23mPLUG50.0650.0399.8066.6599.73MSCOCO-AdverarialOtter70.0768.3574.7371.4054.67LAv256.7753.929368.2786.23InstructBLIP 80.6377.1487.0781.8056.43Shikra79.2774.7888.338159.07Kosmos-2505010066.67100
More related works are provided in Supplementary Materials (Section A).
Details of these four components are provided in Supplementary Materials (Section B).
MMBench provides two evaluation settings (i.e., VanillaEval and CircularEval). VanillaEval is adopted in the default Recipe.
The default Recipe is also demonstrated to display and approach the best performance of each MLLM, as shown in Figure6(a-b).
More evidence of reliability is provided in the Supplementary Materials (Section F).
Question: The passage below describes an experiment.Read the passage and then follow the instructions below.Madelyn applied a thin layer of wax to the underside of her snowboard and rode the board straight down a hill.Then, she removed the wax and rode the snowboard straight down the hill again.She repeated the rides four more times, alternating whether she rode with a thin layer of wax on the board or not.Her friend Tucker timed each ride.Madelyn and Tucker calculated the average time it took to slide straight down the hill on the snowboard with wax compared to the average time on the snowboard without wax.(3) Perplexity (PPL): This Inferencer constrains MLLMs' output within a limited text scope, named as answer pool, and derives the answer by computing the likelihood.The answer pool is either fixed, retrieved, or generated based on the specific Scenario.For example, in multi-choice questionanswering Scenarios, the answer pool is the four options {A, B, C, D}.For certain Scenarios, it includes the ground-truth answer and several negative candidates either generated or retrieved.PPL confines the model's output within a specific range, guaranteeing that the model selects exactly matched answers based on discrimination rather than generating similar responses.Treating MLLMs as discriminative entities for specific Scenario evaluation enhances objectivity and reliability in the evaluation process.(4) Multi-Turn: This method decomposes complex tasks into subtasks and generates answers sequentially based on each subtask.For example, in the context of object detection, the initial Instruction may pertain to the object categories present in the image, followed by subsequent inquiries regarding the bounding boxes for each detected object category.This approach supports objective and reliable evaluation by assessing the model's responses to each subtask, thereby enhancing objectivity and reliability.Notably, various Inferencers can be invoked and seamlessly integrated with one another within multiple turns.For illustration, the CoT can be employed during the initial turn, while the subsequent turn can leverage the Direct.These Inferencers augment the evaluation framework of ChEF, enabling more objective and trustworthy assessments of model performance.B.4 METRICThe selection of Metrics is crucial when evaluating MLLMs, as it should encompass the evaluation capabilities for traditional visual tasks while considering the novel characteristics of MLLMs as generative models.In the context of traditional computer vision tasks, we believe it is more suitable to conduct adaptation based on the existing evaluation metrics.As a result, within the ChEF  To measure MLLMs' ICL ability, we utilize ICE as Instruction for each specific Scenario.We compute their accuracy and use the relative accuracy change as the final score.Specifically, we compute the accuracy under the 0-shot setting (without using ICE) and the average accuracy values for varying numbers of ICE, ranging from 1 to N .In multi-choice question-answering paradigms, random guessing can yield an expected lower-bound accuracy, which can be misleading in terms of performance evaluation.To mitigate the impact of this potentially deceptive performance on robustness assessments, we systematically eliminate the bias introduced by random choice.Therefore, we introduce the Relative ICL Accuracy for Multi-choice (RIAM), adapted from Chen et al. (2023b);Schiappa et al. (2022), to more accurately assess the model's ICL ability.The RIAM primarily calculates the relative accuracy change of the model before and after using ICE.ICE with image ICE without imageC.3 INSTRUCTION FOLLOWINGTaking inspiration from(Li et al., 2023c), we utilize three groups of instructions for verbalizer manipulation: natural, neutral, unnatural, to evaluate how well models can follow instructions that may not align with their priors.The levels in terms of aligning with prior knowledge of these three groups are ranked as natural &gt; neutral &gt; unnatural.We expect the model to answer the question following instructions and generate a new answer corresponding to the original answer.In practice, we select different numbers of verbalizers for each group of verbalizer manipulation, depending on the alignment with the model's prior knowledge.Each verbalizer maps "A|B|C|D" to different new options.(1) Natural."1|2|3|4|5","I|II|III|IV|V" and "first|second|third|fourth|fifth".(1) We observed that some MLLMs do not experience a significant decrease in Acc vm compared to Acc when the MR is low.This can be attributed to cases where the original response is incorrect but become correct after verbalizer manipulation.On the other hand, questions that are initially answered correctly remain largely consistent between before and after verbalizer manipulation.This suggests that the models exhibit higher instruction following ability on confident questions but are more susceptible to disturbance on uncertain questions.It indicates a correlation between instruction following and confidence in question answering.(2) The distributions of Acc and MR are entirely different, where the distribution of MR is more discriminative.For Kosmos2, Acc vm increases because its original accuracy is lower than that of random guessing (35.80 on ScienceQA, 27.57 on MMBench), and random guessing improves accuracy.Kosmos-2 exhibits a similar Acc vm to Shikra but has the lowest MR, further confirming that Kosmos-2 has degenerated into random guessing, leading to its poor performance on instruction following.(3) The results of natural and neutral are significantly higher than that of unnatural, suggesting that the model is more likely to follow instructions in the natural and neutral categories.This is further supported by their probabilities being higher than random guessing, indicating that the model indeed possesses a certain level of understanding of these two sets of instructions rather than making random guesses.On the other hand, most of the unnatural instructions perform well below the level of random guessing, demonstrating that following unnatural instructions is highly challenging for current MLLMs.(4) InstructBLIP performs best in the natural category but exhibits a noticeable performance drop in the neutral category, suggesting that InstructBLIP relies more on prior knowledge for instruction understanding rather than comprehending new instruction content.E.4 LANGUAGE PERFORMANCETable18(a) provides the results of language performance.To illustrate the role of selective sampling of correct, Figure18(b) displays the distribution of accuracy across different score ranges of language performance scores.We also provide a typical example in Figure19, comparing the disparity in language performance between LLaVA and Shikra when both provide correct answers.The following presents our key observations:(1) Kosmos-2 exhibits poor performance due to its inability to provide reasoning processes in practical multi-choice question-answering Scenarios.Conversely, Shikra demonstrates relatively weak performance attributed to its incapacity to deliver reasoning analysis.Despite prompts intended to elicit the reasoning process, Shikra tends to provide direct answers, leading to lower scores.In real[Assistant's Chain of Thought]The animal's feet that are adapted for digging are the groundhog's feet.The image shows two groundhogs, one with its feet on the ground and the other with its feet in the air.This indicates that the groundhog's feet are well-suited for digging and burrowing, which is a natural behavior for groundhogs.<a href="A">Assistant's Final Choice</a>LLaVA [Explanation]The AI assistant's answer is helpful, relevant and accurate in terms of the question asked.The assistant correctly identified the groundhog's feet as being adapted for digging, which is the correct answer as provided by the ground truth.Furthermore, the assistant's inference based on the visual clues in the image -"one with its feet on the ground and the other with its feet in the air" -demonstrates an adequate depth of understanding of the scenario and the natural behavior of this particular animal.The method of deduction and articulation the AI utilized was detailed and appropriate for the context of the question, showing however no special creativity.This showcases the strengths of a multimodal AI in its ability to synthesize visual and contextual data for accurate interpretation.Shikra [Explanation]The assistant did provide the correct answer, which implies some level of accuracy.However, the chain of thought mentioned does not directly connect to the final choice.The assistant indicates two small black and white animals resting on the sand but then proceeds to say the answer is a badger, which is not among the provided options.This inconsistency in the chain of thought may lead to confusion for the user.Regardless, the assistant ultimately provided the correct answer but its line of reasoning wasn't explicitly clear and did not align with the provided options.The assistant could have directly related the image to a groundhog's known ability to dig, providing a more accurate, relevant, and detailed response.[Overall Score] 6GPT-4 Shikra's Language PerformanceLLaVA's Language PerformanceGPT-4 [Question]Question: Which animal's feet are also adapted for digging?Options:Figure19: Examples of language performance evaluation on Shikra and LLaVA, where two models exhibit varying levels of natural language generation quality.GPT-4 generates an evaluation explanation as evidence and then generates an overall score based on the Chain-of-Thought and the final choice generated by the Assistant.Here, we present only one explanation and the overall score generated by GPT-4.Note that in practice, for each sample, GPT-4 generates five explanations and their corresponding overall scores through sampling.The final score for the sample is obtained by averaging these five overall scores.E.6 HALLUCINATIONTable12presents the evaluation results for different difficulty levels of hallucination.Among them, LLaVA, LAMM, mPLUG-Owl, and Kosmos-2 exhibit more severe hallucination issues, as they tend to answer "Yes" very easily.This leads to nearly 100% Recall but with Acc and Precision both close to 50%, akin to random selection.Apart from these four models, the other models achieved relatively meaningful results.Overall, InstructBLIP achieved the best results, while Shikra also performed competitively, with an average accuracy being only 1.72% lower than InstructBLIP's.We evaluate GPT-4V(ision) OpenAI (2023a) andBard Google. (2023)on MMBench and ScienceQA scenarios, as well as the desiderata including in-context learning, instruction following, hallucination, and robustness.Given the API-only access to these two models, we bypass calibration measures due to unavailability of logits and decide against using GPT-4 for language performance evaluation of GPT-4V responses.We extract 30 data samples from ScienceQA and MMBench respectively for both scenario evaluations and each of the desideratum evaluation.10 samples from each of three categories within the COCO dataset are extracted to specifically test for hallucination.During the in-context learning evaluation, for Bard, which accepts only single-image inputs, the given instruction is the ICE without image, as defined in Section C.2. Conversely, for GPT-4V, which supports multi-image inputs, we utilize the ICE with image.PreprintG.2 EVALUATION RESULTSTable13presents the comparison of performance on the same data samples among GPT-4V, Bard and three open-source multimodal large models, LLaVA, Otter, and mPLUG-Owl.It is evident that GPT-4V outperforms the other models in the majority of tasks, while Bard, although inferior to GPT-4V, still significantly surpasses the other two MLLM models.Both Bard and GPT-4V achieve remarkably high accuracy in the ScienceQA and MMBench scenarios.However, in the ICL evaluation, the performance of GPT-4V and Bard is lower compared to LLaVA and Otter, which is specifically trained on in-context instruction tuning data.Figures22  and 23illustrate examples of GPT-4V and Bard performance in the ICL evaluation.It can be observed that Bard provides detailed answers, but it does not adhere to the output format given by the in-context example (ICE), whereas GPT-4V exhibits a better understanding of the ICE, resulting in slightly higher ICL performance than Bard.However, both models experience a decrease in accuracy compared to the performance without ICE instruction, indicating that the content of the ICE might influence the models' final answer.This issue is commonly encountered in existing MLLMs.As ICL capability is significant in real-world multimodal interactions, the results highlight the need for future work to place greater emphasis on this ability.On the ability of instruction following, GPT-4V significantly outperforms Bard.As shown in figure 24, Bard fails to follow the instructions and directly provides an answer, whereas GPT-4V is capable of providing a complete explanation, mentioning the initial selection and the final choice based on the instructions.Regarding robustness, as depicted in figure25and 26, both GPT-4V and Bard demonstrate an understanding of images and text with added noise, although to some extent, it affects the models' final answers.In terms of hallucination evaluation, both GPT-4V and Bard achieve high performance.GPT-4V and Bard have nearly approached the upper bounds of our ChEF benchmark's performance metrics, markedly outperforming existing open-source multimodal large language models.ChEF is mainly intended for the broad research community, aiming to inspire continuous improvement and advancement in open-source models.PreprintFigure26: Examples of robustness evaluation on Bard.The left subfigure is an example of Bard giving the correct answer, while the right subfigure is an example of Bard giving the incorrect answer.
Let there be a clock on the beach: Reducing object hallucination in image captioning. Lluís Ali Furkan Biten, Dimosthenis Gómez, Karatzas, WACV2022</p>
<p>Visit-bench: A benchmark for vision-language instruction following inspired by real-world use. Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, Ludwig Schmidt, CoRR, abs/2308.065952023</p>
<p>On the opportunities and risks of foundation models. Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ B Altman, Simran Arora, Michael S Sydney Von Arx, Jeannette Bernstein, Antoine Bohg, Emma Bosselut, Erik Brunskill, Shyamal Brynjolfsson, Dallas Buch, Rodrigo Card, Niladri S Castellon, Annie S Chatterji, Kathleen Chen, Jared Quincy Creel, Dorottya Davis, Chris Demszky, Moussa Donahue, Esin Doumbouya, Stefano Durmus, John Ermon, Kawin Etchemendy, Li Ethayarajh, Chelsea Fei-Fei, Trevor Finn, Lauren Gale, Karan Gillespie, Noah D Goel, Shelby Goodman, Neel Grossman, Tatsunori Guha, Peter Hashimoto, John Henderson, Daniel E Hewitt, Jenny Ho, Kyle Hong, Jing Hsu, Thomas Huang, Saahil Icard, Dan Jain, Pratyusha Jurafsky, Siddharth Kalluri, Geoff Karamcheti, Fereshte Keeling, Omar Khani, Pang Wei Khattab, Mark S Koh, Ranjay Krass, Rohith Krishna, Kuditipudi, abs/2108.07258NeurIPS. Tom Brown, Benjamin Mann, Nick Ryder, Melanie SubbiahCoRR2021. 2020Language models are few-shot learners</p>
<p>Shikra: Unleashing multimodal llm's referential dialogue magic. Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, Rui Zhao, CoRR, abs/2306.151952023a</p>
<p>Benchmarking robustness of adaptation methods on pre-trained vision-language models. Shuo Chen, Jindong Gu, Zhen Han, Yunpu Ma, H S Philip, Torr, Volker Tresp, CoRR, abs/2306.020802023b</p>
<p>Can large language models be an alternative to human evaluations. David Cheng, -Han Chiang, Hung-Yi Lee, ACL. 2023</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, April 2023142023</p>
<p>Instructblip: Towards general-purpose visionlanguage models with instruction tuning. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven C H Hoi, CoRR, abs/2305.065002023</p>
<p>The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. M Everingham, L Van Gool, C K I Williams, J Winn, A Zisserman, 2012</p>
<p>MME: A comprehensive evaluation benchmark for multimodal large language models. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Rongrong Ji, CoRR, abs/2306.133942023</p>
<p>A framework for few-shot language model evaluation. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony Dipofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle Mcdonell, Niklas Muennighoff, 2021Version v0. 0.1. Sept</p>
<p>Llama-adapter V2: parameter-efficient visual instruction model. Preprint Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, Yu Qiao, CoRR, abs/2304.150102023</p>
<p>Sebastian Gehrmann, Tosin P Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Aremu Anuoluwapo, Antoine Bosselut, Raghavi Khyathi, Miruna-Adriana Chandu, Dipanjan Clinciu, Kaustubh D Das, Wanyu Dhole, Esin Du, Ondrej Durmus, Chris Dusek, Varun Emezue, Cristina Gangal, Tatsunori Garbacea, Yufang Hashimoto, Yacine Hou, Harsh Jernite, Yangfeng Jhamtani, Shailza Ji, Dhruv Jolly, Faisal Kumar, Aman Ladhak, Mounica Madaan, Khyati Maddela, Saad Mahajan, Mahamood, Prasad Bodhisattwa, Pedro Henrique Majumder, Angelina Martins, Simon Mcmillan-Major, Mille, Moin Emiel Van Miltenburg, Shashi Nadeem, Vitaly Narayan, Rubungo Nikolaev, Salomey Andre Niyongabo, Osei, P Ankur, Laura Parikh, Niranjan Perez-Beltrachini, Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, João Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla, Hendrik Cabezudo, Nishant Strobelt, Wei Subramani, Diyi Xu, Akhila Yang, Jiawei Yerukola, Zhou, CoRR, abs/2102.01672The GEM benchmark: Natural language generation, its evaluation and metrics. 2021</p>
<p>Gemv2: Multilingual NLG benchmarking in a single line of code. Sebastian Gehrmann, Abhik Bhattacharjee, Abinaya Mahendiran, Alex Wang, Alexandros Papangelis, Aman Madaan, Angelina Mcmillan-Major, Anna Shvets, Ashish Upadhyay, Bernd Bohnet, EMNLP. 2022</p>
<p>. Google, Bard, 2023</p>
<p>On calibration of modern neural networks. Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q Weinberger, ICML. 2017</p>
<p>Embracing change: Continual learning in deep neural networks. Raia Hadsell, Dushyant Rao, Andrei A Rusu, Razvan Pascanu, Trends in cognitive sciences. 2020</p>
<p>Benchmarking neural network robustness to common corruptions and perturbations. Dan Hendrycks, Thomas G Dietterich, CoRR, abs/1903.122612019</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, 10.1145/35717302023ACM Computing Surveys</p>
<p>OpenNMT: Opensource toolkit for neural machine translation. Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, Alexander Rush, ACL. 2017</p>
<p>Learning multiple layers of features from tiny images. Handbook of Systemic Autoimmune Diseases. A Krizhevsky, G Hinton, 20091</p>
<p>Otter: A multi-modal model with in-context instruction tuning. Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, Ziwei Liu, CoRR, abs/2305.037262023a</p>
<p>Seed-bench: Benchmarking multimodal llms with generative comprehension. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, Ying Shan, CoRR, abs/2307.161252023b</p>
<p>Instruction-following evaluation through verbalizer manipulation. Shiyang Li, Jun Yan, Hai Wang, Zheng Tang, Xiang Ren, Vijay Srinivasan, Hongxia Jin, CoRR, abs/2307.105582023c</p>
<p>Evaluating object hallucination in large vision-language models. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, Ji-Rong Wen, CoRR, abs/2305.103552023d</p>
<p>Holistic evaluation of language models. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D Manning, Christopher Ré, Diana Acosta-Navas, Drew A Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel J Orr, Lucia Zheng, Mert Yüksekgönül, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S Chatterji, Omar Khattab, Peter Preprint Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, Yuta Koreeda, CoRR, abs/2211.091102022</p>
<p>Microsoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, Lawrence Zitnick, ECCV. 2014</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, CoRR, abs/2304.084852023a</p>
<p>What makes good in-context examples for gpt-3? In DeeLIO 2022. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, 10.18653/v1/2022.deelio-1.102022</p>
<p>G-eval: NLG evaluation using GPT-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, CoRR, abs/2303.166342023b</p>
<p>Mmbench: Is your multi-modal model an all. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, Dahua Lin, CoRR, abs/2307.062812023c</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, NeurIPS. 2022</p>
<p>Obtaining well calibrated probabilities using bayesian binning. Gregory F Mahdi Pakdaman Naeini, Milos Cooper, Hauskrecht, AAAI. 2015</p>
<p>Gpt-4v(ision) system card. 2023aOpenAI</p>
<p>GPT-4 technical report. CoRR, abs/2303.087742023bOpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, NeurIPS. 352022</p>
<p>Kosmos-2: Grounding multimodal large language models to the world. Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei ; Jielin, Yi Qiu, Xingjian Zhu, Florian Shi, Zhiqiang Wenzel, Ding Tang, Bo Zhao, Mu Li, Li, CoRR, abs/2212.080442023. 2022Are multimodal models robust to image and text perturbations?</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Learning to count everything. Udbhav Viresh Ranjan, Thu Sharma, Minh Nguyen, Hoai, CVPR. 2021</p>
<p>Sentence-bert: Sentence embeddings using siamese bertnetworks. Nils Reimers, Iryna Gurevych, EMNLP-IJCNLP. 2019</p>
<p>Robustness analysis of video-language models against visual and language perturbations. Madeline Schiappa, Shruti Vyas, Hamid Palangi, Yogesh S Rawat, Vibhav Vineet, NeurIPS2022</p>
<p>Tiny lvlm-ehub: Early multimodal experiments with bard. Wenqi Shao, Yutao Hu, Peng Gao, Meng Lei, Kaipeng Zhang, Fanqing Meng, Peng Xu, Siyuan Huang, Hongsheng Li, Yu Qiao, Ping Luo, CoRR, abs/2308.037292023</p>
<p>Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes. Aarohi Preprint, Abhinav Srivastava, Abhishek Rastogi, Abu Rao, Md Awal, Abubakar Shoeb, Adam Abid, Adam R Fisch, Adam Brown, Aditya Santoro, Adrià Gupta, Agnieszka Garriga-Alonso, Aitor Kluska, Akshat Lewkowycz, Alethea Agarwal, Alex Power, Alex Ray, Alexander W Warstadt, Ali Kocurek, Ali Safaya, Alice Tazarv, Alicia Xiang, Allen Parrish, Aman Nie, Amanda Hussain, Amanda Askell, Ameet Dsouza, Anantharaman S Rahane, Anders Iyer, Andrea Andreassen, Andreas Santilli, Andrew M Stuhlmüller, Andrew Dai, Andrew K La, Andy Lampinen, Angela Zou, Angelica Jiang, Anh Chen, Animesh Vuong, Anna Gupta, Antonio Gottardi, Norelli, 2022Ayla KarakasArun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdemand et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. CoRR, abs/2206.04615</p>
<p>Selective annotation makes language models better few-shot learners. Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Tao Yu, ICLR. 2023</p>
<p>Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurélien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, CoRR, abs/2302.139712023</p>
<p>Evaluate &amp; evaluation on the hub: Better best practices for data and model measurements. Lewis Leandro Von Werra, Abhishek Tunstall, Sasha Thakur, Tristan Luccioni, Aleksandra Thrush, Felix Piktus, Nazneen Marty, Victor Rajani, Helen Mustar, Ngo, EMNLP. 2022</p>
<p>Is chatgpt a good NLG evaluator? A preliminary study. Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, Jie Zhou, CoRR, abs/2303.040482023a</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, CoRR, abs/2305.179262023b</p>
<p>Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, Jifeng Dai, CoRR, abs/2305.111752023c</p>
<p>Textflint: Unified multilingual robustness evaluation toolkit for natural language processing. Xiao Wang, Qin Liu, Tao Gui, Qi Zhang, Yicheng Zou, Xin Zhou, Jiacheng Ye, Yongxin Zhang, Rui Zheng, Zexiong Pang, Qinzhuo Wu, Zhengyan Li, Chong Zhang, Ruotian Ma, Zichu Fei, Ruijian Cai, Jun Zhao, Xingwu Hu, Zhiheng Yan, Yiding Tan, Yuan Hu, Qiyuan Bian, Zhihua Liu, Shan Qin, Bolin Zhu, Xiaoyu Xing, Jinlan Fu, Yue Zhang, Minlong Peng, Xiaoqing Zheng, Yaqian Zhou, Zhongyu Wei, Xipeng Qiu, and Xuanjing Huang. 2021ACL</p>
<p>Pandalm: An automatic evaluation benchmark for LLM instruction tuning optimization. Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, Yue Zhang, CoRR, abs/2306.050872023d</p>
<p>OpenICL: An open-source framework for in-context learning. Zhenyu Wu, Yaoxiang Wang, Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Jingjing Xu, Yu Qiao, ACL. 2023</p>
<p>Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models. Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, Ping Luo, CoRR, abs/2306.092652023</p>
<p>Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, Fei Huang, CoRR, abs/2304.14178mplug-owl: Modularization empowers large language models with multimodality. 2023</p>
<p>LAMM: language-assisted multimodal instruction-tuning dataset, framework, and benchmark. Jiong Zhenfei Yin, Jianjian Wang, Zhelun Cao, Dingning Shi, Mukai Liu, Lu Li, Lei Sheng, Xiaoshui Bai, Zhiyong Huang, Jing Wang, Wanli Shao, Ouyang, CoRR, abs/2306.066872023</p>            </div>
        </div>

    </div>
</body>
</html>
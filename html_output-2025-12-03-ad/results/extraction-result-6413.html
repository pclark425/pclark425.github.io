<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6413 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6413</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6413</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-276558421</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.15631v1.pdf" target="_blank">The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer</a></p>
                <p><strong>Paper Abstract:</strong> Large language models have demonstrated remarkable progress in mathematical reasoning, leveraging chain-of-thought and test-time compute scaling. However, many open questions remain regarding the interplay between reasoning token usage and accuracy gains. In particular, when comparing models across generations, it is unclear whether improved performance results from longer reasoning chains or more efficient reasoning. We systematically analyze chain-of-thought length across o1-mini and o3-mini variants on the Omni-MATH benchmark, finding that o3-mini (m) achieves superior accuracy without requiring longer reasoning chains than o1-mini. Moreover, we show that accuracy generally declines as reasoning chains grow across all models and compute settings, even when controlling for difficulty of the questions. This accuracy drop is significantly smaller in more proficient models, suggesting that new generations of reasoning models use test-time compute more effectively. Finally, we highlight that while o3-mini (h) achieves a marginal accuracy gain over o3-mini (m), it does so by allocating substantially more reasoning tokens across all problems, even the ones that o3-mini (m) can already solve. These findings provide new insights into the relationship between model capability and reasoning length, with implications for efficiency, scaling, and evaluation methodologies.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6413.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6413.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-4o-06-08-2024 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI model evaluated in this study; it underperforms the reasoning-focused o-series models on the Omni-MATH Olympiad-level benchmark, especially on multi-step mathematics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o-06-08-2024</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>OpenAI o-series (proprietary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>version tag (06-08-2024) as reported</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Omni-MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Olympiad-level multi-step mathematical problem solving (algebra, calculus, discrete math, geometry, number theory)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems (Omni-MATH entries with written solutions and exact answers)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Olympiad-level, stratified into Tier 1–Tier 4 (paper uses quartiles); gpt-4o reported across all tiers</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Vanilla user prompt: 'Solve the following problem. Enclose the final answer in a \boxed{{}} environment. Problem: {problem}' (model-generated chain-of-thought was allowed / measured)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (automated judged by Omni-Judge); conditional error rate vs. reasoning-token usage also reported</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>20–30% accuracy across math disciplines (paper: gpt-4o 'performs consistently between 20% and 30% for all math disciplines')</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper includes token-completion analysis for gpt-4o in Appendix (completion tokens include tokens leading up to the answer and the answer). Shows token usage patterns but main histograms and conditional-error analyses focus on reasoning models; gpt-4o lacks the reasoning-token patterns seen in o-series models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Relatively low accuracy; anomalous behavior where gpt-4o performs better on Tier 4 than on some intermediate tiers (suggesting unexpected heuristics or uneven sensitivity to mid-tier complexity).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not a primary test-time-compute reasoning model in this paper; does not show the same test-time compute reasoning benefits as the o-series models evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6413.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6413.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o1-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>o1-mini-12-09-2024 (OpenAI reasoning model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI test-time-compute-enabled reasoning model baseline in this study; substantially improves over gpt-4o but is outperformed by o3-mini variants, and shows greater accuracy decay with longer reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o1-mini-12-09-2024</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>OpenAI o-series (proprietary reasoning models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>version tag (12-09-2024) as reported</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Omni-MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Olympiad-level multi-step mathematical problem solving (algebra, calculus, discrete math, geometry, number theory)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems (Omni-MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Olympiad-level, Tier 1–Tier 4; paper stratifies analyses by tier</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Vanilla prompt (see gpt-4o entry) with chain-of-thought generated by the model and max_completion_tokens set (25,000 for o1-mini in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (Omni-Judge automated evaluator); conditional error rate by reasoning-token bins; logistic regression AME for token effect</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Approximately 40–60% accuracy across domains (paper: 'o1-mini significantly improves accuracy, reaching 40-60% across all domains'). Accuracy decay per 1000 reasoning tokens: −3.16% (average marginal effect from logistic regression controlling for difficulty & domain).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Detailed analysis of reasoning-token distribution and correctness: stacked histograms of correct vs incorrect answers by reasoning-token bins; conditional P(error | tokens>threshold) computed; QQ-plots comparing token distributions for correctly-solved questions vs other models show o1-mini's token distribution nearly identical to o3-mini(m).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Strong accuracy decay as reasoning tokens increase (larger AME magnitude), interpreted as 'overthinking' or that longer chains are associated with questions the model cannot solve; produces more incorrect answers at high token counts compared to stronger models.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Within paper's comparisons, o1-mini is less effective at using test-time compute (reasoning tokens) than o3-mini(m): it does not improve accuracy by using longer chains and exhibits steeper accuracy decay per token.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6413.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6413.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o3-mini (m)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>o3-mini-31-01-2025 medium (OpenAI reasoning model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A more proficient OpenAI reasoning-model generation (o3-mini medium) evaluated extensively; achieves higher accuracy without generating longer chains-of-thought than o1-mini, indicating more efficient use of reasoning tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o3-mini-31-01-2025 (medium)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>OpenAI o-series (proprietary reasoning models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>version tag (31-01-2025) medium (default) as reported</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Omni-MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Olympiad-level multi-step mathematical problem solving (algebra, calculus, discrete math, geometry, number theory)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems (Omni-MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Olympiad-level, Tier 1–Tier 4; analyses stratified by difficulty tiers and domains</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Vanilla prompt (user message) with chain-of-thought allowed; max_completion_tokens = 25,000 for o3-mini (m); test-time compute scaling used to record/allow long reasoning sequences; reasoning tokens measured and analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (Omni-Judge); conditional error rate vs reasoning tokens; logistic regression AME (effect of additional tokens on probability correct, controlling for difficulty & domain).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Achieves ≈50% accuracy in all categories (paper: 'o3-mini (m) further enhances performance, achieving 50% in all categories'). Accuracy decay per 1000 reasoning tokens: −1.96% (AME). Conditional error rate reaches 50% at ~12,000 reasoning tokens (empirical observation in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Extensive internal (behavioral) analysis: token-distribution histograms show o3-mini(m) and o1-mini have nearly identical reasoning-token distributions for correctly-solved questions, indicating o3-mini(m)'s superior accuracy is not due to longer chains but more effective reasoning; QQ-plots, stratified plots by tier & domain, and regression control for difficulty indicate that increased token usage correlates with lower accuracy even within tiers. The paper interprets this as stronger models 'thinking harder' (more effective steps), not 'longer'.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Accuracy typically declines as chain-of-thought length grows; longer chains are associated with problems the model cannot solve and with higher probability of final error. However, o3-mini(m) shows smaller error increase per token than o1-mini. No low-level mechanistic interpretability (e.g., activation probing, attention to digit tokens) is reported—analysis is behavioral/statistical.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Compared to o1-mini, o3-mini(m) demonstrates better utilization of test-time compute: it attains higher accuracy without increasing average reasoning-token usage; per-token accuracy decay is smaller, indicating more efficient scaling of reasoning capability within model family.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6413.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6413.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o3-mini (h)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>o3-mini-31-01-2025 high (OpenAI reasoning model with reasoning_effort=high)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI o3-mini variant configured with high reasoning effort (test-time compute) that attains modest accuracy gains over o3-mini(m) primarily by allocating substantially more reasoning tokens to every problem.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o3-mini-31-01-2025 (high)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>OpenAI o-series (proprietary reasoning models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>version tag (31-01-2025) high configuration (reasoning_effort=high) as reported</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Omni-MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Olympiad-level multi-step mathematical problem solving (algebra, calculus, discrete math, geometry, number theory)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems (Omni-MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Olympiad-level, Tier 1–Tier 4</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Vanilla prompt (user message) with chain-of-thought allowed; reasoning_effort parameter set to 'high'; max_completion_tokens = 100,000 for o3-mini (h) to permit very long reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (Omni-Judge); conditional error rate vs reasoning tokens; logistic regression AME.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Approximately +4% average accuracy over o3-mini(m); surpasses 80% accuracy for Algebra and Calculus per-domain cells (paper states 'o3-mini (h) improves with approximately 4% on average compared to o3-mini (m) and surpasses 80% accuracy for Algebra and Calculus'). Accuracy decay per 1000 reasoning tokens: −0.81% (AME). Conditional error rate reaches 50% at ~30,000 reasoning tokens (empirical observation).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral analyses show o3-mini(h) has a significantly stretched reasoning-token distribution (allocating up to >50,000 tokens for some problems); QQ-plots show o3-mini(h) uses more tokens for all correctly answered questions than o3-mini(m). The slower per-token accuracy decay is attributed largely to this stretched token distribution (x-axis) rather than substantially more effective per-token reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Marginal accuracy improvements come at large extra computational cost (much longer chains across problems, even for ones already solved by o3-mini(m)); longer chains still correlate with higher error probability, albeit less steeply than weaker models. The paper notes potential issues with judge reliability and prompt refusal for a few questions.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Increasing test-time compute (reasoning_effort=high, much larger max_completion_tokens) yields modest accuracy gains but with heavy token cost; per-token decay is smaller, but overall gains are expensive — interpreted as 'thinks harder, not longer' nuance: stronger models use compute more effectively, but raising reasoning_effort can simply make a model allocate more tokens globally.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6413.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6413.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Omni-MATH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Omni-math: A universal olympiad level mathematic benchmark for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A comprehensive Olympiad-level mathematics benchmark used in this study (4428 problems) spanning many sub-domains and stratified difficulty tiers; includes reference answers and written solutions enabling automated evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Omni-math: A universal olympiad level mathematic benchmark for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Omni-MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Olympiad-level multi-step mathematical problem solving across domains (Algebra, Applied Math, Calculus, Discrete Math, Geometry, Number Theory)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language math problems with reference answers and written solutions (used for automated assessment by Omni-Judge)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Stratified into four difficulty tiers (Tier 1–Tier 4) derived from quartiles of difficulty scores; problems reach Olympiad difficulty</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Used with models via a vanilla user prompt; Omni-MATH supplies problems and reference solutions for judge evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (model answers judged by Omni-Judge), domain- & tier-stratified accuracy heatmaps, reasoning-token histograms, conditional-error vs token counts, logistic regression AME</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Dataset size: 4428 problems; used to report per-model accuracies (see model entries).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Benchmark enables analysis of chain-of-thought length vs accuracy by domain and tier; authors use Omni-Judge to automatically assess equivalence to reference answers and to parse model outputs; allows token-stratified analyses and logistic regression controlling for domain and difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Automated judge (Omni-Judge) may diverge from human judgements; Omni-Judge validated primarily on o1-mini, not o3-mini, leaving small possibility of judge-model overlap or mis-evaluation. Some model outputs failed parsing (<1%) and were omitted.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Benchmark demonstrates that increased reasoning-token usage (test-time compute) is necessary for harder problems but exhibits diminishing returns; stronger models show smaller negative marginal effects per extra token.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6413.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6413.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompting & evaluation setup</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vanilla prompt + chain-of-thought generation, max_completion_tokens limits, and Omni-Judge automated evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The experimental prompting and evaluation protocol used: simple instruction prompt, long max_completion_tokens values to permit extended chain-of-thought, a reasoning_effort flag for o3-mini(h), and Omni-Judge for automated answer equivalence checking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Omni-MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step mathematical problem solving with model-generated chain-of-thought</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompt per-problem: 'Solve the following problem. Enclose the final answer in a \boxed{{}} environment. Problem: {problem}'</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Tier 1–Tier 4 (paper stratification)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Vanilla single-shot prompt; chain-of-thought is generated by the models at inference (no manual chain-of-thought few-shot template used). Test-time compute scaling settings: max_completion_tokens = 25,000 for o1-mini and o3-mini(m), 100,000 for o3-mini(h); o3-mini(h) achieved by setting reasoning_effort='high'.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (automated equivalence judged by Omni-Judge); conditional probability P(False | T > threshold) calculated from token histograms; logistic regression average marginal effects (AME) for tokens' effect on correctness controlling for domain & difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Not a model but experimental settings: max_completion_tokens = 25,000 (o1-mini, o3-mini m), 100,000 (o3-mini h); Omni-Judge max_new_tokens = 300 for judging requests. Empirical observations: longer allowed tokens enabled chains up to >50,000 tokens for o3-mini(h).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Protocol permitted measurement of 'reasoning tokens' (the number of tokens in the chain-of-thought before the final boxed answer); analyses included stacked histograms of correct vs incorrect responses by token bins, conditional error curves, QQ-plots, and logistic regression controlling for domain and difficulty. Authors caution that prompting strategy may not generalize to other prompt styles and that Omni-Judge is an automated judge with known validation limits.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Some prompts were refused by models (flagged invalid) and omitted. Judge (Omni-Judge) can disagree with human judgement in some cases; interactions between prompting style and test-time compute may influence token usage and accuracy in ways not fully explored.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Test-time compute (per-problem reasoning tokens and reasoning_effort settings) strongly affect token allocation and can increase accuracy but with diminishing returns and potential large cost (o3-mini(h) example); authors report log-linear performance gains in referenced literature but find in-practice that stronger models can get higher accuracy without longer chains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Omni-math: A universal olympiad level mathematic benchmark for large language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>On the overthinking of o1-like LLMs <em>(Rating: 2)</em></li>
                <li>The impact of reasoning step length on large language models <em>(Rating: 2)</em></li>
                <li>Same task, more tokens: the impact of input length on the reasoning performance of large language models <em>(Rating: 2)</em></li>
                <li>Simple test-time scaling <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6413",
    "paper_id": "paper-276558421",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "gpt-4o",
            "name_full": "gpt-4o-06-08-2024 (OpenAI)",
            "brief_description": "An OpenAI model evaluated in this study; it underperforms the reasoning-focused o-series models on the Omni-MATH Olympiad-level benchmark, especially on multi-step mathematics.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-4o-06-08-2024",
            "model_family": "OpenAI o-series (proprietary)",
            "model_size": "version tag (06-08-2024) as reported",
            "training_data_description": null,
            "benchmark_name": "Omni-MATH",
            "task_type": "Olympiad-level multi-step mathematical problem solving (algebra, calculus, discrete math, geometry, number theory)",
            "problem_format": "Natural-language word problems (Omni-MATH entries with written solutions and exact answers)",
            "difficulty_level": "Olympiad-level, stratified into Tier 1–Tier 4 (paper uses quartiles); gpt-4o reported across all tiers",
            "prompting_method": "Vanilla user prompt: 'Solve the following problem. Enclose the final answer in a \\boxed{{}} environment. Problem: {problem}' (model-generated chain-of-thought was allowed / measured)",
            "performance_metric": "Accuracy (automated judged by Omni-Judge); conditional error rate vs. reasoning-token usage also reported",
            "performance_value": "20–30% accuracy across math disciplines (paper: gpt-4o 'performs consistently between 20% and 30% for all math disciplines')",
            "internal_analysis": "Paper includes token-completion analysis for gpt-4o in Appendix (completion tokens include tokens leading up to the answer and the answer). Shows token usage patterns but main histograms and conditional-error analyses focus on reasoning models; gpt-4o lacks the reasoning-token patterns seen in o-series models.",
            "failure_modes": "Relatively low accuracy; anomalous behavior where gpt-4o performs better on Tier 4 than on some intermediate tiers (suggesting unexpected heuristics or uneven sensitivity to mid-tier complexity).",
            "scaling_trend": "Not a primary test-time-compute reasoning model in this paper; does not show the same test-time compute reasoning benefits as the o-series models evaluated.",
            "uuid": "e6413.0",
            "source_info": {
                "paper_title": "The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "o1-mini",
            "name_full": "o1-mini-12-09-2024 (OpenAI reasoning model)",
            "brief_description": "An OpenAI test-time-compute-enabled reasoning model baseline in this study; substantially improves over gpt-4o but is outperformed by o3-mini variants, and shows greater accuracy decay with longer reasoning chains.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "o1-mini-12-09-2024",
            "model_family": "OpenAI o-series (proprietary reasoning models)",
            "model_size": "version tag (12-09-2024) as reported",
            "training_data_description": null,
            "benchmark_name": "Omni-MATH",
            "task_type": "Olympiad-level multi-step mathematical problem solving (algebra, calculus, discrete math, geometry, number theory)",
            "problem_format": "Natural-language word problems (Omni-MATH)",
            "difficulty_level": "Olympiad-level, Tier 1–Tier 4; paper stratifies analyses by tier",
            "prompting_method": "Vanilla prompt (see gpt-4o entry) with chain-of-thought generated by the model and max_completion_tokens set (25,000 for o1-mini in experiments)",
            "performance_metric": "Accuracy (Omni-Judge automated evaluator); conditional error rate by reasoning-token bins; logistic regression AME for token effect",
            "performance_value": "Approximately 40–60% accuracy across domains (paper: 'o1-mini significantly improves accuracy, reaching 40-60% across all domains'). Accuracy decay per 1000 reasoning tokens: −3.16% (average marginal effect from logistic regression controlling for difficulty & domain).",
            "internal_analysis": "Detailed analysis of reasoning-token distribution and correctness: stacked histograms of correct vs incorrect answers by reasoning-token bins; conditional P(error | tokens&gt;threshold) computed; QQ-plots comparing token distributions for correctly-solved questions vs other models show o1-mini's token distribution nearly identical to o3-mini(m).",
            "failure_modes": "Strong accuracy decay as reasoning tokens increase (larger AME magnitude), interpreted as 'overthinking' or that longer chains are associated with questions the model cannot solve; produces more incorrect answers at high token counts compared to stronger models.",
            "scaling_trend": "Within paper's comparisons, o1-mini is less effective at using test-time compute (reasoning tokens) than o3-mini(m): it does not improve accuracy by using longer chains and exhibits steeper accuracy decay per token.",
            "uuid": "e6413.1",
            "source_info": {
                "paper_title": "The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "o3-mini (m)",
            "name_full": "o3-mini-31-01-2025 medium (OpenAI reasoning model)",
            "brief_description": "A more proficient OpenAI reasoning-model generation (o3-mini medium) evaluated extensively; achieves higher accuracy without generating longer chains-of-thought than o1-mini, indicating more efficient use of reasoning tokens.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "o3-mini-31-01-2025 (medium)",
            "model_family": "OpenAI o-series (proprietary reasoning models)",
            "model_size": "version tag (31-01-2025) medium (default) as reported",
            "training_data_description": null,
            "benchmark_name": "Omni-MATH",
            "task_type": "Olympiad-level multi-step mathematical problem solving (algebra, calculus, discrete math, geometry, number theory)",
            "problem_format": "Natural-language word problems (Omni-MATH)",
            "difficulty_level": "Olympiad-level, Tier 1–Tier 4; analyses stratified by difficulty tiers and domains",
            "prompting_method": "Vanilla prompt (user message) with chain-of-thought allowed; max_completion_tokens = 25,000 for o3-mini (m); test-time compute scaling used to record/allow long reasoning sequences; reasoning tokens measured and analyzed.",
            "performance_metric": "Accuracy (Omni-Judge); conditional error rate vs reasoning tokens; logistic regression AME (effect of additional tokens on probability correct, controlling for difficulty & domain).",
            "performance_value": "Achieves ≈50% accuracy in all categories (paper: 'o3-mini (m) further enhances performance, achieving 50% in all categories'). Accuracy decay per 1000 reasoning tokens: −1.96% (AME). Conditional error rate reaches 50% at ~12,000 reasoning tokens (empirical observation in paper).",
            "internal_analysis": "Extensive internal (behavioral) analysis: token-distribution histograms show o3-mini(m) and o1-mini have nearly identical reasoning-token distributions for correctly-solved questions, indicating o3-mini(m)'s superior accuracy is not due to longer chains but more effective reasoning; QQ-plots, stratified plots by tier & domain, and regression control for difficulty indicate that increased token usage correlates with lower accuracy even within tiers. The paper interprets this as stronger models 'thinking harder' (more effective steps), not 'longer'.",
            "failure_modes": "Accuracy typically declines as chain-of-thought length grows; longer chains are associated with problems the model cannot solve and with higher probability of final error. However, o3-mini(m) shows smaller error increase per token than o1-mini. No low-level mechanistic interpretability (e.g., activation probing, attention to digit tokens) is reported—analysis is behavioral/statistical.",
            "scaling_trend": "Compared to o1-mini, o3-mini(m) demonstrates better utilization of test-time compute: it attains higher accuracy without increasing average reasoning-token usage; per-token accuracy decay is smaller, indicating more efficient scaling of reasoning capability within model family.",
            "uuid": "e6413.2",
            "source_info": {
                "paper_title": "The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "o3-mini (h)",
            "name_full": "o3-mini-31-01-2025 high (OpenAI reasoning model with reasoning_effort=high)",
            "brief_description": "An OpenAI o3-mini variant configured with high reasoning effort (test-time compute) that attains modest accuracy gains over o3-mini(m) primarily by allocating substantially more reasoning tokens to every problem.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "o3-mini-31-01-2025 (high)",
            "model_family": "OpenAI o-series (proprietary reasoning models)",
            "model_size": "version tag (31-01-2025) high configuration (reasoning_effort=high) as reported",
            "training_data_description": null,
            "benchmark_name": "Omni-MATH",
            "task_type": "Olympiad-level multi-step mathematical problem solving (algebra, calculus, discrete math, geometry, number theory)",
            "problem_format": "Natural-language word problems (Omni-MATH)",
            "difficulty_level": "Olympiad-level, Tier 1–Tier 4",
            "prompting_method": "Vanilla prompt (user message) with chain-of-thought allowed; reasoning_effort parameter set to 'high'; max_completion_tokens = 100,000 for o3-mini (h) to permit very long reasoning chains.",
            "performance_metric": "Accuracy (Omni-Judge); conditional error rate vs reasoning tokens; logistic regression AME.",
            "performance_value": "Approximately +4% average accuracy over o3-mini(m); surpasses 80% accuracy for Algebra and Calculus per-domain cells (paper states 'o3-mini (h) improves with approximately 4% on average compared to o3-mini (m) and surpasses 80% accuracy for Algebra and Calculus'). Accuracy decay per 1000 reasoning tokens: −0.81% (AME). Conditional error rate reaches 50% at ~30,000 reasoning tokens (empirical observation).",
            "internal_analysis": "Behavioral analyses show o3-mini(h) has a significantly stretched reasoning-token distribution (allocating up to &gt;50,000 tokens for some problems); QQ-plots show o3-mini(h) uses more tokens for all correctly answered questions than o3-mini(m). The slower per-token accuracy decay is attributed largely to this stretched token distribution (x-axis) rather than substantially more effective per-token reasoning.",
            "failure_modes": "Marginal accuracy improvements come at large extra computational cost (much longer chains across problems, even for ones already solved by o3-mini(m)); longer chains still correlate with higher error probability, albeit less steeply than weaker models. The paper notes potential issues with judge reliability and prompt refusal for a few questions.",
            "scaling_trend": "Increasing test-time compute (reasoning_effort=high, much larger max_completion_tokens) yields modest accuracy gains but with heavy token cost; per-token decay is smaller, but overall gains are expensive — interpreted as 'thinks harder, not longer' nuance: stronger models use compute more effectively, but raising reasoning_effort can simply make a model allocate more tokens globally.",
            "uuid": "e6413.3",
            "source_info": {
                "paper_title": "The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Omni-MATH",
            "name_full": "Omni-math: A universal olympiad level mathematic benchmark for large language models",
            "brief_description": "A comprehensive Olympiad-level mathematics benchmark used in this study (4428 problems) spanning many sub-domains and stratified difficulty tiers; includes reference answers and written solutions enabling automated evaluation.",
            "citation_title": "Omni-math: A universal olympiad level mathematic benchmark for large language models",
            "mention_or_use": "use",
            "model_name": null,
            "model_family": null,
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "Omni-MATH",
            "task_type": "Olympiad-level multi-step mathematical problem solving across domains (Algebra, Applied Math, Calculus, Discrete Math, Geometry, Number Theory)",
            "problem_format": "Natural-language math problems with reference answers and written solutions (used for automated assessment by Omni-Judge)",
            "difficulty_level": "Stratified into four difficulty tiers (Tier 1–Tier 4) derived from quartiles of difficulty scores; problems reach Olympiad difficulty",
            "prompting_method": "Used with models via a vanilla user prompt; Omni-MATH supplies problems and reference solutions for judge evaluation",
            "performance_metric": "Accuracy (model answers judged by Omni-Judge), domain- & tier-stratified accuracy heatmaps, reasoning-token histograms, conditional-error vs token counts, logistic regression AME",
            "performance_value": "Dataset size: 4428 problems; used to report per-model accuracies (see model entries).",
            "internal_analysis": "Benchmark enables analysis of chain-of-thought length vs accuracy by domain and tier; authors use Omni-Judge to automatically assess equivalence to reference answers and to parse model outputs; allows token-stratified analyses and logistic regression controlling for domain and difficulty.",
            "failure_modes": "Automated judge (Omni-Judge) may diverge from human judgements; Omni-Judge validated primarily on o1-mini, not o3-mini, leaving small possibility of judge-model overlap or mis-evaluation. Some model outputs failed parsing (&lt;1%) and were omitted.",
            "scaling_trend": "Benchmark demonstrates that increased reasoning-token usage (test-time compute) is necessary for harder problems but exhibits diminishing returns; stronger models show smaller negative marginal effects per extra token.",
            "uuid": "e6413.4",
            "source_info": {
                "paper_title": "The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Prompting & evaluation setup",
            "name_full": "Vanilla prompt + chain-of-thought generation, max_completion_tokens limits, and Omni-Judge automated evaluation",
            "brief_description": "The experimental prompting and evaluation protocol used: simple instruction prompt, long max_completion_tokens values to permit extended chain-of-thought, a reasoning_effort flag for o3-mini(h), and Omni-Judge for automated answer equivalence checking.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_family": null,
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "Omni-MATH",
            "task_type": "Multi-step mathematical problem solving with model-generated chain-of-thought",
            "problem_format": "Natural-language prompt per-problem: 'Solve the following problem. Enclose the final answer in a \\boxed{{}} environment. Problem: {problem}'",
            "difficulty_level": "Tier 1–Tier 4 (paper stratification)",
            "prompting_method": "Vanilla single-shot prompt; chain-of-thought is generated by the models at inference (no manual chain-of-thought few-shot template used). Test-time compute scaling settings: max_completion_tokens = 25,000 for o1-mini and o3-mini(m), 100,000 for o3-mini(h); o3-mini(h) achieved by setting reasoning_effort='high'.",
            "performance_metric": "Accuracy (automated equivalence judged by Omni-Judge); conditional probability P(False | T &gt; threshold) calculated from token histograms; logistic regression average marginal effects (AME) for tokens' effect on correctness controlling for domain & difficulty.",
            "performance_value": "Not a model but experimental settings: max_completion_tokens = 25,000 (o1-mini, o3-mini m), 100,000 (o3-mini h); Omni-Judge max_new_tokens = 300 for judging requests. Empirical observations: longer allowed tokens enabled chains up to &gt;50,000 tokens for o3-mini(h).",
            "internal_analysis": "Protocol permitted measurement of 'reasoning tokens' (the number of tokens in the chain-of-thought before the final boxed answer); analyses included stacked histograms of correct vs incorrect responses by token bins, conditional error curves, QQ-plots, and logistic regression controlling for domain and difficulty. Authors caution that prompting strategy may not generalize to other prompt styles and that Omni-Judge is an automated judge with known validation limits.",
            "failure_modes": "Some prompts were refused by models (flagged invalid) and omitted. Judge (Omni-Judge) can disagree with human judgement in some cases; interactions between prompting style and test-time compute may influence token usage and accuracy in ways not fully explored.",
            "scaling_trend": "Test-time compute (per-problem reasoning tokens and reasoning_effort settings) strongly affect token allocation and can increase accuracy but with diminishing returns and potential large cost (o3-mini(h) example); authors report log-linear performance gains in referenced literature but find in-practice that stronger models can get higher accuracy without longer chains.",
            "uuid": "e6413.5",
            "source_info": {
                "paper_title": "The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Omni-math: A universal olympiad level mathematic benchmark for large language models",
            "rating": 2,
            "sanitized_title": "omnimath_a_universal_olympiad_level_mathematic_benchmark_for_large_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "On the overthinking of o1-like LLMs",
            "rating": 2,
            "sanitized_title": "on_the_overthinking_of_o1like_llms"
        },
        {
            "paper_title": "The impact of reasoning step length on large language models",
            "rating": 2,
            "sanitized_title": "the_impact_of_reasoning_step_length_on_large_language_models"
        },
        {
            "paper_title": "Same task, more tokens: the impact of input length on the reasoning performance of large language models",
            "rating": 2,
            "sanitized_title": "same_task_more_tokens_the_impact_of_input_length_on_the_reasoning_performance_of_large_language_models"
        },
        {
            "paper_title": "Simple test-time scaling",
            "rating": 2,
            "sanitized_title": "simple_testtime_scaling"
        }
    ],
    "cost": 0.01494475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>THE RELATIONSHIP BETWEEN REASONING AND PERFORMANCE IN LARGE LANGUAGE MODELS-O3 (MINI) THINKS HARDER, NOT LONGER
February 24, 2025</p>
<p>Marthe Ballon marthe.ballon@vub.be 
Data Analytics Lab
Vrije Universiteit Brussel
1050BrusselBelgium</p>
<p>Andres Algaba 
Data Analytics Lab
Vrije Universiteit Brussel
1050BrusselBelgium</p>
<p>Vincent Ginis 
Data Analytics Lab
Vrije Universiteit Brussel
1050BrusselBelgium</p>
<p>School of Engineering and Applied Sciences
Harvard University
02138CambridgeMassachusettsUSA</p>
<p>THE RELATIONSHIP BETWEEN REASONING AND PERFORMANCE IN LARGE LANGUAGE MODELS-O3 (MINI) THINKS HARDER, NOT LONGER
February 24, 2025341FCA2113940A33F3BC4207DC06C096arXiv:2502.15631v1[cs.LG]
Large language models have demonstrated remarkable progress in mathematical reasoning, leveraging chain-of-thought and test-time compute scaling.However, many open questions remain regarding the interplay between reasoning token usage and accuracy gains.In particular, when comparing models across generations, it is unclear whether improved performance results from longer reasoning chains or more efficient reasoning.We systematically analyze chain-of-thought length across o1-mini and o3-mini variants on the Omni-MATH benchmark, finding that o3-mini (m) achieves superior accuracy without requiring longer reasoning chains than o1-mini.Moreover, we show that accuracy generally declines as reasoning chains grow across all models and compute settings, even when controlling for difficulty of the questions.This accuracy drop is significantly smaller in more proficient models, suggesting that new generations of reasoning models use test-time compute more effectively.Finally, we highlight that while o3-mini (h) achieves a marginal accuracy gain over o3-mini (m), it does so by allocating substantially more reasoning tokens across all problems, even the ones that o3-mini (m) can already solve.These findings provide new insights into the relationship between model capability and reasoning length, with implications for efficiency, scaling, and evaluation methodologies.Keywords chain-of-thought • large language models • reasoning models • test-time compute • mathematical benchmarks</p>
<p>Introduction</p>
<p>Large language models (LLMs) have evolved from handling basic natural language processing tasks to solving complex problems [1,2,3,4].Scaling model size, data, and compute [5] has enabled larger models to develop richer internal representations [6,7] and emergent capabilities [8].Recently, a new class of reasoning models has emerged that couples reinforcement learning with test-time compute scaling [9,10].These models leverage reasoning tokens to guide the chain-of-thought process and maintain coherence throughout complex problem-solving tasks [11,12,13].By explicitly optimizing the chain-of-thought in the reasoning tokens during training [14] and iteratively refining outputs at inference, these models achieve superior performance, including on challenging mathematical benchmarks [15,16].Moreover, new test-time scaling laws demonstrate that longer reasoning-i.e. more reasoning tokens-yields log-linear performance gains [9].</p>
<p>In this paper, we examine whether more capable models within a single family (o-series of OpenAI) require a longer chain-of-thought to achieve higher performance or if they can reason more effectively.By systematically comparing the number of tokens in the chain-of-thought generated by o1-mini, o3-mini (m), and o3-mini (h) on the Omni-MATH dataset [17], we find that more proficient models (o1-mini vs. o3-mini (m)) do not generate longer reasoning chains Fig. 1 | Accuracy comparison of OpenAI models gpt-4o, o1-mini, o3-mini (m) and o3-mini (h) on the Omni-MATH benchmark.This figure displays the accuracy of gpt-4o, o1-mini, o3-mini (m) and o3-mini (h) on the Omni-MATH benchmark across disciplines and difficulty tiers.The gpt-4o model fails to attain 50% in any category and consistently lags behind the reasoning models.o1-mini significantly improves accuracy, reaching accuracies of 40-60% across all domains, while the o3-models surpass 50% accuracy in all categories.In general, accuracy declines as difficulty increases, with the exception of gpt-4o, which shows accuracy vs. difficulty level imbalance for Tiers 2, 3, and 4.</p>
<p>to achieve higher accuracy.For all models and compute settings, we find that accuracy generally decreases as the chain-of-thought grows, even when controlling for question difficulty.This effect is notably smaller for more proficient models, indicating that o3-mini (m) tends to overthink less and uses reasoning tokens more effectively than o1-mini.However, within one model (o3-mini (m) vs. o3-mini (h)), we observe that the slower accuracy decrease per token is partially due to a higher average accuracy, but mainly due to the model allocating (more than) double the reasoning tokens for all questions.Our findings contribute to the ongoing discussion about whether models such as o1 tend to overthink or underthink [12,13], while complementing studies on reasoning step length [18], input length [19], reasoning failure modes [11], and the optimization of mathematical reasoning [20].</p>
<p>We use the Omni-MATH dataset [17] to benchmark the reasoning abilities of o1-mini, o3-mini (m) and o3-mini (h).Existing benchmarks such as GSM8K [21] and MATH [22] have become less effective in differentiating the mathematical abilities of LLMs due to the high accuracy rates they achieve on these tests.To address this, the Omni-MATH benchmark provides a rigorous evaluation framework at the Olympiad level.Unlike other benchmarks such as FrontierMath [23], GSM-symbolic [24], and sections of Humanity's Last Exam [25], the Omni-MATH dataset contains problems categorized into over 33 sub-domains and spanning more than 10 distinct difficulty levels.This detailed organization enables a nuanced assessment of LLMs' mathematical reasoning across various disciplines and complexities.In addition, the availability of Omni-Judge [17]-a math-evaluation model designed to verify and correct model-generated answers against reference answers-ensures a straightforward and automated evaluation method.</p>
<p>More general benchmarks like those in [26] and MMLU [27], along with specialized tests such as AI2 Reasoning [28] and GPQA [29], broaden the evaluation landscape to diverse reasoning domains.Additionally, coding benchmarks [30,31] highlight the importance of clearly defined reward models and verification systems in assessing models' performance.Future evaluations may consider additional benchmarks that incorporate broader language understanding II o3 (mini) thinks harder, not longer February 24, 2025 Fig. 2 | Granular performance and reasoning token usage evaluation across domains and difficulty tiers of gpt-4o, o1-mini, o3-mini (m) and o3-mini (h) on the Omni-MATH benchmark.The heatmaps visualize cross-sectional performance scores on a 0-100% scale, represented by the color of the progress bar.The length of the progress bar in each cell represents relative token usage for the test-time scaled models.The extra column is computed by averaging over the rows.The extra row and "average" cell are computed independently to give equal weight to multi-domain questions (see Methods).This figure shows that models allocate more computational resources to problems that require complex combinatorial reasoning (Geometry, Discrete Mathematics and Number Theory), whereas foundational arithmetic and algebra problems demand relatively fewer resources.On average, token usage scales with difficulty level.</p>
<p>or real-world reasoning challenges, though the current focus remains on math and coding due to the relative ease of implementing objective reward models and automated verification procedures in these domains.</p>
<p>Results</p>
<p>Our data consists of 4428 Olympiad-level math problems, the Omni-MATH benchmark, together with a reference answer and relevant metadata fields Domain and Difficulty (Appendix Figs.A1 and A2).We consider six elementary mathematics domains, Algebra, Applied Mathematics, Calculus, Discrete Mathematics, Geometry and Number Theory (Appendix Fig. A3) and divide the data into four difficulty tiers, Tier 1, Tier 2, Tier 3 and Tier 4 (Appendix Fig. A4).o3 (mini) thinks harder, not longer February 24, 2025 Subsequently, we feed the problems to four OpenAI models, namely gpt-4o, o1-mini, o3-mini (m) and o3-mini (h) and make automated requests to the Omni-Judge model [17] to correct their answers (consult Methods for more details).</p>
<p>Accuracy and relative token usage across models, disciplines, and difficulties</p>
<p>Figs. 1 and 2 show the accuracy of OpenAI models gpt-4o, o1-mini, o3-mini (m) and o3-mini (h) across disciplines and difficulty tiers.The gpt-4o model performs consistently between 20% and 30% for all math disciplines but clearly lags behind the three reasoning models.o1-mini significantly improves accuracy in all categories, reaching 40-60% on all domains.The introduction of o3-mini (m) further enhances performance, achieving 50% in all categories.The o3-mini (h) model improves with approximately 4% on average compared to o3-mini (m) and surpasses 80% accuracy for Algebra and Calculus.A notable outlier is Discrete Mathematics, where performance deviates from the overall trend for all models.In general, accuracy declines as tier level increases.An exception is observed in gpt-4o, which performs better on Tier 4 than on Tiers 2 and 3 (the reasoning models also perform slightly better on Calculus Tier 3 than Tier 4).This anomaly suggests that the model might leverage unexpected heuristics or struggle disproportionately with mid-tier complexity.</p>
<p>Besides indicating accuracy (via the colors of the progress bars), Fig. 2 also shows relative use of reasoning tokens (via the length of the progress bars) across the Omni-MATH dataset for o1-mini, o3-mini (m), and o3-mini (h).The relative use of tokens increases with the level of difficulty for all models, highlighting the need for computational resources for more difficult tasks.Discrete Mathematics stands out as a token-intensive domain, indicating a heavier combinatorial or multi-step reasoning load.Foundational mathematics areas such as Calculus and Algebra tend to consume fewer tokens, possibly because they are more procedurally straightforward.Interestingly, we observe that a relatively longer chain of reasoning does not generally lead to better performance, as many Tier 4 math problems from token-intensive domains remain unsolved.Notable exceptions are the Geometry Tier 3 problems, where all three reasoning models allocate more reasoning compute to Tier 3 than Tier 4 problems, resulting in a higher accuracy for Geometry Tier 3 than Tier 4.</p>
<p>Reasoning token distribution and performance vs. token usage</p>
<p>Figs. 3 and 4 display the relationship between the number of reasoning tokens and the performance of o1-mini, o3-mini (m), and o3-mini (h) on the Omni-MATH dataset (consult Appendix Fig. A5 for gpt-4o analysis with completion tokens, which encompass both the tokens leading up to the answer and the answer itself).Fig. 3 shows the proportion of the correct (green bars) versus incorrect (red bars) model responses across the reasoning token distribution.The red dashed line depicts the conditional error rate, i.e. the probability that the model answers incorrectly given the amount of used reasoning tokens (see Methods).One first thing to note is that higher performing models have a better ratio of correct to incorrect answers, even for high token counts.This pattern is also reflected in the conditional error rate (red dashed line): the conditional error rate is almost instantly at 50% for o1-mini whereas it takes about 12, 000 tokens for o3-mini (m) and 30, 000 for o3-mini (h) to reach a 50% error rate.A second thing to note is that the token distributions of o1-mini and o3-mini (m) are very similar.Fig. 4b together with the left QQ-plot in Fig. A6 further investigate this behavior by comparing the distribution of the reasoning tokens only for the questions that the models answered correctly.Indeed, the almost identical token distributions show that o3-mini (m) does not use more reasoning tokens to achieve its superior performance to o1-mini on Omni-MATH.This suggests that o3-mini (m) reasons more effectively.The token distribution of o3-mini (h) spans a significantly wider range of values, with the model allocating over 50, 000 reasoning tokens for some math problems.In addition, the right QQ-plot in Fig. A6 shows that o3-mini (h) uses more reasoning tokens to solve all (correctly answered) questions, indicating that the small accuracy gain of 4% compared to o3-mini (m) is accompanied by a large extra computational cost.</p>
<p>The panels below the histograms in Fig. 3 display the relative proportion of tier levels in each bin.They reveal a clear transition from a region where the majority of the questions come from the lowest tiers to a region where the majority of the questions come from the highest tiers.Note that this pattern is visualized by the purple filled histograms in Fig. 3 (higher token regions sometimes have insufficient data counts to show this pattern).The gradient confirms that more complex questions systematically demand greater reasoning depth, which is in line with prior observations.Fig. 4a shows that the average accuracy decreases with increasing use of reasoning tokens for all three models, but that this trend is the most pronounced for o1-mini and smaller for o3-mini (m) and o3-mini (h).While this could be attributed to higher-tier questions requiring more tokens, Fig. 4c shows that the trend remains even when stratifying by tier level.In Fig. A7, we show this also holds when stratifying across domains.This suggests that increased token usage, rather than question complexity alone, is related to accuracy.We use a logistic regression to quantify the effect size of using additional reasoning tokens on the probability of answering a question correctly, controlling for different levels of difficulty and domains (see Methods).We report the average marginal effects in Appendix C. The accuracy decrease per 1000 reasoning tokens is 3.16% for o1-mini, 1.96% for o3-mini (m), and 0.81% for o3-mini (h).These results indicate that while deeper reasoning is necessary for solving complex problems, there is a diminishing return, where excessive o3 (mini) thinks harder, not longer February 24, 2025 Fig. 3 | Analysis of the reasoning token distribution, evolution of token region accuracy, and consistency between difficulty tiers and token usage for o1-mini, o3-mini (m) and o3-mini (h).The main panels of the figure display the distribution of the reasoning tokens as a stacked histogram, illustrating the proportion of correctly and incorrectly answered questions in the Omni-MATH dataset by o1-mini, o3-mini (m) and o3-mini (h).The secondary y-axis depicts the probability that the model answers incorrectly given that the token count has surpassed the bin threshold (see Methods).The panels below the histogram contain a filled histogram where the color opacity represents the difficulty level of the math questions (cfr.Fig. A4).The figure shows that o1-mini and o3-mini (m) have a similar reasoning token distribution, with o3-mini (m) giving more correct answers for high-token regions.o3-mini (h) has a good ratio of correct vs. incorrect answers, even for very high token counts.The probability of giving an incorrect answer increases with token count for all models.Finally, the relative proportion of tier levels in each bin reveal a clear transition from a region where the majority of the questions come from the lowest tiers to a region where the majority of the questions come from the highest tiers (for bins with a sufficient amount of data points).</p>
<p>token usage correlates with reduced accuracy.However, this effect is smaller for higher performing models.Together with the results from the previous paragraph, we conclude that o3-mini (m) tends to overthink less and uses reasoning tokens more effectively than o1-mini.Within one model (o3-mini (m) vs. o3-mini (h)), the slower accuracy decrease per token is attributed to a stretched out token distribution along the x-axis and a shrunken distribution with respect to the y-axis.</p>
<p>Discussion</p>
<p>By systematically comparing the number of tokens in the chain-of-thought generated by o1-mini, o3-mini (m), and o3-mini (h) on the Omni-MATH dataset [17], we find two important results.First, more proficient models (o1-mini vs. o3-mini (m)) do not require longer reasoning to achieve higher accuracy.Second, while accuracy generally declines with a longer chain-of-thought, this effect is notably smaller in more proficient models, underscoring that "thinking harder" is not the same as "thinking longer".A possible hypothesis for this accuracy drop is that models tend to reason more on problems they cannot solve.Another possibility is that longer reasoning chains inherently have a higher probability of leading to a wrong final solution, highlighting the need for mathematical benchmarks with reference reasoning templates.A practical takeaway from our study is that constraining the chain-of-thought (by setting max_completion_tokens) o3 (mini) thinks harder, not longer February 24, 2025 Fig. 4 | o3 (mini) thinks harder, not longer.This figure shows that o3-mini (m) does not require longer reasoning chains than o1-mini to achieve better accuracy and that, in general, more proficient models exhibit less accuracy decay as reasoning tokens increase.a, Accuracy per reasoning token, computed by dividing the number of correctly answered questions by the total number of questions in each bin of the histograms in Fig. 3. Accuracy declines as reasoning token usage increases.Furthermore, we observe that the slope of the lines becomes flatter for higher performing models.These effects are further quantified in the regression analysis (see Methods).b, The boxplots show the distribution of the reasoning tokens for correctly answered questions.Further investigation in the left panel of Fig. A6 confirms that o1-mini and o3-mini (m) have a very similar token distribution.The token distribution of o3-mini (h) is stretched linearly with respect to the one of o3-mini (m) (Fig. A6 right).c, Stratifying plot a by difficulty level shows that, within difficulty tiers, accuracy also decreases with higher reasoning token usage.This suggests that the number of reasoning tokens, rather that difficulty level alone, can be used as a signal for the correctness of the model's answer.In Fig. A7, we show this also holds when stratifying across domains.is more useful for weaker reasoning models than for stronger ones, as the latter still give a significant amount of correct answers for high-token regions.</p>
<p>The token count for o3-mini (h) contained the following subtlety: although o3-mini (h) solves additional problems compared to o3-mini (m), the model uses more tokens for all math problems.The slower decrease in accuracy per token is thus due to a stretched token distribution along the x-axis rather than a more effective usage of reasoning tokens.</p>
<p>Our study relies on automated correction by Omni-Judge [17], a model-based evaluator whose judgements could diverge from human corrections [32,33].Omni-Judge has only been validated for data leakage checks on o1-mini [17]; extending these checks to o3-mini remains future work, though we assume minimal overlap.Additionally, our prompting strategy employed here [34,35,36] may not generalize to alternative approaches or more constrained prompt settings [37], and their interaction with test-time compute warrants further investigation.Many studies on prompting and reasoning were conducted on LLMs without test-time compute, so the broader implications for the latest generation of reasoning models [15] remain to be fully understood.</p>
<p>Our findings contribute to the debate on whether models such as o1 overthink or underthink [12,13], and extend ongoing inquiries into reasoning step length [18], input length [19], failure modes [11], and the optimization of mathematical reasoning [20].By revealing how stronger models can achieve higher accuracy without proportionally longer chain-of-thought, we offer new insights into efficiency, scaling, and evaluation strategies.We also highlight the o3 (mini) thinks harder, not longer February 24, 2025 potential for next-generation test-time compute models to refine the balance between reasoning depth and resource usage.o3 (mini) thinks harder, not longer February 24, 2025</p>
<p>A Methods</p>
<p>We describe our experimental setup and provide the data processing details necessary to replicate our analysis.At the end of this section, we elaborate on the regression analysis conducted to analyse the effect size of increased reasoning token usage on accuracy.</p>
<p>The Omni-MATH dataset The Omni-MATH benchmark [17] contains Olympiad-level math problems specifically designed to test the reasoning abilities of Large language models.Each entry in the dataset consists of a problem, an exact answer, and a written out solution together with the following metadata fields: Domain, Difficulty, and Source (see Fig. A1).Each problem has between one and three domains of the form Mathematics → Primary domain → . . ., with a maximum length of five.In this paper, we only take the primary domains into account, as a more granular classification gives rise to very imbalanced or underpopulated classes.Fig. A3 shows the number of math problems per (primary) domain where we follow [17] in double-or triple-counting the multi-domain questions.We made sure to delete the duplicate entries, e.g.some data entries had multiple domain trees but the same primary domain.Every domain-specific analysis in the paper follows this convention.Finally, we joined the Calculus and Pre Calculus class and deleted the Other class to obtain a more balanced domain distribution.Math problems are also classified according to difficulty level as presented in Fig. A4.We divide the data into difficulty tiers based on the quartiles of the difficulty distribution (without separating difficulty levels).</p>
<p>OpenAI models We evaluate the performance of the several OpenAI models that are affordable for most users: gpt-4o-06-08-2024, o1-mini-12-09-2024, o3-mini-31-01-2025 medium (default) and o3-mini-31-01-2025 high.The o3-mini high model, instead of medium, is obtained by setting reasoning_effort to high.We feed each model the math problems using the Batch API with the following vanilla prompt as user message:</p>
<p>Solve the following problem.Enclose the final answer in a \boxed{{}} environment.Problem: {problem} Furthermore, we set max_completion_tokens limits of 25, 000 for o1-mini and o3-mini medium, and a 100, 000 token limit for o3-mini high.Each reasoning model refused to answer a few questions (flagged as invalid prompts), which were subsequently omitted from the analysis.</p>
<p>Omni-Judge To correct the responses of the four OpenAI models on the Omni-MATH dataset, we employ another Large language model called Omni-Judge (KbsdJames/Omni-Judge).Omni-Judge is an efficient and low cost opensource math-evaluation model developed by the authors of [17].The model is trained to assess the correctness of an answer generated by an LLM, given the problem and a reference answer (see Fig. A2).Table 9 in [17] shows that Omni-Judge is 91.78% consistent with gpt-4o as a judge (who is 98% consistent with human evaluators) and has almost a 100% success rate of correctly parsing model generated answers.To judge the models' generated answers, we make requests to the chat completions endpoint of the kbsdjames.omni-judgeAPI by running the model in LM Studio.We use the same few-shot prompt as in [17] and set the max_new_tokens parameter to 300.In the very few cases where Omni-Judge fails to parse the model output (&lt; 1%), we omit that question from the performance evaluation.</p>
<p>Conditional probability</p>
<p>The conditional probability appearing in Fig. 3 and Fig. A5 is computed using a full Bayesian model with uninformative priors (we assume that P(False) = P(True) = 0.5).In particular, we have that
P(False | T &gt; B i ) = P(T &gt; B i | False) P(T &gt; B i | False) + P(T &gt; B i | True) ,(1)
where {T &gt; B i } is the event that the number of tokens exceeds the right bin threshold and "False" indicates that the model answered incorrectly.Because B i can only take a finite number of values, we have that
P(False | T &gt; B i ) = n k=i+1 P(T ∈ B k | False) P(T ∈ B k | False) + P(T ∈ B k | True) = n k=i+1 |False ∈ B k | |False ∈ B k | + |True ∈ B k | ,(2)
which can be easily computed using the stacked histogram data.</p>
<p>o3 (mini) thinks harder, not longer February 24, 2025</p>
<p>Estimating effect sizes We use a logistic regression to estimate the effect of additional reasoning tokens on the probability of an accurate response on a question Y i , while controlling for different levels of difficulty and domains.The regression takes the following form:
log Pr(Y i = 1) Pr(Y i = 0) = β 0 + β 1 tokens i + K−1 k=1 δ k difficulty tier k(i) difficulty fixed effects + M −1 m=1 γ m domain m(i) domain fixed effects ,(3)
where i, k, and m denote the question-response pair, the difficulty tier, and the domain, respectively.Moreover, k(i) and m(i) indicate that the difficulty tier k and domain m depend on the question-response pair i.The difficulty tier and domain fixed effects can be estimated by including dummy variables, which are equal to one if the difficulty tier or domain is equal to the difficulty tier or domain of the current question-response pair and equal to zero otherwise, with the exclusion of a reference category (i.e., K − 1 and M − 1).The reference category for difficulty tiers is the lowest difficulty Tier 1 and for the domains it is Algebra.We obtain similar results when using the more fine-grained difficulty levels (0 − 10) instead of difficulty tiers.</p>
<p>To facilitate interpretation, we compute the Average Marginal Effect (AME) of additional reasoning tokens on the probability of an accurate response.Unlike the raw logistic regression coefficients, which are expressed in log-odds, the AME directly quantifies the effect of an additional token in probability terms.Specifically, it represents the average change in the probability of accuracy for a one-token increase, while holding difficulty tier and domain constant.By computing AMEs, we ensure that our estimates account for the full distribution of difficulty levels and domains, rather than relying on effects evaluated at a single reference point.Problem: A table tennis club hosts a series of doubles matches following several rules:</p>
<ol>
<li>each player belongs to two pairs at most; 2. every two distinct pairs play one game against each other at most; 3. players in the same pair do not play against each other when they pair with others respectively.Every player plays a certain number of games in this series.All these distinct numbers make up a set called the set of games.Consider a set A = {a 1 , a 2 , . . ., a k } of positive integers such that every element in A is divisible by 6. Determine the minimum number of players needed to participate in this series so that a schedule for which the corresponding set of games is equal to set A exists.</li>
</ol>
<p>Answer: 1  2 max A + 3.</p>
<p>Solution:</p>
<p>To determine the minimum number of players needed to participate in the series such that the set of games is equal to the set A, we start by analyzing the problem through graph theory.</p>
<p>Consider a graph G where each vertex represents a player and an edge between two vertices represents a pair of players.According to the problem's conditions: (i) Each player belongs to at most two pairs.(ii) Every two distinct pairs play one game against each other at most.(iii) Players in the same pair do not play against each other when they pair with others respectively.Given these conditions, each vertex in G can have a degree of at most 2.This implies that G can be decomposed into disjoint cycles, paths, and isolated vertices.Let max A = a.We need to ensure that there is a player who plays a games, and each element in A is divisible by 6.To achieve this, we need to construct a graph where the number of games each player plays corresponds to the elements in A. ### Proof of optimality: Let x be a vertex with the maximum number of games a.This vertex x must be connected to at least one other vertex.If x has only one edge, it must have a games on it.If x has two edges, each edge must have at least a 2 games.Let xy be an edge with ≥ a 2 games.The remaining vertices must account for at least a 2 edges, ensuring that there are at least a 2 + 2 vertices.However, due to the constraints, there must be at least a 2 + 3 vertices to satisfy all conditions.### Construction: To construct such a graph, we can use clusters of three players, each forming pairs within the cluster.This ensures that each player in a cluster plays 6 games per edge.By assigning each cluster to a vertex in a graph G with b vertices, where b = 1 6 max A + 1, we can ensure that the set of games corresponds to A. Thus, the minimum number of players needed is: 1  2 max A + 3. The answer is:  Fig. A8 | Logistic regression models to estimate the effect size of the number of tokens on response accuracy.We use a logistic regression to estimate the effect of additional reasoning tokens on the probability of an accurate response on a question, while controlling for different levels of difficulty and domains.Estimates are from a logistic regression (Eq. 3) fit by maximum likelihood, with robust (Huber-White) standard errors to account for potential heteroskedasticity.The significance levels are for a two-sided Wald test with a null hypothesis of the regression coefficient being equal to zero ( *** p &lt; 0.01, ** p &lt; 0.05, * p &lt; 0.1).
1 2 max A + 3</p>
<p>Mathematics → Discrete Mathematics → Graph Theory Difficulty: 9.0 Source: China National Olympiad</p>
<p>Fig. A1 |February
A1
Fig. A1 | Sample problem from the Omni-MATH dataset.The Omni-MATH dataset consists of 4428 Olympiadlevel math problems together with an exact answer, a written out solution and metadata Domain, Difficulty and Source.</p>
<p>AcknowledgementsThis research received funding from the Flemish Government (AI Research Program).Andres Algaba acknowledges a fellowship from the Research Foundation Flanders under Grant No.1286924N.Vincent Ginis acknowledges support from Research Foundation Flanders under Grant No.G032822N and G0K9322N.o3 (mini) thinks harder, not longer February 24, 2025Data and code availabilityData associated with this study are publicly available in a public repository at https://doi.org/10.5281/zenodo.14878936.The original Omni-MATH dataset is available at https://huggingface.co/datasets/KbsdJames/Omni-MATH.The original Omni-Judge model is available at https://huggingface.co/KbsdJames/Omni-Judge.The code for this publication is publicly available at https://github.com/MartheBallon/analysis_o3-mini_thinks_harder_not_longer.It is based on the Omni-MATH benchmark analysis code, publicly available at https://github.com/KbsdJames/Omni-MATH.We used Python 3.12.6 (pandas 2.2.3, numpy 2.1.1,matplotlib 3.9.2,seaborn 0.13.2, statsmodels 0.14.4,and scikit-learn 1.5.2) to analyse and visualize data and to conduct statistical analyses.Author contributionsVincent Ginis and Marthe Ballon were responsible for the main idea of the study and designed the setup of the analysis.Andres Algaba was responsible for conducting the regression analysis.Marthe Ballon executed the analysis and designed the figures.All authors discussed the results, and collaboratively drafted and revised the manuscript.February 24, 2025Sample output Omni-judge Problem: A table tennis club hosts a series of doubles matches following several rules: (i) each player belongs to two pairs at most; (ii) every two distinct pairs play one game against each other at most; (iii) players in the same pair do not play against each other when they pair with others respectively.Every player plays a certain number of games in this series.All these distinct numbers make up a set called the set of games.Consider a set A = {a 1 , a 2 , . . ., a k } of positive integers such that every element in A is divisible by 6. Determine the minimum number of players needed to participate in this series so that a schedule for which the corresponding set of games is equal to set A exists.Answer: 1  2 max A + 3.Model generation:We will show that one may 'hit' a prescribed set A = {a 1 , a 2 , . . ., a k } (where each a i is a positive multiple of 6) by the following construction, and that no schedule may use fewer thanplayers.[...] In any such schedule every pair (team) plays some number of games.(Rule (ii) means that once we have fixed a pair P the 'opponent-teams' of P form a set; we denote by d(P ) the number of games in which P plays.)Then a given player's 'score' (the number of games in which he participates) is determined by whether he belongs to one pair (in which case his score is the degree of that pair) or to two pairs (in which case his score is the sum of the two degrees).(Note that by rule (iii) if a player belongs to two pairs then those two pairs never meet so that the numbers add!) Thus if we 'read off' the numbers of games played by the players (omitting any repetitions) we obtain a set -called the 'set of games'.In our problem we wish this set to be exactly the given A.An important idea is that players who belong to two pairs are 'economical' in that one player now serves to 'cover' two teams.(In a 'non-economical' schedule each pair would contribute two 'single-members'.)One may show that in a schedule in which as many players as possible are 'doubles' (that is, belong to two teams) one may 'control' the scores so that in fact every player's number is of the form d(P ) or d(P ) + d(Q) (with d(P ) and d(Q) coming from a suitable graph on the pairs).One may then prove the following two claims:Claim 1.If the schedule uses only double-players then the total number n of players is exactly the number T of pairs, and a routine "pairing" (that is, a perfect matching) of the teams shows that one must have n ≥ 2k (since one obtains exactly n/2 sums, one for each game-of-a-double, and these must contain the k given numbers.)Claim 2. Since a team's degree is at most (n − 1) the maximum score obtainable by a double is at most 2(n − 1).In order to be able to "reach" a number a i we must have 2(n − 1) ≥ a i for every i, so that in particular n ≥ 1 2 max{a i } + 1.A careful double-counting (namely, counting the total number of games by summing the scores of players in two different ways) shows that one must have n ≥ a1+a2+•••+a k 3 + 2k.Moreover one may show by an inductive construction that equality is attainable.One acceptable answer to the problem is therefore to prove that the minimum number of players needed is a1+a2+•••+a k 3 + 2k.An answer equivalent to (1) is completely acceptable.Thus, our final answer isOmni-Judge: ## Student Final Answer: a1+a2+•••+a k 3 +2k.## Equivalence Judgement: FALSE ## Justification: The student's answer is a1+a2+•••+a k 3 + 2k, which does not match the reference answer of 1  2 max A + 3. The discrepancy lies in the treatment of the maximum value of A and the constant term.In the student's solution, they consider the average of all elements in set A, whereas the reference answer focuses on half of the maximum element in A plus an additional constant term.This difference indicates that the student's answer does not capture the same meaning as the reference answer.=== report over === Fig.A2| Sample of the Omni-Judge output when presented with a problem, a reference answer and an o3-mini (h) generated solution.The Omni-Judge model is prompted as in[17], which is to repeat the 'student's' final answer, asses the equivalence of this answer with the reference answer and provide complementary justification.We omit the part where o3-mini (h) repeats the problem specifications, indicated by [...].
Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of Artificial General Intelligence: Early experiments with GPT-4. 2023arXiv preprint</p>
<p>Mathematical discoveries from program search with large language models. Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, Pawan Kumar, Emilien Dupont, Francisco Jr Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, Nature. 62579952024</p>
<p>Solving olympiad geometry without human demonstrations. Yuhuai Trieu H Trinh, Wu, He Quoc V Le, Thang He, Luong, Nature. 62579952024</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Language models represent space and time. Wes Gurnee, Max Tegmark, arXiv:2310.022072024arXiv preprint</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023arXiv preprint</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022arXiv preprint</p>
<p>Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto, arXiv:2501.19393Simple test-time scaling. 20251arXiv preprint</p>
<p>Scaling LLM test-time compute optimally can be more effective than scaling model parameters. Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar, arXiv:2408.033142024arXiv preprint</p>
<p>PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models. Carolyn Jane Anderson, Joydeep Biswas, Aleksander Boruch-Gruszecki, Federico Cassano, Molly Q Feldman, Arjun Guha, Francesca Lucchetti, Zixuan Wu, arXiv:2502.015842025arXiv preprint</p>
<p>Do not think that much for 2+ 3=?. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, arXiv:2412.21187On the overthinking of o1-like LLMs. 2024arXiv preprint</p>
<p>Thoughts are all over the place: On the underthinking of o1-like LLMs. Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, arXiv:2501.185852025arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.12948Deepseek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. 2025arXiv preprint</p>
<p>Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, Mao Yang, arXiv:2501.04519rStar-Math: Small LLMs can master math reasoning with self-evolved deep thinking. 2025arXiv preprint</p>
<p>Omni-math: A universal olympiad level mathematic benchmark for large language models. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, arXiv:2410.079852024arXiv preprint</p>
<p>Mingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, Mengnan Du, arXiv:2401.04925The impact of reasoning step length on large language models. 2024arXiv preprint</p>
<p>Mosh Levy, Alon Jacoby, Yoav Goldberg, arXiv:2402.14848Same task, more tokens: the impact of input length on the reasoning performance of large language models. 2024. February 242025arXiv preprinto3 (mini) thinks harder</p>
<p>Qihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu, Liang Ding, Bo Du, Dacheng Tao, arXiv:2404.14963Achieving&gt; 97% on GSM8K: Deeply understanding the problems makes LLMs perfect reasoners. 2024arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily De Oliveira, Santos, arXiv:2411.04872A benchmark for evaluating advanced mathematical reasoning in AI. 2024arXiv preprint</p>
<p>Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, arXiv:2410.05229Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. GSM-symbolic: Understanding the limitations of mathematical reasoning in large language models. 2024arXiv preprint</p>
<p>Humanity's last exam. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, arXiv:2501.142492025arXiv preprint</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.046152022arXiv preprint</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, arXiv:1803.05457Think you have solved question answering? Try ARC, the AI2 reasoning challenge. 2018arXiv preprint</p>
<p>GPQA: A graduate-level google-proof Q&amp;A benchmark. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, arXiv:2311.120222023arXiv preprint</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De, Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>SWE-bench: Can language models resolve real-world GitHub issues?. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik R Narasimhan, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Replacing judges with juries: Evaluating LLM generations with a panel of diverse models. Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, Patrick Lewis, arXiv:2404.187962024arXiv preprint</p>
<p>Dawei Li, Renliang Sun, Yue Huang, Ming Zhong, Bohan Jiang, Jiawei Han, Xiangliang Zhang, Wei Wang, Huan Liu, arXiv:2502.01534Preference leakage: A contamination problem in LLM-as-a-judge. 2025arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Let me speak freely? A study on the impact of format restrictions on performance of large language models. Rui Zhi, Cheng-Kuang Tam, Yi-Lin Wu, Chieh-Yen Tsai, Hung-Yi Lin, Yun-Nung Lee, Chen, arXiv:2408.024422024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3087 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3087</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3087</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-270562920</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.12753v2.pdf" target="_blank">OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI</a></p>
                <p><strong>Paper Abstract:</strong> The evolution of Artificial Intelligence (AI) has been significantly accelerated by advancements in Large Language Models (LLMs) and Large Multimodal Models (LMMs), gradually showcasing potential cognitive reasoning abilities in problem-solving and scientific discovery (i.e., AI4Science) once exclusive to human intellect. To comprehensively evaluate current models' performance in cognitive reasoning abilities, we introduce OlympicArena, which includes 11,163 bilingual problems across both text-only and interleaved text-image modalities. These challenges encompass a wide range of disciplines spanning seven fields and 62 international Olympic competitions, rigorously examined for data leakage. We argue that the challenges in Olympic competition problems are ideal for evaluating AI's cognitive reasoning due to their complexity and interdisciplinary nature, which are essential for tackling complex scientific challenges and facilitating discoveries. Beyond evaluating performance across various disciplines using answer-only criteria, we conduct detailed experiments and analyses from multiple perspectives. We delve into the models' cognitive reasoning abilities, their performance across different modalities, and their outcomes in process-level evaluations, which are vital for tasks requiring complex reasoning with lengthy solutions. Our extensive evaluations reveal that even advanced models like GPT-4o only achieve a 39.97% overall accuracy, illustrating current AI limitations in complex reasoning and multimodal integration. Through the OlympicArena, we aim to advance AI towards superintelligence, equipping it to address more complex challenges in science and beyond. We also provide a comprehensive set of resources to support AI research, including a benchmark dataset, an open-source annotation platform, a detailed evaluation tool, and a leaderboard with automatic submission features.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3087.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3087.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI multimodal chat model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary multimodal chat model from OpenAI evaluated in OlympicArena; highest-performing model in this paper's experiments but still limited on Olympic-level cognitive reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary large multimodal chat model from OpenAI (used here as an LMM). Evaluated in multimodal, image-caption and text-only experimental settings with zero-shot step-by-step prompts; used as both solver and (separately) GPT-4V used for annotation/evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot step-by-step prompting (chain-of-thought style)', 'multimodal (interleaved text+image) reasoning', 'image-caption (textualized image description) reasoning', 'process-level (step-by-step) reasoning evaluated by GPT-4V']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Models were prompted to 'solve step-by-step' with explicit output formatting (e.g., boxed final answer). The paper treats model outputs as chains-of-thought by converting responses into structured steps (via GPT-4) and scoring each step with GPT-4V (process-level evaluation). GPT-4o was evaluated in three input modalities: text-only, image-caption (images replaced by generated captions), and multimodal interleaved text+images.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>similar styles — GPT-4o was evaluated using the same step-by-step (chain-of-thought style) prompting across modalities; no fundamentally different internal reasoning algorithms were applied to GPT-4o within the experiments (diversity comes from input modality rather than distinct reasoning policies).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>OlympicArena (multidiscipline Olympic-level cognitive reasoning benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A large bilingual (EN/ZH) benchmark of 11,163 Olympic-level problems across seven disciplines (math, physics, chemistry, biology, geography, astronomy, computer science), with text-only and interleaved text+image problems, and both answer-level and process-level evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Overall accuracy (as reported in the paper) for GPT-4o: 39.97% (aggregate across subjects and modalities). GPT-4o shows the highest absolute performance among evaluated models but still <40% overall. In modality comparisons, GPT-4o is listed among a small set of LMMs that show significant improvement with image inputs relative to their text-only counterparts; process-level scores (per-step correctness averaged) are generally higher than answer-level accuracy, indicating GPT-4o often produces some correct intermediate steps even when final answers are wrong.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>The paper contrasts text-only, image-caption, and multimodal (interleaved) settings using the same prompting style; for GPT-4o, multimodal inputs produced measurable but modest gains in some subjects relative to text-only. No alternative internal reasoning strategies (e.g., different chain-of-thought variants, verifier loops, or retrieval-augmented pipelines) were run on GPT-4o in this study — comparisons focus on modality and process-vs-answer evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4o, despite being the top model in these experiments, reaches only ~40% overall accuracy on OlympicArena. It benefits in some cases from multimodal inputs but still fails many complex decompositional and inductive problems. Process-level scoring reveals GPT-4o frequently gets intermediate steps right but fails in later steps, indicating error accumulation in long reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Although GPT-4o is one of the few LMMs to improve with image inputs, the paper notes many LMMs do not improve or sometimes degrade when given interleaved visual information. Even when models (including GPT-4o) potentially had tiny amounts of benchmark leakage detected by n-gram prediction metrics, they still did not reliably answer those leaked instances correctly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3087.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3087.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4V / GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4V (vision-enabled) and GPT-4 (text LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI vision-capable model (GPT-4V used for evaluation/annotation) and GPT-4 (text LLM) were used as annotators/evaluators and baseline solvers; GPT-4V served for image understanding and as a model-based judge for answers and steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 / GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 is OpenAI's large text LLM; GPT-4V is the vision-enabled variant used for multimodal reasoning, data annotation (difficulty/cognitive labels), and as a model-based evaluator for problem answers and process steps in the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['step-by-step (chain-of-thought) zero-shot prompting', 'model-based evaluation (GPT-4V as grader/verifier)', 'image understanding (in multimodal evaluations)', 'process-level step scoring']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>GPT-4/GPT-4V were prompted to produce step-by-step solutions and final boxed answers. GPT-4V was also used as an annotator to categorize difficulty and reasoning ability and as an automatic evaluator for answer types that are hard to rule-match; process-level evaluation used GPT-4 to reformat solutions and GPT-4V to assign correctness scores to each step.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>similar styles — the study used the same chain-of-thought style prompting and employed GPT-4V both as a solver and as an evaluator; diversity in method arises from using GPT-4V in several roles (solver, annotator, evaluator) but the reasoning prompting style remains the stepwise CoT approach.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>OlympicArena (benchmarks and process-level evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same as above: multidisciplinary Olympic-level problems with multimodal content and process-level annotations enabling per-step correctness scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>GPT-4 (text) and GPT-4V (multimodal) achieve substantially higher scores than most open-source LLMs; GPT-4 family models show higher process-level step correctness than final answer accuracy, indicating partial correctness of chains. Exact aggregate numbers: GPT-4 variants rank near the top in table results (GPT-4o and GPT-4 family among top performers). GPT-4V was also used to evaluate ~5% of answers (model-based grading) with ~80% agreement to human judges on sampled items.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>The authors compared text-only, image-caption, and multimodal settings with the same prompting. GPT-4/GPT-4V were effective both as solvers and as evaluators; using GPT-4V as an automated grader enabled process-level scoring but the paper did not perform ablations that change the internal reasoning algorithm (e.g., verifier loops) for GPT-4/GPT-4V.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4/GPT-4V can be used reliably as annotators/evaluators (high inter-annotator agreement) and they achieve strong but far-from-perfect performance as solvers; process-level evaluation reveals partial correctness in many chains of reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Using GPT-4V as an evaluator produces ≈80% agreement with human grading on sampled problems, so it's not perfect; also, even GPT-4/GPT-4V struggle on decompositional and inductive reasoning problems and on tasks requiring precise spatial/abstract-symbol visual interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3087.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3087.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-VL-Max / Qwen-VL-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-VL-Max and Qwen-VL-Chat (Alibaba multimodal models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source/industry multimodal models from Alibaba evaluated on OlympicArena; included as representative strong LMMs showing some but inconsistent multimodal gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-VL-Max / Qwen-VL-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal vision-language models from Alibaba evaluated in multimodal, image-caption, and text-only settings. Paired text-only chat models (Qwen-7B-Chat, Qwen1.5-32B-Chat) were used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot step-by-step prompting (CoT style)', 'multimodal interleaved processing', 'image-caption substitution evaluation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same prompting scheme as other models: instruct to solve step-by-step with LaTeX for math and boxed final answers. The image-caption strategy replaced images with generated captions (via InternVL-Chat-V1.5) to measure benefits of textualizing visual content.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>similar styles — the experiments used the same stepwise prompting for Qwen models as for other LMMs/LLMs; the diversity evaluated is across input modalities (text-only, image-caption, multimodal) rather than multiple internal reasoning strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>OlympicArena (multimodal subset and full benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Large multidisciplinary benchmark including many multimodal problems (images interleaved).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Qwen-VL-Max/GPT-4-family had competitive multimodal performance among LMMs; Qwen-VL-Max (and related Qwen-VL-Chat) show improvements on some visual reasoning abilities (pattern recognition, comparative visualization). However the paper reports inconsistent gains: many LMMs (including some Qwen variants' pairings) 'do not exhibit enhanced performance' with images and sometimes degrade.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>The paper's modality comparison shows that Qwen-VL models sometimes benefit from interleaved images but gains are not uniform across subjects; the authors attribute variability to (1) LMMs focusing on text and neglecting images, (2) visual-tuning impacting language reasoning, and (3) difficulty processing complex interleaved text-image layouts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Qwen-VL models are among the relatively better-performing LMMs at leveraging images, but the overall finding is that most LMMs (including Qwen-VL variants) fail to reliably leverage interleaved visual information for complex cognitive tasks; improvements are task- and model-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Some Qwen base models show potential data leakage detections but do not reliably answer leaked instances. Many Qwen LMM/text pairings show negligible or negative multimodal gains on OlympicArena problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3087.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3087.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA-NeXT-34B (example open-source LMM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA-NeXT-34B (open-source vision-language model variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source large multimodal model evaluated in the paper; representative of many open-source LMMs that underperform proprietary models on OlympicArena.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-NeXT-34B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source vision-language model (LLaVA-NeXT family) used in experiments on OlympicArena; paired with its corresponding LLM (Nous-Hermes-2-Yi-34B in the paper's table). Evaluated under the same zero-shot, step-by-step prompting and multimodal/text-only settings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>34B (as per model name)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['step-by-step chain-of-thought prompting', 'multimodal interleaved input processing', 'image-caption substitution (where applicable)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The model was given the same formatted prompt encouraging stepwise solutions and LaTeX for math; where needed, images were concatenated or replaced with captions to accommodate model input constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single/similar style — LLaVA-NeXT-34B was not evaluated with multiple distinct internal reasoning strategies; diversity analyzed in the paper arises from modality and process-level scoring rather than different prompting strategies or algorithmic variants for LLaVA.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>OlympicArena</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See above — multimodal Olympic-level problem set across disciplines with process-level annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Open-source LMMs such as LLaVA-NeXT-34B typically have much lower overall accuracy than top proprietary models; the paper reports open-source models struggle to reach 20% overall accuracy in aggregate. LLaVA-NeXT-34B shows limited gains from image inputs and often trails proprietary LMMs in visual/spatial reasoning categories.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared directly to its text-only counterpart (or other LLMs), LLaVA-NeXT-34B did not show consistent multimodal advantages; the paper suggests implementation and alignment differences and difficulties with interleaved text+image layouts as possible reasons.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Open-source LMMs (including LLaVA-NeXT variants) currently underperform top proprietary models on challenging multidisciplinary reasoning; they tend to show similar per-ability performance trends as other models (e.g., relatively stronger abductive and cause-effect, weaker inductive and decompositional reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Several open-source models failed to solve any programming competition problems; some LMMs even show decreased effectiveness on multimodal inputs compared to text-only.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3087.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3087.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (baseline LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (OpenAI text LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used text-only large language model included as a baseline in the experiments; performs noticeably worse than GPT-4-family models on OlympicArena.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-3.5 chat model used as a text-only baseline under the same zero-shot step-by-step prompting. Evaluated in text-only and image-caption (via captions) settings where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot step-by-step prompting', 'image-caption substitution (for problems with images)', 'answer-level rule-based evaluation and process-level scoring in sampled problems']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>GPT-3.5 received the same prompt templates requiring stepwise solutions and specific output formatting; image-caption setting replaced images with captions for text-only LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single/similar style — evaluation used standard step-by-step prompting; no distinct internal multi-method strategies were applied.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>OlympicArena</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same benchmark as above.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>GPT-3.5 attains substantially lower accuracy than GPT-4-family models across subjects (tables show GPT-3.5 among lower-performing proprietary LLMs but better than many small open-source models). The paper shows GPT-3.5's process-level step scores are typically lower than GPT-4 variants and that it struggles on decompositional and inductive reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Comparisons in the paper focus on modality and process-vs-answer performance rather than comparing different prompting-style families; GPT-3.5, when given image captions, sometimes improves but remains below GPT-4-family performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3.5 is a useful baseline demonstrating that step-by-step prompting yields partial reasoning ability but that scale and model improvements (GPT-4-family) materially improve performance on difficult reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>As with other LLMs, GPT-3.5 often gets intermediate steps correct but fails later in chains; image-captioning does not uniformly rescue performance on multimodal problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3087.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3087.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Modality & Process Comparison (study-level entity)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modality (text-only / image-caption / multimodal) and Process-level vs Answer-level reasoning comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A core experimental axis of the paper: same models evaluated in three input modalities and at both answer-level and process-level to study how reasoning style and modality affect performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs and LMMs (GPT-4o, GPT-4V, GPT-4, GPT-3.5, Qwen, LLaVA-NeXT, InternVL, Yi-VL, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The paper runs the same zero-shot stepwise prompts across many models and compares three input settings: (1) text-only, (2) image-caption (images replaced by generated captions), (3) multimodal interleaved text+images; it also performs process-level evaluation by converting model outputs to step lists and scoring each step (0/1) with GPT-4V.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought / step-by-step prompting (consistent across all comparisons)', 'image-caption textualization (to measure effect of textualizing visual content)', 'multimodal interleaved processing (native LMM inputs)', 'process-level evaluation (per-step correctness scoring)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>All models used the same stepwise prompt templates; for image-caption experiments the paper used InternVL-Chat-V1.5 to generate captions. Process-level evaluation used GPT-4 to reformat outputs and GPT-4V to score steps, producing per-step correctness averages.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>limited diversity in reasoning styles — the paper intentionally holds prompting style constant (stepwise/CoT) and varies input modality; thus it measures the effect of modality rather than introducing different reasoning algorithms or prompting strategies. The study therefore finds comparisons of 'similar reasoning styles across modalities' rather than comparisons across diverse inference strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>OlympicArena (multimodal and full benchmarks; process-level sampled subset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>As above: multidisciplinary Olympic-level problems with stepwise gold solutions for a sampled subset used in process evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Key aggregated observations reported: (1) GPT-4o overall accuracy 39.97%; (2) most open-source models struggle to reach 20% overall accuracy; (3) in many cases process-level (per-step) accuracy is higher than answer-level accuracy, meaning models perform correct intermediate steps even when final answer is wrong; (4) LMMs often fail to leverage interleaved visual information — some LMMs show no improvement or even degraded performance in multimodal vs text-only; a few (GPT-4o, Qwen-VL-Chat) show notable multimodal gains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Direct comparisons show (A) text-only vs image-caption vs multimodal: many LMMs do not show improved performance in multimodal setting compared to text-only, and some degrade; (B) answer-level vs process-level: process-level scores are often higher, and error steps concentrate toward later steps in chains (error accumulation). These constitute the main contrasts between similar prompting styles applied across modalities and evaluation levels.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Holding reasoning style (step-by-step CoT prompts) constant, modality matters but most LMMs currently cannot effectively integrate interleaved visual inputs for complex cognitive tasks; process-level evaluation reveals partial reasoning competence (intermediate steps often correct) and highlights error accumulation in long deduction chains; overall, models tend to show similar per-ability performance trends (stronger at abductive and cause-effect reasoning, weaker at inductive and decompositional reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Despite the intuitive expectation that multimodal (image+text) inputs would help, the study finds that many LMMs fail to benefit from images and some even perform worse than their text-only counterparts — a negative result indicating that mere multimodal input capability does not guarantee improved complex reasoning. Additionally, instances of dataset leakage detected by n-gram prediction were rare and did not correspond to reliable model success on those instances.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems <em>(Rating: 2)</em></li>
                <li>Measuring faithfulness in chain-of-thought reasoning <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Are we on the right way for evaluating large vision-language models? <em>(Rating: 2)</em></li>
                <li>Math-shepherd: A label-free step-by-step verifier for llms in mathematical reasoning <em>(Rating: 1)</em></li>
                <li>MMCode: Evaluating multimodal code large language models with visually rich programming problems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3087",
    "paper_id": "paper-270562920",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (OpenAI multimodal chat model)",
            "brief_description": "A proprietary multimodal chat model from OpenAI evaluated in OlympicArena; highest-performing model in this paper's experiments but still limited on Olympic-level cognitive reasoning tasks.",
            "citation_title": "OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Proprietary large multimodal chat model from OpenAI (used here as an LMM). Evaluated in multimodal, image-caption and text-only experimental settings with zero-shot step-by-step prompts; used as both solver and (separately) GPT-4V used for annotation/evaluation.",
            "model_size": null,
            "reasoning_methods": [
                "zero-shot step-by-step prompting (chain-of-thought style)",
                "multimodal (interleaved text+image) reasoning",
                "image-caption (textualized image description) reasoning",
                "process-level (step-by-step) reasoning evaluated by GPT-4V"
            ],
            "reasoning_methods_description": "Models were prompted to 'solve step-by-step' with explicit output formatting (e.g., boxed final answer). The paper treats model outputs as chains-of-thought by converting responses into structured steps (via GPT-4) and scoring each step with GPT-4V (process-level evaluation). GPT-4o was evaluated in three input modalities: text-only, image-caption (images replaced by generated captions), and multimodal interleaved text+images.",
            "diversity_of_methods": "similar styles — GPT-4o was evaluated using the same step-by-step (chain-of-thought style) prompting across modalities; no fundamentally different internal reasoning algorithms were applied to GPT-4o within the experiments (diversity comes from input modality rather than distinct reasoning policies).",
            "reasoning_task_name": "OlympicArena (multidiscipline Olympic-level cognitive reasoning benchmark)",
            "reasoning_task_description": "A large bilingual (EN/ZH) benchmark of 11,163 Olympic-level problems across seven disciplines (math, physics, chemistry, biology, geography, astronomy, computer science), with text-only and interleaved text+image problems, and both answer-level and process-level evaluation.",
            "performance_by_method": "Overall accuracy (as reported in the paper) for GPT-4o: 39.97% (aggregate across subjects and modalities). GPT-4o shows the highest absolute performance among evaluated models but still &lt;40% overall. In modality comparisons, GPT-4o is listed among a small set of LMMs that show significant improvement with image inputs relative to their text-only counterparts; process-level scores (per-step correctness averaged) are generally higher than answer-level accuracy, indicating GPT-4o often produces some correct intermediate steps even when final answers are wrong.",
            "comparison_of_methods": "The paper contrasts text-only, image-caption, and multimodal (interleaved) settings using the same prompting style; for GPT-4o, multimodal inputs produced measurable but modest gains in some subjects relative to text-only. No alternative internal reasoning strategies (e.g., different chain-of-thought variants, verifier loops, or retrieval-augmented pipelines) were run on GPT-4o in this study — comparisons focus on modality and process-vs-answer evaluation.",
            "key_findings": "GPT-4o, despite being the top model in these experiments, reaches only ~40% overall accuracy on OlympicArena. It benefits in some cases from multimodal inputs but still fails many complex decompositional and inductive problems. Process-level scoring reveals GPT-4o frequently gets intermediate steps right but fails in later steps, indicating error accumulation in long reasoning chains.",
            "counter_examples_or_negative_results": "Although GPT-4o is one of the few LMMs to improve with image inputs, the paper notes many LMMs do not improve or sometimes degrade when given interleaved visual information. Even when models (including GPT-4o) potentially had tiny amounts of benchmark leakage detected by n-gram prediction metrics, they still did not reliably answer those leaked instances correctly.",
            "uuid": "e3087.0",
            "source_info": {
                "paper_title": "OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4V / GPT-4",
            "name_full": "GPT-4V (vision-enabled) and GPT-4 (text LLM)",
            "brief_description": "OpenAI vision-capable model (GPT-4V used for evaluation/annotation) and GPT-4 (text LLM) were used as annotators/evaluators and baseline solvers; GPT-4V served for image understanding and as a model-based judge for answers and steps.",
            "citation_title": "OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI",
            "mention_or_use": "use",
            "model_name": "GPT-4 / GPT-4V",
            "model_description": "GPT-4 is OpenAI's large text LLM; GPT-4V is the vision-enabled variant used for multimodal reasoning, data annotation (difficulty/cognitive labels), and as a model-based evaluator for problem answers and process steps in the benchmark.",
            "model_size": null,
            "reasoning_methods": [
                "step-by-step (chain-of-thought) zero-shot prompting",
                "model-based evaluation (GPT-4V as grader/verifier)",
                "image understanding (in multimodal evaluations)",
                "process-level step scoring"
            ],
            "reasoning_methods_description": "GPT-4/GPT-4V were prompted to produce step-by-step solutions and final boxed answers. GPT-4V was also used as an annotator to categorize difficulty and reasoning ability and as an automatic evaluator for answer types that are hard to rule-match; process-level evaluation used GPT-4 to reformat solutions and GPT-4V to assign correctness scores to each step.",
            "diversity_of_methods": "similar styles — the study used the same chain-of-thought style prompting and employed GPT-4V both as a solver and as an evaluator; diversity in method arises from using GPT-4V in several roles (solver, annotator, evaluator) but the reasoning prompting style remains the stepwise CoT approach.",
            "reasoning_task_name": "OlympicArena (benchmarks and process-level evaluation)",
            "reasoning_task_description": "Same as above: multidisciplinary Olympic-level problems with multimodal content and process-level annotations enabling per-step correctness scoring.",
            "performance_by_method": "GPT-4 (text) and GPT-4V (multimodal) achieve substantially higher scores than most open-source LLMs; GPT-4 family models show higher process-level step correctness than final answer accuracy, indicating partial correctness of chains. Exact aggregate numbers: GPT-4 variants rank near the top in table results (GPT-4o and GPT-4 family among top performers). GPT-4V was also used to evaluate ~5% of answers (model-based grading) with ~80% agreement to human judges on sampled items.",
            "comparison_of_methods": "The authors compared text-only, image-caption, and multimodal settings with the same prompting. GPT-4/GPT-4V were effective both as solvers and as evaluators; using GPT-4V as an automated grader enabled process-level scoring but the paper did not perform ablations that change the internal reasoning algorithm (e.g., verifier loops) for GPT-4/GPT-4V.",
            "key_findings": "GPT-4/GPT-4V can be used reliably as annotators/evaluators (high inter-annotator agreement) and they achieve strong but far-from-perfect performance as solvers; process-level evaluation reveals partial correctness in many chains of reasoning.",
            "counter_examples_or_negative_results": "Using GPT-4V as an evaluator produces ≈80% agreement with human grading on sampled problems, so it's not perfect; also, even GPT-4/GPT-4V struggle on decompositional and inductive reasoning problems and on tasks requiring precise spatial/abstract-symbol visual interpretation.",
            "uuid": "e3087.1",
            "source_info": {
                "paper_title": "OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Qwen-VL-Max / Qwen-VL-Chat",
            "name_full": "Qwen-VL-Max and Qwen-VL-Chat (Alibaba multimodal models)",
            "brief_description": "Open-source/industry multimodal models from Alibaba evaluated on OlympicArena; included as representative strong LMMs showing some but inconsistent multimodal gains.",
            "citation_title": "OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI",
            "mention_or_use": "use",
            "model_name": "Qwen-VL-Max / Qwen-VL-Chat",
            "model_description": "Multimodal vision-language models from Alibaba evaluated in multimodal, image-caption, and text-only settings. Paired text-only chat models (Qwen-7B-Chat, Qwen1.5-32B-Chat) were used for comparison.",
            "model_size": null,
            "reasoning_methods": [
                "zero-shot step-by-step prompting (CoT style)",
                "multimodal interleaved processing",
                "image-caption substitution evaluation"
            ],
            "reasoning_methods_description": "Same prompting scheme as other models: instruct to solve step-by-step with LaTeX for math and boxed final answers. The image-caption strategy replaced images with generated captions (via InternVL-Chat-V1.5) to measure benefits of textualizing visual content.",
            "diversity_of_methods": "similar styles — the experiments used the same stepwise prompting for Qwen models as for other LMMs/LLMs; the diversity evaluated is across input modalities (text-only, image-caption, multimodal) rather than multiple internal reasoning strategies.",
            "reasoning_task_name": "OlympicArena (multimodal subset and full benchmark)",
            "reasoning_task_description": "Large multidisciplinary benchmark including many multimodal problems (images interleaved).",
            "performance_by_method": "Qwen-VL-Max/GPT-4-family had competitive multimodal performance among LMMs; Qwen-VL-Max (and related Qwen-VL-Chat) show improvements on some visual reasoning abilities (pattern recognition, comparative visualization). However the paper reports inconsistent gains: many LMMs (including some Qwen variants' pairings) 'do not exhibit enhanced performance' with images and sometimes degrade.",
            "comparison_of_methods": "The paper's modality comparison shows that Qwen-VL models sometimes benefit from interleaved images but gains are not uniform across subjects; the authors attribute variability to (1) LMMs focusing on text and neglecting images, (2) visual-tuning impacting language reasoning, and (3) difficulty processing complex interleaved text-image layouts.",
            "key_findings": "Qwen-VL models are among the relatively better-performing LMMs at leveraging images, but the overall finding is that most LMMs (including Qwen-VL variants) fail to reliably leverage interleaved visual information for complex cognitive tasks; improvements are task- and model-dependent.",
            "counter_examples_or_negative_results": "Some Qwen base models show potential data leakage detections but do not reliably answer leaked instances. Many Qwen LMM/text pairings show negligible or negative multimodal gains on OlympicArena problems.",
            "uuid": "e3087.2",
            "source_info": {
                "paper_title": "OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLaVA-NeXT-34B (example open-source LMM)",
            "name_full": "LLaVA-NeXT-34B (open-source vision-language model variant)",
            "brief_description": "An open-source large multimodal model evaluated in the paper; representative of many open-source LMMs that underperform proprietary models on OlympicArena.",
            "citation_title": "OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI",
            "mention_or_use": "use",
            "model_name": "LLaVA-NeXT-34B",
            "model_description": "Open-source vision-language model (LLaVA-NeXT family) used in experiments on OlympicArena; paired with its corresponding LLM (Nous-Hermes-2-Yi-34B in the paper's table). Evaluated under the same zero-shot, step-by-step prompting and multimodal/text-only settings.",
            "model_size": "34B (as per model name)",
            "reasoning_methods": [
                "step-by-step chain-of-thought prompting",
                "multimodal interleaved input processing",
                "image-caption substitution (where applicable)"
            ],
            "reasoning_methods_description": "The model was given the same formatted prompt encouraging stepwise solutions and LaTeX for math; where needed, images were concatenated or replaced with captions to accommodate model input constraints.",
            "diversity_of_methods": "single/similar style — LLaVA-NeXT-34B was not evaluated with multiple distinct internal reasoning strategies; diversity analyzed in the paper arises from modality and process-level scoring rather than different prompting strategies or algorithmic variants for LLaVA.",
            "reasoning_task_name": "OlympicArena",
            "reasoning_task_description": "See above — multimodal Olympic-level problem set across disciplines with process-level annotations.",
            "performance_by_method": "Open-source LMMs such as LLaVA-NeXT-34B typically have much lower overall accuracy than top proprietary models; the paper reports open-source models struggle to reach 20% overall accuracy in aggregate. LLaVA-NeXT-34B shows limited gains from image inputs and often trails proprietary LMMs in visual/spatial reasoning categories.",
            "comparison_of_methods": "Compared directly to its text-only counterpart (or other LLMs), LLaVA-NeXT-34B did not show consistent multimodal advantages; the paper suggests implementation and alignment differences and difficulties with interleaved text+image layouts as possible reasons.",
            "key_findings": "Open-source LMMs (including LLaVA-NeXT variants) currently underperform top proprietary models on challenging multidisciplinary reasoning; they tend to show similar per-ability performance trends as other models (e.g., relatively stronger abductive and cause-effect, weaker inductive and decompositional reasoning).",
            "counter_examples_or_negative_results": "Several open-source models failed to solve any programming competition problems; some LMMs even show decreased effectiveness on multimodal inputs compared to text-only.",
            "uuid": "e3087.3",
            "source_info": {
                "paper_title": "OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-3.5 (baseline LLM)",
            "name_full": "GPT-3.5 (OpenAI text LLM)",
            "brief_description": "A widely used text-only large language model included as a baseline in the experiments; performs noticeably worse than GPT-4-family models on OlympicArena.",
            "citation_title": "OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "OpenAI's GPT-3.5 chat model used as a text-only baseline under the same zero-shot step-by-step prompting. Evaluated in text-only and image-caption (via captions) settings where applicable.",
            "model_size": null,
            "reasoning_methods": [
                "zero-shot step-by-step prompting",
                "image-caption substitution (for problems with images)",
                "answer-level rule-based evaluation and process-level scoring in sampled problems"
            ],
            "reasoning_methods_description": "GPT-3.5 received the same prompt templates requiring stepwise solutions and specific output formatting; image-caption setting replaced images with captions for text-only LLMs.",
            "diversity_of_methods": "single/similar style — evaluation used standard step-by-step prompting; no distinct internal multi-method strategies were applied.",
            "reasoning_task_name": "OlympicArena",
            "reasoning_task_description": "Same benchmark as above.",
            "performance_by_method": "GPT-3.5 attains substantially lower accuracy than GPT-4-family models across subjects (tables show GPT-3.5 among lower-performing proprietary LLMs but better than many small open-source models). The paper shows GPT-3.5's process-level step scores are typically lower than GPT-4 variants and that it struggles on decompositional and inductive reasoning tasks.",
            "comparison_of_methods": "Comparisons in the paper focus on modality and process-vs-answer performance rather than comparing different prompting-style families; GPT-3.5, when given image captions, sometimes improves but remains below GPT-4-family performance.",
            "key_findings": "GPT-3.5 is a useful baseline demonstrating that step-by-step prompting yields partial reasoning ability but that scale and model improvements (GPT-4-family) materially improve performance on difficult reasoning benchmarks.",
            "counter_examples_or_negative_results": "As with other LLMs, GPT-3.5 often gets intermediate steps correct but fails later in chains; image-captioning does not uniformly rescue performance on multimodal problems.",
            "uuid": "e3087.4",
            "source_info": {
                "paper_title": "OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Modality & Process Comparison (study-level entity)",
            "name_full": "Modality (text-only / image-caption / multimodal) and Process-level vs Answer-level reasoning comparisons",
            "brief_description": "A core experimental axis of the paper: same models evaluated in three input modalities and at both answer-level and process-level to study how reasoning style and modality affect performance.",
            "citation_title": "OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI",
            "mention_or_use": "use",
            "model_name": "Various LLMs and LMMs (GPT-4o, GPT-4V, GPT-4, GPT-3.5, Qwen, LLaVA-NeXT, InternVL, Yi-VL, etc.)",
            "model_description": "The paper runs the same zero-shot stepwise prompts across many models and compares three input settings: (1) text-only, (2) image-caption (images replaced by generated captions), (3) multimodal interleaved text+images; it also performs process-level evaluation by converting model outputs to step lists and scoring each step (0/1) with GPT-4V.",
            "model_size": null,
            "reasoning_methods": [
                "chain-of-thought / step-by-step prompting (consistent across all comparisons)",
                "image-caption textualization (to measure effect of textualizing visual content)",
                "multimodal interleaved processing (native LMM inputs)",
                "process-level evaluation (per-step correctness scoring)"
            ],
            "reasoning_methods_description": "All models used the same stepwise prompt templates; for image-caption experiments the paper used InternVL-Chat-V1.5 to generate captions. Process-level evaluation used GPT-4 to reformat outputs and GPT-4V to score steps, producing per-step correctness averages.",
            "diversity_of_methods": "limited diversity in reasoning styles — the paper intentionally holds prompting style constant (stepwise/CoT) and varies input modality; thus it measures the effect of modality rather than introducing different reasoning algorithms or prompting strategies. The study therefore finds comparisons of 'similar reasoning styles across modalities' rather than comparisons across diverse inference strategies.",
            "reasoning_task_name": "OlympicArena (multimodal and full benchmarks; process-level sampled subset)",
            "reasoning_task_description": "As above: multidisciplinary Olympic-level problems with stepwise gold solutions for a sampled subset used in process evaluation.",
            "performance_by_method": "Key aggregated observations reported: (1) GPT-4o overall accuracy 39.97%; (2) most open-source models struggle to reach 20% overall accuracy; (3) in many cases process-level (per-step) accuracy is higher than answer-level accuracy, meaning models perform correct intermediate steps even when final answer is wrong; (4) LMMs often fail to leverage interleaved visual information — some LMMs show no improvement or even degraded performance in multimodal vs text-only; a few (GPT-4o, Qwen-VL-Chat) show notable multimodal gains.",
            "comparison_of_methods": "Direct comparisons show (A) text-only vs image-caption vs multimodal: many LMMs do not show improved performance in multimodal setting compared to text-only, and some degrade; (B) answer-level vs process-level: process-level scores are often higher, and error steps concentrate toward later steps in chains (error accumulation). These constitute the main contrasts between similar prompting styles applied across modalities and evaluation levels.",
            "key_findings": "Holding reasoning style (step-by-step CoT prompts) constant, modality matters but most LMMs currently cannot effectively integrate interleaved visual inputs for complex cognitive tasks; process-level evaluation reveals partial reasoning competence (intermediate steps often correct) and highlights error accumulation in long deduction chains; overall, models tend to show similar per-ability performance trends (stronger at abductive and cause-effect reasoning, weaker at inductive and decompositional reasoning).",
            "counter_examples_or_negative_results": "Despite the intuitive expectation that multimodal (image+text) inputs would help, the study finds that many LMMs fail to benefit from images and some even perform worse than their text-only counterparts — a negative result indicating that mere multimodal input capability does not guarantee improved complex reasoning. Additionally, instances of dataset leakage detected by n-gram prediction were rare and did not correspond to reliable model success on those instances.",
            "uuid": "e3087.5",
            "source_info": {
                "paper_title": "OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems",
            "rating": 2,
            "sanitized_title": "olympiadbench_a_challenging_benchmark_for_promoting_agi_with_olympiadlevel_bilingual_multimodal_scientific_problems"
        },
        {
            "paper_title": "Measuring faithfulness in chain-of-thought reasoning",
            "rating": 2,
            "sanitized_title": "measuring_faithfulness_in_chainofthought_reasoning"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Are we on the right way for evaluating large vision-language models?",
            "rating": 2,
            "sanitized_title": "are_we_on_the_right_way_for_evaluating_large_visionlanguage_models"
        },
        {
            "paper_title": "Math-shepherd: A label-free step-by-step verifier for llms in mathematical reasoning",
            "rating": 1,
            "sanitized_title": "mathshepherd_a_labelfree_stepbystep_verifier_for_llms_in_mathematical_reasoning"
        },
        {
            "paper_title": "MMCode: Evaluating multimodal code large language models with visually rich programming problems",
            "rating": 1,
            "sanitized_title": "mmcode_evaluating_multimodal_code_large_language_models_with_visually_rich_programming_problems"
        }
    ],
    "cost": 0.02097525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI
6 Mar 2025</p>
<p>Zhen Huang 
Soochow University</p>
<p>Generative AI Research Lab (GAIR)</p>
<p>Zengzhi Wang 
Shanghai Jiao Tong University</p>
<p>Generative AI Research Lab (GAIR)</p>
<p>Shijie Xia 
Shanghai Jiao Tong University</p>
<p>Generative AI Research Lab (GAIR)</p>
<p>Xuefeng Li 
Shanghai Jiao Tong University</p>
<p>Generative AI Research Lab (GAIR)</p>
<p>Haoyang Zou 
Generative AI Research Lab (GAIR)</p>
<p>Ruijie Xu 
Shanghai Jiao Tong University</p>
<p>Run-Ze Fan 
Shanghai Jiao Tong University</p>
<p>Generative AI Research Lab (GAIR)</p>
<p>Lyumanshan Ye 
Shanghai Jiao Tong University</p>
<p>Generative AI Research Lab (GAIR)</p>
<p>Ethan Chern 
Shanghai Jiao Tong University</p>
<p>Generative AI Research Lab (GAIR)</p>
<p>Yixin Ye 
Generative AI Research Lab (GAIR)</p>
<p>Yikai Zhang 
Shanghai Jiao Tong University</p>
<p>Generative AI Research Lab (GAIR)</p>
<p>Yuqing Yang 
Shanghai Jiao Tong University</p>
<p>Ting Wu 
Generative AI Research Lab (GAIR)</p>
<p>Binjie Wang 
Generative AI Research Lab (GAIR)</p>
<p>Shichao Sun 
Generative AI Research Lab (GAIR)</p>
<p>Yang Xiao 
Generative AI Research Lab (GAIR)</p>
<p>Yiyuan Li 
Generative AI Research Lab (GAIR)</p>
<p>Fan Zhou 
Shanghai Jiao Tong University</p>
<p>Generative AI Research Lab (GAIR)</p>
<p>Steffi Chern 
Yiwei Qin 
Generative AI Research Lab (GAIR)</p>
<p>Yan Ma 
Generative AI Research Lab (GAIR)</p>
<p>Jiadi Su 
Generative AI Research Lab (GAIR)</p>
<p>Yixiu Liu 
Shanghai Jiao Tong University</p>
<p>Generative AI Research Lab (GAIR)</p>
<p>Yuxiang Zheng 
Shanghai Jiao Tong University</p>
<p>Generative AI Research Lab (GAIR)</p>
<p>Shaoting Zhang 
Shanghai Artificial Intelligence Laboratory</p>
<p>Generative AI Research Lab (GAIR)</p>
<p>Dahua Lin 
Shanghai Artificial Intelligence Laboratory</p>
<p>Generative AI Research Lab (GAIR)</p>
<p>Yu Qiao 
Shanghai Artificial Intelligence Laboratory</p>
<p>Generative AI Research Lab (GAIR)</p>
<p>Pengfei Liu 
Shanghai Jiao Tong University</p>
<p>Shanghai Artificial Intelligence Laboratory</p>
<p>Generative AI Research Lab (GAIR)</p>
<p>OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI
6 Mar 2025079DE266558E7245E2D83D09C76CB95EarXiv:2406.12753v2[cs.CL]
Figure 1: AI participates in the Olympics from the Gaokao [57] venue.Figure 2: The overview of our OlympicArena benchmark.38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks.</p>
<p>Introduction</p>
<p>The landscape of Artificial Intelligence (AI) has undergone a transformative evolution with advances in technologies like Large Language Models [2,3] and Large Multimodal Models (LMMs) [31].These models represent significant milestones on the path to Artificial General Intelligence (AGI) [47,15], demonstrating remarkable cognitive reasoning abilities, which represent drawing meaningful conclusions from incomplete and inconsistent knowledge to solve problems in complex scenarios [16,34].They adeptly handle tasks ranging from simple grade school math problems [13,56,59,64] to complex challenges like those presented at the International Mathematical Olympiad (IMO) [46,42].Furthermore, they are progressively being applied to intricate real-world scenarios, such as using AI agents for software development [37], collaborating on complex decision-making processes [11] and even boosting the field of scientific research (i.e., AI4Science) [50].</p>
<p>These applications highlight AI's growing proficiency in cognitive reasoning, a crucial element in the pursuit of AGI and, potentially, superintelligence [35].Therefore, how to benchmark these abilities has sparked extensive research.Existing benchmarks [18,22,26,63,44,62] utilize multidisciplinary exam problems to assess the problem-solving skills of LLMs, but these problems are predominantly knowledge-intensive which has become relatively easy for current LLMs.Also, these benchmarks primarily focus on text-only modalities.Although some benchmarks begin to target college-level problems [52,40] and incorporate multimodal assessments [58,60,61], they still predominantly focus on knowledge-intensive tasks or simple concept applications (shown in Table 1).Concurrent to our work, He et al. [17] introduces an Olympic-level benchmark yet it is limited to only mathematics and physics.Furthermore, all the above benchmarks lack a systematic and fine-grained evaluation of various cognitive reasoning abilities.For example, they mostly do the evaluation only based on answers, neglecting potential errors in the reasoning process.This underscores the need for more comprehensive evaluations that not only cover a broader range of disciplines but also focus on higher levels of cognitive reasoning as well as fine-grained evaluation.</p>
<p>In this paper, we introduce OlympicArena, a comprehensive, highly-challenging, and rigorously curated benchmark featuring a detailed, fine-grained evaluation mechanism designed to assess advanced AI capabilities across a broad spectrum of Olympic-level challenges (as illustrated in Figure 2).We extensively select, collect, and process problems from seven disciplines-mathematics, physics, chemistry, biology, geography, astronomy, and computer science-encompassing 62 different Olympic-level competitions.This extensive collection has culminated in a benchmark comprising 11,163 problems, categorized into 13 types of answers (e.g., expression, interval).Importantly, OlympicArena enhances its evaluation framework by incorporating process-level evaluations that scrutinize the step-by-step reasoning processes of AI models.This approach is critical for understanding the depth of cognitive reasoning beyond correct answers [29,53], allowing us to identify and rectify gaps in AI reasoning pathways and ensuring more robust AI capabilities.The benchmark is bilingual, featuring both English and Chinese, to enhance its accessibility and global applicability.Additionally, it supports two modalities: text-only and interleaved text and images, catering to the evolving complexity of tasks that modern AI systems must handle.We also perform data leakage detection experiments [54] on some mainstream models to validate our benchmark's effectiveness.</p>
<p>We conduct a series of experiments across existing top-performing LMMs, encompassing both proprietary models (e.g., GPT-4o [36]) and open-source models (e.g., LLaVa-NeXT [31]).Additionally, we evaluate various types of LLMs (e.g., GPT-3.5) in two settings: text-only and image-caption and conduct comprehensive evaluations from both the answer-level and process-level perspectives.For answer-level evaluations, we combine rule-based and model-based (GPT-4V2 in this paper) methods to cover a more diverse range of answer types.For process-level evaluations, we score each reasoning step of the model output, which we consider quite critical in reasoning scenarios.Additionally, we perform fine-grained evaluations and analyses on different types of cognitive reasoning, from both logical and visual perspectives to better interpret the current capabilities of AI.</p>
<p>Our observations from the OlympicArena benchmark are summarized as follows: (1) Even the most advanced model, GPT-4o, achieves only a 39.97% overall accuracy, while other open-source models struggle to reach a 20% overall accuracy, underscoring current models' limitations in handling complex, multidisciplinary problems that require advanced cognitive reasoning-key aspects of scientific discovery.(2) Through more fine-grained analysis § 4.4, we find that LMMs are particularly weak in handling complex, decompositional reasoning problems and exhibit poor spatial and geometric perception visual abilities, as well as difficulties in understanding abstract symbols.(3) Additionally, we discover that current LMMs seem to struggle significantly in leveraging interleaved visual information for complex cognitive reasoning problems.Various LMMs fail to show notable enhancements compared to their text-only counterparts.(4) The process-level evaluation also indicates that most models can correctly execute some reasoning steps in spite of providing incorrect answers, demonstrating the models' significant potential.(5) Through data leakage detection, we find that instances of data leakage in our benchmark are exceedingly rare.Even on the infrequent occasions when leakage does occur, the corresponding models do not consistently solve these problems correctly.This suggests the need for more advanced training strategies to enhance cognitive reasoning capabilities.These observations highlight the immense value of the OlympicArena benchmark in advancing our understanding of AI's capabilities and limitations.</p>
<p>Related Work</p>
<p>Benchmark AI Intelligence How to benchmark AI intelligence has always been a challenging problem.Initially, the Turing Test [47] provided a conceptual framework for evaluating AI Intelligence.However, limitations in past AI technology lead researchers to focus on specialized domains.In computer vision, benchmarks like MNIST [25] and ImageNet [14] catalyze progress, while in natural language processing, GLUE [49] and XTREME [21] set the standard for evaluating linguistic capabilities across tasks and languages.The success of pretrained language models [38,23] particularly recent LLMs emphasizes the evaluation of foundational knowledge and innate abilities as shown in Figure 2.This leads to the creation of benchmarks such as MMLU [18], AGIEval [63], C-Eval [22], and CMMLU [26], which pushed the limits of language models with multidisciplinary, multilingual, and knowledge-intensive tasks.However, the rapid progress of LLMs has rendered these benchmarks insufficient to fully assess the models' growing capabilities.Cognitive Reasoning is crucial as it allows AI systems to apply prior knowledge and logical principles to complex tasks in a more human-like manner, ensuring better robustness and generalization in real-world applications [43].Thus, more attention is paid to more intricate reasoning tasks, benchmarks like GSM8K [13] focused on grade-school mathematical reasoning problems, while MATH [20] introduced high-school level mathematical competition tasks.Furthermore, benchmarks such as JEEBench [4], SciBench [52], GPQA [40] and MMMU [58] have expanded the scope by incorporating multidisciplinary university-level subjects and even multimodal tasks.To further challenge AI systems, researchers have turned to problems from some of the most difficult competitions, specifically International Olympiads [17,46,30] and algorithmic challenges [28,19,41].Nevertheless, there is currently no Olympic-level, multidisciplinary benchmark that comprehensively evaluates comprehensive problem-solving abilities to fully test all-rounded AI's cognitive ability.Table 1 presents a comparison of several related scientific benchmarks.</p>
<p>Rigorous Evaluation for Reasoning While curating comprehensive and appropriate data is crucial in benchmarks, adopting rigorous evaluation methodologies is equally important.Most existing benchmarks, as mentioned above, primarily focus on answer-level evaluation (i.e., only comparing the model's output with the standard answer).Recently, some works have started to focus on the models' intermediate reasoning steps.Some of them [48,29,51] explore using process supervision to train better reward models.Lanham et al. [24] delves into the faithfulness of the chain-of-thought reasoning process, while Xia et al. [53] trains models specifically designed to evaluate the validity and redundancy of reasoning steps for mathematical problems.However, in the evaluation methodologies of existing benchmarks as listed in Table 1, few of them incorporate process-level evaluation.This insufficient evaluation often neglects the reliability and faithfulness of AI models, especially in complex cognitive reasoning scenarios requiring lengthy solutions.In this work, the introduced OlympicArena is equipped with a more fine-grained evaluation methodology (i.e., process-level evaluation), allowing developers to better understand the true reasoning behaviors of models.</p>
<p>3 The OlympicArena Benchmark from many other benchmarks that primarily focus on objective problems.Detailed statistics of OlympicArena are described in Table 2. Also, to identify potential data leakage, we conduct specialized data leakage detection experiments on several models.</p>
<p>Furthermore, in pursuit of a granular analysis of model performance, we categorize cognitive reasoning into 8 types of logical reasoning abilities and 5 types of visual reasoning abilities.This comprehensive categorization aids in the detailed evaluation of the diverse and complex reasoning skills that both LLMs and LMMs can exhibit.Additionally, we specifically investigate all multimodal problems to compare the performance of LMMs against their text-based counterparts, aiming to better assess LMMs' capabilities in handling visual information.Finally, we evaluate the correctness and efficiency of the reasoning process, not just limited to an answer-based assessment.</p>
<p>Data Collection</p>
<p>To ensure comprehensive coverage of Olympic-level problems across various disciplines, we begin by collecting URLs of various competitions where problems are publicly available for download in PDF format.Then, we utilize the Mathpix3 tool to convert these PDF documents into markdown format, making them compatible with input requirements for models.Specifically, for the programming problems of Computer Science, we additionally collect corresponding test cases.We strictly adhere to copyright and licensing considerations, ensuring compliance with all relevant regulations.</p>
<p>Data Annotation</p>
<p>Problem Extraction and Annotation.To extract individual problems from the markdown format of the test papers, we employ about 30 students with background in science and engineering.We have developed a user interface for annotating multimodality data, which has been released. 4To facilitate further research and the process-level evaluation of models, we annotate meta-information like solutions if provided.To ensure data quality, we implement a multi-step validation process after the initial annotation is completed.More details can be seen in Appendix B.1.After collecting all the problems, we perform deduplication within each competition based on model embeddings to remove repeated problems that may appear in multiple test papers from the same year.To further demonstrate that our benchmark emphasizes cognitive reasoning more than most other benchmarks, we categorize the difficulty of the problems into three levels and make comparison with other related benchmarks.Specifically, we classify all problems into: knowledge recall, concept application and cognitive reasoning.We utilize GPT-4V as the annotator for categorizing different difficulty levels 5(detailed definitions and specific prompts can be found in Appendix B.2). 6Annotation of Cognitive Reasoning Abilities.To facilitate better fine-grained analysis, we categorize cognitive reasoning abilities from both logical and visual perspectives [16,43].The logical reasoning abilities encompass Deductive Reasoning (DED), Inductive Reasoning (IND), Abductive Reasoning (ABD), Analogical Reasoning (ANA), Cause-and-Effect Reasoning (CAE), Critical Thinking (CT), Decompositional Reasoning (DEC), and Quantitative Reasoning (QUA).Meanwhile, the visual reasoning abilities include Pattern Recognition (PR), Spatial Reasoning (SPA), Diagrammatic Reasoning (DIA), Symbol Interpretation (SYB), and Comparative Visualization (COM).We also utilize GPT-4V as the annotator for categorizing different cognitive abilities (detailed definitions and specific prompts can be found in Appendix B.3). 6 With these annotations, we can conduct a more fine-grained analysis of the current cognitive reasoning abilities of AI.</p>
<p>Data Splitting</p>
<p>Our benchmark includes</p>
<p>Evaluation</p>
<p>Answer-level Evaluation We combine rule-based and model-based methods to cover a diverse range of problems.For problems with fixed answers, we extract the final answer and perform rulebased matching according to the answer type.For code generation tasks, we use the unbiased pass@k metric [10] to test all test cases.For problems with answer types categorized as "others" which are difficult to be evaluated using rule-based matching (e.g., chemical equation writing problems), we employ GPT-4V as an evaluator to assess the responses.To ensure the reliability of GPT-4V as an evaluator, we manually sample and check the correctness.See Appendix C.5 for more details.</p>
<p>Process-level Evaluation To further investigate the correctness of the reasoning steps, ensuring a rigorous assessment of the cognitive abilities of models, we conduct the process-level evaluation.We first sample 96 problems with reference solutions from OlympicArena.We employ GPT-4 to convert both the references (i.e., gold solutions) and the model-generated solutions into a structured step-by-step format.We then provide these solutions to GPT-4V and score each step for its correctness on a scale ranging from 0 to 1. 8 The experimental details can be seen in Appendix C.6.To validate the consistency with human judgment, we obtain some samples for human annotations.The results indicate that our model-based evaluation method is highly accurate, with an 83% inter-annotator agreement.</p>
<p>Main Results</p>
<p>Table 3 presents the evaluation results of various LMMs and LLMs on OlympicArena.We obtain the following observations: (1) Even the most advanced large model, GPT-4o, achieves only a 39.97% overall accuracy, while other open-source models struggle to reach a 20% overall accuracy.This stark contrast highlights the significant difficulty and rigor of our benchmark, demonstrating its effectiveness in pushing the boundaries of current AI capabilities.(2) Furthermore, compared to subjects like biology and geography, we observe that mathematics and physics remain the two most challenging disciplines, likely due to their reliance on complex reasoning abilities.(3) Computer programming competitions also prove to be highly difficult, with some open-source models failing to solve any of them, indicating current models' poor abilities to design efficient algorithms to solve complex problems.</p>
<p>Fine-grained Analysis</p>
<p>To achieve a more fine-grained analysis of the experimental results, we conduct further evaluations based on different modalities and reasoning abilities.Additionally, we also conduct an analysis of the process-level evaluation.Key findings are as follows:</p>
<p>Models exhibit varied performance across different logical and visual reasoning abilities.As shown in Figure 3, almost all models demonstrate similar performance trends across different logical reasoning abilities.They tend to excel in Abductive Reasoning and Cause-and-Effect Reasoning, doing well in identifying causal relationships from the provided information.Conversely, models perform poorly in Inductive Reasoning and Decompositional Reasoning.This is due to the diverse and unconventional nature of Olympic-level problems, which require the ability to break down complex problems into smaller sub-problems.In terms of visual reasoning abilities, models tend to be better at Pattern Recognition and Comparative Visualization.However, they struggle with tasks involving spatial and geometric reasoning as well as those need to understand abstract symbols.The completed results are presented in Appendix D.1.with image inputs and some even show decreased effectiveness when handling images.Possible reasons include: (1) When text and images are input together, LMMs may focus more on the text, neglecting the information in the images.This conclusion has also been found in some other works [61,9].(2) Some LMMs, while training their visual capabilities based on their text-based models, may lose some of their inherent language abilities (e.g., reasoning abilities), which is particularly evident in our scenarios.(3) Our problems use a complex interleaved text and image format, which some models do not support well, leading to difficulties in processing and understanding the positional information of images embedded within the text.9The correlation between answer-level and process-level scores of all the models over all the sampled problems.(c) Distribution of the locations of incorrect steps, represented as the proportion of steps from left to right in the entire process, over all the sampled problems.</p>
<p>Most</p>
<p>Analysis of process-level evaluation results</p>
<p>Through process-level evaluation (complete results are in Table 14), we discover following insights: (1) There is generally a high consistency between process-level evaluation and answer-level evaluation.When a model produces a correct answer, the quality of the reasoning process tends to be higher most of the time (see Figure 4b).( 2) The accuracy at the process-level is often higher than at the answer-level.This indicates that even for very complex problems, the model can correctly perform some of the intermediate steps.Therefore, the model likely has significant untapped potential for cognitive reasoning, which opens new avenues for researchers to explore.We also find that in a few disciplines, some models that perform well at the answer level fall behind at the process level.We speculate that this is because models sometimes tend to overlook the reasonableness of intermediate steps when generating answers, even though these steps may not be crucial to the final result.(3) Additionally, we conduct a statistical analysis of the location distribution of error steps (see Figure 4c).We identify that a higher proportion of errors occur in the later stages.This suggests that models are more prone to making mistakes as reasoning accumulates, indicating a need for improvement in handling long chains of logical deductions.Given the increasing scale of pre-training corpora, it is crucial to detect potential benchmark leakage.</p>
<p>The opacity of pre-training often makes this task challenging.To this end, we employ a recently proposed instance-level leakage detection metric, N-gram Prediction Accuracy [54].This metric uniformly samples several starting points for each instance, predicts the next n-gram for each starting point, and checks whether all predicted n-grams are correct, indicating that the model has potentially encountered this instance.We apply this metric to all available base or text-only chat models of the evaluated models.As shown in Figure 6, it is surprising yet reasonable that some base models or text-only chat models behind these evaluated models have potentially encountered a few benchmark instances, although the number is negligible compared to the complete benchmark.For instance, the base model of Qwen1.5-32B-Chat has potentially encountered 43 benchmark instances.Furthermore, this raises a natural question: can the model correctly answer these instances?Interestingly, the corresponding text-only chat models and multimodal chat models can correctly answer even fewer of these instances.These results demonstrate that our benchmark has minimal leakage 10 and is sufficiently challenging, as the models cannot correctly answer most of the leaked instances.See Appendix E for more results and analysis.</p>
<p>Conclusion</p>
<p>In this work, we introduce OlympicArena, a comprehensive benchmark for evaluating the cognitive reasoning abilities of LMMs and LLMs on Olympic-level problems.Through our detailed experiments, we find that even the most powerful model at present, GPT-4o, does not perform well in applying cognitive reasoning abilities to solve complex problems.We hope that our Olympi-cArena benchmark serves as a valuable stepping stone for future advancements in AI for science and engineering.</p>
<p>A Detailed Statistics of the Benchmark A.1 Distribution of Problems</p>
<p>Our benchmark collects data from various competitions.The detailed list can be found in Table 4.Note that a small portion of the problems are sampled from other related benchmarks which are marked in the table.The subfields covered by each competition subject are shown in Table 5.</p>
<p>Additionally, the distribution information of our benchmark across different languages and modalities is presented in Table 6.</p>
<p>A.2 Answer Types</p>
<p>Through extensive observation of a large number of problems and a thorough examination of multiple previous benchmarks, we have finally distilled 13 comprehensive answer types.These types are designed to cover as many problems as possible.The specific definitions for each answer type are provided in Table 7.</p>
<p>A.3 Image Types</p>
<p>We categorize and summarize the five most common types of images in our multimodal scientific problems.The definitions of these types can be found in Table 8, and examples are provided in Figure 7.The distribution of different image types in our benchmark is shown in Figure 8 Geometric and Mathematical Diagrams</p>
<p>Statistical and Data Representation</p>
<p>Natural and Environmental Images</p>
<p>Scientific and Technical Diagrams</p>
<p>Abstract and Conceptual Visuals     Problems where the answer is a set, such as {1, 2, 3}.</p>
<p>Interval (IN)</p>
<p>Problems where the answer is a range of values, represented as an interval in LaTeX.</p>
<p>Expression (EX)</p>
<p>Problems requiring an expression containing variables, represented in LaTeX.</p>
<p>Equation (EQ)</p>
<p>Problems requiring an equation containing variables, represented in LaTeX.</p>
<p>Tuple (TUP)</p>
<p>Problems requiring a tuple, usually representing a pair of numbers, such as (x, y).</p>
<p>Multi-part Value (MPV)</p>
<p>Problems requiring multiple quantities to be determined within a single sub-problem, such as solving both velocity and time in a physics problem.</p>
<p>Multiple Answers (MA) Problems with multiple solutions for a single sub-problem, such as a math fill-in-the-blank problem with answers 1 or -2.</p>
<p>Code Generation (CODE)</p>
<p>Problems where the answer is a piece of code, requiring the generation of functional code snippets or complete programs to solve the given task.</p>
<p>Others (OT)</p>
<p>Problems that do not fit into the above categories, such as writing chemical equations or explaining reasons, which require human expert evaluation.</p>
<p>B Data Annotation B.1 Problem Extraction and Annotation</p>
<p>We develop a simple and practical annotation interface using Streamlit 11 (as shown in Figure 9).Approximately 30 university students are employed to use this interface for annotation.We provide each annotator with a wage higher than the local average hourly rate.The specific fields annotated are shown in Figure 10.We use image URLs to represent pictures, which allows for efficient storage and easy access without embedding large image files directly in the dataset.Each annotated problem is ultimately stored as a JSON file, facilitating subsequent processing.It is worth mentioning that we embed several rule-based checks and filtering mechanisms in the annotation interface to minimize noise from the annotations.When the following situations arise, we promptly identify and correct the annotations:</p>
<p>1) When the answer type is Numerical Value, and the annotated answer contains a variable.</p>
<p>2) When the answer type is not Numerical Value, but the annotated answer can be parsed as a numerical value.</p>
<p>3) When the answer type is Expression, and the annotated answer contains an equals sign.</p>
<p>4</p>
<p>) When the answer type is Equation, and the annotated answer does not contain an equals sign.</p>
<p>5)</p>
<p>When the annotated answer contains images that should not be present.</p>
<p>6)</p>
<p>When the annotated answer contains units (since units are a separate field according to Figure 10, we compile a list of common units and manually check and correct answers when suspected units are detected).</p>
<p>7) When the annotated image links cannot be previewed properly.</p>
<p>Additionally, we implement a multi-step validation process after the initial annotation is completed.First, we conduct a preliminary check using predefined rules to identify any error data, which is then corrected.Following this, a secondary review is performed by different annotators to further check and correct any errors in the annotations.This cross-checking mechanism helps ensure the accuracy and consistency of the annotations.The prompt we use for categorizing each problem is shown in Figure 11 B</p>
<p>.3 Cognitive Reasoning Abilities Annotation</p>
<p>We provide detailed definitions for each of these cognitive reasoning abilities.</p>
<p>The logical reasoning abilities:</p>
<p>1) Deductive Reasoning involves starting with a general principle or hypothesis and logically deriving specific conclusions.This process ensures that the conclusion necessarily follows from the premises.</p>
<p>2) Inductive Reasoning involves making broad generalizations from specific observations.This type of reasoning infers general principles from specific instances, enhancing our confidence in the generality of certain phenomena.</p>
<p>3) Abductive Reasoning starts with incomplete observations and seeks the most likely explanation.</p>
<p>It is used to form hypotheses that best explain the available data.Instructions for Classification: Please classify the above problem by selecting the most appropriate category that best represents the type of thinking and approach required to address the problem.Consider the complexity, the need for creativity, and the depth of knowledge required.You should conclude your response with "So, the problem can be categorized as AN SW ER .",where ANSWER should be one of the indexes in 1, 2, 3.</p>
<p>Figure 11: The prompt template used for annotating the difficulty level of problems.The "solution" part marked with * is optional.</p>
<p>5)</p>
<p>Cause-and-Effect Reasoning identifies the reasons behind occurrences and their consequences.This reasoning establishes causal relationships between events.</p>
<p>6) Critical Thinking involves objectively analyzing and evaluating information to form a reasoned judgment.It encompasses questioning assumptions and considering alternative explanations.</p>
<p>7) Decompositional Reasoning breaks down complex problems or information into smaller, more manageable parts for detailed analysis.</p>
<p>8) Quantitative Reasoning involves using mathematical skills to handle quantities and numerical concepts, essential for interpreting data and performing calculations.</p>
<p>The visual reasoning abilities:</p>
<p>1) Pattern Recognition is the ability to identify and understand repeating forms, structures, or recurring themes, especially when presented visually.This skill is critical in subjects like Chemistry for recognizing molecular structures, Biology for identifying cellular components, and Geography for interpreting topographic maps.</p>
<p>2) Spatial Reasoning is the ability to understand objects in both two and three-dimensional terms and draw conclusions about them with limited information.This skill is often applied in subjects like Math.</p>
<p>Two-Dimensional Examples: Plane geometry, segments, lengths.</p>
<p>Three-Dimensional Examples: Solid geometry, spatial visualization 3) Diagrammatic Reasoning represents the capability to solve problems expressed in diagrammatic form, understanding the logical connections between shapes, symbols, and texts.</p>
<p>Examples: Reading various forms of charts and graphs, obtaining and analyzing statistical information from diagrams.</p>
<p>4) Symbol Interpretation is the ability to decode and understand abstract and symbolic visual information.Examples: Understanding abstract diagrams, interpreting symbols, including representations of data structures such as graphs and linked lists 5) Comparative Visualization represents comparing and contrasting visual elements to discern differences or similarities, often required in problem-solving to determine the relationship between variable components.</p>
<p>The prompt we use for annotating different logical reasoning abilities and visual reasoning abilities are shown separately in Figure 12 and Figure 13.</p>
<p>Problem description: {problem}</p>
<p>Answer: {answer} Solution: {solution}* You need to identify and select the specific types of logical reasoning abilities required to solve the question from the list provided below.</p>
<p>Logical Reasoning Abilities:</p>
<ol>
<li>
<p>Deductive Reasoning: Deductive reasoning involves starting with a general principle or hypothesis and logically deriving specific conclusions.This process ensures that the conclusion necessarily follows from the premises.2. Inductive Reasoning: Inductive reasoning involves making broad generalizations from specific observations.This type of reasoning infers general principles from specific instances, enhancing our confidence in the generality of certain phenomena.</p>
</li>
<li>
<p>Abductive Reasoning: Abductive reasoning starts with incomplete observations and seeks the most likely explanation.It is used to form hypotheses that best explain the available data. 4. Analogical Reasoning: Analogical reasoning involves using knowledge from one situation to solve problems in a similar situation by drawing parallels.5. Cause-and-Effect Reasoning: Cause-and-effect reasoning identifies the reasons behind occurrences and their consequences.This reasoning establishes causal relationships between events.6. Critical Thinking: Critical thinking involves objectively analyzing and evaluating information to form a reasoned judgment.It encompasses questioning assumptions and considering alternative explanations.7. Decompositional Reasoning: Decompositional reasoning breaks down complex problems or information into smaller, more manageable parts for detailed analysis.8. Quantitative Reasoning: Quantitative reasoning involves using mathematical skills to handle quantities and numerical concepts, essential for interpreting data and performing calculations.</p>
</li>
</ol>
<p>Analyze the question, its answer and explanation (if provided) to determine which of the above reasoning abilities are necessary.Conclude by clearly stating which reasoning abilities are involved in solving the question using "So, the involved reasoning abilities are ABILIT IES ", where "ABILITIES" represents the numbers corresponding to the list above, separated by commas if multiple abilities are relevant.</p>
<p>Figure 12: The prompt template used for annotating different logical reasoning abilities of problems.</p>
<p>The "solution" part marked with * is optional.</p>
<p>Problem description: {problem}</p>
<p>Answer: {answer} Solution: {solution}* You need to identify and select the specific types of visual reasoning abilities required to solve the question from the list provided below.</p>
<p>Visual Reasoning Abilities:</p>
<ol>
<li>Pattern Recognition: The ability to identify and understand repeating forms, structures, or recurring themes, especially when presented visually.This skill is critical in subjects like Chemistry for recognizing molecular structures, Biology for identifying cellular components, and Geography for interpreting topographic maps.Analyze the question, its answer, and any explanation provided to determine which of the above reasoning abilities are necessary.Conclude by clearly stating which reasoning abilities are involved in solving the question using "So, the involved reasoning abilities are ABILIT IES ", where "ABILITIES" represents the numbers corresponding to the list above, separated by commas if multiple abilities are relevant.</li>
</ol>
<p>Figure 13: The prompt template used for annotating different visual reasoning abilities of problems which have multi-modal inputs.The "solution" part marked with * is optional.</p>
<p>C Experiment Details C.1 Prompt for Image Caption</p>
<p>The prompt we use for captioning each image in the benchmark for LMMs is shown in Figure 14.</p>
<p>C.2 Models</p>
<p>In our experiments, we evaluate a range of both open-source and proprietary LMMs and LLMs.</p>
<p>For LMMs, we select the newly released GPT-4o [36] and the powerful GPT-4V [1] from OpenAI.Additionally, we include Claude3 Sonnet [3] from Anthropic, and Gemini Pro Vision12 [45] from Google, and Qwen-VL-Max [6] from Alibaba.We also evaluate several open-source models, includ-</p>
<p>[Image]</p>
<p>Describe the fine-grained content of the image or figure, including scenes, objects, relationships, and any text present.</p>
<p>Figure 14: The prompt template used for image caption.</p>
<p>Table 9: LMMs and their corresponding LLMs.</p>
<p>LMM LLM
GPT-4o GPT-4o GPT-4v GPT-4 Claude3 Sonnet Claude3 Sonnet Gemini Pro Vision Gemini Pro LLaVA-NeXT-34B Nous-Hermes-2-Yi-34B InternVL-Chat-V1.5 InternLM2-20B-Chat Yi-VL-34B Yi-34B-Chat Qwen-VL-Chat
Qwen-7B-Chat ing LLaVA-NeXT-34B [31], InternVL-Chat-V1.5 [12], Yi-VL-34B [55], and Qwen-VL-Chat [7].</p>
<p>For LLMs, we primarily select the corresponding text models of the aforementioned LMMs, such as GPT-4 [2].Additionally, we include open-source models like Qwen-7B-Chat, Qwen1.5-32B-Chat[5], Yi-34B-Chat [55], and InternLM2-Chat-20B [8].Table 9 shows the relationship between LMMs and their corresponding LLMs.For the proprietary models, we call the APIs, while for the open-source models, we run them on an 8-card A800 cluster.</p>
<p>C.3 Evaluation Prompts</p>
<p>We meticulously design the prompts used for model input during experiments.These prompts are tailored to different answer types, with specific output formats specified for each type.The detailed prompt templates are shown in Figure 15, and the different instructions for each answer type are provided in Table 10.</p>
<p>You are participating in an international {subject} competition and need to solve the following question.</p>
<p>{answer type description}</p>
<p>Here is some context information for this question, which might assist you in solving it: {context}*</p>
<p>Problem: {problem}</p>
<p>All mathematical formulas and symbols you output should be represented with LaTeX.You can solve it step by step and please end your response with: {answer format instruction}.</p>
<p>Figure 15: The prompt template used for problem input.The "context" part marked with * is optional and refers to supplementary information provided during manual annotation when the problem relies on conclusions from previous questions.The {answer type description} and {answer format instruction} are specified in Table 10.</p>
<p>C.4 Model Hyperparameters</p>
<p>For all models, we set the maximum number of output tokens to 2048 and the temperature to 0.0.When performing code generation (CODE) tasks, the temperature is set to 0.2.</p>
<p>C.5 Answer-level Evaluation Protocols</p>
<p>Rule-based Evaluation For problems with fixed answers, we extract the final answer enclosed in "\boxed{}" (using prompts to instruct models to conclude their final answers with boxes) and perform rule-based matching according to the answer type.</p>
<p>1) For numerical value (NV) answers, we handle units by explicitly stating them in the prompts provided to the model, if applicable.During evaluation, we assess only the numerical value output by the model, disregarding the unit.In cases where numerical answers are subject to estimation, such as in physics or chemistry problems, we convert both the model's output and the correct answer to scientific notation.If the exponent of 10 is the same for both, we allow a deviation of 0.1 in the coefficient before the exponent, accounting for minor estimation errors in the model's calculations.</p>
<p>2) For problems where the answer type is an expression (EX) or an equation (EQ), we use the SymPy13 library for comparison.This allows us to accurately assess the equivalence of algebraic expressions and equations by symbolic computation.</p>
<p>3) For problems requiring the solution of multiple quantities (MPV), our evaluation strictly follows the order of output specified in the prompt, ensuring consistency and correctness in the sequence of results.</p>
<p>4) In the case of problems with multiple answers (MA), we require the model to output all possible answers, adequately considering various scenarios.</p>
<p>5) For problems where the answer type is an interval (IN), we strictly compare the open and closed intervals as well as the boundary values of the endpoints.</p>
<p>6) For problems where the answer type is a set (SET), we compare the set output by the model with the standard answer set to ensure they are completely identical.For problems where the answer type is a tuple (TUP), we compare the tuple output by the model with the standard answer tuple to ensure that each corresponding position is exactly equal.</p>
<p>7) For code generation (CODE) problems, we extract the code output by the model and test it through all provided test cases.Specifically, we use the unbiased pass@k metric,
pass @k := E Problems 1 − n−c k n k(1)
where we set k = 1 and n = 5, and c indicates the number of correct samples that pass all test cases.</p>
<p>Model-based Evaluation</p>
<p>To deal with those problems with answer types that cannot be appropriately evaluated using rule-based matching, we employ model-based evaluation.In this approach, we utilize GPT-4V as the evaluator.We design prompts that include the problem, the correct answer, the solution (if provided), and the response from the model being tested (see Figure 16 for details).The evaluator model then judges the correctness of the tested model's response.</p>
<p>To further ensure the reliability of using a model as an evaluator, we uniformly sampled 100 problems across various subjects that involved model evaluation.We have several students with backgrounds in science and engineering independently conduct manual evaluations.It turns out that out of the 100 sampled problems, there is nearly 80% agreement between the human evaluations and the model evaluations.Considering that problems requiring model-based evaluation account for approximately 5% of the total, the error rate can be controlled at around 20% × 5%, which is approximately 1%.Therefore, we consider this method to be reliable.</p>
<p>You are an experienced teacher tasked with grading an Olympic-level {subject} exam paper.</p>
<p>The problem's context: {context}*</p>
<p>Problem: {problem}</p>
<p>The student's answer: {the tested model's response}</p>
<p>The reference answer: {the reference answer}</p>
<p>The reference solution: {the reference solution}* Note:</p>
<p>(1) You can tolerate some markdown formatting issues.</p>
<p>(2) You need to make judgments based on the provided reference answer and reference solution (if provided).</p>
<p>You can analyze the answer step by step, and then output correct or incorrect at the end to express your final judgment.</p>
<p>Figure 16: The prompt used for model-based evaluation.The "context" and "the reference solution" parts marked with * are optional.</p>
<p>C.6 Process-level Evaluation Protocols</p>
<p>To conduct the process-level evaluation, we utilize a method based on GPT-4V.First, we reformat both the gold solution and the model-generated solution for the sampled problems into a neat step-by-step format using GPT-4.Then, we employ a carefully designed prompt(see Figure 17) to guide GPT-4V using the reformatted gold solution to evaluate the correctness of each step in the model's output, assigning a score of 0 for incorrect and 1 for correct steps.The final process-level score for each problem is determined by averaging the scores of all the steps.</p>
<p>D Fine-grained Results</p>
<p>D.1 Results across Logical and Visual Reasoning Abilities</p>
<p>Table 11 and Table 12 show the performance of different models across various logical and visual reasoning abilities separately.</p>
<p>D.2 Results on Multimodal Problems</p>
<p>Table 13 shows the performance of different models on multimodal problems across different subjects.</p>
<p>D.3 Process-level Evaluation Results</p>
<p>Table 14 shows process-level results of different models across different subjects.</p>
<p>D.4 Results across Different Languages</p>
<p>Table 15 shows results of different models in different languages.</p>
<p>You are a teacher skilled in evaluating the intermediate steps of a student's solution to a given problem.</p>
<p>You are given two types of step-by-step solutions: one from the reference answer and the other from the student.Your task is to evaluate the correctness of each step in the student's solutions using binary scoring: assign a score of 1 for correct steps and 0 for incorrect steps.</p>
<p>Use the reference solutions to guide your evaluation.Follow the format:</p>
<p>Step 1: ...</p>
<p>Step 2: ...</p>
<p>E Data Leakage Detection Details</p>
<p>We combine the questions and detailed solutions (or answers if there are no steps) of the problems, then use the n-gram prediction accuracy metric.Specifically, for each sample, we sample k starting points and predict the next 5-gram each time.To evaluate whether the n-gram prediction is correct, we use exact match and more lenient metrics such as edit distance and ROUGE-L.Here, we consider a prediction correct if either the edit distance or ROUGE-L similarity exceeds 75%, to mitigate some reformatting issues during pre-training.We take the union of instances detected by different metrics to obtain the final set of detected instances.</p>
<p>As shown in Tables 16, 17, and 18, the experimental results reveal that indeed, different models exhibit minor leakage across different subjects.An interesting observation is that some leakages detected by the base model are no longer detectable when using the chat model based on the same base model.We hypothesize that optimization for dialogue capabilities potentially impacts the model's ability and performance on the next token prediction.Another similar observation is that leakages detected by text-only chat models tend to decrease when evaluated on multimodal chat models based on the same chat models.Figure 18 presents a data leakage case from Qwen1.5-32B-Chat.</p>
<p>An exact match case of Qwen1.5-32B-Chat</p>
<p>NV</p>
<p>The answer to this question is a numerical value.{unit instruction} Please end your response with: "The final answer is AN SW ER ", where ANSWER is the numerical value without any units.</p>
<p>SET</p>
<p>The answer to this question is a set.{unit instruction} Please end your response with: "The final answer is AN SW ER ", where ANSWER is the set of all distinct answers, each expressed as a numerical value without any units, e.g.ANSWER = {3, 4, 5}.</p>
<p>IN</p>
<p>The answer to this question is a range interval.{unit instruction} Please end your response with: "The final answer is AN SW ER ", where ANSWER is an interval without any units, e.g.ANSWER = (1, 2]∪ [7, +∞).</p>
<p>EX</p>
<p>The answer to this question is an expression.{unit instruction} Please end your response with: "The final answer is AN SW ER ", where ANSWER is an expression without any units and equals signs, e.g.AN-SWER = 1 2 gt 2 .</p>
<p>EQ</p>
<p>The answer to this question is an equation.{unit instruction} Please end your response with: "The final answer is AN SW ER ", where ANSWER is an equation without any units, e.g.ANSWER = x 2 4 + y 2 2 = 1.</p>
<p>TUP</p>
<p>The answer to this question is a tuple.{unit instruction} Please end your response with: "The final answer is AN SW ER ", where ANSWER is a tuple without any units, e.g.ANSWER= (3,5).</p>
<p>MPV</p>
<p>This question involves multiple quantities to be determined.Your final quantities should be output in the following order: {the ordered sequence of the name of multiple quantities}.Their units are, in order, {the ordered sequence of the units}, but units shouldn't be included in your concluded answer.Their answer types are, in order, {the ordered sequence of answer types}.Please end your response with: "The final answers are AN SW ER ", where ANSWER should be the sequence of your final answers, separated by commas, for example: 5, 7,</p>
<p>MA</p>
<p>This question has more than one correct answer, you need to include them all.Their units are, in order, {the ordered sequence of the units}, but units shouldn't be included in your concluded answer.Their answer types are, in order, {the ordered sequence of answer types}.Please end your response with: "The final answers are AN SW ER ", where ANSWER should be the sequence of your final answers, separated by commas, for example: 5, 7,
InternLM2-20B InternLM2-20B-Chat InternVL-Chat-1.5 14 1 2 3 0 0 0 0 0 internLM2-20B-Chat InternLM2-20B-Chat InternVL-Chat1.5 17 1 0 0 0 0 1 1 1 Yi-34B Yi-34B-Chat Yi-VL-34B 10 2 2 1 0 0 0 0 0 Yi-34B-Chat Yi-34B-Chat Yi-VL-34B 2 0 0 0 0 0 0 0 0 Nous-Hermes-2-Yi-34B -LLaVA-NeXT-34B 0 - 0 0 - 0 0 - 0 Qwen-7B Qwen-7B-Chat Qwen-VL-Chat 8 0 0 1 1 0 0 0 0 Qwen1.5-32B Qwen1.5-32B-Chat - 24 3 - 3 1 - 1 0 - Qwen1.5-32B-Chat Qwen1.5-32B-Chat - 19 2 - 4 2 - 3 1 - GPT-4o GPT-4o GPT-4o 0 0 0 0 0 0 0 0 0InternLM2-20B InternLM2-20B-Chat InternVL-Chat-1.5 0 0 0 0 0 0 1 0 0 InternLM2-20B-Chat InternLM2-20B-Chat InternVL-Chat-1.5 0 0 0 0 0 0 0 0 0 Yi-34B Yi-34B-Chat Yi-VL-34B 1 0 0 0 0 0 0 0 0 Yi-34B-Chat Yi-34B-Chat Yi-VL-34B 0 0 0 0 0 0 0 0 0 Nous-Hermes-2-Yi-34B -LLaVA-NeXT-34B 0 - 0 0 - 0 0 - 0 Qwen-7B Qwen-7B-Chat Qwen-VL-Chat 0 0 0 0 0 0 1 0 0 Qwen1.5-32B Qwen1.5-32B-Chat - 1 0 - 0 0 - 5 1 - Qwen1.5-32B-Chat Qwen1.5-32B-Chat - 1 0 - 0 0 - 0 0 - GPT-4o GPT-4o GPT-4o 0 0 0 0 0 0 0 0 0# Leak. # T # MM # Leak. # T # MM InternLM2-20B InternLM2-20B-Chat InternVL-Chat-1.5 1 1 1 19 2 3 InternLM2-20B-Chat InternLM2-20B-Chat InternVL-Chat-1.5 1 1 1 19 3 2 Yi-34B Yi-34B-Chat Yi-VL-34B 0 0 0 12 2 2 Yi-34B-Chat Yi-34B-Chat Yi-VL-34B 0 0 0 2 0 0 Nous-Hermes-2-Yi-34B -LLaVA-NeXT-34B 0 - 0 0 0 0 Qwen-7B Qwen-7B-Chat Qwen-VL-Chat 1 1 1 11 2 1 Qwen1.5-32B Qwen1.5-32B-Chat - 9 - - 43 14 0 Qwen1.5-32B-Chat Qwen1.5-32B-Chat - 3 3 - 30 8 0 GPT-4o GPT-4o GPT-4o 0 0 0 0 0 0 F Case Study F.</p>
<p>Cases for Error Analysis</p>
<p>From Figure 19 to Figure 25, we showcase examples of various error types across different disciplines.</p>
<p>Physics -Logical Reasoning Error</p>
<p>Problem:</p>
<p>A bead is placed on a horizontal rail, along which it can slide frictionlessly.It is attached to the end of a rigid, massless rod of length R. A ball is attached at the other end.Both the bead and the ball have mass M .The system is initially stationary, with the ball directly above the bead.The ball is then given an infinitesimal push, parallel to the rail.[figure1]Assume that the rod and ball are designed in such a way (not shown explicitly in the diagram) so that they can pass through the rail without hitting it.In other words, the rail only constrains the motion of the bead.Two subsequent states of the system are shown below.</p>
<p>[figure2] Derive an expression for the force in the rod when the ball is directly below the bead, as shown at right above.</p>
<p>[figure1] [figure2]</p>
<p>Solution:</p>
<p>At this point the released gravitational potential energy is 2M gR, and both masses are moving horizontally with speed v, where
2 1 2 M v 2 = 2M gR ⇒ v = 2gR
Work in the frame moving to the right with speed v.In this frame the bead is stationary and the ball has velocity 2v and is instantaneously rotating about the bead, so it must be experiencing a centripetal force
M (2v) 2 R = 8M g
Unlike in part (a), there are no additional contribution from the acceleration of the rotation center, because the bead can only ever accelerate horizontally, and the force in the rod at this moment is vertical.Since the ball also experiences a downward force of M g due to gravity, the force in the rod is a tension
T = 9M g</p>
<p>GPT 4V's output:</p>
<p>To derive an expression for the force in the rod when the ball is directly below the bead, we can employ the conservation of energy and principles from circular motion.</p>
<p>Since the bead and ball are on a frictionless rail and assuming no other non-conservative forces are acting on the system, the total mechanical energy is conserved.</p>
<p>The initial potential energy of the ball when it is given an infinitesimal push will convert into kinetic energy as the ball swings down.At the top position (initial state), the potential energy (PE) is maximum and kinetic energy (KE) is zero.We define the gravitational potential energy to be zero at the lowest point.At the highest point, the potential energy is M gR because the height of the ball above the lowest point is R. Hence, P E initial = M gR.</p>
<p>At the bottom position (final state), when the ball is directly below the bead, all the potential energy will have been converted to kinetic energy (assuming conservation of energy).If v is the velocity of the ball at the bottom, then KE final = 1 2 M v 2 .Since energy is conserved, P E initial = KE final .Thus,
M gR = 1 2 M v 2 2gR = v 2 v = 2gR
Now, let us consider the forces acting on the ball at the bottom position.We have the gravitational force M g acting downwards and the force in the rod F acting upwards.When the ball is at the bottom, it is in circular motion and the centripetal force required to keep it in circular motion is provided entirely by the force in the rod, which is the centripetal force F centripetal , since gravity acts in the same direction as the centripetal acceleration at this point.</p>
<p>The centripetal force is given by
F centripetal = M v 2 R . Substituting v = √2gR
into the above equation, we get
F centripetal = M (2gR) R F centripetal = 2M g
Since the centripetal force is provided by the force in the rod and directed towards the center of the circular motion (upwards towards the ball), at this moment, the rod is under tension, not compression.Thus, the force in the rod when the ball is directly below the bead and in tension is F = 2M g.</p>
<p>The final answer is 2M g .Figure 20: An example of a physics problem with a logical reasoning error.</p>
<p>Astronomy -Incomplete Response</p>
<p>Problem:</p>
<p>In order to move a spacecraft between orbits we must apply a thrust using rockets, which changes the velocity of the spacecraft by ∆v.In this question we will ignore changes in the mass of the spacecraft due to the burning of fuel.For an object of mass m in a circular orbit of radius r around an object with mass M (where m ≪ M ) the orbital velocity, v orb , is given by the formula v orb = GM r .Approximating Mars' orbit as circular with a radius of [The ∆v calculated here would be for the spacecraft to enter a circular orbit around the Sun at the same distance as Mars, but this would not constitute landingthe extra ∆v to get to the Martian surface increases the total for the whole transfer to roughly 8 km s −1 (same as to get into LEO).Other, more complicated routes can be taken, some of which offer substantial efficiencies so the ∆v for the trip can be much lower (although the time taken to complete the manoeuvre will be longer, and the ∆v to reach LEO will still be the biggest single step)]</p>
<p>GPT 4V's output:</p>
<p>To calculate the ∆v to go from Earth Low Earth Orbit (LEO) to Mars, we need to consider two main maneuvers: the transfer from Earth's orbit to the transfer orbit (Hohmann transfer orbit), and then from the transfer orbit to Mars' orbit.</p>
<p>1.The first part of the ∆v, denoted as ∆v A , is the change in velocity needed to move from Earth's orbit to the transfer orbit.This is the velocity at perihelion of the transfer orbit minus the Earth's orbital velocity.</p>
<ol>
<li>The second part, ∆v B , is the change in velocity needed to move from the transfer orbit to Mars' orbit.This is Mars' orbital velocity minus the velocity at aphelion of the transfer orbit.The Hohmann transfer orbit is an elliptical orbit that touches Earth's orbit at its closest point to the Sun (perihelion) and Mars' orbit at its farthest point from the Sun (aphelion).The semi-major axis of the transfer orbit (a t ) is the average of the Earth's orbital radius (r E ) and Mars' orbital radius (r M ).Given that 1 AU (Astronomical Unit) is approximately 1.496 × 10 8 km, we have: The orbital velocity of an object is given by v orb = GM r , where G is the gravitational constant and M is the mass of the central object (in this case, the Sun).The velocity at perihelion of the transfer orbit (v p,t ) is given by:
r E =v p,t = GM r E × 2r M r E +r M
The velocity at aphelion of the transfer orbit (v a,t ) is given by:  # Read i n p u t r , h = map ( i n t , i n p u t ( ) .s p l i t ( ) )</li>
</ol>
<h1>C a l c u l a t e and p r i n t t h e r e s u l t p r i n t ( m a x _ b a l l o o n s ( r , h ) )</h1>
<p>This program defines a function "max balloons" that calculates the maximum number of balloons that can fit in the cupboard based on the given dimensions.It then reads the input values for "r" and "h", calls the function with these values, and prints the result.The program takes into account the balloons that fit in the height of the walls, the semicircle on top, and additional balloons that can fit in the corners of the semicircle.</p>
<p>G Consideration for Social Impact</p>
<p>Certainly, it is essential to point out that as AI performs increasingly well on our benchmark, potentially even surpassing human capabilities, there are some potential ethical and moral risks that require collective oversight.</p>
<p>H Limitations and Future Work</p>
<p>Despite the value of this benchmark, there remains work to be done in the future.Firstly, our benchmark inevitably introduces some noisy problems, we will actively utilize community feedback to continuously refine it.Additionally, we aim to release new versions of the benchmark annually to mitigate issues related to data leakage.Moreover, this benchmark is currently limited to evaluating models' abilities to solve complex problems.In the future, we aspire for AI to assist with complex tasks and demonstrate value in real-world applications such as AI4Science and AI4Engineering rather than just problem-solving.This will be the goal of our future benchmark designs for evaluating AI capabilities.Nonetheless, at present, OlympicArena plays an essential role as a catalyst for further advancements.</p>
<p>LMMs are still not proficient at utilizing visual information.As displayed in Figure4a, only a few LMMs (such as GPT-4o and Qwen-VL-Chat) show significant improvement with image inputs compared to their text-based counterpart.Many LMMs do not exhibit enhanced performance</p>
<p>Figure 3 :
3
Figure 3: Performance of various models on logical and visual reasoning abilities.Logical reasoning abilities: Deductive Reasoning (DED), Inductive Reasoning (IND), Abductive Reasoning (ABD), Analogical Reasoning (ANA), Cause-and-Effect Reasoning (CAE), Critical Thinking (CT), Decompositional Reasoning (DEC), and Quantitative Reasoning (QUA).Visual reasoning abilities: Pattern Recognition (PR), Spatial Reasoning (SPA), Diagrammatic Reasoning (DIA), Symbol Interpretation (SYB), and Comparative Visualization (COM).</p>
<p>Figure 4 :
4
Figure 4: (a) Comparison of different LMMs and their corresponding LLMs across three different experimental settings.For details on the corresponding LLMs for each LMM, refer to the Appendix C.2. (b)The correlation between answer-level and process-level scores of all the models over all the sampled problems.(c) Distribution of the locations of incorrect steps, represented as the proportion of steps from left to right in the entire process, over all the sampled problems.</p>
<p>Figure 5 :
5
Figure 5: Error types distribution for sampled error problems from GPT-4V.Error analysis To further concretize models' performance, we sample incorrect responses from GPT-4V (16 problems per subject, with 8 text-only and 8 multimodal) and have human evaluators analyze and annotate the reasons for these errors.As depicted in Figure5, reasoning errors (both logical and visual) constitute the largest category, indicating that our benchmark effectively highlights the current models' deficiencies in cognitive reasoning abilities.Additionally, a significant portion of errors stem from knowledge deficits, suggesting that current models still lack expert-level domain knowledge and the ability to leverage this knowledge to assist in reasoning.Another category of errors arise from understanding biases, which can be attributed to the models' misinterpretation of context and difficulties in integrating complex language structures and multimodal information.More relevant cases are shown in Appendix F.1.</p>
<p>Figure 5 :Figure 6 :
56
Figure 5: Detected number of leaked samples and the number of correct responses by corresponding text-only and multimodal chat models on these samples.</p>
<p>Figure 7 :
7
Figure 7: Examples of Image Types</p>
<p>Figure 8 :
8
Figure 8: Distribution of Image Types</p>
<p>Figure 9 :Figure 10 : 1 ) 2 ) 3 )
910123
Figure 9: Annotation Page</p>
<p>Step 3 :Figure 17 :
317
Figure 17: The prompt used for process-level evaluation.</p>
<p>Text: 3 Prompt: 1 Prompt:Figure 18 :
3118
Figure 18: A potential data leakage case of Qwen1.5-32B-Chat which is presented with the original problem and solution concatenated, separated by a space.</p>
<p>CODE Write a</p>
<p>Python program to solve the given competitive programming problem using standard input and output methods.Pay attention to time and space complexities to ensure efficiency.Notes: (1) Your solution must handle standard input and output.Use input() for reading input and print() for output.(2) Be mindful of the problem's time and space complexity.The solution should be efficient and designed to handle the upper limits of input sizes within the given constraints.(3) It's encouraged to analyze and reason about the problem before coding.You can think step by step, and finally output your final code in the following format: Your Python code here OT --</p>
<p>8 2
8
MThe Earth's orbital velocity (v E ) is:v E = GM r E Mars' orbital velocity (v M ) is: v M = GM r MNow we can calculate ∆v A and ∆v B :∆v A = v p,t − v E ∆v B = v M − v a,tAnd the total ∆v is:∆v = |∆v A | + |∆v B |Let's calculate these values step by step.We will use the gravitational constant G = 6.674 × 10 −20 km 3 kg −1 s −2 and the mass of the Sun M = 1.989 × 10 30 kg.First, we calculate the semi-major axis of the transfer orbit:a t = 1×1.496×10 8+1.52×1.496×10Now,we calculate the velocities: Now we can calculate ∆v A and ∆v B :∆v A = v p,t − v E ∆v B = v M − v a,t And finally, the total ∆v: ∆v = |∆v A | + |∆v B | Let's calculate these values.</p>
<p>Figure 24 :
24
Figure 24: An example of an astronomy problem with an incomplete response.</p>
<p>Figure 25 :
25
Figure 25: An example of a programming problem with an understanding error.</p>
<p>Table 1 :
1
Comparison of various benchmarks."Subjects": Math, Physics, Chemistry, Biology, Geography, Astronomy, Computer Science."Multimodal" indicates whether the benchmark contains visual information."Language": "EN" for English and "ZH" for Chinese."Size" represents the number of test problems."#Answer" shows the number of answer types (from Appendix A.2). "Eval."details evaluation methods: rule-based, model-based, answer-level, process-level."Leak Det." indicates if data leakage detection is conducted."Difficulty" shows problem proportions at difficulty levels: Knowledge Recall, Concept Application, Cognitive Reasoning."#Logic."indicates the average logical reasoning abilities per question, and "#Visual."indicates the average visual reasoning abilities per multimodal question.Cognitive reasoning abilities are detailed in § 3.3.
BenchmarkSubjectsMultimodal Language Size#AnswerEval.Leak Det.Difficulty#Logic. #Visual.SciBench CMMLU MMLU C-Eval MMMU SciEval AGIEval GPQA JEEBench OlympiadBench✓ × × × ✓ × × × × ✓EN ZH EN ZH EN EN EN &amp; ZH 3300 789 1594 2554 3362 3007 15901 EN 448 EN 515 EN &amp; ZH 89521 1 1 1 2 4 2 1 3 7/ / / / / / / / / /× × × × × × × × × ×0.39 0.36 0.44 0.6 0.25 1.12 1.07 2.24 2.41 2.262.35 ---2.75 ----2.96OlympicArena✓EN &amp; ZH 1116313/✓2.733.15</p>
<p>Table 2 :
2
set of 11,163 problems from 62 distinct Olympic competitions, structured with 13 answer types (shown in Appendix A.2) from objective types (e.g., multiple choice and fill-inthe-blanks) to subjective types (e.g., short answers and programming tasks), which distinguishes it Benchmark Statistics
StatisticNumberTotal Problems Total Competitions Total Subjects/Subfields Total Answer Types Problems with Solutions Language (EN: ZH)11163 62 7/34 13 7904 7054: 4109Total Images Problems with Images Image Types7571 4960 5Cognitive Complexity Levels Logical Reasoning Abilities Visual Reasoning Abilities3 8 5Average Problem Tokens Average Solution Tokens244.8 417.1
3.1 OverviewWe introduce the OlympicArena, an Olympic-level, multidisciplinary benchmark designed to rigorously assess the cognitive reasoning abilities of LLMs and LMMs.Our benchmark features a combination of text-only and interleaved text-image modalities, presented bilingually to promote accessibility and inclusivity.It spans seven core disciplines: mathematics, physics, chemistry, biology, geography, astronomy, and computer science, encompassing a total of 34 specialized branches (details are in Appendix A.1) which represent fundamental scientific fields.The benchmark includes a comprehensive</p>
<p>11,163 problems, with 548 designated for model-based evaluation as OlympicArena-ot.We sample 638 problems across subjects to create OlympicArena-val for hyperparameter tuning or small-scale testing.OlympicArena-val problems have step-by-step solutions, supporting research like process-level evaluation.The remaining problems form OlympicArena-test, the official test set with unreleased answers for formal testing.The results in this paper are based on the entire benchmark dataset, including OlympicArena-ot, OlympicArena-val, and OlympicArena-test.
4 Experiments4.1 Experimental SetupTo comprehensively evaluate the capabilities of LLMs and LMMs (selected models are listed in Appendix C.2) across different modalities, we design our experiments to include three distinct settings: multimodal, image-caption, and text-only. In the multimodal setting, we assess the ability of LMMs to leverage visual information by interleaving text and images, simulating real-world scenarios. For models unable to handle interleaved inputs, we concatenate multiple images into a single input. For LMMs requiring necessary image inputs, their text-based counterparts handle text-only problems. In the image-caption setting, we explore whether textual descriptions of images enhance the problem-solving capabilities of LLMs. Using InternVL-Chat-V1.5 7 [12], we generate captions for all images based on prompts detailed in Appendix C.1. These captions replace the original image inputs. In the text-only setting, we evaluate the performance of LLMs without any visual information, serving as a baseline to compare against the multimodal and image-caption settings. All experiments use zero-shot prompts, tailored to each answer type and specifying output formats to facilitate answer extraction and rule-based matching. It also minimizes biases typically associated with few-shot learning [32, 33]. Detailed prompt designs are provided in Appendix C.3.</p>
<p>Table 3 :
3
Experimental results on OlympicArena, expressed as percentages, with the highest score in each setting underlined and the highest scores across all settings bolded.We use the pass@k metric (Equation1) for CS problems.When calculating the overall accuracy, for code generation problems, if any generated code for a problem passes all test cases, the problem is considered correct.
ModelMath Accuracy Accuracy PhysicsChemistry Biology Geography Astronomy Accuracy Accuracy Accuracy AccuracyCS Pass@1 Accuracy OverallLLMsQwen-7B-Chat Yi-34B-Chat Internlm2-20B-Chat Qwen1.5-32B-Chat1.58 3.06 5.88 9.653.74 9.77 9.48 14.547.01 23.53 18.36 29.847.31 32.67 31.90 38.584.53 35.03 32.14 40.695.48 18.15 16.03 28.050 0.17 0.60 0.514.31 17.31 16.62 23.69GPT-3.5 Claude3 Sonnet GPT-4 GPT-4o7.27 7.76 19.46 28.3310.92 17.24 24.77 29.5423.03 29.46 42.52 46.2431.19 38.25 46.47 49.4231.13 40.94 44.97 48.3616.93 24.04 33.44 43.253.85 1.62 7.78 8.4618.27 23.02 32.37 38.17Image caption + LLMsQwen-7B-Chat Yi-34B-Chat Internlm2-20B-Chat Qwen1.5-32B-Chat1.76 3.01 5.94 9.563.56 9.94 10.40 14.316.75 21.45 20.25 29.847.83 31.26 31.00 38.517.17 34.78 32.52 40.756.87 17.33 16.93 27.20 0.17 0.73 0.604.89 16.72 17.07 23.43GPT-3.5 Claude3 Sonnet GPT-4 GPT-4o7.16 7.52 19.46 28.2714.48 18.10 26.21 29.7123.97 29.84 41.58 45.8730.94 38.77 45.89 51.1633.52 41.14 48.18 49.1218.56 22.65 35 43.174.70 2.39 7.63 9.5718.83 23.10 33.00 38.50LMMsQwen-VL-Chat Yi-VL-34B InternVL-Chat-V1.5 LLaVA-NeXT-34B1.73 2.94 6.03 3.034.25 9.94 9.25 10.068.64 19.81 19.12 21.4512.13 27.73 30.39 33.1813.77 25.16 32.96 36.927.85 16.60 15.94 18.150 0 0.38 0.186.90 14.49 16.63 17.38Qwen-VL-Max Gemini Pro Vision Claude3 Sonnet GPT-4V GPT-4o6.93 6.28 7.52 19.27 28.6712.36 12.47 18.16 24.83 29.7123.79 28.14 29.27 41.45 46.6936 37.48 38.96 46.79 52.1840.19 37.42 40.13 49.62 56.2323.39 20.20 25.02 32.46 43.910.77 1.45 1.45 7.00 9.0020.65 20.97 23.13 32.76 39.97</p>
<p>Table 4 :
4
[27] of competitions included in OlympicArena.Competitions marked with * are partially sourced from OlympiadBench[17], and those marked with † are partially sourced from MMcode[27].
Competition NameAbbreviationSubject# ProblemsUK Senior Kangaroo Math Majors of America Tournament for High Schools Math Kangaroo Euclid Mathematics Contest Canadian Open Mathematics Challenge Johns Hopkins Mathematics Tournament Berkeley Math Tournament Stanford Mathematics Tournament Chinese High School Mathematics League (Pre Round) Chinese High School Mathematics League (1st&amp;2nd Round) ZH_Math_12 UKMT_SK MMATHS MK EMC COMC JHMT BMT SMT ZH_Math_PRE Math Math Math Math Math Math Math Math Math Math Duke University Math Meet DMM Math The Princeton University Mathematics Competition PUMaC Math Harvard-MIT Mathematics Tournament HMMT Math William Lowell Putnam Mathematics Competition Putnam Math International Mathematical Olympiad<em> IMO Math Romanian Master of Mathematics</em> RMM Math American Regions Mathematics League<em> ARML Math Euclid Mathematics Competition</em> EMC Math European Girls' Mathematical Olympiad* EGMO MathF=MA Intermediate Physics Challenge (Y11) Senior Physics Challenge Australian Science Olympaids Physics European Physics Olympiad Nordic-Baltic Physics Olympiad World Physics Olympics Asian Physics Olympiad International Physics Olympiad Canadian Association of Physicists Physics Bowl USA Physics Olympiad Chinese Physics Olympiad Physics Challenge (Y13)FMA BPhO_IPC BPhO_SPC ASOP EPhO NBPhO WoPhO APhO IPhO CAP PB USAPhO CPhO PCY13Physics Physics Physics Physics Physics Physics Physics Physics Physics Physics Physics Physics Physics PhysicsChinese High School Biology Challenge International Biology Olympiad The USA Biology Olympiad Indian Biology Olympiad Australian Science Olympiad Biology British Biology Olympiad New Zealand Biology OlympiadGAOKAO_Bio Biology IBO Biology USABO Biology INBO Biology ASOB Biology BBO Biology NZIBO BiologyChem 13 News Chemistry Chinese High School Geography Challenge Chem13News GAOKAO_Geo Geography US Earth Science Organization USESO Geography Australian Science Olympiad Earth Science ASOE Geography The International Geography Olympaid IGeO GeographyChinese High School Astronomy Challenge The International Astronomy and Astrophysics Competition IAAC GAOKAO_Astro Astronomy Astronomy USA Astronomy and Astrophysics Organization USAAAO Astronomy British Astronomy and Astrophysics Olympaid Challenge BAAO_challenge Astronomy British Astronomy and Astrophysics Olympaid-round2 BAAO AstronomyUSA Computing Olympiad Atcoder Codeforces †USACO Atcoder CFCS CS CS</p>
<p>Table 5 :
5
Subfields of each subject included in OlympicArena.
SubjectSubfieldsMath Physics Chemistry Biology Geography Physical Geography, Human Geography, Regional Geography, Environmental Algebra, Geometry, Number Theory, Combinatorics Mechanics, Electricity and Magnetism, Waves and Optics, Thermodynamics, Modern Physics, Fluid Mechanics General Chemistry, Organic Chemistry, Inorganic Chemistry, Analytical Chem-istry, Physical Chemistry, Environmental Chemistry Cell biology, Plant Anatomy and Physiology, Animal Anatomy and Physiology, Ethology, Genetics and Evolution, Ecology , Biosystematics Geography, Geospatial Techniques Astronomy Fundamentals of Astronomy, Stellar Astronomy, Galactic and Extragalactic Astronomy, Astrophysics CS Data Structures, Algorithm</p>
<p>Table 6 :
6
Statistics of OlympicArena benchmark across different disciplines and modalities.
Mathematics Physics Chemistry Biology Geography Astronomy CSEN &amp; text EN &amp; multi-modal2215 193632 646782 235352 554211 517219 26490ZH &amp; text ZH &amp; multi-modal780 45164 298124 444312 34058 804264 4760 0Total EN Total ZH2408 8251278 4621017 568906 652728 862483 7400Total text Total multi-modal2995 238796 944906 679664 894269 1321483 74090Grand Total3233174015851558159012239.19% (696)39.16% (2965)Image Types Geometric and Mathematical Diagrams Statistical and Data Representation Natural and Environmental Images Scientific and Technical Diagrams Abstract and Conceptual Visuals3.17% (240)17.12% (1296)31.36% (2374)</p>
<p>Table 7 :
7
Answer Types and Definitions
Answer TypeDefinitionSingle Choice (SC)Problems with only one correct option (e.g., one out of four, one out of five, etc.).Multiple-choice (MC)Problems with multiple correct options (e.g., two out of four, two out of five, two out of six, etc.).True/False (TF)Problems where the answer is either True or False.Numerical Value (NV)Problems where the answer is a numerical value, including special values like π, e, √ 7, log 2 9, etc., represented in LaTeX.Set (SET)</p>
<p>Table 8 :
8
Definitions and examples of five image types in our multi-modal scientific problems.
Image TypeDefinitionGeometric and Mathematical DiagramsIncludes diagrams representing mathematical concepts, such as 2D and 3D shapes, mathematical notations, function plots.Statistical and Data RepresentationVisualizations for statistical or data information, including multivariate plots, tables, charts (histograms, bar charts, line plots), and infographics.Natural and Environmental ImagesImages of natural scenes or phenomena, including environmental studies visualizations, geological and geographical maps, and satellite images.Scientific and Technical DiagramsDiagrams used in science, such as cell structures and genetic diagrams in Biology, molecular structures and reaction pathways in Chemistry, force diagrams, circuit diagrams, and astrophysical maps in Physics and Astronomy.
Abstract and Conceptual VisualsVisuals explaining theories and concepts, including flowcharts, algorithms, logic models, and symbolic diagrams.</p>
<p>Direct recall of factual information and well-defined procedures.This category examines the memory of simple knowledge points, i.e., whether certain information is known.2. Concept Application: Very basic use of simple concepts to solve easy problems or do straightforward calculations.This involves applying known information to situations without any complex or multi-step reasoning.The focus is on straightforward application rather than reasoning.3. Cognitive Reasoning: Use of logical reasoning or visual reasoning to solve problems.This category includes problems that require clear thinking and problem-solving techniques.It focuses on the ability to reason and analyze to understand and address the issues.
Problem description: {problem}Answer: {answer}Solution: {solution}*Classification Categories: 1. Knowledge Recall:
4) Analogical Reasoning involves using knowledge from one situation to solve problems in a similar situation by drawing parallels.</p>
<p>Table 10 :
10
Descriptions of answer types and corresponding format instructions included in the problem input prompts.Specifically, {unit description} indicates: "Remember, your answer should be calculated in the unit of {unit}, but do not include the unit in your final answer."
Answer Type Answer Type DescriptionAnswer Format InstructionSCThis is a multiple choice question (only one correct answer).Please end your response with: "The final answer is AN SW ER ", where ANSWER should be one of the options: {the options of the problem}.MCThis is a multiple choice question (more than one correct answer).Please end your response with: "The final answer is AN SW ER ", where ANSWER should be two or more of the options: {the options of the problem}.TFThis is a True or False question.Please end your response with: "The final answer isAN SW ER ", where ANSWER should be either "True" or "False".</p>
<p>Table 11 :
11
Experimental results across different logical reasoning abilities on OlympicArena benchmark, expressed as percentages, with the highest score in each setting underlined and the highest scores across all settings bolded.DED: Deductive Reasoning, IND: Inductive Reasoning, ABD: Abductive Reasoning, ANA: Analogical Reasoning, CAE: Cause-and-Effect Reasoning, CT: Critical Thinking, DEC: Decompositional Reasoning, QUA: Quantitative Reasoning.
ModelDED Accuracy Accuracy Accuracy Accuracy Accuracy Accuracy Accuracy Accuracy IND ABD ANA CAE CT DEC QUALLMsQwen-7B-Chat Yi-34B-Chat Internlm2-20B-Chat Qwen1.5-32B-Chat4.85 19.65 17.43 25.944.18 13.84 13.12 21.204.84 26.82 24.74 33.395.29 18.73 16.30 24.875.54 26.51 22.81 32.335.16 25.71 22.51 31.824.09 15.00 13.03 20.194.64 15.55 13.42 22.19GPT-3.5 Claude3 Sonnet GPT-4 GPT-4o19.38 25.40 33.93 39.113.19 17.88 24.80 30.1426.64 34.78 40.66 43.4316.30 23.28 33.33 37.7823.32 30.64 38.48 42.8924.31 31.15 39.32 44.0614.43 18.59 26.84 31.7917.35 22.67 31.72 36.56Image caption + LLMsQwen-7B-Chat Yi-34B-Chat Internlm2-20B-Chat Qwen1.5-32B-Chat5.66 19.08 18.25 25.504.69 13.34 12.69 20.557.27 29.24 28.37 35.816.88 20.11 17.67 26.356.66 25.53 23.84 31.356.02 24.91 23.35 31.394.60 13.79 13.40 19.554.81 14.64 14.52 21.51GPT-3.5 Claude3 Sonnet GPT-4 GPT-4o20.71 25.69 35.06 39.2613.91 19.11 24.44 30.9329.76 35.12 41.35 45.5017.78 24.02 34.39 39.3725.72 30.88 40.17 43.1726.01 31.55 40.72 44.1915.74 18.71 27.26 31.3517.73 22.89 32.47 36.56LMMsQwen-VL-Chat Yi-VL-34B InternVL-Chat-V1.5 LLaVA-NeXT-34B7.87 16.30 17.65 19.726.06 10.60 12.55 14.7012.80 21.11 30.28 30.628.68 16.40 17.25 19.379.90 21.35 22.06 27.409.92 20.76 22.70 25.395.29 11.89 12.56 13.626.29 13.42 14.37 14.79Qwen-VL-Max Gemini Pro Vision Claude3 Sonnet GPT-4V GPT-4o22.97 22.45 25.59 34.59 41.1816.87 17.16 18.89 25.59 32.7333.91 35.47 36.51 46.54 50.3521.38 21.59 23.70 33.33 40.5329.28 25.67 29.89 39.61 45.9428.51 27.54 31.71 41.15 47.1217.26 17.24 18.99 26.47 33.1718.13 18.91 22.87 30.79 37.58</p>
<p>Table 12 :
12
Experimental results across different visual reasoning abilities on OlympicArena benchmark, expressed as percentages, with the highest score in each setting underlined and the highest scores across all settings bolded.PR: Pattern Recognition, SPA: Spatial Reasoning, DIA: Diagrammatic Reasoning, SYB: Symbol Interpretation, COM: Comparative Visualization.
ModelPR Accuracy Accuracy Accuracy Accuracy Accuracy SPA DIA SYB COMLLMsQwen-7B-Chat Yi-34B-Chat Internlm2-20B-Chat Qwen1.5-32B-Chat4.59 23.70 22.89 28.932.64 13.58 13.06 17.944.26 19.56 18.63 24.674.01 17.61 15.73 22.184.66 22.37 21.16 27.83GPT-3.5 Claude3 Sonnet GPT-4 GPT-4o22.33 26.88 33.65 35.9613.27 17.60 23.99 28.7118.40 22.86 30.09 33.2916.05 20.49 27.94 31.5421.05 25.98 32.54 35.00Image caption + LLMsQwen-7B-Chat Yi-34B-Chat Internlm2-20B-Chat Qwen1.5-32B-Chat5.96 21.69 22.97 28.594.11 21.19 12.75 17.815.21 18.01 18.27 23.905.11 14.92 15.49 20.956.30 20.48 21.05 26.73GPT-3.5 Claude3 Sonnet GPT-4 GPT-4o23.96 27.60 34.29 37.0815.26 17.03 26.07 29.1019.72 22.84 31.07 33.6017.34 20.17 28.61 31.2222.30 26.28 33.11 35.91LMMsQwen-VL-Chat Yi-VL-34B InternVL-Chat-V1.5 LLaVA-NeXT-34B9.90 16.72 22.85 24.694.93 9.60 12.11 12.757.46 13.78 17.68 19.726.48 12.10 15.11 16.388.91 15.09 21.27 22.90Qwen-VL-Max Gemini Pro Vision Claude3 Sonnet GPT-4V GPT-4o27.43 28.98 27.18 35.28 41.4916.26 14.83 17.55 23.91 30.6522.35 21.65 22.43 30.25 36.9819.47 19.79 20.84 27.70 33.9126.01 26.13 25.56 34.40 40.58</p>
<p>Table 13 :
13
Experimental results on multimodal problems on OlympicArena benchmark, expressed as percentages, with the highest score in each setting underlined and the highest scores across all settings bolded.
ModelMath Accuracy Accuracy PhysicsChemistry Biology Geography Astronomy Accuracy Accuracy Accuracy AccuracyCS Pass@1 Accuracy OverallLLMsQwen-7B-Chat Yi-34B-Chat Internlm2-20B-Chat Qwen1.5-32B-Chat1.26 5.04 6.30 7.982.54 6.14 6.46 8.906.92 19.15 15.76 23.865.59 27.40 26.96 32.214.16 33.91 31.87 39.742.70 10.00 9.19 18.650 0.28 0.97 0.834.01 19.54 18.51 24.58GPT-3.5 Claude3 Sonnet GPT-4 GPT-4o6.30 8.82 16.81 21.857.20 11.76 18.43 21.8215.46 19.59 32.11 32.1126.85 31.99 39.71 42.1730.66 38.00 41.26 44.2811.62 15.68 23.92 30.686.25 2.64 12.50 12.7818.79 23.79 31.05 34.11Image caption + LLMsQwen-7B-Chat Yi-34B-Chat Internlm2-20B-Chat Qwen1.5-32B-Chat3.78 5.04 6.72 6.722.22 6.57 6.89 8.696.19 14.29 16.35 21.806.49 24.94 25.39 32.107.34 33.61 31.26 39.825.00 8.78 10.27 17.300 0.28 1.18 0.975.32 18.25 18.41 23.99GPT-3.5 Claude3 Sonnet GPT-4 GPT-4o4.20 5.46 16.81 21.0112.39 13.35 20.44 22.1418.11 20.47 29.90 31.2225.73 32.89 38.7 45.1932.32 38.23 45.12 45.199.73 13.38 26.62 30.547.64 3.89 12.26 14.5820.04 23.97 32.36 34.86LMMsQwen-VL-Chat Yi-VL-34B InternVL-Chat-V1.5 LLaVA-NeXT-34B3.36 3.36 7.56 4.622.65 6.46 6.25 6.466.63 9.13 16.05 14.439.84 18.79 24.94 28.3013.85 22.03 32.55 36.115.27 7.43 9.73 10.000 0 0.62 0.287.82 13.00 18.43 19.66Qwen-VL-Max Gemini Pro Vision Claude3 Sonnet GPT-4V GPT-4o6.30 7.56 5.46 13.87 26.477.63 9.11 13.45 18.22 22.1417.82 24.30 19.15 29.31 33.1428.86 32.55 33.22 40.27 46.9840.05 35.81 37.02 46.86 53.7515.14 11.22 17.30 22.43 31.761.25 2.36 2.36 11.25 13.6122.38 22.58 24.05 31.81 38.17</p>
<p>Table 14 :
14
Results of the process-level evaluation on our comprehensive OlympicArena benchmark.Each step of every problem is assigned a score of 0 (indicating incorrect) or 1 (indicating correct), with the highest score in each setting underlined and the highest scores across all settings highlighted in bold.The subject of computer science is neglected in this part due to the lack of solutions.
ModelMath Physics Chemistry Biology Geography Astronomy Overall Score Score Score Score Score Score ScoreLLMsQwen-7B-Chat Yi-34B-Chat Internlm2-20B-Chat Qwen1.5-32B-Chat18.7 30.2 21.2 32.043.7 51.0 35.0 44.035.1 54.0 51.2 61.118.9 31.9 22.7 32.034.5 36.5 32.9 45.231.5 40.3 33.3 48.630.4 40.7 32.7 43.8GPT-3.5 Claude3 Sonnet GPT-4 GPT-4o37.6 40.8 57.0 59.946.9 42.7 53.8 65.932.7 65.3 73.6 67.430.2 30.8 50.0 49.638.7 52.6 50.1 61.426.7 50.5 65.0 69.535.4 47.1 58.2 62.3Image caption + LLMsQwen-7B-Chat Yi-34B-Chat Internlm2-20B-Chat Qwen1.5-32B-Chat23.0 26.3 27.7 35.942.6 45.6 42.6 49.734.6 49.5 46.3 56.817.4 20.0 19.4 33.534.4 45.7 25.5 43.632.3 42.0 43.1 51.430.7 38.2 34.1 45.1GPT-3.5 Claude3 Sonnet GPT-4 GPT-4o32.1 50.7 61.4 54.346.7 51.7 53.8 63.351.2 66.1 62.7 71.829.1 33.4 51.1 58.638.4 55.8 52.0 56.638.2 52.2 62.2 72.639.3 51.7 57.2 62.9LMMsQwen-VL-Chat Yi-VL-34B InternVL-Chat-V1.5 LLaVA-NeXT-34B14.3 28.9 26.6 30.241.7 41.0 40.5 47.135.7 44.2 42.7 50.121.0 18.7 29.4 19.031.0 30.2 43.1 40.623.6 40.3 44.8 47.127.9 33.9 37.8 39.0Qwen-VL-Max Gemini Pro Vision Claude3 Sonnet GPT-4V GPT-4o27.5 28.5 47.3 49.9 60.252.4 46.4 46.8 54.0 54.865.5 45.2 63.2 71.1 72.224.3 19.9 24.2 51.4 51.636.0 33.5 43.2 56.3 59.648.4 40.5 48.1 64.3 74.442.3 35.7 45.5 57.8 62.1</p>
<p>Table 15 :
15
Experimental results across different languages (English and Chinese) on OlympicArena benchmark, expressed as percentages, with the highest score in each setting underlined and the highest scores across all settings bolded.
ModelEnglish Accuracy Accuracy ChineseLLMsQwen-7B-Chat Yi-34B-Chat Internlm2-20B-Chat Qwen1.5-32B-Chat4.17 16.37 16.56 22.734.55 18.89 16.62 25.29GPT-3.5 Claude3 Sonnet GPT-4 GPT-4o19.83 25.73 35.13 40.6515.50 18.20 27.31 33.66Image caption + LLMsQwen-7B-Chat Yi-34B-Chat Internlm2-20B-Chat Qwen1.5-32B-Chat4.71 16.96 17.40 22.935.21 16.26 16.43 24.24GPT-3.5 Claude3 Sonnet GPT-4 GPT-4o20.56 26.31 36.08 41.5015.77 17.43 27.40 33.07LMMsQwen-VL-Chat Yi-VL-34B InternVL-Chat-V1.5 LLaVA-NeXT-34B7.70 17.34 17.07 17.745.55 14.68 15.82 16.74Qwen-VL-Max Gemini Pro Vision Claude3 Sonnet GPT-4V GPT-4o20.14 21.61 26.52 36.18 43.0421.49 18.76 17.21 26.55 34.39</p>
<p>Table 16 :
16
Full results of Data Leakage Detection on the base models or text-only chat models behind the evaluated models (continued).The "Correspondence" column indicates the text-only chat model and multimodal (MM) chat model corresponding to the model being detected."# Leak." denotes the number of leakage instances."# T" represents the number of instances correctly answered among these leaks by the text-only chat model, while "# MM" represents the number of instances correctly answered among these leaks by the multimodal chat model.
Model to-be-detectedCorrespondence Text-only Chat Model MM Chat Model # Leak. # T # MM # Leak. # T # MM # Leak. # T # MM Math Physics Chemistry</p>
<p>Table 17 :
17
Full results of Data Leakage Detection on the base models or text-only chat models behind the evaluated models (continued).The "Correspondence" column indicates the text-only chat model and multimodal (MM) chat model corresponding to the model being detected."# Leak." denotes the number of leakage instances."# T" represents the number of instances correctly answered among these leaks by the text-only chat model, while "# MM" represents the number of instances correctly answered among these leaks by the multimodal chat model.
Model to-be-detectedCorrespondence Text-only Chat Model MM Chat Model # Leak. # T # MM # Leak. # T # MM # Leak. # T # MM Biology Geography Astronomy</p>
<p>Table 18 :
18
Full results of Data Leakage Detection on the base models or text-only chat models behind the evaluated models (continued).The "Correspondence" column indicates the text-only chat model and multimodal (MM) chat model corresponding to the model being detected."# Leak." denotes the number of leakage instances."# T" represents the number of instances correctly answered among these leaks by the text-only chat model, while "# MM" represents the number of instances correctly answered among these leaks by the multimodal chat model.
Model to-be-detectedCorrespondence Text-only Chat Model MM Chat ModelCSOverall</p>
<p>1.52AU, calculate the △v to go from Earth LEO to Mars i.e. ∆v = |∆v A | + |∆v B |. Compare your answer to the ∆v to reach Earth LEO.
Solution:∆v A =6.67 × 10 −11 × 1.99 × 10 30 1.49 × 10 112 × 1.52 1 + 1.52− 1= 2935 m s −1 = 2.94 km s−1∆v B =6.67 × 10 −11 × 1.99 × 10 30 1.52 × 1.49 × 10 111 −2 × 1 1 + 1.52= 2642 m s −1 = 2.64 km s−1∆v = 2.94 + 2.64= 5.58 km s −1</p>
<p>1 × 1.496 × 10 8 km (Earth's orbit radius) r M = 1.52 × 1.496 × 10 8 km (Mars' orbit radius)
a t =
r E +r M 2</p>
<p>https://github.com/GAIR-NLP/OlympicArena
At the time of doing most part of this work, GPT-4o has not been released yet, so GPT-4V is mainly used for annotating, evaluation, and case study.
https://mathpix.com/
https://github.com/GAIR-NLP/OlympicArena/tree/main/annotation
We annotate the validation sets to highlight their characteristics and save costs.
All annotations using GPT-4V are manually verified for reliability.
We use InternVL-Chat-V1.5 for its high performance and cost-effective captioning.
We leave more research on open-source model-based evaluation for future work.
We exclude Yi-VL-34B as it doesn't support multiple image inputs, which may cause an unfair comparison.
We also look forward to the development of more advanced detection tools and approaches.
We do not test Gemini-1.5-pro[39] as there are significant rate limits on accessing the model's API during the time we do experiments.
https://www.sympy.org/
AcknowledgementsWe sincerely appreciate all the laboratory members for their contributions in data annotation, project discussions, and providing valuable suggestions.Additionally, we extend our gratitude to Teacher Xiaoxia Yu from Hefei No. 168 Middle School for providing us with extensive information on various subjects.We also thank everyone who helps annotate the data for our benchmark dataset.Math -Logical Reasoning ErrorTherefore, substituting into ( * ), we obtainGPT 4V's output:To solve this problem, we can use the Pythagorean theorem.In the first placement of rectangle P QRS inside rectangle ABCD, we see that P Q is parallel to AB and QR is parallel to BC.Since P Q = 250 and AB = 718, we can denote the length of BC as x.In the second placement, the rectangle P QRS is rotated such that its corners touch the sides of rectangle ABCD.The diagonal P R of rectangle P QRS becomes the diameter of an inscribed circle within rectangle ABCD.This means that the diagonal P R is the hypotenuse of a right triangle with sides P Q and QR.Since P QRS is a rectangle, P Q = RS and P S = QR.Therefore, the length of QR is also x.Now we have a right triangle with sides 250 and x, and the hypotenuse is the diagonal P R. Using the Pythagorean theorem, we have:The diagonal P R also bisects rectangle ABCD into two right triangles, each with sides AB and BC.Therefore, the length of the diagonal P R can also be calculated using the Pythagorean theorem with sides AB and BC:Since both expressions represent P R 2 , we can set them equal to each other:Subtract x 2 from both sides: 62500 = 515524Now we see that there is a mistake in the calculation because the left side should not equal the right side directly.We need to subtract 62500 from 515524 to find the value of x 2 : x 2 = 515524 − 62500Now we take the square root of both sides to find x:Therefore, the length of BC is 673.The final answer is 673 .Therefore, statement D is correct.In conclusion, the answer is A, C .GPT 4V's output:First, we need to analyze the chart, which shows the variation of the negative logarithm of concentrations (-lgc) of H 2 CO 3 , HCO 3 − , CO 3 2− , and H + at different pH values.This chart is a typical acid-base titration curve where H 2 CO 3 is a diprotic weak acid that can dissociate into HCO 3 − and CO 3 2− .Option A: At pH = a, according to the chart, the concentration of HCO 3 − is the highest, followed by H 2 CO 3 , then CO 3 2− , and the concentration of H + is the lowest.This is consistent with the chart.Option B: At pH = 7, the concentration of H 2 CO 3 should be relatively low because it is a weak acid and does not dissociate completely at neutral pH.The concentration of Na + depends on whether NaOH has been added.If NaOH has been added, the concentration of Na + will increase.However, this option does not provide enough information to determine the exact concentration relationship between Na + and H 2 CO 3 , so we cannot confirm if this option is correct.Option C: At pH = 10.3, according to the chart, the concentration of CO 3 2− is the highest, and the concentration of HCO 3 − is very low at this pH.If NaOH is added to the solution, the concentration of Na + will increase.However, since the total concentration c(H 2 CO 3 ) + c(HCO −3 ) + c(CO 2− 3 ) = 1.000 × 10 −3 mol•L −1 , the concentration of Na + cannot be lower than this value.Therefore, this option is incorrect.Option D: The equilibrium constant K for the reaction3 can be expressed by the following equation:According to the chart, at pH = 10.3, the concentrations of HCOBiology -Logical Reasoning ErrorProblem:The bacterium Bradyrhizobium japonicum can infect soybean (Glycine max) roots and form nodules.The nitrogen fixation catalyzed by nitrogenase occurs in the nodules and the nitrogenase activity can be measured easily by acetylene reduction instead of nitrogen reduction.Scientists generated a defective mutation of NAD + -dependent malic enzyme, the enzyme that generates pyruvate and NADH, and infected soybean seedling roots with wildtype and mutant bacteria.The seedlings were grown in nitrogen-free media.After 14 and 28 days of inoculation, the number and weight of nodules in the seedlings and acetylene reduction activity were recorded [figure1].Which of the following statements are correct: A: Nitrogen fixation activity in nodules of the same treament at 28 days after inoculation is higher than that at 14 days after inoculation.B: Both number and size of nodules increase with time from 14 to 28 days after inoculation with B. japonicum.C: The reduction in nitrogen-fixing activity of nodules infected by the mutant at 28 days after inoculation compared to those at 14 days after inoculation is due to the reduction of nitrogenase activity and nodule formation.D: Nitrogen fixation in B. japonicum -induced nodule is down-regulated by NAD + dependent malic enzyme.Solution:A. True.As shown in the figure, in both wildtype and mutant treaments, acetylene reduction activity in nodules at 28 days after inoculation is higher than that at 14 days after inoculation, indicating higher nitrogen fixation actitity.B. True.In both treaments, number and size of nodules are higher at 28 days compared to those at 14 days after inoculation C. False.Number of nodules at 28 days after inoculation is higher than at 14 days after inoculation D. False.The mutation of NAD + -dependent malic enzyme in the bacteria results in a reduction of acetylene reduction, indicating that the enzyme (in wildtype) up-regulates the nitrogen fixation.So we choose A, B .GPT 4V's output:Let's analyze the data presented in Fig.Q13step by step: A: We need to determine if nitrogen fixation activity in nodules of the same treatment at 28 days after inoculation is higher than that at 14 days after inoculation.Looking at the graphs C and D, which represent acetylene reduction activity (a proxy for nitrogen fixation activity), we can see that for both wild-type (open bars) and mutant (solid bars) B. japonicum, the activity is higher at 28 days than at 14 days.This is true for both per gram of nodule dry weight (graph C) and per nodule (graph D).Therefore, statement A is correct.B: To assess this statement, we need to look at graphs A and B. Graph A shows the nodule number per plant, and graph B shows the nodule dry weight.For both wild-type and mutant B. japonicum, the number and size (weight) of nodules increase from 14 to 28 days after inoculation.Therefore, statement B is correct.C: This statement suggests that the reduction in nitrogen-fixing activity of nodules infected by the mutant at 28 days after inoculation compared to those at 14 days is due to the reduction of nitrogenase activity and nodule formation.From graph C, we can see that the acetylene reduction activity per gram of nodule dry weight for the mutant does decrease from 14 to 28 days, indicating a reduction in nitrogenase activity.However, graph A shows that the number of nodules actually increases for the mutant from 14 to 28 days, which contradicts the second part of the statement.Therefore, statement C is not entirely correct.D: The statement suggests that nitrogen fixation in B. japonicum-induced nodules is down-regulated by NAD+-dependent malic enzyme.The mutant, which has a defective mutation in the NAD+-dependent malic enzyme, shows lower acetylene reduction activity compared to the wild-type in both graphs C and D at both time points.This suggests that the enzyme plays a role in nitrogen fixation, and its absence (or defectiveness) leads to down-regulation of nitrogen fixation.Therefore, statement D is correct.The final answer is A, B, D .Geography -Knowledge Deficit ErrorProblem: From December 17 to 20, 2020, the Beijing team went to Tengchong, Yunnan to participate in the study tour course design competition, using the Beihai Wetland in Tengchong, known for its plateau volcanic dammed lakes, as the theme of the study tour course design.Based on this, complete the following small questions.During the study tour, the team members found that compared to Beijing, Tengchong has ( ).A. The tree shadows at sunrise are more towards the north.B. The temperature difference between day and night is larger.C. The sunset time is earlier.D. The tree shadows at noon are shorter.Solution:According to the material, Tengchong 25 • N, 98 • E belongs to Yunnan and is located to the southwest of Beijing 39 • N, 116 • E .From December 17 to 20, 2020, it was winter, around the winter solstice, with the sun rising in the southeast and the shadow at sunrise being in the northwest.Beijing is at a higher latitude than Tengchong, causing a greater shadow deviation, so A is incorrect.Tengchong is located near a plateau volcanic dammed lake, rich in geothermal energy, resulting in high nighttime temperatures and smaller temperature differences between day and night, so B is incorrect.Tengchong, being at a lower latitude, has longer daylight hours and a later sunset time, making C incorrect.Tengchong's lower latitude also means a larger solar altitude angle at noon, leading to shorter tree shadows, so D is correct.GPT 4V's output:First, we need to understand the geographical locations of Beijing and Tengchong.Beijing is in the northern part of China, while Tengchong is in the southwestern part of China, near the Myanmar border.This means that Tengchong is to the southwest of Beijing. A. The tree shadows at sunrise are more towards the north -Due to the Earth's rotation, the direction of tree shadows at sunrise is mainly determined by latitude, not by the east-west position.Therefore, this option is incorrect.B. The temperature difference between day and night is larger -Tengchong is located in a plateau area, while Beijing is in a plain area.Generally, the temperature difference between day and night in plateau areas is larger than in plain areas because high-altitude areas cool down quickly at night and receive strong solar radiation during the day.Therefore, this option might be correct.C. The sunset time is earlier -The sunset time is mainly determined by longitude, as the Earth rotates from west to east.Tengchong is to the southwest of Beijing, but this does not necessarily mean that the sunset time is earlier.This option requires more information to determine.D. The tree shadows at noon are shorter -The solar altitude angle at noon depends on latitude; the lower the latitude, the higher the solar altitude angle, and the shorter the tree shadows.Tengchong has a lower latitude than Beijing, so the tree shadows at noon should be shorter.In summary, options B and D are both possible.However, we need to determine which one is most related to the "plateau volcanic dammed lake" mentioned in the problem.Since this description emphasizes the plateau characteristics of Tengchong, we can reasonably infer that the larger temperature difference between day and night is most relevant to this characteristic.Therefore, the final answer is B .
Gpt-4v(ision) system card. 2023</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Ai Anthropic, The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card. 2024</p>
<p>Have llms advanced enough? a challenging problem solving benchmark for large language models. Daman Arora, Himanshu Gaurav Singh, arXiv:2305.150742023arXiv preprint</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.166092023Qwen technical report. arXiv preprint</p>
<p>Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou, arXiv:2308.12966Qwen-vl: A frontier large vision-language model with versatile abilities. 2023arXiv preprint</p>
<p>Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou, 2023</p>
<p>. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, arXiv:2403.172972024Internlm2 technical report. arXiv preprint</p>
<p>Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, arXiv:2403.20330Are we on the right way for evaluating large vision-language models?. 2024arXiv preprint</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors. Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong Sun, Jie Zhou, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, arXiv:2312.142382023arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Imagenet: A largescale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. Ieee2009</p>
<p>Computers and thought. Julian Edward A Feigenbaum, Feldman, 1963New York McGraw-Hill</p>
<p>Cognitive reasoning: A personal view. Ulrich Furbach, Steffen Hölldobler, Marco Ragni, Claudia Schon, Frieder Stolzenburg, 201933KI-Künstliche Intelligenz</p>
<p>Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, arXiv:2402.140082024arXiv preprint</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Measuring coding challenge competence with apps. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, arXiv:2105.099382021arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, Melvin Johnson, International Conference on Machine Learning. PMLR2020</p>
<p>C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, Advances in Neural Information Processing Systems. 362024</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Proceedings of NAACL-HLT. Jacob Devlin, Ming-Wei Chang, Kenton , Lee Kristina, Toutanova , NAACL-HLT2019</p>
<p>Measuring faithfulness in chain-of-thought reasoning. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, arXiv:2307.137022023arXiv preprint</p>
<p>Gradient-based learning applied to document recognition. Yann Lecun, Léon Bottou, Yoshua Bengio, Patrick Haffner, Proceedings of the IEEE. 86111998</p>
<p>Cmmlu: Measuring massive multitask language understanding in chinese. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, Timothy Baldwin, arXiv:2306.092122023arXiv preprint</p>
<p>Mmcode: Evaluating multimodal code large language models with visually rich programming problems. Kaixin Li, Yuchen Tian, Qisheng Hu, Ziyang Luo, Jing Ma, arXiv:2404.094862024arXiv preprint</p>
<p>Competition-level code generation with alphacode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Science. 37866242022</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, arXiv:2305.20050Ilya Sutskever, and Karl Cobbe. Let's verify step by step. Bowen Baker, Teddy LeeJan. 2023arXiv preprint</p>
<p>Chengwu Liu, Jianhao Shen, Huajian Xin, Zhengying Liu, Ye Yuan, Haiming Wang, Wei Ju, Chuanyang Zheng, Yichun Yin, Lin Li, arXiv:2309.04295A challenge formal dataset for automated theorem proving. 2023arXiv preprint</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, Yong Jae Lee, Llava-next: Improved reasoning, ocr, and world knowledge. 2024</p>
<p>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, arXiv:2104.087862021arXiv preprint</p>
<p>Fairness-guided few-shot prompting for large language models. Huan Ma, Changqing Zhang, Yatao Bian, Lemao Liu, Zhirui Zhang, Peilin Zhao, Shu Zhang, Huazhu Fu, Qinghua Hu, Bingzhe Wu, Advances in Neural Information Processing Systems. 202436</p>
<p>Meredith Ringel, Morris , Jascha Sohl-Dickstein, Noah Fiedel, Tris Warkentin, Allan Dafoe, Aleksandra Faust, Clement Farabet, Shane Legg, arXiv:2311.02462Levels of agi: Operationalizing progress on the path to agi. 2023arXiv preprint</p>
<p>. OpenAI. Introducing superalignment. OpenAI Blog. 2023</p>
<p>Hello gpt-4o. Openai, OpenAI Blog. 2024</p>
<p>Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, Maosong Sun, arXiv:2307.07924Communicative agents for software development. 2023arXiv preprint</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jeanbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, arXiv:2403.055302024arXiv preprint</p>
<p>David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, arXiv:2311.12022Gpqa: A graduate-level google-proof q&amp;a benchmark. 2023arXiv preprint</p>
<p>Can language models solve olympiad programming?. Quan Shi, Michael Tang, Karthik Narasimhan, Shunyu Yao, arXiv:2404.109522024arXiv preprint</p>
<p>Wu's method can boost symbolic ai to rival silver medalists and alphageometry to outperform gold medalists at imo geometry. Shiven Sinha, Ameya Prabhu, Ponnurangam Kumaraguru, Siddharth Bhat, Matthias Bethge, arXiv:2404.064052024arXiv preprint</p>
<p>A survey of reasoning with foundation models. Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, arXiv:2312.115622023arXiv preprint</p>
<p>Scieval: A multi-level large language model evaluation benchmark for scientific research. Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, Kai Yu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.118052023arXiv preprint</p>
<p>Solving olympiad geometry without human demonstrations. Yuhuai Trieu H Trinh, Wu, He Quoc V Le, Thang He, Luong, Nature. 62579952024</p>
<p>Computing machinery and intelligence. The Turing Test: Verbal Behavior as the Hallmark of Intelligence. Alan M Turing, J Haugeland, 1950</p>
<p>Solving math word problems with process-and outcome-based feedback. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, Irina Higgins, arXiv:2211.142752022arXiv preprint</p>
<p>Glue: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP2018</p>
<p>Scientific discovery in the age of artificial intelligence. Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Nature. 62079722023</p>
<p>Peiyi Wang, Lei Li, Zhihong Shao, Damai Xu, Yifei Dai, Deli Li, Chen, Zhifang Wu, Sui, arXiv:2312.08935Math-shepherd: A label-free step-by-step verifier for llms in mathematical reasoning. 2023arXiv preprint</p>
<p>Scibench: Evaluating college-level scientific problem-solving abilities of large language models. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Shichang Arjun R Loomba, Yizhou Zhang, Wei Sun, Wang, arXiv:2307.106352023arXiv preprint</p>
<p>Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, Pengfei Liu, arXiv:2404.05692Evaluating mathematical reasoning beyond accuracy. 2024arXiv preprint</p>
<p>Benchmarking benchmark leakage in large language models. Ruijie Xu, Zengzhi Wang, Run-Ze Fan, Pengfei Liu, arXiv:2404.188242024arXiv preprint</p>
<p>Open foundation models by 01. Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, arXiv:2403.046522024ai. arXiv preprint</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, Metamath, arXiv:2309.12284Bootstrap your own mathematical questions for large language models. 2023arXiv preprint</p>
<p>. Weizhe Yuan, Pengfei Liu, arXiv:2206.111472022restructured pre-training. arXiv preprint</p>
<p>Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, arXiv:2311.165022023arXiv preprint</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, arXiv:2309.05653Mammoth: Building math generalist models through hybrid instruction tuning. 2023arXiv preprint</p>
<p>Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo, arXiv:2401.11944A chinese massive multi-discipline multimodal understanding benchmark. 2024arXiv preprint</p>
<p>Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, arXiv:2403.146242024arXiv preprint</p>
<p>Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, arXiv:2305.12474Liang He, and Xipeng Qiu. Evaluating the performance of large language models on gaokao benchmark. 2023arXiv preprint</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, arXiv:2304.063642023arXiv preprint</p>
<p>Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, arXiv:2308.079212023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9854 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9854</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9854</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-1ee1f4dd35b8d78af8381a47e4d655856427f02f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1ee1f4dd35b8d78af8381a47e4d655856427f02f" target="_blank">Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction</a></p>
                <p><strong>Paper Venue:</strong> The Web Conference</p>
                <p><strong>Paper TL;DR:</strong> A chain-of-retrieval prompt to guide ChatGPT to generate labeled long-text data step by step and a denoising strategy based on the consistency of cross-document knowledge are proposed, which Generates labeled data by Retrieval and Denoising Knowledge from LLMs, called GenRDK.</p>
                <p><strong>Paper Abstract:</strong> Document-level Relation Triplet Extraction (DocRTE) is a fundamental task in information systems that aims to simultaneously extract entities with semantic relations from a document. Existing methods heavily rely on a substantial amount of fully labeled data. However, collecting and annotating data for newly emerging relations is time-consuming and labor-intensive. Recent advanced Large Language Models (LLMs), such as ChatGPT and LLaMA, exhibit impressive long-text generation capabilities, inspiring us to explore an alternative approach for obtaining auto-labeled documents with new relations. In this paper, we propose a Zero-shot Document-level Relation Triplet Extraction (ZeroDocRTE) framework, which Generates labeled data by Retrieval and Denoising Knowledge from LLMs, called GenRDK. Specifically, we propose a chain-of-retrieval prompt to guide ChatGPT to generate labeled long-text data step by step. To improve the quality of synthetic data, we propose a denoising strategy based on the consistency of cross-document knowledge. Leveraging our denoised synthetic data, we proceed to fine-tune the LLaMA2-13B-Chat for extracting document-level relation triplets. We perform experiments for both zero-shot document-level relation and triplet extraction on two public datasets. The experimental results illustrate that our GenRDK framework outperforms strong baselines.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9854.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9854.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GenRDK</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generates labeled data by Retrieval and Denoising Knowledge from LLMs (GenRDK)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework introduced in this paper that uses an LLM (ChatGPT) to synthesize document-level, labeled relation-triplet data via a multi-step chain-of-retrieval prompt, then denoises that synthetic knowledge by fusing pseudo-labels from a fine-tuned LLaMA2 model into cross-document knowledge graphs and pruning by consistency before fine-tuning a final extractor.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (generation) + LLaMA2-13B-Chat (pre-denoising and final fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline combining: (1) ChatGPT (RLHF-based conversational LLM) used interactively with a multi-step chain-of-retrieval prompt to generate fictional Wikipedia-style documents, entity lists, relation triplets, reasoning explanations and supporting sentence indices; (2) LLaMA2-13B-Chat fine-tuned with LoRA on seen relations to act as a pre-denoising model that produces pseudo-labels for synthetic documents; (3) a graph-based denoising module that fuses synthetic labels and pseudo-label frequencies into a fused knowledge graph and prunes low-consistency facts; (4) final fine-tuning of LLaMA2-13B-Chat on the denoised synthetic data to produce the extractor.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Two types of input corpora: (a) seen human-annotated DocRE data (DocRED / Re-DocRED) used to fine-tune the pre-denoising LLaMA model and to define seen vs unseen relation splits; (b) synthetic, fictional Wikipedia-style documents produced by ChatGPT per unseen relation type (documents include context paragraphs, entity lists, relation triplets, reasoning, and supporting sentence indices). The paper does not use scholarly papers as inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>For each target unseen relation r_i, prompt ChatGPT to (i) select related relations, (ii) generate fiction Wikipedia-style paragraphs containing r_i and related relations, (iii) extract entity lists, (iv) extract relation triplets, (v) provide reasoning for each triplet and supporting sentence indices — producing structured synthetic labeled documents for unseen relation types.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Multi-step chain-of-retrieval prompting of ChatGPT to synthesize documents and structured labels; train a pre-denoising LLaMA2-13B-Chat (LoRA) on seen relation groups to infer pseudo-labels on synthetic docs; build two cross-document knowledge graphs (KG_s from original synthetic labels, KG_p from pseudo labels) where nodes are entities and weighted edges are triplet frequencies; fuse KG_s and KG_p and compute per-triplet consistency score s_ijk = F^s_{ijk} + F^p_{ijk}; compute a dynamic threshold η_k per relation type (based on mean and variance of scores for that relation) and prune triplets below threshold; relabel synthetic documents from the pruned graph; finally fine-tune LLaMA2-13B-Chat on denoised synthetic data as the document-level relation triplet extractor.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Denoised synthetic labeled corpus (document text + entity lists + relation triplets + reasoning + supporting sentence indices) and a denoised cross-document knowledge graph of relation triplets.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>{"title": "The Godfather", "context": "...The screenplay was written by Coppola and Mario Puzo...", "entities": [{"entity":"Mario Puzo","type":"Person"}, ...], "triplets": [["The Godfather","Mario Puzo","screenwriter"]], "reasoning": [{"triplet":"...","explanation":"Mario Puzo was a co-writer..."}], "support_sentence_index": 2}</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Downstream evaluation on zero-shot Document-level Relation Extraction (ZeroDocRE) and zero-shot Document-level Relation Triplet Extraction (ZeroDocRTE) using the Re-DocRED and DocRED datasets; F1 as the main metric; baseline comparisons to multiple LLMs and supervised DocRE models; ablation studies (w/o denoising, w/o seen data, w/o pruning, w/o synthetic data) and prompt comparisons (vanilla, chain-of-thought, chain-of-retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GenRDK (with denoising) improved downstream metrics vs baselines: For ZeroDocRE (m=5 unseen relations) GenRDK achieved 41.3 ± 8.9 F1 on Re-DocRED and 41.5 ± 8.7 F1 on DocRED (Test). For ZeroDocRTE (m=5) GenRDK achieved 13.1 ± 2.6 F1 on Re-DocRED and 14.2 ± 1.3 F1 on DocRED (Test). Using denoised vs original synthetic data improved performance (example improvements ~1.7–2.1 F1 for triplet extraction; tables show consistent gains across backbone models). Chain-of-retrieval prompting outperformed vanilla and chain-of-thought prompting in downstream F1.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Effectively leverages LLM generation (ChatGPT) to produce document-level labeled training data for unseen relations; chain-of-retrieval prompt yields higher-quality, structured synthetic examples; the cross-document consistency denoising recovers missing triplets and prunes many hallucinated facts, improving downstream performance; scalable in that many synthetic examples can be generated without human annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Synthetic data originate from LLM generation (not from scholarly corpora); generation suffers from LLM hallucination causing noisy or incorrect relation facts; denoising relies on frequency/consistency across synthetic documents and a pre-denoising model trained on seen relations, which may not remove all errors and depends on seen-data coverage; input corpus size and generation budget not specified; control over diversity and factual fidelity is limited and identified as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Hallucinated but plausible incorrect relational facts in synthetic documents (paper cites examples like incorrect triplets and missing correct triplets). The paper reports examples where ChatGPT introduced incorrect relations (e.g., an incorrect cast-member relation) or omitted true relations, later corrected or pruned by the denoising step; Michael Jordan / NBA relation was cited as an erroneous synthetic relation example.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9854.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9854.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Retrieval Prompt (CoR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Retrieval Prompting (multi-step generation and extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured multi-step prompting protocol that decomposes document-level synthetic data generation into sequential subtasks: selecting related relations, generating a Wikipedia-style paragraph, extracting entities, extracting triplets, providing reasoning, and indexing supporting sentences — implemented as a conversational chain so each step has memory of prior steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (invoked via chain-of-retrieval prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A prompting strategy (not a model): prompts ChatGPT iteratively to (1) select relations related to an unseen relation, (2) generate a fictional multi-sentence Wikipedia-style paragraph containing the target and related relations (temperature set to 1 for diversity), (3) extract entities with types, (4) extract relation triplets over the full relation set, (5) provide reasoning for each triplet, (6) provide supporting-sentence indices, and (7) assemble final structured JSON labels.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Per unseen relation, the prompt supplies the set of all relation types and asks ChatGPT to produce synthetic documents; the method leverages the unseen relation set defined from DocRED/Re-DocRED splits (no scholarly papers used).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Guided generation to produce documents and labels that explicitly include the given unseen relation and a set of related relations selected by the model, maximizing variety and document-level (cross-sentence) expression of relation triplets.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt engineering: decomposition of the generation+annotation task into multiple explicit chat steps so that each subtask is simpler and has explicit outputs (entities, triplets, reasons, support indices). Compared experimentally against single-shot (vanilla) and chain-of-thought prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured synthetic documents and labels (JSON): title, context paragraph(s), entity list with types, relation triplets, reasoning explanations, support-sentence indices.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>{"title":"The Godfather","context":"...","entities":[{"entity":"Francis Ford Coppola","type":"Person"}],"triplets":[["The Godfather","Francis Ford Coppola","director"]],"reasons":[{"triplet":"...","explanation":"..."}],"support_sentence_index":1}</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Ablation/prompt comparison: models were trained on synthetic data produced by vanilla prompt, chain-of-thought prompt, and chain-of-retrieval prompt; downstream performance (F1) measured on ZeroDocRE and ZeroDocRTE.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Chain-of-Retrieval synthetic data yielded the best downstream performance: e.g., for ZeroDocRE, Chain-of-Retrieval achieved Test F1 49.21 (Re-DocRED) and 48.30 (DocRED) versus Chain-of-Thought 47.80/43.72 and Vanilla 42.45/34.98; for ZeroDocRTE, Chain-of-Retrieval Test F1 13.23/13.38 (Re-DocRED/DocRED) outperforming other prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Decomposes a hard generation+annotation task into manageable steps, producing richer structured outputs (reasoning, support indices) and better downstream utility; increases diversity via temperature control; empirically outperforms simpler prompting variants on downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires a multi-turn conversational interface and more prompt-engineering; still inherits hallucination/noise from the LLM; generation quality depends on temperature and the LLM's internal knowledge; no automatic guarantee of factual correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Produces plausible but incorrect triplets and sometimes omits correct triplets; these failure modes motivated the cross-document denoising stage (examples shown in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9854.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9854.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-Denoise</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Consistency-guided Cross-Document Knowledge Denoising</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A denoising algorithm introduced in this paper that builds two frequency-weighted cross-document knowledge graphs from (a) original synthetic labels and (b) pseudo labels produced by a pre-denoising model, fuses them, computes per-triplet consistency scores, and prunes triplets below a dynamic threshold to reduce hallucinated facts and recover missing ones.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Graph fusion + frequency-based scoring using pseudo-labels from LLaMA2-13B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Construct KG_s (from synthetic labels T_syn) and KG_p (from pseudo labels T_p) with entities as nodes and relation types as labeled edges weighted by frequency; fuse to KG_f and compute consistency score s_ijk = F^s_{ijk} + F^p_{ijk}; compute a dynamic threshold η_k per unseen relation using mean and variance of consistency scores for that relation; prune triplets with s_ijk < η_k to obtain denoised KG_d and relabel synthetic documents accordingly.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B (LLaMA2-13B-Chat used for pseudo labels)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Synthetic documents and labels generated by ChatGPT, and pseudo labels predicted on those synthetic documents by a pre-denoising LLaMA2-13B-Chat model trained on seen relations (DocRED).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Assess cross-document consistency of relational facts for unseen relation types to identify reliable triplets and prune hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Cross-document aggregation and frequency-based consistency scoring; dynamic thresholding per relation type based on empirical score distribution; prune and relabel synthetic data to produce cleaned training data.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Denoised knowledge graph (KG_d) and relabeled synthetic training corpus with unreliable triplets removed and some missing triplets recovered via cross-document evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Original synthetic label: (The Godfather, Vito Corleone, cast member) [incorrect] --> pruned; Added/retained: (The Godfather, Francis Ford Coppola, screenwriter) [recovered via cross-document frequency].</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compare downstream models trained on original synthetic data vs denoised synthetic data; ablation studies removing pruning or seen-data usage; report F1 changes across multiple backbone models and datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Training on denoised synthetic data consistently improved downstream F1 across backbone models. Example: for one backbone (Ours-Roberta-large) +Synthetic Data Test F1 on Re-DocRED = 49.21, +Denoised Synthetic Data = 51.88; the denoising step yields consistent F1 increases (several points) and improved handling of specific unseen relations (figures/tables in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Reduces hallucinated relations and recovers missing true relations by exploiting redundancy across many generated documents; empirically improves downstream extraction performance; straightforward, interpretable scoring and pruning based on frequencies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on frequency signals which can be biased (frequent hallucinations could survive); requires a competent pre-denoising model trained on seen relations; dynamic thresholding hyperparameters and statistics depend on number/variety of synthetic examples; may not remove all errors or may remove rare but correct facts.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Some incorrect synthetic facts persist if they appear frequently across generated documents; rare true facts that appear only once may be pruned if below threshold; examples illustrated in the paper where incorrect or missing relations required further correction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9854.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9854.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (used for synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (OpenAI conversational LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Used as the generative source of synthetic, labeled Wikipedia-style documents, entity annotations, relation triplets, reasoning explanations, and supporting sentence indices via the chain-of-retrieval prompting pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A commercially available RLHF-refined conversational LLM that was prompted interactively in multiple steps (chain-of-retrieval) to generate long-form synthetic documents and structured annotations; generation hyper-parameter temperature set to 1 to increase diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Prompt inputs consisted of the unseen relation type, the entire relation set R (from DocRED/Re-DocRED), and instructive step templates; ChatGPT produced fictional documents and structured labels in JSON format.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Generate documents that contain at least one instance of a specified unseen relation and related relations, then annotate entities/triplets/reasons/supporting-sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt-driven synthesis (chain-of-retrieval); ChatGPT is used as a data annotator and generator rather than for direct theory summarization from scholarly articles.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured synthetic labeled documents (JSON) including title, context, entity list, triplets, reasoning and supporting-sentence indices.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>See Figure 4 example: {"title":"The Godfather","context":"The Godfather is a 1972 American film directed by Francis Ford Coppola...","entities":[...],"triplets":[["The Godfather","Mario Puzo","screenwriter"]],"reasons":[...],"support_sentence_index":2}</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>ChatGPT was evaluated both as a baseline extractor (direct prompting) and as a generator whose synthetic data was used to train other models; downstream F1 scores (Tables 1 & 2) and comparisons with models trained on synthetic data were used to assess utility.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>As a direct baseline for relation extraction/triplet extraction ChatGPT achieved nontrivial but lower F1 than GenRDK-trained models (e.g., Table 2: ChatGPT Test F1 for Re-DocRED ZeroDocRE m=5 = 21.7 ± 7.5; GenRDK-trained models achieve ~41.3 ± 8.9). Synthetic data generated by ChatGPT (when denoised and used to fine-tune extractors) produced substantially better downstream performance.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Produces coherent long-text and stepwise annotations including reasoning and supporting sentence indices; strong zero-shot generation ability enabling creation of document-level multi-triplet examples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Prone to hallucination (plausible but incorrect facts) which requires denoising; output factuality and diversity depend on prompt design and randomness (temperature); no explicit access to or use of scholarly corpora in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Hallucinated relations or incorrect entity-relation attributions in synthetic documents (examples shown where incorrect triplets were generated and later pruned).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9854.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9854.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA2-13B-Chat (LoRA fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA2-13B-Chat fine-tuned with Low-Rank Adaptation (LoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Used as the pre-denoising model (trained on seen relations) to generate pseudo-labels for synthetic documents and as the final relation triplet extractor fine-tuned on denoised synthetic data; LoRA applied for parameter-efficient fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-13B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLaMA2 chat variant with ~13 billion parameters; fine-tuned with LoRA adapters (Low-Rank Adaptation) on seen relation data (DocRED splits) using a random relation-group composition strategy during training; learning rate 1e-6, batch size 20 for triplet extraction experiments as reported.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Fine-tuned on seen-relation documents from DocRED/Re-DocRED (D_s) and then fine-tuned on denoised synthetic data (D_syn^-); used in inference to produce pseudo labels for synthetic documents during denoising.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Given a document and a relation group R_j (subset of relations), produce relation triplet annotations (used both for pseudo-labeling and as the learned extractor).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Parameter-efficient fine-tuning (LoRA) on seen relation groups to build a pre-denoising model; inference on synthetic docs to obtain pseudo labels; final fine-tuning on denoised synthetic data to obtain the extractor.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Pseudo-labels for synthetic documents (triplets) and final extractor outputs (predicted triplets on evaluation documents).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Pseudo-labels P_i = \hat{M}(I, d_i^u, R_u) producing structured triplet lists for each synthetic document.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Fine-tuned LLaMA2-13B-Chat models evaluated on ZeroDocRE and ZeroDocRTE using F1; compared against other LLM baselines and ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>LLaMA2-13B-Chat used as backbone yields reasonable baseline numbers (see Tables 1 and 2). Fine-tuning on denoised synthetic data produces stronger performance than on original synthetic data; GenRDK fine-tuned LLaMA2-13B-Chat achieves the best reported extractor performance in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Can be fine-tuned with LoRA for efficient adaptation to the extraction task; acts as both a denoiser (via pseudo-labeling) and final predictor within the pipeline; integrates well with the synthetic-data approach.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Performance depends on the quality and coverage of synthetic and seen data; requires substantial compute to fine-tune (experiments used multiple high-memory GPUs); initial model alone underperforms ChatGPT when used purely as a zero-shot baseline without fine-tuning on synthetic data.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Pre-denoising model can produce incorrect pseudo-labels if seen-data coverage is insufficient for certain unseen relation patterns, which can propagate errors into fused KG if not corrected by consistency thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction <em>(Rating: 2)</em></li>
                <li>Is GPT-3 a Good Data Annotator? <em>(Rating: 2)</em></li>
                <li>Chain-of-Verification Reduces Hallucination in Large Language Models <em>(Rating: 1)</em></li>
                <li>Scaling instruction-finetuned language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9854",
    "paper_id": "paper-1ee1f4dd35b8d78af8381a47e4d655856427f02f",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "GenRDK",
            "name_full": "Generates labeled data by Retrieval and Denoising Knowledge from LLMs (GenRDK)",
            "brief_description": "A framework introduced in this paper that uses an LLM (ChatGPT) to synthesize document-level, labeled relation-triplet data via a multi-step chain-of-retrieval prompt, then denoises that synthetic knowledge by fusing pseudo-labels from a fine-tuned LLaMA2 model into cross-document knowledge graphs and pruning by consistency before fine-tuning a final extractor.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (generation) + LLaMA2-13B-Chat (pre-denoising and final fine-tuning)",
            "model_description": "Pipeline combining: (1) ChatGPT (RLHF-based conversational LLM) used interactively with a multi-step chain-of-retrieval prompt to generate fictional Wikipedia-style documents, entity lists, relation triplets, reasoning explanations and supporting sentence indices; (2) LLaMA2-13B-Chat fine-tuned with LoRA on seen relations to act as a pre-denoising model that produces pseudo-labels for synthetic documents; (3) a graph-based denoising module that fuses synthetic labels and pseudo-label frequencies into a fused knowledge graph and prunes low-consistency facts; (4) final fine-tuning of LLaMA2-13B-Chat on the denoised synthetic data to produce the extractor.",
            "model_size": "13B",
            "input_corpus_description": "Two types of input corpora: (a) seen human-annotated DocRE data (DocRED / Re-DocRED) used to fine-tune the pre-denoising LLaMA model and to define seen vs unseen relation splits; (b) synthetic, fictional Wikipedia-style documents produced by ChatGPT per unseen relation type (documents include context paragraphs, entity lists, relation triplets, reasoning, and supporting sentence indices). The paper does not use scholarly papers as inputs.",
            "input_corpus_size": null,
            "topic_query_description": "For each target unseen relation r_i, prompt ChatGPT to (i) select related relations, (ii) generate fiction Wikipedia-style paragraphs containing r_i and related relations, (iii) extract entity lists, (iv) extract relation triplets, (v) provide reasoning for each triplet and supporting sentence indices — producing structured synthetic labeled documents for unseen relation types.",
            "distillation_method": "Multi-step chain-of-retrieval prompting of ChatGPT to synthesize documents and structured labels; train a pre-denoising LLaMA2-13B-Chat (LoRA) on seen relation groups to infer pseudo-labels on synthetic docs; build two cross-document knowledge graphs (KG_s from original synthetic labels, KG_p from pseudo labels) where nodes are entities and weighted edges are triplet frequencies; fuse KG_s and KG_p and compute per-triplet consistency score s_ijk = F^s_{ijk} + F^p_{ijk}; compute a dynamic threshold η_k per relation type (based on mean and variance of scores for that relation) and prune triplets below threshold; relabel synthetic documents from the pruned graph; finally fine-tune LLaMA2-13B-Chat on denoised synthetic data as the document-level relation triplet extractor.",
            "output_type": "Denoised synthetic labeled corpus (document text + entity lists + relation triplets + reasoning + supporting sentence indices) and a denoised cross-document knowledge graph of relation triplets.",
            "output_example": "{\"title\": \"The Godfather\", \"context\": \"...The screenplay was written by Coppola and Mario Puzo...\", \"entities\": [{\"entity\":\"Mario Puzo\",\"type\":\"Person\"}, ...], \"triplets\": [[\"The Godfather\",\"Mario Puzo\",\"screenwriter\"]], \"reasoning\": [{\"triplet\":\"...\",\"explanation\":\"Mario Puzo was a co-writer...\"}], \"support_sentence_index\": 2}",
            "evaluation_method": "Downstream evaluation on zero-shot Document-level Relation Extraction (ZeroDocRE) and zero-shot Document-level Relation Triplet Extraction (ZeroDocRTE) using the Re-DocRED and DocRED datasets; F1 as the main metric; baseline comparisons to multiple LLMs and supervised DocRE models; ablation studies (w/o denoising, w/o seen data, w/o pruning, w/o synthetic data) and prompt comparisons (vanilla, chain-of-thought, chain-of-retrieval).",
            "evaluation_results": "GenRDK (with denoising) improved downstream metrics vs baselines: For ZeroDocRE (m=5 unseen relations) GenRDK achieved 41.3 ± 8.9 F1 on Re-DocRED and 41.5 ± 8.7 F1 on DocRED (Test). For ZeroDocRTE (m=5) GenRDK achieved 13.1 ± 2.6 F1 on Re-DocRED and 14.2 ± 1.3 F1 on DocRED (Test). Using denoised vs original synthetic data improved performance (example improvements ~1.7–2.1 F1 for triplet extraction; tables show consistent gains across backbone models). Chain-of-retrieval prompting outperformed vanilla and chain-of-thought prompting in downstream F1.",
            "strengths": "Effectively leverages LLM generation (ChatGPT) to produce document-level labeled training data for unseen relations; chain-of-retrieval prompt yields higher-quality, structured synthetic examples; the cross-document consistency denoising recovers missing triplets and prunes many hallucinated facts, improving downstream performance; scalable in that many synthetic examples can be generated without human annotation.",
            "limitations": "Synthetic data originate from LLM generation (not from scholarly corpora); generation suffers from LLM hallucination causing noisy or incorrect relation facts; denoising relies on frequency/consistency across synthetic documents and a pre-denoising model trained on seen relations, which may not remove all errors and depends on seen-data coverage; input corpus size and generation budget not specified; control over diversity and factual fidelity is limited and identified as future work.",
            "failure_cases": "Hallucinated but plausible incorrect relational facts in synthetic documents (paper cites examples like incorrect triplets and missing correct triplets). The paper reports examples where ChatGPT introduced incorrect relations (e.g., an incorrect cast-member relation) or omitted true relations, later corrected or pruned by the denoising step; Michael Jordan / NBA relation was cited as an erroneous synthetic relation example.",
            "uuid": "e9854.0",
            "source_info": {
                "paper_title": "Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Chain-of-Retrieval Prompt (CoR)",
            "name_full": "Chain-of-Retrieval Prompting (multi-step generation and extraction)",
            "brief_description": "A structured multi-step prompting protocol that decomposes document-level synthetic data generation into sequential subtasks: selecting related relations, generating a Wikipedia-style paragraph, extracting entities, extracting triplets, providing reasoning, and indexing supporting sentences — implemented as a conversational chain so each step has memory of prior steps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (invoked via chain-of-retrieval prompts)",
            "model_description": "A prompting strategy (not a model): prompts ChatGPT iteratively to (1) select relations related to an unseen relation, (2) generate a fictional multi-sentence Wikipedia-style paragraph containing the target and related relations (temperature set to 1 for diversity), (3) extract entities with types, (4) extract relation triplets over the full relation set, (5) provide reasoning for each triplet, (6) provide supporting-sentence indices, and (7) assemble final structured JSON labels.",
            "model_size": null,
            "input_corpus_description": "Per unseen relation, the prompt supplies the set of all relation types and asks ChatGPT to produce synthetic documents; the method leverages the unseen relation set defined from DocRED/Re-DocRED splits (no scholarly papers used).",
            "input_corpus_size": null,
            "topic_query_description": "Guided generation to produce documents and labels that explicitly include the given unseen relation and a set of related relations selected by the model, maximizing variety and document-level (cross-sentence) expression of relation triplets.",
            "distillation_method": "Prompt engineering: decomposition of the generation+annotation task into multiple explicit chat steps so that each subtask is simpler and has explicit outputs (entities, triplets, reasons, support indices). Compared experimentally against single-shot (vanilla) and chain-of-thought prompts.",
            "output_type": "Structured synthetic documents and labels (JSON): title, context paragraph(s), entity list with types, relation triplets, reasoning explanations, support-sentence indices.",
            "output_example": "{\"title\":\"The Godfather\",\"context\":\"...\",\"entities\":[{\"entity\":\"Francis Ford Coppola\",\"type\":\"Person\"}],\"triplets\":[[\"The Godfather\",\"Francis Ford Coppola\",\"director\"]],\"reasons\":[{\"triplet\":\"...\",\"explanation\":\"...\"}],\"support_sentence_index\":1}",
            "evaluation_method": "Ablation/prompt comparison: models were trained on synthetic data produced by vanilla prompt, chain-of-thought prompt, and chain-of-retrieval prompt; downstream performance (F1) measured on ZeroDocRE and ZeroDocRTE.",
            "evaluation_results": "Chain-of-Retrieval synthetic data yielded the best downstream performance: e.g., for ZeroDocRE, Chain-of-Retrieval achieved Test F1 49.21 (Re-DocRED) and 48.30 (DocRED) versus Chain-of-Thought 47.80/43.72 and Vanilla 42.45/34.98; for ZeroDocRTE, Chain-of-Retrieval Test F1 13.23/13.38 (Re-DocRED/DocRED) outperforming other prompts.",
            "strengths": "Decomposes a hard generation+annotation task into manageable steps, producing richer structured outputs (reasoning, support indices) and better downstream utility; increases diversity via temperature control; empirically outperforms simpler prompting variants on downstream tasks.",
            "limitations": "Requires a multi-turn conversational interface and more prompt-engineering; still inherits hallucination/noise from the LLM; generation quality depends on temperature and the LLM's internal knowledge; no automatic guarantee of factual correctness.",
            "failure_cases": "Produces plausible but incorrect triplets and sometimes omits correct triplets; these failure modes motivated the cross-document denoising stage (examples shown in the paper).",
            "uuid": "e9854.1",
            "source_info": {
                "paper_title": "Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "KG-Denoise",
            "name_full": "Consistency-guided Cross-Document Knowledge Denoising",
            "brief_description": "A denoising algorithm introduced in this paper that builds two frequency-weighted cross-document knowledge graphs from (a) original synthetic labels and (b) pseudo labels produced by a pre-denoising model, fuses them, computes per-triplet consistency scores, and prunes triplets below a dynamic threshold to reduce hallucinated facts and recover missing ones.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Graph fusion + frequency-based scoring using pseudo-labels from LLaMA2-13B-Chat",
            "model_description": "Construct KG_s (from synthetic labels T_syn) and KG_p (from pseudo labels T_p) with entities as nodes and relation types as labeled edges weighted by frequency; fuse to KG_f and compute consistency score s_ijk = F^s_{ijk} + F^p_{ijk}; compute a dynamic threshold η_k per unseen relation using mean and variance of consistency scores for that relation; prune triplets with s_ijk &lt; η_k to obtain denoised KG_d and relabel synthetic documents accordingly.",
            "model_size": "13B (LLaMA2-13B-Chat used for pseudo labels)",
            "input_corpus_description": "Synthetic documents and labels generated by ChatGPT, and pseudo labels predicted on those synthetic documents by a pre-denoising LLaMA2-13B-Chat model trained on seen relations (DocRED).",
            "input_corpus_size": null,
            "topic_query_description": "Assess cross-document consistency of relational facts for unseen relation types to identify reliable triplets and prune hallucinations.",
            "distillation_method": "Cross-document aggregation and frequency-based consistency scoring; dynamic thresholding per relation type based on empirical score distribution; prune and relabel synthetic data to produce cleaned training data.",
            "output_type": "Denoised knowledge graph (KG_d) and relabeled synthetic training corpus with unreliable triplets removed and some missing triplets recovered via cross-document evidence.",
            "output_example": "Original synthetic label: (The Godfather, Vito Corleone, cast member) [incorrect] --&gt; pruned; Added/retained: (The Godfather, Francis Ford Coppola, screenwriter) [recovered via cross-document frequency].",
            "evaluation_method": "Compare downstream models trained on original synthetic data vs denoised synthetic data; ablation studies removing pruning or seen-data usage; report F1 changes across multiple backbone models and datasets.",
            "evaluation_results": "Training on denoised synthetic data consistently improved downstream F1 across backbone models. Example: for one backbone (Ours-Roberta-large) +Synthetic Data Test F1 on Re-DocRED = 49.21, +Denoised Synthetic Data = 51.88; the denoising step yields consistent F1 increases (several points) and improved handling of specific unseen relations (figures/tables in paper).",
            "strengths": "Reduces hallucinated relations and recovers missing true relations by exploiting redundancy across many generated documents; empirically improves downstream extraction performance; straightforward, interpretable scoring and pruning based on frequencies.",
            "limitations": "Relies on frequency signals which can be biased (frequent hallucinations could survive); requires a competent pre-denoising model trained on seen relations; dynamic thresholding hyperparameters and statistics depend on number/variety of synthetic examples; may not remove all errors or may remove rare but correct facts.",
            "failure_cases": "Some incorrect synthetic facts persist if they appear frequently across generated documents; rare true facts that appear only once may be pruned if below threshold; examples illustrated in the paper where incorrect or missing relations required further correction.",
            "uuid": "e9854.2",
            "source_info": {
                "paper_title": "Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "ChatGPT (used for synthesis)",
            "name_full": "ChatGPT (OpenAI conversational LLM)",
            "brief_description": "Used as the generative source of synthetic, labeled Wikipedia-style documents, entity annotations, relation triplets, reasoning explanations, and supporting sentence indices via the chain-of-retrieval prompting pipeline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "A commercially available RLHF-refined conversational LLM that was prompted interactively in multiple steps (chain-of-retrieval) to generate long-form synthetic documents and structured annotations; generation hyper-parameter temperature set to 1 to increase diversity.",
            "model_size": null,
            "input_corpus_description": "Prompt inputs consisted of the unseen relation type, the entire relation set R (from DocRED/Re-DocRED), and instructive step templates; ChatGPT produced fictional documents and structured labels in JSON format.",
            "input_corpus_size": null,
            "topic_query_description": "Generate documents that contain at least one instance of a specified unseen relation and related relations, then annotate entities/triplets/reasons/supporting-sentences.",
            "distillation_method": "Prompt-driven synthesis (chain-of-retrieval); ChatGPT is used as a data annotator and generator rather than for direct theory summarization from scholarly articles.",
            "output_type": "Structured synthetic labeled documents (JSON) including title, context, entity list, triplets, reasoning and supporting-sentence indices.",
            "output_example": "See Figure 4 example: {\"title\":\"The Godfather\",\"context\":\"The Godfather is a 1972 American film directed by Francis Ford Coppola...\",\"entities\":[...],\"triplets\":[[\"The Godfather\",\"Mario Puzo\",\"screenwriter\"]],\"reasons\":[...],\"support_sentence_index\":2}",
            "evaluation_method": "ChatGPT was evaluated both as a baseline extractor (direct prompting) and as a generator whose synthetic data was used to train other models; downstream F1 scores (Tables 1 & 2) and comparisons with models trained on synthetic data were used to assess utility.",
            "evaluation_results": "As a direct baseline for relation extraction/triplet extraction ChatGPT achieved nontrivial but lower F1 than GenRDK-trained models (e.g., Table 2: ChatGPT Test F1 for Re-DocRED ZeroDocRE m=5 = 21.7 ± 7.5; GenRDK-trained models achieve ~41.3 ± 8.9). Synthetic data generated by ChatGPT (when denoised and used to fine-tune extractors) produced substantially better downstream performance.",
            "strengths": "Produces coherent long-text and stepwise annotations including reasoning and supporting sentence indices; strong zero-shot generation ability enabling creation of document-level multi-triplet examples.",
            "limitations": "Prone to hallucination (plausible but incorrect facts) which requires denoising; output factuality and diversity depend on prompt design and randomness (temperature); no explicit access to or use of scholarly corpora in this work.",
            "failure_cases": "Hallucinated relations or incorrect entity-relation attributions in synthetic documents (examples shown where incorrect triplets were generated and later pruned).",
            "uuid": "e9854.3",
            "source_info": {
                "paper_title": "Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "LLaMA2-13B-Chat (LoRA fine-tuned)",
            "name_full": "LLaMA2-13B-Chat fine-tuned with Low-Rank Adaptation (LoRA)",
            "brief_description": "Used as the pre-denoising model (trained on seen relations) to generate pseudo-labels for synthetic documents and as the final relation triplet extractor fine-tuned on denoised synthetic data; LoRA applied for parameter-efficient fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA2-13B-Chat",
            "model_description": "Open-source LLaMA2 chat variant with ~13 billion parameters; fine-tuned with LoRA adapters (Low-Rank Adaptation) on seen relation data (DocRED splits) using a random relation-group composition strategy during training; learning rate 1e-6, batch size 20 for triplet extraction experiments as reported.",
            "model_size": "13B",
            "input_corpus_description": "Fine-tuned on seen-relation documents from DocRED/Re-DocRED (D_s) and then fine-tuned on denoised synthetic data (D_syn^-); used in inference to produce pseudo labels for synthetic documents during denoising.",
            "input_corpus_size": null,
            "topic_query_description": "Given a document and a relation group R_j (subset of relations), produce relation triplet annotations (used both for pseudo-labeling and as the learned extractor).",
            "distillation_method": "Parameter-efficient fine-tuning (LoRA) on seen relation groups to build a pre-denoising model; inference on synthetic docs to obtain pseudo labels; final fine-tuning on denoised synthetic data to obtain the extractor.",
            "output_type": "Pseudo-labels for synthetic documents (triplets) and final extractor outputs (predicted triplets on evaluation documents).",
            "output_example": "Pseudo-labels P_i = \\hat{M}(I, d_i^u, R_u) producing structured triplet lists for each synthetic document.",
            "evaluation_method": "Fine-tuned LLaMA2-13B-Chat models evaluated on ZeroDocRE and ZeroDocRTE using F1; compared against other LLM baselines and ablations.",
            "evaluation_results": "LLaMA2-13B-Chat used as backbone yields reasonable baseline numbers (see Tables 1 and 2). Fine-tuning on denoised synthetic data produces stronger performance than on original synthetic data; GenRDK fine-tuned LLaMA2-13B-Chat achieves the best reported extractor performance in this work.",
            "strengths": "Can be fine-tuned with LoRA for efficient adaptation to the extraction task; acts as both a denoiser (via pseudo-labeling) and final predictor within the pipeline; integrates well with the synthetic-data approach.",
            "limitations": "Performance depends on the quality and coverage of synthetic and seen data; requires substantial compute to fine-tune (experiments used multiple high-memory GPUs); initial model alone underperforms ChatGPT when used purely as a zero-shot baseline without fine-tuning on synthetic data.",
            "failure_cases": "Pre-denoising model can produce incorrect pseudo-labels if seen-data coverage is insufficient for certain unseen relation patterns, which can propagate errors into fused KG if not corrected by consistency thresholds.",
            "uuid": "e9854.4",
            "source_info": {
                "paper_title": "Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction",
            "rating": 2
        },
        {
            "paper_title": "Is GPT-3 a Good Data Annotator?",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
            "rating": 1
        },
        {
            "paper_title": "Scaling instruction-finetuned language models",
            "rating": 1
        }
    ],
    "cost": 0.02229525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction</h1>
<p>Qi Sun<br>Nanjing University of Science and Technology<br>China<br>319106003718@njust.edu.cn<br>Rong Tong<br>Singapore Institute of Technology<br>Singapore<br>tong.rong@singaporetech.edu.sg</p>
<p>Kun Huang<br>Nanjing University of Science and Technology<br>China<br>huangkun@njust.edu.cn<br>Xiaocui Yang<br>Northeastern University<br>China<br>yangxiaocui@stumail.neu.edu.cn<br>Kun Zhang<em><br>Nanjing University of Science and Technology<br>China<br>zhangkun@njust.edu.cn<br>Soujanya Poria</em><br>Singapore University of Technology and Design<br>Singapore<br>sporia@sutd.edu.sg</p>
<h2>ABSTRACT</h2>
<p>Document-level Relation Triplet Extraction (DocRTE) is a fundamental task in information systems that aims to simultaneously extract entities with semantic relations from a document. Existing methods heavily rely on a substantial amount of fully labeled data. However, collecting and annotating data for newly emerging relations is time-consuming and labor-intensive. Recent advanced Large Language Models (LLMs), such as ChatGPT and LLaMA, exhibit impressive long-text generation capabilities, inspiring us to explore an alternative approach for obtaining auto-labeled documents with new relations. In this paper, we propose a Zero-shot Document-level Relation Triplet Extraction (ZeroDocRTE) framework, which Generates labeled data by Retrieval and Denoising Knowledge from LLMs, called GenRDK. Specifically, we propose a chain-of-retrieval prompt to guide ChatGPT to generate labeled long-text data step by step. To improve the quality of synthetic data, we propose a denoising strategy based on the consistency of cross-document knowledge. Leveraging our denoised synthetic data, we proceed to fine-tune the LLaMA2-13B-Chat for extracting document-level relation triplets. We perform experiments for both zero-shot document-level relation and triplet extraction on two public datasets. The experimental results illustrate that our GenRDK framework outperforms strong baselines. ${ }^{1}$</p>
<h2>CCS CONCEPTS</h2>
<ul>
<li>Information systems $\rightarrow$ Information retrieval.</li>
</ul>
<h2>KEYWORDS</h2>
<p>Document-level Relation Triplet Extraction, Zero-shot Learning, Knowledge Denoising, Large Language Models, Synthetic Data</p>
<h2>1 INTRODUCTION</h2>
<p>Relation Triplet Extraction (RTE) aims to extract the entity pair and the semantic relation type from the unstructured text, which plays a vital role in various downstream Natural Language Processing (NLP) applications, including knowledge graph construction and information retrieval [15, 22, 29]. Although previous approaches</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Comparison of sentence-level [2] and documentlevel data generated. In sentence-level synthetic data, there exists merely one relation triplet within a sentence. In the case of document-level synthetic data, there are more than 22 relation triplets distributed across different sentences. Entities and relations are marked in different colors.
achieve reasonable performance [25, 34, 37], they heavily rely on the large-scale human-annotated corpus, which is inevitably timeconsuming and labor-intensive. Therefore, recent efforts tend to focus on the Zero-shot Relation Extraction (ZeroRE) [1, 21, 36] and Relation Triplet Extraction (ZeroRTE) [2] tasks.</p>
<p>In the zero-shot scenario, the model needs to generalize to unseen relation types in the absence of available human-annotated training data. To solve this challenge, most of the existing methods attempt to reformulate the ZeroRE task to other tasks, such as the reading comprehension [13], textual entailment [19], and close question answering [9] tasks. Although these approaches show promising performance, they make the unrealistic assumption that the entity pairs are readily accessible. Hence, existing endeavors [2] seek to explore the ZeroRTE task by generating synthetic data based on descriptions of previously unseen relation types.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The original and denoised labels of a synthetic sample. Two main types of noise are reduced by our consistencyguided cross-document knowledge denoising strategy. One is reducing the incorrect triplet as shown in the red dotted line (The Godfather, Vito Corleone, cast member), and another is adding the missing triplet as shown in the green solid line (The Godfather, Francis Ford Coppola, screenwriter).</p>
<p>However, the methods mentioned above primarily concentrate on sentence-level ZeroRE and ZeroRTE tasks, assuming that the entities and relations are confined within a single sentence. In practice, numerous valuable relational facts are expressed across multiple sentences, which cannot be extracted using the aforementioned zero-shot approaches. Therefore, we introduce a Zero-shot Document-level Relation Triplet Extraction task (ZeroDocRTE), which aims to extract relation triplets with unseen relation types in a whole document, formed as: (head entity, tail entity, and unseen relation type). In contrast to sentence-level ZeroRTE, ZeroDocRTE is more challenging due to the intricate semantic contexts and discourse structures of the document. Inspired by the impressive long-text generation capabilities of recent advanced Large language models (LLMs), such as ChatGPT and LLaMA, we leverage existing LLMs to obtain auto-labeled documents with new relations. Different from sentence-level synthetic data generation [2], documentlevel synthetic data need to contain relation triplets spanning multiple sentences, which can be seen in Figure 1.</p>
<p>To address this task, we propose a ZeroDocRTE framework, which Generates labeled data by Retrieval and Denoising Knowledge from LLMs, called GenRDK. Specifically, we propose a chain-ofretrieval prompt to guide ChatGPT to generate labeled long-text data step by step. While we can automatically generate a wide range of synthetic data, the process inevitably introduces noisy labels. As seen in Figure 2, there are many incorrect relational facts in synthetic data due to the hallucination problem [5] of LLMs. Therefore, to mitigate false labels of synthetic data, we propose a consistency-guided cross-document knowledge denoising strategy. First, a pre-denoising DocRTE model is trained with seen relation data to obtain pseudo labels of synthetic data. Next, we construct cross-document knowledge graphs according to the pseudo labels and original labels of synthetic data. By observing that the same relational fact can be expressed in different forms across different synthetic documents, we calculate consistency scores to evaluate the reliability of relational facts. Last, we prune unreliable relational facts and relabel the synthetic data. As seen in Figure 2, the missing relation triplet can be added by cross-document knowledge, and the incorrect relation triplet can be reduced by consistency scores. We proceed to fine-tune the LLaMA2-13B-Chat by our denoised synthetic data for extracting document-level relation triplet.</p>
<p>The main contributions of our work are summarized as follows:</p>
<ul>
<li>We explore a challenging Zero-shot Document-level Relation Triplet Extraction (ZeroDocRTE) task and propose a novel framework that generates synthetic data by retrieving and denoising the implicit knowledge from LLMs.</li>
<li>We propose a chain-of-retrieval prompt for guiding ChatGPT to generate documents that contain intricate semantic contexts and various relation triplets step by step.</li>
<li>We propose a consistency-guided cross-document knowledge denoising strategy aimed at enhancing the quality of synthetic data through the reduction of unreliable relational facts and the addition of missing relational facts.</li>
<li>We perform our framework on zero-shot document-level relation and triplet extraction tasks. The experimental results illustrate that our GenRDKachieves significant performance improvements over competitive baselines.</li>
</ul>
<h2>2 RELATED WORK</h2>
<p>Sentence-level Relation Triplet Extraction. Sentence-level RTE aims to extract the entities and relations from a single sentence simultaneously. Conventional works mainly focus on supervised relation triplet extraction [25, 34, 37]. Although these models achieve great success in the sentence-level RTE task, they heavily rely on the large-scale corpus that needs cumbersome data cleaning and timeconsuming labeling. Moreover, in realistic scenarios, there might be relation types that do not have training data, yet are shown in the inference process, called unseen relation types. To solve this issue, recent research efforts have sought the Zero-shot Relation Extraction (ZeroRE) task that aims to classify the unseen relation type between the given entity pair in a sentence [1, 13, 19, 21, 36]. Nevertheless, these approaches assume the availability of groundtruth head and tail entities within a sentence, which is not always satisfied in the application. Thus, scholars [2] first propose the zeroshot setting for the RTE by using synthetic examples. However, the aforementioned techniques primarily concentrate on ZeroRE and ZeroRTE tasks at the sentence level, posing challenges for their direct application to zero-shot document-level relation and triplet extraction tasks.</p>
<p>Document-level Relation Extraction. Existing approaches mainly focus on the Document-level Relation Extraction (DocRE) task, which employs the transformer-based [12, 14, 31, 38] and the graph-based [3, 7, 18, 20, 24, 27, 28, 32, 33] models to extract contextual and non-local structural information for aggregating entity representations [12, 14, 31, 38]. While these models have achieved remarkable success in the task of DocRE, they necessitate prior knowledge in the form of ground-truth entity positions. Then, recent works attempt to extract entities and relations jointly in an end-to-end manner [8, 35]. However, the aforementioned methods depend on extensive supervised data and do not apply</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The overview of our GenRDK framework. It contains four key parts as follows: (a) Chain-of-retrieval prompt for guiding ChatGPT to generate labeled data step by step; (b) Training the pre-denoising model based on LLaMA2-13B-Chat with LoRA; (c) Consistency-guided cross-document knowledge denoising strategy. (d) Training the relation triplet extractor with the denoised synthetic data.
to ZeroDocRTE and ZeroDocRE tasks. To solve these challenging settings, we propose a novel framework, which synthesizes documents and labels by retrieving the latent knowledge of ChatGPT. To mitigate the issue of noise during the generation process, we introduce a consistency-guided knowledge denoising strategy, which can further improve the quality of synthetic data.</p>
<h2>3 METHODOLOGY</h2>
<p>In this section, we introduce our proposed framework in detail. As shown in Figure 3, our GenRDKcontains four key steps: (1) Chain-of-retrieval prompt to generate labeled data; (2) Training a predenoising model to obtain pseudo labels; (3) Consistency-guided cross-document knowledge denoising; (4) Training the relation triplet extractor.</p>
<h3>3.1 Problem Formulation</h3>
<p>Given a dataset $D=D_{s} \cup D_{u}$ with a set of pre-defined relation types $R=R_{s} \cup R_{u}, R_{s} \cap R_{u}=\emptyset . D_{s}$ is a seen dataset with only seen relation type sets $R_{s}, D_{u}$ is an unseen dataset with both seen $R_{s}$ and unseen relation type sets $R_{u}$. Given a document $d_{i} \in D_{u}$, the zero-shot document-level relation triplet extraction aims to extract relation triplets with unseen relation types, formulated as $\left{\left(e_{s}, e_{o}, r_{k}\right) \mid e_{s}, e_{o} \in E_{i}, r_{k} \in R_{u}\right}$, where $R_{u}$ is the set of unseen relation types, $e_{s}$ is the head entity, $e_{o}$ is the tail entity, $E_{i}$ is the set of entities of document $d_{i}$.</p>
<h3>3.2 Chain-of-Retrieval Prompt</h3>
<p>Large Language Models (LLMs) have shown powerful zero-shot generalization ability in various NLP applications, which benefit from large-scale pre-training. Recent approaches [2, 6] exploit the implicit knowledge of LLMs to generate the synthetic data for the
downstream tasks, formulated as follows:</p>
<p>$$
\left[s_{i}, y_{i}\right]=L L M\left(q_{i}\right)
$$</p>
<p>where $q_{i}$ is the query input sequence, $s_{i}$ and $y_{i}$ is the sentence and label generated by the large language model.</p>
<p>These methods mainly focus on generating sentence-level data that usually have a single semantic structure[2, 6]. However, synthetic data for document-level relation triplet extraction usually contain complex semantic structures and various relation triplets. Therefore, we propose a chain-of-retrieval prompt that partitions the complex generation problem into a sequence of simple questions, which can be seen in Figure 4. The process of generating synthetic data is as follows:</p>
<ul>
<li>For each unseen relation type $r_{i} \in R_{u}$, we prompt ChatGPT to select several relations $\left{r_{i j}\right}<em i="i">{j=1}^{n</em>$ is the number of selected relations.}}$ that most related to the unseen relation type $r_{i}$ from the relation set $R$, where $n_{i</li>
<li>We guide ChatGPT to generate a fictional document $d_{i k}$ that contains the unseen relation type $r_{i}$ and related relations $\left{r_{i j}\right}<em i="i">{j=1}^{n</em>$. To enhance the diversity of the generated document, we set the hyper-parameter temperature of ChatGPT to 1 in this step.}}$, where $k$ is the index of document for unseen relation type $r_{i</li>
<li>Corresponding to the generated document $d_{i k}$, we prompt ChatGPT to extract the entity set $E_{k}$ with the pre-defined entity types.</li>
<li>We prompt ChatGPT to extract all types of relation triplets $\left{\left(e_{s}, e_{o}, r_{l}\right) \mid e_{s}, e_{o} \in E_{k}, r_{l} \in R\right}$ based on the above document $d_{k}$ and entity set $E_{k}$.</li>
<li>After obtaining relation triplets and documents, we prompt ChatGPT to present the reasoning explanation of each relation triplet, formulated as $\left(e_{s}, e_{o}, r_{l}, a_{c}\right)$.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;">Step</th>
<th style="text-align: center;">Prompt Template</th>
<th style="text-align: center;">Output of ChatGPT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Select relation types that most related to the relation type <Unseen Relation Type> from the following relation sets <All Relation Types>.</td>
<td style="text-align: center;"><Related Relations>: <br> 1. director: Screenwriters often collaborate closely with directors in the filmmaking process, as directors bring the script to life on the screen. This relationship is essential in the film industry. <br> 2. producer: Producers work closely with screenwriters to develop and ... <br> 3. cast member: Screenwriters may have interactions ...</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"><Unseen Relation Type>: screenwriter</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"><All Relation Types>: head of government, country, place of birth, place of death, father, mother, spouse, country of citizenship, continent, instance of, ...</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Generate one fictional Wikipedia style paragraph that contains at least 6 sentences and describes one or more following relation types: <Unseen Relation Type> and <Related Relations>. Provide them in JSON format with just following keys: title, context.</td>
<td style="text-align: center;"><Synthetic Document>: <br> ("title": "The Godfather", <br> "context": "The Godfather is a 1972 American film directed by Francis Ford Coppola. The screenplay was written by Coppola and Mario Puzo, Puzo, based on Puzo' s best-selling novel of the same name. The film stars Marlon Brando as the patriarch of the ... ")</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Extract the entities in your above generated document. Provide them in List of JSON format with the following keys: entity, entity type. The entity type can be one of the following types: <Entity Types>. <br> <Entity Types>: Organization, Location, Time, Person, Miscellaneous, Number Blank.</td>
<td style="text-align: center;"><Entities>: <br> [["entity": "The Godfather" "entity type": "Miscellaneous"], <br> {"entity": "Francis Ford Coppola" "entity type": "Person"}, <br> {"entity": "Mario Puzo" "entity type": "Person"}, <br> {"entity": "Marlon Brando" "entity type": "Person"}, <br> {"entity": "Al Pacino" "entity type": "Person"}, <br> {"entity": "James Caan" "entity type": "Person"}, ...])</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Present the relation triplets as: {head entity, tail entity, relation type}. The relation type can be one or more of following relation types: <All Relation Types>.</td>
<td style="text-align: center;"><Triplets>: <br> 1. (The Godfather, Francis Ford Coppola, director) <br> 2. (The Godfather, Mario Puzo, screenwriter) <br> 3. (The Godfather, Marlon Brando, cast member) ...</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">According to the above information, present the reasoning explanation of each relational triplet.</td>
<td style="text-align: center;"><Reasons>: <br> 1. (The Godfather, Mario Puzo, screenwriter) <br> Explanation: Mario Puzo and Francis Ford Coppola wrote ...</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Present support sentence index for each extracted relation triple that shown in the generated document.</td>
<td style="text-align: center;"><Support Sentence>: <br> 1. (The Godfather, Mario Puzo, screenwriter) <br> Support Sentence Index: Sentence 2 ...</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">Organize the above triplet information in the List of JSON format with the following keys: head entity, tail entity, relation type, reasoning explanation of each relation triplet, index of supporting sentence that shown in document.</td>
<td style="text-align: center;"><Synthetic Labels>: <br> [["head entity": "The Godfather", "tail entity": "Mario Puzo", <br> "relation type": "screenwriter", "reasoning explanation": "Mario Puzo was another co-writer of the screenplay for ...", "index of supporting sentence": 2 ], ... ])</td>
</tr>
</tbody>
</table>
<p>Figure 4: A sample of the proposed chain-of-retrieval. The generation procedure is a chatting process, which means each step contains the memory of the previous steps.</p>
<ul>
<li>We then prompt ChatGPT to present the support sentences shown in the generated document $d_{i}$, which can be formulated as $\left(e_{s}, e_{o}, r_{l}, h_{p}\right)$.</li>
<li>We guide ChatGPT to generate the final structured labels based on all the above information.</li>
</ul>
<h3>3.3 Pre-denoising Model</h3>
<p>Despite the ChatGPT ${ }^{2}$ can generate promising synthetic data, it can also produce plausible yet incorrect factual information, which is called hallucination in LLMs [5]. Thus, to further improve the quality of synthetic data, we train a pre-denoising model by data with seen relations to generate pseudo labels.</p>
<p>As shown in Figure 3 (b), we leverage the seen dataset $D_{s}$ to finetune the LLaMA2-13B-Chat ${ }^{3}$ with Low-Rank Adaptation (LoRA) [11], which approximates the weight update by inserting trainable low-rank metrics into transformer layers [10]. During the finetuning process, we introduce a random combination strategy to dynamically compose the relation set. In this way, we can enhance the diversity of training data. Specifically, we partition the seen relation set $R_{s}$ into multiple relation groups. This partition can be</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>expressed as:</p>
<p>$$
R_{s}=\left[R_{1}, R_{2}, \ldots, R_{j}, \ldots, R_{m}\right]
$$</p>
<p>where $m$ is the number of relation groups. We take each relation group $R_{j}=\left{r_{i k}\right}_{k=1}^{s}$ as the input along with the document content. The fine-tuning process of each sample can be expressed as:</p>
<p>$$
\hat{M} \leftarrow \operatorname{Train}\left(M, I, d_{i}^{s}, R_{j}, T_{i j}^{*}\right)
$$</p>
<p>where $M$ denotes the backbone model, $I$ is the task description of DocRTE task, $d_{i}^{s}$ is the $i$-th document in seen relation dataset $D_{s}$, $R_{j}$ is the $j$-th relation group, $T_{i j}$ represents the relation triplets of $j$-th relation group in the $i$-th document, $\hat{M}$ is the fine-tuned model. To obtain the pseudo labels, we perform inference on synthetic data with our pre-denoising model, formulated as follows:</p>
<p>$$
P_{i}=\hat{M}\left(I, d_{i}^{u}, R_{u}\right)
$$</p>
<p>where $\hat{M}$ is the pre-denoising model, $d_{i}^{u}$ is the $i$-th document in unseen dataset $D_{u}, R_{u}$ is the unseen relation set, $P_{i}$ is the pseudo labels of the document $d_{i}$.</p>
<h3>3.4 Consistency guided Knowledge Denoising</h3>
<p>We observe that different documents in synthetic data might be generated by the same relation fact, as shown in Figure 5. Inspired by this phenomenon, we attempt to supplement the losing positive</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: An example of the knowledge expressed by different generated documents. The relation triplets (The Godfather, Francis Ford Coppola, director) and (The Godfather, Mario Puzo, screenwriter) are multiply expressed in different synthetic documents.
relational fact in a single document with cross-document knowledge. Therefore, we propose a consistency-guided cross-document knowledge denoising strategy.</p>
<p>We aim to construct two knowledge graphs $K G_{s}$ and $K G_{p}$ according to the relational facts in pseudo labels and synthetic labels across documents. We take entities as nodes, relation types as edges, and frequencies of relation triplet as weights. Then, we fuse the above two knowledge graphs and calculate a consistency score of each relation triplet by its frequency in the two knowledge graphs, which can be formulated as follows:</p>
<p>$$
s_{i j k}=F_{i j k}^{s}+F_{i j k}^{p}
$$</p>
<p>where, $F_{i j k}^{s}, F_{i j k}^{p}$ is the frequency of relation triplet $\left(e_{i}, e_{j}, r_{k}\right)$ for knowledge graphs $K G_{s}$ and $K G_{p}$. By further considering wrong relational facts that might be introduced in the fused knowledge graph, we prune the fused knowledge graph $K G_{f}$ by consistency scores of relation triplets.</p>
<p>Since frequencies of relation types are varied, we construct a dynamic threshold $\eta_{k}$ for each unseen relation $r_{k}$ to filter unreliable triplets, formulated as follows:</p>
<p>$$
\eta_{k}=\overline{s_{i j k}}-\sqrt{\frac{1}{N_{k}^{\eta}}-1} \sum_{l=1}^{N_{k}^{\eta}}\left(s_{i j k}-\overline{s_{i j k}}\right)^{2}
$$</p>
<p>where $s_{i j k}$ is the consistency score of the relation triplet $\left(e_{i}, e_{j}, r_{k}\right)$. $\overline{s_{i j k}}=\frac{1}{N_{k}^{\eta}} \sum_{l=1}^{N_{k}^{\eta}} s_{i j k}$ is the average of consistency scores of relation triplets with the relation type $r_{k} . N_{k}^{\eta}$ is the quantity of triplets that belong to unseen relation type $k$.</p>
<p>In our pruning strategy, we remove the relation triplet $\left(e_{i}, e_{j}, r_{k}\right)$ if its consistency score $s_{i j k}$ is lower than its threshold $\eta_{k}$. In this way, we can maintain useful knowledge and reduce the incorrect relational facts in the fused knowledge graph. We re-label the synthetic data with the denoised knowledge graph $K G_{d}$. Meanwhile, we also filter synthetic data that lacks valuable unseen relation triplets during the re-labeling process.</p>
<h3>3.5 Relation Triplet Extractor</h3>
<p>With the denoised synthetic data $D_{s y n}^{-}$, we train a relation triplet extractor by fine-tuning the generative language model LLaMA2-13B-Chat. The training process can be expressed as follows:</p>
<p>$$
\hat{M} \leftarrow \operatorname{Train}\left(M, I, \hat{d}<em u="u">{i}^{s y n}, R</em>\right)
$$}, \hat{T}_{i}^{s y n</p>
<p>where $M$ denotes the backbone model. $I$ is the task description of the DocRTE task. $\hat{d}<em n="n" s="s" y="y">{i}^{s y n}$ is the $i$-th document in denoised synthetic datset
$D</em>$ is the documentlevel relation triplet extraction model. We summarize the training procedure of the proposed framework GenRDKin Algorithm 1.}^{-}, R_{u}$ is the unseen relation set, $\hat{T}_{i}^{s y n}$ represents the denoised relation triplets of the $i$-th synthetic document, $\hat{M</p>
<div class="codehilite"><pre><span></span><code>Algorithm<span class="w"> </span>1<span class="w"> </span>GenRDK<span class="w"> </span>Training<span class="w"> </span>Procedure
<span class="w">    </span>Define:<span class="w"> </span>Seen<span class="w"> </span>data<span class="w"> </span>\(D<span class="w"> </span>A_{s}\),<span class="w"> </span>triplets<span class="w"> </span>\(T_{s}\),<span class="w"> </span>and<span class="w"> </span>relation<span class="w"> </span>type<span class="w"> </span>set<span class="w"> </span>\(R_{s}\).
<span class="w">    </span>Unseen<span class="w"> </span>data<span class="w"> </span>\(D<span class="w"> </span>A_{u}\),<span class="w"> </span>triplets<span class="w"> </span>\(T_{u}\),<span class="w"> </span>and<span class="w"> </span>relation<span class="w"> </span>type<span class="w"> </span>set<span class="w"> </span>\(R_{u}\),<span class="w"> </span>Original
<span class="w">    </span>synthetic<span class="w"> </span>data<span class="w"> </span>\(D<span class="w"> </span>A_{s<span class="w"> </span>y<span class="w"> </span>n}\)<span class="w"> </span>and<span class="w"> </span>triplets<span class="w"> </span>\(T_{s<span class="w"> </span>y<span class="w"> </span>n}\),<span class="w"> </span>Denoised<span class="w"> </span>synthetic<span class="w"> </span>data
<span class="w">    </span>\(D<span class="w"> </span>A_{s<span class="w"> </span>y<span class="w"> </span>n}\)<span class="w"> </span>and<span class="w"> </span>triplets<span class="w"> </span>\(\hat{T}_{s<span class="w"> </span>y<span class="w"> </span>n}\),<span class="w"> </span>Pseudo<span class="w"> </span>relation<span class="w"> </span>triplets:<span class="w"> </span>\(T_{p}\),<span class="w"> </span>Knowledge
<span class="w">    </span>graph:<span class="w"> </span>\(K<span class="w"> </span>G\),<span class="w"> </span>Backbone<span class="w"> </span>model:<span class="w"> </span>\(M\),<span class="w"> </span>Chain-of-retrieval<span class="w"> </span>prompt:<span class="w"> </span>\(C<span class="w"> </span>o<span class="w"> </span>R\),
<span class="w">        </span>Predict<span class="w"> </span>relation<span class="w"> </span>triplets:<span class="w"> </span>TR.
</code></pre></div>

<p>Require: $D_{s}, R, R_{s}, R_{u}$.
Ensure: $R_{s} \cap R_{u}=\emptyset$.</p>
<ol>
<li>$D_{s y n} \leftarrow C o R\left(C h a t G P T, R_{u}, R\right)$</li>
<li>$\hat{M}<em s="s">{\text {pre-denoising }} \leftarrow \operatorname{Train}\left(M, D A</em>\right)$}, T_{s}, R_{s</li>
<li>$T_{p} \leftarrow \operatorname{Predict}\left(\hat{M}, D_{s y n}, T_{s y n}, R_{u}\right)$</li>
<li>$K G_{s} \leftarrow T_{s y n}$</li>
<li>$K G_{p} \leftarrow T_{p}$</li>
<li>$K G_{f} \leftarrow \operatorname{Fusion}\left(K G_{s}, K G_{p}\right)$</li>
<li>$K G_{d} \leftarrow \operatorname{Prune}\left(K G_{f}\right)$</li>
<li>$D A_{s y n}, T_{s y n} \leftarrow \operatorname{Denoise}\left(K G_{d}, D_{s y n}, T_{s y n}\right)$</li>
<li>$\hat{M}<em n="n" s="s" y="y">{\text {ZeroDocRTE }} \leftarrow \operatorname{Train}\left(M, D A</em>\right)$}, T_{s y n}, R_{u</li>
<li>$T R \leftarrow \operatorname{Predict}\left(\hat{M}<em u="u">{\text {ZeroDocRTE }}, D A</em>\right)$
return $T R$}, R_{u</li>
</ol>
<h2>4 EXPERIMENTS</h2>
<h3>4.1 Datasets and Settings</h3>
<p>We evaluate our framework on both zero-shot document-level relation and triplet extraction tasks with two public datasets. DocRED [31] is a popular large-scale human-annotated document-level relation extraction dataset with 96 pre-defined relation types, which is constructed from Wikipedia and Wikidata. Re-DocRED [26] is a revised version of DocRED by supplementing positive instances that are ignored in the DocRED dataset. We follow the previous zero-shot setting [2] that partitions the pre-defined relation types into a seen relation set and an unseen relation set. Only documents with labels of the seen set are available for training while documents that contain the unseen set are used for evaluation. The unseen relations are randomly selected from the relation types in datasets. For a fair comparison, we evaluate models under different sizes $m \in{5,10}$ of unseen relation sets and randomly sample three times for each size to obtain different unseen relation sets.</p>
<p>The synthetic data generated by our proposed GenRDK can be used for both zero-shot document-level relation and triplet extraction tasks as we generate the whole document, entities, and triplets. Therefore, to illustrate the effectiveness of our framework, we conduct extensive experiments on both zero-shot documentlevel relation and triplet extraction tasks.</p>
<p>Relation Triplet Extraction. We adopt LLaMA2-13B-Chat as the backbone model. We use LoRA [11] which is a popular parameter-efficient fine-tuning method to fine-tune the LLaMA2-13B-Chat. We set the learning rate to 1e-6. The batch size is 20 . The experiments are conducted on four NVIDIA RTX A6000-48G GPUs.</p>
<p>Table 1: Experimental results on two public datasets for Zero-shot Document-level Relation Triplet Extraction (ZeroDocRTE). The CoR means the model trained by original synthetic data without our consistency-guided denoising strategy.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Re-DocRED</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DocRED</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{m = 5}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{m = 1 0}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{m = 5}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{m = 1 0}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA2-7B</td>
<td style="text-align: center;">$2.4 \pm 1.9$</td>
<td style="text-align: center;">$2.7 \pm 1.9$</td>
<td style="text-align: center;">$1.2 \pm 0.9$</td>
<td style="text-align: center;">$1.3 \pm 0.8$</td>
<td style="text-align: center;">$2.3 \pm 1.6$</td>
<td style="text-align: center;">$2.9 \pm 2.3$</td>
<td style="text-align: center;">$1.2 \pm 1.0$</td>
<td style="text-align: center;">$1.4 \pm 1.0$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA2-7B-Chat</td>
<td style="text-align: center;">$4.9 \pm 2.4$</td>
<td style="text-align: center;">$5.0 \pm 3.0$</td>
<td style="text-align: center;">$3.6 \pm 1.6$</td>
<td style="text-align: center;">$3.8 \pm 1.8$</td>
<td style="text-align: center;">$4.8 \pm 3.0$</td>
<td style="text-align: center;">$5.0 \pm 3.2$</td>
<td style="text-align: center;">$4.3 \pm 2.0$</td>
<td style="text-align: center;">$4.6 \pm 2.3$</td>
</tr>
<tr>
<td style="text-align: center;">Flan-T5-XXL</td>
<td style="text-align: center;">$5.3 \pm 1.4$</td>
<td style="text-align: center;">$4.8 \pm 1.9$</td>
<td style="text-align: center;">$4.1 \pm 1.4$</td>
<td style="text-align: center;">$3.7 \pm 1.0$</td>
<td style="text-align: center;">$5.4 \pm 2.3$</td>
<td style="text-align: center;">$6.0 \pm 1.7$</td>
<td style="text-align: center;">$4.2 \pm 1.1$</td>
<td style="text-align: center;">$4.5 \pm 1.6$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA2-13B</td>
<td style="text-align: center;">$7.2 \pm 2.3$</td>
<td style="text-align: center;">$7.1 \pm 2.6$</td>
<td style="text-align: center;">$3.5 \pm 0.6$</td>
<td style="text-align: center;">$3.1 \pm 3.1$</td>
<td style="text-align: center;">$8.3 \pm 2.2$</td>
<td style="text-align: center;">$8.1 \pm 2.3$</td>
<td style="text-align: center;">$3.6 \pm 0.6$</td>
<td style="text-align: center;">$3.8 \pm 0.9$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA2-13B-Chat</td>
<td style="text-align: center;">$8.1 \pm 1.6$</td>
<td style="text-align: center;">$8.7 \pm 3.0$</td>
<td style="text-align: center;">$5.0 \pm 1.0$</td>
<td style="text-align: center;">$5.2 \pm 0.8$</td>
<td style="text-align: center;">$9.4 \pm 2.0$</td>
<td style="text-align: center;">$9.0 \pm 1.8$</td>
<td style="text-align: center;">$5.6 \pm 1.1$</td>
<td style="text-align: center;">$5.5 \pm 0.8$</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">$11.2 \pm 4.4$</td>
<td style="text-align: center;">$11.8 \pm 3.8$</td>
<td style="text-align: center;">$7.5 \pm 0.9$</td>
<td style="text-align: center;">$8.1 \pm 1.5$</td>
<td style="text-align: center;">$14.7 \pm 8.1$</td>
<td style="text-align: center;">$11.2 \pm 5.1$</td>
<td style="text-align: center;">$8.5 \pm 1.9$</td>
<td style="text-align: center;">$8.9 \pm 2.3$</td>
</tr>
<tr>
<td style="text-align: center;">Our Methods</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CoR</td>
<td style="text-align: center;">$11.0 \pm 0.7$</td>
<td style="text-align: center;">$11.4 \pm 2.3$</td>
<td style="text-align: center;">$6.6 \pm 0.8$</td>
<td style="text-align: center;">$6.6 \pm 1.1$</td>
<td style="text-align: center;">$13.1 \pm 0.9$</td>
<td style="text-align: center;">$12.1 \pm 1.0$</td>
<td style="text-align: center;">$7.6 \pm 1.4$</td>
<td style="text-align: center;">$7.1 \pm 0.6$</td>
</tr>
<tr>
<td style="text-align: center;">GenRDK</td>
<td style="text-align: center;">$\mathbf{1 3 . 3} \pm \mathbf{1 . 2}$</td>
<td style="text-align: center;">$\mathbf{1 3 . 1} \pm \mathbf{2 . 6}$</td>
<td style="text-align: center;">$\mathbf{8 . 2} \pm \mathbf{1 . 5}$</td>
<td style="text-align: center;">$\mathbf{8 . 2} \pm \mathbf{0 . 6}$</td>
<td style="text-align: center;">$\mathbf{1 5 . 2} \pm \mathbf{0 . 7}$</td>
<td style="text-align: center;">$\mathbf{1 4 . 2} \pm \mathbf{1 . 3}$</td>
<td style="text-align: center;">$\mathbf{9 . 2} \pm \mathbf{1 . 4}$</td>
<td style="text-align: center;">$\mathbf{9 . 4} \pm \mathbf{0 . 6}$</td>
</tr>
</tbody>
</table>
<p>Relation Extraction. We adopt the graph-based DocRE model [23] as the backbone model, and apply RoBERTa ${ }<em 1="1">{\text {large }}$ [16] as the context encoder. We use AdamW [17] as the optimizer. We set the learning rate to $3 \mathrm{e}-5$. We apply warmup for the initial $6 \%$ steps. The batch size is 8 for both the training and test process. The experiments are conducted on a single NVIDIA RTX A6000-48G GPU. For both ZeroDocRTE and ZeroDocRE tasks, we use the $F</em>$ as the evaluation metric to evaluate the performance of our framework on unseen relation types.</p>
<h3>4.2 Baseline Methods</h3>
<p>As zero-shot document-level relation and triplet extraction tasks are new task settings, we evaluate the performance of several popular LLMs on the above two task settings as benchmarks. Baseline methods includes LLaMA2-7B, LLaMA2-13B, LLaMA2-7B-Chat, LLaMA2-13B-Chat, Flan-T5-XXL, and ChatGPT. Llama2 is an opensource LLM released by Meta, which is pretrained on publicly available online data sources. Llama2-Chat is the fine-tuned model that leverages reinforcement learning with human feedback. We evaluate the 7B and 13B versions for Llama2 and Llama2-Chat. Flan-T5 [4] is an encoder-decoder LLM released by Google, which is pre-trained on more than 1,800 language tasks. We evaluate the popular Flan-T5 XXL as the benchmark. ChatGPT is a powerful large language model based on reinforcement learning with human feedback released by OpenAI, which shows great ability in various NLP tasks.</p>
<h3>4.3 Experimental Results</h3>
<p>We compare our GenRDK framework with the above baselines for both ZeroDocRTE and ZeroDocRE tasks. The experimental results illustrate that our framework achieves significant performance improvement over the competitive baselines on two public datasets.</p>
<p>Relation Triplet Extraction As shown in Table 1, our framework GenRDK outperforms the previous baselines on both REDocRED and DocRED datasets. Specifically, when there are 5 different unseen relation types, our GenRDK achieves $13.1 \pm 2.6 F_{1}$ and $14.2 \pm 1.3 F_{1}$ on the test set of RE-DocRED and DocRED datasets, respectively. When the number of unseen relation types increases</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Vanilla Prompt</th>
<th style="text-align: center;">Chain-of-Though Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Generate one fictional <br> wikipedia style paragraph <br> that contains at least 6 <br> sentences and describes <br> the relation type: <br> «Unseen Relation Type». <br> Extract the possible <br> relation triplets with the <br> following relation types: &lt;All Relation Types». <br> Outputs: #1. Title. #2. <br> Generated paragraph. #3. <br> Relational facts in JSON <br> List format with following <br> keys: head entity, tail <br> entity, relation type, <br> head entity type, tail <br> entity type.</td>
<td style="text-align: center;">Perform the following instructions step by step: <br> Step one: Generate one fictional wikipedia style <br> paragraph that contains at least 6 sentences and <br> describes the relation type «Unseen Relation <br> Type». <br> Step two: Extract the entities in your above <br> generated document. The entity type can be one <br> of the following types: "Organization", "Location", <br> "Time", "Person", "Miscellaneous", "Number", <br> "Blank". <br> Step three: Extract the possible relation types <br> between the entity pair that exists one or more <br> relation types of the following relation types: &lt;All <br> Relation Types». <br> Outputs: #1. Title. #2. Generated paragraph. #3. <br> Relational facts in JSON List format with following <br> keys: head entity, tail entity, relation type, head <br> entity type, tail entity type.</td>
</tr>
</tbody>
</table>
<p>«Unseen Relation Type»: point in time, league, educated at, platform, child.
«All Relation Types»: head of government, country, place of birth, place of death, father, mother, spouse, country of citizenship, continent, instance of, ...</p>
<p>Figure 6: Illustration of vanilla and chain-of-thought prompt. Our chain-of-retrieval prompt can be seen in Figure 4. We generate different groups of data by the above prompts.
to 10 , our GenRDK achieves $8.2 \pm 0.6 F_{1}$ and $9.4 \pm 0.6 F_{1}$ on the test set of RE-DocRED and DocRED datasets, respectively. We can observe that our model trained by original synthetic data outperforms the baseline model LLaMA2-13B-Chat by around $2.7 F_{1}$ and $3.1 F_{1}$ on the test set of RE-DocRED and DocRED datasets when $m=5$. This suggests that our chain-of-retrieval prompt can effectively generate documents that contain unseen relational facts. Moreover, the performance of the model trained on denoised synthetic data improves by around $1.7 F_{1}$ and $2.1 F_{1}$ on the test set of RE-DocRED and DocRED datasets. This suggests the effectiveness of our consistency-guided cross-document knowledge denoising strategy.</p>
<p>Relation Extraction Our chain-of-retrieval prompt enables LLMs to generate the whole document, entities, and relation triplets. Therefore, to further illustrate the effectiveness of our framework, we perform extensive experiments on document-level zero-shot relation extraction tasks. As shown in table 2, our GenRDK achieves $41.3 \pm 8.9 F_{1}$ and $41.5 \pm 8.7 F_{1}$ on the test set of RE-DocRED and</p>
<p>Table 2: We present experimental results for Zero-shot Document-level Relation Extraction (ZeroDocRE) on two public datasets: RE-DocRED and DocRED. The CoR is trained by original synthetic data without our consistency-guided denoising strategy. The GenRDK is trained by our denoised synthetic data.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Re-DocRED</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DocRED</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{m = 5}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{m = 1 0}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{m = 5}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{m = 1 0}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
</tr>
<tr>
<td style="text-align: center;">Flan-T5-XXL</td>
<td style="text-align: center;">$4.5 \pm 2.2$</td>
<td style="text-align: center;">$3.1 \pm 2.4$</td>
<td style="text-align: center;">$1.6 \pm 0.6$</td>
<td style="text-align: center;">$1.8 \pm 0.9$</td>
<td style="text-align: center;">$4.0 \pm 2.5$</td>
<td style="text-align: center;">$3.9 \pm 2.3$</td>
<td style="text-align: center;">$2.1 \pm 0.8$</td>
<td style="text-align: center;">$1.9 \pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA2-7B-Chat</td>
<td style="text-align: center;">$4.9 \pm 2.0$</td>
<td style="text-align: center;">$4.8 \pm 1.7$</td>
<td style="text-align: center;">$3.1 \pm 1.4$</td>
<td style="text-align: center;">$3.0 \pm 1.0$</td>
<td style="text-align: center;">$5.8 \pm 3.2$</td>
<td style="text-align: center;">$4.5 \pm 2.1$</td>
<td style="text-align: center;">$3.7 \pm 1.9$</td>
<td style="text-align: center;">$3.6 \pm 2.1$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA2-13B-Chat</td>
<td style="text-align: center;">$12.2 \pm 2.0$</td>
<td style="text-align: center;">$12.8 \pm 2.1$</td>
<td style="text-align: center;">$8.7 \pm 0.9$</td>
<td style="text-align: center;">$8.5 \pm 0.9$</td>
<td style="text-align: center;">$12.5 \pm 2.2$</td>
<td style="text-align: center;">$12.8 \pm 2.3$</td>
<td style="text-align: center;">$9.5 \pm 0.6$</td>
<td style="text-align: center;">$9.6 \pm 0.5$</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">$20.6 \pm 7.2$</td>
<td style="text-align: center;">$21.7 \pm 7.5$</td>
<td style="text-align: center;">$13.7 \pm 2.3$</td>
<td style="text-align: center;">$13.0 \pm 1.8$</td>
<td style="text-align: center;">$21.9 \pm 3.6$</td>
<td style="text-align: center;">$23.6 \pm 3.1$</td>
<td style="text-align: center;">$15.5 \pm 0.9$</td>
<td style="text-align: center;">$15.4 \pm 2.9$</td>
</tr>
<tr>
<td style="text-align: center;">Our Methods</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CoR</td>
<td style="text-align: center;">$38.0 \pm 9.7$</td>
<td style="text-align: center;">$37.1 \pm 9.2$</td>
<td style="text-align: center;">$28.7 \pm 4.2$</td>
<td style="text-align: center;">$28.0 \pm 3.7$</td>
<td style="text-align: center;">$38.4 \pm 10.6$</td>
<td style="text-align: center;">$38.5 \pm 9.1$</td>
<td style="text-align: center;">$32.6 \pm 3.7$</td>
<td style="text-align: center;">$31.5 \pm 3.8$</td>
</tr>
<tr>
<td style="text-align: center;">GenRDK</td>
<td style="text-align: center;">$\mathbf{3 9 . 9} \pm \mathbf{1 0 . 9}$</td>
<td style="text-align: center;">$\mathbf{4 1 . 3} \pm \mathbf{8 . 9}$</td>
<td style="text-align: center;">$\mathbf{3 0 . 6} \pm \mathbf{3 . 6}$</td>
<td style="text-align: center;">$\mathbf{3 0 . 1} \pm \mathbf{4 . 2}$</td>
<td style="text-align: center;">$\mathbf{4 2 . 5} \pm \mathbf{1 0 . 6}$</td>
<td style="text-align: center;">$\mathbf{4 1 . 5} \pm \mathbf{8 . 7}$</td>
<td style="text-align: center;">$\mathbf{3 3 . 7} \pm \mathbf{4 . 0}$</td>
<td style="text-align: center;">$\mathbf{3 1 . 4} \pm \mathbf{4 . 6}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Experimental results of models trained by different synthetic data generated by vanilla chain-of-thought and our proposed chain-of-retrieval prompt.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Re-DocRED</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DocRED</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
</tr>
<tr>
<td style="text-align: left;">+ZeroDocRTE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Vanilla Prompt</td>
<td style="text-align: center;">8.35</td>
<td style="text-align: center;">9.04</td>
<td style="text-align: center;">10.32</td>
<td style="text-align: center;">9.77</td>
</tr>
<tr>
<td style="text-align: left;">Chain-of-Thought</td>
<td style="text-align: center;">9.80</td>
<td style="text-align: center;">10.43</td>
<td style="text-align: center;">12.80</td>
<td style="text-align: center;">12.85</td>
</tr>
<tr>
<td style="text-align: left;">Chain-of-Retrieval</td>
<td style="text-align: center;">$\mathbf{1 1 . 1 9}$</td>
<td style="text-align: center;">$\mathbf{1 3 . 2 3}$</td>
<td style="text-align: center;">$\mathbf{1 4 . 1 9}$</td>
<td style="text-align: center;">$\mathbf{1 3 . 3 8}$</td>
</tr>
<tr>
<td style="text-align: left;">+ZeroDocRE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Vanilla Prompt</td>
<td style="text-align: center;">38.58</td>
<td style="text-align: center;">42.45</td>
<td style="text-align: center;">35.38</td>
<td style="text-align: center;">34.98</td>
</tr>
<tr>
<td style="text-align: left;">Chain-of-Thought</td>
<td style="text-align: center;">45.10</td>
<td style="text-align: center;">47.80</td>
<td style="text-align: center;">45.27</td>
<td style="text-align: center;">43.72</td>
</tr>
<tr>
<td style="text-align: left;">Chain-of-Retrieval</td>
<td style="text-align: center;">$\mathbf{4 8 . 5 1}$</td>
<td style="text-align: center;">$\mathbf{4 9 . 2 1}$</td>
<td style="text-align: center;">$\mathbf{5 1 . 0 8}$</td>
<td style="text-align: center;">$\mathbf{4 8 . 3 0}$</td>
</tr>
</tbody>
</table>
<p>DocRED datasets, when there are 5 unseen relation types. When the number of unseen relation types is 10 , our GenRDK achieves $30.1 \pm$ $4.2 F_{1}$ and $31.4 \pm 4.6 F_{1}$ on the test set of RE-DocRED and DocRED datasets. Our GenRDK significantly outperforms the strong baseline ChatGPT by $19.6 F_{1}$ and $17.9 F_{1}$ on the test set of RE-DocRED and DocRED datasets. This demonstrates that our GenRDK can retrieve the implicit knowledge from ChatGPT. Moreover, the DocRE model trained on our denoised synthetic data outperforms the model trained on the original data by $4.2 F_{1}$ and $3.0 F_{1}$ on the test set of RE-DocRED and DocRED datasets when $m=5$. This suggests that our knowledge denoise strategy can reduce the wrong relational facts by the consistency of LLMs. In addition, we can observe that the performance of ZeroDocRE is higher than ZeroDocRTE. This is because the ZeroDocRTE task needs to extract the entity pair and relationship at the same time, which is much more challenging than the ZeroDocRE task.</p>
<h2>5 ANALYSIS AND DISCUSSION</h2>
<p>In this section, we conduct extensive experiments to further analyze the effectiveness of our proposed chain-of-retrieval prompt and consistency-guided knowledge denoising strategy. We also present the case study of denoising synthetic data. Furthermore, we perform
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: Experiment results for different unseen relation types on Re-DocRED and DocRED datasets.
an ablation study to analyze the individual contributions of each component in our framework.</p>
<h3>5.1 Effectiveness of Chain-of-Retrieval</h3>
<p>To demonstrate the effectiveness of our proposed chain-of-retrieval prompt, we leverage different prompts to generate labeled data with the same unseen relation types. We compare our proposed chain-of-retrieval prompt with the vanilla prompt and chain-ofthought prompt, which can be seen in Figure 6. As shown in Table 3, the DocRE model trained on the synthetic data generated by our chain-of-retrieval prompt achieves $49.21 F_{1}$ and $48.30 F_{1}$ on the test set of Re-DocRED and DocRED datasets. For the ZeroDocRTE task, the model trained on the synthetic data generated by our chain-of-retrieval prompt achieves $13.23 F_{1}$ and $13.38 F_{1}$ on the test set of Re-DocRED and DocRED datasets. We can observe that the models trained on the synthetic data generated by our chain-ofretrieval prompt obtained significant performance improvements for both ZeroDocRTE and ZeroDocRE tasks. This demonstrates that our chain-of-retrieval prompt can effectively guide ChatGPT to synthesize document-level relation samples step by step.</p>
<h3>5.2 Effectiveness of Knowledge Denoising</h3>
<p>To intuitively demonstrate the effectiveness of our consistencyguided cross-document knowledge denoising strategy. We present extensive experimental results of different popular DocRE backbone models [30, 38] trained on original and denoised synthetic data. As shown in Table 4, it can be observed that all backbone models</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Synthetic data</th>
<th style="text-align: center;">Original Labels</th>
<th style="text-align: center;">Denoised Labels</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">[1] The Godfather is a 1972 American film directed by Francis Ford Coppola</td>
<td style="text-align: center;">director</td>
<td style="text-align: center;">director</td>
</tr>
<tr>
<td style="text-align: center;">[2] The screenplay was written by Coppola and Mario Puzo, based on Puzō's best-sell ...</td>
<td style="text-align: center;">Francis Ford Coppola The Godfather</td>
<td style="text-align: center;">of Scresowrice <br> Francis Ford Coppola The Godfather</td>
</tr>
<tr>
<td style="text-align: center;">[1] Michael Jordan, born on February 17, 1963, is a former American professional basketball player and entrepreneur.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">[2] He is widely regarded as one of the greatest basketball players of all time.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">[3] Jordan spent the majority of his career playing for the Chicago Bulls in the National Basketball Association (NBA), where he won ...</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 8: Case Study. We present several samples with original and denoised labels of synthetic data.</p>
<p>Table 4: Experimental results of different DocRE backbone models trained on original and denoised synthetic data.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Re-DocRED</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DocRED</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
</tr>
<tr>
<td style="text-align: left;">ATLOP-Bert-base</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">+Synthetic Data</td>
<td style="text-align: center;">45.73</td>
<td style="text-align: center;">45.48</td>
<td style="text-align: center;">46.11</td>
<td style="text-align: center;">45.30</td>
</tr>
<tr>
<td style="text-align: left;">+ Denoised Synthetic Data</td>
<td style="text-align: center;">47.40</td>
<td style="text-align: center;">49.03</td>
<td style="text-align: center;">49.76</td>
<td style="text-align: center;">48.01</td>
</tr>
<tr>
<td style="text-align: left;">NCRL-Bert-base</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">+Synthetic Data</td>
<td style="text-align: center;">45.23</td>
<td style="text-align: center;">45.46</td>
<td style="text-align: center;">46.20</td>
<td style="text-align: center;">45.43</td>
</tr>
<tr>
<td style="text-align: left;">+ Denoised Synthetic Data</td>
<td style="text-align: center;">47.37</td>
<td style="text-align: center;">46.37</td>
<td style="text-align: center;">48.01</td>
<td style="text-align: center;">47.69</td>
</tr>
<tr>
<td style="text-align: left;">Ours-Bert-base</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">+Synthetic Data</td>
<td style="text-align: center;">45.61</td>
<td style="text-align: center;">46.49</td>
<td style="text-align: center;">46.87</td>
<td style="text-align: center;">45.06</td>
</tr>
<tr>
<td style="text-align: left;">+ Denoised Synthetic Data</td>
<td style="text-align: center;">$\mathbf{4 8 . 0 2}$</td>
<td style="text-align: center;">$\mathbf{4 9 . 1 9}$</td>
<td style="text-align: center;">$\mathbf{4 9 . 9 7}$</td>
<td style="text-align: center;">$\mathbf{4 8 . 0 5}$</td>
</tr>
<tr>
<td style="text-align: left;">ATLOP-Roberta-large</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">+Synthetic Data</td>
<td style="text-align: center;">48.43</td>
<td style="text-align: center;">48.74</td>
<td style="text-align: center;">48.38</td>
<td style="text-align: center;">47.75</td>
</tr>
<tr>
<td style="text-align: left;">+ Denoised Synthetic Data</td>
<td style="text-align: center;">49.13</td>
<td style="text-align: center;">50.29</td>
<td style="text-align: center;">51.07</td>
<td style="text-align: center;">49.18</td>
</tr>
<tr>
<td style="text-align: left;">NCRL-Roberta-large</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">+Synthetic Data</td>
<td style="text-align: center;">46.73</td>
<td style="text-align: center;">48.00</td>
<td style="text-align: center;">46.09</td>
<td style="text-align: center;">46.19</td>
</tr>
<tr>
<td style="text-align: left;">+ Denoised Synthetic Data</td>
<td style="text-align: center;">48.41</td>
<td style="text-align: center;">51.38</td>
<td style="text-align: center;">51.74</td>
<td style="text-align: center;">49.29</td>
</tr>
<tr>
<td style="text-align: left;">Ours-Roberta-large</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">+Synthetic Data</td>
<td style="text-align: center;">48.51</td>
<td style="text-align: center;">49.21</td>
<td style="text-align: center;">51.08</td>
<td style="text-align: center;">48.30</td>
</tr>
<tr>
<td style="text-align: left;">+ Denoised Synthetic Data</td>
<td style="text-align: center;">$\mathbf{5 0 . 6 1}$</td>
<td style="text-align: center;">$\mathbf{5 1 . 8 8}$</td>
<td style="text-align: center;">$\mathbf{5 2 . 9 0}$</td>
<td style="text-align: center;">$\mathbf{5 1 . 3 1}$</td>
</tr>
</tbody>
</table>
<p>obtain performance improvement after training on the denoised synthetic data. To intuitively demonstrate the denoising effects, we present the performance of several unseen relation types. As shown in Figure 7, we can observe that the performance of different unseen relation types significantly improves with the denoised synthetic data on both Re-DocRED and DocRED datasets. This suggests that our denoising strategy can improve the quality of generated synthetic data.</p>
<h3>5.3 Case Study</h3>
<p>We present several examples of synthetic data that have been denoised using our consistency-guided cross-document knowledge denoising strategy in Figure 8. It can be observed that our GenRDK is able to reduce label noises in synthetic data by 1) Adding correct relational facts by the cross-document knowledge graph, such as the triplets (The Godfather, Francis Ford Coppola, screenwriter) and</p>
<p>Table 5: Ablation study on the RE-DocRED and DocRED.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Re-DocRED</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DocRED</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
</tr>
<tr>
<td style="text-align: left;">GenRDK</td>
<td style="text-align: center;">$\mathbf{1 3 . 3} \pm \mathbf{1 . 2}$</td>
<td style="text-align: center;">$\mathbf{1 3 . 1} \pm \mathbf{2 . 6}$</td>
<td style="text-align: center;">$\mathbf{1 5 . 2} \pm \mathbf{0 . 7}$</td>
<td style="text-align: center;">$\mathbf{1 4 . 2} \pm \mathbf{1 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">w/o Denoising</td>
<td style="text-align: center;">$11.0 \pm 0.7$</td>
<td style="text-align: center;">$11.4 \pm 2.3$</td>
<td style="text-align: center;">$13.1 \pm 0.9$</td>
<td style="text-align: center;">$12.1 \pm 1.0$</td>
</tr>
<tr>
<td style="text-align: left;">w/o Seen Data</td>
<td style="text-align: center;">$12.2 \pm 1.0$</td>
<td style="text-align: center;">$11.6 \pm 0.5$</td>
<td style="text-align: center;">$12.7 \pm 0.7$</td>
<td style="text-align: center;">$12.5 \pm 0.9$</td>
</tr>
<tr>
<td style="text-align: left;">w/o Pruning</td>
<td style="text-align: center;">$11.9 \pm 0.6$</td>
<td style="text-align: center;">$11.2 \pm 1.6$</td>
<td style="text-align: center;">$13.0 \pm 1.4$</td>
<td style="text-align: center;">$12.1 \pm 0.6$</td>
</tr>
<tr>
<td style="text-align: left;">w/o Synthetic Data</td>
<td style="text-align: center;">$10.9 \pm 1.0$</td>
<td style="text-align: center;">$10.8 \pm 3.0$</td>
<td style="text-align: center;">$12.2 \pm 0.8$</td>
<td style="text-align: center;">$11.7 \pm 1.3$</td>
</tr>
</tbody>
</table>
<p>(Michael Jordan, Chicago Bulls, member of sports team); 2) Reducing the false relational facts by the consistency of knowledge, such as the triplet (Michael Jordan, National Basketball Association, member of sports team).</p>
<h3>5.4 Ablation Study</h3>
<p>To analyze the efficacy of each component within our GenRDK framework, we conduct an ablation study involving the removal of different components. As shown in Table 5, the performance diminishes with the removal of each component, showcasing the contribution of each component in our GenRDK framework. It can be observed that the removal of synthetic data leads to a 2.3 $F_{1}$ and $2.5 F_{1}$ on the test set of Re-DocRED and DocRED. This drop demonstrates the effectiveness of synthetic data generated by our chain-of-retrieval prompt. When we remove our knowledge denoising strategy, the DocRTE trained merely by the original synthetic data achieves $11.4 \pm 2.3 F_{1}$ and $12.1 \pm 1.0 F_{1}$ on the test set of Re-DocRED and DocRED. This indicates that leveraging the consistency constraint of knowledge can improve the quality of synthetic data. We conducted a more fine-grained analysis of our knowledge denoising module. Experimental results illustrate that removing the pruning strategy or data with seen relation types results in varying degrees of performance degradation in the DocRTE model.</p>
<h2>6 CONCLUSION</h2>
<p>In this paper, we propose a novel document-level data generation and denoising framework for the challenging Zero-shot Documentlevel Relation Triplet Extraction task (ZeroDocRTE). Different from previous DocRTE models that heavily rely on human-annotated training data, our framework can distill the latent relational facts from LLMs and generate labeled data with new types of relations. To address the challenge of generating long-text data and multiple</p>
<p>relation triplets, we propose a Chain-of-Retrieval prompt to guide ChatGPT to generate the document, entities, relation triplets, reasons, and support sentences step by step. To alleviate the inevitable noise in synthetic data, we construct cross-document knowledge graphs and propose a consistency-guided knowledge denoising strategy. To improve the quality of synthetic data, we remove unreliable relational facts by evaluating the consistency of knowledge. Leveraging the denoised synthetic data, we fine-tune the LLaMA2-13B-Chat for extracting document-level relation triplets. Experimental results demonstrate that our GenRDK outperforms competitive baselines on both DocRTE and DocRE tasks with zero-shot setting. In addition, extensive experiments illustrate the effectiveness of our denoising strategy. There are various challenges worth exploring, one potential avenue is enhancing the diversity and control of the generated data for ZeroDocRTE.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>To Robert, for the bagels and explaining CMYK and color spaces.</p>
<h2>REFERENCES</h2>
<p>[1] Chih-Yao Chen and Cheng-Te Li. 2021. ZS-BERT: Towards Zero-Shot Relation Extraction with Attribute Representation Learning. In Proceedings of NAACL. $3470-3479$.
[2] Yew Ken Chia, Lidong Bing, Soujanya Poria, and Luo Si. 2022. RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction. In Findings of the Association for Computational Linguistics: ACL 2022. $45-57$.
[3] Fenia Christopoulou, Makoto Miwa, and Sophia Ananiadou. 2019. Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs. In Proceedings of EMNLP. 4927-4938. https://aclanthology.org/D19-1498
[4] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022).
[5] Shehzaad Dholuwala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikydmaz, and Jason Weston. 2023. Chain-of-Verification Reduces Hallucination in Large Language Models. arXiv preprint arXiv:2309.11495 (2023).
[6] Booheng Ding, Chengwu Qiu, Linlin Liu, Yew Ken Chia, Boyang Li, Shafiq Joty, and Lidong Bing. 2023. Is GPT-3 a Good Data Annotator?. In Proceedings of ACL. Toronto, Canada, 11173-11195. https://aclanthology.org/2023.acl-long. 626
[7] Markus Eberts and Adrian Ulges. 2021. An End-to-end Model for Entity-level Relation Extraction using Multi-instance Learning. In Proceedings of EACL. 36503660. https://aclanthology.org/2021.eacl-main. 319
[8] John Giorgi, Gary Bader, and Bo Wang. 2022. A sequence-to-sequence approach for document-level relation extraction. In Proceedings of the 21st Workshop on Biomedical Language Processing. 10-25. https://aclanthology.org/2022.bionlp-1.2
[9] Ankur Goswami, Akehata Bhat, Hadar Ohana, and Theodoros Rekatsinas. 2020. Unsupervised Relation Extraction from Language Models using Constrained Cloze Completion. In Findings of EMNLP. 1263-1276.
[10] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. 2021. Towards a Unified View of Parameter-Efficient Transfer Learning. In International Conference on Learning Representations. https://arxiv.org/pdf/ 2110.04366.pdf
[11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021). https://arxiv.org/abs/ 2106.09685
[12] Quebe Huang, Shengqi Zhu, Yansong Feng, Yuan Ye, Yuxuan Lai, and Dongyan Zhao. 2021. Three Sentences Are All You Need: Local Path Enhanced Document Relation Extraction. In Proceedings of ACL. 998-1004. https://aclanthology.org/ 2021.acl-short. 126
[13] Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. 2017. Zero-Shot Relation Extraction via Reading Comprehension. In Proceedings of CoNLL. 333342 .
[14] Jingye Li, Kang Xu, Fei Li, Hao Fei, Yafeng Ren, and Donghong Ji. 2021. MRN: A Locally and Globally Mentions-Based Reasoning Network for Document-Level Relation Extraction. In Findings of ACL. 1359-1370. https://aclanthology.org/ 2021.findings-acl. 117
[15] Zhao Li, Xin Liu, Xin Wang, Pengkai Liu, and Yuxin Shen. 2022. TransO: a knowledge-driven representation learning method with ontology information constraints. World Wide Web (2022), 1-23.
[16] Zhuang Liu, Wayne Lin, Ya Shi, and Jun Zhao. 2021. A Robustly Optimized BERT Pre-training Approach with Post-training. In China National Conference on Chinese Computational Linguistics. Springer, 471-484.
[17] Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In Proceedings of ICLE. https://openreview.net/forum?id=Bkg6RiCyY7
[18] Guoshun Nan, Zhijiang Guo, Ivan Sekulic, and Wei Lu. 2020. Reasoning with Latent Structure Refinement for Document-Level Relation Extraction. In Proceedings of ACL. 1346-1557. https://aclanthology.org/2020.acl-main. 141
[19] Aboda Obaniayide and Andreas Vlachos. 2018. Zero-shot Relation Classification as Textual Entailment. Proceedings of EMNLP (2018), 72.
[20] Sunil Kumar Sahu, Fenia Christopoulou, Makoto Miwa, and Sophia Ananiadou. 2019. Inter-sentence Relation Extraction with Document-level Graph Convolutional Neural Network. In Proceedings of ACL. 4309-4316. https: //aclanthology.org/P19-1423
[21] Oscar Sainz, Oier Lopez de Lacalle, Gorka Labaka, Ander Barrena, and Eneko Agirre. 2021. Label Verbalization and Entailment for Effective Zero and Few-Shot Relation Extraction. In Proceedings of EMNLP. 1199-1212.
[22] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling. 2018. Modeling relational data with graph convolutional networks. In European Semantic Web Conference. Springer, 593-607.
[23] Qi Sun, Kun Huang, Xiaocui Yang, Pengfei Hong, Kun Zhang, and Soujanya Poria. 2023. Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction. In Proceedings of ACL. Association for Computational Linguistics, Toronto, Canada, 15960-15973. https://doi.org/10.18653/v1/2023.acl-long. 889
[24] Qi Sun, Kun Zhang, Kun Huang, Tiancheng Xu, Xun Li, and Yaodi Liu. 2023. Document-level relation extraction with two-stage dynamic graph attention networks. Knowledge-Based Systems 267 (2023), 110428. https://www.sciencedirect. com/science/article/pii/S0950705123001788
[25] Ryuichi Takanobu, Tianyang Zhang, Jiexi Liu, and Minlie Huang. 2019. A hierarchical framework for relation extraction with reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 7072-7079.
[26] Qingyu Tan, Lu Xu, Lidong Bing, Hwee Tou Ng, and Sharifah Mahani Aljunied. 2022. Revisiting DocRED - Addressing the False Negative Problem in Relation Extraction. In Proceedings of EMNLP. https://arxiv.org/abs/2205.12696
[27] Difeng Wang, Wei Hu, Ermei Cao, and Weijian Sun. 2020. Global-to-Local Neural Networks for Document-Level Relation Extraction. In Proceedings of EMNLP. 3711-3721. https://aclanthology.org/2020.emnlp-main. 303
[28] Kehai Chen Wang Xu and Tiejun Zhao. 2021. Discriminative Reasoning for Document-level Relation Extraction. In Findings of ACL. 1653-1663. https: //aclanthology.org/2021.findings-acl. 144
[29] Chenyan Xiong, Russell Power, and Jamie Callan. 2017. Explicit semantic ranking for academic search via knowledge graph embedding. In Proceedings of the 26th international conference on world wide web. 1271-1279.
[30] Wee Sun Lee Yang Zhou. 2022. None Class Ranking Loss for Document-Level Relation Extraction. In Proceedings of IJCAI. 4538-4544. https://www.ijcai.org/ proceedings/2022/0630
[31] Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Lisin Huang, Jie Zhou, and Maosong Sun. 2019. DocRED: A Large-Scale Document-Level Relation Extraction Dataset. In Proceedings of ACL. 764-777. https://aclanthology.org/P19-1074
[32] Shuang Zeng, Yuting Wu, and Baobao Chang. 2021. SIRE: Separate Intra- and Inter-sentential Reasoning for Document-level Relation Extraction. In Findings of EMNLP. 524-534. https://aclanthology.org/2021.findings-acl. 47
[33] Shuang Zeng, Runxin Xu, Baobao Chang, and Lei Li. 2020. Double Graph Based Reasoning for Document-level Relation Extraction. In Proceedings of EMNLP. 1610-1640. https://aclanthology.org/2020.emnlp-main. 127
[34] Xiangrong Zeng, Daojian Zeng, Shizhu He, Kang Liu, and Jun Zhao. 2018. Extracting relational facts by an end-to-end neural model with copy mechanism. In Proceedings of ACL. 506-514.
[35] Ruoyu Zhang, Yanzeng Li, and Lei Zou. 2023. A Novel Table-to-Graph Generation Approach for Document-Level Joint Entity and Relation Extraction. In Proceedings of ACL. Association for Computational Linguistics, Toronto, Canada, 10853-10865. https://aclanthology.org/2023.acl-long. 607
[36] Jun Zhao, WenYu Zhan, Xin Zhao, Qi Zhang, Tao Gui, Zhongyu Wei, Junzhe Wang, Minlong Peng, and Mingming Sun. 2023. RE-Matching: A Fine-Grained Semantic Matching Method for Zero-Shot Relation Extraction. In Proceedings of ACL. 6680-6691. https://aclanthology.org/2023.acl-long. 369
[37] Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing Hao, Peng Zhou, and Bo Xu. 2017. Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme. In Proceedings of ACL.
[38] Wenxuan Zhou, Kevin Huang, Tengyu Ma, and Jing Huang. 2021. Document-level relation extraction with adaptive thresholding and localized context pooling. In Proceedings of AAAL 14612-14620. https://ojs.aaai.org/index.php/AAAI/article/ view/17717</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://chat.openai.com/
${ }^{3}$ https://ai.meta.com/llama/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
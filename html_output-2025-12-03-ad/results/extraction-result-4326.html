<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4326 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4326</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4326</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-99.html">extraction-schema-99</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <p><strong>Paper ID:</strong> paper-276856814</p>
                <p><strong>Paper Title:</strong> Automation Applied to the Collection and Generation of Scientific Literature</p>
                <p><strong>Paper Abstract:</strong> : Preliminary activities of searching and selecting relevant articles are crucial in scientific research to determine the state of the art (SOTA) and enhance overall outcomes. While there are automatic tools for keyword extraction, these algorithms are often computationally expensive, storage-intensive, and reliant on institutional subscriptions for metadata retrieval. Most importantly, they still require manual selection of literature. This paper introduces a framework that automates keyword searching in article abstracts to help select relevant literature for the SOTA by identifying key terms matching that we, hereafter, call source words . A case study in the food and beverage industry is provided to demonstrate the algorithm’s application. In the study, five relevant knowledge areas were defined to guide literature selection. The database from scientific repositories was categorized using six classification rules based on impact factor (IF), Open Access (OA) status, and JCR journal ranking. This classification revealed the knowledge area with the highest presence and highlighted the effectiveness of the selection rules in identifying articles for the SOTA. The approach included a panel of experts who confirmed the algorithm’s effectiveness in identifying source words in high-quality articles. The algorithm’s performance was evaluated using the F 1 Score, which reached 0.83 after filtering out non-relevant articles. This result validates the algorithm’s ability to extract significant source words and demonstrates its usefulness in building the SOTA by focusing on the most scientifically impactful articles.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4326.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4326.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Panayi BERT+CRF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT (pre-trained on biomedical literature) + Conditional Random Field (CRF) entity/relationship extraction pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline using a biomedical pre-trained BERT model fine-tuned for entity recognition and a CRF layer for optimized entity classification; a separate BERT-based relationship classifier was used to identify relationships between biomedical entities for semi-automated data extraction in systematic reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluation of a prototype machine learning tool to semi-automate data extraction for systematic literature reviews</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>BERT+CRF entity recognition and BERT relationship classifier</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Fine-tune a BERT model pre-trained on biomedical text for token-level entity recognition with a CRF output layer to improve sequence labeling; separately train/finetune a BERT-based classifier to predict relationships between recognized entities. Training used manual annotations of keywords/entities from domain experts as supervision; relationship classifier optimized on cases with sufficient relationship examples.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>BERT pre-trained on biomedical literature (exact checkpoint not specified) with a CRF output layer; a separate BERT-based classifier for relationships</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical (oncology and Fabry disease case studies)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Entity relationships and extracted relations between biomedical entities (structured entity–relation information useful for downstream quantitative synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured entity labels and entity–relationship outputs (entity spans + relation labels)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparison to human annotations; standard information extraction evaluation (precision/recall/F1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Entity recognition: BERT+CRF achieved ~73% accuracy in oncology and ~70% in Fabry disease for entity recognition; relationship classification (BERT) achieved >90% accuracy in cases with sufficient relationship examples (as reported)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against human annotations and other model variants; BERT+CRF reported as best performer for entity recognition in the cited work (no additional numeric baselines in this paper beyond human benchmark and alternate models)</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Lower precision for extracting certain numeric/treatment-dose entities; relationship extraction requires sufficient labeled relationship examples to reach high accuracy; dependency on manual annotation for training.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automation Applied to the Collection and Generation of Scientific Literature', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4326.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4326.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COKE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>COVID-19 Knowledge Extraction (COKE) project</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system leveraging machine-reading models, deep learning, and sentence classification to semi-automatically organize and extract structured information from COVID-19 scientific literature to support rapid evidence synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Semi-automatic systematic literature reviews and information extraction of COVID-19 scientific evidence: Description and preliminary results of the COKE project</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Machine-reading + sentence classification pipeline (COKE)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Apply machine-reading models and deep-learning sentence classifiers to large COVID-19 literature corpora to identify, extract, and organize relevant sentences/claims into a structured format; uses semantic clustering and classification to transform unstructured articles into searchable, structured outputs for rapid decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Machine-reading models / deep-learning classifiers (specific architectures not specified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Epidemiology / COVID-19 literature (public-health/clinical evidence synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Structured extraction of study findings and relationships (evidence units, claims, and sentence-level relations) rather than explicit mathematical laws; supports extraction of quantitative study results and relationships between interventions and outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured records / organized sentence-level outputs and semantic clusters (structured data enabling review/decision-making)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Described as preliminary results; validation method not fully specified in this paper (implies comparison to manual/semi-manual review workflows)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not specified in this paper; positioned as accelerating and structuring manual systematic review processes</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Requires human oversight for interpretation and decision-making; preliminary nature of results implies further validation needed</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automation Applied to the Collection and Generation of Scientific Literature', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4326.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4326.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProtoCode</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ProtoCode: LLM-driven automated generation of machine-readable PCR protocols</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that leverages large language models to parse scientific publications and generate machine-readable representations of PCR protocols and experimental procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ProtoCode: Leveraging large language models (LLMs) for automated generation of machine-readable PCR protocols from scientific publications</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-based protocol extraction and structuring (ProtoCode)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Use LLM(s) to read full-text methods sections in publications, extract procedural steps and numerical parameters (e.g., temperatures, times, volumes), and convert them into a standardized, machine-readable protocol representation (e.g., structured JSON/step lists) suitable for automation or downstream analysis. The pipeline focuses on mapping free-text procedural descriptions into explicit procedural parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Large language models (unspecified in this paper/reference)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Molecular biology / laboratory protocols (PCR protocols)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Procedural numerical parameters and structured experimental protocols (quantitative procedural relationships such as step-wise parameter values)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Machine-readable protocols (structured representations of step sequences and numeric parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Not detailed in this paper excerpt; likely includes difficulties reliably parsing heterogeneous writing styles, numeric ambiguity, and need for human verification</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automation Applied to the Collection and Generation of Scientific Literature', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4326.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4326.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alshami ChatGPT-SLR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT-assisted automation of the systematic review process</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach applying ChatGPT (a large conversational LLM) to automate parts of the systematic literature review workflow, including information extraction and synthesis, demonstrated via a methodology and case study with reported limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Harnessing the power of ChatGPT for automating systematic review process: Methodology, case study, limitations, and future directions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ChatGPT-assisted SLR automation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Use ChatGPT to process abstracts/full-texts and extract key information relevant to a systematic review (e.g., inclusion/exclusion cues, extracted data points), relying on prompt engineering to guide the model; the approach attempts to reduce manual workload in screening and data extraction stages.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>ChatGPT (GPT family; specific model/version not specified in the cited work within this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Systematic review workflows (cross-domain; example case studies unspecified in this excerpt)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Extraction of study-level data and relationships relevant to review synthesis (e.g., classification of relevance, extraction of reported metrics) rather than explicit discovery of mathematical laws</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Textual extractions and structured summaries usable for downstream review (tables/summary fields produced via LLM prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Case study and methodological analysis; discussed limitations and need for future validation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in comparison: Alshami et al. achieved F1 ≈ 0.80 for identifying 'not related' classes (as cited in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared qualitatively to manual review workflows; numeric comparison cited here versus the authors' (source-words algorithm) non-LLM method: Alshami F1=0.80 vs the present paper's F1=0.83 for discarding nonrelevant articles</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Accuracy depends heavily on prompt design; observed efficiency drop on complex tasks; limitations and future directions highlighted by authors (need for cross-domain validation and human oversight)</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automation Applied to the Collection and Generation of Scientific Literature', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4326.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4326.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ofori-Boateng et al.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fully automated keyword extraction method (reviewed)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fully automated method for keyword extraction from literature described in a recent study; used as a point of comparison in this paper, noting the present work extends by embedding keywords within academic context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards the automation of systematic reviews using natural language processing, machine learning, and deep learning: A comprehensive review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Automated keyword extraction pipeline (as reported by Ofori-Boateng et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>An automated keyword extraction approach (details in the cited review) that identifies candidate keywords from documents without manual labelling; the current paper notes that such methods help identify relevant terms but lack contextual embedding into discipline-specific SOTA construction.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>NLP / machine learning / deep learning methods (specific models not detailed in this paper's discussion of Ofori-Boateng et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Systematic review / information extraction (cross-domain applicability)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Extraction of keywords and topical patterns rather than explicit mathematical/quantitative laws</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Lists of keywords / topic descriptors (for indexing and retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Keyword extraction alone does not embed terms in domain context; may produce computationally expensive and storage-intensive solutions and still require manual selection</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automation Applied to the Collection and Generation of Scientific Literature', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4326.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4326.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAPTOR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-oriented approach that uses recursive abstractive processing to organize retrieval outputs in a tree structure, facilitating structured extraction and navigation of information from large text corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RAPTOR: Recursive abstractive processing for tree-organized retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Recursive abstractive retrieval (RAPTOR)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Apply recursive abstractive summarization/processing to retrieved documents and organize the outputs in a hierarchical/tree structure to support multi-step information retrieval and possibly multi-hop extraction of relations; emphasizes abstractive representation rather than plain retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Abstractive processing models (transformer-based architectures likely; specific model not specified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Information retrieval / NLP (general)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Enables multi-step extraction of relationships and structured summaries; not explicitly described as discovering mathematical laws but useful for extracting multi-hop relations and structured patterns</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Hierarchical/ tree-organized abstractive summaries (structured nodes and summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Not detailed in this excerpt; general limitations of abstractive retrieval include faithfulness and potential hallucinations without grounding and need for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automation Applied to the Collection and Generation of Scientific Literature', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluation of a prototype machine learning tool to semi-automate data extraction for systematic literature reviews <em>(Rating: 2)</em></li>
                <li>Semi-automatic systematic literature reviews and information extraction of COVID-19 scientific evidence: Description and preliminary results of the COKE project <em>(Rating: 2)</em></li>
                <li>ProtoCode: Leveraging large language models (LLMs) for automated generation of machine-readable PCR protocols from scientific publications <em>(Rating: 2)</em></li>
                <li>Harnessing the power of ChatGPT for automating systematic review process: Methodology, case study, limitations, and future directions <em>(Rating: 2)</em></li>
                <li>Towards the automation of systematic reviews using natural language processing, machine learning, and deep learning: A comprehensive review <em>(Rating: 2)</em></li>
                <li>RAPTOR: Recursive abstractive processing for tree-organized retrieval <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4326",
    "paper_id": "paper-276856814",
    "extraction_schema_id": "extraction-schema-99",
    "extracted_data": [
        {
            "name_short": "Panayi BERT+CRF",
            "name_full": "BERT (pre-trained on biomedical literature) + Conditional Random Field (CRF) entity/relationship extraction pipeline",
            "brief_description": "A pipeline using a biomedical pre-trained BERT model fine-tuned for entity recognition and a CRF layer for optimized entity classification; a separate BERT-based relationship classifier was used to identify relationships between biomedical entities for semi-automated data extraction in systematic reviews.",
            "citation_title": "Evaluation of a prototype machine learning tool to semi-automate data extraction for systematic literature reviews",
            "mention_or_use": "mention",
            "method_name": "BERT+CRF entity recognition and BERT relationship classifier",
            "method_description": "Fine-tune a BERT model pre-trained on biomedical text for token-level entity recognition with a CRF output layer to improve sequence labeling; separately train/finetune a BERT-based classifier to predict relationships between recognized entities. Training used manual annotations of keywords/entities from domain experts as supervision; relationship classifier optimized on cases with sufficient relationship examples.",
            "llm_model_used": "BERT pre-trained on biomedical literature (exact checkpoint not specified) with a CRF output layer; a separate BERT-based classifier for relationships",
            "scientific_domain": "Biomedical (oncology and Fabry disease case studies)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Entity relationships and extracted relations between biomedical entities (structured entity–relation information useful for downstream quantitative synthesis)",
            "extraction_output_format": "Structured entity labels and entity–relationship outputs (entity spans + relation labels)",
            "validation_method": "Comparison to human annotations; standard information extraction evaluation (precision/recall/F1)",
            "performance_metrics": "Entity recognition: BERT+CRF achieved ~73% accuracy in oncology and ~70% in Fabry disease for entity recognition; relationship classification (BERT) achieved &gt;90% accuracy in cases with sufficient relationship examples (as reported)",
            "baseline_comparison": "Compared against human annotations and other model variants; BERT+CRF reported as best performer for entity recognition in the cited work (no additional numeric baselines in this paper beyond human benchmark and alternate models)",
            "challenges_limitations": "Lower precision for extracting certain numeric/treatment-dose entities; relationship extraction requires sufficient labeled relationship examples to reach high accuracy; dependency on manual annotation for training.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4326.0",
            "source_info": {
                "paper_title": "Automation Applied to the Collection and Generation of Scientific Literature",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "COKE",
            "name_full": "COVID-19 Knowledge Extraction (COKE) project",
            "brief_description": "A system leveraging machine-reading models, deep learning, and sentence classification to semi-automatically organize and extract structured information from COVID-19 scientific literature to support rapid evidence synthesis.",
            "citation_title": "Semi-automatic systematic literature reviews and information extraction of COVID-19 scientific evidence: Description and preliminary results of the COKE project",
            "mention_or_use": "mention",
            "method_name": "Machine-reading + sentence classification pipeline (COKE)",
            "method_description": "Apply machine-reading models and deep-learning sentence classifiers to large COVID-19 literature corpora to identify, extract, and organize relevant sentences/claims into a structured format; uses semantic clustering and classification to transform unstructured articles into searchable, structured outputs for rapid decision-making.",
            "llm_model_used": "Machine-reading models / deep-learning classifiers (specific architectures not specified in this paper)",
            "scientific_domain": "Epidemiology / COVID-19 literature (public-health/clinical evidence synthesis)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Structured extraction of study findings and relationships (evidence units, claims, and sentence-level relations) rather than explicit mathematical laws; supports extraction of quantitative study results and relationships between interventions and outcomes",
            "extraction_output_format": "Structured records / organized sentence-level outputs and semantic clusters (structured data enabling review/decision-making)",
            "validation_method": "Described as preliminary results; validation method not fully specified in this paper (implies comparison to manual/semi-manual review workflows)",
            "performance_metrics": null,
            "baseline_comparison": "Not specified in this paper; positioned as accelerating and structuring manual systematic review processes",
            "challenges_limitations": "Requires human oversight for interpretation and decision-making; preliminary nature of results implies further validation needed",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4326.1",
            "source_info": {
                "paper_title": "Automation Applied to the Collection and Generation of Scientific Literature",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "ProtoCode",
            "name_full": "ProtoCode: LLM-driven automated generation of machine-readable PCR protocols",
            "brief_description": "An approach that leverages large language models to parse scientific publications and generate machine-readable representations of PCR protocols and experimental procedures.",
            "citation_title": "ProtoCode: Leveraging large language models (LLMs) for automated generation of machine-readable PCR protocols from scientific publications",
            "mention_or_use": "mention",
            "method_name": "LLM-based protocol extraction and structuring (ProtoCode)",
            "method_description": "Use LLM(s) to read full-text methods sections in publications, extract procedural steps and numerical parameters (e.g., temperatures, times, volumes), and convert them into a standardized, machine-readable protocol representation (e.g., structured JSON/step lists) suitable for automation or downstream analysis. The pipeline focuses on mapping free-text procedural descriptions into explicit procedural parameters.",
            "llm_model_used": "Large language models (unspecified in this paper/reference)",
            "scientific_domain": "Molecular biology / laboratory protocols (PCR protocols)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Procedural numerical parameters and structured experimental protocols (quantitative procedural relationships such as step-wise parameter values)",
            "extraction_output_format": "Machine-readable protocols (structured representations of step sequences and numeric parameters)",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Not detailed in this paper excerpt; likely includes difficulties reliably parsing heterogeneous writing styles, numeric ambiguity, and need for human verification",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4326.2",
            "source_info": {
                "paper_title": "Automation Applied to the Collection and Generation of Scientific Literature",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Alshami ChatGPT-SLR",
            "name_full": "ChatGPT-assisted automation of the systematic review process",
            "brief_description": "An approach applying ChatGPT (a large conversational LLM) to automate parts of the systematic literature review workflow, including information extraction and synthesis, demonstrated via a methodology and case study with reported limitations.",
            "citation_title": "Harnessing the power of ChatGPT for automating systematic review process: Methodology, case study, limitations, and future directions",
            "mention_or_use": "mention",
            "method_name": "ChatGPT-assisted SLR automation",
            "method_description": "Use ChatGPT to process abstracts/full-texts and extract key information relevant to a systematic review (e.g., inclusion/exclusion cues, extracted data points), relying on prompt engineering to guide the model; the approach attempts to reduce manual workload in screening and data extraction stages.",
            "llm_model_used": "ChatGPT (GPT family; specific model/version not specified in the cited work within this paper)",
            "scientific_domain": "Systematic review workflows (cross-domain; example case studies unspecified in this excerpt)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Extraction of study-level data and relationships relevant to review synthesis (e.g., classification of relevance, extraction of reported metrics) rather than explicit discovery of mathematical laws",
            "extraction_output_format": "Textual extractions and structured summaries usable for downstream review (tables/summary fields produced via LLM prompts)",
            "validation_method": "Case study and methodological analysis; discussed limitations and need for future validation",
            "performance_metrics": "Reported in comparison: Alshami et al. achieved F1 ≈ 0.80 for identifying 'not related' classes (as cited in this paper)",
            "baseline_comparison": "Compared qualitatively to manual review workflows; numeric comparison cited here versus the authors' (source-words algorithm) non-LLM method: Alshami F1=0.80 vs the present paper's F1=0.83 for discarding nonrelevant articles",
            "challenges_limitations": "Accuracy depends heavily on prompt design; observed efficiency drop on complex tasks; limitations and future directions highlighted by authors (need for cross-domain validation and human oversight)",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4326.3",
            "source_info": {
                "paper_title": "Automation Applied to the Collection and Generation of Scientific Literature",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Ofori-Boateng et al.",
            "name_full": "Fully automated keyword extraction method (reviewed)",
            "brief_description": "A fully automated method for keyword extraction from literature described in a recent study; used as a point of comparison in this paper, noting the present work extends by embedding keywords within academic context.",
            "citation_title": "Towards the automation of systematic reviews using natural language processing, machine learning, and deep learning: A comprehensive review",
            "mention_or_use": "mention",
            "method_name": "Automated keyword extraction pipeline (as reported by Ofori-Boateng et al.)",
            "method_description": "An automated keyword extraction approach (details in the cited review) that identifies candidate keywords from documents without manual labelling; the current paper notes that such methods help identify relevant terms but lack contextual embedding into discipline-specific SOTA construction.",
            "llm_model_used": "NLP / machine learning / deep learning methods (specific models not detailed in this paper's discussion of Ofori-Boateng et al.)",
            "scientific_domain": "Systematic review / information extraction (cross-domain applicability)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Extraction of keywords and topical patterns rather than explicit mathematical/quantitative laws",
            "extraction_output_format": "Lists of keywords / topic descriptors (for indexing and retrieval)",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Keyword extraction alone does not embed terms in domain context; may produce computationally expensive and storage-intensive solutions and still require manual selection",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4326.4",
            "source_info": {
                "paper_title": "Automation Applied to the Collection and Generation of Scientific Literature",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "RAPTOR",
            "name_full": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval",
            "brief_description": "A retrieval-oriented approach that uses recursive abstractive processing to organize retrieval outputs in a tree structure, facilitating structured extraction and navigation of information from large text corpora.",
            "citation_title": "RAPTOR: Recursive abstractive processing for tree-organized retrieval",
            "mention_or_use": "mention",
            "method_name": "Recursive abstractive retrieval (RAPTOR)",
            "method_description": "Apply recursive abstractive summarization/processing to retrieved documents and organize the outputs in a hierarchical/tree structure to support multi-step information retrieval and possibly multi-hop extraction of relations; emphasizes abstractive representation rather than plain retrieval.",
            "llm_model_used": "Abstractive processing models (transformer-based architectures likely; specific model not specified in this paper)",
            "scientific_domain": "Information retrieval / NLP (general)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Enables multi-step extraction of relationships and structured summaries; not explicitly described as discovering mathematical laws but useful for extracting multi-hop relations and structured patterns",
            "extraction_output_format": "Hierarchical/ tree-organized abstractive summaries (structured nodes and summaries)",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Not detailed in this excerpt; general limitations of abstractive retrieval include faithfulness and potential hallucinations without grounding and need for evaluation",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4326.5",
            "source_info": {
                "paper_title": "Automation Applied to the Collection and Generation of Scientific Literature",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluation of a prototype machine learning tool to semi-automate data extraction for systematic literature reviews",
            "rating": 2,
            "sanitized_title": "evaluation_of_a_prototype_machine_learning_tool_to_semiautomate_data_extraction_for_systematic_literature_reviews"
        },
        {
            "paper_title": "Semi-automatic systematic literature reviews and information extraction of COVID-19 scientific evidence: Description and preliminary results of the COKE project",
            "rating": 2,
            "sanitized_title": "semiautomatic_systematic_literature_reviews_and_information_extraction_of_covid19_scientific_evidence_description_and_preliminary_results_of_the_coke_project"
        },
        {
            "paper_title": "ProtoCode: Leveraging large language models (LLMs) for automated generation of machine-readable PCR protocols from scientific publications",
            "rating": 2,
            "sanitized_title": "protocode_leveraging_large_language_models_llms_for_automated_generation_of_machinereadable_pcr_protocols_from_scientific_publications"
        },
        {
            "paper_title": "Harnessing the power of ChatGPT for automating systematic review process: Methodology, case study, limitations, and future directions",
            "rating": 2,
            "sanitized_title": "harnessing_the_power_of_chatgpt_for_automating_systematic_review_process_methodology_case_study_limitations_and_future_directions"
        },
        {
            "paper_title": "Towards the automation of systematic reviews using natural language processing, machine learning, and deep learning: A comprehensive review",
            "rating": 2,
            "sanitized_title": "towards_the_automation_of_systematic_reviews_using_natural_language_processing_machine_learning_and_deep_learning_a_comprehensive_review"
        },
        {
            "paper_title": "RAPTOR: Recursive abstractive processing for tree-organized retrieval",
            "rating": 1,
            "sanitized_title": "raptor_recursive_abstractive_processing_for_treeorganized_retrieval"
        }
    ],
    "cost": 0.01491275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Automation Applied to the Collection and Generation of Scientific Literature
6 March 2025</p>
<p>Nadia Paola Valadez-De La Paz 0000-0001-6950-8134
Departamento de Ingenieria Industrial
Tecnologico Nacional de Mexico
Instituto Tecnologico de Celaya
38010CelayaMexico</p>
<p>Jose Antonio Vazquez-Lopez 0000-0002-1687-2140
Departamento de Ingenieria Industrial
Tecnologico Nacional de Mexico
Instituto Tecnologico de Celaya
38010CelayaMexico</p>
<p>Aidee Hernandez-Lopez aidee.hernandezl@sabes.edu.mx 0000-0003-0488-7799
Departamento de Ingenieria Industrial
Universidad del SABES
Campus Celaya37234LeonMexico</p>
<p>Jaime Francisco Aviles-Viñas 0000-0002-4992-2992
Mechatronics Group, Engineering Faculty
Autonomous University of Yucatan
97302MeridaMexico</p>
<p>Jose Luis Navarro-Gonzalez jose.ng@saltillo.tecnm.mx 0000-0001-5742-3096
Departamento de Ingenieria Mecanica
Tecnologico Nacional de Mexico
Instituto Tecnologico de Saltillo
25280SaltilloMexico</p>
<p>Alfredo Valentin alfredoreyes@uadec.edu.mx 0000-0001-5742-3096
Facultad de Sistemas
Unidad Saltillo
Universidad Autonoma de Coahuila
25350SaltilloMexico</p>
<p>Ismael Lopez-Juarez ismael.lopez@cinvestav.edu.mx 0000-0001-6405-5519
Mechatronics Group, Engineering Faculty
Autonomous University of Yucatan
97302MeridaMexico</p>
<p>Robotics and Advanced Manufacturing Department
Centre for Research and Advanced Studies (CINVESTAV)
25900Ramos ArizpeMexico</p>
<p>Autonomous University of Yucatan
Ind No Contaminantes S/N
Col 2797302MeridaMexico</p>
<p>Automation Applied to the Collection and Generation of Scientific Literature
6 March 2025CB58E32103D23F5A029818C7C62B174810.3390/publications13010011Received: 4 January 2025 Revised: 27 February 2025 Accepted: 4 March 2025state-of-the-artmachine learningknowledge managementkeyword extractionsystematic literature review
Preliminary activities of searching and selecting relevant articles are crucial in scientific research to determine the state of the art (SOTA) and enhance overall outcomes.While there are automatic tools for keyword extraction, these algorithms are often computationally expensive, storage-intensive, and reliant on institutional subscriptions for metadata retrieval.Most importantly, they still require manual selection of literature.This paper introduces a framework that automates keyword searching in article abstracts to help select relevant literature for the SOTA by identifying key terms matching that we, hereafter, call source words.A case study in the food and beverage industry is provided to demonstrate the algorithm's application.In the study, five relevant knowledge areas were defined to guide literature selection.The database from scientific repositories was categorized using six classification rules based on impact factor (IF), Open Access (OA) status, and JCR journal ranking.This classification revealed the knowledge area with the highest presence and highlighted the effectiveness of the selection rules in identifying articles for the SOTA.The approach included a panel of experts who confirmed the algorithm's effectiveness in identifying source words in high-quality articles.The algorithm's performance was evaluated using the F 1 Score, which reached 0.83 after filtering out non-relevant articles.This result validates the algorithm's ability to extract significant source words and demonstrates its usefulness in building the SOTA by focusing on the most scientifically impactful articles.</p>
<p>Introduction</p>
<p>The collection and storage of scientific information have undergone significant advancements with the evolution of technology.However, the exponential growth of scientific production has gradually rendered some traditional methods inefficient (Morales-Alarcón et al., 2024).Each year, over 8.8 million experts worldwide produce more than 2.5 million new research publications, reflecting an impressive annual growth rate of 8% to 9% (Morales-Alarcón et al., 2024).Broadly, there are two primary methods for disseminating knowledge in published works: depositing them in subscription-based repositories or making them available as open research in Open Access (OA) repositories.The latter approach is increasingly dominating knowledge dissemination, as major publishing companies are shifting from traditional subscription-based models to OA or hybrid models (Turgel &amp; Chernova, 2024).This openness is quickly becoming the norm, supported by many leading universities transitioning to an open development model, steadily increasing their OA publications and prioritizing the transition (Turgel &amp; Chernova, 2024).Moreover, a significant benefit of publishing in OA is that these articles tend to receive more citations than their subscription-based counterparts, while authors also experience reduced processing times and quicker receipt of initial citations (Abdin &amp; De Pretis, 2024).Furthermore, developed countries like Japan have already announced that all publicly funded research will be made freely available (Ikarashi, 2023).This trend is one of the primary reasons why our research focuses on OA repositories while still considering subscription-based journals.There is a wide variety of OA repositories, including arXiv, bioRxiv, chemRxiv, medRxiv, SSRN, PsyArXiv, HAL, Preprints.org,EarthArXiv, OSF, and EpiPreprints.Many of these repositories are accessible through an application programming interface (API) that facilitates data extraction and automation, as exemplified by the arXiv API (arXiv API Access, 2025), which is incorporated into our framework.</p>
<p>Recent studies have demonstrated that AI can accelerate systematic literature reviews, enhance article selection, and reduce the manual workload involved in indexing (de la Torre-López et al., 2023).Accurate indexing is essential for ensuring that information is categorized and easily retrievable.It requires consistent terminology; a clear hierarchical structure; and, when necessary, the use of automated tools to manage large datasets efficiently (Manning et al., 2008;Sarthi et al., 2024).Effective indexing demands careful attention to elements such as titles, abstracts, summaries, and tables of contents, as outlined in the ISO 5963 (International Organization for Standardization, 1985) and UNE 50-121-91 standards, the latter specifically applying to Spanish-language documents (Asociación Española de Normalización y Certificación, 1991).</p>
<p>Models based on natural language processing (NLP) and deep learning have led to significant improvements in the organization of scientific information, facilitating easier access and classification (Musslick et al., 2025).Although automation has optimized several stages of the process, challenges persist in terms of model interpretability and the need for human oversight to ensure the quality of the processed information.</p>
<p>The evaluation of information retrieval systems dates back to Cyril Cleverdon's development of precision and recall metrics (Cleverdon, 1984).This measurement is particularly important given the inherent subjectivity, as the overlap between terms assigned by different individuals can vary between 30% and 60% (Cleverdon, 1984).Cleverdon's work, which quantified the proportion of relevant documents among the retrieved documents, became a foundational contribution in the 1960s (Jaiswal et al., 2024).</p>
<p>Knowledge management continues to be a critical aspect of the scientific community, enabling the efficient acquisition, construction, and communication of information (Zamiri &amp; Esmaeili, 2024).In this context, the development of automated tools for collecting and analyzing scientific literature represents a strategic advance that can improve the effectiveness and efficiency of knowledge production.</p>
<p>A systematic review aims to identify and describe the current state of the art (SOTA) and AI techniques employed to represent and infer knowledge, enabling the efficient manipulation of text and the ability to learn from large datasets (de la Torre-López et al., 2023).These techniques are particularly beneficial for tasks that are labor-intensive or repetitive for humans, such as analyzing scientific literature.Manually preparing and writing a systematic literature review (SLR) requires substantial time and effort, as it involves strategic planning, the performance of literature searches and analyses, and the reporting of findings (de la Torre-López et al., 2023).Additionally, scientific cartography, or science mapping, offers valuable tools for visualizing and analyzing relationships, trends, and structures within scientific disciplines.Tools like VOSviewer (Kirby, 2023) allow for graphical representations of geographic distribution, researcher collaborations, areas of interest, and the evolution of research topics.</p>
<p>A recent and highly significant development in the field of NLP is the emergence of large language models (LLMs) such as ChatGPT-4.5,Claude 3.5, Gemini 1.5, DeepSeek-R1, Meta-Llama 2, which have dramatically transformed the landscape of NLP (Alkhamissi et al., 2023;Miao &amp; Holmes, 2023).The literature highlights several prompt engineering techniques applied to LLMs for academic document analysis, including the use of structured and context-specific prompts, each tailored for particular information-processing tasks (Liu et al., 2023).However, studies have documented a decrease in efficiency when LLMs are tasked with more complex assignments (Alshami et al., 2023;Schick &amp; Schütze, 2021).Furthermore, the accuracy of these models is highly dependent on the design of task-specific prompts (Vatsal &amp; Dubey, 2024).Different tasks demand varying levels of information and constraints to guide the LLM in producing the intended output (Sivarajkumar et al., 2024).As such, it is essential to design precise, relevant prompts rather than relying on generic or vague instructions, which can lead to confusion or erroneous results (Alshami et al., 2023;Sivarajkumar et al., 2024).Another challenge is inherent in the very nature of large-scale models like GPT-3, which, although capable of learning effectively from few examples, require substantial computational resources for training and operation (Wu et al., 2021).A notable comparison can also be drawn with the work of Ofori-Boateng et al. (2024), which introduced a fully automated method for keyword extraction.While this approach helps identify relevant terms, our research builds upon it by embedding keywords within the specific context of the academic field, thereby improving the accuracy of document selection.The performance of our system highlights its ability to retrieve a large volume of relevant documents with high accuracy, comparing favorably with existing approaches in the literature.However, achieving a balance between accuracy and completeness remains a persistent challenge.</p>
<p>1.1.State-of-the-Art Selection Using Artificial Intelligence Panayi et al. (2023) assessed the use of advanced NLP models such as BERT and BiL-STM for the automated extraction of data from scientific publications related to systematic literature reviews (SLRs) in oncology and Fabry disease.The methodology is illustrated in Figure 1.Manual annotations of keyword were used to train the models, which were then compared with human annotations to assess their accuracy.Additionally, BERT was pre-trained with biomedical literature, and entity classification was optimized using a CRF model, with the F 1 Score metric employed to evaluate performance.The results indicated that the pre-trained BERT+CRF model performed best in entity recognition, achieving an accuracy of 73% in oncology and 70% in Fabry disease (Panayi et al., 2023).The model showed higher accuracy in identifying key clinical metrics, although it had lower precision in extracting treatment doses.On the other hand, the relationship classification model based on BERT achieved over 90% accuracy in cases with sufficient examples of relationships between biomedical entities (Panayi et al., 2023).These findings suggest that, with further adjustments, machine learning could become a key tool for the automated extraction of data in systematic reviews, significantly optimizing processing times and reducing the manual workload involved in analyzing large volumes of scientific literature.Despite advances in the automation of the building of cutting-edge technology, traditional methods of searching for and classifying information still require a considerable investment of time (Joseph &amp; Ravana, 2024).Traditional methods like pooling, sampling, and using evaluation metrics for large document collection are time-consuming and have higher computational costs (Joseph &amp; Ravana, 2024).Recent research has shown that integrating artificial intelligence into these processes can significantly improve efficiency (Golinelli et al., 2022).The COKE (COVID-19 Knowledge Extraction) project showcases the power of artificial intelligence in enhancing the systematic review of scientific literature (Golinelli et al., 2022).By leveraging machine reading models, deep learning, and sentence classification, the system effectively organizes large volumes of information into a structured format, enabling quicker and more informed decision making in high-uncertainty situations like the COVID-19 pandemic (Golinelli et al., 2022).</p>
<p>The implementation of advanced NLP and semantic clustering techniques has significantly improved the accuracy of identifying relevant information within large scientific datasets (Golinelli et al., 2022).Likewise, it has been discussed how the automation of scientific practice, through machine learning and deep learning tools, has made it possible to optimize knowledge management and improve the reproducibility of scientific studies (Musslick et al., 2025).However, the need for human supervision in these processes is also noted, since data interpretation remains a critical factor for decision making in research (Musslick et al., 2025).</p>
<p>1.2.Selecting the State of the Art (SOTA) of a Patent Using a Textual Similarity Search</p>
<p>The method shown in Figure 2 focuses on the automated search for and selection of patents.It provides a detailed analysis of the strategies applied over the past decade for prior art retrieval in patents, with particular emphasis on advancements in NLP techniques and semantic search (Ali et al., 2024).Moreover, it addresses the main challenges affecting the accuracy and coverage of these searches and discusses potential future directions to enhance the effectiveness of information retrieval systems in this domain (Ali et al., 2024).</p>
<p>Systematic literature reviews have proven to be key tools for analyzing recent advancements and trends in patent information retrieval (Ali et al., 2024).In this context, various types of searches are necessary throughout the life cycle of a patent, from its conceptualization to its post-grant management.Figure 2 illustrates the key stages where patent information retrieval is required, including the initial search for the SOTA, the evaluation of patentability during the pre-application and examination stages, and technological monitoring and portfolio assessment in the post-grant phase (Ali et al., 2024).Recent research underlines the need to apply advanced NLP and machine learning techniques for tasks such as article selection and full-text analysis, identifying challenges in transparency and replicability (Jiang et al., 2024).Likewise, in the same article, it was shown that extended language models can transform the collection and synthesis of information in scientific reviews, significantly reducing manual workload and improving accuracy in extracting key data (Jiang et al., 2024).</p>
<p>In this paper, a method that helps researchers quickly find relevant articles by searching for specific keywords in abstracts from scientific repositories is presented.To differentiate the term keywords, which is often used at the end of the article abstract, from keywords found in the abstract itself or in other metadata, such as the body of the article, in its title, etc., we refer to this term from now on as source words.The intention is to create a summary of articles based on the number of times these source words appear, making it easier to identify important texts for research.The algorithm saves time by overcoming challenges like finding synonyms and handling subjective terms.</p>
<p>Aim and Scope</p>
<p>The aim of this research is to automate the process of gathering relevant articles to construct a comprehensive SOTA review in a specific field of knowledge.This is achieved by automating keyword searches within article abstracts, using an innovative algorithm that identifies the most relevant articles based on the frequency of source words.Unlike previous studies exploring the automation of systematic reviews in scientific literature through the use of artificial intelligence, our approach stands out by incorporating more efficient indexing via the generation of key terms and the systematic organization of documents.</p>
<p>The scope of the research is extensive, with manuscripts sourced from both subscription-based scientific repositories, such as Scopus, Web of Science, and IEEE Xplore, and open-access repositories accessed via API (e.g., arXiv).Additionally, specific search criteria, including impact factor (IF), open access (OA) status, and JCR journal classification, are incorporated into the approach.These criteria are considered indicative but not exhaustive.</p>
<p>Framework</p>
<p>The methodological steps involved in the proposed framework to find and choose pertinent articles to build the SOTA are shown in Figure 3. Defining the research problem and knowledge structure constitutes the initial step of our framework.This step establishes the criteria for selecting the source words, which are essential components for conducting an organized bibliographic search.The selection of source words should prioritize terms that capture the core semantics of the topic while avoiding ambiguous or overly generic words.The identification of source words and their synonyms includes (1) defining the problem and its scope, (2) identifying the key concepts of the problem, (3) selecting the main areas of knowledge and their subtopics, and (4) generating source words and synonyms for each subtopic.</p>
<p>The next step is the search for and gathering of scientific data, which is done in two complementary ways: automatically by using an Application Programming Interface (API) in the arXiv repository (arXiv API Access, 2025) and manually by consulting a variety of specialized academic repositories.This two-pronged strategy ensures thorough and organized coverage of the study's pertinent literature.The first research database, also known as the study population is then created, consisting, in our case, of 650 articles in total.The algorithm presented in his article is applied to the articles that comprise the initial database of the study to determine the documents that will comprise the study's final database; simultaneously, a panel of experts conducts a validation process that is applied to a statistical sample that is chosen at random with a 90% confidence level.The F 1 Score metric, which is a measure of precision and recall in the analysis of outcomes, is determined as a result of this validation.The procedure ends with the selection of papers deemed relevant, which serve as a crucial foundation for the development of the SOTA.In turn, papers that are deemed irrelevant or unrelated to the study's goals are identified and discarded by the expert panel.</p>
<p>The proposed framework helps researchers in the selection of literature for the SOTA.The proposed algorithm is applied until the researcher has a collection of information that he considers important for his research, only by searching in scientific repositories, without reading each article in depth.A technical sheet is then prepared in table format, usually in Excel, which includes columns for the title and abstract of each article.The algorithm performs specific source word searches in the abstracts listed in the table, recording the rows in which such words are found.This procedure is repeated with all the defined source words, and the algorithm generates a summary with the source words found in each article.The texts selected as part of the SOTA are those with the highest number of source-word matches, allowing the user to decide the selection threshold.</p>
<p>The proposed algorithm significantly facilitates the identification of the SOTA by addressing challenges such as identifying synonyms or abstract terms related to the source words, thee subjectivity of terms assigned by different people, and the time needed to identify relevant information.In our case of study, the algorithm was applied to develop a system that assesses wine quality based on the analysis of color.Based on the research objectives and expert opinion, five key areas were defined: structural equations, sensory analysis, food and beverages, colorimetry, and artificial intelligence.For each of these areas, source words were established that the algorithm used to search in the abstracts of the selected articles.The employed methodology is further explained in Section 2.</p>
<p>This manuscript is organized as follows.Section 1 provides a review of the SOTA and a presentation of our framework.Section 2 presents the methodology of the framework, whose results of application to a case study are presented in Section 3. Finally, the conclusions, limitations, and future work are described in Section 5.</p>
<p>Methodology</p>
<p>One of the main approaches leading to a framework for research on a topic or field is content analysis, which can help reduce systematic error and random effects and provide more reliable results according to the method proposed by Hachicha and Ghorbel for the formation of frameworks, which consists of two phases (Hachicha &amp; Ghorbel, 2012).The first phase is the definition of sources for the search for articles, and the second is the classification of the selected articles.For our research, this method is used, and a third phase is added, which includes classification using fuzzy rules.</p>
<p>First Phase: Bibliographic Search</p>
<p>1.</p>
<p>The main themes that would determine the bibliographic search were determined by the titles of the articles.The topics that were identified were sensory analysis, physical-chemical analysis, wine evaluation, and structural equations.In relation to the identified topics, a set of source words was established for the bibliographic search: color spaces, colorimetry in food, wine, wine color, wine quality, artificial vision, sensory analysis, structural equations, fuzzy logic, artificial intelligence, sensory quality, color spectrum, CIELab, and color measurements.</p>
<p>2.</p>
<p>The computerized search of the set of source words was limited to scientific articles from the literature and scientific repositories, such as ScienceDirect, WoS, Goggle Scholar, Springer Link, and IEEE Xplore.In addition, using the same source words, the search also included articles extracted from the arXiV repository using an available API.Through a computerized search with the specifications and in the scientific repositories mentioned above, a population of 650 articles was obtained.</p>
<p>3.</p>
<p>The classification rules for the population of articles were established using two evaluation criteria (as shown in Table 1).</p>
<p>Criterion Outputs
OA Yes, No IF &lt;1, 1 &amp; 2, &gt;2
The OA criterion is represented by two possible outputs: "Yes" for articles that are open-access and "No" for articles that have restricted access (by subscription).The IF (impact factor) criterion is represented by three outputs: "IF &lt; 1" for articles published in journals with an impact factor of less than 1, "IF 1 and 2" for articles published in journals with an impact factor between 1 and 2 and, and "IF &gt; 2" for articles published in journals with an impact factor greater than 2.These criteria were considered fundamental for the selection of scientific articles.Table 2 presents the six resulting rules.Figure 4 shows the behavior of the population of articles when applying the six classification rules.In this figure, it can be observed that category R1 represents the largest proportion of articles, with a value of 400, followed by R5 with 168 and R6 with 72.</p>
<p>Rule</p>
<p>In contrast, categories R3 and R4 have a significantly lower share, with values of 2 and 8, respectively.It is worth mentioning that R2 was omitted in this graph because it did not contain any articles.The different colors allow each category to be identified in the figure, which facilitates the interpretation of the distribution.It is important to note that the application of the classification rules (R1-R6) may is optional for each project; however, in this case it was decided to use them to maintain better order in the articles that belong to each of the classification rules, allowing for a better organization and analysis of the data.</p>
<p>Second Phase: Literature Classification</p>
<p>Based on the titles of the articles, the areas of knowledge relevant to the creation of the reference framework were determined, as well as their respective codes, as shown in Table 3.After carrying out the first phase, the need to assign weights to the articles included in each rule was evaluated.The purpose of this was to evaluate the frequency with which the source words (Table 4) appear in the areas of knowledge relevant to the study.The identification of the source words was achieved through the proposed algorithm developed in Python 3.12.</p>
<p>The operation of this algorithm is provided in Algorithm 1.</p>
<p>Algorithm 1 Search for source words in a document 1: Define: Title of the document and column where summary is located 2: Define: Sheet name to be read and the source word to be searched for 3: Input: Sheet name, column location, source word 4: Search: Search for the source word in the corresponding column 5: Store: Store the row number where the source word is found 6: Output: Display the numbers of all rows where the source word is found  To classify the literature, it is necessary to create a relationship diagram in which the relationships between each area of knowledge can be observed.Keep in mind that the knowledge areas were chosen by subject-matter experts who also established the connections between them based on how crucial it is to include a combination of each area in an article.In Figure 5, each relationship is represented by a line; areas with a high degree of relationship are joined with four lines, those with a medium degree of relationship are with three lines, and those with a low degree of relationship are joined with two lines.For example, if an article contains a relationship between areas of knowledge B and C, it is considered to exhibit a high degree of relationship; if it contains a relationship between areas of knowledge D and E, it is considered to have a medium degree of relationship; and if it contains a relationship between areas of knowledge A and D, it is considered a to have low degree of relationship.Each line that represents a relationship between areas of knowledge has a value of seven points.If the relationship is less relevant, one or two points are deducted, depending on the level of importance.</p>
<p>Based on the above considerations, each relationship is assigned a weight ranging from 13 to 28 points.This assignment reflects the degree of interest (Z) of the article for research, where 13 represents the lowest level of interest and 28 is the highest (Table 5).These weights were defined by experts in the field, classifying relationships with values between 13 and 14 as of low interest, those between 19 and 21 as of medium interest, and the rest as of high interest (Table 6).</p>
<p>The inference rules are determined according to the degree of interest to which each relation of the areas of knowledge belongs (Figure 6).These inference rules are based on the Mamdani method, which indicates that logical propositions comply with the "if-then" rule.For this research, three rules were obtained, which are described below:</p>
<p>Fuzzy Sets in Correlation</p>
<p>Results</p>
<p>After searching for source words using the Algorithm 1, they were related to each classification rule as shown in Figure 7, where the distribution of the number of scientific articles according to the classification rules (R1 to R6) and areas of knowledge (A, B, C, D, and E) is shown.Each area of knowledge is represented by a color: structural equations (A) in blue, sensory analysis (B) in red, food and beverages (C) in yellow, colorimetry (d) in green, and artificial intelligence (E) in orange.</p>
<p>For an article to be relevant to this research, it must cover three knowledge areas.It is important to note that in some cases, several source words belonging to the same area were found in the same article, suggesting a closer relationship with the research topic.It can be observed that rules R1 and R5 presented the highest number of articles, while R2, R3, and R4 presented very low values.In R1, area of knowledge "D" (green) dominates, with the highest number of articles, followed by "C" (yellow) and "E" (orange).These data indicate that certain rules concentrate scientific production in specific areas, while others present a lower number of articles in all categories.</p>
<p>The frequency of the appearance of source words in each knowledge area was analyzed.The analysis was performed for each classification rule, starting with the evaluation of the articles contained in R1 (Figures 8-12 are presented in this paper as examples).</p>
<p>Figures 8-12 show stacked bars displaying the repetition of source words in different scientific articles, identified by their row number on the X-axis.Each bar represents an article and is segmented into five knowledge areas (A, B, C, D, and E), differentiated by colors.It can be observed that the frequency of source-word repetition varies between articles, with some reaching up to four repetitions, while others have lower values.Knowledge areas D and E seem to be predominant in most articles, while C and B have lower levels of representation.Furthermore, it can be noticed that, in certain rows, such as 34 and 18, there is a higher concentration of source words compared to others.This analysis allows for the identification of the distribution of source words within articles according to the different knowledge areas, providing a structured view of the thematic classification in this dataset.The first article to highlight in Figure 8   In Figures 9 and 10, it can be observed that most of the articles present combinations of two knowledge areas.In Figure 9, the most recurrent are C and E in the first half of the graph and B and D in the second half, and the opposite occurs for Figure 10.The articles corresponding to rows 57, 81, 125, and 240 stand out, since they contain three knowledge areas simultaneously, suggesting a greater interdisciplinarity in these studies.The variability in the repetition of source words and the combination of areas allow for the identification of trends in the thematic classification of the analyzed articles.In Figure 11, it can be observed that most of the articles present combinations of three areas of knowledge (B, C, and E).The articles corresponding to rows 272 and 302 stand out, since they contain three areas of knowledge simultaneously, suggesting a greater interdisciplinarity in these studies.In Figure 12, it can be observed that most of the articles are represented by area of knowledge A. However, in this dataset, no article stands out, since none presents three areas of knowledge.The second rule lacks information, as no article is found, so the next to be analyzed is R3 (Figure 13).The content of articles for this rule is small, with only six in total.Of these, the following stand out: the article corresponding to row three, which presents source words in areas B and D, and the articles in rows 5 and 6, which cover areas B and C, with a source words in each.It is important to note that the most prominent knowledge area represented in this rule is sensory analysis (B).However, since these articles do not cover three knowledge areas, they are not considered important for this research.The next rule to be analyzed is R4.Most of the literature is found in this rule, so it is necessary to represent the analysis in three figures.The first set observed in Figure 14 comprises articles presented between row 1 and row 61 of the Excel sheet.It is worth mentioning that the articles where no source words was found are omitted.Among this set of articles, the one in row 39 stands out, with source words in each of the three areas of knowledge-B, D, and E. Therefore, it is evident that in this set, the areas of knowledge are sensory analysis (B), colorimetry (D), and artificial intelligence (E).Continuing with the analysis of R4, Figure 15 shows the second set of articles.This set includes the articles from row 62 to row 98.Likewise, the articles in which no source words were found in the abstract are omitted.In this set, several articles stand out that encompass three areas of knowledge (B, C, and D).These articles are found in rows 66, 67, 68, 74, 75, 76, 77, 84, 90, 91, 95, 96, and 98.From this set, a total of thirteen important articles for the research were obtained, covering the areas of knowledge of sensory analysis (B), food and beverages (C), and colorimetry (D).To conclude the analysis of R4, Figure 16 shows the third cluster of articles.This cluster is represented by the articles from rows 99 to 135, omitting the articles where no source words were found.It can be observed that most of the articles contained in this cluster cover three areas of knowledge.These areas of knowledge are the same as the for the previous cluster (B, C, and D).In addition, in this set of articles, there is one in row (134) that does not contain area of knowledge D; however, it does contain area of knowledge A, which corresponds to structural equations.This shows that the main topics are structural equations (A), sensory analysis (B), food and beverages (C), and colorimetry (D).In the following rule (R5), there is only one article (Figure 17), and the identified source words belong to the areas of colorimetry and artificial intelligence (D and E, respectively).This article only covers two areas of knowledge; therefore, it is not considered important for the research.Finally, the analysis of R6 is presented.This analysis is divided into two sets of articles (Figures 18 and 19), the first of which contains the articles from rows 2 to 32 and the second of which contains the articles from rows 33 to 61.In both sets, the articles in which no source words were found were omitted.The articles with the greatest relevance for this research that belong to the first set (Figure 18) are the articles corresponding to rows 2, 12, 16, 17, and 20.These articles cover three areas of knowledge; the article in row 12 covers areas B, D, and E, and the remaining articles ones cover areas B, C, and D.</p>
<p>The second set of R6 articles is shown in Figure 19.It can be observed that the articles to be highlighted are those in rows 44, 45, 46, 49, 51, 53, and 55, covering    Therefore, for R6, the areas of knowledge are sensory analysis (B), food and beverages (C), colorimetry (D), and artificial intelligence (E).Comparing the analysis of each rule with Figure 9, it is evident that, in both instances, the areas of sensory analysis (B), food and beverages (C), and colorimetry (D) are those that present the greatest repetition of source words.This indicates that the majority of the located literature is linked to these three areas in particular.</p>
<p>Accuracy of a Classification Model</p>
<p>The F 1 Score is a metric used to evaluate the accuracy of a classification model by combining two fundamental measures-precision and recall-into a single value.It is considered a balanced measure between these two metrics that is especially useful when the data in the classes are imbalanced, meaning there is a dominant class.The reason for using the harmonic mean instead of the arithmetic mean between precision and recall is that the harmonic mean gives more weight to lower values.This means that the F 1 Score is low if either of these two metrics is low, penalizing models that are imbalanced between precision and recall.Mathematically, the F 1 Score is defined as the harmonic mean between precision and recall.</p>
<p>The formula is expressed as follows:
F 1 Score = 2 × P × R P + R(1)
where
P = precision, R = recall
In turn, we consider TP = true positive, FP = false positive, FN = false negative Precision measures how accurate a model's positive predictions are.It is defined as the number of true positives divided by the total number of positive predictions:
P = TP TP + FP (2)
Recall measures the ability of the model to identify all positive cases.It is calculated as the number of true positives divided by the total number of true positives and false negatives:
R = TP TP + FN(3)
The case study considers a database with a total of 650 articles to be classified into two classes: articles relevant to a specific research topic and articles not relevant to the specific research topic.A panel of specialists in the field of the research topic, consisting of researchers (professors and PhD students), was consulted to review the database and obtain a benchmark classification of the articles it contains.Table 7 provides an overview of the profiles of the expert panel, while Table 8 outlines their professional backgrounds, average age, and gender distribution.The process of reviewing 650 articles requires time and monetary resources, which are limited in research.Therefore, classical statistics were used to review a portion of the 650 articles, selected as a random sample with 90% confidence and a margin of error of 0.05, in order to infer the results to the entire database.Thus, based on the sample size calculation, 100 articles were reviewed, selected through simple random sampling.The statistical approach for calculating the sample size was binomial, since only two classes were considered with the parameters shown in Table 9.In this case, TP is the number of articles that the algorithm classifies as relevant when they really are, that is, the experts classified them as such; FP is be the number of articles that the algorithm classifies as relevant when the experts classified them as not relevant; and FN is the number of articles that the algorithm classified as not relevant when the experts determined that they were.For the case study, the truth table and the corresponding calculations are given in Table 10: In this case, TP is the number of articles that the algorithm classified as not relevant when they are truly not relevant, i.e., the experts classified them as such; FP is the number of articles that the algorithm classifies as not relevant when the experts classified them as relevant; and FN is the number of articles that the algorithm classified as relevant when the experts determined that they were not, i.e., not relevant.For the case study, the truth table and the corresponding calculations are given in Table 11:
P = TP TP + FP = 7 7 + 1 = 0.875 R = TP TP + FN = 7 7 + 25 = 0.2188 F 1 Score = 2 • P • R P + R = 2 • 0.F 1 Score = 2 • P • R P + R = 2 • 0.7283 • 0.9853 0.7283 + 0.9853 = 0.8375
The results indicate that when the selection of non-relevant articles is defined as "success", the F 1 Score is higher than when the selection of relevant articles is defined as "success".Based on these data and the fact that the panel selected more relevant articles than the algorithm, it can be concluded that one of the following three approaches will have to be implemented in future research: i. train the algorithm to make it more flexible, ii.perform a stricter review by the panel, or ii.both strategies simultaneously.Lastly, it can be said that the algorithm can be used to filter out the articles that a human reviewer would deem irrelevant during the early phases of research.Finally, Table 12 shows a summary of the articles that were important for this research.</p>
<p>Discussion</p>
<p>In the case study, five knowledge areas with research-relevant source words were identified for literature selection.The database was sourced from subscription-based scientific repositories and the open-access arXiv repository via an API (arXiv API Access, 2025).The search was structured using six classification rules (R1-R6) based on three criteria: impact factor (IF), open access (OA), and JCR journal ranking.The most represented knowledge areas included colorimetry, food and beverages, and sensory Analysis.Some classification rules were found to have fewer articles, indicating that certain rules are more influential or easier to meet.The repetition of source words played a key role in prioritizing articles, suggesting areas of greater research interest.</p>
<p>The figures representing the analysis of the articles in each classification rule demonstrate the thematic diversity of the selected articles, suggesting a multidisciplinary approach.These results confirm the effectiveness of the applied selection rules in identifying articles that contribute meaningfully to the SOTA.Out of 650 articles retrieved from the computerized search, 51 were deemed relevant for SOTA development (Table 12).By filtering approximately 92% of the literature, the analysis focuses on high-quality studies with a greater scientific impact.</p>
<p>The application of the classification rules and the analysis of the repetitiveness of the source words ensure that the SOTA is based on relevant and up-to-date research, enhancing the reliability and depth of the theoretical analysis.However, the classification rules can be adjusted to meet the specific needs of each research project, as well as the number of source words and/or knowledge areas found in each article to determine their relevance.</p>
<p>The research was deemed satisfactory, particularly regarding the accuracy of the methodology.This was demonstrated by the high F 1 Score achieved in discarding nonrelevant articles, which outperformed similar approaches that utilize large language models (LLMs), such as the work reported by Alshami et al. (2023).While their approach achieved an F 1 Score of 0.80 in identifying "not related" classes, our methodology reached an F 1 Score of 0.83 for the task of discarding non-relevant articles.Although these results are promising, further work is required to conduct more accurate comparisons, as the studies differ in discipline.Therefore, future work should focus on cross-domain validation to enhance the robustness of the findings.Despite these positive outcomes, several areas for improvement were identified, underscoring the limitations of the framework.These include the following:</p>
<p>1.</p>
<p>The framework's performance depends on the quality of the input data, which is influenced by the completeness and structure of the metadata or abstracts in the repositories.Inaccurate or poorly structured data may negatively impact system performance.2.</p>
<p>The accuracy of the system show that irrelevant documents could occasionally be retrieved, leading to unrelated information.To address this, an additional manual filtering phase, conducted by a panel of experts, is necessary.</p>
<p>3.</p>
<p>Manual extraction of information is still required for subscription-based articles, even though APIs can automate the process, as seen with the arXiv repository.However, subscription-based repositories like IEEE Xplore, WoS, and Scopus require institutional access.</p>
<p>Future work will also focus on including additional repositories, such as bioRxiv, chemRxiv, medRxiv, PsyArXiv, and HAL, to enhance generalization across different areas.It will also aim to improve semantic extraction by implementing natural language processing (NLP) using open-source multimodal large language models (LLMs) and compare performance with related approaches.</p>
<p>Conclusions</p>
<p>Effective knowledge management is crucial in today's data-driven world, especially in the acquisition and transmission of information.However, the acquisition process is often slow and error-prone, limiting the development of state-of-the-art (SOTA) knowledge.This paper introduces a framework that automates the keyword search in article abstracts, using an algorithm that identifies the most relevant articles based on the frequency of key terms, referred to as source words.Unlike previous studies that explored automation in the systematic review of scientific literature using artificial intelligence, our methodology distinguishes itself by incorporating more efficient indexing through the generation of key terms and the systematic organization of documents.</p>
<p>A case study identified several knowledge areas with relevant source words for literature selection.The database was sourced from both subscription-based scientific repositories and the open-access arXiv repository via API.Six classification rules based on impact factor (IF), open access (OA), and JCR journal rankings were applied.The knowledge areas of colorimetry, food and beverages, and sensory analysis were the most represented.Some rules resulted in fewer articles, suggesting that certain classification criteria are more influential than others.The results highlight the multidisciplinary nature of the selected articles.This study ensures that the SOTA is based on relevant and up-to-date research by applying classification rules and analyzing source words.The rules can be customized to suit different research needs, adjusting the number of source words and knowledge areas for relevance.</p>
<p>Figure 1 .
1
Figure 1.Process for enhancing entity recognition and relationship extraction in language models.Source: Panayi et al. (2023).</p>
<p>Figure 2 .
2
Figure 2. Patent research to identify related documents at various stages of their life cycles.Source: Ali et al. (2024).</p>
<p>Figure 3 .
3
Figure 3. Proposed framework (created by the authors).</p>
<p>F. &lt; 1) ∩ (O.A. = Yes) R2 (I.F.&lt; 1) ∩ (O.A. = No) R3 (I.F. 1 and 2) ∩ (O.A. = Yes) R4 (I.F.&gt; 2) ∩ (O.A. = Yes) R5 (I.F. 1 and 2) ∩ (O.A. = No) R6 (I.F.&gt; 2) ∩ (O.A. = No)</p>
<p>Figure 4 .
4
Figure 4. Behavior of the article population applying the classification rules.</p>
<p>Figure 5 .
5
Figure 5. Diagram of relationships between the areas of knowledge of the topic to be investigated.</p>
<p>Figure 6 .
6
Figure 6.Mamdani's inference rules for determining the degree of interest of each article.</p>
<p>Figure 7 .
7
Figure 7. Analysis of the literature by areas of knowledge and classification rules.</p>
<p>is located in row 2, since it covers knowledge areas B, C, and D. The next articles to highlight are those in rows 14, 15, and 18, which cover areas B, D, and E.</p>
<p>Figure 8 .
8
Figure 8. Descriptive analysis of the repetitiveness of the source words in each article in R1 (R1-1).</p>
<p>Figure 9 .
9
Figure 9. Descriptive analysis of the repetitiveness of source words in each article in R1 (R1-2).</p>
<p>Figure 10 .
10
Figure 10.Descriptive analysis of the repetitiveness of source words in each article in R1 (R1-3).</p>
<p>Figure 11 .
11
Figure 11.Descriptive analysis of the repetitiveness of source words in each article in R1 (R1-4).</p>
<p>Figure 12 .
12
Figure 12.Descriptive analysis of the repetitiveness of source words in each article in R1 (R1-5).</p>
<p>Figure 13 .
13
Figure 13.Descriptive analysis of the repetitiveness of the source words in each article in R3.</p>
<p>Figure 14 .
14
Figure 14.Descriptive analysis of the repetitiveness of source words in each article in R4 (first set).</p>
<p>Figure 15 .
15
Figure 15.Descriptive analysis of the repetitiveness of source words in each R4 article (second set).</p>
<p>Figure 16 .
16
Figure 16.Descriptive analysis of the repetitiveness of source words in each article of R4 (third set).</p>
<p>Figure 17 .
17
Figure 17.Descriptive analysis of the repetitiveness of the source words in each article of R5.</p>
<p>the same three areas of knowledge: B, C, and D.</p>
<p>Figure 18 .
18
Figure 18.Descriptive analysis of the repetitiveness of the source words in each article of R6 (first set).</p>
<p>Figure 19 .
19
Figure 19.Descriptive analysis of the repetitiveness of the source words in each R6 article (second set).</p>
<p>Table 1 .
1
Evaluation criteria for the population of items.</p>
<p>Table 2 .
2
Classification rules that are effective for research.</p>
<p>Table 3 .
3
Areas of knowledge relevant to research.
CodeAreaAStructural equationsBSensory analysisCFood and beveragesDColorimetryEAI, pattern recognition, and artificial vision</p>
<p>Table 4 .
4
Source words by area of knowledge.
CodeSource WordsAStructural equations and SEMBCATA, RATA, smell, sensory, color, and flavorCFood, beverages, wine, and red wineDColorimetry, sprectrophotometer, colorimeter, CIELab, color, and colorimetricEPattern recognition, artificial vision, AI, Automated learning, artificial intelligence, and intelligent</p>
<p>Table 5 .
5
Correlation of each area of knowledge relevant to the research.</p>
<p>Table A
ABCDEA19141314B282128C2027D20E</p>
<p>Table 6 .
6
Degree of interest according to the weighting of the relationships between the research areas.
Linguistic</p>
<p>Values for Measuring Degree of Interest with Z Research
PointsZ13-14Low19-21Medium27-28High</p>
<p>Table 7 .
7
Expert panel profiles.
Expert</p>
<p>Profiles Panel Information No. of Experts Average Years of Expertise
Public-sector institutions716.6Private sector110Food technology120Statistical technologies325Innovation technologies712.5Artificial intelligence technologies611</p>
<p>Table 8 .
8
Professional background.
Professional BackgroundPanel InformationNo. of ExpertsAdditional DetailsEnology and/or sensory evaluation3Academia and industry8PhDs/average years of experience612.5 yearsDoctoral students/research progress220%Average age41.7 yearsGender distribution2 women and 6 men</p>
<p>Table 9 .
9
A: Number of articles considered as the study population.B: Level of confidence for the statistical calculation of the sample size.C: Expected proportion of the population that has the expected classification.D: Expected margin of error.E: Z value (theoretical value of the standardized normal distribution) corresponding to the value of (B).F: Sample size calculation.nis the sample size used in the research.G: Number of articles classified as "not relevant by the panel of specialists" based on the value of "n".H: Number of articles classified as "relevant by the panel of specialists"based on the value of "n".I: Number of articles that are inferred to be classified as "not relevant by the panel of specialists" based on the value of "A".J: Number of articles that are inferred to be classified as "relevant by the panel of specialists" based on the value of "A".
ABCDEFnGHIJ65090% 0.077 5%1.9676100 68 (68%) 32 (32%) 4422803.2. F 1 Score Calculation3.2.1. Defining the Ranking of Relevant Articles as a Success</p>
<p>Table 10 .
10
Truth table for relevant articles.
875 • 0.2188 0.875 + 0.2188= 0.35</p>
<p>Table 11 .
11
Truth table for Non-relevant articles.
Panel: Non-Relevant ArticleYesNoAlgorithm: Non-relevant articleYes NoTP = 67 FN = 1FP = 25 TN = 7P =TP TP + FP=67 67 + 25= 0.7283R =TP TP + FN=67 67 + 1= 0.9853</p>
<p>Table 12 .
12
Research papers and knowledge areas.
Title</p>
<p>Table 12 .
12
Cont.
Title
Funding: This research was supported by CONACYT through scholarship grant number 1078852.Data Availability Statement: All data related to this article are included within the manuscript.Conflicts of Interest:The authors declare no conflicts of interest.The funder had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or in the decision to publish the results.
Measuring open access uptake: Methods and metrics to assess a market transformation. A Y Abdin, F De Pretis, 10.3390/publications12040033Publications. 124332024</p>
<p>Innovating patent retrieval: A comprehensive review of techniques, trends, and challenges in prior art searches. A Ali, A Tufail, L C De Silva, P E Abas, 10.3390/asi7050091Applied System Innovation. 75912024</p>
<p>OPT-R: Exploring the role of explanations in finetuning and prompting for reasoning skills of large language models. B Alkhamissi, S Verma, P Yu, Z Jin, C Asli, M Diab, 10.18653/v1/2023.nlrse-1.10Proceedings of the 1st workshop on natural language reasoning and structured explanations (NLRSE). the 1st workshop on natural language reasoning and structured explanations (NLRSE)Association for Computational Linguistics2023</p>
<p>Harnessing the power of ChatGPT for automating systematic review process: Methodology, case study, limitations, and future directions. A Alshami, M Elsayed, E Ali, A E E Eltoukhy, T Zayed, 10.3390/systems11070351Systems. 1172023351. [CrossRef</p>
<p>arXiv API access. 2025. 2 February 2025</p>
<p>Information and documentation-Criteria for the assessment of information retrieval systems (UNE 50-121-91). 1991Asociación Española de Normalización y Certificación</p>
<p>Artificial intelligence to automate the systematic review of scientific literature. C W Cleverdon, 10.1007/s00607-023-01181-xInformation Services &amp; Use. de la Torre-López, J., Ramírez, A., &amp; Romero, J. R.41984. 2023Computing</p>
<p>Semi-automatic systematic literature reviews and information extraction of COVID-19 scientific evidence: Description and preliminary results of the COKE project. D Golinelli, A G Nuzzolese, F Sanmarchi, L Bulla, M Mongiovì, A Gangemi, P Rucci, 10.3390/info13030117Information. 1331172022</p>
<p>A survey of control-chart pattern-recognition literature (1991-2010) based on a new conceptual classification scheme. W Hachicha, A Ghorbel, 10.1016/j.cie.2012.03.002Computers &amp; Industrial Engineering. 632012</p>
<p>Information and documentation-Methods for assessing the quality of information retrieval systems. A Ikarashi, 10.1038/d41586-023-03290-1International Organization for Standardization. 6232023. 1985. 1985ISO. International Organization for Standardization</p>
<p>FaN-REMs: Fair and normalized retrieval evaluation metrics for learning retrieval systems. A Jaiswal, M Kumar, A R Pathak, K Y Yigzaw, 10.1109/ACCESS.2024.3514916IEEE Access. 122024</p>
<p>ProtoCode: Leveraging large language models (LLMs) for automated generation of machine-readable PCR protocols from scientific publications. S Jiang, D Evans-Yamamoto, D Bersenev, S K Palaniappan, A Yachie-Kinoshita, 10.1016/j.slast.2024.100134SLAS Technology. 292024. 100134</p>
<p>Reliable information retrieval systems performance evaluation: A review. M H Joseph, S D Ravana, 10.1109/ACCESS.2024.3377239IEEE Access. 122024</p>
<p>Exploratory bibliometrics: Using VOSviewer as a preliminary research tool. A Kirby, 10.3390/publications11010010Publications. 1112023</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, 10.1145/3560815ACM Computing Surveys. 5591952023</p>
<p>Guidance for generative ai in education and research. C D Manning, P Raghavan, H Schütze, F Miao, W Holmes, 10.54675/EWZM95352008. 2023Cambridge University PressIntroduction to information retrieval</p>
<p>Blockchain and its application in the peer review of scientific works: A systematic review. C H Morales-Alarcón, E Bodero-Poveda, H M Villa-Yánez, P A Buñay-Guisñan, 10.3390/publications12040040Publications. 124402024</p>
<p>Automating the practice of science-Opportunities, challenges, and implications. S Musslick, L K Bartlett, S H Chandramouli, M Dubovad, F Gobet, T L Griffiths, W R Holmes, 10.1073/pnas.24012381212025122Proceedings of the National Academy of Sciences USAe2401238121. [CrossRef</p>
<p>Towards the automation of systematic reviews using natural language processing, machine learning, and deep learning: A comprehensive review. R Ofori-Boateng, M Aceves-Martins, N Wiratunga, C F Moreno-Garcia, 10.1007/s10462-024-10844-wArtificial Intelligence Review. 572024. 200</p>
<p>Evaluation of a prototype machine learning tool to semi-automate data extraction for systematic literature reviews. A Panayi, K Ward, A Benhadji-Schaff, A S Ibanez-Lopez, A Xia, R Barzilay, 10.1186/s13643-023-02351-wSystematic Reviews. 122023</p>
<p>RAPTOR: Recursive abstractive processing for tree-organized retrieval. P Sarthi, S Abdullah, A Tuli, S Khanna, A Goldie, C D Manning, The Twelfth International Conference on Learning Representations. Vienna, Austria2024. May 7-11</p>
<p>It's not just size that matters: Small language models are also few-shot learners. T Schick, H Schütze, Proceedings of the 2021 conference of the north american chapter of the association for computational linguistics: Human language technologies. the 2021 conference of the north american chapter of the association for computational linguistics: Human language technologiesAssociation for Computational Linguistics2021</p>
<p>An empirical evaluation of prompting strategies for large language models in zero-shot clinical natural language processing: Algorithm development and validation study. S Sivarajkumar, M Kelley, A Samolyk-Mazzanti, S Visweswaran, Y Wang, 10.2196/55318JMIR Medical Informatics. 122024e55318. [CrossRef</p>
<p>Open science alternatives to scopus and the web of science: A case study in regional resilience. I D Turgel, O A Chernova, 10.3390/publications12040043Publications. 124432024</p>
<p>A survey of prompt engineering methods in large language models for different NLP tasks. S Vatsal, H Dubey, arXiv2024</p>
<p>Yuan 1.0: Large-scale pre-trained language model in zero-shot and few-shot learning. S Wu, X Zhao, T Yu, R Zhang, C Shen, H Liu, F Li, H Zhu, J Luo, L Xu, X Zhang, 10.48550/arXiv.2110.04725arXiv2021</p>
<p>Methods and technologies for supporting knowledge sharing within learning communities: A systematic literature review. M Zamiri, A Esmaeili, 10.3390/admsci14010017Administrative Sciences. 1412024</p>
<p>Disclaimer/Publisher's Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods. instructions or products referred to in the content</p>            </div>
        </div>

    </div>
</body>
</html>
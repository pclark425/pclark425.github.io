<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1524 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1524</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1524</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-45d2d80e95fa81c7f8d2813b36e6bbffde782e92</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/45d2d80e95fa81c7f8d2813b36e6bbffde782e92" target="_blank">BayesSim: adaptive domain randomization via probabilistic inference for robotics simulators</a></p>
                <p><strong>Paper Venue:</strong> Robotics: Science and Systems</p>
                <p><strong>Paper TL;DR:</strong> Results show that the posterior computed from BayesSim can be used for domain randomization outperforming alternative methods that randomize based on uniform priors.</p>
                <p><strong>Paper Abstract:</strong> We introduce BayesSim, a framework for robotics simulations allowing a full Bayesian treatment for the parameters of the simulator. As simulators become more sophisticated and able to represent the dynamics more accurately, fundamental problems in robotics such as motion planning and perception can be solved in simulation and solutions transferred to the physical robot. However, even the most complex simulator might still not be able to represent reality in all its details either due to inaccurate parametrization or simplistic assumptions in the dynamic models. BayesSim provides a principled framework to reason about the uncertainty of simulation parameters. Given a black box simulator (or generative model) that outputs trajectories of state and action pairs from unknown simulation parameters, followed by trajectories obtained with a physical robot, we develop a likelihood-free inference method that computes the posterior distribution of simulation parameters. This posterior can then be used in problems where Sim2Real is critical, for example in policy search. We compare the performance of BayesSim in obtaining accurate posteriors in a number of classical control and robotics problems. Results show that the posterior computed from BayesSim can be used for domain randomization outperforming alternative methods that randomize based on uniform priors.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1524.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1524.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI Gym</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI Gym</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely-used toolkit of reinforcement learning environments (classic control, MuJoCo-based, etc.) used in this paper to run classic control benchmarks (CartPole, Pendulum, MountainCar, Acrobot) and Fetch tasks for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Openai gym</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>OpenAI Gym (classic control and Gym wrappers)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A collection/framework of RL environments; provides classic control simulators (CartPole, Pendulum, MountainCar, Acrobot) and wrappers to other physics backends. Environments implement dynamics (typically as discrete-time differential-equation solvers) and provide state/action trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics (control)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-fidelity classic-control simulators (idealized, simplified dynamical models); when Gym wraps higher-fidelity engines (MuJoCo) it inherits their fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Models discrete-time dynamics; includes parameters such as pole mass, pole length, timestep (dt), engine dynamics; simplifies many real-world effects (sensor noise, unmodelled contacts) — used as black-box dynamical simulators in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>PPO (for CartPole and classic control), BayesSim posterior estimator</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PPO: on-policy deep RL optimizer; BayesSim: likelihood-free posterior estimator implemented as a mixture density random Fourier / neural-feature network trained on simulator-generated state-action statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>System identification (infer simulator parameters from trajectories) and control policy learning (balance CartPole, pendulum swing-up, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>CartPole posterior estimation: BayesSim RFF provided higher log-probabilities for true parameters (see Table I: e.g., pole mass log-probability 0.973 ± 0.26 for BayesSim RFF); PPO policies trained for 2M timesteps (qualitative improvements reported, no numeric reward reported in table).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Evaluation across parameter ranges and intended Sim2Real transfer (paper frames goal as transfer to real robot/real parameters but experiments evaluate robustness across simulator parameter variations).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Policies trained by randomizing using BayesSim posterior were reported as significantly more robust and more peaked around actual parameter values compared to uniform prior randomization (qualitative; plots show higher average reward and lower variance at true parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>No explicit minimal-fidelity specification; paper argues that even sophisticated simulators may miss aspects of reality (inaccurate parametrization, simplifying assumptions, numerical solver precision) and proposes Bayesian posterior over simulator parameters to ameliorate Sim2Real mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Uniform prior domain randomization gave poor performance for open-loop Fetch Slide (discussed in Gym/MuJoCo Fetch experiments) — indicating that naive wide randomization can fail when actions are highly sensitive to friction/dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BayesSim: adaptive domain randomization via probabilistic inference for robotics simulators', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1524.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1524.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PyBullet (bullet physics wrapper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A physics engine used in experiments (Hopper task) to simulate robot dynamics and collect trajectories for BayesSim posterior estimation and policy evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>An open-source physics engine implementing rigid-body dynamics, contacts and friction; used here for the Hopper control benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics (locomotion)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-fidelity rigid-body dynamics simulator (numerical contact and friction modelling); realistic-but-simplified compared to real hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Models contacts, lateral friction and joint dynamics with discrete timestepping and numerical solvers; nonetheless can simplify contact physics, sensor noise, and high-frequency dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>BayesSim posterior estimator; (agent evaluated: Hopper policy trained in PyBullet)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BayesSim: mixture-density random Fourier / neural feature network for likelihood-free posterior estimation; RL agents (not named for Hopper here) are standard policy search algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Inference of dynamics parameter (e.g., lateral friction) from simulated trajectories and training/robustness evaluation of locomotion policies.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Posterior log-probability (Hopper lateral friction) — BayesSim RFF: 2.622 ± 0.64 (Table I) indicating concentrated posterior around true friction vs baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Intended transfer to real-world robot parameters / evaluation across parameter grid in simulator (no physical robot trial reported).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Not reported as real-world numbers; BayesSim-based posterior randomization produced more peaked/stable posteriors and was argued to produce more robust policies (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>No explicit minimum fidelity required; notes that numerical precision and simplistic models can affect transfer and so Bayesian parameter posteriors can compensate for parameter uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No task-specific failure cases beyond general discussion; table shows baseline methods sometimes had higher variance in posterior estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BayesSim: adaptive domain randomization via probabilistic inference for robotics simulators', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1524.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1524.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuJoCo: A physics engine for model-based control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-performance physics engine used for Fetch Push and Fetch Slide tasks in the paper to simulate contact-rich manipulation; used both to generate training data and to evaluate BayesSim and RL policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mujoco: A physics engine for model-based control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A high-performance rigid-body physics engine widely used for robotics research; simulates articulated bodies, contacts, and forces for manipulation and locomotion tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics (manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>high-to-medium fidelity physics engine for model-based control (accurate contact dynamics and articulated rigid-body dynamics, but still an approximation of real-world contacts and surface interactions).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Simulates contacts, friction coefficients, object masses and articulated robot dynamics using numerical solvers; however real-world complexities (fine-grained surface properties, sensor noise, unmodelled deformable contacts) may be absent.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>BayesSim posterior estimator; DDPG + HER (for initial policy training used to generate data); PPO (for some policy experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BayesSim: conditional density estimator (mixture of Gaussians parameterized by random Fourier or neural features) used for likelihood-free inference of simulator parameters; RL agents: DDPG with Hindsight Experience Replay for Fetch tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Estimate friction coefficient from trajectories (system identification) and learn robust manipulation policies (push and slide tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Posterior estimation (Fetch Push friction): BayesSim RFF log-probability 2.423 ± 0.07; (Fetch Slide friction): BayesSim RFF 2.391 ± 0.06 (Table I). RL training: DDPG+HER used for policy used to generate data (200 epochs, 100 episodes/epoch); BayesSim-based posterior randomization yielded higher rewards around true friction in evaluation (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Intended transfer to real robot / real friction parameters; in experiments transfer is operationalized as robustness across parameter values and recovery of posterior around true friction (sim-to-(true-parameter) evaluation within simulator).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Quantitatively: BayesSim posterior concentrated near true friction and led to policies with higher evaluated rewards around true friction; uniform prior randomization produced flat/poor performance for the open-loop Slide task (qualitative description; no real-robot numeric results).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors emphasize that accurate parametrization (e.g., friction) is crucial for transfer in contact-sensitive tasks (Slide) and that domain randomization from the posterior can be more effective than uniform randomization; no explicit minimum fidelity rule is stated.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Uniform wide prior randomization failed for the Fetch Slide open-loop task — poor performance because actions are highly sensitive to friction, and the robot cannot correct trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BayesSim: adaptive domain randomization via probabilistic inference for robotics simulators', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1524.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1524.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Black-box simulator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Black-box generative simulator (dynamical simulator treated as black box)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generic concept used throughout the paper: simulators are treated as black-box generative models g(θ) that map simulator parameters θ to simulated trajectories x^s; BayesSim performs likelihood-free inference without requiring internal equations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Black-box simulator (general concept used in BayesSim)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Any simulator that outputs state-action trajectories given parameters; treated as an implicit generative model (possibly closed-source) where the likelihood is intractable and only forward simulation is available.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general dynamical systems (robotics experiments in this paper; concept applicable to many domains including biology, thermodynamics, circuits as noted in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>variable — paper explicitly allows arbitrary fidelity (from simplified to physically accurate/photo-realistic); BayesSim is designed to operate irrespective of fidelity by inferring parameter posteriors.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Assumes simulator encapsulates dynamical differential equations solved numerically; may suffer from inaccurate parametrization, simplistic dynamic models, or numerical precision limits; BayesSim aims to capture uncertainty over simulator parameters to mitigate resulting Sim2Real gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>BayesSim (mixture density random Fourier network estimator); RL agents trained under domain randomization</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BayesSim: likelihood-free inference framework that learns q_phi(theta | x) as a mixture of Gaussians with features from random Fourier features or neural nets; RL agents: PPO, DDPG+HER used to train control policies under randomized simulator parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Likelihood-free Bayesian inference (system identification) from trajectories; learning robust control policies via domain randomization using the learned posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>BayesSim posterior estimation yields higher log-probabilities for true parameters compared to Rejection ABC and comparable to ε-Free (see Table I; many tasks show BayesSim RFF log-probabilities >1–3 depending on task).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world robot parameters / real system behaviour (conceptual target); in experiments transfer is evaluated as policy robustness across parameter sweeps and recovery of true simulator parameters from observed trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>BayesSim-informed domain randomization produced policies more robust at true parameter values and higher evaluated rewards in sensitive tasks (Fetch Slide), versus uniform prior randomization which often produced poorer results in open-loop tasks; quantitative performance reported as posterior log-probabilities (see Table I); no physical robot transfer results reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper does not prescribe a minimal fidelity but emphasizes that accurate parameterization matters and that Bayesian posterior over parameters can compensate for missing/uncertain simulator aspects; suggests more work needed on learned sufficient statistics and end-to-end approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>General failure modes described: simplistic models, inaccurate parameters, and insufficient numerical precision can harm Sim2Real; concrete experimental failure: uniform prior randomization fails on Fetch Slide task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BayesSim: adaptive domain randomization via probabilistic inference for robotics simulators', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Sim-to-real: Learning agile locomotion for quadruped robots <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Closing the sim-to-real loop: Adapting simulation randomization with real world experience <em>(Rating: 2)</em></li>
                <li>Fast ε-free inference of simulation models with bayesian conditional density estimation <em>(Rating: 2)</em></li>
                <li>Mujoco: A physics engine for model-based control <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1524",
    "paper_id": "paper-45d2d80e95fa81c7f8d2813b36e6bbffde782e92",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "OpenAI Gym",
            "name_full": "OpenAI Gym",
            "brief_description": "A widely-used toolkit of reinforcement learning environments (classic control, MuJoCo-based, etc.) used in this paper to run classic control benchmarks (CartPole, Pendulum, MountainCar, Acrobot) and Fetch tasks for experiments.",
            "citation_title": "Openai gym",
            "mention_or_use": "use",
            "simulator_name": "OpenAI Gym (classic control and Gym wrappers)",
            "simulator_description": "A collection/framework of RL environments; provides classic control simulators (CartPole, Pendulum, MountainCar, Acrobot) and wrappers to other physics backends. Environments implement dynamics (typically as discrete-time differential-equation solvers) and provide state/action trajectories.",
            "scientific_domain": "mechanics / robotics (control)",
            "fidelity_level": "medium-fidelity classic-control simulators (idealized, simplified dynamical models); when Gym wraps higher-fidelity engines (MuJoCo) it inherits their fidelity.",
            "fidelity_characteristics": "Models discrete-time dynamics; includes parameters such as pole mass, pole length, timestep (dt), engine dynamics; simplifies many real-world effects (sensor noise, unmodelled contacts) — used as black-box dynamical simulators in experiments.",
            "model_or_agent_name": "PPO (for CartPole and classic control), BayesSim posterior estimator",
            "model_description": "PPO: on-policy deep RL optimizer; BayesSim: likelihood-free posterior estimator implemented as a mixture density random Fourier / neural-feature network trained on simulator-generated state-action statistics.",
            "reasoning_task": "System identification (infer simulator parameters from trajectories) and control policy learning (balance CartPole, pendulum swing-up, etc.)",
            "training_performance": "CartPole posterior estimation: BayesSim RFF provided higher log-probabilities for true parameters (see Table I: e.g., pole mass log-probability 0.973 ± 0.26 for BayesSim RFF); PPO policies trained for 2M timesteps (qualitative improvements reported, no numeric reward reported in table).",
            "transfer_target": "Evaluation across parameter ranges and intended Sim2Real transfer (paper frames goal as transfer to real robot/real parameters but experiments evaluate robustness across simulator parameter variations).",
            "transfer_performance": "Policies trained by randomizing using BayesSim posterior were reported as significantly more robust and more peaked around actual parameter values compared to uniform prior randomization (qualitative; plots show higher average reward and lower variance at true parameters).",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "No explicit minimal-fidelity specification; paper argues that even sophisticated simulators may miss aspects of reality (inaccurate parametrization, simplifying assumptions, numerical solver precision) and proposes Bayesian posterior over simulator parameters to ameliorate Sim2Real mismatch.",
            "failure_cases": "Uniform prior domain randomization gave poor performance for open-loop Fetch Slide (discussed in Gym/MuJoCo Fetch experiments) — indicating that naive wide randomization can fail when actions are highly sensitive to friction/dynamics.",
            "uuid": "e1524.0",
            "source_info": {
                "paper_title": "BayesSim: adaptive domain randomization via probabilistic inference for robotics simulators",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "PyBullet",
            "name_full": "PyBullet (bullet physics wrapper)",
            "brief_description": "A physics engine used in experiments (Hopper task) to simulate robot dynamics and collect trajectories for BayesSim posterior estimation and policy evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "PyBullet",
            "simulator_description": "An open-source physics engine implementing rigid-body dynamics, contacts and friction; used here for the Hopper control benchmark.",
            "scientific_domain": "mechanics / robotics (locomotion)",
            "fidelity_level": "medium-fidelity rigid-body dynamics simulator (numerical contact and friction modelling); realistic-but-simplified compared to real hardware.",
            "fidelity_characteristics": "Models contacts, lateral friction and joint dynamics with discrete timestepping and numerical solvers; nonetheless can simplify contact physics, sensor noise, and high-frequency dynamics.",
            "model_or_agent_name": "BayesSim posterior estimator; (agent evaluated: Hopper policy trained in PyBullet)",
            "model_description": "BayesSim: mixture-density random Fourier / neural feature network for likelihood-free posterior estimation; RL agents (not named for Hopper here) are standard policy search algorithms.",
            "reasoning_task": "Inference of dynamics parameter (e.g., lateral friction) from simulated trajectories and training/robustness evaluation of locomotion policies.",
            "training_performance": "Posterior log-probability (Hopper lateral friction) — BayesSim RFF: 2.622 ± 0.64 (Table I) indicating concentrated posterior around true friction vs baselines.",
            "transfer_target": "Intended transfer to real-world robot parameters / evaluation across parameter grid in simulator (no physical robot trial reported).",
            "transfer_performance": "Not reported as real-world numbers; BayesSim-based posterior randomization produced more peaked/stable posteriors and was argued to produce more robust policies (qualitative).",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "No explicit minimum fidelity required; notes that numerical precision and simplistic models can affect transfer and so Bayesian parameter posteriors can compensate for parameter uncertainty.",
            "failure_cases": "No task-specific failure cases beyond general discussion; table shows baseline methods sometimes had higher variance in posterior estimates.",
            "uuid": "e1524.1",
            "source_info": {
                "paper_title": "BayesSim: adaptive domain randomization via probabilistic inference for robotics simulators",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "MuJoCo",
            "name_full": "MuJoCo: A physics engine for model-based control",
            "brief_description": "A high-performance physics engine used for Fetch Push and Fetch Slide tasks in the paper to simulate contact-rich manipulation; used both to generate training data and to evaluate BayesSim and RL policies.",
            "citation_title": "Mujoco: A physics engine for model-based control",
            "mention_or_use": "use",
            "simulator_name": "MuJoCo",
            "simulator_description": "A high-performance rigid-body physics engine widely used for robotics research; simulates articulated bodies, contacts, and forces for manipulation and locomotion tasks.",
            "scientific_domain": "mechanics / robotics (manipulation)",
            "fidelity_level": "high-to-medium fidelity physics engine for model-based control (accurate contact dynamics and articulated rigid-body dynamics, but still an approximation of real-world contacts and surface interactions).",
            "fidelity_characteristics": "Simulates contacts, friction coefficients, object masses and articulated robot dynamics using numerical solvers; however real-world complexities (fine-grained surface properties, sensor noise, unmodelled deformable contacts) may be absent.",
            "model_or_agent_name": "BayesSim posterior estimator; DDPG + HER (for initial policy training used to generate data); PPO (for some policy experiments)",
            "model_description": "BayesSim: conditional density estimator (mixture of Gaussians parameterized by random Fourier or neural features) used for likelihood-free inference of simulator parameters; RL agents: DDPG with Hindsight Experience Replay for Fetch tasks.",
            "reasoning_task": "Estimate friction coefficient from trajectories (system identification) and learn robust manipulation policies (push and slide tasks).",
            "training_performance": "Posterior estimation (Fetch Push friction): BayesSim RFF log-probability 2.423 ± 0.07; (Fetch Slide friction): BayesSim RFF 2.391 ± 0.06 (Table I). RL training: DDPG+HER used for policy used to generate data (200 epochs, 100 episodes/epoch); BayesSim-based posterior randomization yielded higher rewards around true friction in evaluation (qualitative).",
            "transfer_target": "Intended transfer to real robot / real friction parameters; in experiments transfer is operationalized as robustness across parameter values and recovery of posterior around true friction (sim-to-(true-parameter) evaluation within simulator).",
            "transfer_performance": "Quantitatively: BayesSim posterior concentrated near true friction and led to policies with higher evaluated rewards around true friction; uniform prior randomization produced flat/poor performance for the open-loop Slide task (qualitative description; no real-robot numeric results).",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Authors emphasize that accurate parametrization (e.g., friction) is crucial for transfer in contact-sensitive tasks (Slide) and that domain randomization from the posterior can be more effective than uniform randomization; no explicit minimum fidelity rule is stated.",
            "failure_cases": "Uniform wide prior randomization failed for the Fetch Slide open-loop task — poor performance because actions are highly sensitive to friction, and the robot cannot correct trajectories.",
            "uuid": "e1524.2",
            "source_info": {
                "paper_title": "BayesSim: adaptive domain randomization via probabilistic inference for robotics simulators",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "Black-box simulator",
            "name_full": "Black-box generative simulator (dynamical simulator treated as black box)",
            "brief_description": "Generic concept used throughout the paper: simulators are treated as black-box generative models g(θ) that map simulator parameters θ to simulated trajectories x^s; BayesSim performs likelihood-free inference without requiring internal equations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "Black-box simulator (general concept used in BayesSim)",
            "simulator_description": "Any simulator that outputs state-action trajectories given parameters; treated as an implicit generative model (possibly closed-source) where the likelihood is intractable and only forward simulation is available.",
            "scientific_domain": "general dynamical systems (robotics experiments in this paper; concept applicable to many domains including biology, thermodynamics, circuits as noted in related work)",
            "fidelity_level": "variable — paper explicitly allows arbitrary fidelity (from simplified to physically accurate/photo-realistic); BayesSim is designed to operate irrespective of fidelity by inferring parameter posteriors.",
            "fidelity_characteristics": "Assumes simulator encapsulates dynamical differential equations solved numerically; may suffer from inaccurate parametrization, simplistic dynamic models, or numerical precision limits; BayesSim aims to capture uncertainty over simulator parameters to mitigate resulting Sim2Real gaps.",
            "model_or_agent_name": "BayesSim (mixture density random Fourier network estimator); RL agents trained under domain randomization",
            "model_description": "BayesSim: likelihood-free inference framework that learns q_phi(theta | x) as a mixture of Gaussians with features from random Fourier features or neural nets; RL agents: PPO, DDPG+HER used to train control policies under randomized simulator parameters.",
            "reasoning_task": "Likelihood-free Bayesian inference (system identification) from trajectories; learning robust control policies via domain randomization using the learned posterior.",
            "training_performance": "BayesSim posterior estimation yields higher log-probabilities for true parameters compared to Rejection ABC and comparable to ε-Free (see Table I; many tasks show BayesSim RFF log-probabilities &gt;1–3 depending on task).",
            "transfer_target": "Real-world robot parameters / real system behaviour (conceptual target); in experiments transfer is evaluated as policy robustness across parameter sweeps and recovery of true simulator parameters from observed trajectories.",
            "transfer_performance": "BayesSim-informed domain randomization produced policies more robust at true parameter values and higher evaluated rewards in sensitive tasks (Fetch Slide), versus uniform prior randomization which often produced poorer results in open-loop tasks; quantitative performance reported as posterior log-probabilities (see Table I); no physical robot transfer results reported in this paper.",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Paper does not prescribe a minimal fidelity but emphasizes that accurate parameterization matters and that Bayesian posterior over parameters can compensate for missing/uncertain simulator aspects; suggests more work needed on learned sufficient statistics and end-to-end approaches.",
            "failure_cases": "General failure modes described: simplistic models, inaccurate parameters, and insufficient numerical precision can harm Sim2Real; concrete experimental failure: uniform prior randomization fails on Fetch Slide task.",
            "uuid": "e1524.3",
            "source_info": {
                "paper_title": "BayesSim: adaptive domain randomization via probabilistic inference for robotics simulators",
                "publication_date_yy_mm": "2019-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2
        },
        {
            "paper_title": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "rating": 2
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2
        },
        {
            "paper_title": "Closing the sim-to-real loop: Adapting simulation randomization with real world experience",
            "rating": 2
        },
        {
            "paper_title": "Fast ε-free inference of simulation models with bayesian conditional density estimation",
            "rating": 2
        },
        {
            "paper_title": "Mujoco: A physics engine for model-based control",
            "rating": 1
        }
    ],
    "cost": 0.014596749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>BayesSim: adaptive domain randomization via probabilistic inference for robotics simulators</h1>
<p>Fabio Ramos<em> ${ }^{</em> \dagger}$ Rafael Carvalhaes Possas<em> ${ }^{</em> \dagger}$ Dieter Fox<em> ${ }^{</em>}$<br>*NVIDIA ${ }^{\dagger}$ University of Sydney ${ }^{\ddagger}$ University of Washington</p>
<h4>Abstract</h4>
<p>We introduce BayesSim ${ }^{1}$, a framework for robotics simulations allowing a full Bayesian treatment for the parameters of the simulator. As simulators become more sophisticated and able to represent the dynamics more accurately, fundamental problems in robotics such as motion planning and perception can be solved in simulation and solutions transferred to the physical robot. However, even the most complex simulator might still not be able to represent reality in all its details either due to inaccurate parametrization or simplistic assumptions in the dynamic models. BayesSim provides a principled framework to reason about the uncertainty of simulation parameters. Given a black box simulator (or generative model) that outputs trajectories of state and action pairs from unknown simulation parameters, followed by trajectories obtained with a physical robot, we develop a likelihood-free inference method that computes the posterior distribution of simulation parameters. This posterior can then be used in problems where Sim2Real is critical, for example in policy search. We compare the performance of BayesSim in obtaining accurate posteriors in a number of classical control and robotics problems. Results show that the posterior computed from BayesSim can be used for domain randomization outperforming alternative methods that randomize based on uniform priors.</p>
<h2>I. INTRODUCTION</h2>
<p>Simulators are emerging as one of the most important tools for efficient learning in robotics. With physically accurate and photo-realistic simulation, perception models and control policies can be trained more easily before being transferred to real robots, saving both time and costs of running complex experiments. Unfortunately, in many cases, models and policies trained in simulation are not seamlessly transferable to the real systems. Lack of knowledge about the correct simulation parameters, oversimplified simulation models, or insufficient numerical precision for differential equation solvers can all play a significant role in this problem. To ameliorate this problem, a popular approach is to sample different simulation parameters during training and thereby learn models that are robust to simulation perturbations. This approach, often referred to as domain randomization (DR), has been shown to perform surprisingly well in areas such as learning to control a humanoid robot [23], manipulate table top objects [38], estimating 6D object poses from images [17], or dexterous in-hand manipulation [2].</p>
<p>A crucial question regarding domain randomization is which simulation parameters to randomize over and from which distributions to sample their values from. Typically, these parameters and their distributions are determined in a manual</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>process by iteratively testing whether a model learned in randomized simulation works well on the real system. If the model does not work on the real robot, the randomization parameters are changed so that they better cover the conditions observed in the real world. To overcome this manual tuning process, [10] recently showed how policy executions on a real robot can be used to automatically update a Gaussian distribution over the sampling parameters such that the simulator better matches reality. However, by restricting sampling distributions to Gaussians, this approach cannot model more complex uncertainties and dependencies among parameters. Alternatively, one could perform system identification to better estimate simulation parameters from the real data. Since most of these techniques assume that the simulation equations are known and only provide point estimates for the parameters, they do not account for the uncertainties associated with the measurement process, numerical precision of differential equation solvers, or simplistic models [14].</p>
<p>In this paper we provide a principled Bayesian method to compute full posteriors over simulator parameters, thereby overcoming the limitations of previous approaches. Our technique, called BayesSim, leverages recent advances in likelihood-free inference for Bayesian analysis to update posteriors over simulation parameters based on small sets of observations obtained on the real system. The main difficulty in computing such posteriors relates to the evaluation of the likelihood function, which models the relationship between simulation parameters and corresponding system behavior, or observations in the real world. While a simulator implicitly defines this relationship, the likelihood function requires the inverse of the simulator model, i.e., how observed system behavior can be used to derive corresponding simulation parameters. Importantly, BayesSim does not assume access to the internal differential equations underlying the simulator and treats the simulator as a black box.</p>
<p>We make the following contributions: First, we introduce BayesSim as a generic framework for probabilistic inference with robotics simulators and show that it can provide a full space of simulation parameters that best fit observed data. This is in contrast to traditional system identification methods that only provide the best fitting solution. Second, we propose a novel mixture density random Fourier network to approximate the conditional distribution $p(\boldsymbol{\theta} \mid \mathbf{x}^{r})$ directly by learning from pairs $\left{\boldsymbol{\theta}<em i="i">{i}, \mathbf{x}</em>$ generated from the proposal prior and the simulator. Finally, we show that learning policies with domain randomization where the simulator parameters are randomized}^{s}\right}_{i=1}^{N</p>
<p>according to the posterior provided by BayesSim generates policies that are significantly more robust and easier to train than randomization directly from the prior.</p>
<h2>II. Related Work</h2>
<p>Simulators accelerate machine learning impact by allowing faster, highly-scalable and low cost data collection. Many other scientific domains such as economics [15], evolutionary biology [4] and cosmology [29] also rely on simulator-based modelling to provide further advancements in research. In robotics, "reality gap" is not only seen in control, robotics vision is also affected by this problem [38]. Algorithms trained on images from a simulation can frequently fail on different environments as the appearance of the world can differ greatly from one system to the other.</p>
<p>Randomizing the dynamics of a simulator while training a control policy has proven to mitigate the reality gap problem [25]. Simulation parameters could vary from physical settings like damping, friction and object masses [25] to visual parameters like objects textures, shapes and etc [38]. Another similar approach is that of adding noise to the system parameters [37] instead of sampling new parameters from a uniform prior distribution. Perturbation can also be seen on robot locomotion [22] where planning is done through an ensemble of perturbed models. Lastly, interleaving policy roll outs between simulation and reality has also proven to work well on swing-peg-in-hole and opening a cabinet drawer tasks [11].</p>
<p>Learning models from simulations of data can leverage one’s understanding of the physical world potentially helping to solve the aforementioned problem. Until recently, Approximate Bayesian Computation [4] has been one of the main methods used to tackle this type of problem. Rejection ABC [26] is the most basic method where parameter settings are accepted/rejected if they are within a certain specified range. The set of accepted parameters approximates the posterior for the real parameters. Markov Chain Monte Carlo ABC (MCMCABC) [20] improves over its precedent by perturbing accepted parameters instead of independently proposing new parameters. Lastly, Sequential Monte Carlo ABC (SMC-ABC) [6] leverages sequential importance sampling to simulate slowly-change distributions where the last one is an approximation of the true parameter posterior. In this work, we use a $\epsilon$-free approach [24] for likelihood-free inference, where a Mixture of Density Random Fourier Network estimates the parameters of the true posterior through a Gaussian mixture.</p>
<p>A wide range of complex robotics control problems have been recently solved using Deep Reinforcement Learning (Deep RL) techniques [2, 25, 37]. Classic control problems like Pendulum, Mountain Car, Acrobot and Cartpole have been successfully tackled using policy search with algorithms like Trust Region Policy Optimization (TRPO) [32] and Proximal Policy Optimization (PPO) [33]. More complex tasks in robotics such as the ones in manipulation are still difficult to solve using traditional policy search. Both Push and Slide tasks (Figure 1) on the fetch robot [8] were only solved recently using the combination of Deep Deterministic Policy Gradients (DDPG) [18] and Hindsight Experience Replay (HER) [1].</p>
<h2>III. PRELIMINARIES</h2>
<p>In this section we provide background on likelihood free inference and reinforcement learning. As we shall see, policy search via domain randomization is one of the applications in which BayesSim proved to be valuable.</p>
<h3>III-A. Likelihood-free inference</h3>
<p>BayesSim takes a prior $p(\boldsymbol{\theta})$ over simulation parameters $\boldsymbol{\theta}$, a black box generative model or simulator $\mathbf{x}^{s}=g(\boldsymbol{\theta})$ that generates simulated observations $\mathbf{x}^{s}$ from these parameters, and observations from the physical world $\mathbf{x}^{r}$ to compute the posterior $p(\boldsymbol{\theta}|\mathbf{x}^{s},\mathbf{x}^{r})$. The main difficulty in computing this posterior relates to the evaluation of the likelihood function $p(\mathbf{x}|\boldsymbol{\theta})$ which is defined implicitly from the simulator [12]. Here we assume that the simulator is a set of dynamical differential equations associated with a numerical or analytical solver which are typically intractable and expensive to evaluate. Furthermore, we do not assume these equations are known and treat the simulator as a black box. This allows our method to be utilized with many robotics simulators (even closed source ones) but requires a method where the likelihood cannot be evaluated directly but instead only sampled from, by performing forward simulations. This is referred to in statistics as likelihood-free inference of which the most popular family of algorithms to address it are known as approximate Bayesian computation (ABC) [4, 20, 34].</p>
<p>In ABC, the simulator is used to generate synthetic observations from samples following the parameters prior. These samples are accepted when features or sufficient statistics computed from the synthetic data are similar to those from real observations obtained from physical experiments. As a sampling-based technique, ABC can be notoriously slow to converge, particularly when the dimensionality of the parameter space is large. Formally, ABC approximates the posterior $p(\boldsymbol{\theta}|\mathbf{x}=\mathbf{x}^{r}) \propto p\left(\mathbf{x}=\mathbf{x}^{r} \mid \boldsymbol{\theta}\right) p(\boldsymbol{\theta})$ using the Bayes' rule. However as the likelihood function $p\left(\mathbf{x}=\mathbf{x}^{r} \mid \boldsymbol{\theta}\right)$ is not available, conventional methods for Bayesian inference cannot be applied. ABC sidesteps this problem by approximating $p\left(\mathbf{x}=\mathbf{x}^{r} \mid \boldsymbol{\theta}\right)$ by $p\left(\left|\mathbf{x}-\mathbf{x}^{r}\right|&lt;\epsilon \mid \boldsymbol{\theta}\right)$, where $\epsilon$ is a small value defining a sphere around real observations $\mathbf{x}^{r}$, and using Monte Carlo to estimate its value. The quality of the approximation increases as $\epsilon$ decreases however, the computational cost can become prohibitive as most simulations will not fall within the acceptable region.</p>
<h3>III-B. Reinforcement learning and policy search in robotics</h3>
<p>We consider the default RL scenario where an agent interacts in discrete timesteps with an environment $\mathbf{E}$. At each step $t$ the agent receives an observation $\mathbf{o}<em t="t">{t}$, takes an action $\mathbf{a}</em>}$ and receives a real number reward $r_{t}$. In general, actions in robotics are real valued $\mathbf{a<em t="t">{t} \in \mathbb{R}^{D}$ and environments are usually partially observed so that the entire history of observation, action pairs $\boldsymbol{\eta}=\left{\mathbf{s}</em>}, \mathbf{a<em t="t">{t}, \mathbf{o}</em>$. The goal is to maximize}\right}_{t=0}^{T-1</p>
<p>the expected sum of discounted future rewards by following a policy $\pi(\mathbf{a}<em t="t">{t}|\mathbf{s}</em>$,};\boldsymbol{\beta})$, parametrized by $\boldsymbol{\beta</p>
<p>$J(\boldsymbol{\beta})=\mathbb{E}<em t="0">{\boldsymbol{\eta}}\left[\sum</em>}^{T-1}\gamma^{(t)}r(\mathbf{s<em t="t">{t},\mathbf{a}</em>\right].$ (1)})|\boldsymbol{\beta</p>
<p>Many approaches in reinforcement learning make use of the recursive relationship known as the Bellman equation where $Q^{\pi}$ is the action-value function describing the expected return after taking an action $\mathbf{a}<em t="t">{t}$, in state $\mathbf{s}</em>$ and thereafter following policy $\pi$.</p>
<p>$Q^{\pi}(\mathbf{s}<em t="t">{t},\mathbf{a}</em>})=\mathbb{E<em t="t">{r</em>},s_{t+1}}[r(\mathbf{s<em t="t">{t},\mathbf{a}</em>})+\gamma\mathbb{E<em t_1="t+1">{a</em>}}[Q^{\pi}(\mathbf{s<em t_1="t+1">{t+1},\mathbf{a}</em>)]]$ (2)</p>
<p>In recent years, the advancements in traditional RL methods have allowed their application to control tasks with continuous action spaces. Inheriting ideas from DQN [21], Deep Deterministic Policy Gradients have been relatively successful in a wide range of control problems. The main caveat of DDPG algorithms is that they rely on efficient experience sampling to perform well. Improving the way how experience is collected is one of most important topics in today’s RL community. Experience Replay [19] and Prioritized Experience replay [31] still performs poorly in a repertoire of robotics tasks where the reward signal is sparse. Hindsight Experience replay (HER) [1], on the other hand, performs well in this scenario as it breaks down single trajectories/goals into smaller ones and, thus, provides the policy optimization algorithm with better reward signals. HER has been mostly based in a recent RL concept: Multi-Goal learning with Universal Function Approximators [30].</p>
<p>Another set of successful policy search algorithms is based on optimization through trust regions. They are less sensitive to the experience sampling problem mentioned above. The maximum step size for exploration is determined by its trust region and the optimal point is then evaluated progressively until convergence has been reached. The main idea is that updates are always limited by their own trust region, and, therefore, learning speed is better controlled. Proximal Policy Optimization [33] and Trust Region Policy optimization [32] have applied these ideas providing state of the art performance in a wide range of control problems.</p>
<p>Both techniques differ on the way they sample experiences. While the first is an off-policy algorithm - experiences are generated by a behaviour policy, the second is an on-policy algorithm where the policy used to generated experience is the same used to perform the control task. These algorithms will have comparable performance on different robotics control scenarios therefore should be considered the current state of the art on such problems.</p>
<h2>IV. BayesSim</h2>
<h3>A. Problem setup</h3>
<p>Following [24], BayesSim approximates the intractable posterior $p(\boldsymbol{\theta}|\mathbf{x}=\mathbf{x}^{r})$ by directly learning a conditional density $q_{\phi}(\boldsymbol{\theta}|\mathbf{x})$ parameterised by parameters $\phi$. As we shall see, $q_{\phi}(\boldsymbol{\theta}|\mathbf{x})$ takes the form of a mixture density random feature network. To learn the parameters $\phi$ we first generate a dataset</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Fetch Push and Sliding tasks: the robot has full access to the entire table and multiple iterations with the object (pushing) or one shot at pushing the object to its target (sliding).</p>
<p>with $N$ pairs $(\boldsymbol{\theta}<em n="n">{n}, \mathbf{x}</em>})$ where $\boldsymbol{\theta<em n="n">{n}$ is drawn independently from a distribution $\tilde{p}(\boldsymbol{\theta})$ referred to as the proposal prior. $\mathbf{x}</em>}$ is obtained by running the simulator with parameter $\boldsymbol{\theta<em n="n">{n}$ such that $\mathbf{x}</em>}=g\left(\boldsymbol{\theta<em _phi="\phi">{n}\right)$. In [24] the authors show that $q</em>}(\boldsymbol{\theta}|\mathbf{x})$ is proportional to $\frac{\tilde{p}(\boldsymbol{\theta})}{\tilde{p}(\boldsymbol{\theta})} p(\boldsymbol{\theta}|\mathbf{x})$ when the likelihood $\prod_{n} q_{\phi}\left(\boldsymbol{\theta<em n="n">{n} \mid \mathbf{x}</em>\right)$ is maximised w.r.t. $\phi$. We follow a similar procedure and maximise the log likelihood,</p>
<p>$$
\mathcal{L}(\phi)=\frac{1}{N} \sum_{n} \log q_{\phi}\left(\boldsymbol{\theta}<em n="n">{\boldsymbol{n}} \mid \mathbf{x}</em>\right)
$$</p>
<p>to determine $\phi$. After this is done, an estimate of the posterior is obtained by</p>
<p>$$
\hat{p}(\boldsymbol{\theta} \mid \mathbf{x}=\mathbf{x}^{r}) \propto \frac{p(\boldsymbol{\theta})}{\hat{p}(\boldsymbol{\theta})} q_{\phi}\left(\boldsymbol{\theta} \mid \mathbf{x}=\mathbf{x}^{r}\right)
$$</p>
<p>where $p(\boldsymbol{\theta})$ is the desirable prior that might be different than the proposal prior. In the case when $\hat{p}(\boldsymbol{\theta})=p(\boldsymbol{\theta})$, it follows that $\hat{p}(\boldsymbol{\theta} \mid \mathbf{x}=\mathbf{x}^{r}) \propto q_{\phi}(\boldsymbol{\theta} \mid \mathbf{x}=\mathbf{x}^{r})$. When $\hat{p}(\boldsymbol{\theta}) \neq p(\boldsymbol{\theta})$ we need to adjust the posterior as detailed in Section IV-E.</p>
<h3>B. Mixture density random feature networks</h3>
<p>We model the conditional density $q_{\phi}(\boldsymbol{\theta} \mid \mathbf{x})$ as a mixture of $K$ Gaussians,</p>
<p>$$
q_{\phi}(\boldsymbol{\theta} \mid \mathbf{x})=\sum_{k} \alpha_{k} \mathcal{N}\left(\boldsymbol{\theta} \mid \boldsymbol{\mu}<em k="k">{k}, \boldsymbol{\Sigma}</em>\right)
$$</p>
<p>where $\boldsymbol{\alpha}=\left(\alpha_{1}, \ldots, \alpha_{K}\right)$ are mixing coefficients, $\left{\mu_{k}\right}$ are means and $\left{\Sigma_{k}\right}$ are covariance matrices. This is analogous to mixture density networks [5] except that we replace the feedforward neural network with Quasi Monte Carlo (QMC) random Fourier features when computing $\boldsymbol{\alpha}, \boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$. We justify and describe these features in the next section.</p>
<p>Denoting $\Phi(\boldsymbol{x})$ as the feature vector, the mixing coeficients are calculated as</p>
<p>$$
\boldsymbol{\alpha}=\operatorname{softmax}\left(\mathbf{W}<em _boldsymbol_alpha="\boldsymbol{\alpha">{\boldsymbol{\alpha}} \Phi(\mathbf{x})+\mathbf{b}</em>\right)
$$}</p>
<p>Note that the operator $\operatorname{softmax}\left(\mathbf{z}<em i="i">{i}\right)=\frac{\exp\left(z</em>$ for $i=$ $1, \ldots, K$ enforces that the sum of coeficients equals to 1 and each coefficient is between 0 and 1 .}\right)}{\sum_{k=1}^{K}\exp z_{k}</p>
<p>The means are defined as linear combinations of feature vectors. For each component of the mixture,</p>
<p>$$
\boldsymbol{\mu}<em _boldsymbol_mu="\boldsymbol{\mu">{k}=\mathbf{W}</em><em _boldsymbol_mu="\boldsymbol{\mu">{k}} \Phi(\mathbf{x})+\mathbf{b}</em>
$$}_{k}</p>
<p>Finally we parametrize the covariance matrices as diagonals matrices with</p>
<p>$$
\operatorname{diag}\left(\boldsymbol{\Sigma}<em _boldsymbol_Sigma="\boldsymbol{\Sigma">{k}\right)=\operatorname{mELU}\left(\mathbf{W}</em><em _boldsymbol_Sigma="\boldsymbol{\Sigma">{k}} \Phi(\mathbf{x})+\mathbf{b}</em>\right)
$$}_{k}</p>
<p>where mELU is a modified exponential linear unit defined as</p>
<p>$$
m E L U(z)= \begin{cases}\alpha\left(e^{z}-1\right)+1 &amp; \text { for } z \leq 0 \ z+1 &amp; \text { for } z&gt;0\end{cases}
$$</p>
<p>to enforce positive values. Experimentally this parametrization provided slightly better results than with the exponential function. The diagonal parametrization assumes independence between the dimensions of the simulator parameters $\boldsymbol{\theta}$. This turns out to be not too restrictive if the number of components in the mixture is large enough.</p>
<p>The full set of parameters for the mixture density network is then,</p>
<p>$$
\phi=\left(\mathbf{W}<em _boldsymbol_alpha="\boldsymbol{\alpha">{\boldsymbol{\alpha}}, \mathbf{b}</em>}},\left{\mathbf{W<em k="k">{\boldsymbol{\mu}</em>}}, \mathbf{b<em k="k">{\boldsymbol{\mu}</em>}}, \mathbf{W<em k="k">{\boldsymbol{\Sigma}</em>}}, \mathbf{b<em k="k">{\boldsymbol{\Sigma}</em>\right)
$$}}\right}_{k=1}^{K</p>
<h2>C. Neural Network features</h2>
<p>BayesSim can use neural network features creating a model similar to the mixture density network in [5]. For a feedforward neural network with two fully connected layers, the features take the form</p>
<p>$$
\Phi(\mathbf{x})=\sigma\left(\mathbf{W}<em 1="1">{2}\left(\sigma\left(\mathbf{W}</em>} \mathbf{x}+\mathbf{b<em 2="2">{1}\right)\right)+\mathbf{b}</em>\right)
$$</p>
<p>where $\sigma(\cdot)$ is a sigmoid function; we use $\sigma(\cdot)=\tanh (\cdot)$ in our experiments. This network structure was used in the experiments and compared to the Quasi Monte Carlo random features described below.</p>
<h2>D. Quasi Monte Carlo random features</h2>
<p>BayesSim can use random Fourier features [27] instead of neural nets to parameterise the mixture density. There are several reasons why this can be good choice. Notably, 1) random Fourier features - of which QMC features are a particular type - approximate possibly infinite Hilbert spaces with properties defined by the choice of the associated kernel. In this way prior information about properties of the function space can be readily incorporated by selecting a suitable positive semidefinite kernel; 2) the approximation converges to the original Hilbert space with order $\mathcal{O}(1 / \sqrt{s})$, where $s$ is the number of features, therefore independent of the input dimensionality; 3) experimentally, we verified that mixture densities with random Fourier features are more stable to different initialisations and converge to the same local maximum in most cases.</p>
<p>Random Fourier features approximate a shift invariant kernel $k(\boldsymbol{\tau})$, where $\boldsymbol{\tau}=\left|\mathbf{x}-\mathbf{x}^{\prime}\right|$, by a dot product $k(\boldsymbol{\tau}) \approx \Phi(\mathbf{x})^{T} \Phi(\mathbf{x})$ of finite dimensional features $\Phi(\mathbf{x})$. This is possible by first applying the Bochner's theorem [36] stated below:</p>
<p>Theorem 1 (Bochner's Theorem) A shift invariant kernel $k(\boldsymbol{\tau})$, $\boldsymbol{\tau} \in \mathbb{R}^{D}$, associated with a positive finite measure $d \mu(\boldsymbol{\omega})$ can be represented in terms of its Fourier transform as,</p>
<p>$$
k(\boldsymbol{\tau})=\int_{\mathbb{R}^{D}} e^{-i \boldsymbol{\omega} \cdot \boldsymbol{\tau}} d \mu(\boldsymbol{\omega})
$$</p>
<p>The proof can be found in [13]. When $\mu$ has density $\mathcal{K}(\boldsymbol{\omega})$ then $\mathcal{K}$ represents the spectral distribution for a positive semidefinite $k$. In this case $k(\boldsymbol{\tau})$ and $\mathcal{K}(\boldsymbol{\omega})$ are Fourier duals:</p>
<p>$$
k(\boldsymbol{\tau})=\int \mathcal{K}(\boldsymbol{\omega}) e^{-i \boldsymbol{\omega} \cdot \boldsymbol{\tau}} d \boldsymbol{\omega}
$$</p>
<p>Approximating Equation 13 with a Monte Carlo estimate with $N$ samples, yields</p>
<p>$$
k(\boldsymbol{\tau}) \approx \frac{1}{N} \sum_{n=1}^{N}\left(e^{-i \boldsymbol{\omega}<em n="n">{n} \mathbf{x}}\right)\left(e^{-i \boldsymbol{\omega}</em>\right)
$$} \mathbf{x}^{\prime}</p>
<p>where $\boldsymbol{\omega}$ is sampled from the density $\mathcal{K}(\boldsymbol{\omega})$.
Finally, using Euler's formula $\left(e^{-i x}=\cos (x)-i \sin (x)\right)$ we recover the features:</p>
<p>$$
\begin{aligned}
\Phi(\mathbf{x}) &amp; =\frac{1}{\sqrt{N}}\left[\cos \left(\boldsymbol{\omega}<em 1="1">{1} \mathbf{x}+b</em>}\right), \ldots, \cos \left(\boldsymbol{\omega<em n="n">{n} \mathbf{x}+b</em>\right)\right. \
&amp; \left.-i \cdot \sin \left(\boldsymbol{\omega}<em 1="1">{1} \mathbf{x}+b</em>}\right), \ldots,-i \cdot \sin \left(\boldsymbol{\omega<em n="n">{n} \mathbf{x}+b</em>\right)\right]
\end{aligned}
$$</p>
<p>where bias terms $\mathbf{b}_{i}$ are introduced with the goal of rotating the projection and allowing for more flexibility in capturing the correct frequencies.</p>
<p>This approximation can be used with all shift invariant kernels proving flexibility in introducing prior knowledge by selecting a suitable kernel for the problem. For example, the RBF kernel can be approximated using the features above with $\boldsymbol{\omega} \sim \mathcal{N}\left(0,2 \sigma^{-2} I\right)$ and $b \sim \mathcal{U}[-\pi, \pi] . \sigma$ is a hyperparameter that corresponds to the kernel length scale and is usually set up with cross validation.</p>
<p>We further adopt a quasi Monte Carlo strategy for sampling the frequencies. In particular we use Halton sequences [7] which has been shown in [3] to have better convergence rate and lower approximation error than standard Monte Carlo.</p>
<h2>E. Posterior recovery</h2>
<p>From Equation 4 we note that when the proposal prior is different than the desirable prior, we need to adjust the posterior by weighting it with the ratio $p(\boldsymbol{\theta}) / \hat{p}(\boldsymbol{\theta})$.</p>
<p>In this paper we assume the prior to be uniform, either with finite support - defined within a range and zero elsewhere or improper, constant value everywhere. Therefore,</p>
<p>$$
\hat{p}\left(\boldsymbol{\theta} \mid \mathbf{x}=\mathbf{x}^{r}\right) \propto \frac{q_{\phi}\left(\boldsymbol{\theta} \mid \mathbf{x}^{r}\right)}{\hat{p}(\boldsymbol{\theta})}
$$</p>
<p>When the proposal prior is Gaussian, we can compute the division between a mixture and a single Gaussian analytically.</p>
<p>In this case, since $q_{\phi}(\boldsymbol{\theta}|\mathbf{x})$ is a mixture of Gaussians and $\tilde{p}(\boldsymbol{\theta}) \sim \mathcal{N}\left(\boldsymbol{\theta} \mid \boldsymbol{\mu}<em 0="0">{0}, \boldsymbol{\Sigma}</em>\right)$, the solution is given by</p>
<p>$$
\tilde{p}\left(\boldsymbol{\theta} \mid \mathbf{x}=\mathbf{x}^{r}\right)=\sum_{k} \alpha_{k}^{\prime} \mathcal{N}\left(\boldsymbol{\theta} \mid \boldsymbol{\mu}<em k="k">{k}^{\prime}, \boldsymbol{\Sigma}</em>\right)
$$}^{\prime</p>
<p>where,</p>
<p>$$
\begin{aligned}
\boldsymbol{\Sigma}<em k="k">{k}^{\prime} &amp; =\left(\boldsymbol{\Sigma}</em>}^{-1}-\boldsymbol{\Sigma<em k="k">{0}^{-1}\right)^{-1} \
\boldsymbol{\mu}</em>}^{\prime} &amp; =\boldsymbol{\Sigma<em k="k">{k}^{-1}\left(\boldsymbol{\Sigma}</em>}^{-1} \boldsymbol{\mu<em 0="0">{k}-\boldsymbol{\Sigma}</em>}^{-1} \boldsymbol{\mu<em k="k">{0}\right) \
\alpha</em>
\end{aligned}
$$}^{\prime} &amp; =\frac{\alpha_{k} \exp \left(-\frac{1}{2} \lambda_{k}\right)}{\sum_{k^{\prime}} \alpha_{k^{\prime}} \exp \left(-\frac{1}{2} \lambda_{k^{\prime}}\right)</p>
<p>and the coefficients $\lambda_{k}$ are given by</p>
<p>$$
\begin{aligned}
\lambda_{k}=\log \operatorname{det} \boldsymbol{\Sigma}<em 0="0">{k}-\log &amp; \operatorname{det} \boldsymbol{\Sigma}</em>}-\log \operatorname{det} \boldsymbol{\Sigma<em k="k">{k}^{\prime}+\boldsymbol{\mu}</em>}^{T} \boldsymbol{\Sigma<em k="k">{k}^{-1} \boldsymbol{\mu}</em> \
&amp; -\boldsymbol{\mu}<em 0="0">{0}^{T} \boldsymbol{\Sigma}</em>}^{-1} \boldsymbol{\mu<em k_prime="k^{\prime">{0}-\boldsymbol{\mu}</em>}}^{\prime T} \boldsymbol{\Sigma<em k="k">{k}^{\prime-1} \boldsymbol{\mu}</em>
\end{aligned}
$$}^{\prime</p>
<h2>F. Sufficient statistics for state-action trajectories</h2>
<p>Trajectories of state and action pairs in typical problems can be long sequences making the input dimensionality to the model prohibitive large and computationally expensive. We adopt a strategy commonly used in ABC ; instead of inputting raw state and action sequences to the model, we first compute some sufficient statistics. Formally, $\mathbf{x}=\psi(\mathbf{S}, \mathbf{A})$ where $\mathbf{S}=\left{\mathbf{s}^{t}\right}<em t="1">{t=1}^{T}$ and $\mathbf{A}=\left{\mathbf{a}^{t}\right}</em>$ are sequences of states and actions from $t=1$ to $T$. There are many options in the literature for sufficient statistics for time series or trajectory data. For example, the mean, log variance and autocorrelation for each time series as well as cross-correlation between two time series. Another possibility is to learn these from data, for example with an unsupervised encoder-decoder recurrent neural network [35]. However, such a representation would need to be trained with simulated trajectories and might not be able to capture complexities in the real trajectories. This will be investigated in future work. Here we adopt a simpler strategy and use statistics commonly applied to stochastic dynamic systems such as the Lotka-Volterra model [40].}^{T</p>
<p>Defining $\boldsymbol{\tau}=\left{\mathbf{s}^{t}-\mathbf{s}^{t-1}\right}_{t=1}^{T}$ as the difference between immediate future states and current states, the statistics</p>
<p>$$
\psi(\mathbf{S}, \mathbf{A})=\left(\left{\left\langle\boldsymbol{\tau}<em j="j">{i}, \mathbf{A}</em>\right\rangle\right}<em s="s">{i=1, j=1}^{D</em>]\right)
$$}, D_{a}}, \mathrm{E}[\boldsymbol{\tau}], \operatorname{Var}[\boldsymbol{\tau</p>
<p>where $D_{s}$ is the dimensionality of the state space, $D_{a}$ is the dimensionality of the action space, $\langle\cdot, \cdot\rangle$ denotes the dot product, $\mathrm{E}[\cdot]$ is the expectation, and $\operatorname{Var}[\cdot]$ the variance.</p>
<h2>G. Example: CartPole posterior</h2>
<p>We provide a simple example to demonstrate the algorithm in estimating unknown simulation parameters for the famous CartPole problem. In this problem a pole installed on a cart needs to be balanced by applying forces to the left or to the right of the cart. For this example we assume that both the mass and the length of the pole are not available and we use BayesSim to obtain the posterior for these parameters. We assume uniform priors for both parameters and collect 1000 simulations following a rl-zoo policy ${ }^{2}$ to train BayesSim.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>With the model trained, we collected 10 trajectories with the correct parameters to simulate the real observations. Figure 2 shows the posteriors for both problems. As with many problems involving two related variables, masspole and pole length exhibit statistical dependencies that generate multiple explanations for their values. For example, the pole might have lower mass and longer length, or vice versa. BayesSim is able to recover the multi-modality nature of the posterior providing densities that represent the uncertainty of the problem accurately.</p>
<h2>H. Domain randomization with BayesSim</h2>
<p>Here we describe the domain randomization strategy to take full advantage of the posterior obtained by the inference method. Given the posterior obtained from the simulation parameters $\tilde{p}\left(\boldsymbol{\theta} \mid \mathbf{x}=\mathbf{x}^{r}\right)$ we maximize the objective,</p>
<p>$$
J(\boldsymbol{\beta})=\mathbb{E}<em _boldsymbol_eta="\boldsymbol{\eta">{\boldsymbol{\theta}}\left[\mathbb{E}</em>}}\left[\sum_{t=0}^{T-1} \gamma^{(t)} r\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>\right]\right]
$$}\right) \mid \boldsymbol{\beta</p>
<p>where $\boldsymbol{\theta} \sim \tilde{p}\left(\boldsymbol{\theta} \mid \mathbf{x}=\mathbf{x}^{r}\right)$ with respect to the policy parameters $\boldsymbol{\beta}$. Since the posterior is a mixture of Gaussians, the first expectation can be approximated by sampling a mixture component following the distribution over $\boldsymbol{\alpha}$ to obtain a component $k$, followed by sampling the corresponding Gaussian $\mathcal{N}\left(\boldsymbol{\theta} \mid \boldsymbol{\mu}<em k="k">{k}, \boldsymbol{\Sigma}</em>\right)$.</p>
<h2>V. EXPERIMENTS</h2>
<p>Experiments are presented in two different cases to demonstrate and assess the performance of BayesSim. In Section V-A we verify and compare the accuracy of the posterior recovered. In Section V-B we compare the robustness of policies trained by randomizing following the prior versus posterior distribution over simulation parameters.</p>
<h2>A. Posterior recovery</h2>
<p>The first analysis we carry out is the quality of the posteriors obtained for different problems and methods. We use the log probability of the target under the mixture model as the measure, defined as $\log p\left(\boldsymbol{\theta}_{<em>} \mid \mathbf{x}=\mathbf{x}^{r}\right)$, where $\boldsymbol{\theta}_{</em>}$ is the actual value for the parameter. We compare RejectionABC [26] as the baseline, the recent $\epsilon$-Free [24] which also provides a mixture model as the posterior, and BayesSim using either a two layer neural network with 24 units in each layer, and BayesSim with quasi random Fourier Features. For the later we use the Matern 5/2 kernel [28] and set up the the sampling precision $\sigma$ by cross validation. Three different simulators were used for different problems; OpenAI Gym [9], PyBullet ${ }^{3}$, and MuJoCo [39]. Finally, the following problems were considered; CartPole (Gym), Pendulum (Gym), Mountain Car (Gym), Acrobot (Gym), Hopper (PyBullet), Fetch Push (MuJoCo) and Fetch Slide (MuJoCo). For all configurations of methods and parameters, training and testing were performed 5 times with the log probabilities averaged and standard deviation computed. To extract the real observations,</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: Example of joint posteriors obtained for the CartPole problem with different parametrizations for length and masspole. The true value is indicated by a star. Note that the joint posteriors capture the multimodality of the problem when two or more explanations seem likely, for example, a longer pole length with a lighter masspole or vice versa.</p>
<p>we simulate the environments with the actual parameters 10 times and average the sufficient statistics to obtain x<sup>r</sup>. In all cases we collect sufficient statistics by performing rollouts for either a maximum of 200 time steps or until the end of the episode.</p>
<p>Table I shows the results (means and standard deviations) for the log probabilities. BayesSim with either RFF or Neural Network features provides generally higher log-probabilities and lower standard deviation than Rejection ABC. This indicates that the posteriors provided by BayesSim are more peaked and centered around the correct values for the parameters. Compared to ε-Free, the results are equivalent in terms of the means but BayesSim generally provides lower standard deviation across multiple runs of the method, indicating it is more stable than ε-Free. Comparing BayesSim with RFF and NN, the RFF features lead to higher log probabilities in most cases but BayesSim with neural networks have lower standard deviation.</p>
<p>These results suggest that BayesSim with either RFF or NN is comparable to the state-of-art, and in many cases superior when estimating the posterior distribution over the simulation parameters. For the robotics problems analyzed in the next section, however, BayesSim with RFF provide significant superior results than the other methods and slightly better than BayesSim with NN. This can be better observed when we plot the posteriors in Figure 3. BayesSim RFF is significantly more peaked and centered around the true friction value.</p>
<h3><em>B. Robustness of policies</em></h3>
<p>We evaluate robustness of policies by comparing their performance on the uniform prior and the learned posterior provided by BayesSim. Evaluation is done over a pre-defined range of simulator settings and the average reward is shown for each parameter value.</p>
<p>In the first set of experiments we use the CartPole problem as a simple example to illustrate the benefits of posterior randomization. We trained two policies, the first randomizing with a uniform prior for <em>length</em> and <em>masspole</em> as indicated in Table I. The second, randomized based on the posterior provided by BayesSim with RFF. In both cases we use PPO to train the policies with 100 samples from the prior and posterior, for 2M timesteps. The results are presented in Figure 4, averaged over several runs with the corresponding standard deviations. It can be observed that randomization over the posterior yields a significantly more robust policy, in particular at the actual parameter value. Also noticeable is the reduction in performance for lower <em>length</em> values and higher <em>masspole</em> values. This is expected as it is more difficult to control the pole position when the length is short due to the increased dynamics of the system. Similarly, when the mass increases too much, beyond the value it was actually trained on, the controller struggles to maintain the pole balanced. Importantly, the policy learned with the posterior seems much</p>
<table>
<thead>
<tr>
<th>Problem</th>
<th>Parameter</th>
<th>Uniform prior</th>
<th>Rejection ABC</th>
<th>$\epsilon$-Free</th>
<th>BayesSim RFF</th>
<th>BayesSim NN</th>
</tr>
</thead>
<tbody>
<tr>
<td>CartPole</td>
<td>pole length</td>
<td>$[0.1,2.0]$</td>
<td>-0.342$\pm$0.15</td>
<td>-0.211$\pm$0.07</td>
<td>-0.609$\pm$0.39</td>
<td>-0.657$\pm$0.25</td>
</tr>
<tr>
<td></td>
<td>pole mass</td>
<td>$[0.1,2.0]$</td>
<td>0.032$\pm$0.21</td>
<td>0.056$\pm$0.14</td>
<td>0.973 $\pm$ 0.26</td>
<td>0.633$\pm$ 0.52</td>
</tr>
<tr>
<td>Pendulum</td>
<td>dt</td>
<td>$[0.01,0.3]$</td>
<td>2.101$\pm$1.04</td>
<td>2.307$\pm$0.84</td>
<td>3.192$\pm$0.30</td>
<td>3.199$\pm$0.17</td>
</tr>
<tr>
<td>Mountain Car</td>
<td>power</td>
<td>$[0.0005,0.1]$</td>
<td>3.69$\pm$1.21</td>
<td>3.800$\pm$1.06</td>
<td>3.863$\pm$0.52</td>
<td>3.901$\pm$0.2</td>
</tr>
<tr>
<td>Acrobot</td>
<td>link mass 1</td>
<td>$[0.5,2.0]$</td>
<td>1.704$\pm$0.82</td>
<td>1.883$\pm$0.79</td>
<td>2.046$\pm$0.37</td>
<td>1.331$\pm$0.22</td>
</tr>
<tr>
<td></td>
<td>link mass 2</td>
<td>$[0.5,2.0]$</td>
<td>1.832$\pm$0.93</td>
<td>2.237$\pm$0.76</td>
<td>0.321$\pm$1.85</td>
<td>1.513$\pm$0.39</td>
</tr>
<tr>
<td></td>
<td>link length 1</td>
<td>$[0.1,1.5]$</td>
<td>2.421$\pm$0.75</td>
<td>2.135$\pm$0.50</td>
<td>2.072$\pm$0.76</td>
<td>1.856$\pm$0.18</td>
</tr>
<tr>
<td></td>
<td>link length 2</td>
<td>$[0.5,1.5]$</td>
<td>-0.521$\pm$0.36</td>
<td>-0.703$\pm$0.16</td>
<td>-0.148$\pm$0.19</td>
<td>-0.672$\pm$0.09</td>
</tr>
<tr>
<td>Hopper</td>
<td>lateral friction</td>
<td>$[0.3,0.5]$</td>
<td>3.032$\pm$0.43</td>
<td>3.154$\pm$0.81</td>
<td>2.622$\pm$0.64</td>
<td>3.391$\pm$0.08</td>
</tr>
<tr>
<td>Fetch Push</td>
<td>friction</td>
<td>$[0.1,1.0]$</td>
<td>1.332$\pm$0.54</td>
<td>2.013$\pm$0.09</td>
<td>2.423$\pm$0.07</td>
<td>2.404$\pm$0.05</td>
</tr>
<tr>
<td>Fetch Slide</td>
<td>friction</td>
<td>$[0.1,1.0]$</td>
<td>1.014$\pm$0.38</td>
<td>1.614$\pm$0.12</td>
<td>2.391$\pm$0.06</td>
<td>2.111$\pm$0.03</td>
</tr>
</tbody>
</table>
<p>TABLE I: Mean and standard deviation of log predicted probabilities for several likelihood-free methods, applied to seven different problems and parameters.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 4: Accumulated rewards for CartPole policies trained with PPO by randomizing over prior and posterior joint densities. Top left: Performance of the policy trained with the prior, over parameter length. masspole is set to actual. Top right: Similar to top left, but over multiple masspole values. Bottom left: Performance of policy trained with the posterior, over parameter length. Bottom right: Similar to bottom left, but over multiple masspole values.</p>
<p>more stable across multiple runs as indicated by the lower variance in the plots.</p>
<p>In the second set of experiments we use a Fetch robot available in OpenAI Gym [8] to perform both push and slide tasks. The first is a closed loop scenario, where the arm is always in range of the entire table and, hence, it can correct its trajectories according to the input it receives from the environment. The second is a more difficult open loop scenario, where the robot has usually only one shot at pushing the puck to its desired target. For both tasks, the friction coefficient of the object and the surface plays a major role in the final result as they are strictly related to how far the object goes after each force is applied. A very low friction coefficient means that the object is harder to control as it slides more easily and a very high one means that more force needs to be applied in order to make the object to move.</p>
<p>Our goal is to recover a good approximation of the posterior over friction coefficients using BayesSim. Initially, we need to learn a policy with a fixed friction coefficient that will be used for data generation purposes. We train this policy using</p>
<p>DDPG with experiences being sampled using HER for 200 epochs with 100 episodes/rollouts per epoch. Gradient updates are done using Adam with step size of 0.001. We then run this policy multiple times with different friction coefficients in order to approximate the likelihood function and recover the full posterior over simulation parameters. With the dynamics model in hand, we can finally recover the desired posterior using some data sampled from the environment we want to learn the dynamics from. Training is carried out using the same aforementioned settings but instead of using a fixed friction coefficient, we sample a new one from its respective distribution every time a new episode starts.</p>
<p>The results from both tasks are presented on Figure 5. As it has been shown in previously work [25], the uniform prior works remarkably well on the push task. This happens as the robot has the opportunity to correct its trajectory whether something goes wrong. As it has been exposed to a wide range of scenarios involving different dynamics, it can then use the input of the environment to perform corrective actions and still be able to achieve its goal. However, the results for slide task differ significantly since using a wide uniform prior has led the robot to achieve a very poor performance. This happens as not only the actions for different coefficients in most times are completely different but also because the robot has no option of correcting its trajectory. This is where methods like BayesSim are useful as it recovers a distribution with very high density around the true parameter and, hence, leads to a better overall control policy. Our results shows that higher rewards are achieved around the true friction value while the uniform prior results are mostly flat throughout all values.</p>
<h2>VI. CONCLUSIONS</h2>
<p>This paper represents the first step towards a Bayesian treatment of robotics simulation parameters, combined with domain randomization for policy search. Our approach is connected to system identification in that both attempt to estimate dynamic models, but ours uses a black-box generative model, or simulator, totally integrated into the framework. Prior distributions can also be provided and incorporated into the model to compute a full, potentially multi-modal posterior over the parameters. The method proposed here, BayesSim, performs comparably to other state-of-the-art likelihood-free approaches for Bayesian inference but appears more stable to different initializations, and across multiple runs when recovering the true posterior. Finally, we show that domain randomization with the posterior leads to more robust policies over multiple parameter values compared to policies trained on uniform prior randomization.</p>
<p>The two applications described in the paper for likelihoodfree inference are two instances of a large range of problems where simulators can make use of a full set of parametrizations to best represent reality. In this manner, our framework can be integrated in many other problems involving simulators. An interesting line of research for future work is to use BayesSim to help simulators synthesize images by randomizing over background properties. This can potentially help in making many computer vision problems more robust to environment variability in many tasks including object recognition, 3D pose estimation, or motion tracking.</p>
<p>As typical in the likelihood-free inference literature, BayesSim relies on the definition of meaningful sufficient statistics for the trajectories of states and actions. Alternatively, a lower dimensional representation for the trajectories could be created using recent encoder-decoder methods and recurrent neural networks known to perform well for time series prediction such as LSTMs [16]. Hence, the entire framework can be learnt end to end. This is an interesting area for future development but careful consideration should be given to potential overfitting to simulation data. LSTMs usually require a lot of data for training therefore most of the training trajectories will be generated from simulated trajectories. This can introduce undesirable specific characteristics of the simulator in the low dimensional representation that are not observed in real trajectories, making the representation less sensitive to variations in the simulator parameters to be estimated. Despite this, automating the process of generating robust statistics from trajectories remains a valuable direction for future research.</p>
<h2>REFERENCES</h2>
<p>[1] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, pages 5048-5058, 2017.
[2] Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. arXiv preprint arXiv:1808.00177, 2018.
[3] Haim Avron, Vikas Sindhwani, Jiyan Yang, and Michael W. Mahoney. Quasi-Monte Carlo feature maps for shift-invariant kernels. Journal of Machine Learning Research, 17(120):1-38, 2016.
[4] Mark A Beaumont, Wenyang Zhang, and David J Balding. Approximate bayesian computation in population genetics. Genetics, 4(162):2025-2035, 2002.
[5] Christopher M Bishop. Mixture density networks. Technical report, Citeseer, 1994.
[6] Fernando V Bonassi, Mike West, et al. Sequential Monte Carlo with adaptive weights for approximate bayesian computation. Bayesian Analysis, 10(1):171-187, 2015.
[7] E. Braaten and G. Weller. An improved low-discrepancy sequence for multidimensional quasi-Monte Carlo integration. Journal of Computational Physics, 33:249-258, November 1979.
[8] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
[9] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.
[10] Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff, and Dieter Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. In IEEE International Conference on Robotics and Automation (ICRA), 2018.
[11] Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff, and Dieter Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. arXiv preprint arXiv:1810.05687, 2018.
[12] P.J. Diggle and R.J. Gratton. Monte Carlo methods of inference for implicit statistical models. Journal of the Royal Statistical Society. Series B (Methodological), 2 (46):193-227, 1984.
[13] I. I. Gihman and A. V. Skorohod. The Theory of Stochastic Processes, volume 1. Springer Verlag, Berlin, 1974.
[14] Graham C. Goodwin and Robert L. Payne. Dynamic System Identification: Experiment Design and Data Analysis. Academic Press, 1977.
[15] Christian Gourieroux, Alain Monfort, and Eric Renault. Indirect inference. Journal of applied econometrics, 8 (S1):S85-S118, 1993.
[16] Sepp Hochreiter and Jürgen Schmidhuber. Long shortterm memory. Neural Comput., 9(8):1735-1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997. 9.8.1735. URL http://dx.doi.org/10.1162/neco.1997.9.8. 1735.
[17] Balakumar Sundaralingam Yu Xiang Dieter Fox Jonathan Tremblay, Thang To and Stan Birchfield. Deep object pose estimation for semantic robotic grasping of household objects. In Conference on Robot Learning (CoRL), 2018.
[18] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
[19] Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning, 8(3-4):293-321, 1992.
[20] Paul Marjoram, John Molitor, Vincent Plagnol, and Simon Tavaré. Markov chain Monte Carlo without likelihoods. Proceedings of the National Academy of Sciences, 100(26):15324-15328, 2003.
[21] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.
[22] Igor Mordatch, Kendall Lowrey, and Emanuel Todorov. Ensemble-cio: Full-body dynamic motion planning that transfers to physical humanoids. In Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on, pages 5307-5314. IEEE, 2015.
[23] Lowrey K. Mordatch, I. and E. Todorov. EnsembleCIO: Full-body dynamic motion planning that transfers to physical humanoids. In Intelligent Robots and Systems (IROS), IEEE/RSJ International Conference on. IEEE, 2015.
[24] George Papamakarios and Iain Murray. Fast $\varepsilon$-free inference of simulation models with bayesian conditional density estimation. In Advances in Neural Information Processing Systems, pages 1028-1036, 2016.
[25] Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of robotic control with dynamics randomization. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 1-8. IEEE, 2018.
[26] Jonathan K Pritchard, Mark T Seielstad, Anna PerezLezaun, and Marcus W Feldman. Population growth of human y chromosomes: a study of y chromosome microsatellites. Molecular biology and evolution, 16(12): 1791-1798, 1999.
[27] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. C. Platt, D. Koller, Y. Singer, and S. T. Roweis, editors, Advances in Neural</p>
<p>Information Processing Systems 20, pages 1177-1184. Curran Associates, Inc., 2008.
[28] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning). The MIT Press, 2005.
[29] Chad M Schafer and Peter E Freeman. Likelihood-free inference in cosmology: Potential for the estimation of luminosity functions. In Statistical Challenges in Modern Astronomy V, pages 3-19. Springer, 2012.
[30] Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In International Conference on Machine Learning, pages 1312-1320, 2015.
[31] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015.
[32] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pages 1889-1897, 2015.
[33] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[34] S.A. Sisson, Y. Fan, and M.M. Tanaka. Sequential Monte Carlo without likelihoods. Proceedings of the National Academy of Sciences, 104(6):1760-1765, 2007.
[35] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video representations using lstms. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 843-852, Lille, France, 07-09 Jul 2015. PMLR.
[36] M. L. Stein. Interpolation of Spatial Data. SpringerVerlag, New york, 1999.
[37] Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, and Vincent Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. arXiv preprint arXiv:1804.10332, 2018.
[38] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on, pages 23-30. IEEE, 2017.
[39] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026-5033. IEEE, 2012.
[40] Darren J. Wilkinson. Stochastic Modelling for Systems Biology, Second Edition. Chapman \&amp; Hall/CRC Mathematical and Computational Biology. Taylor \&amp; Francis, 2011.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://github.com/araffin/rl-baselines-zoo&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ https://pypi.org/project/pybullet/&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3148 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3148</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3148</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-28a5a53dafacebad8a7c47773079caeffb9a5baa</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/28a5a53dafacebad8a7c47773079caeffb9a5baa" target="_blank">Representing Numbers in NLP: a Survey and a Vision</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work synthesizes best practices for representing numbers in text and articulate a vision for holistic numeracy in NLP, comprised of design trade-offs and a unified evaluation.</p>
                <p><strong>Paper Abstract:</strong> NLP systems rarely give special consideration to numbers found in text. This starkly contrasts with the consensus in neuroscience that, in the brain, numbers are represented differently from words. We arrange recent NLP work on numeracy into a comprehensive taxonomy of tasks and methods. We break down the subjective notion of numeracy into 7 subtasks, arranged along two dimensions: granularity (exact vs approximate) and units (abstract vs grounded). We analyze the myriad representational choices made by over a dozen previously published number encoders and decoders. We synthesize best practices for representing numbers in text and articulate a vision for holistic numeracy in NLP, comprised of design trade-offs and a unified evaluation.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3148.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3148.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large autoregressive transformer language model shown to perform well zero-shot on many NLP tasks, including simple arithmetic when operands are low-digit.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive Transformer (Brown et al., 2020); publicly reported at 175B parameters; trained with causal language modeling on large web corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Zero-shot simple arithmetic (small-digit addition/subtraction), synthetic arithmetic tasks</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Pattern memorization and learned token-sequence mapping: solves many low-digit arithmetic examples from training distribution/patterns rather than explicit symbolic algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Paper reports that GPT-3 "performs extremely well at zero shot simple arithmetic" for low-digit operands; the surveyed literature links good performance to tokenization and training distribution that contain many simple arithmetic examples (cited Brown et al., 2020).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Performance degrades with larger numbers/digit counts (limited extrapolation), suggesting absence of robust algorithmic arithmetic; no direct interpretability evidence for algorithmic internal steps provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>None applied in this survey beyond noting zero-shot prompting; related interventions (digit tokenization, fine-tuning) discussed elsewhere.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Not evaluated for GPT-3 specifically in this paper; general observation that tokenization at character/digit level improves arithmetic for transformer models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Described qualitatively as "extremely well" on low-digit arithmetic zero-shot; no numeric accuracy values reported in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Fails/extrapolates poorly as operand digit length increases; tokenization- and distribution-dependent errors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Compared implicitly: success is distributional/pattern-based rather than true symbolic computation like calculators; extrapolation limitations contrast with exact symbolic methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Representing Numbers in NLP: a Survey and a Vision', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3148.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3148.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformers (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer-based language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Family of attention-based sequence models (encoder-only, decoder-only, or encoder-decoder) used as the backbone for many language models; their numeracy depends strongly on tokenization and numeric representation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer-based LMs (BERT, GPT variants, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Self-attention-based architectures (Vaswani et al., 2017) used in encoder (BERT) and autoregressive (GPT) configurations; sizes vary by instantiation.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Simple arithmetic (synthetic addition/subtraction), arithmetic word problems, numeric language modeling, numeration/magnitude probes</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Learned sequence-to-sequence/pattern mappings influenced by tokenization and numeric surface forms; internal embeddings may implicitly encode magnitude when real-valued encoders or special representations are used.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Survey cites multiple probing studies showing that contextualized embeddings contain numeric information (numeration, magnitude); digit/char tokenization and explicit numeric encoders (Value, DICE) improve numeric probes and some arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Default subword tokenization treats numbers as arbitrary tokens and leads to poor arithmetic and extrapolation (e.g., BERT performs worse on numeric answers in DROP), implying that vanilla transformers do not inherently implement robust numeric algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Tokenization changes (character/digit-level), numeric-specific encoders/decoders (Value, LogValue, DICE), scientific notation pretraining (NumBERT), finetuning on arithmetic (GenBERT), distributional decoders (GMM, Log-Laplace, DExp, MCC).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Interventions generally improve numeracy: char/digit tokenization and pooled digit encoders improve arithmetic and probes; scientific notation pretraining (NumBERT) helps numeric masked LM and measurement estimation; real-based encoders improve magnitude probes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey reports qualitative and comparative results: e.g., subword models worse than char models on probes (Wallace et al., 2019), BERT five-fold worse on numeric answers in DROP (cited Dua et al., 2019); no unified numeric benchmark scores provided.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Poor extrapolation to larger unseen numbers, sensitivity to tokenization and notation, poor performance on exact grounded tasks (arithmetics in word problems) unless specialized components are used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Transformers behave more like pattern learners with limited extrapolation, differing from human approximate number systems and from exact symbolic computation; cognitive-science-inspired log-scale encodings sometimes better match approximate reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Representing Numbers in NLP: a Survey and a Vision', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3148.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3148.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bidirectional Encoder Representations from Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Masked language model based on transformer encoder stacks; frequently used as baseline for numeracy probes and augmented with numeric-specific methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BERT: Pre-training of deep bidirectional transformers for language understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (base/large variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Masked Transformer encoder pretrained on large corpora with next-sentence/prediction objectives; typical base sizes ~110M, large ~340M parameters (paper does not re-run sizes).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Masked numeric language modeling, numeration/magnitude probes, measurement estimation, numeric commonsense (NumerSense), arithmetic-related QA (DROP).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Encodes numeric information implicitly in contextual embeddings; default subword tokenization fragments numbers, limiting numeric generalization; can be improved by pretraining with scientific notation or digit-level tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Probing studies (Wallace et al., 2019; Naik et al., 2019) show BERT contains numeric signal but underperforms vs char-tokenized/contextual char models; Zhang et al. (2020) show NumBERT (scientific notation pretraining) converges to similar MLM loss and improves measurement estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>On DROP QA, BERT is ~5x worse when the answer is a number vs a text span (cited Dua et al., 2019), indicating limitations for numeric extraction/reasoning in downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Pretraining with scientific notation (NumBERT), finetuning on numeric tasks, replacing tokenization with char/digit-level, adding real-valued number encoders (Value/LogValue), pooled DigitRNN/DigitCNN embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Scientific-notation pretraining (NumBERT) improves measurement estimation and numeric masked LM behavior while preserving general-language performance; digit/char tokenization and pooled digit encoders improve numeration/magnitude probes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey cites qualitative and comparative findings: NumBERT achieves similar MLM loss to BERT on GLUE; BERT has substantially degraded performance on number-answer cases in DROP (~5x worse) — specific numeric accuracies not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Subword tokenization splits numerals arbitrarily, harming representation; poor extrapolation to unseen numeric ranges; struggles on exact arithmetic and grounded numeric facts without augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>BERT's implicit numeric encodings are unlike symbolic calculators and only partially align with human approximate number representations; log-scale encodings inspired by cognitive models help in approximate tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Representing Numbers in NLP: a Survey and a Vision', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3148.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3148.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GenBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GenBERT (BERT finetuned on arithmetic tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BERT-based model tokenizing numbers at digit-level and finetuned on auxiliary arithmetic tasks; used to inject numerical reasoning skills to improve QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Injecting numerical reasoning skills into language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GenBERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BERT architecture with digit-level tokenization; finetuned on synthetic arithmetic and arithmetic word-problem auxiliary tasks to improve numerical QA.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Simple arithmetic, arithmetic word problems; auxiliary pretraining tasks for downstream QA (e.g., DROP).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Training on arithmetic tasks teaches the model pattern-based mappings and assists the model in mapping counts/operations into tokens and embeddings — effectively a fine-tuning intervention that imparts task-specific algorithmic behavior or heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Paper reports GenBERT finetuned on arithmetic improves scores on DROP QA relative to baselines, indicating transfer from explicit arithmetic finetuning to downstream numeric QA.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Survey notes that state-of-the-art AWP solvers often bypass numeric embedding by predicting equations then filling numeric values, suggesting that encoding numbers into embeddings alone may not fully solve grounded arithmetic problems.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Auxiliary finetuning on simple arithmetic and arithmetic word problems; digit-level tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improved performance on DROP QA and related numeric QA tasks compared to vanilla BERT baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported qualitatively: GenBERT finetuned on simple arithmetic helps it do well on downstream QA (DROP); specific numeric accuracy values are not enumerated in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>May rely on pattern learning limited to finetuning distribution; grounded arithmetic word problems sometimes solved by equation-prediction methods rather than improved numeric embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Finetuning imparts task-specific strategies akin to training humans on arithmetic examples; still distinct from explicit symbolic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Representing Numbers in NLP: a Survey and a Vision', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3148.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3148.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NumBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NumBERT (scientific-notation-pretrained BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BERT model pretrained from scratch on data where numbers are rewritten in scientific notation to emphasize exponent information and improve numeric representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Do language embeddings capture scales?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NumBERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BERT pretrained on a modified corpus where numerals are converted to scientific notation (mantissa+exponent tokens), with subword tokenization; otherwise same training objectives as BERT.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Masked numeric language modeling, measurement estimation, numeric prediction tasks (distributional quantities).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>By rewriting numbers into scientific notation, NumBERT shifts learning focus to exponent (scale) embeddings, enabling models to capture magnitude information more robustly than decimal tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Zhang et al. (2020) show NumBERT converges to the same MLM loss as BERT and performs better on measurement estimation tasks; Spokoyny & Berg-Kirkpatrick report exponent embeddings can outperform DigitRNN-sci, supporting the hypothesis that exponent/scale features are most informative.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Mantissa modeling may be unnecessary/overkill for some corpora; scientific-notation changes do not in themselves produce explicit arithmetic algorithmic ability or full extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Pretraining corpus transformation (scientific notation), standard masked LM pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improved performance on measurement estimation and certain numeric masked LM tasks while maintaining general-language performance on GLUE-like benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey reports NumBERT "converges to the same loss as BERT on masked language modelling" and scores nearly the same on GLUE; specific numeric error/accuracy numbers not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Still limited on exact arithmetic and some grounded numeric tasks; representation focus on exponent can ignore useful mantissa details in some domains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>NumBERT's focus on scale (exponent) echoes human approximate magnitude representations (log-like), but it is not equivalent to symbolic arithmetic systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Representing Numbers in NLP: a Survey and a Vision', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3148.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3148.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DigitRNN/DigitCNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Digit-level RNN/CNN pooled encoders (DigitRNN / DigitCNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>String-based approach that tokenizes numerals at the digit/character level and pools those digit embeddings via an RNN or CNN to produce a single number embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Numeracy for language models: Evaluating and improving their ability to predict numbers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DigitRNN / DigitCNN pooled encoders</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Character/digit tokenization followed by a pooling network (RNN or CNN) over digits to produce a fixed-size embedding representing the full numeral, plugged into downstream models.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Numeration probes, magnitude comparison, numeric language modeling, simple arithmetic when used in models.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>By processing digits sequentially or via convolution, these encoders can capture structure and magnitude of numerals from surface form, enabling better generalization than naive subword tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Survey cites Spithourakis & Riedel (2018) and Wallace et al. (2019) showing pooled digit encoders outperform subword baselines on numeration/magnitude probes; DigitCNNs reported as best number encoders in probes.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Pooling approaches lack controlled comparisons with unpooled scientific-notation methods (NumBERT) in some studies; digit RNNs may be outperformed by simple exponent embedding variants in some corpora (Spokoyny & Berg-Kirkpatrick).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Replace tokenization with digit/char-level and add pooling network (RNN/CNN) to produce number embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improves numeration and magnitude comparison probes, and can improve arithmetic performance when integrated; digit-level tokenization improves extrapolation compared to subword tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported relatively: DigitCNNs and character-tokenized models outperform subword ones on abstract probes (Wallace et al., 2019); no absolute accuracies included in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>May be computationally heavier; performance depends on pooling architecture and training stability; may still fail on large-range continuous value prediction without discretization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Digit-based encodings more akin to human parsing of numerals but still produce distributed embeddings rather than explicit symbolic calculation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Representing Numbers in NLP: a Survey and a Vision', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3148.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3148.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DICE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deterministic Independent-of-Corpus Embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Handcrafted (deterministic) real-based number encoder designed to preserve relative magnitude relations in embedding space (monotonic relation between scalar distance and embedding cosine distance).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Methods for numeracy-preserving word embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DICE encoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deterministic mapping f: R -> R^d crafted so cosine distances between embeddings reflect numeric distances/ordering, usable as a plug-in numeric encoder or as an auxiliary loss to shape learned embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Numeration and magnitude comparison probes, numeric language modeling transfer tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Imposes explicit inductive bias of numeric continuity/ordering onto embeddings so models can use geometric relations to infer magnitude comparisons and order.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Survey reports DICE scores highly on numeration and magnitude comparison probes and can act as an auxiliary loss to improve numeracy-preserving word embeddings (Sundararaman et al., 2020).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>DICE is encoder-only and not easily reversible for decoding numbers; discretized or learned decoders may be needed for generative numeric tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Replace or augment numeric token embeddings with deterministic DICE encodings; optionally use as auxiliary training loss.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improves abstract probes (numeration, magnitude); transfer benefits observed for related numeric tasks (e.g., order-of-magnitude prediction).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported qualitatively: DICE shows high scores on numeration/magnitude comparison probes in cited work; no absolute numeric metrics provided in this survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Not reversible for decoding; constrains representational variance which may hurt tasks needing discrete enumeration/precision; coverage decisions required.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Imposes continuity bias akin to human approximate magnitude representation; differs from symbolic arithmetic in being an embedding constraint rather than procedural computation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Representing Numbers in NLP: a Survey and a Vision', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3148.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3148.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Value / LogValue embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Value embedding and Log-Value embedding (real-based encoders/decoders)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Parametric shallow neural encoders/decoders that map scalar numeric magnitude (linearly or on log scale) to/from vector embeddings; log-scaling inspired by cognitive models of numerosity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Do language embeddings capture scales?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Value / LogValue encoders/decoders</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Value: shallow NN projecting scalar to embedding and reverse via probe g: R^d->R. LogValue: same but operate on log(number) to emphasize scale and compress large ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Numeration (decoding), magnitude comparison, measurement estimation, numeric language modeling (regression-style decoding).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Encodes numeric magnitude directly into continuous latent space; log scaling compresses wide numeric ranges and aligns with human approximate (log-like) perception.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Survey notes Value and LogValue excel on numeration and magnitude probes; log-scale preferred in several studies (Zhang et al., 2020; Wallace et al., 2019) and matches cognitive science intuition (Dehaene, 2011).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Value embeddings struggle to extrapolate to larger numbers and continuous prediction across large ranges is practically difficult (training instability); continuous decoders underperform discrete binning on some data (Zhang et al., 2020).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Replace/augment numeric tokens with a learned mapping from scalar value (linear or log) to embedding; use regression probes for decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improves approximate numeric tasks and probes; LogValue better for wide-range estimation and measurement tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported qualitatively: Value/LogValue do well on numeration/magnitude probes; continuous regression worse than binning (dense cross-entropy) for distributional ground-truth in some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Poor extrapolation to very large/unseen numbers; instability when training continuous regressors over large ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>LogValue aligns with human approximate-number-system modeling (log-like perception) but is not a symbolic arithmetic procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Representing Numbers in NLP: a Survey and a Vision', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3148.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3148.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Distributional decoders (GMM / Log-Laplace / Flow-Laplace / DExp / MCC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gaussian mixture and distribution-parameterized numeric decoders (GMM, Log-Laplace, Flow-Laplace, Discrete Exponent, MCC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Real-valued decoders that output distributions (continuous or discretized) over numbers instead of point estimates, enabling multimodal and uncertainty-aware numeric predictions for numeric language modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An empirical investigation of contextualized number prediction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Distributional numeric decoders (GMM, Log-Laplace, Flow-Laplace, DExp, MCC)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoders parameterize distributions over numbers: GMMs (mixture of Gaussians), Log-Laplace (model on log scale), Flow-Laplace (learned density transform), DExp (discrete exponent + Gaussian), MCC (multi-class classification over log-scaled bins).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Numeric language modeling, masked numeric prediction, measurement estimation (distributional targets), numeric generation with uncertainty/multi-modality.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Modeling predictive uncertainty/multimodality in numeric targets via explicit probabilistic outputs allows better fit to real-world numeric distributions (e.g., multimodal times of day).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Spithourakis & Riedel (2018) find GMM effective for causal numeric LM; Spokoyny & Berg-Kirkpatrick (2020) show Log-Laplace/Flow-Laplace/DExp give flexible modeling and that exponent-only embeddings can be strong baselines; Zhang et al. (2020) find MCC works well for distributional quantity prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Some decoders are complex and require corpus-specific pretraining (EM for GMM means fitting means/variances); unimodal decoders fail on multimodal ground truth unless mixture/discrete approaches are used.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Replace point-estimate decoders with distributional decoders (GMM/Log-Laplace/Flow/DExp) or discretize outputs into bins (MCC).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improved numeric LM performance on corpora with multimodal numeric targets and better handling of uncertainty; MCC better on distributional measurement estimation in some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey-level summary: GMM best for causal numeric LM (Spithourakis & Riedel, 2018); MCC outperforms regression on Distribution-over-Quantities (Zhang et al., 2020); explicit numeric performance numbers not presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Require discretization/precision modeling heuristics (GMM precision sampling); pretraining and EM steps; complexity and corpus dependence; may overfit to distributional shapes seen in training data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Distributional decoders mirror human uncertainty in measurement estimation more closely than point estimates; still distinct from deterministic symbolic calculators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Representing Numbers in NLP: a Survey and a Vision', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3148.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e3148.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tokenization & Notation interventions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tokenization (character/digit vs subword) and numeric notation (scientific vs decimal) interventions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Practical interventions that change how numerals are represented in model input (token-level) or in pretraining corpora (notation) to improve numeric behavior and arithmetic generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Tokenization / Notation strategies (char/digit tokenization, scientific notation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Modify input representation: tokenize numerals at character/digit level or rewrite numerals into scientific notation (mantissa+exponent), used with or without pooled digit encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Simple arithmetic (synthetic), arithmetic extrapolation, numeration/magnitude probes, measurement estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Character/digit tokenization preserves digit structure enabling models to learn arithmetic-like mappings and extrapolate better; scientific notation isolates scale (exponent) enabling models to focus on magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Survey cites multiple works: Nogueira et al. (2021) and Wallace et al. (2019) show digit-level tokenization improves arithmetic; Zhang et al. (2020) and Spokoyny & Berg-Kirkpatrick (2020) show scientific notation (exponent embeddings) yields improvements and in some cases exponent embeddings outperform full mantissa modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Pooled digit encoders and notation changes have not been universally compared in controlled settings; scientific notation may discard helpful mantissa information in some domains.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Change tokenization to char/digit-level; pretrain corpus rewrite to scientific notation; pool digit embeddings with RNN/CNN.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Generally improves numeration/magnitude probes and arithmetic performance on low-to-moderate digit operands; NumBERT retains general-language performance while improving measurement estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey-level: character-level tokenization outperforms subword models on probes; NumBERT converges to same MLM loss as BERT while improving measurement estimates; no single numeric accuracy reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Tokenization alone doesn't guarantee robust arithmetic algorithms or extrapolation to arbitrarily large numbers; trade-offs in vocabulary size and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Digit-level processing aligns with human digit parsing; notation (scientific) leverages scale separation akin to some cognitive scale representations but still differs from symbolic arithmetic systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Representing Numbers in NLP: a Survey and a Vision', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Do NLP models know numbers? probing numeracy in embeddings <em>(Rating: 2)</em></li>
                <li>Injecting numerical reasoning skills into language models <em>(Rating: 2)</em></li>
                <li>Do language embeddings capture scales? <em>(Rating: 2)</em></li>
                <li>An empirical investigation of contextualized number prediction <em>(Rating: 2)</em></li>
                <li>Numeracy for language models: Evaluating and improving their ability to predict numbers <em>(Rating: 2)</em></li>
                <li>Methods for numeracy-preserving word embeddings <em>(Rating: 2)</em></li>
                <li>Investigating the limitations of the transformers with simple arithmetic tasks <em>(Rating: 2)</em></li>
                <li>Analysing mathematical reasoning abilities of neural models <em>(Rating: 1)</em></li>
                <li>Neural arithmetic logic units <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3148",
    "paper_id": "paper-28a5a53dafacebad8a7c47773079caeffb9a5baa",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "GPT-3",
            "name_full": "Generative Pre-trained Transformer 3",
            "brief_description": "Large autoregressive transformer language model shown to perform well zero-shot on many NLP tasks, including simple arithmetic when operands are low-digit.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_description": "Large autoregressive Transformer (Brown et al., 2020); publicly reported at 175B parameters; trained with causal language modeling on large web corpora.",
            "arithmetic_task_type": "Zero-shot simple arithmetic (small-digit addition/subtraction), synthetic arithmetic tasks",
            "reported_mechanism": "Pattern memorization and learned token-sequence mapping: solves many low-digit arithmetic examples from training distribution/patterns rather than explicit symbolic algorithm.",
            "evidence_for_mechanism": "Paper reports that GPT-3 \"performs extremely well at zero shot simple arithmetic\" for low-digit operands; the surveyed literature links good performance to tokenization and training distribution that contain many simple arithmetic examples (cited Brown et al., 2020).",
            "evidence_against_mechanism": "Performance degrades with larger numbers/digit counts (limited extrapolation), suggesting absence of robust algorithmic arithmetic; no direct interpretability evidence for algorithmic internal steps provided in survey.",
            "intervention_type": "None applied in this survey beyond noting zero-shot prompting; related interventions (digit tokenization, fine-tuning) discussed elsewhere.",
            "effect_of_intervention": "Not evaluated for GPT-3 specifically in this paper; general observation that tokenization at character/digit level improves arithmetic for transformer models.",
            "performance_metrics": "Described qualitatively as \"extremely well\" on low-digit arithmetic zero-shot; no numeric accuracy values reported in this survey.",
            "notable_failure_modes": "Fails/extrapolates poorly as operand digit length increases; tokenization- and distribution-dependent errors.",
            "comparison_to_humans_or_symbolic": "Compared implicitly: success is distributional/pattern-based rather than true symbolic computation like calculators; extrapolation limitations contrast with exact symbolic methods.",
            "uuid": "e3148.0",
            "source_info": {
                "paper_title": "Representing Numbers in NLP: a Survey and a Vision",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Transformers (general)",
            "name_full": "Transformer-based language models",
            "brief_description": "Family of attention-based sequence models (encoder-only, decoder-only, or encoder-decoder) used as the backbone for many language models; their numeracy depends strongly on tokenization and numeric representation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Transformer-based LMs (BERT, GPT variants, etc.)",
            "model_description": "Self-attention-based architectures (Vaswani et al., 2017) used in encoder (BERT) and autoregressive (GPT) configurations; sizes vary by instantiation.",
            "arithmetic_task_type": "Simple arithmetic (synthetic addition/subtraction), arithmetic word problems, numeric language modeling, numeration/magnitude probes",
            "reported_mechanism": "Learned sequence-to-sequence/pattern mappings influenced by tokenization and numeric surface forms; internal embeddings may implicitly encode magnitude when real-valued encoders or special representations are used.",
            "evidence_for_mechanism": "Survey cites multiple probing studies showing that contextualized embeddings contain numeric information (numeration, magnitude); digit/char tokenization and explicit numeric encoders (Value, DICE) improve numeric probes and some arithmetic tasks.",
            "evidence_against_mechanism": "Default subword tokenization treats numbers as arbitrary tokens and leads to poor arithmetic and extrapolation (e.g., BERT performs worse on numeric answers in DROP), implying that vanilla transformers do not inherently implement robust numeric algorithms.",
            "intervention_type": "Tokenization changes (character/digit-level), numeric-specific encoders/decoders (Value, LogValue, DICE), scientific notation pretraining (NumBERT), finetuning on arithmetic (GenBERT), distributional decoders (GMM, Log-Laplace, DExp, MCC).",
            "effect_of_intervention": "Interventions generally improve numeracy: char/digit tokenization and pooled digit encoders improve arithmetic and probes; scientific notation pretraining (NumBERT) helps numeric masked LM and measurement estimation; real-based encoders improve magnitude probes.",
            "performance_metrics": "Survey reports qualitative and comparative results: e.g., subword models worse than char models on probes (Wallace et al., 2019), BERT five-fold worse on numeric answers in DROP (cited Dua et al., 2019); no unified numeric benchmark scores provided.",
            "notable_failure_modes": "Poor extrapolation to larger unseen numbers, sensitivity to tokenization and notation, poor performance on exact grounded tasks (arithmetics in word problems) unless specialized components are used.",
            "comparison_to_humans_or_symbolic": "Transformers behave more like pattern learners with limited extrapolation, differing from human approximate number systems and from exact symbolic computation; cognitive-science-inspired log-scale encodings sometimes better match approximate reasoning.",
            "uuid": "e3148.1",
            "source_info": {
                "paper_title": "Representing Numbers in NLP: a Survey and a Vision",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "BERT",
            "name_full": "Bidirectional Encoder Representations from Transformers",
            "brief_description": "Masked language model based on transformer encoder stacks; frequently used as baseline for numeracy probes and augmented with numeric-specific methods.",
            "citation_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "mention_or_use": "mention",
            "model_name": "BERT (base/large variants)",
            "model_description": "Masked Transformer encoder pretrained on large corpora with next-sentence/prediction objectives; typical base sizes ~110M, large ~340M parameters (paper does not re-run sizes).",
            "arithmetic_task_type": "Masked numeric language modeling, numeration/magnitude probes, measurement estimation, numeric commonsense (NumerSense), arithmetic-related QA (DROP).",
            "reported_mechanism": "Encodes numeric information implicitly in contextual embeddings; default subword tokenization fragments numbers, limiting numeric generalization; can be improved by pretraining with scientific notation or digit-level tokenization.",
            "evidence_for_mechanism": "Probing studies (Wallace et al., 2019; Naik et al., 2019) show BERT contains numeric signal but underperforms vs char-tokenized/contextual char models; Zhang et al. (2020) show NumBERT (scientific notation pretraining) converges to similar MLM loss and improves measurement estimation.",
            "evidence_against_mechanism": "On DROP QA, BERT is ~5x worse when the answer is a number vs a text span (cited Dua et al., 2019), indicating limitations for numeric extraction/reasoning in downstream tasks.",
            "intervention_type": "Pretraining with scientific notation (NumBERT), finetuning on numeric tasks, replacing tokenization with char/digit-level, adding real-valued number encoders (Value/LogValue), pooled DigitRNN/DigitCNN embeddings.",
            "effect_of_intervention": "Scientific-notation pretraining (NumBERT) improves measurement estimation and numeric masked LM behavior while preserving general-language performance; digit/char tokenization and pooled digit encoders improve numeration/magnitude probes.",
            "performance_metrics": "Survey cites qualitative and comparative findings: NumBERT achieves similar MLM loss to BERT on GLUE; BERT has substantially degraded performance on number-answer cases in DROP (~5x worse) — specific numeric accuracies not reported here.",
            "notable_failure_modes": "Subword tokenization splits numerals arbitrarily, harming representation; poor extrapolation to unseen numeric ranges; struggles on exact arithmetic and grounded numeric facts without augmentation.",
            "comparison_to_humans_or_symbolic": "BERT's implicit numeric encodings are unlike symbolic calculators and only partially align with human approximate number representations; log-scale encodings inspired by cognitive models help in approximate tasks.",
            "uuid": "e3148.2",
            "source_info": {
                "paper_title": "Representing Numbers in NLP: a Survey and a Vision",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "GenBERT",
            "name_full": "GenBERT (BERT finetuned on arithmetic tasks)",
            "brief_description": "BERT-based model tokenizing numbers at digit-level and finetuned on auxiliary arithmetic tasks; used to inject numerical reasoning skills to improve QA.",
            "citation_title": "Injecting numerical reasoning skills into language models",
            "mention_or_use": "mention",
            "model_name": "GenBERT",
            "model_description": "BERT architecture with digit-level tokenization; finetuned on synthetic arithmetic and arithmetic word-problem auxiliary tasks to improve numerical QA.",
            "arithmetic_task_type": "Simple arithmetic, arithmetic word problems; auxiliary pretraining tasks for downstream QA (e.g., DROP).",
            "reported_mechanism": "Training on arithmetic tasks teaches the model pattern-based mappings and assists the model in mapping counts/operations into tokens and embeddings — effectively a fine-tuning intervention that imparts task-specific algorithmic behavior or heuristics.",
            "evidence_for_mechanism": "Paper reports GenBERT finetuned on arithmetic improves scores on DROP QA relative to baselines, indicating transfer from explicit arithmetic finetuning to downstream numeric QA.",
            "evidence_against_mechanism": "Survey notes that state-of-the-art AWP solvers often bypass numeric embedding by predicting equations then filling numeric values, suggesting that encoding numbers into embeddings alone may not fully solve grounded arithmetic problems.",
            "intervention_type": "Auxiliary finetuning on simple arithmetic and arithmetic word problems; digit-level tokenization.",
            "effect_of_intervention": "Improved performance on DROP QA and related numeric QA tasks compared to vanilla BERT baselines.",
            "performance_metrics": "Reported qualitatively: GenBERT finetuned on simple arithmetic helps it do well on downstream QA (DROP); specific numeric accuracy values are not enumerated in this survey.",
            "notable_failure_modes": "May rely on pattern learning limited to finetuning distribution; grounded arithmetic word problems sometimes solved by equation-prediction methods rather than improved numeric embeddings.",
            "comparison_to_humans_or_symbolic": "Finetuning imparts task-specific strategies akin to training humans on arithmetic examples; still distinct from explicit symbolic computation.",
            "uuid": "e3148.3",
            "source_info": {
                "paper_title": "Representing Numbers in NLP: a Survey and a Vision",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "NumBERT",
            "name_full": "NumBERT (scientific-notation-pretrained BERT)",
            "brief_description": "BERT model pretrained from scratch on data where numbers are rewritten in scientific notation to emphasize exponent information and improve numeric representations.",
            "citation_title": "Do language embeddings capture scales?",
            "mention_or_use": "mention",
            "model_name": "NumBERT",
            "model_description": "BERT pretrained on a modified corpus where numerals are converted to scientific notation (mantissa+exponent tokens), with subword tokenization; otherwise same training objectives as BERT.",
            "arithmetic_task_type": "Masked numeric language modeling, measurement estimation, numeric prediction tasks (distributional quantities).",
            "reported_mechanism": "By rewriting numbers into scientific notation, NumBERT shifts learning focus to exponent (scale) embeddings, enabling models to capture magnitude information more robustly than decimal tokenization.",
            "evidence_for_mechanism": "Zhang et al. (2020) show NumBERT converges to the same MLM loss as BERT and performs better on measurement estimation tasks; Spokoyny & Berg-Kirkpatrick report exponent embeddings can outperform DigitRNN-sci, supporting the hypothesis that exponent/scale features are most informative.",
            "evidence_against_mechanism": "Mantissa modeling may be unnecessary/overkill for some corpora; scientific-notation changes do not in themselves produce explicit arithmetic algorithmic ability or full extrapolation.",
            "intervention_type": "Pretraining corpus transformation (scientific notation), standard masked LM pretraining.",
            "effect_of_intervention": "Improved performance on measurement estimation and certain numeric masked LM tasks while maintaining general-language performance on GLUE-like benchmarks.",
            "performance_metrics": "Survey reports NumBERT \"converges to the same loss as BERT on masked language modelling\" and scores nearly the same on GLUE; specific numeric error/accuracy numbers not provided here.",
            "notable_failure_modes": "Still limited on exact arithmetic and some grounded numeric tasks; representation focus on exponent can ignore useful mantissa details in some domains.",
            "comparison_to_humans_or_symbolic": "NumBERT's focus on scale (exponent) echoes human approximate magnitude representations (log-like), but it is not equivalent to symbolic arithmetic systems.",
            "uuid": "e3148.4",
            "source_info": {
                "paper_title": "Representing Numbers in NLP: a Survey and a Vision",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "DigitRNN/DigitCNN",
            "name_full": "Digit-level RNN/CNN pooled encoders (DigitRNN / DigitCNN)",
            "brief_description": "String-based approach that tokenizes numerals at the digit/character level and pools those digit embeddings via an RNN or CNN to produce a single number embedding.",
            "citation_title": "Numeracy for language models: Evaluating and improving their ability to predict numbers",
            "mention_or_use": "mention",
            "model_name": "DigitRNN / DigitCNN pooled encoders",
            "model_description": "Character/digit tokenization followed by a pooling network (RNN or CNN) over digits to produce a fixed-size embedding representing the full numeral, plugged into downstream models.",
            "arithmetic_task_type": "Numeration probes, magnitude comparison, numeric language modeling, simple arithmetic when used in models.",
            "reported_mechanism": "By processing digits sequentially or via convolution, these encoders can capture structure and magnitude of numerals from surface form, enabling better generalization than naive subword tokenization.",
            "evidence_for_mechanism": "Survey cites Spithourakis & Riedel (2018) and Wallace et al. (2019) showing pooled digit encoders outperform subword baselines on numeration/magnitude probes; DigitCNNs reported as best number encoders in probes.",
            "evidence_against_mechanism": "Pooling approaches lack controlled comparisons with unpooled scientific-notation methods (NumBERT) in some studies; digit RNNs may be outperformed by simple exponent embedding variants in some corpora (Spokoyny & Berg-Kirkpatrick).",
            "intervention_type": "Replace tokenization with digit/char-level and add pooling network (RNN/CNN) to produce number embeddings.",
            "effect_of_intervention": "Improves numeration and magnitude comparison probes, and can improve arithmetic performance when integrated; digit-level tokenization improves extrapolation compared to subword tokenization.",
            "performance_metrics": "Reported relatively: DigitCNNs and character-tokenized models outperform subword ones on abstract probes (Wallace et al., 2019); no absolute accuracies included in this survey.",
            "notable_failure_modes": "May be computationally heavier; performance depends on pooling architecture and training stability; may still fail on large-range continuous value prediction without discretization.",
            "comparison_to_humans_or_symbolic": "Digit-based encodings more akin to human parsing of numerals but still produce distributed embeddings rather than explicit symbolic calculation.",
            "uuid": "e3148.5",
            "source_info": {
                "paper_title": "Representing Numbers in NLP: a Survey and a Vision",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "DICE",
            "name_full": "Deterministic Independent-of-Corpus Embeddings",
            "brief_description": "Handcrafted (deterministic) real-based number encoder designed to preserve relative magnitude relations in embedding space (monotonic relation between scalar distance and embedding cosine distance).",
            "citation_title": "Methods for numeracy-preserving word embeddings",
            "mention_or_use": "mention",
            "model_name": "DICE encoder",
            "model_description": "Deterministic mapping f: R -&gt; R^d crafted so cosine distances between embeddings reflect numeric distances/ordering, usable as a plug-in numeric encoder or as an auxiliary loss to shape learned embeddings.",
            "arithmetic_task_type": "Numeration and magnitude comparison probes, numeric language modeling transfer tasks.",
            "reported_mechanism": "Imposes explicit inductive bias of numeric continuity/ordering onto embeddings so models can use geometric relations to infer magnitude comparisons and order.",
            "evidence_for_mechanism": "Survey reports DICE scores highly on numeration and magnitude comparison probes and can act as an auxiliary loss to improve numeracy-preserving word embeddings (Sundararaman et al., 2020).",
            "evidence_against_mechanism": "DICE is encoder-only and not easily reversible for decoding numbers; discretized or learned decoders may be needed for generative numeric tasks.",
            "intervention_type": "Replace or augment numeric token embeddings with deterministic DICE encodings; optionally use as auxiliary training loss.",
            "effect_of_intervention": "Improves abstract probes (numeration, magnitude); transfer benefits observed for related numeric tasks (e.g., order-of-magnitude prediction).",
            "performance_metrics": "Reported qualitatively: DICE shows high scores on numeration/magnitude comparison probes in cited work; no absolute numeric metrics provided in this survey text.",
            "notable_failure_modes": "Not reversible for decoding; constrains representational variance which may hurt tasks needing discrete enumeration/precision; coverage decisions required.",
            "comparison_to_humans_or_symbolic": "Imposes continuity bias akin to human approximate magnitude representation; differs from symbolic arithmetic in being an embedding constraint rather than procedural computation.",
            "uuid": "e3148.6",
            "source_info": {
                "paper_title": "Representing Numbers in NLP: a Survey and a Vision",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Value / LogValue embeddings",
            "name_full": "Value embedding and Log-Value embedding (real-based encoders/decoders)",
            "brief_description": "Parametric shallow neural encoders/decoders that map scalar numeric magnitude (linearly or on log scale) to/from vector embeddings; log-scaling inspired by cognitive models of numerosity.",
            "citation_title": "Do language embeddings capture scales?",
            "mention_or_use": "mention",
            "model_name": "Value / LogValue encoders/decoders",
            "model_description": "Value: shallow NN projecting scalar to embedding and reverse via probe g: R^d-&gt;R. LogValue: same but operate on log(number) to emphasize scale and compress large ranges.",
            "arithmetic_task_type": "Numeration (decoding), magnitude comparison, measurement estimation, numeric language modeling (regression-style decoding).",
            "reported_mechanism": "Encodes numeric magnitude directly into continuous latent space; log scaling compresses wide numeric ranges and aligns with human approximate (log-like) perception.",
            "evidence_for_mechanism": "Survey notes Value and LogValue excel on numeration and magnitude probes; log-scale preferred in several studies (Zhang et al., 2020; Wallace et al., 2019) and matches cognitive science intuition (Dehaene, 2011).",
            "evidence_against_mechanism": "Value embeddings struggle to extrapolate to larger numbers and continuous prediction across large ranges is practically difficult (training instability); continuous decoders underperform discrete binning on some data (Zhang et al., 2020).",
            "intervention_type": "Replace/augment numeric tokens with a learned mapping from scalar value (linear or log) to embedding; use regression probes for decoding.",
            "effect_of_intervention": "Improves approximate numeric tasks and probes; LogValue better for wide-range estimation and measurement tasks.",
            "performance_metrics": "Reported qualitatively: Value/LogValue do well on numeration/magnitude probes; continuous regression worse than binning (dense cross-entropy) for distributional ground-truth in some datasets.",
            "notable_failure_modes": "Poor extrapolation to very large/unseen numbers; instability when training continuous regressors over large ranges.",
            "comparison_to_humans_or_symbolic": "LogValue aligns with human approximate-number-system modeling (log-like perception) but is not a symbolic arithmetic procedure.",
            "uuid": "e3148.7",
            "source_info": {
                "paper_title": "Representing Numbers in NLP: a Survey and a Vision",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Distributional decoders (GMM / Log-Laplace / Flow-Laplace / DExp / MCC)",
            "name_full": "Gaussian mixture and distribution-parameterized numeric decoders (GMM, Log-Laplace, Flow-Laplace, Discrete Exponent, MCC)",
            "brief_description": "Real-valued decoders that output distributions (continuous or discretized) over numbers instead of point estimates, enabling multimodal and uncertainty-aware numeric predictions for numeric language modeling.",
            "citation_title": "An empirical investigation of contextualized number prediction",
            "mention_or_use": "mention",
            "model_name": "Distributional numeric decoders (GMM, Log-Laplace, Flow-Laplace, DExp, MCC)",
            "model_description": "Decoders parameterize distributions over numbers: GMMs (mixture of Gaussians), Log-Laplace (model on log scale), Flow-Laplace (learned density transform), DExp (discrete exponent + Gaussian), MCC (multi-class classification over log-scaled bins).",
            "arithmetic_task_type": "Numeric language modeling, masked numeric prediction, measurement estimation (distributional targets), numeric generation with uncertainty/multi-modality.",
            "reported_mechanism": "Modeling predictive uncertainty/multimodality in numeric targets via explicit probabilistic outputs allows better fit to real-world numeric distributions (e.g., multimodal times of day).",
            "evidence_for_mechanism": "Spithourakis & Riedel (2018) find GMM effective for causal numeric LM; Spokoyny & Berg-Kirkpatrick (2020) show Log-Laplace/Flow-Laplace/DExp give flexible modeling and that exponent-only embeddings can be strong baselines; Zhang et al. (2020) find MCC works well for distributional quantity prediction.",
            "evidence_against_mechanism": "Some decoders are complex and require corpus-specific pretraining (EM for GMM means fitting means/variances); unimodal decoders fail on multimodal ground truth unless mixture/discrete approaches are used.",
            "intervention_type": "Replace point-estimate decoders with distributional decoders (GMM/Log-Laplace/Flow/DExp) or discretize outputs into bins (MCC).",
            "effect_of_intervention": "Improved numeric LM performance on corpora with multimodal numeric targets and better handling of uncertainty; MCC better on distributional measurement estimation in some datasets.",
            "performance_metrics": "Survey-level summary: GMM best for causal numeric LM (Spithourakis & Riedel, 2018); MCC outperforms regression on Distribution-over-Quantities (Zhang et al., 2020); explicit numeric performance numbers not presented here.",
            "notable_failure_modes": "Require discretization/precision modeling heuristics (GMM precision sampling); pretraining and EM steps; complexity and corpus dependence; may overfit to distributional shapes seen in training data.",
            "comparison_to_humans_or_symbolic": "Distributional decoders mirror human uncertainty in measurement estimation more closely than point estimates; still distinct from deterministic symbolic calculators.",
            "uuid": "e3148.8",
            "source_info": {
                "paper_title": "Representing Numbers in NLP: a Survey and a Vision",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Tokenization & Notation interventions",
            "name_full": "Tokenization (character/digit vs subword) and numeric notation (scientific vs decimal) interventions",
            "brief_description": "Practical interventions that change how numerals are represented in model input (token-level) or in pretraining corpora (notation) to improve numeric behavior and arithmetic generalization.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Tokenization / Notation strategies (char/digit tokenization, scientific notation)",
            "model_description": "Modify input representation: tokenize numerals at character/digit level or rewrite numerals into scientific notation (mantissa+exponent), used with or without pooled digit encoders.",
            "arithmetic_task_type": "Simple arithmetic (synthetic), arithmetic extrapolation, numeration/magnitude probes, measurement estimation.",
            "reported_mechanism": "Character/digit tokenization preserves digit structure enabling models to learn arithmetic-like mappings and extrapolate better; scientific notation isolates scale (exponent) enabling models to focus on magnitude.",
            "evidence_for_mechanism": "Survey cites multiple works: Nogueira et al. (2021) and Wallace et al. (2019) show digit-level tokenization improves arithmetic; Zhang et al. (2020) and Spokoyny & Berg-Kirkpatrick (2020) show scientific notation (exponent embeddings) yields improvements and in some cases exponent embeddings outperform full mantissa modeling.",
            "evidence_against_mechanism": "Pooled digit encoders and notation changes have not been universally compared in controlled settings; scientific notation may discard helpful mantissa information in some domains.",
            "intervention_type": "Change tokenization to char/digit-level; pretrain corpus rewrite to scientific notation; pool digit embeddings with RNN/CNN.",
            "effect_of_intervention": "Generally improves numeration/magnitude probes and arithmetic performance on low-to-moderate digit operands; NumBERT retains general-language performance while improving measurement estimation.",
            "performance_metrics": "Survey-level: character-level tokenization outperforms subword models on probes; NumBERT converges to same MLM loss as BERT while improving measurement estimates; no single numeric accuracy reported here.",
            "notable_failure_modes": "Tokenization alone doesn't guarantee robust arithmetic algorithms or extrapolation to arbitrarily large numbers; trade-offs in vocabulary size and efficiency.",
            "comparison_to_humans_or_symbolic": "Digit-level processing aligns with human digit parsing; notation (scientific) leverages scale separation akin to some cognitive scale representations but still differs from symbolic arithmetic systems.",
            "uuid": "e3148.9",
            "source_info": {
                "paper_title": "Representing Numbers in NLP: a Survey and a Vision",
                "publication_date_yy_mm": "2021-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Do NLP models know numbers? probing numeracy in embeddings",
            "rating": 2
        },
        {
            "paper_title": "Injecting numerical reasoning skills into language models",
            "rating": 2
        },
        {
            "paper_title": "Do language embeddings capture scales?",
            "rating": 2
        },
        {
            "paper_title": "An empirical investigation of contextualized number prediction",
            "rating": 2
        },
        {
            "paper_title": "Numeracy for language models: Evaluating and improving their ability to predict numbers",
            "rating": 2
        },
        {
            "paper_title": "Methods for numeracy-preserving word embeddings",
            "rating": 2
        },
        {
            "paper_title": "Investigating the limitations of the transformers with simple arithmetic tasks",
            "rating": 2
        },
        {
            "paper_title": "Analysing mathematical reasoning abilities of neural models",
            "rating": 1
        },
        {
            "paper_title": "Neural arithmetic logic units",
            "rating": 1
        }
    ],
    "cost": 0.01910075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Representing Numbers in NLP: a Survey and a Vision</h1>
<p>Avijit Thawani and Jay Pujara and Filip Ilievski and Pedro Szekely<br>University of Southern California<br>Information Sciences Institute<br>{thawani, jpujara,ilievski,pszekely}@isi.edu</p>
<h4>Abstract</h4>
<p>NLP systems rarely give special consideration to numbers found in text. This starkly contrasts with the consensus in neuroscience that, in the brain, numbers are represented differently from words. We arrange recent NLP work on numeracy into a comprehensive taxonomy of tasks and methods. We break down the subjective notion of numeracy into 7 subtasks, arranged along two dimensions: granularity (exact vs approximate) and units (abstract vs grounded). We analyze the myriad representational choices made by over a dozen previously published number encoders and decoders. We synthesize best practices for representing numbers in text and articulate a vision for holistic numeracy in NLP, comprised of design trade-offs and a unified evaluation.</p>
<h2>1 Introduction</h2>
<p>Numbers are an integral part of text. To understand a simple sentence like I woke up at 11, we need not just literacy but also numeracy. We must decode the string 11 to the quantity 11 and infer 11 to denote a time of the day, probably 11 a.m. We need commonsense to reason that 11 a.m. is quite late in the morning. This interpretation of 11 is strongly contextual, as I earn $\$ 11$ per month evokes different units and value expectations. Note how the semantics remains the same for both sentences if 11 was replaced by 10 , i.e., the context is tolerant to some variability.</p>
<p>Numbers are everywhere. Reasoning with quantities and counts is crucial to understanding the world. Evolutionary learning has given numerical cognition skills to several animals, including human beings (Dehaene, 2011). Our ancient ancestors furthered numeracy by developing multiple number systems, similar to but independent from the evolution of languages. Numeracy is an essential skill for language understanding, since numbers are often interspersed in text: the 6 million pages in English Wikipedia have over 150 million numbers.</p>
<p>Numbers are neglected. In NLP, however, numbers are either filtered out explicitly during preprocessing (Graff et al., 2003), or treated the same as words, often collapsing them into an UNK token. Subword tokenization approaches like BPE (Sennrich et al., 2016) and WordPiece (Wu et al., 2016) instead retain numbers, but split them into arbitrary tokens, for example 1234 might be split into two tokens as 12-34 or 123-4 or 1-234.</p>
<p>Recent work has shown that these are suboptimal number representations (Wallace et al., 2019; Zhang et al., 2020). On the DROP Question Answering benchmark, BERT performs five times worse when the answer is a number instead of a span of text (Dua et al., 2019). Relatively simple strategies like switching from subword to char-level tokenization (Geva et al., 2020), or from decimal to scientific notation (Zhang et al., 2020) already boost performance. Such results warrant a deeper study into the best number representations.</p>
<p>Numbers are important. Given the ubiquity of numbers and their fundamental differences with words, enabling NLP systems to represent them effectively is beneficial for domains like scientific articles (Spithourakis and Riedel, 2018) and financial documents (Chen et al., 2019; Jiang et al., 2020). Number understanding is also useful to detect sarcasm (Dubey et al., 2019) and to model dialogues involving price negotiations (Chawla et al., 2020).</p>
<p>Recent NLP progress towards numeracy has been sporadic but encouraging. In this paper, we survey prior work and highlight the kind of numeracy targeted (e.g., arithmetic, measurement, numeration) as well as the kind of representation used (e.g., value embeddings, DigitRNNs). We provide the first NLP-centric taxonomy of numeracy tasks (Section 2) and of number representations (Section 3) for the reader to succinctly comprehend the challenge posed by numeracy. We synthesize key takeaways (Section 5) and propose a unifying vision for future research (Section 6).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Benchmarking or Probing Tasks</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Downstream <br> Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Abstract</td>
<td style="text-align: center;">Grounded</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Exact</td>
<td style="text-align: center;">Simple Arithmetic $(2+3=5)$</td>
<td style="text-align: center;">AWP (2 balls +3 balls $=5$ balls), <br> Exact Facts (birds have two legs)</td>
<td style="text-align: center;">Question Answering, <br> Science Problems</td>
</tr>
<tr>
<td style="text-align: left;">Approx</td>
<td style="text-align: center;">Numeration ('2' = 2.0), <br> Magnitude ('2' &lt; '5')</td>
<td style="text-align: center;">Measurement (dogs weigh 50 lbs), <br> Numerical Language Modeling</td>
<td style="text-align: center;">Sarcasm Detection, <br> Numeral Categorization</td>
</tr>
</tbody>
</table>
<p>Table 1: Seven numeracy tasks, arranged along the axes of (rows) granularity - exact vs approximate, and (columns) units - abstract vs grounded. We also list downstream applications requiring a similar granularity of numeracy.</p>
<h2>2 Tasks</h2>
<p>There are several different aspects of numeracy. The DROP dataset alone offers a wide variety of numeric reasoning questions such as retrieval-based (How many yards did Brady run?), count-based (How many goals were scored? given a comprehension describing multiple goals), and simple arithmetic (How many years after event 1 did event 2 occur? given dates of both events). Besides downstream applications, there have also been probing experiments to evaluate whether NLP models can decode numbers from strings (e.g., 19 to 19.0), or estimate quantities (e.g., how tall are lions?).</p>
<p>Such a diverse range of abilities are usually all referred to collectively as numeracy, which gives rise to confusion. We limit this abuse of terminology and provide a neat taxonomy for arranging the different tasks proposed under numeracy.</p>
<h3>2.1 Our Taxonomy of Tasks</h3>
<p>Drawing from work in cognitive science (Feigenson et al., 2004), we propose the following two dimensions to organize tasks within numeracy:</p>
<ol>
<li>Granularity: whether the encoding of the number is (1) exact, e.g., birds have two legs, or (2) approximate, e.g., Jon is about 180 cms tall.</li>
<li>Units: whether the numbers are (1) abstract, e.g., $2+3=5$, or (2) grounded, e.g., 2 apples +3 apples $=5$ apples. While abstract mathematical tasks are easy to probe and create artificial datasets for, numbers grounded in units are challenging since they need to be understood in the context of words.</li>
</ol>
<h3>2.2 Survey of Existing Tasks</h3>
<p>We now describe 7 numeracy tasks, arranged according to our taxonomy in Table 1, as well as downstream tasks (right-most column in the table).</p>
<p>Simple Arithmetic is the task of addition, subtraction, etc. over numbers alone. It is convenient
to create synthetic datasets involving such math operations for both masked (Geva et al., 2020) and causal language models (GPT-3 Brown et al. 2020).</p>
<p>Numeration or Decoding refers to the task of mapping a string form to its numeric value, e.g., 19 to 19.0. Within NLP, this task is set up as a linear regressor probe over a (frozen) representation of the string. Numeration has been probed for in static word embeddings (Naik et al., 2019), contextualized language models (Wallace et al., 2019), and multilingual number words, e.g., nineteen or dix-neuf (Johnson et al., 2020).</p>
<p>Magnitude Comparison is the ability to tell which of two (or more) numbers is larger. For language models, this has been probed in an argmax setup (choose the largest of five numbers) as well as a binary classification task, e.g., given 23 and 32, pick the label 1 to indicate that $32&gt;23$ (Naik et al., 2019; Wallace et al., 2019).</p>
<p>Arithmetic Word Problems (AWP) are the grounded version of simple arithmetic that we find in school textbooks, e.g., Mary had two cookies. She gave one away. How many does she have left? There exist several NLP datasets on math word problems (Amini et al., 2019; Saxton et al., 2019; Roy and Roth, 2015; Hendrycks et al., 2021).</p>
<p>Exact Facts in the context of numeracy involves commonsense knowledge such as dice have 6 faces or birds have two legs. An approximate sense of quantity would be of little help here since assertions like dice have 5 faces or birds have three legs are factually incorrect. Two recent datasets for numeric commonsense facts are Numbergame (Mishra et al., 2020) and NumerSense (Lin et al., 2020).</p>
<p>Measurement Estimation is a task in psychology in which subjects are asked to approximately guess measures of objects along certain dimensions, e.g., number of seeds in a watermelon or weight of a telephone (Bullard et al., 2004). VerbPhysics (Forbes and Choi, 2017) is a benchmark of binary</p>
<p>comparisons between physical attributes of various objects, e.g., ball $&lt;_{\text {size }}$ tiger. DoQ (Elazar et al., 2019) is a web-extracted dataset of Distributions over Quantities, which can be used as a benchmark for language models' measurement estimation abilities (Zhang et al., 2020). Lastly, MC-TACO (Zhou et al., 2020) is a collection of temporal-specific measurement estimates, e.g., going for a vacation spans a few days/weeks.</p>
<p>Numerical Language Modeling in its literal sense is not a task but a setup, analogous to masked/causal language modeling for words. Other tasks could be modeled as numeric language modeling, e.g., arithmetic $(2+3=[M A S K])$ and measurement estimation (lions weigh [MASK] pounds). In practice, numerical language modeling refers to the task of making numeric predictions for completing unlabelled, naturally occurring text.</p>
<p>Word predictions in language modeling are typically evaluated with classification metrics such as accuracy or perplexity. Numeric predictions, on the other hand, are evaluated with regression metrics such as mean absolute error, root mean squared error, or their log and percentage variants (Spokoyny and Berg-Kirkpatrick, 2020). Spithourakis and Riedel (2018) also propose an Adjusted Perplexity metric to cancel the effect of the out-of-vocabulary rate on the perplexity of numeric tokens.</p>
<p>Downstream Applications for numeracy are abound. Dubey et al. (2019) detect sarcasm in tweets based on numbers. Chen et al. (2020) identify claims in financial documents using alternative number representations and the auxiliary task of numeral understanding or categorization (Chen et al., 2018). Similarly, simple arithmetic and math word problems serve as auxiliary tasks for GenBERT (Geva et al., 2020) towards improving its score on the DROP QA benchmark.</p>
<h3>2.3 Other Numeracy Tasks</h3>
<p>Here, we describe foundational numeracy-related tasks that cut across our taxonomy of tasks:
(Numeric) Paraphrasing is what we call the task of identifying one-to-one correspondences between different surface forms of the same number. Twelve is the same as ' 12 ', also referred to as a dozen. This task cuts across all the tasks we discussed, since the same number, expressed in several different ways, should be nevertheless identified by an NLP model before any subsequent reasoning. Similar to how WordNet (Miller, 1995) provides
a huge list of synonyms, numeric paraphrases can be obtained by libraries ${ }^{1}$ which convert numerals to words, words to numerals, etc. One could also envision this as a learning task given a large enough corpus, such as the NumGen dataset (Williams and Power, 2010) containing 2000 fact-aligned numeric expressions over 110 articles.</p>
<p>Quantity Entailment tasks (Ravichander et al., 2019; Roy et al., 2015), analogous to Natural Language Inference, require understanding of not equivalence (as in paraphrasing) but deeper relations like entailment and contradiction, e.g., the premise he was 16 yrs old entails the hypothesis he was a teenager. On similar lines, Mishra et al. (2020) modify the QuaRel dataset (Tafjord et al., 2019) to force models to perform quantity entailment, e.g., dog1 is light, dog2 is heavy is replaced with dog1 weighs 70 lbs, dog2 weighs 90 lbs.</p>
<p>Numeral Understanding is the task of categorizing numbers into percentages, prices, dates, times, quantities, etc. and their respective subcategories (Chen et al., 2018).</p>
<p>Fused-Head Resolution for numbers is essential to ground them when the context is implicit. For example, the sentence I woke up at 11 has a.m. or o'clock as the fused head to be resolved (Elazar and Goldberg, 2019).</p>
<p>Counting is the task of keeping track of discrete instances of some object. When kids count a set of objects, they quickly learn to keep a track, say on their fingers, but struggle with realizing the Cardinal Principle, i.e., the last counter value denotes the number of entities being considered (Wynn, 1990). Similarly, LSTMs (Suzgun et al., 2019) and transformers (Bhattamishra et al., 2020) have been shown to possess counting skills but in order to answer counting questions, they must also learn to map the counts to number words or numerals. Counting tasks have been proposed in computer vision (Testolin et al., 2020) as well as in NLP (Postma et al., 2018; Talmor et al., 2020).</p>
<p>Domain-specific tasks require background knowledge in addition to exact mathematical skills. Numbergame (Mishra et al., 2020) includes questions on Physics (find the distance travelled in 2 hrs by a train moving at 50 mph ) and Chemistry (find the mass percentage of $H$ in C6H6). Project Aristo (Clark et al., 2019) solves elementary and high school science problems, which often involve numeric reasoning.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>3 Methods</h2>
<p>Analogous to our taxonomy of subtasks in the previous section, here we attempt to arrange the wide variety of alternative number representations proposed in recent literature. We limit our analysis to methods of encoding (numbers $\rightarrow$ embeddings) and/or decoding (embeddings $\rightarrow$ numbers) numbers. We do not discuss, for example, methods that use symbolic reasoning (Andor et al., 2019) or modify activation functions to enhance numeracy (Trask et al., 2018).</p>
<p>A typical example of the base architecture could be BERT (Devlin et al., 2019), the workhorse of modern NLP. We assume that there exists an independent parallel process of mapping words into embeddings, such as subword tokenization followed by lookup embeddings in BERT.</p>
<h3>3.1 Our Taxonomy</h3>
<p>We look at two kinds of representations: stringbased and real-based. Real-based representations perform some computation involving the numerical value of the number. The string-based representations instead see numbers in their surface forms; they must assign arbitrary token IDs and look up their embeddings to feed into the architecture.</p>
<h3>3.1.1 String Based</h3>
<p>By default, language models treat numbers as strings, the same as words. However, within string representations, one could tweak simple changes:</p>
<p>Notation: The number 80 could be written in Hindu-Arabic numerals (80), Roman numerals (LXXX), scientific notation (8e1), English words (eighty), or with base 20 as in French (quatrevingts). Nogueira et al. (2021) exclusively study the effect of many such notation choices in language models, on the task of simple arithmetic.</p>
<p>Tokenization: Word level tokenizations are ineffective for numbers, since they are likely to map most numbers to an UNK token, except for a few commonly occuring ones (e.g., 1, 2, 5, 10, 100). Other possibilities are subword tokenizations like BPE and WordPiece, as well as character (or digit) level tokenizations.</p>
<p>Pooling: The pooling dimension of variation springs up after analyzing the effect of tokenization. With subword and character level tokenizations, a single number may now correspond to multiple tokens, e.g., 100 segmented into $10-0$ or $1-0-0$. Prior work (Spithourakis and Riedel, 2018) has ar-
gued for using RNNs or CNNs to instead pool the embeddings of these tokens into a single embedding before feeding to the language model. The default way that language models see numbers are the same as words, hence no pooling is applied.</p>
<h3>3.1.2 Real Based</h3>
<p>Real-based number encoders can be expressed as $f: \mathbb{R} \rightarrow \mathbb{R}^{d}$ whereas decoders can be expressed as $g: \mathbb{R}^{d} \rightarrow \mathbb{R}$. Real-based methods proposed in literature can vary on account of direction (whether they encode, decode or both), scale (linear vs log), and discretization (binning vs continuous valued).</p>
<p>Direction: Some proposed methods are encoderonly, e.g., DICE (Sundararaman et al., 2020), while some can be decoder-only, e.g., those requiring sampling from a parameterized distribution (Spokoyny and Berg-Kirkpatrick, 2020).</p>
<p>Scale: Inspired by cognitive science literature (Dehaene, 2011), several methods have attempted to model numbers in the log (instead of linear) scale, i.e., to perform mathematical operations on the logarithm of the number to be represented. The first operation in a log-scaled $f$ is $\log (\cdot)$ and the last operation in a log-scaled $g$ is $\exp (\cdot)$. We discuss more scales in the following subsection, such as the stabilized log scale (Jiang et al., 2020) and the learned scale/flow (Spokoyny and BergKirkpatrick, 2020).</p>
<p>Discretization: Training continuous value functions for a large range of numbers turns out to be practically infeasible (Wallace et al., 2019). Some real-based methods first bin numbers before learning embeddings for each bin. These bins could be on the linear scale $(0-10,10-20,20-30, \ldots)$ or the $\log$ scale $(0.01-0.1,0.1-1,1-10, \ldots)$, and the lookup embeddings can be learnt by the regular cross entropy (Chen et al., 2020) or dense cross entropy (Zhang et al., 2020).</p>
<h3>3.2 Survey of Existing Methods</h3>
<p>Having established dimensions of variance of number representations, we describe some key stringbased and real-based methods used in prior work. Table 2 depicts these methods as individual rows, with the first three columns showing their position in our taxonomy (§ 3.1). The last seven columns correspond to the seven tasks (§ 2.2), with each cell denoting a representative work that introduce it.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Exact</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Approximate</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Arith</td>
<td style="text-align: center;">Facts</td>
<td style="text-align: center;">AWP</td>
<td style="text-align: center;">Num</td>
<td style="text-align: center;">Mag</td>
<td style="text-align: center;">Meas</td>
</tr>
<tr>
<td style="text-align: center;">String-Based</td>
<td style="text-align: center;">Notation</td>
<td style="text-align: center;">Tokenization</td>
<td style="text-align: center;">Pooling</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Word Vectors</td>
<td style="text-align: center;">Decimal</td>
<td style="text-align: center;">Word</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">W+19</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">W+19</td>
<td style="text-align: center;">W+19</td>
<td style="text-align: center;">G+19</td>
</tr>
<tr>
<td style="text-align: center;">Contextualized</td>
<td style="text-align: center;">Decimal</td>
<td style="text-align: center;">Subword</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">W+19</td>
<td style="text-align: center;">L+20</td>
<td style="text-align: center;">G+20</td>
<td style="text-align: center;">W+19</td>
<td style="text-align: center;">W+19</td>
<td style="text-align: center;">Z+20</td>
</tr>
<tr>
<td style="text-align: center;">GenBERT</td>
<td style="text-align: center;">Decimal</td>
<td style="text-align: center;">Char</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">G+20</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">G+20</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">NumBERT</td>
<td style="text-align: center;">Scientific</td>
<td style="text-align: center;">Subword</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Z+20</td>
</tr>
<tr>
<td style="text-align: center;">DigitRNN/CNN</td>
<td style="text-align: center;">Decimal</td>
<td style="text-align: center;">Char</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">W+19</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">W+19</td>
<td style="text-align: center;">W+19</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DigitRNN-sci</td>
<td style="text-align: center;">Scientific</td>
<td style="text-align: center;">Char</td>
<td style="text-align: center;">RNN</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Exponent</td>
<td style="text-align: center;">Scientific</td>
<td style="text-align: center;">Word</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Real-Based</td>
<td style="text-align: center;">Scale</td>
<td style="text-align: center;">Direction</td>
<td style="text-align: center;">Binning</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DICE</td>
<td style="text-align: center;">Linear</td>
<td style="text-align: center;">Enc-only</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">S+20</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">S+20</td>
<td style="text-align: center;">S+20</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Value</td>
<td style="text-align: center;">Linear</td>
<td style="text-align: center;">Both</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">W+19</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">W+19</td>
<td style="text-align: center;">W+19</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Log Value</td>
<td style="text-align: center;">Log</td>
<td style="text-align: center;">Both</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">W+19</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">W+19</td>
<td style="text-align: center;">W+19</td>
<td style="text-align: center;">Z+20</td>
</tr>
<tr>
<td style="text-align: center;">MCC</td>
<td style="text-align: center;">Log</td>
<td style="text-align: center;">Dec-only</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Z+20</td>
</tr>
<tr>
<td style="text-align: center;">Log Laplace</td>
<td style="text-align: center;">Log</td>
<td style="text-align: center;">Dec-only</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Flow Laplace</td>
<td style="text-align: center;">Learn</td>
<td style="text-align: center;">Dec-only</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DExp</td>
<td style="text-align: center;">Log</td>
<td style="text-align: center;">Dec-only</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GMM</td>
<td style="text-align: center;">Linear</td>
<td style="text-align: center;">Dec-only</td>
<td style="text-align: center;">Both**</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GMM-proto</td>
<td style="text-align: center;">Linear</td>
<td style="text-align: center;">Enc-only*</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">J+20</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">J+20</td>
<td style="text-align: center;">J+20</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SOM-proto</td>
<td style="text-align: center;">Log</td>
<td style="text-align: center;">Enc-only*</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">J+20</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">J+20</td>
<td style="text-align: center;">J+20</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 2: An overview of numeracy in NLP: Each row is a method (\$3.2), arranged as per our taxonomy (\$3.1) split by string and real, further branching into three dimensions each. The last seven columns correspond to the seven subtasks of numeracy ( $\$ 2.2$ ), split by Exact and Approximate granularity ( $\$ 2.1$ ). The cells point to representative (not exhaustive) works that have experimented with a given method (row) on a given task (column). Notes: Prototype<em> is encoder-only but reuses embeddings for the decoder (Jiang et al., 2020). GMM</em>* has been discretized (Spithourakis and Riedel, 2018) as well as continuous valued (Spokoyny and Berg-Kirkpatrick, 2020).</p>
<h3>3.2.1 String-based methods</h3>
<p>Word Vectors \&amp; Contextualized Embeddings Word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), ELMo (Peters et al., 2018), and BERT (Devlin et al., 2019) have been probed as baselines against several contending methods.</p>
<p>GenBERT Geva et al. (2020) present GenBERT, a question answering model with pretrained BERT serving as both its encoder and decoder. GenBERT tokenizes numbers at the digit level, and is finetuned on auxiliary tasks of arithmetic word problems and simple arithmetic.</p>
<p>NumBERT Zhang et al. (2020) pretrain BERT from scratch over a modified dataset such that all numbers have been converted into scientific notation, i.e., 314.1 is expressed as 3141[EXP]2). NumBERT hence follows a scientific notation, subword tokenization, and no pooling. ${ }^{2}$</p>
<p>DigitRNN, DigitCNN Spithourakis and Riedel (2018) and Wallace et al. (2019) experimented with poolingof digit embeddings into a single embedding representing the full number. Both used RNNs as well as CNNs for pooling.</p>
<p>DigitRNN-sci \&amp; Exponent (Embedding) Spokoyny and Berg-Kirkpatrick (2020) use a scientific notation variant of DigitRNNs (which</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>we refer to as DigitRNN-sci in Table 2), as well as a simpler alternative: exponent embedding. The latter merely learns a lookup embedding for the exponent, completely ignoring the mantissa.</p>
<h3>3.2.2 Real-based methods</h3>
<p>DICE Determinisitic Independent-of-Corpus Embeddings (Sundararaman et al., 2020) is an attempt to handcraft number encoder ${ }^{3} f$ so as to preserve the relative magnitude between two numerals and their embeddings. Given two scalars $i$ and $j$, and their embeddings $f(i)$ and $f(j)$, the cosine distance between $f(i)$ and $f(j)$ is intended to monotonically increase/decrease with the Euclidean distance between $i$ and $j$. DICE is offered as not only a deterministic encoding but also as an auxiliary loss function for softly training number embeddings alongside, say, SQuAD (Rajpurkar et al., 2016)</p>
<p>Value Embedding The most intuitive parameterized encoder for real numbers is one that feeds the scalar magnitude of the number through a shallow neural network. The converse of value embedding is to learn a shallow neural network mapping $g: \mathbb{R}^{d} \rightarrow \mathbb{R}$. This decoder is simply the probe used for decoding/numeration task.</p>
<p>The idea of projecting number magnitudes into</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>an NLP model that otherwise inputs only lookup embeddings may appear flawed. But Vaswani et al. (2017) have (rather successfully) encoded positional information into transformers using both learned embeddings (similar to Value) and fixed ones (similar to DICE).</p>
<p>Log Value Wallace et al. (2019) also experiment with a log-scaled value encoder in addition to the one on a linear scale. Zhang et al. (2020) experiment with a log value decoder for measurement estimation, which they call the RGR (regress) method. Log scaling has a neuroscientific inspiration since observations of human (and animal) understanding of numbers is better modelled by a log-scale representation (Dehaene, 2011).</p>
<p>Log Laplace In contrast to the point estimate output of the RGR decoder, models can also be used to parameterize a distribution over numbers. Such a formulation is helpful when estimating approximate quantities. Vectors representing some context can be used to parameterize, say, the mean and variance of a Gaussian or Laplace distribution. Spokoyny and Berg-Kirkpatrick (2020) instead transform the space being modeled by parameterizing the location parameter of a Log-Laplace distribution $L(X, 1)$ where $X$ is the context representation of unmasked tokens, in a masked (numerical) language modelling setup. When inferring or decoding a number, they sample a point $z \sim L(X, 1)$ and exponentiate it, such that the output is $\exp (z)$.</p>
<p>Flow Laplace The expressivity of number decoders can be expanded or contracted by merely parameterizing a different distribution. Spokoyny and Berg-Kirkpatrick (2020) propose a more expressive decoder where instead of the log scale, the model learns its own density mapping. After sampling $z \sim L(X, 1)$, the output is transformed to $\frac{\exp \left(\frac{z-\alpha}{2}\right)}{\alpha}$, where $a, b$, and $c$, are also parameters emitted by the same model.</p>
<p>MCC or multi-class classification is another number decoder which outputs a distribution, but a discrete one: over log-scaled bins of numbers, e.g., 1-10, 10-100, and so on (Zhang et al., 2020). Previously described decoders either output a point estimate or a unimodal distribution, thus failing to hedge its predictions for a multimodal ground truth. Given a masked number prediction problem We went to the restaurant at [MASK] p.m., MCC is better equipped to estimate two peaks: one around lunch time (say, 1-2 p.m.) and another around dinner (say, 7-9 p.m.).</p>
<p>Discrete Latent Exponent (DExp) is another potentially multimodal distribution (Spokoyny and Berg-Kirkpatrick, 2020) where the model parameterizes a multinomial distribution for the exponent (similar to MCC) and uses it to sample an exponent $e$, which then acts as a latent variable for emitting the mean $\mu$ of a Gaussian (standard deviation fixed at 0.05 ). This Gaussian is finally used to sample the output number $z \sim N(\mu, 0.05)$.</p>
<p>GMM Another attempt to circumvent the unimodal Gaussians or point estimates is to learn a Gaussian mixture model. Spithourakis and Riedel (2018) learn a mixture of $K$ Gaussians by pretraining their means $\left(\mu_{i}\right)$ and variances $\left(\sigma_{i}{ }^{2}\right)$ over the training corpus with Expectation Maximization algorithms, while the mixing weights $\pi_{i}$ are derived from the model. Next, to sample a single number from the GMM probability mass function $q(u)=\sum_{i=1}^{K} \pi_{i} N\left(u ; \mu_{i} ; \sigma_{i}\right)$, the authors first sample the precision (number of decimal places) from yet another Gaussian and use that to discretize the probability mass function into equal sized bins, over which the probabilities are summed. If the sampled precision is, say 2 , then the probability of emitting a number 3.14 is given by $\int_{3.135}^{3.145} q(u) d u$. This likelihood estimate is used to train a causal language model.</p>
<p>Spokoyny and Berg-Kirkpatrick (2020)'s GMM implementation is slightly different: it alters the last inference step by sampling directly from the mixture of Gaussians, as they did with Log Laplace, Flow Laplace, and DExp.</p>
<p>GMM-prototype by Jiang et al. (2020) similarly pretrains (with EM/hard-EM) the mean, the variances, but also the mixture weights $\pi_{i}$ s of a GMM over the training corpus. They then learn $K$ prototype embeddings $e_{i}$ s corresponding to the $K$ Gaussians. When encoding a new numeral $n$, its (input) embedding is calculated as: $E(n)=$ $\sum_{i=1}^{K} w_{i} \cdot e_{i}$, where the weights are induced from the GMM:</p>
<p>$$
w_{i}=P(Z=i \mid U=n)=\frac{\pi_{i} N\left(n ; \mu_{i} ; \sigma_{i}\right)}{\sum_{j=1}^{K} \pi_{j} N\left(n ; \mu_{j} ; \sigma_{j}\right)}
$$</p>
<p>Thus the difference between GMM and GMMprototypes is that after fixing mean and standard deviations of the Gaussian mixtures, in GMM the model learns to predict the mixture weights $\pi_{i}$ for each individual number prediction, whereas in GMM-prototype, $\pi_{i}$ 's are frozen and the model learns prototype embeddings $e_{i}$ 's. Note that proto-</p>
<p>type embeddings are encoder-only.To decode numbers, the authors implement weight-sharing across input and output embeddings, similar to how word vectors are trained Mikolov et al., 2013), i.e., finding out which of the numerals in the corpus has the closest embedding.</p>
<p>SOM-prototype GMM-prototype, in effect, merely use the mixture of Gaussians to infer prototypes and to get the weights $w_{i}$ 's. Jiang et al. (2020) tried another variant by identifying prototype numerals with Self Organizing Maps (Kohonen, 1990) and by defining the weights as: $w_{i}=$ $\left|g\left(x_{i}\right)-g(n)\right|^{-1}$ where $x_{i}$ is the $i$ th prototype, $n$ is the number to be encoded, and $g$ is a log-based squashing function.</p>
<h2>4 Results</h2>
<p>Having organized the landscape of numeracy tasks and methods, we now present come key results for each numeracy task in NLP from previously published experiments over a subset of the described number representations:</p>
<p>Abstract Probes Word Embeddings vastly outperform random embedding baselines on abstract probes such as numeration, magnitude comparison, and sorting Wallace et al., 2019; Naik et al., 2019). DICE, Value and Log Value embeddings excel at these probes, which makes intuitive sense given that they explicitly encode the numbers' magnitude - although Value embeddings do not easily extrapolate to larger numbers, possibly due to instability in training. The best number encoders with respect to these probes were found to be DigitCNNs, and character-tokenized models, e.g., ELMo, in general outperform subword ones, e.g., BERT Wallace et al., 2019).</p>
<p>Arithmetic GPT-3 Brown et al., 2020) performs extremely well at zero shot simple arithmetic, as long as the number of digits in the operands are low. The tokenization scheme could be the cause for limited extrapolation, since language models get better at arithmetic when numbers are tokenized at the digit/character level (Nogueira et al., 2021; Wallace et al., 2019). For arithmetic word problems, state of the art solvers rely on predicting an equation, which is then filled in with specific numeric values from the question Patel et al., 2021), altogether bypassing the need for encoding numbers into embeddings.</p>
<p>Masked Language Modelling Zhang et al. (2020) show that BERT pretrained over datasets
where numbers are in scientific notation (NumBERT) converges to the same loss as BERT on masked language modelling objective, and scores nearly the same on GLUE language understanding benchmarks. For (causal) numeric language modelling, Spithourakis and Riedel (2018) show that Gaussian Mixture Models are the best decoders. For (masked) numeric language modelling, Spokoyny and Berg-Kirkpatrick (2020) show that modelling the mantissa in scientific notation may be an overkill, since exponent embeddings alone outperform DigitRNN-sci over financial news and scientific articles.</p>
<p>Measurement Estimation Zhang et al. (2020) train a regression probe to predict measurements of objects over the CLS embeddings of BERT/NumBERT. Given a template-lexicalized sentence such as "the dog is heavy," the model must predict the weight of a typical dog, against ground truth from the Distribution over Quantities dataset (Elazar et al., 2019). They find that NumBERT is a better text encoder than BERT for measurement estimation, the only difference between them being the notation used by the respective pretraining corpora. They also experiment with two number decoders: MCC (multi-class classification) and RGR (regression / Log Value embedding). MCC performs better when trying to predict Distributions over Quantities - perhaps due to the ground truth resembling the predicted gaussians - but not on VerbPhysics - where the ground truth is less noisy. Lastly, even static word embeddings like GloVe have been shown to contain enough knowledge of measurement estimates to contrast two objects, e.g., classifying whether a car is bigger/heavier/fasster than a ball (Goel et al., 2019).</p>
<p>Exact Facts BERT and RoBERTa capture limited numerical commonsense, evident over NumerSense (Lin et al., 2020) sentences such as a tricycle has [MASK] wheels, with the answer choices limited to the integers 0-10. Results can be further improved by finetuning over a Wikipediaextracted dataset of numeric information. Mishra et al. (2020) find commonsense question answering to be one of the hardest among their Numbergame challenge, using the NumNetv2 model (Ran et al., 2019) which is commonly used for DROP question answering. Both of these experiments evaluate on exact match metrics, hence it remains to be seen if representing approximate magnitudes yields benefit in modelling numeric facts.</p>
<h2>5 Recommendations</h2>
<p>Based on the above results, we now synthesize key insights into a set of directed takeaways to guide practitioners' design of number representations:</p>
<p>Rule of thumb for string-based methods? Scientific notation is superior to decimal notation (Zhang et al., 2020) since models can learn to attend mostly to the exponent embedding rather than the mantissa (Spokoyny and Berg-Kirkpatrick, 2020). Character level tokenization outperforms subword level (Nogueira et al., 2021; Wallace et al., 2019; Geva et al., 2020). Pooled representations (DigitRNN, DigitCNN) lack a controlled study with unpooled ones (NumBERT, GenBERT) which makes it hard to proclaim a winner among the two.</p>
<p>Rule of thumb for real-based methods? Log scale is preferred over linear scale (Zhang et al., 2020; Jiang et al., 2020; Wallace et al., 2019; Spokoyny and Berg-Kirkpatrick, 2020), which makes intuitive sense but lacks as rigorous a study as has been undertaken in the cognitive science community (Feigenson et al., 2004). Regarding discretization, Zhang et al. (2020) show that binning (dense cross entropy loss) works better than continuous value prediction (MAE loss) on datasets where ground truth distributions are available. Lastly, modeling continuous predictions is notoriously hard for large ranges (Wallace et al., 2019) but Spithourakis and Riedel (2018) offer a way of binning such distributions by picking a precision level.</p>
<p>Encoding vs Decoding numbers? In our simplified discussions above, we avoid differentiating between methods for encoding and decoding numbers. Value Embedding, for instance, can be used to encode numbers (projecting scalars onto vector space) as well as to decode numbers (collapsing a vector into a scalar). On the other hand, manually-designed encoders like DICE are not easily reversible into decoding methods. Even with reversible methods, the encoders and decoders must usually be independently parameterized, unlike the input and output word embeddings which often share weights (Press and Wolf, 2016). Prototype embeddings by Jiang et al. (2020) are an exception, which share input/output embeddings for a fixed vocabulary of numbers.</p>
<p>Can we mix-and-match multiple methods? Given the wide range of number representations, an obvious next step is to try an ensemble of embeddings. Spokoyny and Berg-Kirkpatrick (2020)
show that for encoding numbers, exponent embeddings added to DigitRNN (scientific notation) embeddings barely outperforms the exponent embeddings alone. Similar experiments with a mix of real and string methods are yet to be seen.</p>
<p>Which methods for which tasks? Based on our taxonomy of tasks in Table 1, abstract tasks are good early probes for the grounded ones, e.g., finetuning GenBERT (Geva et al., 2020) on simple arithmetic helps it do well on downstream question answering, and the high scores of DICE (Sundararaman et al., 2020) on numeration and magnitude comparison are an indicator of similar boosts on (numeric) language modelling. With respect to granularity, real-based methods work well for approximate tasks such as measurement estimation and language modeling (Zhang et al., 2020; Spokoyny and Berg-Kirkpatrick, 2020) but not for exact tasks like arithmetic word problems or commonsense. DigitRNNs are broad-purpose number encoders, whereas distribution modeling methods like DExp are effective at decoding numbers.</p>
<h2>6 Vision for Unified Numeracy in NLP</h2>
<p>Numeracy is a core system of human intelligence (Kinzler and Spelke, 2007). Teaching numeracy to students works best when taught holistically, while less effective teachers deal with areas of mathematics discretely (Askew and Askew, 1997). While the NLP community genuinely strives to improve language models' numeric skills, not all aspects of numeracy have been sufficiently targeted. It is evident from the sparsity in Table 2 that the community is far from achieving, a holistic solution to numeracy. In this section, we outline our vision for such a unified solution, in the form of three prerequisites to consider for numerical NLU:</p>
<p>Evaluation. The first step towards a holistic solution to numeracy requires a benchmark covering its different subtasks. Aggregated leaderboards in NLP like GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) have incentivized research on natural language understanding, with scores categorized into semantic, syntactic, logical, and background knowledge.</p>
<p>An analogous leaderboard could be constructed to evaluate models on numeric reasoning tasks, categorized according to the skills evaluated, e.g., exact vs approximate granularity, or abstract vs grounded numeracy. Numbergame (Mishra et al., 2020) is one such aggregation focusing on exact</p>
<p>numeracy benchmarks, as evaluated by F1 and exact match scores in a reading comprehension setup. Both Numbergame and our own list of tasks (Section 2.2) are preliminary attempts at teasing apart the different aspects of numeracy. We encourage researchers to extend and refine such taxonomies.</p>
<p>A suite of numeracy tasks, matched with evaluations of their respective numerical skills, can enable testing model generalization from one skill to another. Some progress has already been made in this transfer learning setup, e.g., GenBERT (Geva et al., 2020), finetuned on a synthetic dataset of arithmetic problems, is found to score higher on DROP QA. Similarly, DICE (Sundararaman et al., 2020), optimized for numeration, improves score on Numeracy600K order-of-magnitude prediction task. Going forward, we need several such studies, ideally for each pair of tasks to see whether some numeracy skills help models generalize to others.</p>
<p>Design Principles. Number representations vary based on design trade-offs between inductive biases and data-driven variance. The default BERT setup, with subword tokenization and lookup embeddings, occupies the variance end of the spectrum, allowing freedom in representing numbers. Value embeddings and DICE encodings, on the other hand, are closer to the bias end of the spectrum, since the inductive bias of continuity on the number line constrains the learning space. It is important to identify where on the bias-variance scale any representation stands, for a fair comparison.</p>
<p>Following parallel work in cognitive science, the community could explore whether exact and approximate numeracy require two specialized modules (Feigenson et al., 2004) or could be handled with a single representation (Cordes et al., 2001).</p>
<p>Model designers must also make a choice on coverage: whether to target a broad or a narrow range of numbers to be represented. Multi-class classification (Zhang et al., 2020) over a fixed number of bins, restricts the range of numbers expressed, as do DICE embeddings (Sundararaman et al., 2020). Value embeddings are continuous and theoretically unrestricted, but must practically be capped for bugfree training. On the other hand, string-based representations could always fall back to subword/charlevel token embeddings to represent not only floats but also irrational $(\sqrt{2})$ and complex $(1+2 i)$ numbers. Roy et al. (2015) introduced the QuantityValue Representation format to allow closed and open ranges alongside scalar point numbers.</p>
<p>Broader Impact. Numbers are ubiquitous in natural language and are easily identified, at least in numeral forms. But they are by no means the only class of ordered concepts required for natural language understanding. Successful number representations can inspire work on incorporating more continuous domains into natural language processing systems. For instance, gradable adjectives like good, great, amazing, etc. are arguably on some cardinal scale, which can be mapped using value embeddings or Gaussian mixture models (Sharp et al., 2018; de Marneffe et al., 2010). Days of the week (Mon-Sun) and months of an year (Jan-Dec) form periodic patterns which can be modeled with sinusoidal functions (Martinez et al., 2020).</p>
<p>Lastly, numeracy is essential for natural language understanding. Consider the sentence: "Programmers earn \$200,000 versus \$100,000 for researchers." An intelligent agent with numeracy skills would identify that $\$ 100 \mathrm{k}$ is half of $\$ 200 \mathrm{k}$, that $\$ 100 \mathrm{k}$ possibly denotes annual salary, and infer that higher salaries lead to higher standards of living. In short, it was able to learn something about the two concepts programmers and researchers, by crossing the continuous semantic space of numbers! The agent could now make use of this knowledge in a number-free situation, e.g., the mask in "He could not afford a car for several years after earning a CS degree because she took a job as a [MASK]" might better be filled with the word researcher, than with programmer. A key goal of imparting numeracy to NLP models is to help them understand more about the world, using numbers.</p>
<h2>7 Conclusion</h2>
<p>This paper summarizes and contextualizes recent work on numeracy in NLP. We propose the first taxonomy of tasks and methods concerning textcentric numeric reasoning. We highlight key takeaways from the several experiments in literature, along with caveats and scope for confirming some of the observed trends. We present a case for lack of a holistic solution to numeracy in NLP, and put forward a set of aspects to consider when working towards one. We draw the following two major conclusions from our study: (1) the default subword segmentation with lookup embeddings used to represent words is clearly suboptimal for numbers (2) there are several unanswered research questions on the level of specificity, coverage, and inductive bias needed to holistically solve numeracy.</p>
<h2>8 Acknowledgements</h2>
<p>This work was funded by the Defense Advanced Research Projects Agency with award N660011924033. We would like to thank the countless suggestions we accumulated during preliminary presentations at MLSS 2020, WeCNLP 2020, and GSS 2020, as well as over email correspondences with Biplav Srivastava, Antoine Bosselut, and Harsh Agarwal. We would like to thank the anonymous NAACL 2021 reviewers (particularly #3) for pointing out blind spots in our submission, which we have tried our best to rectify.</p>
<h2>Ethical Considerations</h2>
<p>This work revolves around the Hindu-Arabic Numeral system and English number words, which are not the only number systems still in use today. We encourage follow-up work to take these systems into consideration, on the lines of Johnson et al. (2020) and Nefedov (2020).</p>
<h2>References</h2>
<p>Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2357-2367, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Daniel Andor, Luheng He, Kenton Lee, and Emily Pitler. 2019. Giving BERT a calculator: Finding operations and arguments with reading comprehension. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 59475952, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Mike Askew and Mike Askew. 1997. Effective teachers of numeracy. King's College London London.</p>
<p>Satwik Bhattamishra, Arkil Patel, and Navin Goyal. 2020. On the computational power of transformers and its implications in sequence modeling. In Proceedings of the 24th Conference on Computational Natural Language Learning, pages 455-475, Online. Association for Computational Linguistics.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,</p>
<p>Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.</p>
<p>Sarah E Bullard, Deborah Fein, Mary Kay Gleeson, Nita Tischer, Robert L Mapou, and Edith Kaplan. 2004. The biber cognitive estimation test. Archives of clinical neuropsychology, 19(6):835-846.</p>
<p>Kushal Chawla, Gale Lucas, Jonathan Gratch, and Jonathan May. 2020. Bert in negotiations: Early prediction of buyer-seller negotiation outcomes. arXiv preprint arXiv:2004.02363.</p>
<p>Chung-Chi Chen, Hen-Hsen Huang, and Hsin-Hsi Chen. 2020. Numclaim: Investor's fine-grained claim detection. In Proceedings of the 29th ACM International Conference on Information and Knowledge Management, CIKM '20, page 1973-1976, New York, NY, USA. Association for Computing Machinery.</p>
<p>Chung-Chi Chen, Hen-Hsen Huang, Yow-Ting Shiue, and Hsin-Hsi Chen. 2018. Numeral understanding in financial tweets for fine-grained crowd-based forecasting. In 2018 IEEE/WIC/ACM International Conference on Web Intelligence (WI), pages 136-143. IEEE.</p>
<p>Chung-Chi Chen, Hen-Hsen Huang, Hiroya Takamura, and Hsin-Hsi Chen. 2019. Numeracy-600K: Learning numeracy for detecting exaggerated information in market comments. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6307-6313, Florence, Italy. Association for Computational Linguistics.</p>
<p>Peter Clark, Oren Etzioni, Daniel Khashabi, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, Niket Tandon, et al. 2019. From'f'to'a'on the ny regents science exams: An overview of the aristo project. arXiv preprint arXiv:1909.01958.</p>
<p>Sara Cordes, Rochel Gelman, Charles R Gallistel, and John Whalen. 2001. Variability signatures distinguish verbal from nonverbal counting for both large and small numbers. Psychonomic bulletin \&amp; review, 8(4):698-707.</p>
<p>Marie-Catherine de Marneffe, Christopher D. Manning, and Christopher Potts. 2010. "was it good? it was provocative." learning the meaning of scalar adjectives. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 167-176, Uppsala, Sweden. Association for Computational Linguistics.</p>
<p>Stanislas Dehaene. 2011. The number sense: How the mind creates mathematics. OUP USA.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2368-2378, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Abhijeet Dubey, Lakshya Kumar, Arpan Somani, Aditya Joshi, and Pushpak Bhattacharyya. 2019. "when numbers matter?". Detecting sarcasm in numerical portions of text. In Proceedings of the Tenth Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 72-80, Minneapolis, USA. Association for Computational Linguistics.</p>
<p>Yanai Elazar and Yoav Goldberg. 2019. Where's my head? Definition, data set, and models for numeric fused-head identification and resolution. Transactions of the Association for Computational Linguistics, 7:519-535.</p>
<p>Yanai Elazar, Abhijit Mahabal, Deepak Ramachandran, Tania Bedrax-Weiss, and Dan Roth. 2019. How large are lions? inducing distributions over quantitative attributes. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3973-3983, Florence, Italy. Association for Computational Linguistics.</p>
<p>Lisa Feigenson, Stanislas Dehaene, and Elizabeth Spelke. 2004. Core systems of number. Trends in cognitive sciences, 8(7):307-314.</p>
<p>Maxwell Forbes and Yejin Choi. 2017. Verb physics: Relative physical knowledge of actions and objects. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 266-276, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Mor Geva, Ankit Gupta, and Jonathan Berant. 2020. Injecting numerical reasoning skills into language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 946-958, Online. Association for Computational Linguistics.</p>
<p>Pranav Goel, Shi Feng, and Jordan Boyd-Graber. 2019. How pre-trained word representations capture commonsense physical comparisons. In Proceedings
of the First Workshop on Commonsense Inference in Natural Language Processing, pages 130-135, Hong Kong, China. Association for Computational Linguistics.</p>
<p>David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2003. English gigaword. Linguistic Data Consortium, Philadelphia, 4(1):34.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874.</p>
<p>Chengyue Jiang, Zhonglin Nian, Kaihao Guo, Shanbo Chu, Yinggong Zhao, Libin Shen, and Kewei Tu. 2020. Learning numeral embedding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2586-2599, Online. Association for Computational Linguistics.</p>
<p>Devin Johnson, Denise Mak, Andrew Barker, and Lexi Loessberg-Zahl. 2020. Probing for multilingual numerical understanding in transformer-based language models. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 184-192, Online. Association for Computational Linguistics.</p>
<p>Katherine D Kinzler and Elizabeth S Spelke. 2007. Core systems in human cognition. Progress in brain research, 164:257-264.</p>
<p>Teuvo Kohonen. 1990. The self-organizing map. Proceedings of the IEEE, 78(9):1464-1480.</p>
<p>Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xiang Ren. 2020. Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-Trained Language Models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 68626868, Online. Association for Computational Linguistics.</p>
<p>Richard Diehl Martinez, Scott Novotney, Ivan Bulyko, Ariya Rastrow, and Andreas Stolcke. 2020. Contextual datetime language model adaptation for speech recognition. West Coast NLP Summit.</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</p>
<p>George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):3941.</p>
<p>Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, and Chitta Baral. 2020. Towards question format independent numerical reasoning: A set of prerequisite tasks.</p>
<p>Aakanksha Naik, Abhilasha Ravichander, Carolyn Rose, and Eduard Hovy. 2019. Exploring numeracy in word embeddings. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3374-3380, Florence, Italy. Association for Computational Linguistics.</p>
<p>Mikhail Nefedov. 2020. Dataset for evaluation of mathematical reasoning abilities in russian. In Conference on Artificial Intelligence and Natural Language, pages 135-144. Springer.</p>
<p>Rodrigo Nogueira, Zhiying Jiang, and Jimmy Li. 2021. Investigating the limitations of the transformers with simple arithmetic tasks. arXiv preprint arXiv:2102.13019.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are nlp models really able to solve simple math word problems?</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532-1543, Doha, Qatar. Association for Computational Linguistics.</p>
<p>Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227-2237, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Marten Postma, Filip Ilievski, and Piek Vossen. 2018. SemEval-2018 task 5: Counting events and participants in the long tail. In Proceedings of The 12th International Workshop on Semantic Evaluation, pages 70-80, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Ofir Press and Lior Wolf. 2016. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.</p>
<p>Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. 2019. NumNet: Machine reading comprehension with numerical reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2474-2484, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Abhilasha Ravichander, Aakanksha Naik, Carolyn Rose, and Eduard Hovy. 2019. EQUATE: A benchmark evaluation framework for quantitative reasoning in natural language inference. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 349-361, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1743-1752, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Subhro Roy, Tim Vieira, and Dan Roth. 2015. Reasoning about quantities in natural language. Transactions of the Association for Computational Linguistics, 3:1-13.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. Analysing mathematical reasoning abilities of neural models. In International Conference on Learning Representations.</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17151725, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Rebecca Sharp, Mithun Paul, Ajay Nagesh, Dane Bell, and Mihai Surdeanu. 2018. Grounding gradable adjectives through crowdsourcing. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).</p>
<p>Georgios Spithourakis and Sebastian Riedel. 2018. Numeracy for language models: Evaluating and improving their ability to predict numbers. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2104-2115, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Daniel Spokoyny and Taylor Berg-Kirkpatrick. 2020. An empirical investigation of contextualized number prediction. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4754-4764, Online. Association for Computational Linguistics.</p>
<p>Dhanasekar Sundararaman, Shijing Si, Vivek Subramanian, Guoyin Wang, Devamanyu Hazarika, and Lawrence Carin. 2020. Methods for numeracypreserving word embeddings. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4742-4753, Online. Association for Computational Linguistics.</p>
<p>Mirac Suzgun, Yonatan Belinkov, Stuart Shieber, and Sebastian Gehrmann. 2019. LSTM networks can perform dynamic counting. In Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges, pages 44-54, Florence. Association for Computational Linguistics.</p>
<p>Oyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau Yih, and Ashish Sabharwal. 2019. Quarel: A dataset and models for answering questions about qualitative relationships. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7063-7071.</p>
<p>Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, and Jonathan Berant. 2020. Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge. Advances in Neural Information Processing Systems, 33.</p>
<p>Alberto Testolin, Serena Dolfi, Mathijs Rochus, and Marco Zorzi. 2020. Visual sense of number vs. sense of magnitude in humans and machines. Scientific reports, 10(1):1-13.</p>
<p>Andrew Trask, Felix Hill, Scott E Reed, Jack Rae, Chris Dyer, and Phil Blunsom. 2018. Neural arithmetic logic units. In Advances in Neural Information Processing Systems, pages 8035-8044.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008.</p>
<p>Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019. Do NLP models know numbers? probing numeracy in embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 53075315, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, pages 3266-3280.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353-355, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Sandra Williams and Richard Power. 2010. A factaligned corpus of numerical expressions. In Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC'10), Valletta, Malta. European Language Resources Association (ELRA).</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation.</p>
<p>Karen Wynn. 1990. Children's understanding of counting. Cognition, 36(2):155-193.</p>
<p>Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, and Dan Roth. 2020. Do language embeddings capture scales? In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4889-4896, Online. Association for Computational Linguistics.</p>
<p>Ben Zhou, Qiang Ning, Daniel Khashabi, and Dan Roth. 2020. Temporal common sense acquisition with minimal supervision. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7579-7589, Online. Association for Computational Linguistics.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Pooling as described in $\S 3.1 .1$.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ Number encoder-decoder as defined in $\S 3.1 .2$.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
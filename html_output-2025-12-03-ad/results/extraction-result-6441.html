<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6441 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6441</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6441</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-273374998</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.12409v1.pdf" target="_blank">Revealing the Barriers of Language Agents in Planning</a></p>
                <p><strong>Paper Abstract:</strong> Autonomous planning has been an ongoing pursuit since the inception of artificial intelligence. Based on curated problem solvers, early planning agents could deliver precise solutions for specific tasks but lacked generalization. The emergence of large language models (LLMs) and their powerful reasoning capabilities has reignited interest in autonomous planning by automatically generating reasonable solutions for given tasks. However, prior research and our experiments show that current language agents still lack human-level planning abilities. Even the state-of-the-art reasoning model, OpenAI o1, achieves only 15.6% on one of the complex real-world planning benchmarks. This highlights a critical question: What hinders language agents from achieving human-level planning? Although existing studies have highlighted weak performance in agent planning, the deeper underlying issues and the mechanisms and limitations of the strategies proposed to address them remain insufficiently understood. In this work, we apply the feature attribution study and identify two key factors that hinder agent planning: the limited role of constraints and the diminishing influence of questions. We also find that although current strategies help mitigate these challenges, they do not fully resolve them, indicating that agents still have a long way to go before reaching human-level intelligence.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6441.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6441.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Episodic memory updating</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Episodic (working) memory updating via prompt-level insights</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-based memory strategy where agents are given summarized insights (agent-generated or human-written) from previous attempts (Behavioral Cloning, Oracle Feedback, or Reference) and must select/apply those insights during inference; used to reiterate/refine constraint information to guide planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Episodic-memory-guided agent (insight-prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>The agent receives a small episodic memory as textual insights appended in the prompt; during inference it must select relevant insights and plan accordingly. Updates are produced by summarizing past failed attempts (Behavioral Cloning / Oracle Feedback) or by providing human-written Reference insights. A vote/filtering mechanism is used during training to curate which insights are shown at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (evaluated across closed- and open-source models: GPT‑4o, GPT‑4o‑Mini, Llama3.1-8B, Llama3.1-70B, Qwen2-7B, Qwen2-72B, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic prompt-level memory (short-term / working memory stored as text in the prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw textual insights (human-written or agent-generated summaries of past failures and corrective feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>explicitly prepended/appended prompt text; agent selects which insights to use during inference (selection is part of the prompt task); insights are filtered by vote counts (only high-vote insights shown); no external retrieval index — access is via normal prompt conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BlocksWorld; TravelPlanner</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>planning (classical and real-world travel planning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Modest, consistent improvements over direct prompting across models and benchmarks (paper reports small absolute gains; e.g., average metric deltas on TravelPlanner labeled in the paper in the low single digits, roughly +1.7 to +4.4 points across reported metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Direct prompting baseline (no episodic memory). Performance is lower than with episodic memory; exact baseline metrics vary by model and benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Pass rate / micro & macro metrics reported for benchmarks (paper reports 'Final Pass Rate' and other micro/macro metrics for TravelPlanner and success/completion metrics for BlocksWorld).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Episodic memory is simple to apply (prompt-level) but provides only modest gains; when combined with parametric (fine-tuned) agents where constraints are already embedded, episodic memory can be redundant or even harmful (degrades performance). Episodic memory relies on global reiteration rather than fine-grained referencing, and can increase prompt length/latency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Agents using episodic memory tend to use insights at a coarse/global level and struggle to apply individual insights in a fine-grained fashion; episodic updates struggle with dynamic or context-specific constraints (e.g., TravelPlanner dynamic constraints) and provide only partial mitigation of the diminishing influence of the question across long horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Jian Xie, Kexun Zhang, Jiangjie Chen, Siyu Yuan, Kai Zhang, Yikai Zhang, Lei Li, Yanghua. 2024. Revealing the Barriers of Language Agents in Planning. arXiv:2410.12409</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Revealing the Barriers of Language Agents in Planning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6441.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6441.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Parametric memory updating</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parametric memory updating via supervised fine-tuning / LoRA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A strategy that updates model parameters (supervised fine-tuning or LoRA) on training examples so that knowledge (constraints, commonsense, frequently needed reasoning behavior) becomes embedded in the model weights, improving planning behavior at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Parametrically-updated agent (SFT / LoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Memory is encoded into model parameters through supervised fine-tuning (SFT) or LoRA on the training set (ground-truth plans/insights). Access to this memory is implicit (via model activations) rather than through an explicit retrieval interface. Training was done with SFT for smaller contexts and LoRA for long-context TravelPlanner due to cost.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (OpenAI models via their official training scripts; Llama3.1-8B and Qwen2-7B fine-tuned locally; Llama3.1-405B used for inference in some experiments), typically 7B–70B and larger.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>parametric memory (knowledge encoded in model weights via fine-tuning / LoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>parameters / weight updates that encode commonsense rules, constraints and planning heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>implicit access via the model's forward pass; no explicit retrieval — learned behavior is elicited by prompts due to changed parameters</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BlocksWorld; TravelPlanner</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>planning (classical and real-world travel planning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Substantial improvements compared to direct prompting for many models and metrics; the paper reports larger deltas than episodic updates (examples in paper show parametric deltas up to double-digit improvements on some metrics, e.g., reported ∆ values up to +23.7 on some measures).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Direct prompting baseline (no fine-tuning). Parametric updating shows clear gains over this baseline in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Pass rate / micro & macro metrics (Final Pass Rate and other micro/macro breakdowns for TravelPlanner and BlocksWorld success metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Fine-tuning yields larger gains but can lead to models that no longer explicitly reference constraints (making appended episodic memory redundant); combining parametric and episodic updates harmed performance in experiments. Parametric updates require compute for fine-tuning and may overfit to static patterns (shortcut learning).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Although parametric updates increase the question's attribution and improve short-horizon planning, they still suffer from a diminishing influence of the question as planning horizon increases and from shortcut learning (favoring static/common-sense rules over dynamic multi-constraint integration). Combining with episodic memory may degrade performance if constraints have become parameterized.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Jian Xie, Kexun Zhang, Jiangjie Chen, Siyu Yuan, Kai Zhang, Yikai Zhang, Lei Li, Yanghua. 2024. Revealing the Barriers of Language Agents in Planning. arXiv:2410.12409</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Revealing the Barriers of Language Agents in Planning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6441.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6441.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3.1-8B (SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama3.1-8B fine-tuned (supervised fine-tuning) agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source Llama3.1-8B model that was supervised fine-tuned on the training splits of BlocksWorld and TravelPlanner (SFT / LoRA as appropriate) to embed planning knowledge parametrically.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Llama3.1-8B (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Llama3.1-8B was fine-tuned (SFT for BlocksWorld; LoRA for TravelPlanner when needed) on ground-truth plans; this parametric update encodes constraints and planning heuristics into model weights, altering how the model attends to questions and constraints at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>parametric memory (fine-tuned model weights)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>updated network parameters encoding common constraints/heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>implicit via forward pass of fine-tuned model; no external retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BlocksWorld; TravelPlanner</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>planning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported (Table 1 / Table 3): fine-tuned Llama3.1-8B shows substantially higher scores than vanilla in many reported metrics; example from Table 1: Llama3.1-8B_sft scored 48.4 (with constraints shown) vs 45.8 (without constraints) on BlocksWorld (numbers correspond to the table entries reported in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Vanilla direct prompting Llama3.1-8B scored much lower in other settings (e.g., in some direct-prompting BlocksWorld runs the model had near-zero scores), and gains from fine-tuning were substantial for this model on planning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Final pass rate / micro & macro metrics (as reported in paper tables)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Fine-tuning required compute (8× A100 GPUs for local fine-tuning); once constraints were parameterized, providing episodic memory gave little to no additional benefit and could slightly harm performance due to redundancy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Even after fine-tuning, the model's ability to maintain question influence over long planning horizons still declines; fine-tuned model may rely on learned shortcuts and fail on dynamic constraint integration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Jian Xie, Kexun Zhang, Jiangjie Chen, Siyu Yuan, Kai Zhang, Yikai Zhang, Lei Li, Yanghua. 2024. Revealing the Barriers of Language Agents in Planning. arXiv:2410.12409</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Revealing the Barriers of Language Agents in Planning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6441.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6441.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2-7B (SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2-7B fine-tuned agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source Qwen2-7B model evaluated in both vanilla and fine-tuned (SFT) forms; fine-tuning parametrically embedded constraints/heuristics leading to large performance changes in planning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Qwen2-7B (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Qwen2-7B was fine-tuned on training examples (SFT) to encode planning constraints and strategies into model parameters; after fine-tuning the agent relied less on explicit prompt constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>parametric memory (SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>model weight updates encoding planning constraints and heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>implicit via model activations after fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BlocksWorld; TravelPlanner</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>planning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported (Table 1): Qwen2-7B_sft achieved 45.4 (with constraints) and 45.4 (without constraints) on BlocksWorld in the paper's table, indicating that fine-tuning parameterized the constraints (removing prompt constraint descriptions did not reduce performance after SFT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Vanilla Qwen2-7B (direct prompting) scored 2.4 (with constraints) and 3.6 (without constraints) on BlocksWorld (Table 1), indicating very low baseline performance before fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Pass rate / benchmark-specific success metrics (as in paper tables)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Fine-tuning moved constraint knowledge into model parameters so that prompt-level constraints became redundant; however, this can cause episodic memory to be ineffective or harmful when combined, and promotes shortcut learning on static patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>After fine-tuning, the model still suffers from diminishing attention to the question over long planning horizons and struggles with dynamic constraints that require fine-grained, context-specific referencing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Jian Xie, Kexun Zhang, Jiangjie Chen, Siyu Yuan, Kai Zhang, Yikai Zhang, Lei Li, Yanghua. 2024. Revealing the Barriers of Language Agents in Planning. arXiv:2410.12409</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Revealing the Barriers of Language Agents in Planning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Expel: Llm agents are experiential learners <em>(Rating: 2)</em></li>
                <li>Agenttuning: Enabling generalized agent abilities for llms <em>(Rating: 2)</em></li>
                <li>Autoguide: Automated generation and selection of state-aware guidelines for large language model agents <em>(Rating: 2)</em></li>
                <li>Clin: A continually learning language agent for rapid task adaptation and generalization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6441",
    "paper_id": "paper-273374998",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "Episodic memory updating",
            "name_full": "Episodic (working) memory updating via prompt-level insights",
            "brief_description": "A prompt-based memory strategy where agents are given summarized insights (agent-generated or human-written) from previous attempts (Behavioral Cloning, Oracle Feedback, or Reference) and must select/apply those insights during inference; used to reiterate/refine constraint information to guide planning.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Episodic-memory-guided agent (insight-prompting)",
            "agent_description": "The agent receives a small episodic memory as textual insights appended in the prompt; during inference it must select relevant insights and plan accordingly. Updates are produced by summarizing past failed attempts (Behavioral Cloning / Oracle Feedback) or by providing human-written Reference insights. A vote/filtering mechanism is used during training to curate which insights are shown at inference.",
            "model_size": "various (evaluated across closed- and open-source models: GPT‑4o, GPT‑4o‑Mini, Llama3.1-8B, Llama3.1-70B, Qwen2-7B, Qwen2-72B, etc.)",
            "memory_used": true,
            "memory_type": "episodic prompt-level memory (short-term / working memory stored as text in the prompt)",
            "memory_representation": "raw textual insights (human-written or agent-generated summaries of past failures and corrective feedback)",
            "memory_access_mechanism": "explicitly prepended/appended prompt text; agent selects which insights to use during inference (selection is part of the prompt task); insights are filtered by vote counts (only high-vote insights shown); no external retrieval index — access is via normal prompt conditioning",
            "task_name": "BlocksWorld; TravelPlanner",
            "task_category": "planning (classical and real-world travel planning)",
            "performance_with_memory": "Modest, consistent improvements over direct prompting across models and benchmarks (paper reports small absolute gains; e.g., average metric deltas on TravelPlanner labeled in the paper in the low single digits, roughly +1.7 to +4.4 points across reported metrics).",
            "performance_without_memory": "Direct prompting baseline (no episodic memory). Performance is lower than with episodic memory; exact baseline metrics vary by model and benchmark.",
            "has_comparative_results": true,
            "performance_metric": "Pass rate / micro & macro metrics reported for benchmarks (paper reports 'Final Pass Rate' and other micro/macro metrics for TravelPlanner and success/completion metrics for BlocksWorld).",
            "tradeoffs_reported": "Episodic memory is simple to apply (prompt-level) but provides only modest gains; when combined with parametric (fine-tuned) agents where constraints are already embedded, episodic memory can be redundant or even harmful (degrades performance). Episodic memory relies on global reiteration rather than fine-grained referencing, and can increase prompt length/latency.",
            "limitations_or_failure_cases": "Agents using episodic memory tend to use insights at a coarse/global level and struggle to apply individual insights in a fine-grained fashion; episodic updates struggle with dynamic or context-specific constraints (e.g., TravelPlanner dynamic constraints) and provide only partial mitigation of the diminishing influence of the question across long horizons.",
            "citation": "Jian Xie, Kexun Zhang, Jiangjie Chen, Siyu Yuan, Kai Zhang, Yikai Zhang, Lei Li, Yanghua. 2024. Revealing the Barriers of Language Agents in Planning. arXiv:2410.12409",
            "uuid": "e6441.0",
            "source_info": {
                "paper_title": "Revealing the Barriers of Language Agents in Planning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Parametric memory updating",
            "name_full": "Parametric memory updating via supervised fine-tuning / LoRA",
            "brief_description": "A strategy that updates model parameters (supervised fine-tuning or LoRA) on training examples so that knowledge (constraints, commonsense, frequently needed reasoning behavior) becomes embedded in the model weights, improving planning behavior at inference time.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Parametrically-updated agent (SFT / LoRA)",
            "agent_description": "Memory is encoded into model parameters through supervised fine-tuning (SFT) or LoRA on the training set (ground-truth plans/insights). Access to this memory is implicit (via model activations) rather than through an explicit retrieval interface. Training was done with SFT for smaller contexts and LoRA for long-context TravelPlanner due to cost.",
            "model_size": "various (OpenAI models via their official training scripts; Llama3.1-8B and Qwen2-7B fine-tuned locally; Llama3.1-405B used for inference in some experiments), typically 7B–70B and larger.",
            "memory_used": true,
            "memory_type": "parametric memory (knowledge encoded in model weights via fine-tuning / LoRA)",
            "memory_representation": "parameters / weight updates that encode commonsense rules, constraints and planning heuristics",
            "memory_access_mechanism": "implicit access via the model's forward pass; no explicit retrieval — learned behavior is elicited by prompts due to changed parameters",
            "task_name": "BlocksWorld; TravelPlanner",
            "task_category": "planning (classical and real-world travel planning)",
            "performance_with_memory": "Substantial improvements compared to direct prompting for many models and metrics; the paper reports larger deltas than episodic updates (examples in paper show parametric deltas up to double-digit improvements on some metrics, e.g., reported ∆ values up to +23.7 on some measures).",
            "performance_without_memory": "Direct prompting baseline (no fine-tuning). Parametric updating shows clear gains over this baseline in many cases.",
            "has_comparative_results": true,
            "performance_metric": "Pass rate / micro & macro metrics (Final Pass Rate and other micro/macro breakdowns for TravelPlanner and BlocksWorld success metrics).",
            "tradeoffs_reported": "Fine-tuning yields larger gains but can lead to models that no longer explicitly reference constraints (making appended episodic memory redundant); combining parametric and episodic updates harmed performance in experiments. Parametric updates require compute for fine-tuning and may overfit to static patterns (shortcut learning).",
            "limitations_or_failure_cases": "Although parametric updates increase the question's attribution and improve short-horizon planning, they still suffer from a diminishing influence of the question as planning horizon increases and from shortcut learning (favoring static/common-sense rules over dynamic multi-constraint integration). Combining with episodic memory may degrade performance if constraints have become parameterized.",
            "citation": "Jian Xie, Kexun Zhang, Jiangjie Chen, Siyu Yuan, Kai Zhang, Yikai Zhang, Lei Li, Yanghua. 2024. Revealing the Barriers of Language Agents in Planning. arXiv:2410.12409",
            "uuid": "e6441.1",
            "source_info": {
                "paper_title": "Revealing the Barriers of Language Agents in Planning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Llama3.1-8B (SFT)",
            "name_full": "Llama3.1-8B fine-tuned (supervised fine-tuning) agent",
            "brief_description": "An open-source Llama3.1-8B model that was supervised fine-tuned on the training splits of BlocksWorld and TravelPlanner (SFT / LoRA as appropriate) to embed planning knowledge parametrically.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Llama3.1-8B (fine-tuned)",
            "agent_description": "Llama3.1-8B was fine-tuned (SFT for BlocksWorld; LoRA for TravelPlanner when needed) on ground-truth plans; this parametric update encodes constraints and planning heuristics into model weights, altering how the model attends to questions and constraints at inference.",
            "model_size": "8B",
            "memory_used": true,
            "memory_type": "parametric memory (fine-tuned model weights)",
            "memory_representation": "updated network parameters encoding common constraints/heuristics",
            "memory_access_mechanism": "implicit via forward pass of fine-tuned model; no external retrieval",
            "task_name": "BlocksWorld; TravelPlanner",
            "task_category": "planning",
            "performance_with_memory": "Reported (Table 1 / Table 3): fine-tuned Llama3.1-8B shows substantially higher scores than vanilla in many reported metrics; example from Table 1: Llama3.1-8B_sft scored 48.4 (with constraints shown) vs 45.8 (without constraints) on BlocksWorld (numbers correspond to the table entries reported in the paper).",
            "performance_without_memory": "Vanilla direct prompting Llama3.1-8B scored much lower in other settings (e.g., in some direct-prompting BlocksWorld runs the model had near-zero scores), and gains from fine-tuning were substantial for this model on planning tasks.",
            "has_comparative_results": true,
            "performance_metric": "Final pass rate / micro & macro metrics (as reported in paper tables)",
            "tradeoffs_reported": "Fine-tuning required compute (8× A100 GPUs for local fine-tuning); once constraints were parameterized, providing episodic memory gave little to no additional benefit and could slightly harm performance due to redundancy.",
            "limitations_or_failure_cases": "Even after fine-tuning, the model's ability to maintain question influence over long planning horizons still declines; fine-tuned model may rely on learned shortcuts and fail on dynamic constraint integration.",
            "citation": "Jian Xie, Kexun Zhang, Jiangjie Chen, Siyu Yuan, Kai Zhang, Yikai Zhang, Lei Li, Yanghua. 2024. Revealing the Barriers of Language Agents in Planning. arXiv:2410.12409",
            "uuid": "e6441.2",
            "source_info": {
                "paper_title": "Revealing the Barriers of Language Agents in Planning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Qwen2-7B (SFT)",
            "name_full": "Qwen2-7B fine-tuned agent",
            "brief_description": "An open-source Qwen2-7B model evaluated in both vanilla and fine-tuned (SFT) forms; fine-tuning parametrically embedded constraints/heuristics leading to large performance changes in planning benchmarks.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Qwen2-7B (fine-tuned)",
            "agent_description": "Qwen2-7B was fine-tuned on training examples (SFT) to encode planning constraints and strategies into model parameters; after fine-tuning the agent relied less on explicit prompt constraints.",
            "model_size": "7B",
            "memory_used": true,
            "memory_type": "parametric memory (SFT)",
            "memory_representation": "model weight updates encoding planning constraints and heuristics",
            "memory_access_mechanism": "implicit via model activations after fine-tuning",
            "task_name": "BlocksWorld; TravelPlanner",
            "task_category": "planning",
            "performance_with_memory": "Reported (Table 1): Qwen2-7B_sft achieved 45.4 (with constraints) and 45.4 (without constraints) on BlocksWorld in the paper's table, indicating that fine-tuning parameterized the constraints (removing prompt constraint descriptions did not reduce performance after SFT).",
            "performance_without_memory": "Vanilla Qwen2-7B (direct prompting) scored 2.4 (with constraints) and 3.6 (without constraints) on BlocksWorld (Table 1), indicating very low baseline performance before fine-tuning.",
            "has_comparative_results": true,
            "performance_metric": "Pass rate / benchmark-specific success metrics (as in paper tables)",
            "tradeoffs_reported": "Fine-tuning moved constraint knowledge into model parameters so that prompt-level constraints became redundant; however, this can cause episodic memory to be ineffective or harmful when combined, and promotes shortcut learning on static patterns.",
            "limitations_or_failure_cases": "After fine-tuning, the model still suffers from diminishing attention to the question over long planning horizons and struggles with dynamic constraints that require fine-grained, context-specific referencing.",
            "citation": "Jian Xie, Kexun Zhang, Jiangjie Chen, Siyu Yuan, Kai Zhang, Yikai Zhang, Lei Li, Yanghua. 2024. Revealing the Barriers of Language Agents in Planning. arXiv:2410.12409",
            "uuid": "e6441.3",
            "source_info": {
                "paper_title": "Revealing the Barriers of Language Agents in Planning",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Expel: Llm agents are experiential learners",
            "rating": 2,
            "sanitized_title": "expel_llm_agents_are_experiential_learners"
        },
        {
            "paper_title": "Agenttuning: Enabling generalized agent abilities for llms",
            "rating": 2,
            "sanitized_title": "agenttuning_enabling_generalized_agent_abilities_for_llms"
        },
        {
            "paper_title": "Autoguide: Automated generation and selection of state-aware guidelines for large language model agents",
            "rating": 2,
            "sanitized_title": "autoguide_automated_generation_and_selection_of_stateaware_guidelines_for_large_language_model_agents"
        },
        {
            "paper_title": "Clin: A continually learning language agent for rapid task adaptation and generalization",
            "rating": 1,
            "sanitized_title": "clin_a_continually_learning_language_agent_for_rapid_task_adaptation_and_generalization"
        }
    ],
    "cost": 0.015151749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Revealing the Barriers of Language Agents in Planning
16 Oct 2024</p>
<p>Jian Xie jianxie22@m.fudan.edu.cn 
Fudan University ♡ Carnegie Mellon University ♢ ByteDance Inc
The Ohio State University</p>
<p>Kexun Zhang 
Jiangjie Chen jiangjiec@bytedance.com 
Siyu Yuan syyuan21@m.fudan.edu.cn 
Fudan University ♡ Carnegie Mellon University ♢ ByteDance Inc
The Ohio State University</p>
<p>Kai Zhang 
Yikai Zhang ykzhang22@m.fudan.edu.cn 
Fudan University ♡ Carnegie Mellon University ♢ ByteDance Inc
The Ohio State University</p>
<p>Lei Li leili@cs.cmu.edu 
♡ Yanghua 
Fudan University ♡ Carnegie Mellon University ♢ ByteDance Inc
The Ohio State University</p>
<p>Fudan University</p>
<p>Revealing the Barriers of Language Agents in Planning
16 Oct 202430EBC6262CF6CA03F831F4A2B3C3490DarXiv:2410.12409v1[cs.AI]
Autonomous planning has been an ongoing pursuit since the inception of artificial intelligence.Based on curated problem solvers, early planning agents could deliver precise solutions for specific tasks but lacked generalization.The emergence of large language models (LLMs) and their powerful reasoning capabilities has reignited interest in autonomous planning by automatically generating reasonable solutions for given tasks.However, prior research and our experiments show that current language agents still lack human-level planning abilities.Even the state-of-the-art reasoning model, Ope-nAI o1, achieves only 15.6% on one of the complex real-world planning benchmarks.This highlights a critical question: What hinders language agents from achieving human-level planning?Although existing studies have highlighted weak performance in agent planning, the deeper underlying issues and the mechanisms and limitations of the strategies proposed to address them remain insufficiently understood.In this work, we apply the feature attribution study and identify two key factors that hinder agent planning: the limited role of constraints and the diminishing influence of questions.We also find that although current strategies help mitigate these challenges, they do not fully resolve them, indicating that agents still have a long way to go before reaching human-level intelligence.Resources are available on the GitHub.</p>
<p>Introduction</p>
<p>Planning is the process of determining the sequence of actions needed to achieve a goal.It involves goal decomposition, constraint consideration, and foresight for simulating and predicting outcomes.In the development of artificial intelligence, this capability is considered the "Holy Grail" for achieving or even surpassing human intelligence (Kahne- man, 2011; OpenAI, 2023b).However, the path to achieving autonomous planning is a long journey.Researchers have long focused on building custom systems tailored to specific tasks (Newell et al., 1959;McDermott, 1992;Silver et al., 2017).While these systems could deliver precise solutions through rigorous problem solvers, the extensive effort required for task-specific design prevents them from achieving universal problem-solving capabilities or general intelligence.The advent of language agents (Weng, 2023;Su, 2023;Sumers et al., 2024), which are powered by large language models (LLMs; OpenAI (2022, 2023a); G Team et al. (2023); Dubey et al. (2024); Yang et al. (2024)), changes the landscape.Thanks to the flexibility of natural language, LLM-based language agents have shown strong potential to generalize to various planning tasks without relying on traditional curated, task-specific solvers written in domain-specific languages like Planning Domain Definition Language (PDDL).However, despite these language agents demonstrating impressive capabilities across various tasks (Yao et al., 2022(Yao et al., , 2023;;Zheng et al., 2024a;Gu et al., 2024), their performance in planning remains disappointing and is viewed as mere "approximate retrieval" (Kambhampati et al., 2024) rather than engaging in genuine reasoning.Specifically, even the most capable model, OpenAI o1 (OpenAI, 2024), which claims to surpass human PhD-level accuracy on several reasoning tasks, achieves only 15.6% in a realworld travel planning benchmark, TravelPlanner (see Figure 2), far below human-level planning abilities.To uncover the fundamental reasons behind the weak performance, we seek to answer the first research question in this paper: RQ1: Why do current language agents struggle with planning?</p>
<p>In order to enhance language agents' performance in planning tasks, numerous strategies have been proposed recently, which can be categorized into three main branches, as shown in Figure 1: episodic memory updating through prompt optimization (Zhao et al., 2024;Shinn et al., 2024;Fu et al., 2024), parametric memory updating through model training (Zeng et al., 2023a;Song et al., 2024;Yin et al., 2024), and translating queries into formal planning languages, followed by resolution using external solvers (Liu et al., 2023;Dagan et al., 2023).Although these strategies have shown performance improvements across various tasks, their underlying mechanisms remain largely opaque.Moreover, these strategies still fall short of human-level intelligence (Valmeekam et al., 2024a,b;Stechly et al., 2024), particularly in complex real-world tasks (Xie et al., 2024b;Gundawar et al., 2024;Chen et al., 2024).Therefore, based on the findings from RQ1, this paper seeks to answer the research questions, RQ2: What happens during memory updating for language agents and RQ3: What hinders these strategies from achieving high-level planning abilities?Specifically, we focus on language agents' vanilla planning as well as planning following memory updating, which reflect the internal planning capabilities of language agents rather than the translation ability.</p>
<p>In this paper, we delve into the two main components of planning: constraints and questions, which serve as the foundational elements for planning tasks.Constraints refer to the rules that agents must adhere to when generating a plan, while questions represent the goals that drive the planning process.Understanding how agents handle these elements is crucial for improving their performance in complex planning tasks.Using Permutation Feature Importance (Breiman, 2001;Fisher et al., 2019) to analyze the feature attribution of constraints and questions, our investigation reveals several key findings: 1) Language agents show a limited understanding of constraints, and the influence of the question weakens as the planning horizon increases.</p>
<p>2) Episodic memory updating improves constraint understanding but relies on global understanding, and it's still difficult for agents to reference constraints in a fine-grained manner.3) Parametric memory updating enhances the question's impact on the final plan, but the diminishing influence of the question remains a challenge.4) Both strategies resemble "shortcut learning" and struggle with dynamic constraints in planning.</p>
<p>Related Work</p>
<p>Language Agent</p>
<p>The advent of large language models sparks widespread attention due to their remarkable abilities, such as mathematical reasoning, creative writing, and information retrieval (Gómez-Rodríguez and Williams, 2023;Zhang et al., 2023;Lou et al., 2024;Zhu et al., 2024).Building on these models, language agents expand their capabilities to engage with the real world, including utilizing tools (Gu et al., 2024), grounding environments (Zheng et al., 2024a), and even controlling real-world robotics (Zeng et al., 2023b), functioning as a "reasoning brain" beyond mere text generation.The conceptual framework of language agents includes: 1) Memory module handles both longterm memory embedded in the model's parameters, such as commonsense (West et al., 2022), and shortterm memory specific to tasks (Majumder et al., 2023).2) Tool-use module enables agents to utilize external tools to compensate for inherent limitations, such as calling a calculator for arithmetic tasks or retrieving up-to-date information from external databases (Lu et al., 2023;Xie et al., 2024a;Wu et al., 2024).3) Planning module controls the entire task process, including goal decomposition, action sequencing, and forward estimation, requiring comprehensive and advanced reasoning abilities (Weng, 2023;Sumers et al., 2024).</p>
<p>Planning in Language Agents</p>
<p>Planning, a hallmark of human intelligence, serves as a critical component in language agent systems, as it directly controls task execution and goal achievement.Improving an agent's planning abilities thus leads to overall improvements across various tasks.However, previous studies show that current agents still struggle with planning tasks, such as classical tasks like block manipulation (Valmeekam et al., 2024a) or real-world tasks like travel planning (Xie et al., 2024b;Zhang et al., 2024).While these studies highlight agents' weaker performance in planning, they mainly pro-vide high-level observations, leaving the deeper, underlying reasons less explored.Furthermore, although strategies such as updating episodic memory (also referred to as working memory (Zhao et al., 2024)), which allows learning from past trials and errors (Shinn et al., 2024;Fu et al., 2024), or improving parametric memory through finetuning (Yin et al., 2024) have been proposed, the mechanisms driving these performance improvements, as well as their limitations, remain unclear.Therefore, in this work, we aim to understand the challenges faced by current language agents in planning and provide promising directions for addressing weaknesses in planning strategies to guide the development of more effective agents.</p>
<p>Interpretability of Language Models</p>
<p>Despite the impressive capabilities of LLMs, their thinking processes remain opaque.Interpreting these models is essential for improving their reliability and transparency in real-world applications.Attention visualization helps explore how models allocate attention across different input elements, attention heads, and layers within the model (Katz and Belinkov, 2023;Zheng et al., 2024b;Luo and Specia, 2024).Additionally, feature attribution methods analyze the importance of each input feature using techniques like perturbation (Ribeiro et al., 2016;Fisher et al., 2019) and gradients (Sundararajan et al., 2017;Mudrakarta et al., 2018).However, much of the existing work focuses on traditional tasks such as classification, which do not fully reflect the complexity of planning tasks.Planning requires handling long-horizon dependencies and balancing multiple objectives or constraints, presenting unique challenges that remain underexplored.To address this gap, this work utilizes interpretability techniques to investigate why agents struggle with planning tasks.</p>
<p>Background</p>
<p>Dataset</p>
<p>We choose Blocksworld and TravelPlanner as our testbeds, which cover both classical planning and real-world complex planning scenarios:</p>
<p>• BlocksWorld (Valmeekam et al., 2024a) is a planning benchmark that provides a domain description, including action and constraint definitions, and requires agents to execute actions to transition from an initial state to a goal state.All actions must adhere to the explicit constraints outlined in the prompt.</p>
<p>• TravelPlanner (Xie et al., 2024b) is a realworld travel planning benchmark that requires language agents to generate plans based on provided information and user queries, aligning with commonsense and the hard constraints specified in the queries.Unlike the static nature of BlocksWorld, the hard constraints in TravelPlanner are dynamic, as they need to be inferred from the query and satisfied through item selection.We use the "soleplanning" mode to focus on the agents' planning ability, excluding the influence of tooluse abilities required in the "two-stage" mode.</p>
<p>Permutation Feature Importance</p>
<p>Permutation Feature Importance (Breiman, 2001;Fisher et al., 2019) is a strategy for evaluating the importance of features in a system.Specifically, if a feature is important, its removal or alteration will significantly affect the system's result, whereas an unimportant feature will have little to no impact.</p>
<p>In this paper, we adopt Permutation Feature Importance as our analysis strategy for testing the inner workings of language agents when planning.Formally, given a language model P θ , a feature sequence X = {x 1 , x 2 , . . ., x n }, and a target sequence Y = {y 1 , y 2 , . . ., y m }, the attribution score S i,j for the contribution of feature x i to the target y j is defined as follows1 :</p>
<p>Si,j = P θ (yj | X, Y1:j−1) − P θ (yj | Xi, Y1:j−1).(1)</p>
<p>A low or near-zero S i,j indicates that the feature x i is almost independent of the target, while a higher score suggests a stronger contribution.Here, P θ (y j | X, Y 1:j−1 ) represents the conditional probability of the target y j given the original input sequence X and the preceding targets Y 1:j−1 as predicted by the model.Xi denotes the input sequence X with the values of feature x i permuted.</p>
<p>Experimental Setting</p>
<p>Data Slice In BlocksWorld, the dataset is randomly split into a training set (100 samples) and a validation set (500 samples).In TravelPlanner, we use the original training (45 samples) and validation sets (180 samples) for our experiments.Episodic and parametric memory updating either summarize insights from prior attempts on the training set or train on it, with evaluation performed on the validation set.Due to the high computational cost, for BlocksWorld, we randomly select 200 samples when computing attribution scores.</p>
<p>Episodic Memory Updating For episodic memory updating, following previous work (Zhao et al., 2024;Fu et al., 2024), we require language agents to summarize insights from previous attempts, categorized into two groups and one additional humanwritten reference: 1) Behavioral Cloning -The agent is provided with previous failed attempts along with a ground truth plan (from the external solver in BlocksWorld or human annotations in TravelPlanner).2) Oracle Feedback -The agent is provided with previous failed attempts along with feedback from the solver or evaluator, explaining the reasons for failure.3) Reference -This setting provides human-written insights, serving as a ground truth summary of the constraints.Specifically, in BlocksWorld, this refers to the reiteration of constraint descriptions, and in TravelPlanner, it includes refined summaries of commonsense and hard constraints.</p>
<p>Why Do Current Language Agents</p>
<p>Struggle with Planning?</p>
<p>Status Quo</p>
<p>Agents demonstrate performance nearing or on par with humans in tasks like tool-use and web navigation (Lu et al., 2023;Zheng et al., 2024a).However, when it comes to planning, which requires advanced reasoning, such as goal decomposition, constraint analysis, and foresight, agents still face significant challenges.Specifically, as Figure 2 demonstrates, with direct prompting, most of the current agents only complete less than half of the tasks in BlocksWorld.In a more complex, real-world benchmark TravelPlanner, agent performance is even lower, with none surpassing a 20% final pass rate, including OpenAI's flagship reasoning model o1.This raises an important question: Why do current language agents struggle with planning, and what hinders them from achieving advanced planning capabilities?In this section, we delve into the core aspects of planning to uncover the reasons behind the weak performance.</p>
<p>Limited Role of Constraints and Diminishing Influence of Questions</p>
<p>We use Permutation Feature Importance (Breiman, 2001;Fisher et al., 2019) as the analysis strategy to evaluate the attribution score of each part of the prompts in relation to the final plan, covering various foundation models and two benchmarks.Specifically, for BlocksWorld, we divide the prompts into three components: action definitions, constraint descriptions, and questions.Each part is replaced with an empty token to compute its attribution score relative to the final plan.For TravelPlanner, a real-world benchmark that requires actions to rely on commonsense embedded in the model's parameters, we focus on the attribution score of constraints and questions.When evaluating the constraint component, we replace the attributes (e.g., price) of elements selected by the agents with an empty token to evaluate whether agents are genuinely incorporating constraints into their planning or merely generating constraint-conforming plans by chance.For the question component, we apply the same substitution strategy used in BlocksWorld.</p>
<p>Agents do not adequately reference constraints during planning.Constraints, as one of the key restraining factors, play a crucial role in planning.Violating constraints directly leads to failed plans since it results in illegal actions or unsatisfied goals, as highlighted in previous studies (Valmeekam et al., 2024a;Xie et al., 2024b).To investigate the reason why agents cannot obey constraints, we compute the attribution score of the constraint component, with the results presented in Figure 3. Compared to the upper bound score (100), which indicates a dominant role, constraints account for only a small portion of the planning process, with all scores being less than 25.Furthermore, we find agents are not able to reference constraints precisely.For example, when executing the action "Pick Up", agents should reference all related constraint descriptions for "Pick Up", and the attribution score should be significantly positive, as the constraint description contributes to the final plan.However, as shown by the detailed score distribution of the Llama3.1-70Bmodel in Figure 4, agents exhibit weak constraintreferencing behavior.Comparing the left side of the figure, where actions align with their descriptions, the current agents fail to reference constraints effectively during planning.Similarly, in Trav-elPlanner, if agents were planning based on the required attributes, the attribution score between the attribute and the final item selection should be significantly positive.For example, the price should influence the item choice if the agent is performing real reasoning.Yet, across both bench-marks, none of the agents show the ability to fully adhere to this constraint-referencing behavior, resulting in unmet preconditions in BlocksWorld and unfulfilled hard constraints in TravelPlanner.</p>
<p>Moreover, in BlocksWorld, we find that for Qwen2-7B, the attribution score of the constraint component is negative, indicating that the presence of constraints negatively impacts planning.To investigate further, we test the performance of three models-Qwen2-7B, Llama3.1-8B, and Llama3.1-70B-whenno constraint descriptions are provided.As shown in Table 1, removing constraints leads to higher scores for Qwen2-7B, while Llama3.1-70Bexhibited a significant decline.This suggests that agents struggle to effectively reference constraints during planning, and in some weaker agents, constraints may even distract them and degrade their performance, which also verifies the effectiveness of our analysis strategy.As the planning horizon increases, the influence of the question on plan generation decreases.</p>
<p>We find that while agents can deliver a complete plan, these plans often fail to meet the goals specified in the question, with failure rates increasing as the planning horizon increases.Specifically, as shown in the upper part of Figure 5, in both BlocksWorld and TravelPlanner, agent performance declines as the number of generated steps or travel days increases.Could this be similar to the "lost in the middle" phenomenon (Liu et al., 2024), where they lose track of the goal as the planning horizon increases?To investigate the underlying reasons, we compute the attribution score of the question at different steps in the plan, as shown in the lower part of Figure 5.We find that as the planning horizon increases, the attribution score of the question decreases.This suggests that the question's influence on specific action or item selection diminishes as the plan progresses.This explains why agents perform worse as the plan length increases: if agents fail to focus on the goal specified in the query and lose track of it, they cannot deliver a successful plan.</p>
<p>What Happens in Memory Updating</p>
<p>for Language Agents?</p>
<p>While previous work and our experiments, as shown in Figure 2, show that both parametric mem-ory updating and episodic memory updating can improve agents' performance in planning tasks, the underlying mechanisms remain unclear.In this section, we aim to address the following two questions: 1) Why do memory updating strategies help improve agents' planning abilities?2) What limitations of these strategies prevent agents from achieving more advanced planning abilities?</p>
<p>Episodic memory Updating</p>
<p>Episodic memory updating refines and reiterates constraint information, making it easier for agents to recognize and apply.Rather than incorporating new information, we find that simply refining or reiterating existing insights in episodic memory updating can lead to performance improvements.In TravelPlanner, performance gains are observed when refined information (e.g., insights like selecting cheaper items, which agents would otherwise need to infer themselves) is introduced.Similarly, in BlocksWorld, both agent-generated and human-written insights-despite being slight modifications or emphases of the original constraint descriptions-still result in performance enhancements with episodic memory updating.This is intriguing, as such repetition typically offers little value in human reasoning.</p>
<p>To assess the impact of episodic memory updating on plan generation, we compute the attribution score of episodic memory (Figure 3).Specifically, these refined or reiterated insights show positive attribution scores to the final plans, indicating that agents actively consider them during planning.However, the figure also shows that while vague and implicit episodic memories (e.g., "select cheap items" in TravelPlanner) do contribute, more explicit and direct constraints (e.g., "cannot 'Pick Up' when the hand is not empty" in BlockWorld) are easier for agents to utilize, as demonstrated by their higher attribution scores.</p>
<p>Agents understand episodic memory on a global level and cannot reference it in a fine-grained manner.While episodic memory updating improves performance, the gains remain relatively minor.To investigate this further, we decompose the episodic memory into discrete components (i.e., treating each insight independently) to assess whether agents can reference these insights in a fine-grained manner.For example, in Trav-elPlanner, agents are expected to consider specific insights related to accommodation when selecting Steps Attribution Score
1 3 5 7 −0.2 0 0.2 0.4 0.6 0.8 1 1.2 Days Attribution Score GPT-4o GPT-4o-Mini Qwen2-7B Qwen2-72B Llama3.1-8B
Llama3.1-70B Llama3.1-405BSFT lodging options.However, as shown in Figure 6, while agents reference the overall episodic memory during planning, they struggle to apply individual insights in a detailed, fine-grained manner, reflected in their relatively low scores.Moreover, in BlocksWorld, we observe that the constraint description for "Unstack" plays only a minor role (scoring 0.0188) in the original constraint attribution (Figure 4), but contributes significantly more in the episodic memory (0.1954; Figure 6).We hypothesize that episodic memory complements information that agents might have initially overlooked.However, agents still struggle to apply this information effectively during planning when dealing with more vague or implicit episodic memories, such as those in TravelPlanner.</p>
<p>Parametric Memory Updating</p>
<p>Parametric memory updating improves the attribution score of questions.Although para-metric memory updating improves agents' performance in planning tasks, its underlying mechanisms remain unclear.Building on previous findings that the attribution score of questions relates to an agent's planning performance, we investigate whether this score changes after parametric memory updating.As shown in Figure 5, we observe a positive correlation between the question attribution score and final performance.For example, in BlocksWorld, from step 2 to step 4, the question attribution score increases, resulting in both finetuned agents achieving their highest scores at step 4.This suggests that through fine-tuning, agents are able to have a stronger focus on the goal than before, leading to improved planning outcomes.</p>
<p>While parametric memory updating increases the attribution score of questions, it still struggles as the planning horizon increases.fine-tuning compared to the vanilla agents, a decline is observed after step 4 in BlocksWorld, leading to a corresponding drop in performance.A similar trend is also noted in TravelPlanner.This suggests a promising direction: maintaining a strong focus on the goal throughout planning is essential for overcoming short-horizon limitations and advancing agents' planning abilities.</p>
<p>Discussion</p>
<p>When constraints are already parameterized, episodic memory updating does not improve performance and may even degrade it.If both parametric and episodic memory updating are effective for agent planning, an interesting question arises: Would it be better to combine these two strategies?Surprisingly, this mixture does not improve the performance of fine-tuned agents and even harms it.</p>
<p>As shown in Table 2, both fine-tuned agents exhibit a performance decline after episodic memory updating.Moreover, as shown in Figure 7, the attribution scores of both constraints and episodic memory play only a minor or even negative role, indicating that the fine-tuned agents no longer explicitly reference these constraints, rendering them redundant and ineffective.We hypothesize that reiterated episodic memory becomes redundant when constraints are already embedded within the model's parameters.This redundancy disrupts the model's decision-making coherence and undermines its ability to leverage the pre-existing constraint knowledge, resulting in weaker planning performance.</p>
<p>To explore this further, we also report the performance of fine-tuned agents with the constraints removed in the Llama3.1-8Bsf t shows only a slight decline, and Qwen2-7B sf t even exhibits no decrease at all, suggesting that the constraints had already been parameterized within them.</p>
<p>Both strategies resemble shortcut learning, focusing on short-horizon and low-level planning.</p>
<p>Although both strategies offer performance improvements, they fail to achieve our expected highlevel intelligence.Our findings suggest that these strategies resemble "shortcut learning," favoring static rule learning over dynamic problem-solving.For example, in TravelPlanner, agents learn commonsense rules effectively, especially through parametric memory updating (see Table 3), as commonsense is often based on static patterns learned in training data.However, these strategies remain insufficient when faced with hard constraints requiring advanced model abilities, such as maintaining a strong focus on long-horizon tasks, precise referencing for multiple-constraint integration, and sophisticated planning skills like foresight, simulation, and backtracking for trajectory adjustments.</p>
<p>Conclusion</p>
<p>This paper utilizes Permutation Feature Importance to investigate why current language agents struggle with planning tasks.Our findings show that constraints play only a minor role in agent planning, indicating that agents are not effectively consid-ering constraints during planning.Additionally, the question's influence diminishes as the planning horizon extends, causing agents to lose focus on the goal and resulting in failed plans.Furthermore, we examine the effects of episodic and parametric memory updating on agent performance.While both strategies improve the impact of constraints and questions in planning, they only mitigate the underlying issues rather than fully resolve them.We hope this paper provides valuable insights and sparks future research to address language agents' key challenges in planning, ultimately moving closer to achieving human-level intelligence.</p>
<p>Limitations</p>
<p>In this paper, we use Permutation Feature Importance to calculate the attribution scores across various open-source model families and sizes, aiming to provide insights into the obstacles that current language agents face in planning tasks.We also test the performance of the widely used GPT family.However, due to the limited access provided by OpenAI's API, which restricts control over output token generation, we are unable to compute attribution scores for these models.Nonetheless, the consistent conclusions drawn from the two used model families across different sizes and benchmarks support the robustness of our methodology and validate our overall findings.</p>
<p>Appendix</p>
<p>Within this supplementary material, we elaborate on the following aspects:</p>
<p>B Experimental Setup Details B.1 Episodic Memory Updating</p>
<p>Training For episodic memory updating, we follow the methodology proposed by Zhao et al. (2024), where the agent is tasked with summarizing insights from previous attempts.The process of filtering these insights involves a voting system, where the agent can take one of the following actions:</p>
<p>• Add: Introduce new, general insights that are not restricted to specific queries and are missing from the current set.New insights are beginning with one vote.</p>
<p>• Modify: Revise existing insights if they are incomplete or partially incorrect.This action preserves the original number of votes for the insight.</p>
<p>• Support: Endorse correct insights by increasing their vote count by one.This action ensures that useful insights are retained and emphasized.</p>
<p>• Oppose: Challenge incorrect or irrelevant insights, decreasing their vote count by one.This process helps eliminate inaccuracies.</p>
<p>Inference When inference in the validation set, agents are required to use the insight learned in the training set.First, they are required to select the insight that they think is useful and then plan based on these insights.Only the votes surpassing five will be shown to the agents.During inference on the validation set, agents are required to apply the insights learned during training.First, agents must select the insights they find helpful and then use them to guide their planning.Only insights with a vote count exceeding five are displayed to the agents for use during planning.</p>
<p>B.2 Parametric Memory Updating</p>
<p>OpenAI Models We use the official training script and default hyperparameters for OpenAI models.Specifically, for BlocksWorld, the hyperparameters are training steps set to 3, batch size set to 1, learning rate multiplier set to 2, and random seed set to 341541772.For TravelPlanner, the hyperparameters are training steps set to 3, batch size set to 1, learning rate multiplier set to 2, and random seed set to 1294003109.</p>
<p>Open-source Models We fine-tune Llama3.1-8B and Qwen2-7B on 8×A100 GPUs.For BlocksWorld, the training step is 50, the batch size is 16, the learning rate is 1e-5, the learning rate schedule is cosine, and the warmup ratio is set to 0.1.</p>
<p>For TravelPlanner, due to the high computational cost associated with its longer context, we adopt LoRA as the training strategy.The training step is 200, the batch size is 2, and the learning rate is 1e-4.Other hyperparameters remain the same as in BlocksWorld.</p>
<p>B.3 Model Access</p>
<p>Our experiments utilize four closed-source LLMs accessed via API and five open-source LLMs.For the open-source models, we use instruction-tuned versions of each.Due to the high cost of deploying Llama3.1-405B,we perform inference on the Google Vertex AI platform and compute attribution scores locally.To ensure reproducibility, we have included the prompts used in our experiments in Appendix C. For closed-source models, we use GPT-4o-2024-08-06, GPT-4o-mini-2024-07-18, o1-preview-2024-09-12, and o1-mini-2024-09-12</p>
<p>¦ ¥</p>
<p>B.5 Attribution Score Calculation</p>
<p>To obtain accurate attribution scores, we focus only on "meaningful words" in the analysis.For instance, in BlocksWorld, we consider only actions such as "Pick Up", "Put Down", "Stack", and "Unstack", along with relevant objects like "red block", while discarding non-essential words like "the" A similar approach is applied to TravelPlanner, where only the values in the JSON format are considered.For example, in "Accommodation: XXX", only "XXX" is used for calculating attribution scores.</p>
<p>B.6 Attribution Score Normalization</p>
<p>Due to the varying planning steps and models, for example, the attribution scores in different models are in different scales, which cannot be compared directly.To address this, we normalize the attribution scores by dividing each score by the maximum absolute value along the relevant dimension, which ensures that all scores are scaled consistently across different models.This normalization process allows for a fair comparison of the attribution scores by bringing them into a comparable range, typically between −1 and 1, without distorting the relative importance of features within the same model.</p>
<p>C Prompt List</p>
<p>We provide the prompts utilized in this paper here.</p>
<p>C.1 Behavioral Learning Prompt § ¤</p>
<p>You are tasked with analyzing both successful and failed plans from previous attempts based on a specific query and background information .These failed plans are presented in chronological order , with the most recent plan including a detailed trajectory .As these plans fail to meet certain constraints , you are encouraged to refine the insights to improve them .</p>
<p>Use the following format to systematically analyze failed plans :</p>
<p>[ State ]: Describe the current situation , including factors like remaining budget , time constraints , and any other specified conditions in the query or provided information .</p>
<p>[ Thought ]: Explain the reasoning behind your decisions , considering the current state .</p>
<p>[ Action ]: Detail the specific parts of your plan in response to the [ State ] and [ Thought ].</p>
<p>For the successful plan , add a [ Best Practice ] section after the final analysis to summarize the key experiences and practices that led to success .For the failed plan , add an [ Error ] section immediately after each defective [ Action ].This section should identify and explain why the chosen actions or used insights were inappropriate , given the [ State ] and [ Thought ].</p>
<p>After evaluating the plans and previous insights , refine the current insight set based on findings from previous attempts and newly identified errors .</p>
<p>Your task involves adding , editing , supporting , and opposing insights from the existing set :</p>
<p>[ Add ]: Integrate new pairs that are missing in the current set .Add new ones only when absolutely necessary .</p>
<p>[ Edit ]: Revise pairs that are incomplete or partially incorrect .Editing an insight retains its number of votes .</p>
<p>[ Support ]: Endorsing specific pairs to emphasize their value .Increase the number of votes for the supported pair by 1 each time .Some previously used insights might have been edited ( with the same index ).If you support the new version , vote for it .</p>
<p>[ Oppose ]: Challenging insights that are incorrect , outdated , or only applicable under specific conditions .This will decrease the number of votes by 1 each time .</p>
<p>C.2 Oracle Feedback Learning Prompt § ¤</p>
<p>You are tasked with analyzing failed plans from previous attempts , along with their evaluation results , based on a specific query and background information .These failed plans are presented in chronological order , with the most recent plan including a detailed trajectory .As these plans fail to meet certain constraints , you are encouraged to refine the insights to improve them .</p>
<p>Use the following format to systematically analyze failed plans :</p>
<p>[ State ]: Describe the current situation , including factors like remaining budget , time constraints , and any other specified conditions in the query or provided information .</p>
<p>[ Thought ]: Explain the reasoning behind your decisions , considering the current state .</p>
<p>[ Action ]: Detail the specific parts of your plan in response to the [ State ] and [ Thought ].</p>
<p>For the failed plan , add an [ Error ] section immediately after each defective [ Action ].This section should identify and explain why the chosen actions or used insights were inappropriate the [ State ] and [ Thought ].</p>
<p>After evaluating the plans and previous insights , refine the current insight set based on findings from previous attempts and newly identified errors .</p>
<p>Your task involves adding , editing , supporting , and opposing insights from the existing set :</p>
<p>[ Add ]: Integrate new pairs that are missing in the current set .Add new ones only when absolutely necessary .</p>
<p>[ Edit ]: Revise pairs that are incomplete or partially incorrect .Editing an insight retains its number of votes .</p>
<p>[ Support ]: Endorsing specific pairs to emphasize their value .Increase the number of votes for the supported pair by 1 each time .Some previously used insights might have been edited ( with the same index ).If you support the new version , vote for it .</p>
<p>[ Oppose ]: Challenging insights that are incorrect , outdated , or only applicable under specific conditions .This will decrease the number of votes by 1 each time .</p>
<p>Opposing and editing are highly encouraged to resolve any conflicting insights .Avoid proposing insights with similar purposes .Note : Ensure that the insights are high -level and generalizable , rather than detailed and specific to particular queries .Make sure your contributions do not introduce unrelated insights or go beyond the scope of the provided information .</p>
<p>Figure 1 :
1
Figure 1: Memory updating strategies for language agents.Insights are learned from previous attempts.</p>
<p>2 We use Reference to implement the episodic memory updating strategy in the feature attribution study to avoid discrepancies in insights generated by different agents and ensure experimental control and consistency in our analysis.More details are provided in Appendix B.1.Parametric Memory UpdatingWe use supervised fine-tuning (SFT) for parametric memory updating, with the ground truth in the training set as the optimization objective.All local training and inference experiments are conducted on 8×A100 GPUs.For OpenAI models, we use the official scripts for training.Please refer to Appendix B.2 for more details.</p>
<p>Figure 2 :Figure 3 :
23
Figure 2: Main results of 9 models with different strategies on two benchmarks.The results of o1-Preview and o1-Mini on BlocksWorld are from Valmeekam et al. (2024b)."Beh.Clo." and "Ora.Fee."indicate BehavioralCloning and Oracle Feedback, respectively.Llama3.1-8B and Qwen2-7B tend to provide case-specific insights that lack general applicability; thus, these models are excluded from the "Beh.Clo." and "Ora.Fee."settings.</p>
<p>Figure 4 :
4
Figure 4: The distribution of attribution scores for action and constraint descriptions relative to the actions in the final plans in Llama3.1-70B on BlocksWorld.The distribution of attribution scores and discussion of Trav-elPlanner are in Appendix A.1.</p>
<p>Figure 5 :Figure 6 :
56
Figure 5: Performance comparison with increasing planning horizon.The upper part shows the performance of different agents, while the lower part shows their attribution scores of questions as the planning horizon extends.</p>
<p>Figure 7 :
7
Figure 7: Attribution scores of constraints and episodic memory on BlocksWorld for two fine-tuned agents.</p>
<p>•</p>
<p>Figure A.1: The distribution of attribution scores for constraint descriptions relative to the actions in the final plan in Llama3.1-70B on TravelPlanner.</p>
<p>Opposing and editing are highly encouraged to resolve any conflicting insights .Avoid proposing insights with similar purposes .Legal Action onCurrent Insight Set : [ Add / Edit / Support / Oppose ] [ Insight 1]: [ Content ]. use the following format for your response ( do not output in the markdown style ) : Successful Plan Analysis : Failed Plan Analysis : Action on Current Insight Set : [ Finished ] ¦ ¥</p>
<p>Legal</p>
<p>Action on Current Insight Set : [ Add / Edit / Support / Oppose ] [ Insight 1]: [ Content ]. following format for your response ( do not output in the markdown style ) : Failed Plan Analysis : Action on Current Insight Set : [ Finished ] ¦ ¥</p>
<p>Table 1 :
1
Performance comparison with and without constraint descriptions in the prompts on BlocksWorld.
w/ Constraints w/o ConstraintsQwen2-7B2.43.6Llama3.1-8B0.60.6Llama3.1-70B38.89.8Qwen2-7B sf t45.445.4Llama3.1-8B sf t48.445.8</p>
<p>Table 2 :
2
Comparison between two fine-tuned models with and without episodic memory updating on BlocksWorld.
Despite</p>
<p>Table 1 .
1
Unlike the vanilla Llama3.1-70B,which shows a noticeable performance drop,
CommonsenseHardFinalMicro Macro Micro MacroPass RateDirect PromptingGPT-4o84.731.153.631.17.8GPT-4o-Mini84.422.242.420.02.2Llama3.1-8B60.10.07.92.80.0Llama3.1-70B 82.818.933.116.12.2Qwen2-7B49.91.12.10.00.0Qwen2-72B74.811.723.88.91.7Episodic Memory UpdatingGPT-4o89.241.751.727.28.3GPT-4o-Mini84.122.239.822.85.0Llama3.1-70B 84.923.939.524.46.1Qwen2-72B75.613.828.810.63.3∆+1.8+4.4+1.7+2.3+2.2Parametric Memory UpdatingGPT-4o95.368.962.639.425.0GPT-4o-Mini94.761.749.317.212.2Llama3.1-8B78.317.819.36.13.8Qwen2-7B59.00.60.20.00.0∆+12.1 +23.7+6.4+2.2+7.8</p>
<p>Table 3 :
3
Comparison between different agents on Trav-elPlanner."∆" represents the average improvement compared to the same model using direct prompting.</p>
<p>across all tests.
constraints .8. Ensure accommodations comply withspecific rules and preferences ,including room type and restrictions onparties , smoking , pets , or visitors .9. Adjust transportation options andother preferences according to the user 's specified requirements , such asavoiding flights or self -driving .10. Opt for budget -friendlyaccommodations , restaurants , andtransportation methods .B.4 Human-Written InsightsWe provide human-written insights forBlocksWorld and TravelPlanner here.  §¤[ BlocksWorld ]1. Only pick up or unstack one block ata time , ensuring your hand is emptybefore doing so .2. A block can be picked up or unstackedonly if it ' s clear and on the table .3. A block is clear if it has no blockson top and is not currently being held .4. When unstacking , ensure the block you' re removing is actually on top andclear .5. After picking up or unstacking ablock , you must hold it until it ' splaced down or stacked .6. You can only place a block you ' reholding , and stacking can only occur ifthe target block is clear .7. Once a block is placed down orstacked , your hand becomes empty , andthe block below a newly stacked one isno longer clear .[ TravelPlanner ]1. Verify transportation and attractionavailability before planning and providealternatives if needed .2. Ensure all plan details andactivities are based on available datawithin the designated environment toavoid inaccuracies .3. Include all essential details , suchas accommodations and daily activities ,ensuring they align logically with theplanned city and timeline .4. Maintain diversity by avoidingrepetition of restaurant or attractionchoices throughout the trip .5. Ensure transportation methods areconsistent and logical within the trip ' scontext , avoiding conflicting optionslike self -driving and flights .6. Follow any specified minimum nightstay requirements when bookingaccommodations .7. Plan activities , accommodations , andmeals to align with the user ' s budget
For simplicity, we omit transformations like log and softmax here.
TravelPlanner prohibits providing evaluation metrics directly to the agent. This setting is used solely for analysis, and the results will not be submitted to the leaderboard.
Tinghui Zhu,Kai Zhang, Jian Xie, and Yu Su. 2024. Deductive beam search: Decoding deducible rationale for chain-of-thought reasoning. In First Conference on Language Modeling.
C.3 BlocksWorld Inference Prompt § ¤{ query }To help your plan , some insights from a set summarized by previous agents will be provided .Not all insights will be appropriate ; you need to select the relevant ones to guide your plan .The values in brackets indicate the reliability of the insights , with higher values representing greater reliability .Insight Set : { insight_set } You should specify the insights you have chosen ( beginning with [ Chosen Insights ]) , followed by your final plan ( beginning with[ Plan ]) .¦ ¥C. Plan : [ {{ " days ": 1 , " current_city ": " from St .Petersburg to Rockford ", " transportation ": " Flight Number : F3573659 , from St .Petersburg to Rockford , Departure Time : 15:40 , Arrival Time : 17:04" , " breakfast ": " -" , " attraction ": " -" , " lunch ": " -" , " dinner ": " Coco Bambu , Rockford ", " accommodation ": " Pure luxury one bdrm + sofa bed on Central Park , Rockford " }} , {{ " days ": 2 , " current_city ": " Rockford ", " transportation ": " -" , " breakfast ": " Flying Mango , Rockford ", " attraction ": " Burpee Museum of Natural History , Rockford ; Midway Village Museum , Rockford ; Discovery Center Museum , Rockford ", " lunch ": " Grappa -Shangri -La ' s -Eros Hotel , Rockford ", " dinner ": " Dunkin ' Donuts , Rockford ", " accommodation ": " Pure luxury one bdrm + sofa bed on Central Park , Rockford " }} , {{ " days ": 3 , " current_city ": " from Rockford to St .Petersburg ", " transportation ": " Flight Number : F3573120 , from Rockford to St .Petersburg , Departure Time : 19:00 , Arrival Time : 22:43" , " breakfast ": " Subway , Rockford ", " attraction ": " Klehm Arboretum &amp; Botanic Garden , Rockford ; Sinnissippi Park , Rockford " , " lunch ": " Cafe Coffee Day , Rockford ", " dinner ": " Dial A Cake , Rockford ", " accommodation ": " -" }} ] <strong><em>*</em> Example Ends </strong>***To help your plan , some insights from a set summarized by previous agents will be provided .Not all insights will be appropriate ; you need to select the relevant ones to guide your plan .The values in brackets indicate the reliability of the insights , with higher values representing greater reliability .Insight Set : { insight_set } Given information : { text } Query : { query } You should specify the insights you have chosen ( beginning with [ Chosen Insights ]) , followed by your final plan ( beginning with[ Plan ]) .
Random forests. Leo Breiman, Machine learning. 452001</p>
<p>Can we rely on llm agents to draft long-horizon plans? let's take travelplanner as an example. Yanan Chen, Ali Pesaranghader, Tanmana Sadhu, Dong Hoon, Yi , arXiv:2408.063182024arXiv preprint</p>
<p>Dynamic planning with a llm. Frank Gautier Dagan, Alex Keller, Lascarides, arXiv:2308.063912023arXiv preprint</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>All models are wrong, but many are useful: Learning a variable's importance by studying an entire class of prediction models simultaneously. Aaron Fisher, Cynthia Rudin, Francesca Dominici, Machine Learning Research. 201772019</p>
<p>Autoguide: Automated generation and selection of state-aware guidelines for large language model agents. Yao Fu, Dong-Ki Kim, Jaekyeom Kim, Sungryull Sohn, Lajanugen Logeswaran, Kyunghoon Bae, Honglak Lee, arXiv:2403.089782024arXiv preprint</p>
<p>Rohan Gemini G Team, Sebastian Anil, Yonghui Borgeaud, Jean-Baptiste Wu, Jiahui Alayrac, Radu Yu, Johan Soricut, Andrew M Schalkwyk, Anja Dai, Hauth, arXiv:2312.11805Gemini: a family of highly capable multimodal models. 2023arXiv preprint</p>
<p>A confederacy of models: a comprehensive evaluation of llms on creative writing. Carlos Gómez, -Rodríguez , Paul Williams, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Yu Gu, Yiheng Shu, Hao Yu, Xiao Liu, Yuxiao Dong, Jie Tang, Jayanth Srinivasa, Hugo Latapie, Yu Su, arXiv:2402.14672Middleware for llms: Tools are instrumental for language agents in complex environments. 2024arXiv preprint</p>
<p>Robust planning with llm-modulo framework: Case study in travel planning. Atharva Gundawar, Mudit Verma, Lin Guan, Karthik Valmeekam, Siddhant Bhambri, Subbarao Kambhampati, arXiv:2405.206252024arXiv preprint</p>
<p>Thinking, fast and slow. Farrar. Daniel Kahneman, 2011Straus and Giroux</p>
<p>Position: Llms can't plan, but can help planning in llm-modulo frameworks. Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Mudit Verma, Kaya Stechly, Siddhant Bhambri, Lucas Paul Saldyt, Anil B Murthy, Forty-first International Conference on Machine Learning. 2024</p>
<p>Visit: Visualizing and interpreting the semantic information flow of transformers. Shahar Katz, Yonatan Belinkov, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, Peter Stone, arXiv:2304.11477Llm+p: Empowering large language models with optimal planning proficiency. 2023arXiv preprint</p>
<p>Lost in the middle: How language models use long contexts. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, 10.1162/tacl_a_00638Transactions of the Association for Computational Linguistics. 122024</p>
<p>MUFFIN: Curating multi-faceted instructions for improving instruction following. Renze Lou, Kai Zhang, Jian Xie, Yuxuan Sun, Janice Ahn, Hanzi Xu, Yu Su, Wenpeng Yin, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Chameleon: Plug-and-play compositional reasoning with large language models. Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>From understanding to utilization: A survey on explainability for large language models. Haoyan Luo, Lucia Specia, arXiv:2401.128742024arXiv preprint</p>
<p>Clin: A continually learning language agent for rapid task adaptation and generalization. Prasad Bodhisattwa, Bhavana Majumder, Peter Dalvi Mishra, Oyvind Jansen, Niket Tafjord, Li Tandon, Chris Zhang, Peter Callison-Burch, Clark, arXiv:2310.101342023arXiv preprint</p>
<p>Robot planning. AI magazine. Pramod Kaushik Mudrakarta, Ankur Taly, Mukund Sundararajan, and Kedar Dhamdhere. Drew Mcdermott, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational Linguistics1992. 20181Did the model understand the question?</p>
<p>Report on a general problem solving program. Allen Newell, John C Shaw, Herbert A Simon, IFIP congress. Pittsburgh, PA195925664</p>
<p>arXiv:2303.08774OpenAI. 2023a. Gpt-4 technical report. arXiv preprint</p>
<p>Planning for agi and beyond. 2023b. 2024OpenAIOpenai o1</p>
<p>why should i trust you?" explaining the predictions of any classifier. Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin, Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. the 22nd ACM SIGKDD international conference on knowledge discovery and data mining2016</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202436</p>
<p>Mastering the game of go without human knowledge. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, nature. 55076762017</p>
<p>Trial and error: Exploration-based trajectory optimization of LLM agents. Yifan Song, Xiang Da Yin, Jie Yue, Sujian Huang, Bill Li, Lin Yuchen, 10.18653/v1/2024.acl-long.409Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024</p>
<p>On the self-verification limitations of large language models on reasoning and planning tasks. Kaya Stechly, Karthik Valmeekam, Subbarao Kambhampati, arXiv:2402.081152024arXiv preprint</p>
<p>Language agents: a critical evolutionary step of artificial intelligence. Yu Su, 2023</p>
<p>Cognitive architectures for language agents. Theodore Sumers, Shunyu Yao, Karthik Narasimhan, Thomas Griffiths, Transactions on Machine Learning Research. 2024</p>
<p>Axiomatic attribution for deep networks. Mukund Sundararajan, Ankur Taly, Qiqi Yan, International conference on machine learning. PMLR2017</p>
<p>Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, Advances in Neural Information Processing Systems. 2024a36</p>
<p>Llms still can't plan; can lrms? a preliminary evaluation of openai's o1 on planbench. Karthik Valmeekam, Kaya Stechly, Subbarao Kambhampati, arXiv:2409.133732024barXiv preprint</p>
<p>Llm-powered autonomous agents. Lilian Weng, 2023</p>
<p>Symbolic knowledge distillation: from general language models to commonsense models. Peter West, Chandra Bhagavatula, Jack Hessel, Jena Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, Yejin Choi, 10.18653/v1/2022.naacl-main.341Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesUnited StatesAssociation for Computational Linguistics2022Seattle</p>
<p>How easily do irrelevant inputs skew the responses of large language models?. Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, Yanghua Xiao, First Conference on Language Modeling. 2024</p>
<p>Adaptive chameleon or stubborn sloth: Revealing the behavior of large language models in knowledge conflicts. Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, Yu Su, The Twelfth International Conference on Learning Representations. 2024a</p>
<p>Travelplanner: A benchmark for real-world planning with language agents. Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, Yu Su, Forty-first International Conference on Machine Learning. 2024b</p>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, arXiv:2407.10671Qwen2 technical report. 2024arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik R Narasimhan, 2023In Proceedings of NeurIPS</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Yuan Karthik R Narasimhan, Cao, Proceedings of ICLR. ICLR2022</p>
<p>Agent lumos: Unified and modular training for open-source language agents. Faeze Da Yin, Abhilasha Brahman, Khyathi Ravichander, Kai-Wei Chandu, Yejin Chang, Bill Choi, Lin Yuchen, 10.18653/v1/2024.acl-long.670Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Agenttuning: Enabling generalized agent abilities for llms. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, Jie Tang, arXiv:2310.128232023aPreprint</p>
<p>Large language models for robotics: A survey. Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S Yu, arXiv:2311.072262023barXiv preprint</p>
<p>Aligning instruction tasks unlocks large language models as zero-shot relation extractors. Kai Zhang, Bernal Jiménez Gutiérrez, Yu Su, Findings of ACL. 2023</p>
<p>TimeArena: Shaping efficient multitasking language agents in a time-aware simulation. Yikai Zhang, Siyu Yuan, Caiyu Hu, Kyle Richardson, Yanghua Xiao, Jiangjie Chen, 10.18653/v1/2024.acl-long.215Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Expel: Llm agents are experiential learners. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, Gao Huang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Gpt-4v (ision) is a generalist web agent, if grounded. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, Yu Su, Forty-first International Conference on Machine Learning. 2024a</p>
<p>Zifan Zheng, Yezhaohui Wang, Yuxin Huang, Shichao Song, Bo Tang, Feiyu Xiong, Zhiyu Li, arXiv:2409.03752Attention heads of large language models: A survey. 2024barXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1286 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1286</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1286</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-802168a81571dde28f5ddb94d84677bc007afa7b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/802168a81571dde28f5ddb94d84677bc007afa7b" target="_blank">Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work proposes an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates.</p>
                <p><strong>Paper Abstract:</strong> Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1286.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1286.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deep Ensembles</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Ensembles for Predictive Uncertainty Estimation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble-based, non-Bayesian method that trains multiple probabilistic neural networks (each predicting a predictive distribution) using proper scoring rules, optionally augmented with adversarial training, and combines them by averaging predictive distributions to produce calibrated uncertainty estimates and robustness to out-of-distribution inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Deep Ensembles (ML-5 / ML-M variants)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An ensemble of M independently-initialized neural networks; each network parametrizes a predictive distribution p_theta(y|x) (for regression outputs mean and variance, for classification softmax probabilities). Training uses a proper scoring rule (negative log-likelihood), optional adversarial-training augmentation, and predictions are combined by averaging predictive distributions (mixture approximation with mean/variance computed). Default ensemble size M=5 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>none (not an adaptive experimental-design agent); uses ensemble averaging + adversarial data augmentation to improve robustness to unknown inputs</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>No sequential/adaptive experimental design is used. Adaptation in this method consists of (i) averaging predictions across independently trained networks to capture model uncertainty, and (ii) adversarial training (fast gradient sign perturbations) to smooth local predictive distributions around training points; these mechanisms improve uncertainty estimates under domain shift but do not adapt data collection or action selection online.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Standard supervised benchmark datasets and out-of-distribution test sets (MNIST, NotMNIST, SVHN, CIFAR-10 (as OOD), ImageNet (dog/non-dog split), multiple regression benchmarks including Kin8nm, Naval, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Supervised classification/regression datasets; OOD/unknown-class tests are distribution shift scenarios (unknown classes not seen during training). Environments are static datasets (not sequential decision processes); inputs are high-dimensional images for vision tasks and tabular features for regression. Not described as partially-observable or stochastic transition environments.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Varies by benchmark: low-dimensional (1D toy regression), medium tabular regression datasets, and high-dimensional image datasets (MNIST 28x28 grayscale, SVHN/CIFAR-10 32x32x3, ImageNet large-scale multi-class). The paper does not specify RL-style state/action dimensions or episode lengths because tasks are supervised.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Quantitative improvements in uncertainty metrics and often in accuracy compared to baselines. Examples from the paper: regression NLL improvements — Kin8nm: Deep Ensembles NLL = -1.20 ± 0.02 vs MC-dropout -0.95 ± 0.03 (and PBP -0.90 ± 0.01); Naval propulsion plant NLL = -5.63 ± 0.05 vs MC-dropout -3.80 ± 0.05. Classification: ensembles yield lower NLL, Brier score and higher accuracy on MNIST, SVHN and ImageNet as ensemble size M increases (figures and Table summaries in paper). Deep ensembles also show higher predictive entropy on out-of-distribution (unknown) classes (MNIST->NotMNIST, SVHN->CIFAR-10, ImageNet dog/non-dog) compared to MC-dropout, indicating better recognition of unknowns.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines reported include MC-dropout and probabilistic backpropagation (PBP). Example comparisons: Kin8nm NLL: MC-dropout -0.95 ±0.03, PBP -0.90 ±0.01; Naval: MC-dropout -3.80 ±0.05, PBP -3.73 ±0.01. On classification tasks MC-dropout exhibits higher overconfident errors on OOD examples relative to deep ensembles (qualitative and histogram comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not applicable / not reported in sequential/adaptive sampling terms; experiments are standard supervised training on fixed datasets (no online sample-efficiency curves reported).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Not applicable — method does not perform sequential exploration/exploitation; adversarial training perturbs inputs deterministically for smoothing, ensembles capture model uncertainty but there is no explicit exploration policy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>MC-dropout (Gal & Ghahramani), Probabilistic Backpropagation (PBP), ensembles trained with MSE (empirical-variance baseline), variational inference methods referenced, and other calibration baselines (Brier score, NLL metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>1) Ensembles of probabilistic NNs trained with proper scoring rules produce well-calibrated predictive uncertainties and outperform or match MC-dropout and PBP on NLL across many regression datasets; 2) learning predictive variance (training for NLL) is superior to using empirical variance of point-prediction ensembles; 3) adversarial training (fast gradient sign perturbations) smooths predictive distributions and can improve calibration and robustness to OOD for some tasks; 4) ensemble size M improves uncertainty quality and OOD entropy (deep ensembles increase predictive entropy on unseen classes faster than MC-dropout); 5) deep ensembles produce fewer overconfident wrong predictions (accuracy vs confidence curves demonstrate robustness).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Noted limitations include increased memory footprint (ensemble has M times parameters), potential redundancy without explicit de-correlation of members (authors suggest future work to encourage diversity), adversarial training does not always help (benefit depends on dataset and ensemble size), bagging hurt performance in experiments, and method is not an explicit Bayesian posterior approximation (so theoretical Bayesian guarantees do not apply). The approach does not perform adaptive experimental design or online exploration in partially observable environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1286.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1286.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bootstrapped DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep exploration via bootstrapped DQN</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced RL method that uses bootstrapped ensembles of Q-networks to induce deep (Thompson-sampling-like) exploration in reinforcement learning; cited in this paper as an example of implicit ensembles/multi-head architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep exploration via bootstrapped DQN</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bootstrapped DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Only cited in discussion as related work / an example of ensemble-based architectures (multiple heads) for exploration; no implementation details or experiment specifics are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Mentioned only as an example of implicit ensembles / multi-head methods that can be used for exploration (cited for future/related work); no results or details are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not discussed in this paper (only cited).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1286.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1286.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adaptive Mixtures of Experts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive mixtures of local experts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classic method (cited) for adaptively combining specialized expert models via gating networks; referenced as a possible direction to optimize ensemble weights or adaptively combine experts to improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Adaptive mixtures of local experts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Adaptive mixtures of experts (Jacobs et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Referenced in discussion as an example of optimizing ensemble weights or adaptive combination strategies (stacking / adaptive mixture of experts) that could improve ensemble performance; the paper itself does not implement or evaluate this method.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Only suggested as a potential avenue to improve ensembles (optimizing weights or adaptive mixtures), but not used or evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not discussed in this paper (only cited as future work possibility).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1286.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1286.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BO via Robust BNNs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian optimization with robust Bayesian neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work combining robust Bayesian neural networks with Bayesian optimization; cited in references but not used or evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bayesian optimization with robust Bayesian neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bayesian optimization with robust BNNs</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Only appears in the references; the current paper does not describe or evaluate this approach.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>No content in this paper; cited in references as related work connecting Bayesian neural networks and Bayesian optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep exploration via bootstrapped DQN <em>(Rating: 2)</em></li>
                <li>Adaptive mixtures of local experts <em>(Rating: 2)</em></li>
                <li>Bayesian optimization with robust Bayesian neural networks <em>(Rating: 2)</em></li>
                <li>Distributional smoothing by virtual adversarial examples <em>(Rating: 1)</em></li>
                <li>Ensemble adversarial training: Attacks and defenses <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1286",
    "paper_id": "paper-802168a81571dde28f5ddb94d84677bc007afa7b",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "Deep Ensembles",
            "name_full": "Deep Ensembles for Predictive Uncertainty Estimation",
            "brief_description": "An ensemble-based, non-Bayesian method that trains multiple probabilistic neural networks (each predicting a predictive distribution) using proper scoring rules, optionally augmented with adversarial training, and combines them by averaging predictive distributions to produce calibrated uncertainty estimates and robustness to out-of-distribution inputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Deep Ensembles (ML-5 / ML-M variants)",
            "agent_description": "An ensemble of M independently-initialized neural networks; each network parametrizes a predictive distribution p_theta(y|x) (for regression outputs mean and variance, for classification softmax probabilities). Training uses a proper scoring rule (negative log-likelihood), optional adversarial-training augmentation, and predictions are combined by averaging predictive distributions (mixture approximation with mean/variance computed). Default ensemble size M=5 in experiments.",
            "adaptive_design_method": "none (not an adaptive experimental-design agent); uses ensemble averaging + adversarial data augmentation to improve robustness to unknown inputs",
            "adaptation_strategy_description": "No sequential/adaptive experimental design is used. Adaptation in this method consists of (i) averaging predictions across independently trained networks to capture model uncertainty, and (ii) adversarial training (fast gradient sign perturbations) to smooth local predictive distributions around training points; these mechanisms improve uncertainty estimates under domain shift but do not adapt data collection or action selection online.",
            "environment_name": "Standard supervised benchmark datasets and out-of-distribution test sets (MNIST, NotMNIST, SVHN, CIFAR-10 (as OOD), ImageNet (dog/non-dog split), multiple regression benchmarks including Kin8nm, Naval, etc.)",
            "environment_characteristics": "Supervised classification/regression datasets; OOD/unknown-class tests are distribution shift scenarios (unknown classes not seen during training). Environments are static datasets (not sequential decision processes); inputs are high-dimensional images for vision tasks and tabular features for regression. Not described as partially-observable or stochastic transition environments.",
            "environment_complexity": "Varies by benchmark: low-dimensional (1D toy regression), medium tabular regression datasets, and high-dimensional image datasets (MNIST 28x28 grayscale, SVHN/CIFAR-10 32x32x3, ImageNet large-scale multi-class). The paper does not specify RL-style state/action dimensions or episode lengths because tasks are supervised.",
            "uses_adaptive_design": false,
            "performance_with_adaptation": "Quantitative improvements in uncertainty metrics and often in accuracy compared to baselines. Examples from the paper: regression NLL improvements — Kin8nm: Deep Ensembles NLL = -1.20 ± 0.02 vs MC-dropout -0.95 ± 0.03 (and PBP -0.90 ± 0.01); Naval propulsion plant NLL = -5.63 ± 0.05 vs MC-dropout -3.80 ± 0.05. Classification: ensembles yield lower NLL, Brier score and higher accuracy on MNIST, SVHN and ImageNet as ensemble size M increases (figures and Table summaries in paper). Deep ensembles also show higher predictive entropy on out-of-distribution (unknown) classes (MNIST-&gt;NotMNIST, SVHN-&gt;CIFAR-10, ImageNet dog/non-dog) compared to MC-dropout, indicating better recognition of unknowns.",
            "performance_without_adaptation": "Baselines reported include MC-dropout and probabilistic backpropagation (PBP). Example comparisons: Kin8nm NLL: MC-dropout -0.95 ±0.03, PBP -0.90 ±0.01; Naval: MC-dropout -3.80 ±0.05, PBP -3.73 ±0.01. On classification tasks MC-dropout exhibits higher overconfident errors on OOD examples relative to deep ensembles (qualitative and histogram comparisons).",
            "sample_efficiency": "Not applicable / not reported in sequential/adaptive sampling terms; experiments are standard supervised training on fixed datasets (no online sample-efficiency curves reported).",
            "exploration_exploitation_tradeoff": "Not applicable — method does not perform sequential exploration/exploitation; adversarial training perturbs inputs deterministically for smoothing, ensembles capture model uncertainty but there is no explicit exploration policy.",
            "comparison_methods": "MC-dropout (Gal & Ghahramani), Probabilistic Backpropagation (PBP), ensembles trained with MSE (empirical-variance baseline), variational inference methods referenced, and other calibration baselines (Brier score, NLL metrics).",
            "key_results": "1) Ensembles of probabilistic NNs trained with proper scoring rules produce well-calibrated predictive uncertainties and outperform or match MC-dropout and PBP on NLL across many regression datasets; 2) learning predictive variance (training for NLL) is superior to using empirical variance of point-prediction ensembles; 3) adversarial training (fast gradient sign perturbations) smooths predictive distributions and can improve calibration and robustness to OOD for some tasks; 4) ensemble size M improves uncertainty quality and OOD entropy (deep ensembles increase predictive entropy on unseen classes faster than MC-dropout); 5) deep ensembles produce fewer overconfident wrong predictions (accuracy vs confidence curves demonstrate robustness).",
            "limitations_or_failures": "Noted limitations include increased memory footprint (ensemble has M times parameters), potential redundancy without explicit de-correlation of members (authors suggest future work to encourage diversity), adversarial training does not always help (benefit depends on dataset and ensemble size), bagging hurt performance in experiments, and method is not an explicit Bayesian posterior approximation (so theoretical Bayesian guarantees do not apply). The approach does not perform adaptive experimental design or online exploration in partially observable environments.",
            "uuid": "e1286.0",
            "source_info": {
                "paper_title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "Bootstrapped DQN",
            "name_full": "Deep exploration via bootstrapped DQN",
            "brief_description": "A referenced RL method that uses bootstrapped ensembles of Q-networks to induce deep (Thompson-sampling-like) exploration in reinforcement learning; cited in this paper as an example of implicit ensembles/multi-head architectures.",
            "citation_title": "Deep exploration via bootstrapped DQN",
            "mention_or_use": "mention",
            "agent_name": "Bootstrapped DQN",
            "agent_description": "Only cited in discussion as related work / an example of ensemble-based architectures (multiple heads) for exploration; no implementation details or experiment specifics are provided in this paper.",
            "adaptive_design_method": null,
            "adaptation_strategy_description": null,
            "environment_name": null,
            "environment_characteristics": null,
            "environment_complexity": null,
            "uses_adaptive_design": null,
            "performance_with_adaptation": null,
            "performance_without_adaptation": null,
            "sample_efficiency": null,
            "exploration_exploitation_tradeoff": null,
            "comparison_methods": null,
            "key_results": "Mentioned only as an example of implicit ensembles / multi-head methods that can be used for exploration (cited for future/related work); no results or details are provided in this paper.",
            "limitations_or_failures": "Not discussed in this paper (only cited).",
            "uuid": "e1286.1",
            "source_info": {
                "paper_title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "Adaptive Mixtures of Experts",
            "name_full": "Adaptive mixtures of local experts",
            "brief_description": "A classic method (cited) for adaptively combining specialized expert models via gating networks; referenced as a possible direction to optimize ensemble weights or adaptively combine experts to improve performance.",
            "citation_title": "Adaptive mixtures of local experts",
            "mention_or_use": "mention",
            "agent_name": "Adaptive mixtures of experts (Jacobs et al.)",
            "agent_description": "Referenced in discussion as an example of optimizing ensemble weights or adaptive combination strategies (stacking / adaptive mixture of experts) that could improve ensemble performance; the paper itself does not implement or evaluate this method.",
            "adaptive_design_method": null,
            "adaptation_strategy_description": null,
            "environment_name": null,
            "environment_characteristics": null,
            "environment_complexity": null,
            "uses_adaptive_design": null,
            "performance_with_adaptation": null,
            "performance_without_adaptation": null,
            "sample_efficiency": null,
            "exploration_exploitation_tradeoff": null,
            "comparison_methods": null,
            "key_results": "Only suggested as a potential avenue to improve ensembles (optimizing weights or adaptive mixtures), but not used or evaluated in this paper.",
            "limitations_or_failures": "Not discussed in this paper (only cited as future work possibility).",
            "uuid": "e1286.2",
            "source_info": {
                "paper_title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "BO via Robust BNNs",
            "name_full": "Bayesian optimization with robust Bayesian neural networks",
            "brief_description": "A referenced work combining robust Bayesian neural networks with Bayesian optimization; cited in references but not used or evaluated in this paper.",
            "citation_title": "Bayesian optimization with robust Bayesian neural networks",
            "mention_or_use": "mention",
            "agent_name": "Bayesian optimization with robust BNNs",
            "agent_description": "Only appears in the references; the current paper does not describe or evaluate this approach.",
            "adaptive_design_method": null,
            "adaptation_strategy_description": null,
            "environment_name": null,
            "environment_characteristics": null,
            "environment_complexity": null,
            "uses_adaptive_design": null,
            "performance_with_adaptation": null,
            "performance_without_adaptation": null,
            "sample_efficiency": null,
            "exploration_exploitation_tradeoff": null,
            "comparison_methods": null,
            "key_results": "No content in this paper; cited in references as related work connecting Bayesian neural networks and Bayesian optimization.",
            "limitations_or_failures": "Not discussed in this paper.",
            "uuid": "e1286.3",
            "source_info": {
                "paper_title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles",
                "publication_date_yy_mm": "2016-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep exploration via bootstrapped DQN",
            "rating": 2
        },
        {
            "paper_title": "Adaptive mixtures of local experts",
            "rating": 2
        },
        {
            "paper_title": "Bayesian optimization with robust Bayesian neural networks",
            "rating": 2
        },
        {
            "paper_title": "Distributional smoothing by virtual adversarial examples",
            "rating": 1
        },
        {
            "paper_title": "Ensemble adversarial training: Attacks and defenses",
            "rating": 1
        }
    ],
    "cost": 0.0145365,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles</h1>
<p>Balaji Lakshminarayanan Alexander Pritzel Charles Blundell DeepMind<br>{balajiln,apritzel, cblundell}@google.com</p>
<h4>Abstract</h4>
<p>Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.</p>
<h2>1 Introduction</h2>
<p>Deep neural networks (NNs) have achieved state-of-the-art performance on a wide variety of machine learning tasks [35] and are becoming increasingly popular in domains such as computer vision [32], speech recognition [25], natural language processing [42], and bioinformatics [2, 61]. Despite impressive accuracies in supervised learning benchmarks, NNs are poor at quantifying predictive uncertainty, and tend to produce overconfident predictions. Overconfident incorrect predictions can be harmful or offensive [3], hence proper uncertainty quantification is crucial for practical applications.</p>
<p>Evaluating the quality of predictive uncertainties is challenging as the 'ground truth' uncertainty estimates are usually not available. In this work, we shall focus upon two evaluation measures that are motivated by practical applications of NNs. Firstly, we shall examine calibration [12, 13], a frequentist notion of uncertainty which measures the discrepancy between subjective forecasts and (empirical) long-run frequencies. The quality of calibration can be measured by proper scoring rules [17] such as log predictive probabilities and the Brier score [9]. Note that calibration is an orthogonal concern to accuracy: a network's predictions may be accurate and yet miscalibrated, and vice versa. The second notion of quality of predictive uncertainty we consider concerns generalization of the predictive uncertainty to domain shift (also referred to as out-of-distribution examples [23]), that is, measuring if the network knows what it knows. For example, if a network trained on one dataset is evaluated on a completely different dataset, then the network should output high predictive uncertainty as inputs from a different dataset would be far away from the training data. Well-calibrated predictions that are robust to model misspecification and dataset shift have a number of important practical uses (e.g., weather forecasting, medical diagnosis).</p>
<p>There has been a lot of recent interest in adapting NNs to encompass uncertainty and probabilistic methods. The majority of this work revolves around a Bayesian formalism [4], whereby a prior distribution is specified upon the parameters of a NN and then, given the training data, the posterior distribution over the parameters is computed, which is used to quantify predictive uncertainty. Since exact Bayesian inference is computationally intractable for NNs, a variety of approximations have been developed including Laplace approximation [40], Markov chain Monte Carlo (MCMC) methods [46], as well as recent work on variational Bayesian methods [6, 19, 39], assumed density filtering [24], expectation propagation [21, 38] and stochastic gradient MCMC variants such as Langevin diffusion methods [30, 59] and Hamiltonian methods [53]. The quality of predictive uncertainty obtained using Bayesian NNs crucially depends on (i) the degree of approximation due to computational constraints and (ii) if the prior distribution is 'correct', as priors of convenience can lead to unreasonable predictive uncertainties [50]. In practice, Bayesian NNs are often harder to implement and computationally slower to train compared to non-Bayesian NNs, which raises the need for a 'general purpose solution' that can deliver high-quality uncertainty estimates and yet requires only minor modifications to the standard training pipeline.
Recently, Gal and Ghahramani [15] proposed using Monte Carlo dropout (MC-dropout) to estimate predictive uncertainty by using Dropout [54] at test time. There has been work on approximate Bayesian interpretation [15, 29, 41] of dropout. MC-dropout is relatively simple to implement leading to its popularity in practice. Interestingly, dropout may also be interpreted as ensemble model combination [54] where the predictions are averaged over an ensemble of NNs (with parameter sharing). The ensemble interpretation seems more plausible particularly in the scenario where the dropout rates are not tuned based on the training data, since any sensible approximation to the true Bayesian posterior distribution has to depend on the training data. This interpretation motivates the investigation of ensembles as an alternative solution for estimating predictive uncertainty.
It has long been observed that ensembles of models improve predictive performance (see [14] for a review). However it is not obvious when and why an ensemble of NNs can be expected to produce good uncertainty estimates. Bayesian model averaging (BMA) assumes that the true model lies within the hypothesis class of the prior, and performs soft model selection to find the single best model within the hypothesis class [43]. In contrast, ensembles perform model combination, i.e. they combine the models to obtain a more powerful model; ensembles can be expected to be better when the true model does not lie within the hypothesis class. We refer to [11, 43] and [34, $\S 2.5]$ for related discussions. It is important to note that even exact BMA is not guaranteed be robust to mis-specification with respect to domain shift.
Summary of contributions: Our contribution in this paper is two fold. First, we describe a simple and scalable method for estimating predictive uncertainty estimates from NNs. We argue for training probabilistic NNs (that model predictive distributions) using a proper scoring rule as the training criteria. We additionally investigate the effect of two modifications to the training pipeline, namely (i) ensembles and (ii) adversarial training [18] and describe how they can produce smooth predictive estimates. Secondly, we propose a series of tasks for evaluating the quality of the predictive uncertainty estimates, in terms of calibration and generalization to unknown classes in supervised learning problems. We show that our method significantly outperforms (or matches) MC-dropout. These tasks, along with our simple yet strong baseline, serve as an useful benchmark for comparing predictive uncertainty estimates obtained using different Bayesian/non-Bayesian/hybrid methods.
Novelty and Significance: Ensembles of NNs, or deep ensembles for short, have been successfully used to boost predictive performance (e.g. classification accuracy in ImageNet or Kaggle contests) and adversarial training has been used to improve robustness to adversarial examples. However, to the best of our knowledge, ours is the first work to investigate their usefulness for predictive uncertainty estimation and compare their performance to current state-of-the-art approximate Bayesian methods on a series of classification and regression benchmark datasets. Compared to Bayesian NNs (e.g. variational inference or MCMC methods), our method is much simpler to implement, requires surprisingly few modifications to standard NNs, and well suited for distributed computation, thereby making it attractive for large-scale deep learning applications. To demonstrate scalability of our method, we evaluate predictive uncertainty on ImageNet (and are the first to do so, to the best of our knowledge). Most work on uncertainty in deep learning focuses on Bayesian deep learning; we hope that the simplicity and strong empirical performance of our approach will spark more interest in non-Bayesian approaches for predictive uncertainty estimation.</p>
<h1>2 Deep Ensembles: A Simple Recipe For Predictive Uncertainty Estimation</h1>
<h3>2.1 Problem setup and High-level summary</h3>
<p>We assume that the training dataset $\mathcal{D}$ consists of $N$ i.i.d. data points $\mathcal{D}=\left{\mathbf{x}<em n="n">{n}, y</em>\right}<em _theta="\theta">{n=1}^{N}$, where $\mathbf{x} \in \mathbb{R}^{D}$ represents the $D$-dimensional features. For classification problems, the label is assumed to be one of $K$ classes, that is $y \in{1, \ldots, K}$. For regression problems, the label is assumed to be real-valued, that is $y \in \mathbb{R}$. Given the input features $\mathbf{x}$, we use a neural network to model the probabilistic predictive distribution $p</em>)$ over the labels, where $\theta$ are the parameters of the NN.
We suggest a simple recipe: (1) use a proper scoring rule as the training criterion, (2) use adversarial training [18] to smooth the predictive distributions, and (3) train an ensemble. Let $M$ denote the number of NNs in the ensemble and $\left{\theta_{m}\right}_{m=1}^{M}$ denote the parameters of the ensemble. We first describe how to train a single neural net and then explain how to train an ensemble of NNs.}(y \mid \mathbf{x</p>
<h3>2.2 Proper scoring rules</h3>
<p>Scoring rules measure the quality of predictive uncertainty (see [17] for a review). A scoring rule assigns a numerical score to a predictive distribution $p_{\theta}(y \mid \mathbf{x})$, rewarding better calibrated predictions over worse. We shall consider scoring rules where a higher numerical score is better. Let a scoring rule be a function $S\left(p_{\theta},(y, \mathbf{x})\right)$ that evaluates the quality of the predictive distribution $p_{\theta}(y \mid \mathbf{x})$ relative to an event $y \mid \mathbf{x} \sim q(y \mid \mathbf{x})$ where $q(y, \mathbf{x})$ denotes the true distribution on $(y, \mathbf{x})$-tuples. The expected scoring rule is then $S\left(p_{\theta}, q\right)=\int q(y, \mathbf{x}) S\left(p_{\theta},(y, \mathbf{x})\right) d y d \mathbf{x}$. A proper scoring rule is one where $S\left(p_{\theta}, q\right) \leq S(q, q)$ with equality if and only if $p_{\theta}(y \mid \mathbf{x})=q(y \mid \mathbf{x})$, for all $p_{\theta}$ and $q$. NNs can then be trained according to measure that encourages calibration of predictive uncertainty by minimizing the $\operatorname{loss} \mathcal{L}(\theta)=-S\left(p_{\theta}, q\right)$.
It turns out many common NN loss functions are proper scoring rules. For example, when maximizing likelihood, the score function is $S\left(p_{\theta},(y, \mathbf{x})\right)=\log p_{\theta}(y \mid \mathbf{x})$, and this is a proper scoring rule due to Gibbs inequality: $S\left(p_{\theta}, q\right)=\mathbb{E}<em _theta="\theta">{q(\mathbf{x})} q(y \mid \mathbf{x}) \log p</em>}(y \mid \mathbf{x}) \leq \mathbb{E<em _theta="\theta">{q(\mathbf{x})} q(y \mid \mathbf{x}) \log q(y \mid \mathbf{x})$. In the case of multi-class $K$-way classification, the popular softmax cross entropy loss is equivalent to the log likelihood and is a proper scoring rule. Interestingly, $\mathcal{L}(\theta)=-S\left(p</em>$},(y, \mathbf{x})\right)=K^{-1} \sum_{k=1}^{K}\left(\delta_{k=y}-\right.$ $\left.p_{\theta}(y=k \mid \mathbf{x})\right)^{2}$, i.e., minimizing the squared error between the predictive probability of a label and one-hot encoding of the correct label, is also a proper scoring rule known as the Brier score [9]. This provides justification for this common trick for training NNs by minimizing the squared error between a binary label and its associated probability and shows it is, in fact, a well defined loss with desirable properties. ${ }^{1</p>
<h3>2.2.1 Training criterion for regression</h3>
<p>For regression problems, NNs usually output a single value say $\mu(\mathbf{x})$ and the parameters are optimized to minimize the mean squared error (MSE) on the training set, given by $\sum_{n=1}^{N}\left(y_{n}-\mu\left(\mathbf{x}_{n}\right)\right)^{2}$. However, the MSE does not capture predictive uncertainty. Following [47], we use a network that outputs two values in the final layer, corresponding to the predicted mean $\mu(\mathbf{x})$ and variance ${ }^{2}$ $\sigma^{2}(\mathbf{x})&gt;0$. By treating the observed value as a sample from a (heteroscedastic) Gaussian distribution with the predicted mean and variance, we minimize the negative log-likelihood criterion:</p>
<p>$$
-\log p_{\theta}\left(y_{n} \mid \mathbf{x}<em _theta="\theta">{n}\right)=\frac{\log \sigma</em>
$$}^{2}(\mathbf{x})}{2}+\frac{\left(y-\mu_{\theta}(\mathbf{x})\right)^{2}}{2 \sigma_{\theta}^{2}(\mathbf{x})}+\text { constant </p>
<p>We found the above to perform satisfactorily in our experiments. However, two simple extensions are worth further investigation: (i) Maximum likelihood estimation over $\mu_{\theta}(\mathbf{x})$ and $\sigma_{\theta}^{2}(\mathbf{x})$ might overfit; one could impose a prior and perform maximum-a-posteriori (MAP) estimation. (ii) In cases where the Gaussian is too-restrictive, one could use a complex distribution e.g. mixture density network [5] or a heavy-tailed distribution.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>2.3 Adversarial training to smooth predictive distributions</h1>
<p>Adversarial examples, proposed by Szegedy et al. [55] and extended by Goodfellow et al. [18], are those which are 'close' to the original training examples (e.g. an image that is visually indistinguishable from the original image to humans), but are misclassified by the NN. Goodfellow et al. [18] proposed the fast gradient sign method as a fast solution to generate adversarial examples. Given an input $\mathbf{x}$ with target $y$, and loss $\ell(\theta, \mathbf{x}, y)$ (e.g. $-\log p_{\theta}(y \mid \mathbf{x})$ ), the fast gradient sign method generates an adversarial example as $\mathbf{x}^{\prime}=\mathbf{x}+\epsilon \operatorname{sign}\left(\nabla_{\mathbf{x}} \ell(\theta, \mathbf{x}, y)\right)$, where $\epsilon$ is a small value such that the max-norm of the perturbation is bounded. Intuitively, the adversarial perturbation creates a new training example by adding a perturbation along a direction which the network is likely to increase the loss. Assuming $\epsilon$ is small enough, these adversarial examples can be used to augment the original training set by treating $\left(\mathbf{x}^{\prime}, y\right)$ as additional training examples. This procedure, referred to as adversarial training, ${ }^{3}$ was found to improve the classifier's robustness [18].</p>
<p>Interestingly, adversarial training can be interpreted as a computationally efficient solution to smooth the predictive distributions by increasing the likelihood of the target around an $\epsilon$-neighborhood of the observed training examples. Ideally one would want to smooth the predictive distributions along all $2^{D}$ directions in ${1,-1}^{D}$; however this is computationally expensive. A random direction might not necessarily increase the loss; however, adversarial training by definition computes the direction where the loss is high and hence is better than a random direction for smoothing predictive distributions. Miyato et al. [44] proposed a related idea called virtual adversarial training (VAT), where they picked $\Delta \mathbf{x}=\arg \max _{\Delta \mathbf{x}} \operatorname{KL}{p(y \mid \mathbf{x}) | p(y \mid \mathbf{x}+\Delta \mathbf{x})}$; the advantage of VAT is that it does not require knowledge of the true target $y$ and hence can be applied to semi-supervised learning. Miyato et al. [44] showed that distributional smoothing using VAT is beneficial for efficient semi-supervised learning; in contrast, we investigate the use of adversarial training for predictive uncertainty estimation. Hence, our contributions are complementary; one could use VAT or other forms of adversarial training, cf. [33], for improving predictive uncertainty in the semi-supervised setting as well.</p>
<h3>2.4 Ensembles: training and prediction</h3>
<p>The most popular ensembles use decision trees as the base learners and a wide variety of method have been explored in the literature on ensembles. Broadly, there are two classes of ensembles: randomization-based approaches such as random forests [8], where the ensemble members can be trained in parallel without any interaction, and boosting-based approaches where the ensemble members are fit sequentially. We focus only on the randomization based approach as it is better suited for distributed, parallel computation. Breiman [8] showed that the generalization error of random forests can be upper bounded by a function of the strength and correlation between individual trees; hence it is desirable to use a randomization scheme that de-correlates the predictions of the individual models as well as ensures that the individual models are strong (e.g. high accuracy). One of the popular strategies is bagging (a.k.a. bootstrapping), where ensemble members are trained on different bootstrap samples of the original training set. If the base learner lacks intrinsic randomization (e.g. it can be trained efficiently by solving a convex optimization problem), bagging is a good mechanism for inducing diversity. However, if the underlying base learner has multiple local optima, as is the case typically with NNs, the bootstrap can sometimes hurt performance since a base learner trained on a bootstrap sample sees only $63 \%$ unique data points. ${ }^{4}$ In the literature on decision tree ensembles, Breiman [8] proposed to use a combination of bagging [7] and random subset selection of features at each node. Geurts et al. [16] later showed that bagging is unnecessary if additional randomness can be injected into the random subset selection procedure. Intuitively, using more data for training the base learners helps reduce their bias and ensembling helps reduce the variance.</p>
<p>We used the entire training dataset to train each network since deep NNs typically perform better with more data, although it is straightforward to use a random subsample if need be. We found that random initialization of the NN parameters, along with random shuffling of the data points, was sufficient to obtain good performance in practice. We observed that bagging deteriorated performance in our experiments. Lee et al. [36] independently observed that training on entire dataset with random initialization was better than bagging for deep ensembles, however their goal was to improve</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>predictive accuracy and not predictive uncertainty. The overall training procedure is summarized in Algorithm 1.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Pseudocode of the training procedure for our method
    \(\triangleright\) Let each neural network parametrize a distribution over the outputs, i.e. \(p_{\theta}(y \mid \mathbf{x})\). Use a proper
    scoring rule as the training criterion \(\ell(\theta, \mathbf{x}, y)\). Recommended default values are \(M=5\) and
    \(\epsilon=1 \%\) of the input range of the corresponding dimension (e.g 2.55 if input range is [0,255]).
    Initialize \(\theta_{1}, \theta_{2}, \ldots, \theta_{M}\) randomly
    for \(m=1: M\) do \(\quad \triangleright\) train networks independently in parallel
        Sample data point \(n_{m}\) randomly for each net \(\quad \triangleright\) single \(n_{m}\) for clarity, minibatch in practice
        Generate adversarial example using \(\mathbf{x}_{n_{m}}^{\prime}=\mathbf{x}_{n_{m}}+\boldsymbol{\epsilon} \operatorname{sign}\left(\nabla_{\mathbf{x}_{n_{m}}} \ell\left(\theta_{m}, \mathbf{x}_{n_{m}}, y_{n_{m}}\right)\right)\)
        Minimize \(\ell\left(\theta_{m}, \mathbf{x}_{n_{m}}, y_{n_{m}}\right)+\ell\left(\theta_{m}, \mathbf{x}_{n_{m}}^{\prime}, y_{n_{m}}\right)\) w.r.t. \(\theta_{m} \quad \triangleright\) adversarial training (optional)
</code></pre></div>

<p>We treat the ensemble as a uniformly-weighted mixture model and combine the predictions as $p(y \mid \mathbf{x})=M^{-1} \sum_{m=1}^{M} p_{\theta_{m}}\left(y \mid \mathbf{x}, \theta_{m}\right)$. For classification, this corresponds to averaging the predicted probabilities. For regression, the prediction is a mixture of Gaussian distributions. For ease of computing quantiles and predictive probabilities, we further approximate the ensemble prediction as a Gaussian whose mean and variance are respectively the mean and variance of the mixture. The mean and variance of a mixture $M^{-1} \sum \mathcal{N}\left(\mu_{\theta_{m}}(\mathbf{x}), \sigma_{\theta_{m}}^{2}(\mathbf{x})\right)$ are given by $\mu_{<em>}(\mathbf{x})=M^{-1} \sum_{m} \mu_{\theta_{m}}(\mathbf{x})$ and $\sigma_{</em>}^{2}(\mathbf{x})=M^{-1} \sum_{m}\left(\sigma_{\theta_{m}}^{2}(\mathbf{x})+\mu_{\theta_{m}}^{2}(\mathbf{x})\right)-\mu_{*}^{2}(\mathbf{x})$ respectively.</p>
<h1>3 Experimental results</h1>
<h3>3.1 Evaluation metrics and experimental setup</h3>
<p>For both classification and regression, we evaluate the negative log likelihood (NLL) which depends on the predictive uncertainty. NLL is a proper scoring rule and a popular metric for evaluating predictive uncertainty [49]. For classification we additionally measure classification accuracy and the Brier score, defined as $B S=K^{-1} \sum_{k=1}^{K}\left(t_{k}^{<em>}-p\left(y=k \mid \mathbf{x}^{</em>}\right)\right)^{2}$ where $t_{k}^{<em>}=1$ if $k=y^{</em>}$, and 0 otherwise. For regression problems, we additionally measured the root mean squared error (RMSE). Unless otherwise specified, we used batch size of 100 and Adam optimizer with fixed learning rate of 0.1 in our experiments. We use the same technique for generating adversarial training examples for regression problems. Goodfellow et al. [18] used a fixed $\epsilon$ for all dimensions; this is unsatisfying if the input dimensions have different ranges. Hence, in all of our experiments, we set $\epsilon$ to 0.01 times the range of the training data along that particular dimension. We used the default weight initialization in Torch.</p>
<h3>3.2 Regression on toy datasets</h3>
<p>First, we qualitatively evaluate the performance of the proposed method on a one-dimensional toy regression dataset. This dataset was used by Hernández-Lobato and Adams [24], and consists of 20 training examples drawn as $y=x^{3}+\epsilon$ where $\epsilon \sim \mathcal{N}\left(0,3^{2}\right)$. We used the same architecture as [24].
A commonly used heuristic in practice is to use an ensemble of NNs (trained to minimize MSE), obtain multiple point predictions and use the empirical variance of the networks' predictions as an approximate measure of uncertainty. We demonstrate that this is inferior to learning the variance by training using NLL. ${ }^{3}$ The results are shown in Figure 1.
The results clearly demonstrate that (i) learning variance and training using a scoring rule (NLL) leads to improved predictive uncertainty and (ii) ensemble combination improves performance, especially as we move farther from the observed training data.</p>
<h3>3.3 Regression on real world datasets</h3>
<p>In our next experiment, we compare our method to state-of-the-art methods for predictive uncertainty estimation using NNs on regression tasks. We use the experimental setup proposed by HernándezLobato and Adams [24] for evaluating probabilistic backpropagation (PBP), which was also used</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Results on a toy regression task: x-axis denotes x. On the y-axis, the blue line is the ground truth curve, the red dots are observed noisy training data points and the gray lines correspond to the predicted mean along with three standard deviations. Left most plot corresponds to empirical variance of 5 networks trained using MSE, second plot shows the effect of training using NLL using a single net, third plot shows the additional effect of adversarial training, and final plot shows the effect of using an ensemble of 5 networks respectively.</p>
<p>by Gal and Ghahramani [15] to evaluate MC-dropout. Each dataset is split into 20 train-test folds, except for the protein dataset which uses 5 folds and the Year Prediction MSD dataset which uses a single train-test split. We use the identical network architecture: 1-hidden layer NN with ReLU nonlinearity [45], containing 50 hidden units for smaller datasets and 100 hidden units for the larger protein and Year Prediction MSD datasets. We trained for 40 epochs; we refer to [24] for further details about the datasets and the experimental protocol. We used 5 networks in our ensemble. Our results are shown in Table 1, along with the PBP and MC-dropout results reported in their respective papers.</p>
<table>
<thead>
<tr>
<th>Datasets</th>
<th>RMSE</th>
<th></th>
<th></th>
<th>NLL</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>PBP</td>
<td>MC-dropout</td>
<td>Deep Ensembles</td>
<td>PBP</td>
<td>MC-dropout</td>
<td>Deep Ensembles</td>
</tr>
<tr>
<td>Boston housing</td>
<td>3.01 ± 0.18</td>
<td>2.97 ± 0.85</td>
<td>3.28 ± 1.00</td>
<td>2.57 ± 0.09</td>
<td>2.46 ± 0.25</td>
<td>2.41 ± 0.25</td>
</tr>
<tr>
<td>Concrete</td>
<td>5.67 ± 0.09</td>
<td>5.23 ± 0.53</td>
<td>6.03 ± 0.58</td>
<td>3.16 ± 0.02</td>
<td>3.04 ± 0.09</td>
<td>3.06 ± 0.18</td>
</tr>
<tr>
<td>Energy</td>
<td>1.80 ± 0.05</td>
<td>1.66 ± 0.19</td>
<td>2.09 ± 0.29</td>
<td>2.04 ± 0.02</td>
<td>1.99 ± 0.09</td>
<td>1.38 ± 0.22</td>
</tr>
<tr>
<td>Kin8nm</td>
<td>0.10 ± 0.00</td>
<td>0.10 ± 0.00</td>
<td>0.09 ± 0.00</td>
<td>-0.90 ± 0.01</td>
<td>-0.95 ± 0.03</td>
<td>-1.20 ± 0.02</td>
</tr>
<tr>
<td>Naval propulsion plant</td>
<td>0.01 ± 0.00</td>
<td>0.01 ± 0.00</td>
<td>0.00 ± 0.00</td>
<td>-3.73 ± 0.01</td>
<td>-3.80 ± 0.05</td>
<td>-5.63 ± 0.05</td>
</tr>
<tr>
<td>Power plant</td>
<td>4.12 ± 0.03</td>
<td>4.02 ± 0.18</td>
<td>4.11 ± 0.17</td>
<td>2.84 ± 0.01</td>
<td>2.80 ± 0.05</td>
<td>2.79 ± 0.04</td>
</tr>
<tr>
<td>Protein</td>
<td>4.73 ± 0.01</td>
<td>4.36 ± 0.04</td>
<td>4.71 ± 0.06</td>
<td>2.97 ± 0.00</td>
<td>2.89 ± 0.01</td>
<td>2.83 ± 0.02</td>
</tr>
<tr>
<td>Wine</td>
<td>0.64 ± 0.01</td>
<td>0.62 ± 0.04</td>
<td>0.64 ± 0.04</td>
<td>0.97 ± 0.01</td>
<td>0.93 ± 0.06</td>
<td>0.94 ± 0.12</td>
</tr>
<tr>
<td>Yacht</td>
<td>1.02 ± 0.05</td>
<td>1.11 ± 0.38</td>
<td>1.58 ± 0.48</td>
<td>1.63 ± 0.02</td>
<td>1.55 ± 0.12</td>
<td>1.18 ± 0.21</td>
</tr>
<tr>
<td>Year Prediction MSD</td>
<td>8.88 ± NA</td>
<td>8.85 ± NA</td>
<td>8.89 ± NA</td>
<td>3.60 ± NA</td>
<td>3.59 ± NA</td>
<td>3.35 ± NA</td>
</tr>
</tbody>
</table>
<p>Table 1: Results on regression benchmark datasets comparing RMSE and NLL. See Table 2 for results on variants of our method.</p>
<p>We observe that our method outperforms (or is competitive with) existing methods in terms of NLL. On some datasets, we observe that our method is slightly worse in terms of RMSE. We believe that this is due to the fact that our method optimizes for NLL (which captures predictive uncertainty) instead of MSE. Table 2 in Appendix A.1 reports additional results on variants of our method, demonstrating the advantage of using an ensemble as well as learning variance.</p>
<h3>3.4 Classification on MNIST, SVHN and ImageNet</h3>
<p>Next we evaluate the performance on classification tasks using MNIST and SVHN datasets. Our goal is not to achieve the state-of-the-art performance on these problems, but rather to evaluate the effect of adversarial training as well as the number of networks in the ensemble. To verify if adversarial training helps, we also include a baseline which picks a random signed vector. For MNIST, we used an MLP with 3-hidden layers with 200 hidden units per layer and ReLU non-linearities with batch normalization. For MC-dropout, we added dropout after each non-linearity with 0.1 as the dropout rate. Results are shown in Figure 2(a). We observe that adversarial training and increasing the number of networks in the ensemble significantly improve performance in terms of both classification accuracy as well as NLL and Brier score, illustrating that our method produces well-calibrated uncertainty estimates. Adversarial training leads to better performance than augmenting with random direction. Our method also performs much better than MC-dropout in terms of all the performance measures. Note that augmenting the training dataset with invariances (such as random crop and horizontal flips) is complementary to adversarial training and can potentially improve performance.</p>
<p>^{6}We do not compare to VI [19] as PBP and MC-dropout outperform VI on these benchmarks.</p>
<p>^{7}We also tried dropout rate of 0.5, but that performed worse.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Evaluating predictive uncertainty as a function of ensemble size $M$ (number of networks in the ensemble or the number of MC-dropout samples): Ensemble variants significantly outperform MC-dropout performance with the corresponding $M$ in terms of all 3 metrics. Adversarial training improves results for MNIST for all $M$ and SVHN when $M=1$, but the effect drops as $M$ increases.</p>
<p>To measure the sensitivity of the results to the choice of network architecture, we experimented with a two-layer MLP as well as a convolutional NN; we observed qualitatively similar results; see Appendix B.1 in the supplementary material for details.
We also report results on the SVHN dataset using an VGG-style convolutional NN. ${ }^{8}$ The results are in Figure 2(b). Ensembles outperform MC dropout. Adversarial training helps slightly for $M=1$, however the effect drops as the number of networks in the ensemble increases. If the classes are well-separated, adversarial training might not change the classification boundary significantly. It is not clear if this is the case here, further investigation is required.
Finally, we evaluate on the ImageNet (ILSVRC-2012) dataset [51] using the inception network [56]. Due to computational constraints, we only evaluate the effect of ensembles on this dataset. The results on ImageNet (single-crop evaluation) are shown in Table 4. We observe that as $M$ increases, both the accuracy and the quality of predictive uncertainty improve significantly.
Another advantage of using an ensemble is that it enables us to easily identify training examples where the individual networks disagree or agree the most. This disagreement ${ }^{9}$ provides another useful qualitative way to evaluate predictive uncertainty. Figures 10 and 11 in Appendix B. 2 report qualitative evaluation of predictive uncertainty on the MNIST dataset.</p>
<h1>3.5 Uncertainty evaluation: test examples from known vs unknown classes</h1>
<p>In the final experiment, we evaluate uncertainty on out-of-distribution examples from unseen classes. Overconfident predictions on unseen classes pose a challenge for reliable deployment of deep learning models in real world applications. We would like the predictions to exhibit higher uncertainty when the test data is very different from the training data. To test if the proposed method possesses this desirable property, we train a MLP on the standard MNIST train/test split using the same architecture as before. However, in addition to the regular test set with known classes, we also evaluate it on a test set containing unknown classes. We used the test split of the NotMNIST ${ }^{10}$ dataset. The images in this dataset have the same size as MNIST, however the labels are alphabets instead of digits. We do not have access to the true conditional probabilities, but we expect the predictions to be closer to uniform on unseen classes compared to the known classes where the predictive probabilities should concentrate on the true targets. We evaluate the entropy of the predictive distribution and use this to evaluate the quality of the uncertainty estimates. The results are shown in Figure 3(a). For known classes (top row), both our method and MC-dropout have low entropy as expected. For unknown classes (bottom row), as $M$ increases, the entropy of deep ensembles increases much faster than MC-dropout indicating that our method is better suited for handling unseen test examples. In particular, MC-dropout seems to give high confidence predictions for some of the test examples, as evidenced by the mode around 0 even for unseen classes. Such overconfident wrong predictions can be problematic in practice when tested on a mixture of known and unknown classes, as we will see in Section 3.6. Comparing different variants of our method, the mode for adversarial training increases slightly faster than the mode for vanilla ensembles indicating that adversarial training is beneficial</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>for quantifying uncertainty on unseen classes. We qualitatively evaluate results in Figures 12(a) and 12(b) in Appendix B.2. Figure 12(a) shows that the ensemble agreement is highest for letter ‘$I$’ which resembles 1 in the MNIST training dataset, and that the ensemble disagreement is higher for examples visually different from the MNIST training dataset.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Histogram of the predictive entropy on test examples from known classes (top row) and unknown classes (bottom row), as we vary ensemble size $M$.</p>
<p>We ran a similar experiment, training on SVHN and testing on CIFAR-10 [31] test set; both datasets contain $32 \times 32 \times 3$ images, however SVHN contains images of digits whereas CIFAR-10 contains images of object categories. The results are shown in Figure 3(b). As in the MNIST-NotMNIST experiment, we observe that MC-dropout produces over-confident predictions on unseen examples, whereas our method produces higher uncertainty on unseen classes.</p>
<p>Finally, we test on ImageNet by splitting the training set by categories. We split the dataset into images of dogs (known classes) and non-dogs (unknown classes), following Vinyals et al. [58] who proposed this setup for a different task. Figure 5 shows the histogram of the predictive entropy as well as the maximum predicted probability (i.e. confidence in the predicted class). We observe that the predictive uncertainty improves on unseen classes, as the ensemble size increases.</p>
<h1>3.6 Accuracy as a function of confidence</h1>
<p>In practical applications, it is highly desirable for a system to avoid overconfident, incorrect predictions and fail gracefully. To evaluate the usefulness of predictive uncertainty for decision making, we consider a task where the model is evaluated only on cases where the model's confidence is above an user-specified threshold. If the confidence estimates are well-calibrated, one can trust the model's predictions when the reported confidence is high and resort to a different solution (e.g. use human in a loop, or use prediction from a simpler model) when the model is not confident.</p>
<p>We re-use the results from the experiment in the previous section where we trained a network on MNIST and test it on a mix of test examples from MNIST (known classes) and NotMNIST (unknown
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Results on ImageNet: Deep Ensembles lead to lower classification error as well as better predictive uncertainty as evidenced by lower NLL and Brier score.</p>
<p>Figure 5: ImageNet trained only on dogs: Histogram of the predictive entropy (left) and maximum predicted probability (right) on test examples from known classes (dogs) and unknown classes (non-dogs), as we vary the ensemble size.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Accuracy vs Confidence curves: Networks trained on MNIST and tested on both MNIST test containing known classes and the NotMNIST dataset containing unseen classes. MC-dropout can produce overconfident wrong predictions, whereas deep ensembles are significantly more robust.</p>
<p>classes). The network will produce incorrect predictions on out-of-distribution examples, however we would like these predictions to have low confidence. Given the prediction $p(y=k \mid \mathbf{x})$, we define the predicted label as $\hat{y}=\arg \max <em k="k">{k} p(y=k \mid \mathbf{x})$, and the confidence as $p(y=\hat{y} \mid \mathbf{x})=\max </em> \%$, we can trust the model only in cases where the confidence is greater than the corresponding threshold. Hence, we can compare accuracy of the models for a desired confidence threshold of the application. MC-dropout can produce overconfident wrong predictions as evidenced by low accuracy even for high values of $\tau$, whereas deep ensembles are significantly more robust.} p(y=k \mid \mathbf{x})$. We filter out test examples, corresponding to a particular confidence threshold $0 \leq \tau \leq 1$ and plot the accuracy for this threshold. The confidence vs accuracy results are shown in Figure 6. If we look at cases only where the confidence is $\geq 90 \%$, we expect higher accuracy than cases where confidence $\geq 80 \%$, hence the curve should be monotonically increasing. If the application demands an accuracy $\mathrm{x</p>
<h1>4 Discussion</h1>
<p>We have proposed a simple and scalable non-Bayesian solution that provides a very strong baseline on evaluation metrics for predictive uncertainty quantification. Intuitively, our method captures two sources of uncertainty. Training a probabilistic NN $p_{\theta}(y \mid \mathbf{x})$ using proper scoring rules as training objectives captures ambiguity in targets $y$ for a given $\mathbf{x}$. In addition, our method uses a combination of ensembles (which captures "model uncertainty" by averaging predictions over multiple models consistent with the training data), and adversarial training (which encourages local smoothness), for robustness to model misspecification and out-of-distribution examples. Ensembles, even for $M=5$, significantly improve uncertainty quality in all the cases. Adversarial training helps on some datasets for some metrics and is not strictly necessary in all cases. Our method requires very little hyperparameter tuning and is well suited for large scale distributed computation and can be readily implemented for a wide variety of architectures such as MLPs, CNNs, etc including those which do not use dropout e.g. residual networks [22]. It is perhaps surprising to the Bayesian deep learning community that a non-Bayesian (yet probabilistic) approach can perform as well as Bayesian NNs. We hope that our work will encourage the community to consider non-Bayesian approaches (such as ensembles) and other interesting evaluation metrics for predictive uncertainty. Concurrent with our work, Hendrycks and Gimpel [23] and Guo et al. [20] have also independently shown that non-Bayesian solutions can produce good predictive uncertainty estimates on some tasks. Abbasi and Gagné [1], Tramèr et al. [57] have also explored ensemble-based solutions to tackle adversarial examples, a particularly hard case of out-of-distribution examples.</p>
<p>There are several avenues for future work. We focused on training independent networks as training can be trivially parallelized. Explicitly de-correlating networks' predictions, e.g. as in [37], might promote ensemble diversity and improve performance even further. Optimizing the ensemble weights, as in stacking [60] or adaptive mixture of experts [28], can further improve the performance. The ensemble has $M$ times more parameters than a single network; for memory-constrained applications, the ensemble can be distilled into a simpler model [10, 26]. It would be also interesting to investigate so-called implicit ensembles the where ensemble members share parameters, e.g. using multiple heads [36, 48], snapshot ensembles [27] or swapout [52].</p>
<h1>Acknowledgments</h1>
<p>We would like to thank Samuel Ritter and Oriol Vinyals for help with ImageNet experiments, and Daan Wierstra, David Silver, David Barrett, Ian Osband, Martin Szummer, Peter Dayan, Shakir Mohamed, Theophane Weber, Ulrich Paquet and the anonymous reviewers for helpful feedback.</p>
<h2>References</h2>
<p>[1] M. Abbasi and C. Gagné. Robustness to adversarial examples through an ensemble of specialists. arXiv preprint arXiv:1702.06856, 2017.
[2] B. Alipanahi, A. Delong, M. T. Weirauch, and B. J. Frey. Predicting the sequence specificities of DNA-and RNA-binding proteins by deep learning. Nature biotechnology, 33(8):831-838, 2015.
[3] D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Mané. Concrete problems in AI safety. arXiv preprint arXiv:1606.06565, 2016.
[4] J. M. Bernardo and A. F. Smith. Bayesian Theory, volume 405. John Wiley \&amp; Sons, 2009.
[5] C. M. Bishop. Mixture density networks. 1994.
[6] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight uncertainty in neural networks. In ICML, 2015.
[7] L. Breiman. Bagging predictors. Machine learning, 24(2):123-140, 1996.
[8] L. Breiman. Random forests. Machine learning, 45(1):5-32, 2001.
[9] G. W. Brier. Verification of forecasts expressed in terms of probability. Monthly weather review, 1950.
[10] C. Bucila, R. Caruana, and A. Niculescu-Mizil. Model compression. In KDD. ACM, 2006.
[11] B. Clarke. Comparing Bayes model averaging and stacking when model approximation error cannot be ignored. J. Mach. Learn. Res. (JMLR), 4:683-712, 2003.
[12] A. P. Dawid. The well-calibrated Bayesian. Journal of the American Statistical Association, 1982.
[13] M. H. DeGroot and S. E. Fienberg. The comparison and evaluation of forecasters. The statistician, 1983.
[14] T. G. Dietterich. Ensemble methods in machine learning. In Multiple classifier systems. 2000.
[15] Y. Gal and Z. Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In ICML, 2016.
[16] P. Geurts, D. Ernst, and L. Wehenkel. Extremely randomized trees. Machine learning, 63(1): $3-42,2006$.
[17] T. Gneiting and A. E. Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association, 102(477):359-378, 2007.
[18] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In $I C L R, 2015$.
[19] A. Graves. Practical variational inference for neural networks. In NIPS, 2011.
[20] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On calibration of modern neural networks. arXiv preprint arXiv:1706.04599, 2017.
[21] L. Hasenclever, S. Webb, T. Lienart, S. Vollmer, B. Lakshminarayanan, C. Blundell, and Y. W. Teh. Distributed Bayesian learning with stochastic natural-gradient expectation propagation and the posterior server. arXiv preprint arXiv:1512.09327, 2015.</p>
<p>[22] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages $770-778,2016$.
[23] D. Hendrycks and K. Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016.
[24] J. M. Hernández-Lobato and R. P. Adams. Probabilistic backpropagation for scalable learning of Bayesian neural networks. In ICML, 2015.
[25] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. Signal Processing Magazine, IEEE, 29(6): 82-97, 2012.
[26] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.
[27] G. Huang, Y. Li, G. Pleiss, Z. Liu, J. E. Hopcroft, and K. Q. Weinberger. Snapshot ensembles: Train 1, get M for free. ICLR submission, 2017.
[28] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):79-87, 1991.
[29] D. P. Kingma, T. Salimans, and M. Welling. Variational dropout and the local reparameterization trick. In NIPS, 2015.
[30] A. Korattikara, V. Rathod, K. Murphy, and M. Welling. Bayesian dark knowledge. In NIPS, 2015.
[31] A. Krizhevsky. Learning multiple layers of features from tiny images. 2009.
[32] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.
[33] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial machine learning at scale. arXiv preprint arXiv:1611.01236, 2016.
[34] B. Lakshminarayanan. Decision trees and forests: a probabilistic perspective. PhD thesis, UCL (University College London), 2016.
[35] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436-444, 2015.
[36] S. Lee, S. Purushwalkam, M. Cogswell, D. Crandall, and D. Batra. Why M heads are better than one: Training a diverse ensemble of deep networks. arXiv preprint arXiv:1511.06314, 2015.
[37] S. Lee, S. P. S. Prakash, M. Cogswell, V. Ranjan, D. Crandall, and D. Batra. Stochastic multiple choice learning for training diverse deep ensembles. In NIPS, 2016.
[38] Y. Li, J. M. Hernández-Lobato, and R. E. Turner. Stochastic expectation propagation. In NIPS, 2015.
[39] C. Louizos and M. Welling. Structured and efficient variational deep learning with matrix Gaussian posteriors. arXiv preprint arXiv:1603.04733, 2016.
[40] D. J. MacKay. Bayesian methods for adaptive models. PhD thesis, California Institute of Technology, 1992.
[41] S.-i. Maeda. A Bayesian encourages dropout. arXiv preprint arXiv:1412.7003, 2014.
[42] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.
[43] T. P. Minka. Bayesian model averaging is not model combination. 2000.</p>
<p>[44] T. Miyato, S.-i. Maeda, M. Koyama, K. Nakae, and S. Ishii. Distributional smoothing by virtual adversarial examples. In $I C L R, 2016$.
[45] V. Nair and G. E. Hinton. Rectified linear units improve restricted Boltzmann machines. In ICML, 2010.
[46] R. M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag New York, Inc., 1996.
[47] D. A. Nix and A. S. Weigend. Estimating the mean and variance of the target probability distribution. In IEEE International Conference on Neural Networks, 1994.
[48] I. Osband, C. Blundell, A. Pritzel, and B. Van Roy. Deep exploration via bootstrapped DQN. In NIPS, 2016.
[49] J. Quinonero-Candela, C. E. Rasmussen, F. Sinz, O. Bousquet, and B. Schölkopf. Evaluating predictive uncertainty challenge. In Machine Learning Challenges. Springer, 2006.
[50] C. E. Rasmussen and J. Quinonero-Candela. Healing the relevance vector machine through augmentation. In ICML, 2005.
[51] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211-252, 2015.
[52] S. Singh, D. Hoiem, and D. Forsyth. Swapout: Learning an ensemble of deep architectures. In NIPS, 2016.
[53] J. T. Springenberg, A. Klein, S. Falkner, and F. Hutter. Bayesian optimization with robust Bayesian neural networks. In Advances in Neural Information Processing Systems, pages $4134-4142,2016$.
[54] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. JMLR, 2014.
[55] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In $I C L R, 2014$.
[56] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2818-2826, 2016.
[57] F. Tramèr, A. Kurakin, N. Papernot, D. Boneh, and P. McDaniel. Ensemble adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204, 2017.
[58] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al. Matching networks for one shot learning. In NIPS, 2016.
[59] M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In ICML, 2011.
[60] D. H. Wolpert. Stacked generalization. Neural networks, 5(2):241-259, 1992.
[61] J. Zhou and O. G. Troyanskaya. Predicting effects of noncoding variants with deep learningbased sequence model. Nature methods, 12(10):931-934, 2015.</p>
<h1>Supplementary material</h1>
<h2>Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles</h2>
<h2>A Additional results on regression</h2>
<h2>A. 1 Additional results on regression benchmarks</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Datasets</th>
<th style="text-align: center;">Ensemble-5 (MSE)</th>
<th style="text-align: center;">Ensemble-10 (MSE)</th>
<th style="text-align: center;">ML-1</th>
<th style="text-align: center;">ML-1 + AT</th>
<th style="text-align: center;">ML-5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Boston housing</td>
<td style="text-align: center;">$3.09 \pm 0.84$</td>
<td style="text-align: center;">$3.10 \pm 0.83$</td>
<td style="text-align: center;">$3.17 \pm 1.00$</td>
<td style="text-align: center;">$3.18 \pm 0.99$</td>
<td style="text-align: center;">$3.28 \pm 1.00$</td>
</tr>
<tr>
<td style="text-align: left;">Concrete</td>
<td style="text-align: center;">$5.73 \pm 0.50$</td>
<td style="text-align: center;">$5.76 \pm 0.50$</td>
<td style="text-align: center;">$6.08 \pm 0.56$</td>
<td style="text-align: center;">$6.09 \pm 0.54$</td>
<td style="text-align: center;">$6.03 \pm 0.58$</td>
</tr>
<tr>
<td style="text-align: left;">Energy</td>
<td style="text-align: center;">$1.61 \pm 0.19$</td>
<td style="text-align: center;">$1.62 \pm 0.18$</td>
<td style="text-align: center;">$2.11 \pm 0.30$</td>
<td style="text-align: center;">$2.09 \pm 0.29$</td>
<td style="text-align: center;">$2.09 \pm 0.29$</td>
</tr>
<tr>
<td style="text-align: left;">Kin8nm</td>
<td style="text-align: center;">$0.08 \pm 0.00$</td>
<td style="text-align: center;">$0.08 \pm 0.00$</td>
<td style="text-align: center;">$0.09 \pm 0.00$</td>
<td style="text-align: center;">$0.09 \pm 0.00$</td>
<td style="text-align: center;">$0.09 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: left;">Naval propulsion plant</td>
<td style="text-align: center;">$0.00 \pm 0.00$</td>
<td style="text-align: center;">$0.00 \pm 0.00$</td>
<td style="text-align: center;">$0.00 \pm 0.00$</td>
<td style="text-align: center;">$0.00 \pm 0.00$</td>
<td style="text-align: center;">$0.00 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: left;">Power plant</td>
<td style="text-align: center;">$4.08 \pm 0.15$</td>
<td style="text-align: center;">$4.07 \pm 0.15$</td>
<td style="text-align: center;">$4.10 \pm 0.15$</td>
<td style="text-align: center;">$4.10 \pm 0.15$</td>
<td style="text-align: center;">$4.11 \pm 0.17$</td>
</tr>
<tr>
<td style="text-align: left;">Protein</td>
<td style="text-align: center;">$4.49 \pm 0.04$</td>
<td style="text-align: center;">$4.50 \pm 0.02$</td>
<td style="text-align: center;">$4.64 \pm 0.01$</td>
<td style="text-align: center;">$4.75 \pm 0.11$</td>
<td style="text-align: center;">$4.71 \pm 0.17$</td>
</tr>
<tr>
<td style="text-align: left;">Wine</td>
<td style="text-align: center;">$0.64 \pm 0.04$</td>
<td style="text-align: center;">$0.64 \pm 0.04$</td>
<td style="text-align: center;">$0.64 \pm 0.04$</td>
<td style="text-align: center;">$0.64 \pm 0.04$</td>
<td style="text-align: center;">$0.64 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: left;">Yacht</td>
<td style="text-align: center;">$2.78 \pm 0.59$</td>
<td style="text-align: center;">$2.68 \pm 0.57$</td>
<td style="text-align: center;">$1.43 \pm 0.57$</td>
<td style="text-align: center;">$1.47 \pm 0.58$</td>
<td style="text-align: center;">$1.58 \pm 0.48$</td>
</tr>
<tr>
<td style="text-align: left;">Year Prediction MSD</td>
<td style="text-align: center;">$8.92 \pm$ nan</td>
<td style="text-align: center;">$8.95 \pm$ nan</td>
<td style="text-align: center;">$8.89 \pm$ nan</td>
<td style="text-align: center;">$9.02 \pm$ nan</td>
<td style="text-align: center;">$8.89 \pm$ nan</td>
</tr>
<tr>
<td style="text-align: left;">Datasets</td>
<td style="text-align: center;">Ensemble-5 (MSE)</td>
<td style="text-align: center;">Ensemble-10 (MSE)</td>
<td style="text-align: center;">ML-1</td>
<td style="text-align: center;">ML-1 + AT</td>
<td style="text-align: center;">ML-5</td>
</tr>
<tr>
<td style="text-align: left;">Boston housing</td>
<td style="text-align: center;">$17.28 \pm 6.17$</td>
<td style="text-align: center;">$10.61 \pm 4.37$</td>
<td style="text-align: center;">$2.55 \pm 0.36$</td>
<td style="text-align: center;">$2.57 \pm 0.37$</td>
<td style="text-align: center;">$2.41 \pm 0.25$</td>
</tr>
<tr>
<td style="text-align: left;">Concrete</td>
<td style="text-align: center;">$16.07 \pm 5.75$</td>
<td style="text-align: center;">$8.96 \pm 1.73$</td>
<td style="text-align: center;">$3.22 \pm 0.31$</td>
<td style="text-align: center;">$3.21 \pm 0.26$</td>
<td style="text-align: center;">$3.06 \pm 0.18$</td>
</tr>
<tr>
<td style="text-align: left;">Energy</td>
<td style="text-align: center;">$9.54 \pm 4.54$</td>
<td style="text-align: center;">$6.70 \pm 2.39$</td>
<td style="text-align: center;">$1.61 \pm 0.40$</td>
<td style="text-align: center;">$1.51 \pm 0.28$</td>
<td style="text-align: center;">$1.38 \pm 0.22$</td>
</tr>
<tr>
<td style="text-align: left;">Kin8nm</td>
<td style="text-align: center;">$2.12 \pm 0.97$</td>
<td style="text-align: center;">$0.11 \pm 0.20$</td>
<td style="text-align: center;">$-1.11 \pm 0.04$</td>
<td style="text-align: center;">$-1.12 \pm 0.04$</td>
<td style="text-align: center;">$-1.20 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: left;">Naval propulsion plant</td>
<td style="text-align: center;">$-5.68 \pm 0.34$</td>
<td style="text-align: center;">$-5.85 \pm 0.15$</td>
<td style="text-align: center;">$-5.65 \pm 0.28$</td>
<td style="text-align: center;">$-4.08 \pm 0.13$</td>
<td style="text-align: center;">$-5.63 \pm 0.26$</td>
</tr>
<tr>
<td style="text-align: left;">Power plant</td>
<td style="text-align: center;">$35.78 \pm 12.87$</td>
<td style="text-align: center;">$22.04 \pm 4.42$</td>
<td style="text-align: center;">$2.82 \pm 0.04$</td>
<td style="text-align: center;">$2.82 \pm 0.04$</td>
<td style="text-align: center;">$2.79 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: left;">Protein</td>
<td style="text-align: center;">$40.98 \pm 7.43$</td>
<td style="text-align: center;">$25.73 \pm 1.59$</td>
<td style="text-align: center;">$2.87 \pm 0.03$</td>
<td style="text-align: center;">$2.91 \pm 0.03$</td>
<td style="text-align: center;">$2.83 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: left;">Wine</td>
<td style="text-align: center;">$33.73 \pm 10.75$</td>
<td style="text-align: center;">$20.55 \pm 3.72$</td>
<td style="text-align: center;">$1.95 \pm 4.08$</td>
<td style="text-align: center;">$1.58 \pm 2.30$</td>
<td style="text-align: center;">$0.94 \pm 0.12$</td>
</tr>
<tr>
<td style="text-align: left;">Yacht</td>
<td style="text-align: center;">$10.18 \pm 4.86$</td>
<td style="text-align: center;">$6.85 \pm 2.84$</td>
<td style="text-align: center;">$1.26 \pm 0.29$</td>
<td style="text-align: center;">$1.28 \pm 0.36$</td>
<td style="text-align: center;">$1.18 \pm 0.21$</td>
</tr>
<tr>
<td style="text-align: left;">Year Prediction MSD</td>
<td style="text-align: center;">$39.02 \pm$ nan</td>
<td style="text-align: center;">$21.45 \pm$ nan</td>
<td style="text-align: center;">$3.41 \pm$ nan</td>
<td style="text-align: center;">$3.39 \pm$ nan</td>
<td style="text-align: center;">$3.35 \pm$ nan</td>
</tr>
</tbody>
</table>
<p>Table 2: Additional results on regression benchmark datasets: the top table reports RMSE and bottom table reports NLL. Ensemble- $M$ (MSE) denotes ensemble of $M$ networks trained to minimize mean squared error (MSE); the predicted variance is the empirical variance of the individual networks' predictions. ML-1 denotes maximum likelihood with a single network trained to predict the mean and variance as described in Section 2.2.1. ML-1 is significantly better than Ensemble-5 (MSE) as well as Ensemble-10 (MSE), clearly demonstrating the effect of learning variance. ML-1+AT denotes additional effect of adversarial training (AT); AT does not significantly help on these benchmarks. ML-5, referred to as deep ensembles in Table 1, is an ensemble of 5 networks trained to predict mean and variance. ML-5+AT results are very similar to ML-5 (the error bars overlap), hence we do not report them here.</p>
<h2>A. 2 Effect of training using MSE vs training using NLL</h2>
<p>A commonly used heuristic in practice, is to train an ensemble of NNs to minimize MSE and use the empirical variance of the networks' predictions as an approximate measure of uncertainty. However, this generally does not lead to well-calibrated predictive probabilities as MSE is not a scoring rule that captures predictive uncertainty. As a motivating example, we report calibration curves (also known as reliability diagrams) on the Year Prediction MSD dataset in Figure 7. First, we compute the $z \%$ (e.g. $90 \%$ ) prediction interval for each test data point based on Gaussian quantiles using predictive mean and variance. Next, we measure what fraction of test observations fall within this prediction interval. For a well-calibrated regressor, the observed fraction should be close to $z \%$. We compute observed fraction for $z=10 \%$ to $z=90 \%$ in increments of 10 . A well-calibrated regressor should lie very close to the diagonal; on the left subplot we observe that the proposed method, which learns the predictive variance, leads to a well-calibrated regressor. However, on the right subplot, we observe that the empirical variance obtained from NNs which do not learn the predictive variance (specifically, five NNs trained to minimize MSE) consistently underestimates the true uncertainty. For instance, the $80 \%$ prediction interval contains only $20 \%$ of the test observations, which means the empirical variance significantly underestimates the true predictive uncertainty.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Calibration results on the Year Prediction MSD dataset: $x$-axis denotes the expected fraction and $y$-axis denotes the observed fraction; ideal output is the dashed blue line. Predicted variance (left) is significantly better calibrated than the empirical variance (right). See main text for further details.</p>
<h1>B Additional results on classification</h1>
<h2>B. 1 Additional results on MNIST</h2>
<p>Figures 8 and 9 report results on MNIST dataset using different architecture than those in Figure 2(a). We observe qualitatively similar results. Ensembles outperform MC-dropout and adversarial training improves performance.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Results on MNIST dataset using the same setup as that in Figure 2(a) except that we use two hidden layers in the MLP instead of three. Ensembles and adversarial training improve performance and our method outperforms MC-dropout.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Results on MNIST dataset using a convolutional network as opposed to the 3-layer MLP in Figure 2(a). Even on a different architecture, ensembles and adversarial training improve performance and our method outperforms MC-dropout.</p>
<h2>B. 2 Qualitative evaluation of uncertainty</h2>
<p>We qualitatively evaluate the performance of uncertainty in terms of two measures, namely the confidence of the predicted label (i.e. maximum of the softmax output), and the disagreement between the ensemble members in terms of Jensen-Shannon divergence. The results on known classes are</p>
<p>shown in Figures 10 and 11. Similarly, the results on unknown classes are shown in Figures 12(a) and 12(b). We observe that both measures of uncertainty capture meaningful ambiguity.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: Results on MNIST showing test examples with high or low disagreement between the networks in the ensemble: Top two rows denote the test examples with least disagreement and the bottom two rows denote test examples with the most disagreement.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: Results on MNIST showing test examples with high or low confidence: Top two rows denote the test examples with highest confidence and the bottom two rows denote test examples with the least confidence.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12: Deep ensemble trained on MNIST and tested on the NotMNIST dataset containing unseen classes: On the left, top two rows denote the test examples with highest confidence and the bottom two rows denote the test examples with the least confidence. On the right, top two rows denote the test examples with highest confidence and the bottom two rows denote the test examples with the least confidence. We observe that these measures capture meaningful ambiguity: for example, the ensemble agreement is high for letters ' I ' and some variants of ' J ' which resemble 1 in the MNIST training dataset. The ensemble disagreement is high and confidence is low for examples visually dis-similar from the MNIST training dataset.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ The architecture is similar to the one described in http://torch.ch/blog/2015/07/30/cifar.html.
${ }^{9}$ More precisely, we define disagreement as $\sum_{m=1}^{M} \mathrm{KL}\left(p_{\theta_{m}}(y \mid \mathbf{x})\right)\left|p_{E}(y \mid \mathbf{x})\right)$ where KL denotes the Kullback-Leibler divergence and $p_{E}(y \mid \mathbf{x})=M^{-1} \sum_{m} p_{\theta_{m}}(y \mid \mathbf{x})$ is the prediction of the ensemble.
${ }^{10}$ Available at http://yaroslavvb.blogspot.co.uk/2011/09/notmnist-dataset.html&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
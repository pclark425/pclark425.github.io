<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1146 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1146</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1146</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-258179505</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2304.07665v2.pdf" target="_blank">Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling</a></p>
                <p><strong>Paper Abstract:</strong> Active learning provides a framework to adaptively query the most informative experiments towards learning an unknown black-box function. Various approaches of active learning have been proposed in the literature, however, they either focus on exploration or exploitation in the design space. Methods that do consider exploration-exploitation simultaneously employ fixed or ad-hoc measures to control the trade-off that may not be optimal. In this paper, we develop a Bayesian hierarchical approach, referred as BHEEM, to dynamically balance the exploration-exploitation trade-off as more data points are queried. To sample from the posterior distribution of the trade-off parameter, We subsequently formulate an approximate Bayesian computation approach based on the linear dependence of queried data in the feature space. Simulated and real-world examples show the proposed approach achieves at least 21% and 11% average improvement when compared to pure exploration and exploitation strategies respectively. More importantly, we note that by optimally balancing the trade-off between exploration and exploitation, BHEEM performs better or at least as well as either pure exploration or pure exploitation.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1146.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1146.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BHEEM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Hierarchical Exploration-Exploitation Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active learning agent that dynamically estimates a trade-off parameter η within a Bayesian hierarchical model and uses ABC-MCMC (Metropolis-within-Gibbs) and an ALD (approx. linear dependence) kernel-based summary statistic to select queries for Gaussian Process regression.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BHEEM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Active learner composed of: (i) Gaussian Process Regression surrogate (zero-mean GP, Matérn-3/2 kernel in experiments), (ii) a Bayesian hierarchical model placing a Beta(α,β) prior on the per-stage trade-off η_j with hyperpriors on α,β, (iii) ABC-MCMC (Metropolis within Gibbs) to sample η when the likelihood p(y,X|η) is intractable, and (iv) an ALD (approximate linear dependence) kernel-based acceptance test (δ ≥ ν) used as the ABC summary statistic to decide whether a proposed candidate query brings new information.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>active learning (Bayesian adaptive experimental design via hierarchical modeling + ABC-MCMC)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each query stage j, BHEEM treats η_j as a random variable drawn from a Beta prior whose hyperparameters α,β are inferred in a hierarchical model; α,β,η are sampled with Gibbs where η is drawn via ABC-MCMC using a Metropolis proposal N(η_old,τ^2). Candidate query x proposed under the combined acquisition (η F1 + (1−η) F2) is accepted only if its kernel-space ALD distance δ > ν (ν=0.001) — i.e., it is not linearly dependent on existing queried points — providing an ABC summary that measures new information. The posterior mean of η (averaged MCMC samples) is used to set the exploration/exploitation mix dynamically across stages.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Unknown black-box functions (simulated benchmarks F1–F6) and MAX-phase materials lattice-constant prediction dataset</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown, noisy black-box mapping from continuous design space x to scalar outputs; continuous, possibly multi-dimensional (1D, 2D, 3D, 10D in simulations), potentially discontinuous or having sharp local features; limited labeling budget (pool-based/ sequential active learning: unlabeled candidate set and small number of queried labels); observation noise (signal-to-noise ratio fixed to 10 in simulations).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Varied: simulated benchmarks include 1D functions (F1,F2), 2D (F3,F4), 3D (Hartmann F5), and 10D (F6); action space is continuous selection of next query x from the domain; experiments used up to 100 queried points (per run) and 1000 test grid points for RMSE evaluation; committee size for QBC baseline was 10 GPR models when used.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Reported average RMSE improvements relative to baselines: across simulated experiments BHEEM achieved substantial reductions in RMSE versus pure-exploration and pure-exploitation baselines — paper reports at-least ~21% improvement against pure exploration and ~11% against pure exploitation (abstract); other places report per-experiment numbers (e.g., in one comparison: ~7% improvement vs iGS and ~21% vs QBC; in the MAX-phase materials case study BHEEM achieved ~10.4% and ~11.3% average improvement over pure exploration and exploitation respectively). (Metric: relative decrease in RMSE; absolute RMSE values per function are plotted in paper figures but not summarized numerically in text.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Pure-exploration (iGS or maximum-variance) and pure-exploitation (QBC) baselines show higher RMSE: reported relative differences vary by benchmark — e.g., BHEEM vs iGS: improvements ranged from ~5.7% up to ~21% depending on exploration baseline and benchmark; BHEEM converged faster (fewer queries) for most tested functions. (No single absolute baseline RMSE value universally reported in main text.)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>BHEEM typically converged with fewer queried points than many baselines across the tested functions (plots show lower RMSE earlier in the query sequence). Experiments used up to 100 queried points and repeated each simulation 100 times; the paper reports faster convergence in all cases but does not provide uniform numeric sample-count thresholds (only RMSE-vs-#queries curves per function).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Trade-off parameter η is modeled hierarchically and inferred from data: η_j ∼ p(η|α_j,β_j) with α,β given hyperpriors; η is sampled via ABC-MCMC and its posterior mean at each stage determines mixing in acquisition x* = argmax_x [η F_exploration(x) + (1−η) F_exploitation(x)]. The ALD-based ABC acceptance ensures the sampled η leads to queries that actually add new information. Proposal variance τ and ALD threshold ν control exploration of η and sample selection (paper fixes ν=0.001 and τ=0.1 after sensitivity analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Pure exploration: improved Greedy Sampling (iGS), maximum variance, maximum entropy; Pure exploitation: Query-by-Committee (QBC); Trade-off baselines: static trade-offs (fixed η values 0.25,0.5,0.75) and probabilistic (ϵ-decreasing / annealing-style) trade-off (Elreedy et al. style with α=0.7). Random sampling baseline also included.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>BHEEM (hierarchical Bayesian update of η + ABC-MCMC with ALD summary) generally outperformed or matched the better of pure-exploration or pure-exploitation strategies across six simulated functions and one materials dataset; it often attained lower RMSE with fewer queries (faster convergence). Quantitatively, the paper reports average relative RMSE reductions typically on the order of ~5–25% depending on baseline and benchmark (examples: ~21% vs some exploration baselines and ~11–21% vs exploitation baseline in various reported summaries). BHEEM incurred higher computation (≈2–2.5× slower than single acquisition baselines) but is suitable for offline experiments where query cost dominates compute cost. The ALD kernel-based ABC summary enables likelihood-free inference of η by measuring new information in kernel feature space.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Higher computational cost (BHEEM ~2× slower than QBC and ~2.5× slower than iGS). Sensitivity to design choices: ALD threshold ν and proposal variance τ affect acceptance/convergence (authors fixed ν=0.001, τ=0.1 after sensitivity analysis). The methodology assumes Beta prior on η and authors note a restricted prior as a limitation; ABC-MCMC convergence and dependence on summary statistic choice are additional constraints. Performance varies by kernel and surrogate choice and in some high-dimensional cases (10D F6) different baselines can sometimes be competitive; no experiments in truly adversarial/non-stationary or partially-observable (POMDP) dynamics beyond pool-based active learning were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Key implementation details: GPR surrogate with Matérn 3/2 kernel (signal variance σ_f^2=1), committee of 10 diverse GPR kernels used for QBC; SN ratio fixed to 10 for simulated noise; RMSE computed on 1000 test points; experiments repeated 100× for statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1146.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1146.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QBC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Query-by-Committee</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An exploitation-oriented active learning strategy that maintains an ensemble (committee) of models and queries the point where committee disagreement (max pairwise prediction difference) is largest.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Query-by-committee</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Query-by-Committee (QBC)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Committee-based active learner using an ensemble of Q regression models (in experiments: 10 Gaussian Process models with varied kernels); acquisition selects x maximizing max_{l,p} |h_l(x) − h_p(x)|.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>active learning via model-committee disagreement</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Selects queries where committee predictions diverge most, focusing sampling on regions of high predictive disagreement (exploitation of uncertain/discontinuous regions). Does not adapt a trade-off parameter dynamically — fixed acquisition rule.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same benchmarks as BHEEM (simulated functions F1–F6 and MAX-phase materials dataset) when used as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown noisy continuous black-box functions; focuses on regions with predictive model disagreement (often near discontinuities).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Used with varied dimensionalities (1D,2D,3D,10D); committee size 10 in experiments; selection from continuous candidate set.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baseline performance (no η adaptation). Paper reports that QBC often over-exploits uncertain regions and can ignore global exploration; relative to BHEEM, QBC had higher RMSE in many benchmarks (BHEEM reported up to ~21% lower RMSE compared to QBC in some summaries). In some 3D and high-dim problems QBC was competitive with BHEEM.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Less sample-efficient than BHEEM in many tested functions (QBC tends to focus queries in narrow uncertain regions), as inferred from RMSE-vs-#queries curves; no absolute sample-count threshold provided.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Implicit exploitation bias (committee disagreement); no explicit trade-off parameter — favors exploitation of disagreement regions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against BHEEM, iGS, maximum variance, maximum entropy, static and probabilistic trade-offs, random sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>QBC effectively exploits discontinuous/rough regions but may ignore global coverage, leading to higher generalization error when used alone; it is often outperformed by BHEEM which balances exploration and exploitation dynamically.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Tendency to over-sample locally around disagreements (exploitation bias); may miss other informative regions and thereby yield poor global surrogate fits; computational overhead for maintaining multiple models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1146.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1146.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>iGS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>improved Greedy Sampling (iGS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An exploration-oriented acquisition that selects candidate points farthest from existing training points in both input and predicted output spaces (product of distances), promoting diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Active learning for regression using greedy sampling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>improved Greedy Sampling (iGS)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Acquisition computes u(x_j) = ||x_j − X_o||_2 and v(x_j) = ||f_j − y_o||_2 where f_j is the model prediction, then uses min(u(x_j), v(x_j)) or product u(x_j) v(x_j) to score candidates and selects the maximizer to encourage exploration in both input and output spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>active learning with exploration-focused acquisition (diversity-based)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Always chooses the most 'novel' point relative to already queried data in input and output spaces; no explicit dynamic trade-off parameter, purely exploration-biased.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same experimental benchmarks (F1–F6 and MAX-phase dataset) as used in paper</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Designed for continuous unknown functions; seeks broad coverage to reduce surrogate variance; handles noisy outputs implicitly via inclusion of output distance metric.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Applied across 1D–10D simulated benchmarks; selection from continuous candidate pools; used 100 queried points in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>As pure-exploration baseline: in experiments iGS sometimes achieved low RMSE (e.g., in some 1D problems it was close to BHEEM), but overall BHEEM usually outperformed iGS; reported relative improvements vary (paper cites e.g., BHEEM achieving ~5–21% lower RMSE vs exploration baselines depending on scenario).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Exploration-focused; can require more queries to refine local sharp features compared to exploitation methods — BHEEM often converged faster by balancing exploration/exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Pure exploration strategy (no exploitation component).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared versus BHEEM, QBC, maximum variance, maximum entropy, static/probabilistic trade-offs, random.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>iGS ensures coverage and good global surrogate behavior but can miss sharp local features (peaks) that exploitation approaches capture; combined in BHEEM it helps provide exploratory component.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Fails to adequately focus on sharp peaks or discontinuities (underfits highly localized structure); not adaptive to discovered local uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1146.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1146.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MaxVar</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Maximum Variance Acquisition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An exploration acquisition that selects the input with the highest predictive variance under the surrogate model (GPR), aiming to minimize future generalization error by reducing model variance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A variance maximization criterion for active learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Maximum Variance</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Acquisition uses diagonal elements of the GP posterior covariance (V(x)) and selects x maximizing predictive variance: x* = argmax_x V(x).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>active learning (uncertainty sampling / variance reduction)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Selects points where model predicts largest posterior variance, i.e., likely less-explored regions; does not adapt mixing parameter η.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Used as exploration baseline on same benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Continuous noisy black-box functions; variance higher in less-explored regions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Applied across varied dimensions in experiments; used with GPR surrogate (Matérn 3/2 in main experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Paper reports BHEEM achieved about 5.7% lower average RMSE over maximum-variance exploration baseline in one comparative summary (and ~2.3% lower vs QBC in that pairing).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Exploration method that can be sample-inefficient in focusing on variance-only (may ignore high-error regions with small variance); BHEEM reported better overall efficiency by combining variance exploration with exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Pure exploration (uncertainty based).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against BHEEM, iGS, QBC, max-entropy, static/probabilistic trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Works well to explore unobserved regions and reduce model variance but can be suboptimal versus methods that balance exploitation of sharp features.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>May ignore regions where model bias is large but variance is moderate; not sufficient alone to minimize generalization error in presence of sharp local structure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1146.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1146.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MaxEnt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Maximum Entropy Sampling (Uncertainty Sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An acquisition function that selects points maximizing Shannon entropy of the GP posterior (equivalently aiming to reduce model uncertainty/information content).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Entropy-based active learning for object recognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Maximum Entropy</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Computes Shannon entropy of the GP posterior H[f, cov(f)] = 1/2 log |cov(f)| + D/2 log(2πe) and selects x with largest entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>active learning via information-theoretic uncertainty sampling</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Queries points with highest posterior entropy to reduce model uncertainty globally; no explicit η adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Used as exploration baseline on paper benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Continuous noisy black-box; entropy reflects uncertainty over multivariate GP predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Applied across same simulated and real datasets; computational cost grows with covariance matrix size.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Generally exploration-biased; often outperformed by BHEEM which dynamically trades off exploration and exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Exploration-focused; may require more queries to capture sharp localized features depending on GP smoothness assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Pure or strong exploration (information-theoretic).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against BHEEM, QBC, iGS, max-variance, static/probabilistic strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Effective at reducing global uncertainty but can miss local discontinuities if GP kernel smoothness mismatches true function.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Dependent on GP covariance assumptions; computationally demanding for large candidate pools due to determinant computations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1146.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1146.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StaticTradeoff</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Static Trade-off (fixed η)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of baseline strategies that fixes the linear mixing weight η between exploration and exploitation acquisition functions for the entire learning run.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Static trade-off (fixed η)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Preselect a constant η ∈ {0.25,0.5,0.75} and use x* = argmax_x [η F_exploration + (1−η) F_exploitation] across all iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>none (fixed mixing of acquisition functions)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>No adaptation — the exploration/exploitation balance is constant and set a priori (trial-and-error or prior belief).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same benchmark functions and MAX dataset as used in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown continuous functions; fixed strategy does not respond to observed data dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same varied dimensionalities; performance depends strongly on problem structure and chosen η.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Paper reports that no single fixed η works well for all functions or all iterations; heatmaps show varying RMSE across functions and iterations depending on η. BHEEM outperforms static trade-offs in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Generally inferior to BHEEM because it cannot adapt to changing needs across query stages.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Fixed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against BHEEM and probabilistic trade-off.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Static choices of η are suboptimal across diverse functions and across time; retrospective tuning (cross-validation) is not helpful for online query selection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Non-adaptive; sensitive to prior tuning; poor transfer across different functions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1146.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1146.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProbTradeoff</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probabilistic (decaying) Trade-off</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline adaptive strategy that probabilistically chooses exploration vs exploitation each iteration with an exploration probability that decays over time (ϵ-decreasing / annealing-like).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A novel active learning regression framework for balancing the explorationexploitation trade-off</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Probabilistic trade-off (ϵ-decreasing)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>At iteration t, exploration is chosen with probability p_R = α^(t−1) (α<1, paper uses α=0.7); a uniform random draw then selects exploration (η=1) or exploitation (η=0) accordingly. This yields an initially exploratory policy that becomes more exploitative over iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>probabilistic annealing of exploration probability</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Exploration probability decays geometrically over iterations, independent of observed data; adaptation is time-based rather than data-driven.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown noisy continuous functions; this method assumes generic need for more exploration early and exploitation later.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>As in paper; performance depends on the decay rate α which is fixed (α=0.7) in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Paper reports probabilistic trade-off is outperformed by BHEEM; heatmap comparisons show BHEEM provides better RMSE across many functions and iterations. No single decay schedule works for all problems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Better than static in some contexts but inferior to data-driven BHEEM; sample efficiency sensitive to choice of α.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Time-decaying probabilistic switch (from exploration to exploitation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against BHEEM and static trade-offs in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Probabilistic decays (annealing) are a simple adaptive scheme but their schedule must be tuned to problem; BHEEM's data-driven hierarchical adaptation outperforms fixed decay schedules.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Decay schedule may be mismatched to the problem leading to too little or too much exploration at critical times; does not use observed data to control transition.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1146.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1146.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UCB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Upper Confidence Bound</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement-learning / bandit exploration strategy that trades off mean and uncertainty (confidence bounds) to balance exploration and exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using confidence bounds for exploitation-exploration trade-offs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Upper Confidence Bound (UCB)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Selection via optimistic estimates: choose action that maximizes μ(x) + κ · σ(x) (mean plus scaled uncertainty), widely used in bandits and Bayesian optimization to balance exploration and exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>optimistic exploration (confidence-bound based)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Balances exploration/exploitation by augmenting mean predictions with an uncertainty bonus whose scale can decay or be tuned; adaptation relies on posterior uncertainty estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Mentioned as RL literature example of exploration-exploitation balancing; not experimentally used in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Uncertainty-weighted optimistic selection; tunable bonus parameter κ controls balance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Mentioned as a celebrated approach in RL for balancing exploration versus exploitation; not applied or evaluated in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Active learning with statistical models <em>(Rating: 2)</em></li>
                <li>Query-by-committee <em>(Rating: 2)</em></li>
                <li>A novel active learning regression framework for balancing the explorationexploitation trade-off <em>(Rating: 2)</em></li>
                <li>Using confidence bounds for exploitation-exploration trade-offs <em>(Rating: 2)</em></li>
                <li>An adaptive exploration-exploitation algorithm for constructing metamodels in random simulation using a novel sequential experimental design <em>(Rating: 1)</em></li>
                <li>A variance maximization criterion for active learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1146",
    "paper_id": "paper-258179505",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "BHEEM",
            "name_full": "Bayesian Hierarchical Exploration-Exploitation Model",
            "brief_description": "An active learning agent that dynamically estimates a trade-off parameter η within a Bayesian hierarchical model and uses ABC-MCMC (Metropolis-within-Gibbs) and an ALD (approx. linear dependence) kernel-based summary statistic to select queries for Gaussian Process regression.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "BHEEM",
            "agent_description": "Active learner composed of: (i) Gaussian Process Regression surrogate (zero-mean GP, Matérn-3/2 kernel in experiments), (ii) a Bayesian hierarchical model placing a Beta(α,β) prior on the per-stage trade-off η_j with hyperpriors on α,β, (iii) ABC-MCMC (Metropolis within Gibbs) to sample η when the likelihood p(y,X|η) is intractable, and (iv) an ALD (approximate linear dependence) kernel-based acceptance test (δ ≥ ν) used as the ABC summary statistic to decide whether a proposed candidate query brings new information.",
            "adaptive_design_method": "active learning (Bayesian adaptive experimental design via hierarchical modeling + ABC-MCMC)",
            "adaptation_strategy_description": "At each query stage j, BHEEM treats η_j as a random variable drawn from a Beta prior whose hyperparameters α,β are inferred in a hierarchical model; α,β,η are sampled with Gibbs where η is drawn via ABC-MCMC using a Metropolis proposal N(η_old,τ^2). Candidate query x proposed under the combined acquisition (η F1 + (1−η) F2) is accepted only if its kernel-space ALD distance δ &gt; ν (ν=0.001) — i.e., it is not linearly dependent on existing queried points — providing an ABC summary that measures new information. The posterior mean of η (averaged MCMC samples) is used to set the exploration/exploitation mix dynamically across stages.",
            "environment_name": "Unknown black-box functions (simulated benchmarks F1–F6) and MAX-phase materials lattice-constant prediction dataset",
            "environment_characteristics": "Unknown, noisy black-box mapping from continuous design space x to scalar outputs; continuous, possibly multi-dimensional (1D, 2D, 3D, 10D in simulations), potentially discontinuous or having sharp local features; limited labeling budget (pool-based/ sequential active learning: unlabeled candidate set and small number of queried labels); observation noise (signal-to-noise ratio fixed to 10 in simulations).",
            "environment_complexity": "Varied: simulated benchmarks include 1D functions (F1,F2), 2D (F3,F4), 3D (Hartmann F5), and 10D (F6); action space is continuous selection of next query x from the domain; experiments used up to 100 queried points (per run) and 1000 test grid points for RMSE evaluation; committee size for QBC baseline was 10 GPR models when used.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Reported average RMSE improvements relative to baselines: across simulated experiments BHEEM achieved substantial reductions in RMSE versus pure-exploration and pure-exploitation baselines — paper reports at-least ~21% improvement against pure exploration and ~11% against pure exploitation (abstract); other places report per-experiment numbers (e.g., in one comparison: ~7% improvement vs iGS and ~21% vs QBC; in the MAX-phase materials case study BHEEM achieved ~10.4% and ~11.3% average improvement over pure exploration and exploitation respectively). (Metric: relative decrease in RMSE; absolute RMSE values per function are plotted in paper figures but not summarized numerically in text.)",
            "performance_without_adaptation": "Pure-exploration (iGS or maximum-variance) and pure-exploitation (QBC) baselines show higher RMSE: reported relative differences vary by benchmark — e.g., BHEEM vs iGS: improvements ranged from ~5.7% up to ~21% depending on exploration baseline and benchmark; BHEEM converged faster (fewer queries) for most tested functions. (No single absolute baseline RMSE value universally reported in main text.)",
            "sample_efficiency": "BHEEM typically converged with fewer queried points than many baselines across the tested functions (plots show lower RMSE earlier in the query sequence). Experiments used up to 100 queried points and repeated each simulation 100 times; the paper reports faster convergence in all cases but does not provide uniform numeric sample-count thresholds (only RMSE-vs-#queries curves per function).",
            "exploration_exploitation_tradeoff": "Trade-off parameter η is modeled hierarchically and inferred from data: η_j ∼ p(η|α_j,β_j) with α,β given hyperpriors; η is sampled via ABC-MCMC and its posterior mean at each stage determines mixing in acquisition x* = argmax_x [η F_exploration(x) + (1−η) F_exploitation(x)]. The ALD-based ABC acceptance ensures the sampled η leads to queries that actually add new information. Proposal variance τ and ALD threshold ν control exploration of η and sample selection (paper fixes ν=0.001 and τ=0.1 after sensitivity analysis).",
            "comparison_methods": "Pure exploration: improved Greedy Sampling (iGS), maximum variance, maximum entropy; Pure exploitation: Query-by-Committee (QBC); Trade-off baselines: static trade-offs (fixed η values 0.25,0.5,0.75) and probabilistic (ϵ-decreasing / annealing-style) trade-off (Elreedy et al. style with α=0.7). Random sampling baseline also included.",
            "key_results": "BHEEM (hierarchical Bayesian update of η + ABC-MCMC with ALD summary) generally outperformed or matched the better of pure-exploration or pure-exploitation strategies across six simulated functions and one materials dataset; it often attained lower RMSE with fewer queries (faster convergence). Quantitatively, the paper reports average relative RMSE reductions typically on the order of ~5–25% depending on baseline and benchmark (examples: ~21% vs some exploration baselines and ~11–21% vs exploitation baseline in various reported summaries). BHEEM incurred higher computation (≈2–2.5× slower than single acquisition baselines) but is suitable for offline experiments where query cost dominates compute cost. The ALD kernel-based ABC summary enables likelihood-free inference of η by measuring new information in kernel feature space.",
            "limitations_or_failures": "Higher computational cost (BHEEM ~2× slower than QBC and ~2.5× slower than iGS). Sensitivity to design choices: ALD threshold ν and proposal variance τ affect acceptance/convergence (authors fixed ν=0.001, τ=0.1 after sensitivity analysis). The methodology assumes Beta prior on η and authors note a restricted prior as a limitation; ABC-MCMC convergence and dependence on summary statistic choice are additional constraints. Performance varies by kernel and surrogate choice and in some high-dimensional cases (10D F6) different baselines can sometimes be competitive; no experiments in truly adversarial/non-stationary or partially-observable (POMDP) dynamics beyond pool-based active learning were reported.",
            "notes": "Key implementation details: GPR surrogate with Matérn 3/2 kernel (signal variance σ_f^2=1), committee of 10 diverse GPR kernels used for QBC; SN ratio fixed to 10 for simulated noise; RMSE computed on 1000 test points; experiments repeated 100× for statistics.",
            "uuid": "e1146.0",
            "source_info": {
                "paper_title": "Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "QBC",
            "name_full": "Query-by-Committee",
            "brief_description": "An exploitation-oriented active learning strategy that maintains an ensemble (committee) of models and queries the point where committee disagreement (max pairwise prediction difference) is largest.",
            "citation_title": "Query-by-committee",
            "mention_or_use": "use",
            "agent_name": "Query-by-Committee (QBC)",
            "agent_description": "Committee-based active learner using an ensemble of Q regression models (in experiments: 10 Gaussian Process models with varied kernels); acquisition selects x maximizing max_{l,p} |h_l(x) − h_p(x)|.",
            "adaptive_design_method": "active learning via model-committee disagreement",
            "adaptation_strategy_description": "Selects queries where committee predictions diverge most, focusing sampling on regions of high predictive disagreement (exploitation of uncertain/discontinuous regions). Does not adapt a trade-off parameter dynamically — fixed acquisition rule.",
            "environment_name": "Same benchmarks as BHEEM (simulated functions F1–F6 and MAX-phase materials dataset) when used as baseline",
            "environment_characteristics": "Unknown noisy continuous black-box functions; focuses on regions with predictive model disagreement (often near discontinuities).",
            "environment_complexity": "Used with varied dimensionalities (1D,2D,3D,10D); committee size 10 in experiments; selection from continuous candidate set.",
            "uses_adaptive_design": false,
            "performance_with_adaptation": null,
            "performance_without_adaptation": "Baseline performance (no η adaptation). Paper reports that QBC often over-exploits uncertain regions and can ignore global exploration; relative to BHEEM, QBC had higher RMSE in many benchmarks (BHEEM reported up to ~21% lower RMSE compared to QBC in some summaries). In some 3D and high-dim problems QBC was competitive with BHEEM.",
            "sample_efficiency": "Less sample-efficient than BHEEM in many tested functions (QBC tends to focus queries in narrow uncertain regions), as inferred from RMSE-vs-#queries curves; no absolute sample-count threshold provided.",
            "exploration_exploitation_tradeoff": "Implicit exploitation bias (committee disagreement); no explicit trade-off parameter — favors exploitation of disagreement regions.",
            "comparison_methods": "Compared against BHEEM, iGS, maximum variance, maximum entropy, static and probabilistic trade-offs, random sampling.",
            "key_results": "QBC effectively exploits discontinuous/rough regions but may ignore global coverage, leading to higher generalization error when used alone; it is often outperformed by BHEEM which balances exploration and exploitation dynamically.",
            "limitations_or_failures": "Tendency to over-sample locally around disagreements (exploitation bias); may miss other informative regions and thereby yield poor global surrogate fits; computational overhead for maintaining multiple models.",
            "uuid": "e1146.1",
            "source_info": {
                "paper_title": "Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "iGS",
            "name_full": "improved Greedy Sampling (iGS)",
            "brief_description": "An exploration-oriented acquisition that selects candidate points farthest from existing training points in both input and predicted output spaces (product of distances), promoting diversity.",
            "citation_title": "Active learning for regression using greedy sampling",
            "mention_or_use": "use",
            "agent_name": "improved Greedy Sampling (iGS)",
            "agent_description": "Acquisition computes u(x_j) = ||x_j − X_o||_2 and v(x_j) = ||f_j − y_o||_2 where f_j is the model prediction, then uses min(u(x_j), v(x_j)) or product u(x_j) v(x_j) to score candidates and selects the maximizer to encourage exploration in both input and output spaces.",
            "adaptive_design_method": "active learning with exploration-focused acquisition (diversity-based)",
            "adaptation_strategy_description": "Always chooses the most 'novel' point relative to already queried data in input and output spaces; no explicit dynamic trade-off parameter, purely exploration-biased.",
            "environment_name": "Same experimental benchmarks (F1–F6 and MAX-phase dataset) as used in paper",
            "environment_characteristics": "Designed for continuous unknown functions; seeks broad coverage to reduce surrogate variance; handles noisy outputs implicitly via inclusion of output distance metric.",
            "environment_complexity": "Applied across 1D–10D simulated benchmarks; selection from continuous candidate pools; used 100 queried points in experiments.",
            "uses_adaptive_design": false,
            "performance_with_adaptation": null,
            "performance_without_adaptation": "As pure-exploration baseline: in experiments iGS sometimes achieved low RMSE (e.g., in some 1D problems it was close to BHEEM), but overall BHEEM usually outperformed iGS; reported relative improvements vary (paper cites e.g., BHEEM achieving ~5–21% lower RMSE vs exploration baselines depending on scenario).",
            "sample_efficiency": "Exploration-focused; can require more queries to refine local sharp features compared to exploitation methods — BHEEM often converged faster by balancing exploration/exploitation.",
            "exploration_exploitation_tradeoff": "Pure exploration strategy (no exploitation component).",
            "comparison_methods": "Compared versus BHEEM, QBC, maximum variance, maximum entropy, static/probabilistic trade-offs, random.",
            "key_results": "iGS ensures coverage and good global surrogate behavior but can miss sharp local features (peaks) that exploitation approaches capture; combined in BHEEM it helps provide exploratory component.",
            "limitations_or_failures": "Fails to adequately focus on sharp peaks or discontinuities (underfits highly localized structure); not adaptive to discovered local uncertainty.",
            "uuid": "e1146.2",
            "source_info": {
                "paper_title": "Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "MaxVar",
            "name_full": "Maximum Variance Acquisition",
            "brief_description": "An exploration acquisition that selects the input with the highest predictive variance under the surrogate model (GPR), aiming to minimize future generalization error by reducing model variance.",
            "citation_title": "A variance maximization criterion for active learning",
            "mention_or_use": "use",
            "agent_name": "Maximum Variance",
            "agent_description": "Acquisition uses diagonal elements of the GP posterior covariance (V(x)) and selects x maximizing predictive variance: x* = argmax_x V(x).",
            "adaptive_design_method": "active learning (uncertainty sampling / variance reduction)",
            "adaptation_strategy_description": "Selects points where model predicts largest posterior variance, i.e., likely less-explored regions; does not adapt mixing parameter η.",
            "environment_name": "Used as exploration baseline on same benchmarks",
            "environment_characteristics": "Continuous noisy black-box functions; variance higher in less-explored regions.",
            "environment_complexity": "Applied across varied dimensions in experiments; used with GPR surrogate (Matérn 3/2 in main experiments).",
            "uses_adaptive_design": false,
            "performance_with_adaptation": null,
            "performance_without_adaptation": "Paper reports BHEEM achieved about 5.7% lower average RMSE over maximum-variance exploration baseline in one comparative summary (and ~2.3% lower vs QBC in that pairing).",
            "sample_efficiency": "Exploration method that can be sample-inefficient in focusing on variance-only (may ignore high-error regions with small variance); BHEEM reported better overall efficiency by combining variance exploration with exploitation.",
            "exploration_exploitation_tradeoff": "Pure exploration (uncertainty based).",
            "comparison_methods": "Compared against BHEEM, iGS, QBC, max-entropy, static/probabilistic trade-offs.",
            "key_results": "Works well to explore unobserved regions and reduce model variance but can be suboptimal versus methods that balance exploitation of sharp features.",
            "limitations_or_failures": "May ignore regions where model bias is large but variance is moderate; not sufficient alone to minimize generalization error in presence of sharp local structure.",
            "uuid": "e1146.3",
            "source_info": {
                "paper_title": "Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "MaxEnt",
            "name_full": "Maximum Entropy Sampling (Uncertainty Sampling)",
            "brief_description": "An acquisition function that selects points maximizing Shannon entropy of the GP posterior (equivalently aiming to reduce model uncertainty/information content).",
            "citation_title": "Entropy-based active learning for object recognition",
            "mention_or_use": "use",
            "agent_name": "Maximum Entropy",
            "agent_description": "Computes Shannon entropy of the GP posterior H[f, cov(f)] = 1/2 log |cov(f)| + D/2 log(2πe) and selects x with largest entropy.",
            "adaptive_design_method": "active learning via information-theoretic uncertainty sampling",
            "adaptation_strategy_description": "Queries points with highest posterior entropy to reduce model uncertainty globally; no explicit η adaptation.",
            "environment_name": "Used as exploration baseline on paper benchmarks",
            "environment_characteristics": "Continuous noisy black-box; entropy reflects uncertainty over multivariate GP predictions.",
            "environment_complexity": "Applied across same simulated and real datasets; computational cost grows with covariance matrix size.",
            "uses_adaptive_design": false,
            "performance_with_adaptation": null,
            "performance_without_adaptation": "Generally exploration-biased; often outperformed by BHEEM which dynamically trades off exploration and exploitation.",
            "sample_efficiency": "Exploration-focused; may require more queries to capture sharp localized features depending on GP smoothness assumptions.",
            "exploration_exploitation_tradeoff": "Pure or strong exploration (information-theoretic).",
            "comparison_methods": "Compared against BHEEM, QBC, iGS, max-variance, static/probabilistic strategies.",
            "key_results": "Effective at reducing global uncertainty but can miss local discontinuities if GP kernel smoothness mismatches true function.",
            "limitations_or_failures": "Dependent on GP covariance assumptions; computationally demanding for large candidate pools due to determinant computations.",
            "uuid": "e1146.4",
            "source_info": {
                "paper_title": "Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "StaticTradeoff",
            "name_full": "Static Trade-off (fixed η)",
            "brief_description": "A family of baseline strategies that fixes the linear mixing weight η between exploration and exploitation acquisition functions for the entire learning run.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Static trade-off (fixed η)",
            "agent_description": "Preselect a constant η ∈ {0.25,0.5,0.75} and use x* = argmax_x [η F_exploration + (1−η) F_exploitation] across all iterations.",
            "adaptive_design_method": "none (fixed mixing of acquisition functions)",
            "adaptation_strategy_description": "No adaptation — the exploration/exploitation balance is constant and set a priori (trial-and-error or prior belief).",
            "environment_name": "Same benchmark functions and MAX dataset as used in experiments",
            "environment_characteristics": "Unknown continuous functions; fixed strategy does not respond to observed data dynamics.",
            "environment_complexity": "Same varied dimensionalities; performance depends strongly on problem structure and chosen η.",
            "uses_adaptive_design": false,
            "performance_with_adaptation": null,
            "performance_without_adaptation": "Paper reports that no single fixed η works well for all functions or all iterations; heatmaps show varying RMSE across functions and iterations depending on η. BHEEM outperforms static trade-offs in experiments.",
            "sample_efficiency": "Generally inferior to BHEEM because it cannot adapt to changing needs across query stages.",
            "exploration_exploitation_tradeoff": "Fixed.",
            "comparison_methods": "Compared against BHEEM and probabilistic trade-off.",
            "key_results": "Static choices of η are suboptimal across diverse functions and across time; retrospective tuning (cross-validation) is not helpful for online query selection.",
            "limitations_or_failures": "Non-adaptive; sensitive to prior tuning; poor transfer across different functions.",
            "uuid": "e1146.5",
            "source_info": {
                "paper_title": "Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "ProbTradeoff",
            "name_full": "Probabilistic (decaying) Trade-off",
            "brief_description": "A baseline adaptive strategy that probabilistically chooses exploration vs exploitation each iteration with an exploration probability that decays over time (ϵ-decreasing / annealing-like).",
            "citation_title": "A novel active learning regression framework for balancing the explorationexploitation trade-off",
            "mention_or_use": "use",
            "agent_name": "Probabilistic trade-off (ϵ-decreasing)",
            "agent_description": "At iteration t, exploration is chosen with probability p_R = α^(t−1) (α&lt;1, paper uses α=0.7); a uniform random draw then selects exploration (η=1) or exploitation (η=0) accordingly. This yields an initially exploratory policy that becomes more exploitative over iterations.",
            "adaptive_design_method": "probabilistic annealing of exploration probability",
            "adaptation_strategy_description": "Exploration probability decays geometrically over iterations, independent of observed data; adaptation is time-based rather than data-driven.",
            "environment_name": "Same benchmarks",
            "environment_characteristics": "Unknown noisy continuous functions; this method assumes generic need for more exploration early and exploitation later.",
            "environment_complexity": "As in paper; performance depends on the decay rate α which is fixed (α=0.7) in experiments.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Paper reports probabilistic trade-off is outperformed by BHEEM; heatmap comparisons show BHEEM provides better RMSE across many functions and iterations. No single decay schedule works for all problems.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Better than static in some contexts but inferior to data-driven BHEEM; sample efficiency sensitive to choice of α.",
            "exploration_exploitation_tradeoff": "Time-decaying probabilistic switch (from exploration to exploitation).",
            "comparison_methods": "Compared against BHEEM and static trade-offs in experiments.",
            "key_results": "Probabilistic decays (annealing) are a simple adaptive scheme but their schedule must be tuned to problem; BHEEM's data-driven hierarchical adaptation outperforms fixed decay schedules.",
            "limitations_or_failures": "Decay schedule may be mismatched to the problem leading to too little or too much exploration at critical times; does not use observed data to control transition.",
            "uuid": "e1146.6",
            "source_info": {
                "paper_title": "Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "UCB",
            "name_full": "Upper Confidence Bound",
            "brief_description": "A reinforcement-learning / bandit exploration strategy that trades off mean and uncertainty (confidence bounds) to balance exploration and exploitation.",
            "citation_title": "Using confidence bounds for exploitation-exploration trade-offs",
            "mention_or_use": "mention",
            "agent_name": "Upper Confidence Bound (UCB)",
            "agent_description": "Selection via optimistic estimates: choose action that maximizes μ(x) + κ · σ(x) (mean plus scaled uncertainty), widely used in bandits and Bayesian optimization to balance exploration and exploitation.",
            "adaptive_design_method": "optimistic exploration (confidence-bound based)",
            "adaptation_strategy_description": "Balances exploration/exploitation by augmenting mean predictions with an uncertainty bonus whose scale can decay or be tuned; adaptation relies on posterior uncertainty estimates.",
            "environment_name": "",
            "environment_characteristics": "Mentioned as RL literature example of exploration-exploitation balancing; not experimentally used in this paper.",
            "environment_complexity": "",
            "uses_adaptive_design": null,
            "performance_with_adaptation": null,
            "performance_without_adaptation": null,
            "sample_efficiency": null,
            "exploration_exploitation_tradeoff": "Uncertainty-weighted optimistic selection; tunable bonus parameter κ controls balance.",
            "comparison_methods": "",
            "key_results": "Mentioned as a celebrated approach in RL for balancing exploration versus exploitation; not applied or evaluated in the paper's experiments.",
            "limitations_or_failures": "",
            "uuid": "e1146.7",
            "source_info": {
                "paper_title": "Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Active learning with statistical models",
            "rating": 2,
            "sanitized_title": "active_learning_with_statistical_models"
        },
        {
            "paper_title": "Query-by-committee",
            "rating": 2,
            "sanitized_title": "querybycommittee"
        },
        {
            "paper_title": "A novel active learning regression framework for balancing the explorationexploitation trade-off",
            "rating": 2,
            "sanitized_title": "a_novel_active_learning_regression_framework_for_balancing_the_explorationexploitation_tradeoff"
        },
        {
            "paper_title": "Using confidence bounds for exploitation-exploration trade-offs",
            "rating": 2,
            "sanitized_title": "using_confidence_bounds_for_exploitationexploration_tradeoffs"
        },
        {
            "paper_title": "An adaptive exploration-exploitation algorithm for constructing metamodels in random simulation using a novel sequential experimental design",
            "rating": 1,
            "sanitized_title": "an_adaptive_explorationexploitation_algorithm_for_constructing_metamodels_in_random_simulation_using_a_novel_sequential_experimental_design"
        },
        {
            "paper_title": "A variance maximization criterion for active learning",
            "rating": 1,
            "sanitized_title": "a_variance_maximization_criterion_for_active_learning"
        }
    ],
    "cost": 0.0185665,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling
30 Sep 2023</p>
<p>Upala Junaida Islam uislam@asu.edu 
School of Computing and Augmented Intelligence
Arizona State University
USA</p>
<p>Kamran Paynabar 
H. Milton Stewart School of Industrial &amp; Systems Engineering
Georgia Institute of Technology
USA</p>
<p>George Runger 
School of Computing and Augmented Intelligence
Arizona State University
USA</p>
<p>Ashif Sikandar Iquebal 
School of Computing and Augmented Intelligence
Arizona State University
USA</p>
<p>Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling
30 Sep 2023224DD6367E2AD1F5C329761DB612BCE6arXiv:2304.07665v2[cs.LG]Active learning regressionExploration-exploitation trade-offBayesian hierarchical modelApproximate Bayesian Computation
Active learning provides a framework to adaptively query the most informative experiments towards learning an unknown black-box function.Various approaches of active learning have been proposed in the literature, however, they either focus on exploration or exploitation in the design space.Methods that do consider exploration-exploitation simultaneously employ fixed or ad-hoc measures to control the trade-off that may not be optimal.In this paper, we develop a Bayesian hierarchical approach, referred as BHEEM, to dynamically balance the explorationexploitation trade-off as more data points are queried.To sample from the posterior distribution of the trade-off parameter, We subsequently formulate an approximate Bayesian computation approach based on the linear dependence of queried data in the feature space.Simulated and realworld examples show the proposed approach achieves at least 21% and 11% average improvement when compared to pure exploration and exploitation strategies respectively.More importantly, we note that by optimally balancing the trade-off between exploration and exploitation, BHEEM performs better or at least as well as either pure exploration or pure exploitation.</p>
<p>Introduction</p>
<p>The past decade has witnessed a widespread adoption of machine learning techniques across a range of applications such as design and discovery of novel materials [1], monitoring and control of advanced manufacturing [2], and decision-making in healthcare [3].However, a majority of the success stories of machine learning methods have been limited to fitting large experimental and simulated datasets [4].With rising concerns about resource availability and increasing costs of conducting physical experiments and obtaining labeled datasets, there is a need to develop methodologies that will guide experimental efforts towards gathering the most informative experiments.</p>
<p>Active learning provides a framework to address these challenges by allowing the learning algorithm to adaptively select the most informative experiments in learning a concept (a classification or a regression model), reducing the need for obtaining large datasets [5].Here, the informativeness of data is usually assessed via an acquisition function.Following the seminal work of Angluin on "Queries and Concept Learning" [6] where the acquisition function was a majority vote among a set of candidate hypotheses, several acquisition functions were introduced that are based on entropy [7],</p>
<p>prediction uncertainty [8], etc.However, recent studies have shown that they may have exploration or exploitation bias [9].For instance, a greedy sampling-based acquisition function proposed in [10] (see Equation (11)), tends to acquire the data points uniformly in the unexplored search space and therefore favors exploration, irrespective of the underlying target concept.An example of this is shown in Figure 3(a).In contrast, a Query-by-Committee (QBC) approach for active learning [11] tends to exploit the search space in the neighborhood where the target concept violates the continuity and, in some cases, uniform continuity assumptions.An example is shown in Figure 3(b).This raises the classic, albeit important question: how can we encode exploration-exploitation trade-off in active learning for regression problems.Based on our survey of the literature, we note two prominent schools of thought.The first is based on constructing new acquisition functions or appropriate modifications thereof to balance exploration and exploitation [12].An example of the latter is the hierarchal expected improvement [13] that was introduced in an attempt of achieving non-myopic search in the context of Bayesian optimization.This was accomplished by modifying expected improvement [14] to encourage more exploration.But the most commonly investigated approach is based on optimizing a combination of different acquisition functions for exploration and exploitation through a trade-off parameter [9] as,
x * = argmax x (ηF 1 (x) + (1 − η)F 2 (x))(1)
Here, η ∈ [0, 1] is a parameter controlling the trade-off between F 1 (•) and F 2 (•), the objective functions corresponding to exploration and exploitation acquisition functions respectively, and x * is the optimal data to query according to the new strategy.Nonetheless, the challenge with this approach is that η is unknown and needs to be estimated during the learning process.Since it is difficult to estimate η without observing future data points, existing approaches have relied on trial and error [15] or predefined ad-hoc measures depending on the number of queried data [16].</p>
<p>The exploration-exploitation bias has received attention in the reinforcement learning literature as well.However, unlike active learning, the agent in reinforcement learning is only concerned with maximizing the long-term reward by finding an optimal sequence of actions via explorationexploitation.Conversely, in active learning, we are concerned with maximizing the reward over a finite (often very small) number of experiments that we can perform.The active learner, therefore, needs to iteratively and effectively update the trade-off parameter as new data are queried.</p>
<p>During active learning, the trade-off parameter is influenced by the noise in the labeled data at each query stage and by the variability in the labeled data across different query stages.This bi-level structure allows us to use a Bayesian hierarchical model to find the optimal trade-off parameter during the learning process.Here, the dependency of parameters related to the hierarchy is reflected in a joint probability model.The posterior density and uncertainty quantification of the parameters are obtained via Bayesian inference [17].In the absence of closed-form expressions for the posterior distribution, a sampling-based Markov chain Monte Carlo (MCMC) method is used to obtain the marginal posterior distributions [18].</p>
<p>To this end, we present a Bayesian Hierarchical Model to automatically balancing Exploration-Exploitation (referred as BHEEM) trade-off in actively learning an unknown black box function.In particular, BHEEM iteratively updates the trade-off parameter as data points are queried to minimize the generalization error.By imposing a hierarchical structure, we account for the variability in η arising from within each query iteration as well as across subsequent queries.We present an Approximate Bayesian Computation (ABC) approach along with Metropolis within Gibbs algorithm to sample from the posterior distribution of η.We subscribe to Gaussian Process Regression to obtain the best fit for the underlying black-box function.It is critical to note that the proposed methodology is generalizable and not contingent on the choice of exploration and exploitation or the choice of the regression function.We evaluate the efficacy of BHEEM on six simulated and benchmark datasets, and one real-world example in materials discovery.We also provide sensitivity analysis and numerical convergence results to establish the consistency of BHEEM.</p>
<p>The rest of the paper is organized as follows.Section 2 presents a brief relevant literature review.</p>
<p>Section 3.1 unfolds the Bayesian hierarchical model for estimating the trade-off parameter to balance exploration and exploitation.Section 3.2 explains the Gaussian process regression methodology we employed active learning with.Exploration and exploitation approaches adopted in this work are presented in Section 3.3 followed by trade-off strategies commonly used in the literature summarized in Section 3.4.Experimental setup, results from simulation and real-world experiments, and analysis over sensitivity and convergence are presented in Section 4. The paper is concluded in Section 5.</p>
<p>Literature Review</p>
<p>The roots of active learning can be traced back to the early 1960s with initial works focused on space-filling designs via sequential experimentation [19].Later studies developed model-based sequential experimental designs where an underlying data-generating model is considered to guide the search for the subsequent experiments [20]- [22].Model-based sequential design is commonly referred to as active learning in the machine learning community [5].Over the past two decades, active learning has witnessed significant developments in intelligently querying and labeling data, both in the classification [23]- [25] and regression tasks [10], [11], [26].In this section, we investigate active learning in regression problems with a particular emphasis on the exploration-exploitation trade-off.</p>
<p>Two pioneering works in active learning for regression problems are Active Learning -Mackay (ALM) based on maximizing the expected information gain [27] and Active Learning -Cohn (ALC) based on minimizing the generalization error [8].Mackay defined the information gain as the change of Shannon's entropy [28] before and after labeling data.Entropy has also been formulated as a measure for querying data in classification problems within the framework of "uncertainty sampling" [29].Cohn et al. [8], on the other hand, demonstrated that the expected generalization error is composed of data noise, model bias, and variance.Data noise is independent of the model, and model bias is invariant given a fixed model.Thus the criteria for querying data was simplified to the minimization of the total variance.Meka et al. [30] extended the ALC strategy by adding a regularizing term to integrate the information of both queried and unqueried data.</p>
<p>A more theoretically-motivated active learning strategy called Query-by-Committee (QBC) [31] was first developed for regression by Krogh and Helseby [32].QBC maintains a committee of hypotheses that are simultaneously trained on the labeled data.The data that maximizes the disagreement between the committee members is deemed the most informative.A variation of QBC, Expected Model Change Maximization (EMCM) was also developed in a regression setting where the expected model change was constructed using the gradient of the error concerning candidate data [26].O'Neil et al. [33] compared QBC and EMCM along with a few model-free strategies that select the unlabeled data based on density, or diversity, and through an acquisition function named Exploration Guided Active Learning (EGAL) combining both density and diversity [34].Results</p>
<p>from O'Neal et al. [33] indicated that the integral properties of the dataset, such as its geometry, can be promising candidates for active learning strategies.A similar diversity-based passive sampling strategy, Greedy sampling (GS) was adapted as improved Greedy Sampling (iGS) for active learning by incorporating the response information ensuring exploration in both input and output space [10].</p>
<p>While significant developments have taken place in the active learning literature, only a handful of the efforts have considered the problem of exploration-exploitation trade-off, the majority of which is limited to classification problems.For example, past researches have used maximum entropy [7], and distance and similarity-based metrics for exploration [35] while subscribing to mostly uncertainty and redundancy to exploit near the current decision boundary [36].Approaches to handle the exploration-exploitation bias include switching between exploration and exploitation based on their performance at each iteration [35], explore-then exploit approach [37], random probabilistic measure [16], or combining exploration and exploitation in a predefined ratio [9], [33], [38].Combining the acquisition functions based on cross-validation, sensitivity analysis, or trial-and-error [15] can only be performed retrospectively and does not help with prospective data queries.</p>
<p>Beyond active learning, the exploration-exploitation problem has also been investigated in the reinforcement learning literature.Here, the exploitation of the greedy approach has been confronted with various exploratory approaches.Upper Confidence Bound (UCB) is one of the most celebrated approaches against this bias.Here, uncertainty is used to balance the exploration and exploitation instead of choosing the greedy action according to current knowledge [39].The ϵ−greedy approach decays the randomness or the rate of exploration over the learning process to encourage more exploitation of the observed knowledge [40].The Boltzmann exploration approach uses exponential weighting schemes to balance exploration and exploitation in a probabilistic fashion [41].The balancing parameter has also been obtained using a trial-and-error process [42] or controlled based on a variation of action results and perception of environmental change [43].</p>
<p>Based on the literature review, we notice that the existing methods have either considered either a predefined ratio, probabilistic function or trial and error methods to control the explorationexploitation trade-off.None of the efforts have focused on adaptively updating the trade-off parameter as more data is queried.This study advances the research on dynamically updating the trade-off between exploration and exploitation as more knowledge is gathered about the black-box function from the queried data.</p>
<p>Methodology</p>
<p>In this section, we present the general schema of active learning along with the models and strategies we employ in this study.Let X = {x 1 , x 2 , . ..} and y = {y 1 , y 2 , . ..} represent the possible collection of all design points and their corresponding responses respectively.In the generic problem of active learning, we consider that the actively growing set of queried (i.e., labeled) data
D N = {(x o i , y o i )} N i=1
is accessed by the underlying model.Here, x o i ∈R d and y o i ∈R are the queried data and their corresponding output responses respectively.The objective of active learning in a regression setting is to select the next data x o N +1 from the search space, X, which will be the most informative towards learning the unknown black-box function f (x) given the current knowledge.The response y o N +1 is obtained via simulation or physical experiments.By actively selecting the data to learn from, it can rapidly reduce the generalization error which is the prediction error of the algorithm at the data unobserved so far.The process is iterated until stopping criteria e.g., the maximum number of labeled data, or predicted change, have been satisfied.See Figure 1 for a flow chart of the active learning approach.</p>
<p>Exploration-exploitation trade-off</p>
<p>In machine learning, exploration refers to generating novel information in the search space while exploitation focuses on improving decisions (e.g., maximizing reward) based on the available information.Although this is a generally accepted definition of exploration and exploitation [44], its usage varies depending on the context.Particularly, in the regression setting, exploration refers to sampling data from the unobserved regions in the search space to gather more (potentially novel) information about the black-box function, such as local minima/maxima.In contrast, exploitation aims at accurately capturing the black-box function in the regions where it is highly unpredictable and has a sharp change or discontinuity.As mentioned in the foregoing, it is possible to encode exploration and exploitation separately in an acquisition function [34], however, it is challenging to dynamically balance between the two [45].We consider a general framework for sampling data points that maximizes a linear combination of acquisition functions aimed at simultaneous exploration and exploitation as presented in Equation (1).The next data is queried through pure exploitation when η = 0, and pure exploration when η = 1.By increasing η from 0 to 1, the degree of exploration increases in the constructed acquisition function.</p>
<p>To estimate η dynamically, we consider two levels of variability.First is the variability in queried data emerging from the noise and measurement errors in y at each querying stage.The second level of variability is associated with the dynamic nature of η between querying stages due to the nature of the unknown black-box function (e.g., jumps or discontinuities).This bi-level nature of variability motivates the need for a hierarchical model.A Bayesian hierarchical model allows us to capture this hierarchy while simultaneously encoding the uncertainty at each of the levels.The trade-off parameter η j captures the first level of variability at querying stage j given the set of hyperparameters θ.We model {η j |θ j } iid ∼ p(η|θ j ).Here, the conditional independence of {η j |θ j } tells us that the trade-off parameters at one sampling stage are exchangeable (de Finetti's theorem [46]), but not independent.To capture the variability in η across multiple sampling stages, we model
{θ j |ψ} iid ∼ p(θ|ψ)
where ψ is another set of hyper-parameters fixed from a priori assumptions.</p>
<p>De Finetti's theorem from this conditional independence shows that the trade-off parameters at subsequent querying stages are exchangeable as well.Indeed, this aligns with our assumption that the trade-off parameter at any querying stage is not completely independent of the previous actions (exploration or exploitation).In the next two subsections, we discuss the prior and the sampling distributions employed in this study.</p>
<p>Prior distribution</p>
<p>Towards optimally estimating η using a Bayesian hierarchical framework, we begin by considering a prior distribution based on the knowledge that it lies between 0 (pure exploitation) and 1 (pure exploration).With this knowledge, we let η follow a beta distribution in the form of η ∼ Beta(α, β)</p>
<p>with hyperparameters α and β.The combined selection of α and β defines the shape of the prior distribution of η as observed in Figure 2. A larger value of α shifts the bulk of the probability towards 1 and emphasizes exploration (e.g., α = 5, β = 0.1), whereas an increase in β moves the distributions towards 0 and encourages exploitation (e.g., α = 0.1, β = 5).α = β &lt; 1 creates a U-shape distribution with maximum probability near 0 and 1 imposing a similar higher probability on both exploration and exploitation (e.g., α = β = 0.1).On the other hand, α = β &gt; 1 generates a bell-shape distribution with the maximum probability at the middle (η = 0.5) imposing a lower probability on pure exploration and exploitation, and a higher probability on a mixture of them</p>
<p>Posterior and sampling distribution</p>
<p>The joint posterior distribution of η and the hyper-parameters is, p(η, α, β|y, X) ∝ p(α)p(β)p(η|α, β)p(y, X|η)</p>
<p>where {(X, y)} represents the collection of both labeled and unlabeled data.In Equation (2), p(y, X|η) is the likelihood of η denoted by L(η; y, X), and p(η|α, β) is the prior.The hyperprior p(α)p(β) = p(α|a)p(β|b) due to the independence of α and β as well as a priori selection of a and b.The joint posterior distribution in Equation ( 2) is not available in the closed form due to the intractable likelihood function, p(y, X|η).Therefore, the unknown parameters are sampled from their respective full conditional distributions using Gibbs sampling.The individual full conditional distribution of α, β, η is factorized as,
p(α|β, η) ∝ p(α)p(η|α, β) p(β|α, η) ∝ p(β)p(η|α, β) p(η|α, β, y, X) ∝ p(η|α, β)p(y, X|η)(3)
The Gibbs sampler proceeds by iteratively constructing a dependent sequence of parameter values whose distribution converges to the target joint posterior distribution [47].But the likelihood of η is intractable unlike that of α and β since the generation of new data from observed η does not follow any known distribution.In the absence of a tractable likelihood, we subscribe to Approximate Bayesian Computation (ABC) to approximate the posterior distribution of η which has been used in numerous previous applications where evaluation of the likelihood is expensive or infeasible [48].</p>
<p>The traditional ABC algorithm [49], also known as ABC rejection sampler, follows a two-step procedure to sample from the posterior distribution.First, it draws samples of the unknown parameter from its prior distribution.Second, the sampled parameter values are accepted based on the similarity between the data simulated under some model specified by the sampled parameter values and the queried data, or between the summary statistics thereof.In other words, ABC uses summary statistics to filter samples that do not agree with the queried data.Though efficient in simple probability models, ABC rejection is limited in complex ones where the posterior is greatly different from the prior since it involves sampling from the prior [50].Different computational methods have been presented in the literature to overcome this limitation of ABC inference, among which ABC-MCMC and its variants have been widely used and accepted [50].The ABC-MCMC algorithm combines ABC with Metropolis-Hastings or Metropolis algorithm for iterative approximate Bayesian computation.In the following, we will discuss the Metropolis sampling followed by a new approach to implement ABC and summary statistics into BHEEM.</p>
<p>Metropolis sampling is a simplified form of the Metropolis-Hastings sampling algorithm.It proposes a symmetric proposal distribution J(θ|θ old ) for the parameter vector θ given the most recent accepted sample θ old at each sampling stage.Each proposed sample is either accepted or rejected depending on an acceptance ratio computed from the prior and likelihood of the proposed and old samples.ABC-MCMC algorithm shadows the steps of a typical Metropolis algorithm.The initial sample is drawn from the prior, and the next ones are sampled from a proposal distribution centered at the most recently accepted sample.But the acceptance ratio constructed by ABC uses summary statistics, therefore replacing the likelihood ratio like in the regular Metropolis algorithm.</p>
<p>We initiate the Gibbs sampler with θ (0) = {α (0) , β (0) , η (0) }, and generate a dependent sequence of the parameter vector, {θ (1) , θ (2) , . . ., θ (s) }.Given a current state of the parameters θ (s) = {α (s) , β (s) , η (s) }, the next state of the parameter vector is generated as follows,</p>
<p>Step 1: Sample α (s+1) ∼ p(α|β (s) , η (s) )</p>
<p>Step 2: Sample β (s+1) ∼ p(β|α (s+1) , η (s) )</p>
<p>Step 3: Sample η (s+1) ∼ p(η|α (s+1) , β (s+1) , y, X): As mentioned before, we employ ABC-MCMC at this stage.Similar to a typical Metropolis algorithm, we sample η (s+1) till it converges.</p>
<p>We propose η from a proposal symmetric distribution centered at η in the absence of the likelihood, ABC reconstructs the acceptance ratio r η as,
r η = p η|α (s+1) new , β (s+1) new p η (s+1) old |α (s+1) new , β (s+1) new I{d(S(x), S(X o )) ≥ ν}(4)
where S(x) and S(X o ) are the sufficient statistics for data nominated by η and the queried data respectively, d(•, •) is a distance measure between the two statistics, and I{•} is an indicator function.</p>
<p>In traditional ABC, the fitness of a candidate sample is measured in terms of its similarity to the observed data.However, in active learning, it is measured in terms of the new information generated by each new query.To measure the fitness of candidate queries, we utilize their linear dependency on the already queried data in the Hilbert space.Any data x is considered approximately linearly dependent (ALD) on the already queried set
X o if, δ = min a N −1 i=1 a i ϕ(x o i ) − ϕ(x) 2 ≤ ν(5)
ν is an threshold parameter determining the level of sparsity, and ϕ is a finite-dimensional mapping of the feature vectors, x, to a Hilbert space.If the ALD condition in Equation ( 5) holds, ϕ(x) can be inferred by the already queried data, X o , as shown in the expression that ϕ(x) = i a i ϕ(x o i ).In this case, no new information is gained and we reject the candidate query.In contrast, if the ALD criterion is not satisfied, i.e., δ &gt; ν, then we select the candidate query and collect its response.</p>
<p>However, to use the ALD criterion in order to measure the fitness of candidate queries, we need to define the mapping ϕ.In the absence of the function phi, we invoke Mercer's Theorem [51] that guarantees the existence of a kernel function k(x, x ′ ) such that k = ⟨ϕ(x), ϕ(x ′ )⟩.As a result, all the calculations in the feature space can be performed by defining the kernel function instead of the function ϕ.With the substitution of ϕ with the kernel function, we obtain,
a * = K(X o , X o ) −1 K(X o , x) and δ = K(x, x) − K(X o , x) T a *(6)
For each candidate η proposed by the Metropolis algorithm, we check the linear dependency of the nominated query x.In particular, x is accepted for querying if δ ≥ ν such that I{δ ≥ ν} = 1.In that case, we compare the prior of η and η irrespective of the priors.Authors in [52] experimented with different values for ν using cross-validation.Through the sensitivity analysis ν as discussed in Section 4.5, we fixed ν = 0.001 for all our experimentation.</p>
<p>Step 4: Let θ (s+1) = {α (s+1) , β (s+1) , η (s+1) }.After generating {θ (1) , θ (2) , . . ., θ (s) }, the approximation of η j is obtained by averaging the observed {η
(1) j , η(2)
j , . . ., η
(s) j }.
In this work, we defined the proposal distribution as N (η old , τ 2 ) where η old is the sample parameter accepted in the last iteration, and τ 2 is the variance.The performance of the MCMC chain depends on the variance of proposal distribution, τ 2 .If τ is too small, i.e., 0.001, the Metropolis can get stuck at one sample and may take a long time for the chain to converge.On the other hand, if τ is too large, i.e., 0.25, it is possible to sample highly diverse and sometimes infeasible values increasing the rejection rate as well as hampering the rate of convergence.A detailed sensitivity analysis for the effect of τ on the convergence of the algorithm is discussed in Section 4.4.</p>
<p>Gaussian Process Regression</p>
<p>In the absence of any known functional form of f (x), we consider Gaussian process regression (GPR) as our underlying learning model.GPR considers a distribution over the underlying function f (x) and aims to specify the black-box function by its mean and covariance.Due to the noise inherited in labeled data without the learner's knowledge, the output y comprises f (x) as y = f (x) + ε where ε ∼ N 0, σ 2 n .Observing the noisy outputs, y o , GPR attempts to reconstruct the underlying function f (x) by removing the contaminating noise, ε [53].We denote the queried dataset as (X o , y o ).GPR imposes a zero-mean Gaussian process prior over the noisy outputs such that,
y o ∼ N 0, K(X o , X o ) + σ 2 n I(7)
The joint prior distribution between the training output set y and test output set f is as following,
   y f    ∼ N m   0,    K(X o , X o ) + σ 2 n I K(X o , X u ) K(X u , X o ) K(X u , X u )       (8)
where X u is the set of unlabeled testing points.The posterior distribution at the test data is given
as { f |X o , y o , X u } ∼ N f , cov( f ) where, f = K(X u , X o )[K(X o , X o ) + σ 2 n I] −1 y o (9) cov( f ) = K(X u , X u ) − K(X u , X o )[K(X o , X o ) + σ 2 n I] −1 K(X o , X u )(10)</p>
<p>Acquisition functions</p>
<p>In this section, we present some of the most commonly used acquisition functions and characterize them either as an exploration or an exploitation strategy based on the literature and our extensive simulation studies.</p>
<p>• Improved Greedy Sampling: As mentioned in Section 2, the concept of improved greedy sampling (iGS) was introduced to ensure diversity in the queried data [10].At every iteration, iGS determines the unexplored region by searching over the entire region in both the input and output spaces.It queries the next data which is located the farthest from its nearest training point or in an unobserved region according to the then prediction by the regression model.The acquisition function at x j is calculated as min (u(x j )v(x j )) where u(
x j ) = ∥x j − X o ∥ 2 and v(x j ) = ∥ fj − y o ∥ 2
refer to the distance of x j with training points in input and output space respectively.Here y o is the set of the observed output or the labels at training points X o , fj is the predicted output at test point x j , and ∥ • ∥ 2 is the L2 norm.IGS defines the distance at x j as u(x j )v(x j ) instead of u(x j ) + v(x j ) or (u(x j )) 2 + (v(x j )) 2 due to the possibility of significantly different scale of u and v which can hamper the latter two formulas with the dominance of one measure over another.It then selects the next data, x * iGS , such that,
x * iGS = argmax x j min ∥x j − X o ∥ 2 ∥ fj − y o ∥ 2(11)
By selecting data from unexplored regions in both input and output spaces, iGS avoids sampling from any concentrated region and hence satisfies the requirement for an exploration strategy.</p>
<p>• Query by Committee: Successfully applied in classification and regression-based problems, Query by Committee (QBC) is a framework rooted in the concept of utilizing an ensemble of hypotheses [31].Maintaining a committee of models, QBC queries the data where the committee members disagree the most about a measure of criteria.The committee members are all trained on the same set of training points but with competing hypotheses or regression models.Considering a committee of Q models denoted as h 1 , h 2 , . . ., h Q , we define the measure of disagreement between two models h l and h p at x j as the absolute difference of prediction by the respective models at that point, or |h l (x j )−h p (x j )|.Then the acquisition function at x j is defined as max l,p (|h l (x j )−h p (x j )|), representing the maximum disagreement at x j where l, p = 1, 2, . . ., Q. Then the next data, x * QBC is selected such that,
x * QBC = argmax x j max l,p (|h l (x j ) − h p (x j )|)(12)
QBC approach based on GPR allows us to exploit the regions in the search space where the function's behavior is uncertain i.e., discontinuities or change points.As indicated in [53], the mean square (MS) continuity and differentiability of kernel functions control their flexibility.For example, let us compare GPR models with exponential, Matérn 3/2, Matérn 5/2, and squared exponential kernel functions.The non-differentiable exponential kernel generates the roughest prediction, whereas the infinitely differentiable squared exponential kernel produces the smoothest ones.An intermediate level of smoothness can be observed in Matérn 3/2 and Matérn 5/2 kernels which are one and two times MS differentiable respectively [53].The sharp changes in underlying functions are captured by the rough predictions via dense queries.But the smoother prediction deviates from the underlying model at the region.Therefore, kernel functions with different MS continuity and differentiability behave differently where the functional form is unpredictable with discontinuity or sharp change.When QBC selects the data at which committee members differ the most in their prediction, it keeps exploiting that very region.</p>
<p>• Maximum variance: This strategy defines the acquisition function as the variance predicted by the regression model [54].A learner's expected error can be decomposed into noise, bias, and variance [8].Since the noise is independent of the model or data, and bias is invariant given a fixed model, minimizing the variance is intuitively guaranteed to minimize the future generalization error of the model [5].Equation (10) provides the Gaussian process posterior covariance, and the diagonal elements of cov( f ) provide the predicted variance, V.The next data is queried following,
x * VAR = argmax x j (V(x j ))(13)
Since predicted variance is higher mostly in less-explored regions, it can be implemented as an adequate exploration acquisition function to query the next data [54].</p>
<p>• Maximum entropy: Another active learning strategy formulated based on uncertainty is the maximum entropy strategy [5].Entropy is an information-theoretic measure that often has been defined as the amount of information needed to "encode" a distribution and has been related to the uncertainty of the underlying model [5].Thus minimizing model entropy can reasonably lead to revealing the model uncertainty.Shannon's entropy has been used as an acquisition function named maximum entropy sampling [7] or uncertainty sampling [5].Since the posterior prediction of the Gaussian process follows a multivariate normal distribution, Shannon's entropy of the distribution,
H[ f , cov( f )] = 1 2 log |cov( f )| + D 2 log(2πe)
where D is the dimension of the variable, and f and cov( f ) are the predicted mean and covariance according to Eqs. ( 9) and ( 10) respectively [53].By minimizing the learning model uncertainty, this strategy is also more prone to exploration than exploitation.The maximum entropy strategy queries the data where it has the highest entropy following,
x * ENT = argmax x j H<a href="14"> f , cov( f )</a></p>
<p>Exploration-exploitation trade-off functions</p>
<p>As touched on briefly in the introduction, the existing approaches have tried achieving the trade-off between exploration and exploitation using different methodologies [16], [38], [55], [56].</p>
<p>Here we have listed a few that we will compare BHEEM with.</p>
<p>• Static trade-off: Many of the previous studies have held on to static trade-offs between exploration and exploitation [38].Prior information can be used to decide on the trade-off (e.g., equal importance to both or more importance to one based on the nature of the function).Existing approaches have conducted trial and error with different trade-off values between exploration and exploitation between appropriate exploration and exploitation acquisition functions to conduct the simulated experiments, and then select the one that promises overall better accuracy [55].By fixing the exploration-exploitation trade-off throughout the whole learning process, this method reduces the computation complexity to a great extent.Nevertheless, the same trade-off cannot be expected to demonstrate similar performance for every function (as can be seen in the experimental results).</p>
<p>• Probabilistic trade-off: Inspired by simulated annealing [57] and built on the ϵ−decreasing greedy algorithm [56], this updates the exploration-exploitation combination in a probabilistic approach [16].For instance, the exploration probability is defined as p R = α t−1 where α is less than 1 and t is the current time step or iteration number.To query new data, a uniform random variable Z is generated, if Z ≤ p R , we consider η = 1, and exploration is performed, otherwise, we consider η = 0, and exploitation is applied [16].Hence the strategy starts with pure exploration.</p>
<p>The exploration probability decays gradually, and the learning model gets more prone to pure exploitation over time.Intuitively, this transition is practical since the initial queried data should focus more on exploration, and after learning the overall trend of the function, we can identify the irregular regions via exploitation.However, the transition rate depends on the choice of α which we fixed at 0.7 following Elreedy et al. [16].But again, the transition rate should depend on the nature of the function and fixing it will decrease the efficiency of the learning process.</p>
<p>Experimental results</p>
<p>In this section, we present the experimental setup and focus on the performance evaluation of the proposed methodology.We compare the performance of BHEEM with other active learning strategies over six simulated experiments and one real-world case study for predicting the property of MAX phase materials.Later on, we discuss convergence and sensitivity analysis of the process.</p>
<p>Experimental setup</p>
<p>To demonstrate the efficacy of BHEEM, we employ the Matérn 3/2 kernel function in the GPR model to avoid too rough (exponential kernel) or too wavy (squared exponential kernel) prediction.</p>
<p>The kernel function is defined as,
K ν=3/2 (z) = σ 2 f 1 + √ 3z/l exp − √ 3z/l (15)
where σ 2 f is the signal variance that we fixed at 1, z = ||x − x ′ || 2 , and l is the length-scale parameter optimized by maximizing the log marginal likelihood of the Gaussian process regression model.</p>
<p>While applying the QBC strategy, we maintained a committee of ten Gaussian process models, each with a different kernel function; (i) squared exponential, (ii) exponential, (iii) Matérn 3/2, (iv) Matérn 5/2, (v) rational quadratic, (vi) product of dot product and constant kernel, (vii) product of i and iii, (viii) product of i and iv, (ix) product of ii and iii, (x) product of ii and iv [53].For the generation of the data, we fixed the signal-to-noise (SN) ratio to 10 which we define as the decibel of the ratio of signal power to the power of the data noise.For the performance evaluation, we calculated the root mean squared error as,
RMSE = 1000 k=1 ( f (x k ) − f (x k )) 2 1000 (16)
where f (•) and f (•) represent the true and predicted response respectively at the set of equidistant test points, {x k } 1000 k=1 .We repeated all simulated experiments 100 times to achieve a consistent estimate of the performance.</p>
<p>Simulated experiments</p>
<p>To demonstrate the performance of BHEEM against the existing strategies, we considered the six following functions.
• F 1 (x) =        3.5 exp − (x−10) 2 200 + ϵ, if x ≤ 25 8 − 3.5 exp − (x−35) 2 200 + ϵ, otherwise.(17)• F 2 (x) = sin(x) + 2 exp(−30x 2 ) + ϵ x ∈ <a href="18">−2, 2</a>
• Three-hump camel function:
F 3 (x) = 2x 2 1 − 1.05x 4 1 + x 6 1 /6 + x 1 x 2 + x 2 2 + ϵ x 1 , x 2 ∈ <a href="19">−5, 5</a>
• Six-hump camel function:
F 4 (x) = (4 − 2.1x 2 1 + x 4 1 /3)x 2 1 + x 1 x 2 + (4x 4 2 − 4)x 2 2 + ϵ x 1 , x 2 ∈ <a href="20">−2, 2</a>
• Hartmann 3D function:
F 5 (x) = − 4 i=1 α i exp − 3 j=1 A ij (x j − P ij ) 2 + ϵ x i ∈ [0, 1] ∀i = 1, 2, 3(21)
where α = (1.0,1.2, 3.0, 3.2)
T , A =         3         • F 6 (x) = 1 2 10 i=1 x 2 i − cos (2πx i ) + ϵ x i ∈ [−5, 5] ∀i = 1, 2, . . . , 10(22)
First, we implement BHEEM by employing iGS and QBC as the pure exploration and exploitation strategy respectively.With three random initial queried data, Figure 3 shows the result of iGS, QBC, and BHEEM after the 12 th iteration for the underlying function from Equation (18).Unsurprisingly, iGS spreads out its queried data in both the x and y direction predicting the smooth portions of the function competently, but fails to fit the peak of the function due to the sudden change near x = 0. QBC exploits this uncertainty in predictions across different committee members who lead to different predictions near the sharp peak.Therefore, QBC queries most of the data in this region while ignoring other regions in the process.BHEEM queries data near x = 0 enough times to fit the peak there but has also explored and provided satisfactory prediction in other regions achieving an overall lower error than the other two acquisition functions.17), (b) Equation ( 18), (c) Equation ( 19), (d) Equation ( 20), (e) Equation ( 21), (f) Equation ( 22) as well as average computational time for all strategies.</p>
<p>Nevertheless, BHEEM is either better (Figure 4(b,e)) or at least as accurate as one of the approaches (Figure 4(c)).</p>
<p>Among the one-dimensional problems, BHEEM achieved the lowest RMSE compared to others after the query of the first 10 data in the case of F 1 (x) as observed in Figure 4(a).IGS was the closest one in this case and achieved only about 5% higher RMSE on an average.While predicting F 2 (x), BHEEM converged the fastest and performed the best as shown in Figure 4 (b).The second best strategy was iGS which scored about 60% higher RMSE on average from BHEEM after querying 25 data.Among the two-dimensional problems, in F 3 (x), BHEEM surpassed others most of the time and at the 50 th addition of point, achieved about 8.5% lower RMSE from QBC which scored the closest to BHEEM for the corresponding functions.For F 4 (x), the consistency of lower generalization error and fast convergence persisted for BHEEM.For the three-dimensional F 5 (x), BHEEM and QBC were the two strategies achieving the lowest RMSE across the querying stages.BHEEM was able to achieve significantly lower RMSE than the pure exploration strategy, iGS.In the ten-dimensional problem of F 6 (x) (Figure 4(f)), all the active learning strategies are observed to compete with each other over the learning process.The pure exploration strategy, iGS, is performing better than all other acquisition functions at the 50 th addition of data whereas QBC seems to achieve the lowest RMSE at the 100 th addition.In most of these cases, all active learning strategies performed better than the random sampling strategy.Our result was affected by a few decisions including our choice of kernel, sampling algorithm and proposal distribution, number of points in the initial set of queried data, etc.However, the overall result demonstrates that our proposed approach tends to perform at least tantamount to pure exploration or pure exploitation.</p>
<p>To compare the computational time taken by the applied methodologies, Figure 4 also presents the average time for each methodology to query 100 data during the learning process.Our proposed methodology, BHEEM, takes a significantly larger processing time, about 2.5 and 2 times higher than the duration of iGS and QBC, respectively.Hence, BHEEM is more suitable for offline applications that involve time-consuming physical experiments than the onlineones, which is the case for most applications in manufacturing and materials that involve conducting time consuming physical experiments.We also compare our proposed methodology with static and probabilistic updates of η.For the static trade-off, we considered η = 0.25, 0.5, 0.75 during the learning process individually and calculated the RMSE for each of them.To note, η = 0 and η = 1 refer to pure exploitation and pure exploration respectively which we have already compared with BHEEM in Figure 5 and Figure 6.</p>
<p>For the probabilistic update of η, we followed the strategy described in Section 3.3.2.A heatmap for the average RMSE scaled for each of the comparative trade-off methodologies is presented in Figure 6 where blue and red cells represent more and less accurate models respectively.The heatmap clearly shows that no one value of η works well for every function, or even for every iteration in one function.BHEEM distinctly provides better results than all the static and probabilistic trade-offs.Finally, Figure 7 presents the average improvement achieved by BHEEM from pure exploration and exploitation strategy across the functions.Here, we have used iGS and maximum variance as the exploration strategy in Figure 7(a) and Figure 7(b) respectively, and QBC as the exploitation strategy in both cases.In Figure 7(a), we observe about 7% and 21% lower average RMSE from pure exploration (iGS) and exploitation (QBC) respectively.In Figure 7(b), we observe about 5.7% and 2.3% lower average RMSE from pure exploration (maximum variance) and exploitation (QBC) respectively.Overall, our proposed methodology of trade-off always promises better accuracy than pure exploration and exploitation irrespective of the choice of strategies.</p>
<p>Case study: MAX phase materials</p>
<p>The layered ternary carbides and nitrides with the general formula M n+1 AX n are called MAX phase materials where M and A are early transition metal and A-group elements respectively, whereas X is Nitrogen or Carbon and n is an integer between 1 and 4 [58].Their layered structures kink and delaminate the materials during deformation resulting in an unusual and unique combination of both ceramic and metallic properties which makes them attractive candidates for structural and fuel coating applications.In this study, we intended to predict the lattice constants of MAX phase materials by analyzing their compositions and elastic constants using the data from Aryal et al. [59].The lattice constant is an important piece of information to define the overall lattice structure, which helps model the microstructure evolution.To represent the discrete categorical composition features into numeric input to the models, we used one-hot encoding, a popular approach replacing the categorical variable with as many variables as categories [60].Assuming that we wish to differentiate between two materials with the same elements in M and A, and the same value of n, one of them is a carbide, while the other is a nitride.It is possible to represent this information with two binary variables using one-hot encoding where 0 and 1 represent the non-existence and existence of that element in the material respectively.</p>
<p>Each category value is represented as a 2-dimensional, sparse vector of 1 for one of the dimensions, and 0 for the other.In general, for variables of cardinality d, the one-hot encoding would transfer it to d number of binary variables where each observation indicates the presence (1) or absence (0) of the dichotomous binary variable [60].In the MAX phase problem, we have ten elements in M, twelve elements A, and two elements in X.Hence, after using one-hot encoding and adding the numerical variable for n, we have a total of 25 variables representing the composition of the materials.Including the composition and the elastic constants, there is a total of 30 predictors to predict the lattice constant.</p>
<p>Convergence</p>
<p>In this section, we discuss the convergence of sampled η one querying stage, its relation with the choice of hyperparameters, and the dynamics η during the active learning process.The results in this section are all based on the learning process of F 1 (x).</p>
<p>• Convergence of Metropolis: We visualize the progression of the Metropolis algorithm in</p>
<p>Conclusion</p>
<p>In this work, we address the exploration-exploitation problem in active learning for regression problems.Our approach, BHEEM, is based on dynamically balancing the trade-off between exploration and exploitation during the learning process using a Bayesian hierarchical model.BHEEM captures the variability in the trade-off parameter during each query stages and across successive query stages.In the absence of a likelihood function, we devised an approximate Bayesian computation based on the linear dependence of the data in the Hilbert space to sample from the posterior distribution of the trade-off parameter.We demonstrated and compared BHEEM with exploration, exploitation, and other well-known strategies to balance the two in the six simulated and one real-world case study.From the average percentage improvement in the simulated experiments, BHEEM achieved about 21% and 24% lower average RMSE from pure exploration and exploitation respectively irrespective of the chosen strategies.Overall, when compared to existing active learning approaches, BHEEM converged with fewer iterations in all cases and performed better or at least as effective as the more efficient strategy among the two it combines.The proposed approach to dynamically balancing exploration and exploitation has wide applications in various domains where conducting experiments and labeling data is costly such as materials characterization, mechanical testing, manufacturing, and medical sciences.Some of the limitations of the current studies include the assumption on the filtering distance threshold ν and more importantly, a restricted prior on the trade-off parameter η.In future works, we plan to consider a more expressive and flexible Dirichlet process prior to ensure consistency and faster convergence of the hierarchical model.]</p>
<p>Figure 1 :
1
Figure 1: General schema of an active learner.</p>
<p>Figure 2 :
2
Figure 2: Probability density function for the beta distribution.</p>
<p>new = η with probability min(1, r η ) or rejected η (s+1) new = η (s+1) old with probability 1 − r η where the acceptance ratio, r η , is constructed as, r η = p (η|α, β, y, X) p η (s+1) old |α, β, y, X old But we do not have access to an explicit expression for the likelihood p(y, X|η).To check the fitness of the proposed sample η and to compare it with the current sample η (s+1) old</p>
<p>old before accepting one of them as η (s+1) new .On the other hand, if δ &lt; ν, it makes I{δ ≥ ν} = 0, implying η</p>
<p>Figure 3 :Figure 4
34
Figure 3: (a) Queried data and fitted function using exploration, (b) queried data and fitted function using exploitation, (c) queried data and fitted function using BHEEM with a dynamic trade-off between exploration and exploitation.</p>
<p>Figure 4 :
4
Figure 4: RMSE for BHEEM, iGS, QBC, maximum variance, maximum entropy, and random sampling strategy for (a) Equation (17), (b) Equation (18), (c) Equation (19), (d) Equation (20), (e) Equation (21), (f) Equation (22) as well as average computational time for all strategies.</p>
<p>Figure 5
5
Figure 5 presents the percentage improvement of BHEEM from the pure exploration (iGS) and pure exploitation (QBC) calculated from the average RMSE obtained in the simulated examples.</p>
<p>Figure 5 :
5
Figure 5: Percentage improvement of BHEEM from iGS and QBC acquisition function for the six simulated studies.</p>
<p>Figure 6 :
6
Figure 6: Average RMSE for BHEEM, static trade-off, and probabilistic trade-off after adding the 5th, 10th, and 25th data for the simulated functions.</p>
<p>Figure 7 :
7
Figure 7: (a) Average percentage improvement of BHEEM from pure exploration (iGS) and pure exploitation (QBC) acquisition function across simulated studies, (b) Average percentage improvement of BHEEM from pure exploration (maximum variance) and pure exploitation (QBC) acquisition function across simulated studies.</p>
<p>Figure 8
8
provides a comparative analysis of RMSE of different methodologies employed in this case study.Our proposed methodology (combining iGS and QBC) outperformed the other strategies and surpassed at least one of the pure exploration or exploitation in almost every iteration.BHEEM achieved about 10.4% and 11.3% average improvement from the pure exploration and exploitation strategy respectively.</p>
<p>Figure 8 :
8
Figure 8: Comparison of RMSE for different strategies for Max phase material case study.</p>
<p>Figure 9
9
Figure 9 for a representative simulation example of F 1 (x).It plots the accepted η at one stage of the querying process.The chains were initiated with different ranges of values for η and continued till</p>
<p>Figure 9 :
9
Figure 9: (a) Metropolis sampling chain with first 50 accepted η in one querying iteration with initial samples, (b) Metropolis sampling chain with first 10,000 accepted η in one querying iteration with different initial samples.Different colored plots in each figure represent chains with initial samples at different ranges.(c) Progression of posterior mean of η while learning a function.</p>
<p>Figure 10(c) and Figure 10(d) present the RMSE and the η selected respectively for different values of τ , 0.001, 0.01, 0.1, and 0.25, while keeping ν = 0.001.Here also, we observe the behavior of RMSE and η to be indifferent towards the value of τ .But due to our previous discussion on the relation between τ and the speed of convergence of Metropolis algorithm in the last paragraph of Section 3.1.2,we chose τ = 0.1 throughout all experiments.</p>
<p>Active learning in materials science with emphasis on adaptive sampling using uncertainties for targeted design. T Lookman, Computational Materials. 512019</p>
<p>Machine learning in manufacturing: Advantages, challenges, and applications. T Wuest, Production &amp; Manufacturing Research. 412016</p>
<p>Implementing machine learning in health care-addressing ethical challenges. D S Char, The New England Journal of Medicine. 378119812018</p>
<p>Metaheuristic-based inverse design of materials -A survey. T W Liao, G Li, 10.1016/j.jmat.2020.02.011Journal of Materiomics. 2352-847862Jun. 2020</p>
<p>Active learning literature survey. B Settles, 16482009University of Wisconsin-Madison, Computer SciencesTechnical Report</p>
<p>Queries and concept learning. D Angluin, Machine Learning. 19882</p>
<p>Entropy-based active learning for object recognition. A Holub, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops. IEEE2008</p>
<p>Active learning with statistical models. D A Cohn, Journal of Artificial Intelligence Research. 41996</p>
<p>Active learning for object classification: From exploration to exploitation. N Cebron, M R Berthold, Data Mining and Knowledge Discovery. 182009</p>
<p>Active learning for regression using greedy sampling. D Wu, Information Sciences. 4742019</p>
<p>Active learning for regression based on query by committee. R Burbidge, Lecture Notes in Computer Science. 2007</p>
<p>Stream-based joint exploration-exploitation active learning. C C Loy, 2012 IEEE Conference on Computer Vision and Pattern Recognition. IEEE2012</p>
<p>A hierarchical expected improvement method for bayesian optimization. Z Chen, arXiv:1911.07285Online</p>
<p>Efficient Global Optimization of Expensive Black-Box Functions. D R Jones, 10.1023/A:1008306431147Journal of Global Optimization. 1573-2916134Dec. 1998</p>
<p>An adaptive exploration-exploitation algorithm for constructing metamodels in random simulation using a novel sequential experimental design. A Ajdari, H Mahlooji, Communications in Statistics -Simulation and Computation. 201443</p>
<p>A novel active learning regression framework for balancing the explorationexploitation trade-off. D Elreedy, Entropy. 2172019</p>
<p>Bayesian data analysis. A Gelman, 1995Chapman and Hall/CRC</p>
<p>A bayesian hierarchical model for analysis of single-nucleotide polymorphisms diversity in multilocus, multipopulation samples. F Guo, Journal of the American Statistical Association. 1044852009</p>
<p>Sequential design of experiments. H Chernoff, The Annals of Mathematical Statistics. 3031959</p>
<p>Sequential exploration of complex surfaces using minimum energy designs. V R Joseph, Technometrics. 5712015</p>
<p>Sequential designs based on bayesian uncertainty quantification in sparse representation surrogate modeling. R.-B Chen, Technometrics. 5922017</p>
<p>Strategies for sequential design of experiments and augmentation. L Lu, C M Anderson-Cook, Quality and Reliability Engineering International. 3752021</p>
<p>Hierarchical sampling for active learning. S Dasgupta, D Hsu, Proceedings of the 25th International Conference on Machine Learning. the 25th International Conference on Machine Learning2008</p>
<p>Adaptive active learning for image classification. X Li, Y Guo, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2013</p>
<p>The power of ensembles for active learning in image classification. W H Beluch, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018</p>
<p>Maximizing expected model change for active learning in regression. W Cai, 2013 IEEE 13th International Conference on Data Mining. IEEE2013</p>
<p>Information-based objective functions for active data selection. D J Mackay, Neural Computation. 441992</p>
<p>A mathematical theory of communication. The Bell system technical journal. C E Shannon, 1948</p>
<p>Heterogeneous uncertainty sampling for supervised learning. D D Lewis, J Catlett, Machine Learning Proceedings. Elsevier1994. 1994</p>
<p>An active learning methodology for efficient estimation of expensive noisy black-box functions using gaussian process regression. R Meka, IEEE Access. 82020</p>
<p>Query by committee. H S Seung, 10.1145/130385.130417Proceedings of the fifth annual workshop on Computational learning theory. the fifth annual workshop on Computational learning theoryNew York, NY, USAAssociation for Computing MachineryJul. 1992</p>
<p>Neural network ensembles, cross validation, and active learning. A Krogh, J Vedelsby, Advances in Neural Information Processing Systems. 19957</p>
<p>Model-free and model-based active learning for regression. J O'neill, Advances in Computational Intelligence Systems. Springer2017</p>
<p>Egal: Exploration guided active learning for tcbr. R Hu, International Conference on Case-Based Reasoning. Springer2010</p>
<p>Online choice of active learning algorithms. Y Baram, Journal of Machine Learning Research. 5Mar. 2004</p>
<p>Deep similarity-based batch mode active learning with exploration-exploitation. C Yin, 2017 IEEE International Conference on Data Mining (ICDM). IEEE2017</p>
<p>Imprecise action selection in substance use disorder: Evidence for active learning impairments when solving the explore-exploit dilemma. R Smith, Drug and alcohol dependence. 2152020</p>
<p>Query-by-committee improvement with diversity and density in batch active learning. S Kee, Information Sciences. 4542018</p>
<p>Using confidence bounds for exploitation-exploration trade-offs. P Auer, Journal of Machine Learning Research. 3Nov. 2002</p>
<p>Reinforcement learning for virtual network embedding in wireless sensor networks. H Afifi, H Karl, 2020 16th International Conference on Wireless and Mobile Computing, Networking and Communications (WiMob. IEEE202050308</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, 2018MIT press</p>
<p>Reinforcement learning and evolutionary algorithms for non-stationary multi-armed bandit problems. D E Koulouriotis, A Xanthopoulos, Applied Mathematics and Computation. 19622008</p>
<p>Control of exploitation-exploration meta-parameter in reinforcement learning. S Ishii, Neural Networks. 154-62002</p>
<p>J D Cohen, Should i stay or should i go? how the human brain manages. </p>
<p>Errors and error correction in choice-response tasks. P A Rabbitt, Journal of Experimental Psychology. 7122641966</p>
<p>The concept of exchangeability and its applications. J M Bernardo, Far East Journal of Mathematical Sciences. 41996</p>
<p>A first course in Bayesian statistical methods. P D Hoff, 2009Springer580</p>
<p>Stable bayesian parameter estimation for biological dynamical systems. A G Busetto, J M Buhmann, 2009 International Conference on Computational Science and Engineering, IEEE. 20091</p>
<p>Population growth of human y chromosomes: A study of y chromosome microsatellites. J K Pritchard, Molecular Biology and Evolution. 16121999</p>
<p>Markov chain monte carlo without likelihoods. P Marjoram, Proceedings of the National Academy of Sciences. the National Academy of Sciences2003100328</p>
<p>Xvi. functions of positive and negative type, and their connection the theory of integral equations. J Mercer, Philosophical Transactions of the Royal Society of London. Series A. 209441-4581909</p>
<p>The kernel recursive least-squares algorithm. Y Engel, IEEE Transactions on Signal Processing. 5282004</p>
<p>C K Williams, C E Rasmussen, Gaussian Processes for Machine Learning. The MIT Press2006</p>
<p>A variance maximization criterion for active learning. Y Yang, M Loog, Pattern Recognition. 201878</p>
<p>A new active learning approach for adsorbate-substrate structural elucidation in silico. M P Lourenço, Journal of Molecular Modeling. 2862022</p>
<p>Algorithms for multi-armed bandit problems. V Kuleshov, D Precup, arXiv:1402.60282014arXiv preprint</p>
<p>Simulated annealing. P J Van Laarhoven, E H Aarts, Simulated annealing: Theory and applications. Springer1987</p>
<p>Homoepitaxial growth of ti-si-c max-phase thin films on bulk Ti3SiC2 substrates. P Eklund, Journal of Crystal Growth. 30412007</p>
<p>A genomic approach to the stability, elastic, and electronic properties of the max phases. S , Physica Status Solidi (b). 25182014</p>
<p>A comparative study of categorical variable encoding techniques for neural network classifiers. K Potdar, International Journal of Computer Applications. 17542017</p>            </div>
        </div>

    </div>
</body>
</html>
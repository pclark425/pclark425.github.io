<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9684 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9684</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9684</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-273185876</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.04601v2.pdf" target="_blank">ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation</a></p>
                <p><strong>Paper Abstract:</strong> Automated generation of scientific protocols executable by robots can significantly accelerate scientific research processes. Large Language Models (LLMs) excel at Scientific Protocol Formulation Tasks (SPFT), but the evaluation of their capabilities rely on human evaluation. Here, we propose a flexible, automatic framework to evaluate LLMs' capability on SPFT: ProtoMed-LLM. This framework prompts the target model and GPT-4 to extract pseudocode from biology protocols using only predefined lab actions and evaluates the output of the target model using LLAM-EVAL, the pseudocode generated by GPT-4 serving as a baseline and Llama-3 acting as the evaluator. Our adaptable prompt-based evaluation method, LLAM-EVAL, offers significant flexibility in terms of evaluation model, material, criteria, and is free of cost. We evaluate GPT variations, Llama, Mixtral, Gemma, Cohere, and Gemini. Overall, we find that GPT and Cohere are powerful scientific protocol formulators. We also introduce BIOPROT 2.0, a dataset with biology protocols and corresponding pseudocodes, which can aid LLMs in formulation and evaluation of SPFT. Our work is extensible to assess LLMs on SPFT across various domains and other fields that require protocol generation for specific goals.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9684.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9684.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProtoMed-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ProtoMed-LLM (Automatic Evaluation Framework for LLMs in Medical Protocol Formulation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-based, automated framework introduced in this paper to evaluate LLM capabilities on Scientific Protocol Formulation Tasks (SPFT) by (i) prompting target LLMs to produce pseudocode from protocols using a predefined set of laboratory actions, (ii) obtaining a GPT-4-generated pseudocode baseline, and (iii) scoring target outputs via LLAM-EVAL (an LLM-based evaluator).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4 (baseline), various target models (GPT-3/3.5/4/4o, Llama variants, Mixtral, Gemma, Cohere, Gemini, Claude3)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Framework uses GPT-4 to generate baseline pseudocode and evaluates multiple target LLMs (GPT variants, Llama family, Mixtral, Gemma, Cohere, Gemini, Claude3) under consistent prompting; Llama-3 is used as the evaluator LLM in LLAM-EVAL.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biology / laboratory protocol formulation (biomedical experimental procedures)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automatic three-step pipeline: (1) prompt target LLM to produce pseudocode constrained to a predefined action set; (2) prompt GPT-4 to produce a baseline pseudocode from the same protocol; (3) evaluate target vs baseline using LLAM-EVAL (Llama-3 prompts produce scores over predefined criteria). Reference-based metrics (normalized Levenshtein for function names, BLEU, SciBERTScore, precision/recall) are also reported for compatibility.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Primary LLM-evaluator criteria: Coherence, Consistency, Fluency, Relevance (from G-Eval) plus two domain-specific criteria introduced here: Precision and Coverage; additionally reference-based metrics: Normalized Levenshtein distance (L_dn) for function names, BLEU, SciBERTScore, and precision/recall on function inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>BIOPROT 2.0 — a dataset introduced in this work containing biology protocols and corresponding pseudocode (edited pseudocode) used for training/evaluation; reported statistics: ~300 evaluated protocols in experiments (paper reports #protocols=300 in Table 5 summary stats).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ProtoMed-LLM enabled fully automatic evaluation without manual pseudofunction extraction; Llama-3 was selected as the evaluator (highest self-self scores), and top-performing protocol formulators included GPT-4o and Cohere+ according to LLAM-EVAL. Scores are reported on a 1–5 scale as mean ± std across multiple runs; reference-based metrics (BLEU, SciBERTScore, L_dn) were computed for complementary analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Predefined action set may be incomplete or have insufficiently specified arguments; GPT-4-generated baseline may not equal ground truth; evaluator bias/hallucination risk (results depend on chosen evaluator LLM); BIOPROT 2.0 may be limited in size; domain transfer requires redefining actions; automatic metrics may not reflect real-world executability of protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Compared to prior (manual) frameworks like BioPlanner, ProtoMed-LLM removes manual pseudofunction extraction and human annotation by using a fixed action set and LLM-based evaluation; authors note that traditional automatic metrics correlate modestly with human judgments and that high automatic scores do not guarantee human-perceived quality or experimental validity.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Predefine a finite, expert-reviewed action set to reduce representation variance; use both LLM-based evaluation (LLAM-EVAL) and reference-based metrics for coverage; perform evaluator-model selection via a self-self comparison; supply explicit prompts and allow evaluator to be prompted per-criterion; include automated retry/feedback to ensure numeric evaluator outputs; when porting domains, redefine domain-specific actions and validate evaluator choice.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9684.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9684.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLAM-EVAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLAM-EVAL (Prompt-based LLM evaluation procedure)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic, flexible form-filling evaluation method introduced here that uses an evaluator LLM (Llama-3 in the paper) to rate a target LLM's output against a baseline on predefined discrete scores and criteria, returning expected score = sum_i s_i * p(s_i).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Llama-3 (chosen evaluator); GPT-4 used to create baseline pseudocode for comparison</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>LLAM-EVAL uses Llama-3 prompted per-criterion to produce a distribution over discrete scores (S = {1,..,5} in this work) comparing a target pseudocode to a baseline; the method uses GPT-4 to produce task-specific evaluation steps (CoT-like) that Llama-3 follows when scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biology (protocol pseudocode evaluation), generalizable to other SPFT domains</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Form-filling prompt paradigm: provide evaluator LLM with (ground-truth/baseline text, target text, one evaluation criterion at a time, and stepwise evaluation instructions generated by GPT-4); get probability distribution over discrete scores p(s_i); compute expected score. Automatic retries performed if evaluator fails to return numeric scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Evaluated per-criterion: Coherence, Consistency, Fluency, Relevance, Precision, Coverage (each rated 1–5); prompts include evaluation steps (read baseline, read target, compare line-by-line, assess attributes, assign score).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied on BIOPROT 2.0 protocol/pseudocode pairs; used GPT-4-generated pseudocode and original protocol as two different baselines in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LLAM-EVAL with Llama-3 produced stable numeric evaluations; a self-self comparison procedure showed Llama-3 attained the highest self-self agreement across the six criteria and was selected as the evaluator. The method produced per-model rankings consistent with prior work in relative terms, and identified GPT-4o and Cohere+ as strong formulators.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Evaluator outputs can be non-numeric or verbose (necessitating parameter/prompt tuning and retry loops); evaluator bias (preference for outputs from itself or models used in prompt generation) remains a concern; scores depend on evaluator choice; original G-Eval-style criteria were designed for summarization and may not fully capture protocol executability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>LLAM-EVAL automates what previously required human annotators or manual pseudofunction mapping; it implements some of G-Eval's ideas but adapts them to SPFT and introduces domain-specific criteria (Precision, Coverage). The authors note automatic evaluator scores may not substitute for real-world wet-lab validation.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use a self-self comparison task to select an evaluator LLM (expect near-maximum scores when evaluating identical baseline and target); supply explicit per-criterion evaluation steps (CoT) generated by a high-quality LLM (e.g., GPT-4) and enforce numeric-only outputs; combine LLAM-EVAL with reference-based metrics for complementary perspectives; limit dependence on a single evaluator by testing alternatives when feasible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9684.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9684.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BIOPROT 2.0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BIOPROT 2.0 (Biology protocol and pseudocode dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset introduced in this paper containing biology protocols (title, description, steps) and corresponding pseudocodes constrained to the paper's predefined pseudofunction/action set, curated from protocols.io with automated and manual refinement to support SPFT evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Used with GPT-4 and various target LLMs for pseudocode generation and evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Not an LLM itself; used as evaluation/training data for LLMs. Authors report dataset statistics used in experiments: #protocols reported as 300 in experimental statistics, average tokens/ protocol ≈ 812.3, average steps ≈ 14.8, average tokens/generate pseudocode ≈ 623.8.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biology / experimental protocols</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Serves as the set of protocol inputs for pseudocode generation by target LLMs and for baseline creation (GPT-4) and evaluation via LLAM-EVAL and reference-based metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Not an evaluation method itself; used with LLAM-EVAL criteria (Coherence, Consistency, Fluency, Relevance, Precision, Coverage) and reference metrics (BLEU, L_dn, SciBERTScore, precision/recall).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>BIOPROT 2.0 — curated from protocols.io (≈15k collected pre-refinement; experiments used a refined subset; Table 5 provides statistics and the 'EditedPseudocode' form used for reference metric computation).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Dataset enabled multi-model comparisons; provided inputs for LLAM-EVAL scoring and reference-based metrics. Authors report dataset statistics (tokens, steps, pseudofunction counts) and used it in all evaluation experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Dataset size (number of protocols) may be insufficient for broad evaluation; curated action set may not capture all lab actions; public protocols include duplicated/inconsistent entries and required manual verification; ethical restrictions on using some protocol sources limit inclusion.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>BIOPROT 2.0 aims to replace labor-intensive manual pseudocode annotation by providing standardized protocol–pseudocode pairs constrained to a fixed action set; still requires domain expertise in curation, and does not replace real-world wet-lab validation.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Release edited pseudocode alongside original protocols for reproducibility; use the 'EditedPseudocode' format (pseudofunctions at beginning) to compute reference metrics; expand dataset size and domain coverage and refine action argument specifications for better fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9684.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9684.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation Criteria (SPFT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation Criteria used for Scientific Protocol Formulation Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of criteria applied in LLAM-EVAL for assessing generated pseudocode: Coherence, Consistency, Fluency, Relevance (from G-Eval) plus two domain-specific additions: Precision and Coverage, each rated on a 1–5 scale with detailed definitions and stepwise evaluation instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Evaluator Llama-3 (applies these criteria); GPT-4 used to generate per-criterion evaluation steps</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Criteria are defined and operationalized in prompts for Llama-3; GPT-4 produces chain-of-thought-like evaluation steps which the evaluator follows.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biology / protocol pseudocode evaluation, generalizable to SPFT</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Per-criterion evaluation via LLAM-EVAL: the evaluator reads baseline and target pseudocode, compares them line-by-line per provided evaluation steps, and assigns a numeric rating (1–5) for each criterion. The expected score is computed from the evaluator's score distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Coherence (overall quality and completeness), Consistency (factual alignment to baseline), Fluency (grammar/style), Relevance (selection of important information), Precision (exactness/terminology suited to protocols), Coverage (extent to which all necessary steps/details are represented).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to outputs on BIOPROT 2.0 and to GPT-4-generated pseudocode baselines and original protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Criteria enabled multi-dimensional evaluation; example prompt for 'Coherence' and stepwise rubric are provided in Appendix A.2; combined criterion scores produced model rankings and informed evaluator selection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Original G-Eval criteria were designed for summarization tasks and may not fully reflect procedural executability; some criteria (e.g., Precision, Coverage) require domain-specific calibration; evaluator may produce non-numeric outputs without careful prompt/parameter tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>These automated LLM-based criteria formalize aspects humans assess when judging protocols, but authors note that human-perceived quality and real-world feasibility may diverge from LLM evaluator scores.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Provide explicit, stepwise rubrics for each criterion; use GPT-4 (or similar) to produce evaluation steps that the evaluator LLM follows; rate criteria separately and combine for an overall profile rather than relying on a single aggregate metric.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9684.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9684.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reference-based metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reference-based automatic metrics (BLEU, Normalized Levenshtein L_dn, SciBERTScore, precision/recall)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Conventional textual similarity metrics used as complementary evaluations: BLEU (n-gram overlap), normalized Levenshtein distance for function names, SciBERTScore (cosine similarity of SciBERT embeddings between predicted and baseline inputs), and precision/recall for function inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Used to compare predicted pseudocode or pseudofunction inputs against GPT-4 baseline / edited pseudocode references; not an LLM itself</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Not an LLM; set of automated string/embedding metrics applied to model outputs to provide reference-based quantitative comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>textual evaluation of biology protocol pseudocode (SPFT)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute normalized Levenshtein distance for function names, BLEU for n-gram overlap between generated pseudocode and reference, SciBERTScore using SciBERT sentence encoder for comparing function inputs, and precision/recall for input components.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Strict textual and embedding similarity measures (BLEU, L_dn, SciBERTScore) plus precision/recall for function inputs; reported alongside LLAM-EVAL to ensure compatibility with previous work.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to BIOPROT 2.0 EditedPseudocode (reference format) and to generated pseudocode outputs from target models.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reference metrics are reported in Table 4, showing improvements when using predefined actions in prompts for many models (except recall); authors emphasize that high automatic metric scores do not always map to human judgments or protocol executability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Require manual alignment/annotation in prior pipelines (but in this work authors used EditedPseudocode to automate scoring); these metrics measure surface/form similarity and may miss logical or procedural correctness and real-world executability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>These metrics have modest correlation with human judgments in SPFT according to prior work; authors used them only as compatibility checks alongside the primary LLAM-EVAL method.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use reference-based metrics as complementary signals to LLM-based evaluation; format references consistently (e.g., EditedPseudocode with pseudofunctions at beginning) to improve metric applicability; interpret results cautiously for procedural correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9684.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9684.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluator selection (self-self)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluator LLM selection via self-self comparison</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedure used in the paper to choose an evaluator LLM by having each candidate LLM generate a pseudocode and then evaluate that same pseudocode; the preferred evaluator is the one that assigns near-maximum scores when baseline and target are identical.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Candidates tested included GPT-4, GPT-4o, GPT-3.5, Llama-3 variants, Mixtral, Gemma, Cohere, Gemini; Llama-3 was selected as the evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Selection method measures self-agreement: an LLM should return high scores when asked to evaluate identical baseline and target pseudocode; authors ran self-self tasks across multiple models and criteria to pick Llama-3.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>evaluation methodology for SPFT</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>For each candidate evaluator LLM, generate pseudocode and use the same model to evaluate it (baseline == target). Assess mean and std of returned scores across the six criteria; choose model with scores closest to maximum and stable outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same six criteria used in LLAM-EVAL (Coherence, Consistency, Fluency, Relevance, Precision, Coverage).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to a larger dataset used for self-self comparison (details in Appendix A.3) and to the BIOPROT 2.0-derived test set for evaluator selection experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Llama-3 achieved the highest self-self scores across the six criteria and was therefore selected as the evaluator; some models failed to return numeric scores reliably and were excluded.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Self-self comparison may favor models that prefer their own outputs and could hide evaluator biases; numerical-output instability requires prompt/parameter tuning and retry loops; not all candidate LLMs produce numerical-only outputs reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>This is an automated alternative to manual evaluator selection (human calibration), trading human validation for an empirical automatic consistency check.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use self-self comparison as an initial filter for evaluator choice, but also validate evaluator behavior across baselines from other models and the original protocol to detect bias; implement prompt/parameter adjustments and automatic retries to force numeric outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9684.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9684.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reported results & findings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Key experimental results and empirical findings from ProtoMed-LLM experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Summary of empirical outcomes: Llama-3 selected as evaluator; GPT-4o and Cohere+ performed best at protocol pseudocode generation under LLAM-EVAL; predefined action prompts improved performance for most models; original protocol can be used as baseline though scores are lower than GPT-4 baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Evaluated models include GPT-4/GPT-4o/GPT-3.5, Llama-3 (8b, 70b), Mixtral, Gemma, Cohere+, Cohere, Gemini family, Claude3</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Paper reports mean ± std scores (1–5) across five runs per model per task using LLAM-EVAL; reference-based metric results (BLEU, SciBERTScore, L_dn, precision/recall) are also provided.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biology / SPFT</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>LLAM-EVAL scoring with GPT-4 pseudocode baseline (task 1), same task with no predefined actions (task 2), and using original protocol as baseline with predefined actions (task 3); complementary reference-based metrics computed.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Coherence, Consistency, Fluency, Relevance, Precision, Coverage (1–5 scale); reference metrics reported separately.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Experiments run on BIOPROT 2.0 subset (paper reports #protocols=300 in statistics); results aggregated as mean ± std over multiple runs.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Llama-3 chosen as evaluator after self-self tests. GPT-4o and Cohere+ achieved top average LLAM-EVAL scores across the six criteria (paper highlights them as powerful protocol formulators). Using predefined action lists in prompts improved most models' performance (except recall). Evaluating against the original protocol baseline yielded lower absolute scores but preserved relative model ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Quantitative scores depend on evaluator choice and baseline source (GPT-4 baseline vs original protocol) and may not reflect experimental executability; some models failed to produce numeric evaluator outputs; predefined actions improved some metrics but had mixed effect on recall; dataset size and action definitions limit generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Relative model rankings are compatible with prior human-involved evaluation (BioPlanner), but LLAM-EVAL automates evaluation and reduces manual extraction; authors caution that automated scores cannot replace physical experiment validation or detailed human review of protocol feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Report both LLM-evaluator scores and reference-based metrics; test with/without predefined actions to assess domain-knowledge effects; consider multiple baselines (GPT-generated pseudocode, original protocol); validate evaluator choice (self-self and cross-model checks) and be wary of evaluator/model biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>G-eval: NLG evaluation using GPT-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>BioPlanner: Automatic evaluation of LLMs on protocol planning in biology <em>(Rating: 2)</em></li>
                <li>Automated extraction of chemical synthesis actions from experimental procedures <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9684",
    "paper_id": "paper-273185876",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "ProtoMed-LLM",
            "name_full": "ProtoMed-LLM (Automatic Evaluation Framework for LLMs in Medical Protocol Formulation)",
            "brief_description": "A prompt-based, automated framework introduced in this paper to evaluate LLM capabilities on Scientific Protocol Formulation Tasks (SPFT) by (i) prompting target LLMs to produce pseudocode from protocols using a predefined set of laboratory actions, (ii) obtaining a GPT-4-generated pseudocode baseline, and (iii) scoring target outputs via LLAM-EVAL (an LLM-based evaluator).",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4 (baseline), various target models (GPT-3/3.5/4/4o, Llama variants, Mixtral, Gemma, Cohere, Gemini, Claude3)",
            "llm_description": "Framework uses GPT-4 to generate baseline pseudocode and evaluates multiple target LLMs (GPT variants, Llama family, Mixtral, Gemma, Cohere, Gemini, Claude3) under consistent prompting; Llama-3 is used as the evaluator LLM in LLAM-EVAL.",
            "scientific_domain": "biology / laboratory protocol formulation (biomedical experimental procedures)",
            "evaluation_method": "Automatic three-step pipeline: (1) prompt target LLM to produce pseudocode constrained to a predefined action set; (2) prompt GPT-4 to produce a baseline pseudocode from the same protocol; (3) evaluate target vs baseline using LLAM-EVAL (Llama-3 prompts produce scores over predefined criteria). Reference-based metrics (normalized Levenshtein for function names, BLEU, SciBERTScore, precision/recall) are also reported for compatibility.",
            "evaluation_criteria": "Primary LLM-evaluator criteria: Coherence, Consistency, Fluency, Relevance (from G-Eval) plus two domain-specific criteria introduced here: Precision and Coverage; additionally reference-based metrics: Normalized Levenshtein distance (L_dn) for function names, BLEU, SciBERTScore, and precision/recall on function inputs.",
            "benchmark_or_dataset": "BIOPROT 2.0 — a dataset introduced in this work containing biology protocols and corresponding pseudocode (edited pseudocode) used for training/evaluation; reported statistics: ~300 evaluated protocols in experiments (paper reports #protocols=300 in Table 5 summary stats).",
            "results_summary": "ProtoMed-LLM enabled fully automatic evaluation without manual pseudofunction extraction; Llama-3 was selected as the evaluator (highest self-self scores), and top-performing protocol formulators included GPT-4o and Cohere+ according to LLAM-EVAL. Scores are reported on a 1–5 scale as mean ± std across multiple runs; reference-based metrics (BLEU, SciBERTScore, L_dn) were computed for complementary analysis.",
            "limitations_or_challenges": "Predefined action set may be incomplete or have insufficiently specified arguments; GPT-4-generated baseline may not equal ground truth; evaluator bias/hallucination risk (results depend on chosen evaluator LLM); BIOPROT 2.0 may be limited in size; domain transfer requires redefining actions; automatic metrics may not reflect real-world executability of protocols.",
            "comparison_to_human_or_traditional": "Compared to prior (manual) frameworks like BioPlanner, ProtoMed-LLM removes manual pseudofunction extraction and human annotation by using a fixed action set and LLM-based evaluation; authors note that traditional automatic metrics correlate modestly with human judgments and that high automatic scores do not guarantee human-perceived quality or experimental validity.",
            "recommendations_or_best_practices": "Predefine a finite, expert-reviewed action set to reduce representation variance; use both LLM-based evaluation (LLAM-EVAL) and reference-based metrics for coverage; perform evaluator-model selection via a self-self comparison; supply explicit prompts and allow evaluator to be prompted per-criterion; include automated retry/feedback to ensure numeric evaluator outputs; when porting domains, redefine domain-specific actions and validate evaluator choice.",
            "uuid": "e9684.0",
            "source_info": {
                "paper_title": "ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLAM-EVAL",
            "name_full": "LLAM-EVAL (Prompt-based LLM evaluation procedure)",
            "brief_description": "An automatic, flexible form-filling evaluation method introduced here that uses an evaluator LLM (Llama-3 in the paper) to rate a target LLM's output against a baseline on predefined discrete scores and criteria, returning expected score = sum_i s_i * p(s_i).",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Llama-3 (chosen evaluator); GPT-4 used to create baseline pseudocode for comparison",
            "llm_description": "LLAM-EVAL uses Llama-3 prompted per-criterion to produce a distribution over discrete scores (S = {1,..,5} in this work) comparing a target pseudocode to a baseline; the method uses GPT-4 to produce task-specific evaluation steps (CoT-like) that Llama-3 follows when scoring.",
            "scientific_domain": "biology (protocol pseudocode evaluation), generalizable to other SPFT domains",
            "evaluation_method": "Form-filling prompt paradigm: provide evaluator LLM with (ground-truth/baseline text, target text, one evaluation criterion at a time, and stepwise evaluation instructions generated by GPT-4); get probability distribution over discrete scores p(s_i); compute expected score. Automatic retries performed if evaluator fails to return numeric scores.",
            "evaluation_criteria": "Evaluated per-criterion: Coherence, Consistency, Fluency, Relevance, Precision, Coverage (each rated 1–5); prompts include evaluation steps (read baseline, read target, compare line-by-line, assess attributes, assign score).",
            "benchmark_or_dataset": "Applied on BIOPROT 2.0 protocol/pseudocode pairs; used GPT-4-generated pseudocode and original protocol as two different baselines in experiments.",
            "results_summary": "LLAM-EVAL with Llama-3 produced stable numeric evaluations; a self-self comparison procedure showed Llama-3 attained the highest self-self agreement across the six criteria and was selected as the evaluator. The method produced per-model rankings consistent with prior work in relative terms, and identified GPT-4o and Cohere+ as strong formulators.",
            "limitations_or_challenges": "Evaluator outputs can be non-numeric or verbose (necessitating parameter/prompt tuning and retry loops); evaluator bias (preference for outputs from itself or models used in prompt generation) remains a concern; scores depend on evaluator choice; original G-Eval-style criteria were designed for summarization and may not fully capture protocol executability.",
            "comparison_to_human_or_traditional": "LLAM-EVAL automates what previously required human annotators or manual pseudofunction mapping; it implements some of G-Eval's ideas but adapts them to SPFT and introduces domain-specific criteria (Precision, Coverage). The authors note automatic evaluator scores may not substitute for real-world wet-lab validation.",
            "recommendations_or_best_practices": "Use a self-self comparison task to select an evaluator LLM (expect near-maximum scores when evaluating identical baseline and target); supply explicit per-criterion evaluation steps (CoT) generated by a high-quality LLM (e.g., GPT-4) and enforce numeric-only outputs; combine LLAM-EVAL with reference-based metrics for complementary perspectives; limit dependence on a single evaluator by testing alternatives when feasible.",
            "uuid": "e9684.1",
            "source_info": {
                "paper_title": "ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "BIOPROT 2.0",
            "name_full": "BIOPROT 2.0 (Biology protocol and pseudocode dataset)",
            "brief_description": "A dataset introduced in this paper containing biology protocols (title, description, steps) and corresponding pseudocodes constrained to the paper's predefined pseudofunction/action set, curated from protocols.io with automated and manual refinement to support SPFT evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Used with GPT-4 and various target LLMs for pseudocode generation and evaluation",
            "llm_description": "Not an LLM itself; used as evaluation/training data for LLMs. Authors report dataset statistics used in experiments: #protocols reported as 300 in experimental statistics, average tokens/ protocol ≈ 812.3, average steps ≈ 14.8, average tokens/generate pseudocode ≈ 623.8.",
            "scientific_domain": "biology / experimental protocols",
            "evaluation_method": "Serves as the set of protocol inputs for pseudocode generation by target LLMs and for baseline creation (GPT-4) and evaluation via LLAM-EVAL and reference-based metrics.",
            "evaluation_criteria": "Not an evaluation method itself; used with LLAM-EVAL criteria (Coherence, Consistency, Fluency, Relevance, Precision, Coverage) and reference metrics (BLEU, L_dn, SciBERTScore, precision/recall).",
            "benchmark_or_dataset": "BIOPROT 2.0 — curated from protocols.io (≈15k collected pre-refinement; experiments used a refined subset; Table 5 provides statistics and the 'EditedPseudocode' form used for reference metric computation).",
            "results_summary": "Dataset enabled multi-model comparisons; provided inputs for LLAM-EVAL scoring and reference-based metrics. Authors report dataset statistics (tokens, steps, pseudofunction counts) and used it in all evaluation experiments reported.",
            "limitations_or_challenges": "Dataset size (number of protocols) may be insufficient for broad evaluation; curated action set may not capture all lab actions; public protocols include duplicated/inconsistent entries and required manual verification; ethical restrictions on using some protocol sources limit inclusion.",
            "comparison_to_human_or_traditional": "BIOPROT 2.0 aims to replace labor-intensive manual pseudocode annotation by providing standardized protocol–pseudocode pairs constrained to a fixed action set; still requires domain expertise in curation, and does not replace real-world wet-lab validation.",
            "recommendations_or_best_practices": "Release edited pseudocode alongside original protocols for reproducibility; use the 'EditedPseudocode' format (pseudofunctions at beginning) to compute reference metrics; expand dataset size and domain coverage and refine action argument specifications for better fidelity.",
            "uuid": "e9684.2",
            "source_info": {
                "paper_title": "ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Evaluation Criteria (SPFT)",
            "name_full": "Evaluation Criteria used for Scientific Protocol Formulation Tasks",
            "brief_description": "Set of criteria applied in LLAM-EVAL for assessing generated pseudocode: Coherence, Consistency, Fluency, Relevance (from G-Eval) plus two domain-specific additions: Precision and Coverage, each rated on a 1–5 scale with detailed definitions and stepwise evaluation instructions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Evaluator Llama-3 (applies these criteria); GPT-4 used to generate per-criterion evaluation steps",
            "llm_description": "Criteria are defined and operationalized in prompts for Llama-3; GPT-4 produces chain-of-thought-like evaluation steps which the evaluator follows.",
            "scientific_domain": "biology / protocol pseudocode evaluation, generalizable to SPFT",
            "evaluation_method": "Per-criterion evaluation via LLAM-EVAL: the evaluator reads baseline and target pseudocode, compares them line-by-line per provided evaluation steps, and assigns a numeric rating (1–5) for each criterion. The expected score is computed from the evaluator's score distribution.",
            "evaluation_criteria": "Coherence (overall quality and completeness), Consistency (factual alignment to baseline), Fluency (grammar/style), Relevance (selection of important information), Precision (exactness/terminology suited to protocols), Coverage (extent to which all necessary steps/details are represented).",
            "benchmark_or_dataset": "Applied to outputs on BIOPROT 2.0 and to GPT-4-generated pseudocode baselines and original protocols.",
            "results_summary": "Criteria enabled multi-dimensional evaluation; example prompt for 'Coherence' and stepwise rubric are provided in Appendix A.2; combined criterion scores produced model rankings and informed evaluator selection.",
            "limitations_or_challenges": "Original G-Eval criteria were designed for summarization tasks and may not fully reflect procedural executability; some criteria (e.g., Precision, Coverage) require domain-specific calibration; evaluator may produce non-numeric outputs without careful prompt/parameter tuning.",
            "comparison_to_human_or_traditional": "These automated LLM-based criteria formalize aspects humans assess when judging protocols, but authors note that human-perceived quality and real-world feasibility may diverge from LLM evaluator scores.",
            "recommendations_or_best_practices": "Provide explicit, stepwise rubrics for each criterion; use GPT-4 (or similar) to produce evaluation steps that the evaluator LLM follows; rate criteria separately and combine for an overall profile rather than relying on a single aggregate metric.",
            "uuid": "e9684.3",
            "source_info": {
                "paper_title": "ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Reference-based metrics",
            "name_full": "Reference-based automatic metrics (BLEU, Normalized Levenshtein L_dn, SciBERTScore, precision/recall)",
            "brief_description": "Conventional textual similarity metrics used as complementary evaluations: BLEU (n-gram overlap), normalized Levenshtein distance for function names, SciBERTScore (cosine similarity of SciBERT embeddings between predicted and baseline inputs), and precision/recall for function inputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "Used to compare predicted pseudocode or pseudofunction inputs against GPT-4 baseline / edited pseudocode references; not an LLM itself",
            "llm_description": "Not an LLM; set of automated string/embedding metrics applied to model outputs to provide reference-based quantitative comparisons.",
            "scientific_domain": "textual evaluation of biology protocol pseudocode (SPFT)",
            "evaluation_method": "Compute normalized Levenshtein distance for function names, BLEU for n-gram overlap between generated pseudocode and reference, SciBERTScore using SciBERT sentence encoder for comparing function inputs, and precision/recall for input components.",
            "evaluation_criteria": "Strict textual and embedding similarity measures (BLEU, L_dn, SciBERTScore) plus precision/recall for function inputs; reported alongside LLAM-EVAL to ensure compatibility with previous work.",
            "benchmark_or_dataset": "Applied to BIOPROT 2.0 EditedPseudocode (reference format) and to generated pseudocode outputs from target models.",
            "results_summary": "Reference metrics are reported in Table 4, showing improvements when using predefined actions in prompts for many models (except recall); authors emphasize that high automatic metric scores do not always map to human judgments or protocol executability.",
            "limitations_or_challenges": "Require manual alignment/annotation in prior pipelines (but in this work authors used EditedPseudocode to automate scoring); these metrics measure surface/form similarity and may miss logical or procedural correctness and real-world executability.",
            "comparison_to_human_or_traditional": "These metrics have modest correlation with human judgments in SPFT according to prior work; authors used them only as compatibility checks alongside the primary LLAM-EVAL method.",
            "recommendations_or_best_practices": "Use reference-based metrics as complementary signals to LLM-based evaluation; format references consistently (e.g., EditedPseudocode with pseudofunctions at beginning) to improve metric applicability; interpret results cautiously for procedural correctness.",
            "uuid": "e9684.4",
            "source_info": {
                "paper_title": "ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Evaluator selection (self-self)",
            "name_full": "Evaluator LLM selection via self-self comparison",
            "brief_description": "A procedure used in the paper to choose an evaluator LLM by having each candidate LLM generate a pseudocode and then evaluate that same pseudocode; the preferred evaluator is the one that assigns near-maximum scores when baseline and target are identical.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Candidates tested included GPT-4, GPT-4o, GPT-3.5, Llama-3 variants, Mixtral, Gemma, Cohere, Gemini; Llama-3 was selected as the evaluator",
            "llm_description": "Selection method measures self-agreement: an LLM should return high scores when asked to evaluate identical baseline and target pseudocode; authors ran self-self tasks across multiple models and criteria to pick Llama-3.",
            "scientific_domain": "evaluation methodology for SPFT",
            "evaluation_method": "For each candidate evaluator LLM, generate pseudocode and use the same model to evaluate it (baseline == target). Assess mean and std of returned scores across the six criteria; choose model with scores closest to maximum and stable outputs.",
            "evaluation_criteria": "Same six criteria used in LLAM-EVAL (Coherence, Consistency, Fluency, Relevance, Precision, Coverage).",
            "benchmark_or_dataset": "Applied to a larger dataset used for self-self comparison (details in Appendix A.3) and to the BIOPROT 2.0-derived test set for evaluator selection experiments.",
            "results_summary": "Llama-3 achieved the highest self-self scores across the six criteria and was therefore selected as the evaluator; some models failed to return numeric scores reliably and were excluded.",
            "limitations_or_challenges": "Self-self comparison may favor models that prefer their own outputs and could hide evaluator biases; numerical-output instability requires prompt/parameter tuning and retry loops; not all candidate LLMs produce numerical-only outputs reliably.",
            "comparison_to_human_or_traditional": "This is an automated alternative to manual evaluator selection (human calibration), trading human validation for an empirical automatic consistency check.",
            "recommendations_or_best_practices": "Use self-self comparison as an initial filter for evaluator choice, but also validate evaluator behavior across baselines from other models and the original protocol to detect bias; implement prompt/parameter adjustments and automatic retries to force numeric outputs.",
            "uuid": "e9684.5",
            "source_info": {
                "paper_title": "ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Reported results & findings",
            "name_full": "Key experimental results and empirical findings from ProtoMed-LLM experiments",
            "brief_description": "Summary of empirical outcomes: Llama-3 selected as evaluator; GPT-4o and Cohere+ performed best at protocol pseudocode generation under LLAM-EVAL; predefined action prompts improved performance for most models; original protocol can be used as baseline though scores are lower than GPT-4 baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Evaluated models include GPT-4/GPT-4o/GPT-3.5, Llama-3 (8b, 70b), Mixtral, Gemma, Cohere+, Cohere, Gemini family, Claude3",
            "llm_description": "Paper reports mean ± std scores (1–5) across five runs per model per task using LLAM-EVAL; reference-based metric results (BLEU, SciBERTScore, L_dn, precision/recall) are also provided.",
            "scientific_domain": "biology / SPFT",
            "evaluation_method": "LLAM-EVAL scoring with GPT-4 pseudocode baseline (task 1), same task with no predefined actions (task 2), and using original protocol as baseline with predefined actions (task 3); complementary reference-based metrics computed.",
            "evaluation_criteria": "Coherence, Consistency, Fluency, Relevance, Precision, Coverage (1–5 scale); reference metrics reported separately.",
            "benchmark_or_dataset": "Experiments run on BIOPROT 2.0 subset (paper reports #protocols=300 in statistics); results aggregated as mean ± std over multiple runs.",
            "results_summary": "Llama-3 chosen as evaluator after self-self tests. GPT-4o and Cohere+ achieved top average LLAM-EVAL scores across the six criteria (paper highlights them as powerful protocol formulators). Using predefined action lists in prompts improved most models' performance (except recall). Evaluating against the original protocol baseline yielded lower absolute scores but preserved relative model ranking.",
            "limitations_or_challenges": "Quantitative scores depend on evaluator choice and baseline source (GPT-4 baseline vs original protocol) and may not reflect experimental executability; some models failed to produce numeric evaluator outputs; predefined actions improved some metrics but had mixed effect on recall; dataset size and action definitions limit generalization.",
            "comparison_to_human_or_traditional": "Relative model rankings are compatible with prior human-involved evaluation (BioPlanner), but LLAM-EVAL automates evaluation and reduces manual extraction; authors caution that automated scores cannot replace physical experiment validation or detailed human review of protocol feasibility.",
            "recommendations_or_best_practices": "Report both LLM-evaluator scores and reference-based metrics; test with/without predefined actions to assess domain-knowledge effects; consider multiple baselines (GPT-generated pseudocode, original protocol); validate evaluator choice (self-self and cross-model checks) and be wary of evaluator/model biases.",
            "uuid": "e9684.6",
            "source_info": {
                "paper_title": "ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "G-eval: NLG evaluation using GPT-4 with better human alignment",
            "rating": 2,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        },
        {
            "paper_title": "BioPlanner: Automatic evaluation of LLMs on protocol planning in biology",
            "rating": 2,
            "sanitized_title": "bioplanner_automatic_evaluation_of_llms_on_protocol_planning_in_biology"
        },
        {
            "paper_title": "Automated extraction of chemical synthesis actions from experimental procedures",
            "rating": 2,
            "sanitized_title": "automated_extraction_of_chemical_synthesis_actions_from_experimental_procedures"
        }
    ],
    "cost": 0.01700775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation
11 Apr 2025</p>
<p>Seungjun Yi 
Department of Biomedical Engineering
University of Texas at Austin</p>
<p>Korea Institute of Science and Technology (KIST) Europe</p>
<p>Jaeyoung Lim 
Department of Computer Science and Engineering
Ulsan National Institute of Science and Technology</p>
<p>Korea Institute of Science and Technology (KIST) Europe</p>
<p>Juyong Yoon juyong.yoon@kist-europe.de 
Korea Institute of Science and Technology (KIST) Europe</p>
<p>ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation
11 Apr 2025895FB3CFD91D205F23A90ADE1BE2426CarXiv:2410.04601v2[cs.CL]
Automated generation of scientific protocols executable by robots can significantly accelerate scientific research processes.Large Language Models (LLMs) excel at Scientific Protocol Formulation Tasks (SPFT), but the evaluation of their capabilities rely on human evaluation.Here, we propose a flexible, automatic framework to evaluate LLMs' capability on SPFT: ProtoMed-LLM 1 .This framework prompts the target model and GPT-4 to extract pseudocode from biology protocols using only predefined lab actions and evaluates the output of target model using LLAM-EVAL, the pseudocode generated by GPT-4 serving as a baseline and Llama-3 acting as the evaluator.Our adaptable prompt-based evaluation method, LLAM-EVAL, offers significant flexibility in terms of evaluation model, material, criteria, and is free of cost.We evaluate GPT variations, Llama, Mixtral, Gemma, Cohere, and Gemini.Overall, we find that GPT and Cohere is a powerful scientific protocol formulators.We also introduce BIOPROT 2.0, a dataset with biology protocols and corresponding pseudocodes, which can aid LLMs in formulation and evaluation of SPFT.Our work is extensible to assess LLMs on SPFT across various domains and other fields that require protocol generation for specific goals.Previous work suggests a framework to assess the capabilities of LLMs on SPFT: BioPlanner (O' Donoghue et al., 2023).This method outlines three primary steps: (i) extracting pseudofunctions</p>
<p>Introduction</p>
<p>Laboratory automation is essential for accelerating scientific research processes.However, most contemporary laboratories use manual labor, especially in the field of biology.This not only constrains the scope for scalability, but also introduces potential vulnerabilities in reproducibility (Kwok, 2010).</p>
<p>One of the barriers for automation in biology is the reliance on manual experiments when validating scientific protocols.Traditionally, trial-anderror approach has been employed to formulate A protocol containing a title, descriptions, stepby-step instructions, and predefined biology lab actions is given to both a target model and GPT-4 for pseudocode generation.Then, Llama-3 evaluates these outputs considering the target model's pseudocode as the prediction (ŷ) and GPT-4's as a baseline (y).</p>
<p>a protocol to achieve a certain goal.As a breakthrough, LLMs have demonstrated remarkable capabilities in formulating precise experimental protocols across diverse fields (White et al., 2023;Jablonka et al., 2023).These protocols comprise pseudocodes with actionable sequences that can be executed by machines which can be automated.Yet, efforts in biology to utilize LLMs for pseudocode formulation are yet to achieve desired outcomes (Inagaki et al., 2023).These works rely on human evaluations, and objective evaluation methods for protocol formulation are nonexistent.Therefore, it is necessary to establish an automated evaluation framework on formulating protocols to move beyond manual labor.and pseudocode2 from a protocol using an evaluator, (ii) using the target model to produce pseudocode given the pseudofunctions, and (iii) evaluating the pseudocode generated in step (ii) against the original pseudocode in (i).Using this framework, they performed evaluation exclusively on GPTs (Brown et al., 2020;OpenAI, 2023).</p>
<p>We highlight the following key observations: (1) Various representations of pseudofunctions corresponding to identical experimental actions, causes performance degradation and inconsistency of the evaluation framework.(2) The repertoire of actions executed in biology labs is confined to a finite set of actions.(3) High values in traditional automatic metrics (i) does not necessarily imply human-perceived good quality in scientific protocols.(4) The use of automatic metrics (i) requires manual labor, which limits the transition to fully automatic evaluation.</p>
<p>Here, we propose an evaluation framework that evaluates the capabilities of LLMs in SPFT: ProtoMed-LLM (Figure 1).</p>
<p>First, we define a set of actions in advance (Table 1), which eliminates individual action (pseudofunction) extraction step and variations of actions on each occasion.Second, we independently zeroshot prompted the target model and GPT-4 (Ope-nAI, 2023) to extract pseudocode from biology protocols, only using predefined actions as pseudofunctions.Lastly, we use LLAM-EVAL to evaluate the response, treating the target model's pseudocode as a prediction (ŷ) and that of GPT-4's as a baseline (y).LLAM-EVAL offers significant flexibility in terms of evaluation model, material, and criteria.This approach is inspired by the automated extraction of chemical synthesis actions from experimental procedures3 (Vaucher et al., 2020).We compared multiple LLMs to our framework, including GPT variations (Brown et al., 2020;OpenAI, 2023), Llama, Mixtral, Gemma, Cohere, andGemini (Google, 2024).We find that GPT-4o and Co-here+ is a powerful scientific protocol formulator.</p>
<p>We also introduce BIOPROT 2.0, a larger dataset with scientific protocols and the corresponding pseudocodes that can aid LLMs in formulation and evaluation of SPFT.</p>
<p>Overall, we make the following contributions:</p>
<ol>
<li>We propose ProtoMed-LLM: a flexible, automatic framework for evaluating LLMs on SPFT using domain knowledge and LLAM-EVAL.2. We propose LLAM-EVAL, an evaluation method that uses a form-filling paradigm offering significant flexibility in terms of evaluation model, material, and criteria.3. We introduce the BIOPROT 2.0 dataset, featuring protocols and corresponding pseudocode for evaluating and aiding LLMs on SPFT.</li>
</ol>
<p>Related Works</p>
<p>Task-specific Evaluation LLMs have been evaluated based on their performance in specific tasks.Information extraction abilities were measured by the generated quality of summaries (Durmus et al., 2020;Wang et al., 2020), paper reviews (Zhou et al., 2024), question correction (Fan et al., 2024), or combination of a few tasks (Labrak et al., 2024).However, these studies do not provide comprehensive evaluations and only assess very limited aspects, thus limiting their generalizability to other abilities or tasks.</p>
<p>LLM Evaluation on SPFT Recent work proposes a three-step framework (Section 1) for the evaluation of scientific protocols in biology: BioPlanner (O' Donoghue et al., 2023).This work evaluates GPT's performance in three tasks: next-step prediction, pseudocode generation, and pseudofunction retrieval.It employs statistical scoring methods including Levenshtein distance (L d ) and BLEU (Papineni et al., 2002) to measure the relevance between a baseline and generated protocols, despite their modest correlation with human judgments.</p>
<p>Domain-specific LLMs in Science</p>
<p>A Large number of LLMs have been trained, finetuned, or augmented for domain-specific uses.ChemBERTa/-2 (Chithrananda et al., 2020;Ahmad et al., 2022), MatSciBERT (Gupta et al., 2021), MaterialsBERT (Shetty et al., 2023), Chemcrow (Bran et al., 2023), and LLM augmentation methods for various experiment-related tasks (Guo et al., 2023) has been introduced in chemistry.</p>
<p>BioGPT (Luo et al., 2022), BioBERT (Lee et al., 2019), CamemBERT-bio (Touchent et al., 2024), BlueBERT (Peng et al., 2019), PubmedBERT (Gu et al., 2020), BioMegatron (Shin et al., 2020), and</p>
<p>Action Name Description</p>
<p>Transfer</p>
<p>Move substances between containers using lab equipment, such as pipettes.</p>
<p>Centrifuge</p>
<p>Spin at high speed to separate mixture components by density.</p>
<p>Vortex</p>
<p>Mix solutions by creating a vortex for even distribution.</p>
<p>SetTemp</p>
<p>Set specific temperatures for reactions or processes.</p>
<p>Wait</p>
<p>Period of inactivity to allow reactions or condition stabilization.</p>
<p>Wash</p>
<p>Rinse materials, often with solvents to remove contaminants.</p>
<p>Measure</p>
<p>Quantify substances or properties using instruments.</p>
<p>Microscopy</p>
<p>Use a microscope to observe and analyze cell morphology and structures.</p>
<p>CellDetachment</p>
<p>Release adherent cells from a culture surface using enzymatic or mechanical methods.</p>
<p>CellCount</p>
<p>Determine the number of cells in a sample using a hemocytometer or automated counter.</p>
<p>InvalidAction</p>
<p>Undefined action due to documentation error or ambiguity.</p>
<p>OtherLanguage</p>
<p>Text in non-English, indicating translation need.</p>
<p>NoAction</p>
<p>Text not corresponding to any defined action.</p>
<p>PCR</p>
<p>Amplify DNA segments through Polymerase Chain Reaction.</p>
<p>Gel</p>
<p>Separate molecules by size in a gel with electric field.</p>
<p>Culture</p>
<p>Grow cells in lab to study behavior or for experimentation.</p>
<p>Dilute</p>
<p>Reducing the concentration of a solution by adding solvent.ProtoCode (Jiang et al., 2024) has been introduced in biology.</p>
<p>Evaluating LLMs with LLMs Evaluation of LLMs encompasses a dual-method approach:</p>
<p>(i) Statistical scoring: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), Levenshtein Distance (ii) Model-based scoring: G-Eval (Liu et al., 2023), Prometheus (Kim et al., 2023), BLEURT (Sellam et al., 2020), Natural Language Inference (NLI) (iii) Combination of (i) and (ii): GPTScore (Fu et al., 2023), SelfCheckGPT (Manakul et al., 2023), BERTScore (Zhang et al., 2020), SciBERTScore (O'Donoghue et al., 2023), WMD (Kusner et al., 2015), MoverScore (Zhao et al., 2019), Question Answer Generation (QAG) Score</p>
<p>In tasks where reasoning is involved, (ii)(iii) outperforms (i).Previous work adopted (i) with (iii) being minimal (O'Donoghue et al., 2023).In this work, we adopt the notion of G-Eval (Liu et al., 2023), a framework for evaluating LLM-generated text, which prompts GPT with text and criteria, then scores based on its output.</p>
<p>Methods</p>
<p>The ProtoMed-LLM framework can evaluate the capability of LLMs on SPFT in three steps (Figure 2):</p>
<p>(1) prompt the target LLM to generate pseudocode based on the given protocol, (2) repeat previous step for GPT-4, and (3) LLAM-EVAL for evaluation.To utilize this framework, we curated protocols in biology (Section 3.1), predefined actions performed in biology labs (Section 3.2), prompted LLMs for pseudocode generation (Section 3.3), and prompted Llama-3 for evaluation (LLAM-EVAL) (Section 3.6).</p>
<p>Data Curation of Protocols in Biology</p>
<p>Each protocol is composed of three core elements: a title, description, and experimental steps.We curated the dataset through a process of collection and refinement.We collected a set of keywords relevant to biology.Then, we used a scoring system based on the number of keywords included in the description of each protocol from protocols.io4(Teytelman et al., 2016).We refined the dataset collected in the previous step using automated and manual methods.(Appendix A.1.)</p>
<p>Defining Actions</p>
<p>The defined actions are composed of two parts: (i) basic actions corresponding to a single action which can be performed directly in biology labs, and (ii) coarse-grained actions which corresponds to a large set of basic actions repeated throughout various protocols.Defined actions were reviewed by experts with intensive experiences in biology experiments.The target model specifies the arguments for each action on each occasion.</p>
<p>Basic Actions Since the repertoire of actions executed in biology labs is confined to a finite set of actions, we defined a set of actions performed in biology labs prior to the extraction of pseudocode from protocols (Table 1).We performed a comprehensive literature review to define the set of basic actions performed in biology labs.</p>
<p>Coarse-grained Actions</p>
<p>We observed that a series of complex, repetitive actions can be effectively encapsulated and described by a single, comprehensive action.For instance, the process of diluting a solution is conceptually straightforward and can possibly defined by basic actions.However, this involves intricate calculations and logical reasoning, which can result in performance degradation by calculation mistakes and posing variations in representations of an identical process.To this end, we coarse-grained these complex set of actions into a singular action.</p>
<p>Prompting Pseudocode Generation</p>
<p>To evaluate the target LLMs on SPFT, we prompted the models to generate pseudocode based on a protocol collected at Section 3.1.Models are instructed to use only the actions defined in Section 3.2 as the function name.However, they were allowed to define the arguments for each pseudofunction as needed for each occasion.If applicable, the fixed prompt, including the instructions and predefined actions, was provided in the system message, while the protocol was included in the user message.In this work, we prompted GPT-3 (Brown et al., 2020), GPT-4 (OpenAI, 2023), Gemini (Google, 2024), Claude3 (Anthropic, 2023), and Cohere.Below is the prompt for generating pseudocode based on the given protocol.Note that actions and corresponding descriptions presented in Table 1 are placed at {actions}.</p>
<p>You are an AI that generates Python pseudocode for biology protocols.This pseudocode must accurately describe a complete scientific protocol to obtain a result.You will be provided with the title, description, and steps of the biology protocol, and your task is to convert it to Python pseudocode.</p>
<p>You may define the arguments on your own.You must ONLY use these functions.</p>
<p>{actions} Do NOT provide any captions.ONLY present the pseudocode and pseudofunctions used inside the code.Present the pseudofunctions at the beginning and then the pseudocode.Do NOT provide any descriptions inside the code.</p>
<p>title: {title} description: {description} steps: {steps}</p>
<p>Metrics and Evaluation</p>
<p>We observe that using automatic metrics (i) necessitates manual annotation of functions and pseudocodes each time, which significantly hampers the automation of the evaluation process.Moreover, evaluating the function and input5 separately falls short of flexible and comprehensive evaluation in a protocol manner.</p>
<p>To this end, we propose LLAM-EVAL, an automatic, flexible prompt-based framework to evaluate the quality of LLM responses.This framework requires three elements: two input texts (one serving as the baseline and the other as the target) and an evaluator LLM: Llama-36 .This method encompasses predefining a set of scores7 S = {s 1 , s 2 , ..., s n }, prompting Llama-3 to rate the outcomes of a target LLM with that of GPT-4 in the scale of S, calculating the probability of each score p(s i ), and calculating the final score as following.This method is inspired by G-Eval (Liu et al., 2023).
score = n i=1 s i p(s i )
Llama-3 is prompted to evaluate according to one criterion at a time.The original prompts targeting summarizing tasks are modified to perform evaluation on SPFT.In this work, we evaluate the pseudocode generated by the target LLM based on six criteria: the four original criteria used in G-Eval (Liu et al., 2023) (Coherence, Consistency, Fluency, and Relevance) and two criteria we propose (Precision, and Coverage), considering the context of SPFT.For example, the definition of Coherence is:</p>
<p>Coherence (1-5) -the overall quality of all lines in the pseudocode.The target pseudocode should not be a rough overview but should provide a precise description of a baseline pseudocode.</p>
<p>The definitions of other criteria in prompts can be found at Appendix A.2.To automatically implement chain-of-thoughts (CoT) in the evaluation process, we instructed GPT-4 to create specific evaluation steps for each criterion.GPT is capable of producing these evaluation steps by itself (Liu et al., 2023).GPT-4 was given a task and evaluation criteria, then prompted to generate the evaluation steps using a form-filling paradigm.An example prompt containing GPT-4 generated instructions for evaluation can be found at Appendix A.2.We also implemented an automatic feedback loop to regenerate the response up to five or ten times if the output did not contain scores.We evaluated using two baselines: the GPT-generated pseudocode and the original protocol.</p>
<p>This approach is not constrained by the output structure of the target models, eliminates the need for manual annotation efforts during the parsing process as required in reference-based metrics, enables a comprehensive evaluation, and thereby makes ProtoMed-LLM significantly more flexible and automatic.</p>
<p>To ensure compatibility, we also use conventional reference-based metrics: Normalized Levenshtein distance (L dn ) for function names, BLEU (Papineni et al., 2002)
SciBERTScore = 1 N N i=0 ⟨E(a pred i ), E(a BL i )⟩ ∥E(a pred i )∥∥E(a BL i )∥</p>
<p>Evaluator LLM Selection</p>
<p>To select a specific LLM as an evaluator, we propose self-self comparison task as a baseline, where an LLM generates a pseudocode8 for a protocol and then evaluates the score using the same LLM against the generated pseudocode.For example, this means evaluating GPT-4 generated pseudocode against the same pseudocode using GPT-4.Our assumption was that the score should be close to the maximum 9 when the baseline and target pseudocode are the same.Our goal was to select the model with the best results as the evaluator.We evaluated each model based on six criteria in Section 3.4.More details in Appendix A.3.While using G-EVAL, we encountered instances where the output was a sentence instead of a score (number).To address this issue, we modified the parameters, dataset, and prompts.Further details are in Appendix A.4.</p>
<p>Evaluating LLMs using LLAM-EVAL</p>
<p>Using LLAM-EVAL, we evaluate across three tasks for each model: (1) GPT-4 generated pseudocode as a baseline with predefined actions given in prompt, (2) the same task with no predefined actions, (3) the original protocol as a baseline with predefined actions.We evaluate GPT variations (Brown et al., 2020;OpenAI, 2023), Llama, Mixtral, Gemma, Cohere, and Gemini (Google, 2024).Details are in Appendix A.5.</p>
<p>Implementation Details</p>
<p>To ensure a fair evaluation of LLMs, we considered additional factors that may affect performance and present several settings.We consider that LLMs tend to perform better when the actions are presented in the same order as in the protocol.While previous work extracted different actions from each protocol10 , we predefined the actions which is equivalent to shuffling.</p>
<p>Analysis</p>
<p>Evaluator LLM Selection</p>
<p>Llama-3 achieved the highest scores across all six tasks, while there were small differences across models (Table 2).We chose Llama-3 as an evaluator, which is free of cost to date.Note that evaluations for other models not presented in the table were not feasible, as numerical responses were not generated.More details are in Appendix A.3.</p>
<p>Evaluating LLMs on SPFT</p>
<p>Our results show that GPT-4o and Cohere+ is a powerful protocol formulator (Table 3).We found our work compatible to previous work (O'Donoghue et al., 2023).</p>
<p>Is applying domain knowledge an effective strategy for evaluation?We applied domain knowledge by predefining the finite set of actions performed in biology labs.To evaluate the efficacy of this method, we compare the responses generated with predefined actions included in the prompts to those generated without them (Table 4).The performance is enhanced for most models, with the exception of the Recall.Further research should be conducted to explore these findings.</p>
<p>Can the original protocol itself serve as a baseline?Evaluation of LLMs in SPFT in previous work requires manual processes and pseudocode extraction step in SPFT.However, evaluation using the original protocol itself completely eliminates the manual processes of pseudofunction evaluation and the GPT-generated pseudocode extraction step, thereby enhancing flexibility and automation.To this end, we evaluate using the original protocol as a baseline.While scores obtained using this approach is not close to the maximum score (Table 3), we observe that the relative ranking of the models remains relevant to the results of using the pseudocode as a baseline.</p>
<p>Will LLM as an evaluator prefer responses from itself?It is reported that LLM as an evaluator prefer responses from itself over human responses in text summarization tasks (Liu et al., 2023).Therefore, a potential concern is that the evaluator may prefer outputs from itself regardless of its quality.While results in Table 2 and 4 address this concern, Table 3 shows that Llama-3 as an evaluator does not prefer its outputs over that of GPT-4.Our results suggest that GPT's preference for its own responses in previous work (Liu et al., 2023) may be a phenomenon unique to GPT.</p>
<p>The BIOPROT 2.0 Dataset</p>
<p>We introduce BIOPROT 2.0, a dataset with scientific protocols and the corresponding pseudocodes with a larger number of datapoints.Previous work highlights that a dataset with these two components can aid protocol formulation of LLMs (O'Donoghue et al., 2023).The pseudocode extracted from protocols are only composed of pseudofunctions (actions) predefined above the previous step, as each model was prompted to use only the provided functions but to define the arguments on their own.The summary of generated pseudocode are in Table 5.This dataset can be used to formulate scientific protocols to achieve a prompted goal using a toolformer like (Schick et al., 2023) chain-of-thought LLM agent (Wei et al., 2023).</p>
<p>Conclusion</p>
<p>We introduce ProtoMed-LLM, a flexible and automatic framework designed to evaluate LLMs' capabilities on Scientific Protocol Formulation Tasks 5.00 ± 0.02 5.00 ± 0.00 5.00 ± 0.00 5.00 ± 0.00 5.00 ± 0.00 5.00 ± 0.06 ✓ ✗ 5.00 ± 0.00 5.00 ± 0.00 5.00 ± 0.08 5.00 ± 0.00 4.99 ± 0.11 5.00 ± 0.00 5.00 (Baseline) ✗ ✗ 5.00 ± 0.00 5.00 ± 0.00 5.00 ± 0.04 5.00 ± 0.03 5.00 ± 0.03 5.00 ± 0.00 5.00 Table 3: ProtocoLLM Evaluation Results of three tasks for each model: (1) GPT-4 generated pseudocode as a baseline with predefined actions given in prompt, (2) the same task with no predefined actions, (3) the original protocol as a baseline with predefined actions.'Ac' and 'Pr' represent whether the predefined actions and the original protocol were given for evaluation, respectively.We report the mean, standard deviation, and average of scores over five runs.The best and second best performance besides a baseline (GPT-4) for each criterion and task is bolded and underlined, respectively.The scores range from a minimum of 1 to a maximum of 5. Higher values for all metrics represent better performance.(SPFT).This framework prompts the target model and GPT-4 to extract pseudocode from biology protocols using only predefined lab actions, then evaluates the target model's output using LLAM-EVAL, with the GPT-4 generated pseudocode as a baseline and Llama-3 as the evaluator.Our prompt-based evaluation method, LLAM-EVAL, provides significant flexibility in terms of evaluation models, materials, criteria, and is free of cost.We assess various models, including GPT variants, Llama, Mixtral, Gemma, Cohere, and Gemini, and find GPT and Cohere to be particularly effective in formulating scientific protocols.Additionally, we present BIO-PROT 2.0, a dataset containing biology protocols and corresponding pseudocodes, which supports LLMs in the formulation and evaluation of SPFT.Our work is extensible to the assessment of LLMs on SPFT across various domains and other fields that require protocol generation for specific goals.
✓
We recognize several limitations.The predefined actions may not encompass all actions performed in a biology labs.The definitions of predefined actions may be incomplete.To precisely define an action, it is necessary to define not only the function but also the function arguments.The number of protocols in BIOPROT 2.0 may be insufficient for evaluation purposes.The performance of ProtoMed-LLM may decline outside of biology.Addressing this requires redefining domain-specific actions and exploring other LLMs for diverse fields.Future work should investigate these cross-disciplinary implications.LLMs are continuously evolving due to regular updates.The LLMs used for evaluation in this work might become unavailable in the future.Upgraded versions of LLMs may result in performance degradation and metrics may differ from those obtained using previous models.Due to selecting Llama-3 as the evaluator, our results may be susceptible to its biases and hallucinations.The outcomes when evaluated with models other than Llama-3 are unknown.Future work should investigate the outcomes using different LLMs as an evaluator.Using an API of LLMs as an evaluator, such GPT, is often not free of charge and can be costly.We used GPT-4 generated responses as a baseline; however, it may not accurately represent the ground truth.Future work should explore the implications of employing alternative resources (e.g., manually annotated pseudocodes, responses generated by other models) as the baseline.We observed basic actions classified as NoAction in minor cases.It has been reported that GPT prefers outputs from LLMs, which also produced our evaluation materials including all ground truth and target pseudocodes.This can potentially influence the scores.The four criteria mentioned in G-Eval may not sufficiently fulfill the role of evaluating protocols where real-world validation is crucial.Also, applying these criteria originally designed for summarization tasks may be inappropriate for evaluating SPFT.Even if the protocol pseudocode is successfully synthesized, real-world experiments may fail depending on the person performing the protocol or the condition of the physical equipment, especially in cases that are more complex than stem cell culture or require delicate manual work and experience.</p>
<p>Ethical Considerations</p>
<p>The use of manually verified protocols in LLMs is strictly prohibited for generating false protocols on platforms like STAR Protocols (Cell Press) and Nature Protocols.Numerous sites also prohibit the use of these protocols in conjunction with any form of AI tool.Our framework can be applied to the protocols of these sites.Although we have endeavored to exclude protocols that can create dangerous substances, there remains the potential for generating protocols that inadvertently produce hazardous products or byproducts.</p>
<p>Figure 1 :
1
Figure 1: Overview of the ProtoMed-LLM Framework.A protocol containing a title, descriptions, stepby-step instructions, and predefined biology lab actions is given to both a target model and GPT-4 for pseudocode generation.Then, Llama-3 evaluates these outputs considering the target model's pseudocode as the prediction (ŷ) and GPT-4's as a baseline (y).</p>
<p>Figure 2 :
2
Figure 2: The ProtoMed-LLM Framework.</p>
<p>Table 1 :
1
Predefined Set of Actions.List of actions performed in biological experiments and the corresponding descriptions.Actions above the line represent the basic actions, with the last three specifically designated for instances where a new protocol introduces an undefined action.Actions below represent the coarse-grained actions.</p>
<p>, precision, recall, and SciBERTScore (O'Donoghue et al., 2023) for function inputs.SciBERTScore is calculated using the encoded predicted E(a</p>
<p>pred i ) and baseline values E(a BL i ) using the SciBERT (Beltagy et al., 2019) sentence encoder E.</p>
<p>Table 2 :
2
Self-Self Comparison Task Results: We report the mean and standard deviation of scores over five or ten runs.Values in bold indicate the highest scores for each criterion.Higher values for all metrics represent better performance.Note that a larger dataset was used for this task.Details in Appendix A.3.
PromptOriginal CriteriaNew CriteriaModelsAc PrCoherence ConsistencyFluencyRelevancePrecisionCoverageAverageGPT-4o✓✗ 4.10 ± 0.79 3.80 ± 0.85 3.86 ± 0.67 4.32 ± 0.71 4.02 ± 0.65 4.26 ± 0.734.06✗✗ 4.28 ± 0.50 3.94 ± 0.64 4.04 ± 0.37 4.45 ± 0.54 4.18 ± 0.41 4.39 ± 0.504.21✓ ✓ 4.29 ± 0.57 4.73 ± 0.50 4.42 ± 0.53 4.75 ± 0.48 3.90 ± 0.48 4.67 ± 0.564.46GPT-4</p>
<p>✓ 4.32 ± 0.53 4.70 ± 0.58 4.53 ± 0.51 4.75 ± 0.44 3.99 ± 0.29 4.67 ± 0.48
4.49GPT-3.5✓✗ 3.61 ± 0.97 3.51 ± 1.02 3.58 ± 0.85 4.11 ± 0.78 3.82 ± 0.73 3.90 ± 0.833.75✗✗ 3.83 ± 0.82 3.71 ± 0.81 3.76 ± 0.68 4.19 ± 0.64 3.96 ± 0.57 3.97 ± 0.713.90✓ ✓ 4.13 ± 0.65 4.76 ± 0.49 4.48 ± 0.52 4.69 ± 0.49 3.79 ± 0.58 4.49 ± 0.674.39Llama3-8b✓✗ 2.25 ± 1.00 1.93 ± 0.99 2.27 ± 0.83 2.39 ± 1.08 2.61 ± 0.96 2.56 ± 1.092.33✗✗ 2.90 ± 0.89 2.69 ± 0.92 3.02 ± 0.88 3.41 ± 0.81 3.47 ± 0.70 3.19 ± 0.823.12✓ ✓ 2.80 ± 1.02 3.00 ± 1.26 3.10 ± 1.00 3.39 ± 1.09 2.93 ± 0.92 3.27 ± 1.063.08Llama3-70b ✓✗ 3.61 ± 0.94 3.14 ± 1.10 3.53 ± 0.82 3.73 ± 0.97 3.72 ± 0.70 3.77 ± 0.793.58✗✗ 3.98 ± 0.64 3.72 ± 0.75 3.92 ± 0.49 4.20 ± 0.57 4.03 ± 0.36 4.09 ± 0.533.99✓ ✓ 4.02 ± 0.75 4.17 ± 0.98 4.15 ± 0.66 4.37 ± 0.74 3.78 ± 0.59 4.25 ± 0.694.12Mixtral✓✗ 3.41 ± 1.03 2.90 ± 1.13 3.57 ± 0.83 3.36 ± 1.14 3.68 ± 0.77 3.54 ± 0.933.41✗✗ 3.95 ± 0.68 3.68 ± 0.79 3.94 ± 0.53 4.18 ± 0.66 4.05 ± 0.43 4.00 ± 0.613.97✓ ✓ 4.06 ± 0.69 4.32 ± 0.84 4.28 ± 0.59 4.37 ± 0.71 3.88 ± 0.44 4.31 ± 0.704.21Gemma-7b✓✗ 3.06 ± 0.97 2.81 ± 1.03 3.47 ± 0.86 3.52 ± 0.93 3.55 ± 0.78 3.19 ± 0.893.27✗✗ 2.93 ± 0.85 2.66 ± 0.88 3.63 ± 0.76 3.61 ± 0.71 3.66 ± 0.61 3.06 ± 0.803.26✓ ✓ 3.81 ± 0.75 4.13 ± 0.83 4.25 ± 0.61 4.26 ± 0.75 3.76 ± 0.60 3.94 ± 0.794.02Cohere+✓✗ 3.95 ± 0.74 3.63 ± 0.87 3.87 ± 0.60 4.11 ± 0.74 3.98 ± 0.50 4.07 ± 0.633.94✗✗ 3.97 ± 0.60 3.71 ± 0.73 3.95 ± 0.46 4.15 ± 0.56 4.03 ± 0.38 4.04 ± 0.503.98✓ ✓ 4.44 ± 0.52 4.63 ± 0.61 4.53 ± 0.52 4.73 ± 0.47 4.04 ± 0.30 4.66 ± 0.494.50Cohere✓✗ 3.51 ± 0.91 3.06 ± 1.02 3.56 ± 0.74 3.66 ± 0.87 3.71 ± 0.63 3.70 ± 0.763.53✗✗ 3.71 ± 0.68 3.44 ± 0.83 3.83 ± 0.56 4.05 ± 0.53 3.94 ± 0.41 3.84 ± 0.563.80✓ ✓ 3.98 ± 0.63 4.11 ± 0.87 4.14 ± 0.51 4.29 ± 0.63 3.83 ± 0.48 4.24 ± 0.644.10Gemini-1.0✓✗ 2.77 ± 1.09 2.30 ± 1.08 2.90 ± 0.95 2.80 ± 1.10 3.13 ± 0.92 3.15 ± 1.012.84✗✗ 3.46 ± 0.93 3.22 ± 1.01 3.59 ± 0.83 3.89 ± 0.77 3.80 ± 0.69 3.66 ± 0.793.60✓ ✓ 3.37 ± 0.93 3.68 ± 1.11 3.73 ± 0.80 3.87 ± 0.87 3.42 ± 0.78 3.86 ± 0.843.66Gemini-2.0✓✗ 3.09 ± 1.05 2.53 ± 1.10 3.75 ± 0.70 2.98 ± 1.08 3.63 ± 0.73 3.43 ± 0.893.24✗✗ 3.88 ± 0.82 3.61 ± 0.91 4.11 ± 0.60 4.13 ± 0.73 4.14 ± 0.54 3.93 ± 0.733.97✓ ✓ 3.80 ± 0.80 3.95 ± 0.97 4.30 ± 0.58 4.18 ± 0.72 3.80 ± 0.49 4.11 ± 0.684.02Gemini-1.5✓✗ 3.02 ± 1.05 2.48 ± 1.02 3.10 ± 0.93 2.97 ± 1.07 3.32 ± 0.84 3.42 ± 0.933.05✗✗ 4.12 ± 0.66 3.86 ± 0.72 4.03 ± 0.55 4.33 ± 0.62 4.13 ± 0.50 4.21 ± 0.594.11✓ ✓ 3.34 ± 0.95 3.62 ± 1.04 3.76 ± 0.76 3.81 ± 0.86 3.36 ± 0.77 3.80 ± 0.843.61</p>
<p>Table 4 :
4
Evaluation Results Using Reference-Based Metrics.Comparison with and without predefined actions given in prompts.We report mean and standard deviation of scores.The best and second best performance for each criterion is bolded and underlined, respectively.Except for L dn , higher values for all metrics represent better performance.
StatisticValue (m ± σ)# of protocols300Tokens / protocol812.3 ± 469.9# of steps14.81 ± 10.74Tokens / step54.28 ± 42.41Tokens / description139.0 ± 135.7Tokens / generated pseudocode623.8 ± 223.2# of lines / generated pseudocode83.06 ± 28.89# of pseudofunctions / edited pseudocode 10.28 ± 6.582</p>
<p>Table 5 :
5
Statistics of BIOPROT 2.0.'EditedPseudocode'refers to the pseudocode that was reformatted, while preserving its content, to obtain the scores presented in Table4.</p>
<p>The dataset and code are available here. *Corresponding Authors
Pseudofunctions represent laboratory actions, while pseudocode embodies protocols composed of these pseudofunc-
tions.3  A set of actions in chemistry labs were defined prior to the pseudocode extraction process.
A platform for reproducible protocol sharing provides access to more than 15k publicly available protocols, and has no limitations regarding the use of LLMs.
Input refers to the function parameters and arguments.</p>
<p>s1=1 and sn=5 is set in this work.
Pseudocode with pseudofunctions defined at the beginning to be precise.
In previous work, this required shuffling, as LLMs presented the pseudofunctions in the same order as in the protocol.
AcknowledgementsWe would like to thank Karim Md.Adnan for assisting us with the action defining process.This research was supported by a KIST project (2E32351) and Bio-Cluster Industry Capacity Enhancement Project of Jeonbuk Technopark (JBTP)AppendixA BIOPROT 2.0A.1 Data CurationWe used protocols.io(Teytelman et al., 2016)API for data collection.Protocols of 1 ≤ score ≤ 5 and 3 ≤ steps are collected.The collected data was in a .jsonformat, every data point with slight differences in keys.Some protocols were present in the git repository but could not be found when retrieved using the API, and vice versa 11 .Also, even if the file ID in the git repository and the protocol ID retrieved using the API are the same, the dictionary key number_of_steps may differ 12 .Keywords 13 extracted from the keywords.txtfile and the descriptions were converted to lowercase temporarily for comparison and scoring.As of May 2024, we collected a total of approximately 15k mirrored public protocols from protocols.io'sGitHub before refinement.Protocols were excluded if dictionary key steps is empty.Protocols were manually verified by experts in biology.The protocols were removed if they were multiple duplicated files for an identical protocol 14 .For the same title, we score the latest version of the protocol.A.2 Metrics and EvaluationDefinitions of Evaluation Criteria• Consistency: Consistency (1-5) -the factual alignment between the source and the target pseudocode.A factually consistent pseudocode contains only statements that are entailed by the source pseudocode.Annotators 11 The protocol with ID 3737 exists in protocol.iobut doesn't exist in git repository. 12The number_of_steps for the protocol with ID 10489 is 3 in the git repository but 0 when retrieved using the API. 13 The keywords are: Biology, Cell, DNA, Protein, Stem Cell, Molecular Biology, Molecular, Gene, Virus, E. coli, cDNA, Agarose, Agarose Gel, in vitro, PCR, NGS, Ethanol, Illumina, Cell Theory, Evolution, Genetics, Homeostasis, Cell Membrane, Mitochondria, Nucleus, Ribosomes, DNA Replication, Mutation, Chromosomes, Gene Expression, Natural Selection, Speciation, Adaptation, Phylogenetics, Ecosystems, Biodiversity, Conservation, Bacteria, Viruses, Fungi, Pathogens, Proteins, Enzymes, Metabolism, Photosynthesis, Gel Electrophoresis, Cloning, CRISPR-Cas9, Neurons, Brain, Synapses, Neurotransmitters, Antibodies, Vaccines, Immune Response, Autoimmunity, Embryogenesis, Stem Cells, Morphogenesis, Regeneration, Pollination, Growth Hormones, Tropisms, Coral Reefs, Oceanic Zones, Marine Conservation, Aquatic Ecosystems, Endangered Species, Habitat Destruction, Conservation Strategies, Rewilding, Genetic Engineering, Bioreactors, Bioinformatics, and Synthetic Biology.14 such as protocol ID: 9216were also asked to penalize summaries that contained hallucinated facts.• Fluency: Fluency (1-5): the quality of the pseudocode in terms of grammar, spelling, punctuation, word choice, and structure.• Relevance: Relevance (1-5) -selection of important information from the source pseudocode.The target pseudocode should include only important information from the source document.Annotators were instructed to penalize summaries which contained redundancies and excess information.• Precision: Precision (1-5) -the exactness and accuracy of the expressions and terminology used in the pseudocode.The target pseudocode should avoid vague or ambiguous terms and should use specific and appropriate terminology that accurately reflects the intended operations and logic.• Coverage: Coverage (1-5) -the extent to which the target pseudocode addresses all aspects of the source pseudocode.The target pseudocode should comprehensively represent all the necessary steps, operations, and details present in the source pseudocode without omitting any critical information.Note that above are criteria used for evaluation when GPT-generated pseudocode was a baseline.This was slightly modified when evaluating based on original protocol.Example LLAM-EVAL Prompt Below is a prompt evaluating the generated pseudocode from a target LLM based on the criteria Coherence using the GPT-generated pseudocode as the ground truth.The GPT-generated pseudocode for each protocol is placed inside {{Ground_truth_pseudocode}}, and the target model-generated pseudocode is placed inside {{Target_pseudocode}}.You will be given a source pseudocode as a ground truth.You will then be given a target pseudocode which is generated from an identical source of protocol.Your task is to rate the target pseudocode on one metric.Please make sure you read and understand these instructions carefully.Please keep this document open while reviewing, and refer to it as needed.Evaluation Criteria: Coherence (1-5) -the overall quality of all lines in the pseudocode.The target pseudocode should not be a rough overview but should provide a precise description of the ground truth pseudocode.Evaluation Steps: 1. Read the Ground Truth Pseudocode: Carefully read and understand the source pseudocode provided as the ground truth.Ensure you comprehend the logic, flow, and details of the algorithm or protocol described.2. Read the Target Pseudocode: Thoroughly read the target pseudocode that needs to be evaluated.Pay attention to the details, structure, and clarity of the pseudocode.3. Compare Against Ground Truth: Compare each line and section of the target pseudocode with the corresponding parts of the ground truth pseudocode.Ensure that all critical steps, variables, and logic present in the ground truth are accurately reflected in the target pseudocode.4. Assess Coherence: Evaluate the overall quality of the target pseudocode based on how well it translates the ground truth.Consider the following aspects: Clarity: Is the pseudocode easy to understand?Completeness: Does it cover all the steps and details present in the ground truth?Precision: Are the descriptions and instructions in the pseudocode precise and unambiguous?Consistency: Are there any contradictions or logical inconsistencies?5. Assign a Coherence Rating (1-5): 1 (Poor): The target pseudocode is incomplete, confusing, and lacks most details from the ground truth. 2 (Fair): The target pseudocode is partially complete but has significant gaps and is often unclear.3 (Good): The target pseudocode covers most details from the ground truth but has some minor inconsistencies or lacks clarity in parts.4 (Very Good): The target pseudocode is mostly complete and clear, with very few minor issues.5 (Excellent): The target pseudocode is complete, clear, precise, and fully coherent with the ground truth.Source Pseudocode: {{Ground_truth_pseudocode}}Target Pseudocode: {{Target_pseudocode}} Evaluation Form (scores ONLY):-Coherence:A.3 Evaluator LLM SelectionModels without numerical responses include: Llama3-8b, Llama3-70b, Mixtral, and Gemma.A.4 Implementation DetailsExcept for n and seed, parameters were set to their default values.We used approximately $1000 for GPT API calls, $20 for Gemini, and other models were free of cost.Counting Tokens We counted the tokens of the concatenated string of the title, original description, and steps, separated by "\n\n".The reason for this approach is to match the token count with that of the previous work.Inconsistencies G-EVAL Outputs To address this issue, we attempted the following methods:(1) Modified max_token = 5 to max_token = 1 : The scores became integers, but the model still generated sentences in addition to scores.(2
Walid Ahmad, Elana Simon, Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. 2022. Chemberta-2: Towards chemical foundation models. </p>
<p>The claude 3 model family: Opus, sonnet, haiku. Anthropic, 2023Technical report</p>
<p>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or SummarizationAnn Arbor, Michigan2005Association for Computational Linguistics</p>
<p>SciB-ERT: A pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, 10.18653/v1/D19-1371Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Sam Andres M Bran, Oliver Cox, Carlo Schilter, Andrew D Baldassari, Philippe White, Schwaller, Chemcrow: Augmenting large-language models with chemistry tools. 2023</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish2020. 2020Chemberta: Large-scale self. supervised pretraining for molecular property prediction</p>
<p>FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. Esin Durmus, He He, Mona Diab, 10.18653/v1/2020.acl-main.454Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Evaluating generative language models in information extraction as subjective question correction. Yuchen Fan, Yantao Liu, Zijun Yao, Jifan Yu, Lei Hou, Juanzi Li, 2024</p>
<p>Gptscore: Evaluate as you desire. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, 2023</p>
<p>Gemini: A family of highly capable multimodal models. Gemini Team, Google , 2024</p>
<p>Domain-specific language model pretraining for biomedical natural language processing. Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, Hoifung Poon, 2020</p>
<p>What can large language models do in chemistry?. Taicheng Guo, Kehan Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, 2023a comprehensive benchmark on eight tasks</p>
<p>Matscibert: A materials domain language model for text mining and information extraction. Tanishq Gupta, Mohd Zaki, N M Anoop Krishnan, Mausam , 2021</p>
<p>Llms can generate robotic scripts from goal-oriented instructions in biological laboratory automation. Akari Inagaki, Koichi Kato, Haruka Takahashi, Genki N Ozaki, Kanda, 2023</p>
<p>14 examples of how llms can transform materials science and chemistry. Kevin Maik, Jablonka , Qianxiang Ai, Alexander Al-Feghali, Shruti Badhwar, Joshua D Bocarsly, Andres M Bran, Stefan Bringuier, L Catherine Brinson, Kamal Choudhary, Defne Circi, Sam Cox, Wibe A Jong, Matthew L Evans, Nicolas Gastellu, Jerome Genzling, Victoria María, Ankur K Gil, Zhi Gupta, Alishba Hong, Sabine Imran, Anne Kruschwitz, Jakub Labarre, Tao Lála, Steven Liu, Sauradeep Ma, Garrett W Majumdar, Nicolas Merz, Elias Moitessier, Beatriz Moubarak, Brenden Mouriño, Michael Pelkie, Mayk Pieler, Bojana Caldas Ramos, Ranković, G Samuel, Jacob N Rodriques, Philippe Sanders, Marcus Schwaller, Jiale Schwarting, Berend Shi, Ben E Smit, Joren Smith, Christoph Van Herck, Logan Völker, Sean Ward, Warren, 10.1039/D3DD00113JBenjamin Weiser, Sylvester Zhang, Xiaoqi Zhang, Ghezal Ahmad Zia, Aristana Scourtas, K. J. Schmidt, Ian Foster, Andrew D. White, and Ben Blaiszik20232a reflection on a large language model hackathon. Digital Discovery</p>
<p>Protocode: Leveraging large language models (llms) for automated generation of machine-readable pcr protocols from scientific publications. Shuo Jiang, Daniel Evans-Yamamoto, Dennis Bersenev, K Sucheendra, Ayako Palaniappan, Yachie-Kinoshita, 10.1016/J.SLAST.2024.100134SLAS Technology. 291001342024</p>
<p>Prometheus: Inducing finegrained evaluation capability in language models. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, Minjoon Seo, 2023</p>
<p>From word embeddings to document distances. Matt Kusner, Yu Sun, Nicholas Kolkin, Kilian Weinberger, Proceedings of the 32nd International Conference on Machine Learning. the 32nd International Conference on Machine LearningLille, FrancePMLR201537</p>
<p>Five hard truths for synthetic biology. R Kwok, 10.1038/463288aNature. 4632010</p>
<p>A zero-shot and few-shot study of instructionfinetuned large language models applied to clinical and biomedical tasks. Yanis Labrak, Mickael Rouvier, Richard Dufour, 2024</p>
<p>Biobert: a pre-trained biomedical language representation model for biomedical text mining. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang, 10.1093/bioinformatics/btz682Bioinformatics. 3642019</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, G-eval: Nlg evaluation using gpt-4 with better human alignment. 2023</p>
<p>Biogpt: generative pre-trained transformer for biomedical text generation and mining. Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, Tie-Yan Liu, 10.1093/bib/bbac409Briefings in Bioinformatics. 6232022</p>
<p>Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. Potsawee Manakul, Adian Liusie, Mark J F Gales, 2023</p>
<p>BioPlanner: Automatic evaluation of LLMs on protocol planning in biology. Aleksandar Odhran O'donoghue, John Shtedritski, Ralph Ginger, Ali Abboud, Samuel Ghareeb, Rodriques, 10.18653/v1/2023.emnlp-main.162Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingapore2023Association for Computational Linguistics</p>
<p>Gpt-4 technical report. 2023OpenAI</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02. the 40th Annual Meeting on Association for Computational Linguistics, ACL '02USA. Association for Computational Linguistics2002</p>
<p>Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets. Yifan Peng, Shankai Yan, Zhiyong Lu, Proceedings of the 2019 Workshop on Biomedical Natural Language Processing. the 2019 Workshop on Biomedical Natural Language Processing2019. 2019</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, 2023</p>
<p>Bleurt: Learning robust metrics for text generation. Thibault Sellam, Dipanjan Das, Ankur P Parikh, 2020</p>
<p>A general-purpose material property data extraction pipeline from large polymer corpora using natural language processing. P Shetty, A C Rajan, C Kuenneth, 10.1038/s41524-023-01003-wnpj Computational Materials. 9522023</p>
<p>Hoo-Chang Shin, Yang Zhang, Evelina Bakhturina, Raul Puri, Mostofa Patwary, Mohammad Shoeybi, Raghav Mani, Biomegatron: Larger biomedical domain language model. 2020</p>
<p>Protocols.io: Virtual communities for protocol development and discussion. Leonid Teytelman, Alexei Stoliartchouk, Lori Kindler, Bonnie L Hurwitz, 10.1371/journal.pbio.1002538PLOS Biology. 1482016</p>
<p>Camembert-bio: Leveraging continual pre-training for cost-effective models on french biomedical data. Rian Touchent, Laurent Romary, 2024and Eric de la Clergerie</p>
<p>Automated extraction of chemical synthesis actions from experimental procedures. Alain C Vaucher, Federico Zipoli, Jonas Geluykens, 10.1038/s41467-020-17266-6Nature Communications. 1136012020</p>
<p>Asking and answering questions to evaluate the factual consistency of summaries. Alex Wang, Kyunghyun Cho, Mike Lewis, 10.18653/v1/2020.acl-main.450Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>Assessment of chemistry knowledge in large language models that generate code. Andrew D White, Glen M Hocky, A Heta, Mehrad Gandhi, Sam Ansari, Geemi P Cox, Subarna Wellawatte, Ziyue Sasmal, Kangxin Yang, Yuvraj Liu, Willmor J Peña Singh, Ccoa, 10.1039/D2DD00087CDigital Discovery. 22023</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, 2020</p>
<p>Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, Steffen Eger, 2019</p>
<p>Is LLM a reliable reviewer? a comprehensive evaluation of LLM on automatic paper reviewing tasks. Ruiyang Zhou, Lu Chen, Kai Yu, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCL2024</p>            </div>
        </div>

    </div>
</body>
</html>
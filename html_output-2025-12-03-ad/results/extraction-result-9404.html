<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9404 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9404</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9404</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-164.html">extraction-schema-164</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-274165986</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.13724v1.pdf" target="_blank">Exploring Large Language Models for Climate Forecasting</a></p>
                <p><strong>Paper Abstract:</strong> With the increasing impacts of climate change, there is a growing demand for accessible tools that can provide reliable future climate information to support planning, finance, and other decision-making applications. Large language models (LLMs), such as GPT-4, present a promising approach to bridging the gap between complex climate data and the general public, offering a way for non-specialist users to obtain essential climate insights through natural language interaction. However, an essential challenge remains under-explored: evaluating the ability of LLMs to provide accurate and reliable future climate predictions, which is crucial for applications that rely on anticipating climate trends. In this study, we investigate the capability of GPT-4 in predicting rainfall at short-term (15-day) and long-term (12-month) scales. We designed a series of experiments to assess GPT's performance under different conditions, including scenarios with and without expert data inputs. Our results indicate that GPT, when operating independently, tends to generate conservative forecasts, often reverting to historical averages in the absence of clear trend signals. This study highlights both the potential and challenges of applying LLMs for future climate predictions, providing insights into their integration with climate-related applications and suggesting directions for enhancing their predictive capabilities in the field.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9404.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9404.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT-4o (GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large language model (LLM) used in this study to generate short-term (15-day) and long-term (12-month) rainfall forecasts for 15 U.S. cities under multiple prompting conditions, both with and without expert-model inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring Large Language Models for Climate Forecasting</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pre-trained large language model from OpenAI used via prompting to produce numerical rainfall predictions; the paper treats it as a general-purpose LLM combining internal world knowledge and pattern completions from textual input. No model internals (architecture beyond 'LLM') or parameter counts are specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Future rainfall amounts: daily precipitation for Oct 1–15, 2023 (short-term) and monthly precipitation for Oct 2023–Sep 2024 (long-term) for 15 selected U.S. cities.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Zero-shot / few-shot prompting of the LLM to output numeric rainfall forecasts under several conditions: (Exp1) GPT-only (no extra data), (Exp2) GPT given Expert Model (EM) predictions, (Exp3) GPT given regional climate variables (Tmin/Tmax), (Exp4) GPT given teleconnection indices (Nino3.4, PDO, NAO), and a variant where EM predictions plus per-month historical standard deviation (STD) were provided. Outputs were generated as numerical forecasts (rainfall amounts) via natural-language prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Not applicable — GPT produced point forecasts (numeric rainfall amounts) rather than explicit probabilities or likelihoods; no probability or calibrated probability distributions were elicited from the LLM in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Forecasts compared to observed rainfall using Root Mean Square Error (RMSE) as primary error metric, plus Pearson correlation coefficient and Nash–Sutcliffe efficiency coefficient to assess trend-capturing ability; comparisons were made against an Expert Model (2-layer LSTM) and a 30-year historical average baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>GPT-4o produced conservative, historically-centered point forecasts that often reverted to the 30-year average. Aggregate short-term RMSE across cities: Exp1 (GPT-only) = 0.23, Exp2 (GPT+EM) = 0.20, while the Expert Model (EM, 2-layer LSTM) had RMSE = 0.06. Correlation between GPT forecasts and 30-year average (long-term): Exp1 = 0.86, Exp2 = 0.82, Exp3 = 0.76, Exp4 = 0.62; EM correlation with 30-year average = 0.59. Providing per-month historical STD along with EM predictions improved GPT outputs, making them closer to EM forecasts (qualitative and time-series improvements shown), but explicit numeric calibration metrics for that variant beyond RMSE improvements are not fully enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>LLM defaulted to conservative forecasts near long-term averages, showing low sensitivity to anomalies and extremes; struggled to effectively integrate or leverage expert-model numeric outputs (treating them like textual input), lacked explicit probabilistic output or calibration, and has no embedded physical constraints (e.g., conservation laws). Adding domain inputs sometimes degraded long-term performance. The study also notes limited data scope (15 cities) and limited prompt/technique exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared against two baselines: (1) Expert Model (EM) — a 2-layer LSTM trained on historical temperature and precipitation (EM outperformed GPT by large margins; e.g., EM RMSE 0.06 vs GPT ~0.20-0.23 short-term), and (2) 30-year historical average baseline — GPT forecasts closely matched this baseline (high correlation), while EM deviated to capture trends/anomalies. In some cases GPT reduced extreme peaks present in EM forecasts, moving predictions closer to the historical average.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Paper suggests techniques to improve LLM performance including supplying uncertainty information (per-month STD) which improved alignment with EM, and proposals for future work such as knowledge distillation, prompt engineering, and multi-task learning frameworks to help GPT learn weights/importance of climate factors; no post-hoc calibration or probabilistic elicitation methods were implemented in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Large Language Models for Climate Forecasting', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Openai, GPT-4 Technical Report <em>(Rating: 2)</em></li>
                <li>Local climate services for all, courtesy of large language models <em>(Rating: 2)</em></li>
                <li>ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on Climate Change <em>(Rating: 2)</em></li>
                <li>Thus spoke GPT-3: Interviewing a large-language model on climate finance <em>(Rating: 1)</em></li>
                <li>CLIMATEBERT: A Pretrained Language Model for Climate-Related Text <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9404",
    "paper_id": "paper-274165986",
    "extraction_schema_id": "extraction-schema-164",
    "extracted_data": [
        {
            "name_short": "GPT-4o",
            "name_full": "ChatGPT-4o (GPT-4o)",
            "brief_description": "A large language model (LLM) used in this study to generate short-term (15-day) and long-term (12-month) rainfall forecasts for 15 U.S. cities under multiple prompting conditions, both with and without expert-model inputs.",
            "citation_title": "Exploring Large Language Models for Climate Forecasting",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "A pre-trained large language model from OpenAI used via prompting to produce numerical rainfall predictions; the paper treats it as a general-purpose LLM combining internal world knowledge and pattern completions from textual input. No model internals (architecture beyond 'LLM') or parameter counts are specified in the paper.",
            "model_size": null,
            "prediction_target": "Future rainfall amounts: daily precipitation for Oct 1–15, 2023 (short-term) and monthly precipitation for Oct 2023–Sep 2024 (long-term) for 15 selected U.S. cities.",
            "prediction_method": "Zero-shot / few-shot prompting of the LLM to output numeric rainfall forecasts under several conditions: (Exp1) GPT-only (no extra data), (Exp2) GPT given Expert Model (EM) predictions, (Exp3) GPT given regional climate variables (Tmin/Tmax), (Exp4) GPT given teleconnection indices (Nino3.4, PDO, NAO), and a variant where EM predictions plus per-month historical standard deviation (STD) were provided. Outputs were generated as numerical forecasts (rainfall amounts) via natural-language prompts.",
            "probability_format": "Not applicable — GPT produced point forecasts (numeric rainfall amounts) rather than explicit probabilities or likelihoods; no probability or calibrated probability distributions were elicited from the LLM in this study.",
            "evaluation_method": "Forecasts compared to observed rainfall using Root Mean Square Error (RMSE) as primary error metric, plus Pearson correlation coefficient and Nash–Sutcliffe efficiency coefficient to assess trend-capturing ability; comparisons were made against an Expert Model (2-layer LSTM) and a 30-year historical average baseline.",
            "results": "GPT-4o produced conservative, historically-centered point forecasts that often reverted to the 30-year average. Aggregate short-term RMSE across cities: Exp1 (GPT-only) = 0.23, Exp2 (GPT+EM) = 0.20, while the Expert Model (EM, 2-layer LSTM) had RMSE = 0.06. Correlation between GPT forecasts and 30-year average (long-term): Exp1 = 0.86, Exp2 = 0.82, Exp3 = 0.76, Exp4 = 0.62; EM correlation with 30-year average = 0.59. Providing per-month historical STD along with EM predictions improved GPT outputs, making them closer to EM forecasts (qualitative and time-series improvements shown), but explicit numeric calibration metrics for that variant beyond RMSE improvements are not fully enumerated.",
            "limitations_or_challenges": "LLM defaulted to conservative forecasts near long-term averages, showing low sensitivity to anomalies and extremes; struggled to effectively integrate or leverage expert-model numeric outputs (treating them like textual input), lacked explicit probabilistic output or calibration, and has no embedded physical constraints (e.g., conservation laws). Adding domain inputs sometimes degraded long-term performance. The study also notes limited data scope (15 cities) and limited prompt/technique exploration.",
            "comparison_to_baselines": "Compared against two baselines: (1) Expert Model (EM) — a 2-layer LSTM trained on historical temperature and precipitation (EM outperformed GPT by large margins; e.g., EM RMSE 0.06 vs GPT ~0.20-0.23 short-term), and (2) 30-year historical average baseline — GPT forecasts closely matched this baseline (high correlation), while EM deviated to capture trends/anomalies. In some cases GPT reduced extreme peaks present in EM forecasts, moving predictions closer to the historical average.",
            "methods_for_improvement": "Paper suggests techniques to improve LLM performance including supplying uncertainty information (per-month STD) which improved alignment with EM, and proposals for future work such as knowledge distillation, prompt engineering, and multi-task learning frameworks to help GPT learn weights/importance of climate factors; no post-hoc calibration or probabilistic elicitation methods were implemented in this study.",
            "uuid": "e9404.0",
            "source_info": {
                "paper_title": "Exploring Large Language Models for Climate Forecasting",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Openai, GPT-4 Technical Report",
            "rating": 2,
            "sanitized_title": "openai_gpt4_technical_report"
        },
        {
            "paper_title": "Local climate services for all, courtesy of large language models",
            "rating": 2,
            "sanitized_title": "local_climate_services_for_all_courtesy_of_large_language_models"
        },
        {
            "paper_title": "ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on Climate Change",
            "rating": 2,
            "sanitized_title": "climategpt_towards_ai_synthesizing_interdisciplinary_research_on_climate_change"
        },
        {
            "paper_title": "Thus spoke GPT-3: Interviewing a large-language model on climate finance",
            "rating": 1,
            "sanitized_title": "thus_spoke_gpt3_interviewing_a_largelanguage_model_on_climate_finance"
        },
        {
            "paper_title": "CLIMATEBERT: A Pretrained Language Model for Climate-Related Text",
            "rating": 1,
            "sanitized_title": "climatebert_a_pretrained_language_model_for_climaterelated_text"
        }
    ],
    "cost": 0.0062685,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Exploring Large Language Models for Climate Forecasting</p>
<p>Yang Wang 
Hassan A Karimi 
Exploring Large Language Models for Climate Forecasting
7A54C7BF84F935728A85D7C4353EC623
With the increasing impacts of climate change, there is a growing demand for accessible tools that can provide reliable future climate information to support planning, finance, and other decision-making applications.Large language models (LLMs), such as GPT-4o, present a promising approach to bridging the gap between complex climate data and the general public, offering a way for non-specialist users to obtain essential climate insights through natural language interaction.However, an essential challenge remains underexplored: evaluating the ability of LLMs to provide accurate and reliable future climate predictions, which is crucial for applications that rely on anticipating climate trends.In this study, we investigate the capability of GPT-4o in predicting rainfall at short-term (15day) and long-term (12-month) scales.We designed a series of experiments to assess GPT's performance under different conditions, including scenarios with and without expert data inputs.Our results indicate that GPT, when operating independently, tends to generate conservative forecasts, often reverting to historical averages in the absence of clear trend signals.This study highlights both the potential and challenges of applying LLMs for future climate predictions, providing insights into their integration with climate-related applications and suggesting directions for enhancing their predictive capabilities in the field.</p>
<p>I. INTRODUCTION</p>
<p>As the impacts of climate change intensify, obtaining accurate information about future climate trends has become increasingly important.Fields such as energy planning, urban development, and weather derivatives increasingly rely on precise climate forecasts to make critical decisions [1].However, accessing, analyzing, and interpreting climate data typically requires interdisciplinary knowledge in areas such as climatology, geography, statistics, and computer science, making it challenging for the general public to utilize this information effectively.Thus, there is a growing need for straightforward, accessible ways for the public to obtain relevant and important information or services based on climate predictions [2].In recent years, large language models (LLMs) like ChatGPT-4 have provided a convenient means for the public to access specialized information [3].With advanced natural language processing capabilities, LLMs can provide individuals with complex domain-specific knowledge.By processing simple language queries, LLMs allow users to obtain the necessary Yang Wang, Hassan A. Karimi are with the Geoinformatics Laboratory, School of Computing and Information, University of Pittsburgh, 135 N Bellefield Ave, Pittsburgh, PA 15213, USA (e-mail: yaw70@pitt.edu;hkarimi@pitt.edu).</p>
<p>climate-related insights without requiring any training.The remarkable progress in knowledge integration and commonsense reasoning exhibited by LLMs has led to their exploration in various data-intensive fields, including scientific computing and financial forecasting, making climate science a promising area for LLM applications [4].In the climate domain, several studies have attempted to apply LLMs to professional communication and information dissemination [5], [6].For instance, the ClimSight project uses future forecast data from the Climate Modeling Intercomparison Project (CMIP) to provide agricultural recommendations through local climate services [7].ClimateGPT, on the other hand, trains LLMs on climate science literature to generate more specialized climate knowledge and descriptive content [8].However, most existing studies on climate LLMs focus on "descriptive output", in particular, on how to make LLMs generate content that reflects climate science terminology accurately [9].While these models excel at interpretative tasks, the essential component of LLM responses for future climate scenarios lies in their ability to accurately predict future climate trends.Without this capacity to predict future climate factors reliably, LLM outputs may fall short of meeting the practical demands of climate adaptation and planning.</p>
<p>To understand what the general public, who is not trained in climate modeling and analysis, would be able to get from LLMs on climate predictions, this study aims to explore and assess the performance of LLMs in climate prediction tasks, particularly focusing on their ability to capture trends when generating future climate data We selected ChatGPT-4o, a representative LLM, for analysis and designed a series of experiments to evaluate its performance in generating short-term (15-day scale) and long-term (12-month scale) rainfall forecasts.The contribution of our study lies in advancing beyond existing climate LLM studies that focus primarily on descriptive output.We explore the predictive capabilities of LLMs, specifically their potential to generate accurate future climate trends with and without the assistance of domain-specific knowledge.The structure of this article is as follows: Section 2 introduces the data used and the model employed as the climate expert model (EM).Section 3 describes the experiments conducted.Section 4 presents the comparative results.The discussion and conclusions are provided in Section 5.</p>
<p>II. DATA AND METHOD</p>
<p>In this study, we focused on rainfall prediction at two different time scales: short-term (15-day scale) and long-term (12-month scale).We considered 15 cities across the contiguous United States: 'Washington DC' 'Tucson, AZ' 'Salt Lake City, UT' 'Chicago, IL' 'Birmingham, AL' 'Baton Rouge, LA' and 'Atlanta, GA' These cities represent varying levels of precipitation, covering high, medium, and low rainfall areas.For each city, we collected daily data on maximum temperature, minimum temperature, and precipitation, which were used as inputs for future rainfall prediction.Figure 1 shows the locations of the cities and their annual average rainfall.The data were sourced from the following publicly available databases: Figure 1 The locations of selected cities in the United States and their corresponding annual rainfall amounts.</p>
<p>For rainfall prediction, historical daily maximum temperature, minimum temperature, and precipitation were used as inputs.</p>
<p>For temperature prediction, only minimum and maximum temperature were used as inputs.After preprocessing, the data from 1900 to 2022 were used, with 80% for training and 20% for validation.We employed a two-layer LSTM model as our climate expert model (EM) for rainfall prediction.The model used an input window of 60-time steps, corresponding to 60 days for shortterm prediction and 60 months for long-term prediction.The output window was set to 15-time steps for short-term prediction and 12-time steps for long-term prediction, respectively.The LSTM hidden size was set to 128, and a batch size of 64 was used.The model was trained over 500 epochs, with the Adam optimizer.The specific input features for rainfall prediction included historical minimum temperature, maximum temperature, and precipitation over the input window, predicting future precipitation.For temperature prediction, only the corresponding minimum and maximum temperatures were used as inputs.</p>
<p>To evaluate the model's predictive accuracy, we used data starting from September 30, 2023, as the baseline.For shortterm predictions, we used the 60 days prior to this date to forecast daily rainfall from October 1 to October 15, 2023.For long-term predictions, we used the 60 months prior to October 2023 to forecast monthly rainfall from October 2023 to September 2024.This setup allowed us to evaluate the EM model's performance on both short-term and long-term rainfall forecasting tasks.</p>
<p>III. EXPERIMENTAL SETUP</p>
<p>In this study, to systematically evaluate the performance of LLMs in climate forecasting tasks and to analyze their ability to generate future climate data trends and patterns under various input scenarios, we designed several experiments.These experiments are intended to assess the LLM's ability to generate rainfall predictions with and without the support of specialized knowledge.The different experimental conditions also range from direct to indirect provision of expert information, allowing for a comprehensive analysis of GPT's potential in integrating climate data and generating forecasts.</p>
<p>The specific experimental setups are as follows:</p>
<p>Experiment 1: GPT-only Prediction</p>
<p>In the first experiment, GPT-4o was tasked with independently generating rainfall predictions without any additional expert information or data input.This condition mainly assesses GPT's ability to produce short-term and long-term rainfall predictions based solely on its pre-trained knowledge.By analyzing its predictions, we aim to uncover GPT's inclination in judging future climate trends without external guidance.This experiment helps us understand the GPT's interpretation of climate events based on its common-sense knowledge.We used the following prompt sample: --Please use the supplied data to predict the rainfall for the above period.</p>
<p>Experiment 2: GPT-EM Prediction</p>
<p>In the second experiment, we provided GPT-4o with rainfall predictions generated by a climate expert model (EM) and asked it to make further predictions based on this expert data.This condition is designed to examine whether GPT, after receiving direct forecast support from a specialized model, can effectively utilize this data to capture trends or adjust its predictions.By comparing GPT's predictions with and without the expert data support, we can analyze its ability to integrate external information.We used the following prompt sample:  {v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,v11,v12,v13,v14,v15} --Please use the supplied data to predict the rainfall for the above period.
Prompt</p>
<p>Experiment 3: GPT-Regional Climate Prediction</p>
<p>In the third experiment, we indirectly provided GPT with predictions of regional climate factors related to rainfall (minimum and maximum temperatures) and tasked it with generating rainfall predictions based on these climate factors.In this setup, GPT did not receive direct rainfall predictions but instead relied on relevant climate variables as hints to infer trends.This experiment aims to evaluate whether GPT can use indirect climate information to generate reasonable rainfall predictions and assess its ability to understand the relationships between climate variables in the absence of explicit rainfall data.We used the following prompt sample:  Tmin: {v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,v11,v12,v13,v14,v15} Tmax: {v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,v11,v12,v13,v14,v15} --Please use the supplied data to predict the rainfall for the above period.
Prompt</p>
<p>Experiment 4: GPT-Teleconnection Prediction</p>
<p>In the fourth experiment, to further explore GPT's understanding of large-scale climate indices, we provided it with global teleconnection indices, such as Nino3.--potential forecast--Period: {October 1, 2023, to October 15, 2023} Nino3.4: {v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,v11,v12,v13,v14,v15} PDO: {v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,v11,v12,v13,v14,v15} NAO: {v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,v11,v12,v13,v14,v15} --Please use the supplied data to predict the rainfall for the above period.</p>
<p>Baseline Comparison: 30-year Historical Average</p>
<p>To assess the accuracy and trend consistency of GPT's predictions, we used the average daily/monthly rainfall over the past 30 years as a baseline, against which we compared GPT's predictions generated under different experimental conditions.This baseline allows us to evaluate whether GPT's predictions tend to revert to historical averages and observe the differences in GPT's forecasts across various experimental settings.We used Root Mean Square Error (RMSE) to measure the error between different predictions and actual rainfall.Additionally, we evaluated the accuracy and trend-capturing ability of the predictions using Pearson's correlation coefficient and Nash-Sutcliffe's efficiency coefficient.</p>
<p>IV. RESULT As shown, the EM model achieves the best results in both longterm and short-term predictions.We are interested in examining the differences in LLM inferences when professional knowledge is directly incorporated compared to when it is not.However, when comparing Exp1 and Exp2, we found that providing GPT with relevant domain-specific knowledge and asking it to integrate this with its own knowledge did not significantly improve the results over relying solely on GPT.In short-term predictions, the average RMSE across all cities for Exp1 and Exp2 was 0.23 and 0.20, respectively, both notably higher than the EM model's 0.06.For long-term predictions, adding domain-specific knowledge actually increased RMSE compared to Exp1 and reduced the correlation coefficient.</p>
<p>Although the provided domain knowledge might offer better predictive insights, results from both short-term and long-term predictions indicate that GPT's internal knowledge still plays a dominant role in its inferences.We are also interested in observing what results GPT can provide when we input factors related to rainfall indirectly, rather than inputting rainfall data directly.Specifically, for long-term predictions, we added teleconnection-related factors (Exp4).In short-term predictions, the indirect provision of regional meteorological factors had limited impact on results; Exp3 showed an RMSE similar to Exp1 and Exp2, with a slightly improved correlation coefficient.However, in longterm predictions, whether adding regional factors or global teleconnection factors, GPT's results declined compared to when this knowledge was not added.</p>
<p>Comparing the results of different experiments with the 30-year historical average, we observed that GPT-generated predictions closely align with the 30-year average in both short-term and long-term forecasts.This similarity suggests that, in the absence of strong trends or notable anomalies, GPT tends to generate conservative predictions that resemble long-term statistical averages.This tendency is even more pronounced in long-term forecasts: we calculated the correlation coefficients between each experiment's results and the 30-year average, finding values of 0.86 for Exp1, 0.82 for Exp2, 0.76 for Exp3, and 0.62 for Exp4, while the EM model showed a lower correlation of only 0.59.This may be due to GPT's limited understanding of physical climate processes, leading it to default to safer, historically consistent forecasts.Additionally, this tendency reflects GPT's low sensitivity to complex climate patterns; in scenarios with limited data or high uncertainty, it relies on historical averages for robustness.However, this approach limits GPT's ability to capture potential trend shifts and the likelihood of extreme climate events.We further explored the differences between the results generated by GPT-4o and those from the EM model at each time point.In Figures 4 and 5, we compare the time series of short-term and long-term predictions.For the short-term predictions, we found that Exp2 tends to produce results closer to the multi-year average, especially during peak phases in the series.For peak values in the EM predictions, Exp2 noticeably dampened the magnitude of these peaks.For example, in Atlanta, there was an increase in rainfall on October 11 and 12, 2023, relative to the preceding days.The EM model effectively captured this upward trend, while Exp2 significantly reduced the peak values on these dates, bringing them down from the EM's 0.5-0.6 range to the 30-year average level of around 0.1-0.2.Similar patterns were observed on October 11 in Pensacola, October 6 and 14 in New York, and October 4 in Dallas.For peak values that were incorrectly predicted by the EM model, Exp2 also reduced their magnitude, as seen on October 15 in Phoenix and October 4 in Tucson.</p>
<p>We also found that Exp2's tendency to generate results close to the multi-year average was more pronounced in cities with higher rainfall.For instance, in Mobile and Baton Rouge, the results generated by Exp2 at each time point were closer to the 30-year average.A similar pattern can also be observed in the long-term predictions, where for all cities, Exp2's monthlyscale results were closer to the multi-year average at each time point.This tendency likely reflects GPT's preference for a more stable prediction aligned with the historical average, rather than adjusting fully to the extreme values suggested by the professional model.It also indicates that when encountering anomalies or extreme values, GPT is inclined to revert toward the average, leading to a smoothing effect in its predictions.As a large language model, GPT is fundamentally trained to learn language patterns from vast amounts of textual data, rather than physical processes.This means that it lacks the inherent physical constraints present in climate systems (such as energy conservation, atmospheric and ocean dynamics) and cannot consider causal relationships in predictions as physical models do.Thus, when predicting future rainfall trends, GPT primarily relies on "patterns in text" and "common-sense associations."When it fails to detect clear trend signals, it defaults to generating results aligned with historical averagesa relatively safe and conservative inference approach that avoids extreme or highly volatile predictions.This strategy reduces the risk of producing anomalous forecasts but also limits GPT's sensitivity to changing trends.GPT's generation mechanism, based on language patterns, is more adept at producing general and trend-based content, yet often lacks sensitivity to rare or anomalous events.Extreme climate events are relatively infrequent in historical data, so without specialized training, GPT may lean toward predictions near the average, thus minimizing the risk of extreme error.This tendency causes GPT to smooth out results when dealing with extreme rainfall events, missing abnormal signals and downplaying the significance of extreme events.When directly provided with rainfall predictions from professional models (such as the EM model), GPT may not accurately interpret the data or effectively adjust its prediction strategy based on it.In other words, GPT is likely to treat this input as textual information, struggling to extract useful climate patterns or physical principles from it.This limitation diminishes its effectiveness in integrating specialized knowledge into its predictions.</p>
<p>In our study, we experimented with an alternative approach for long-term monthly-scale predictions.We calculated the standard deviation of historical rainfall for each calendar month.A high standard deviation indicates greater rainfall variability for that month, which corresponds to a higher prediction difficulty, while a low standard deviation suggests that rainfall levels are more consistent, indicating potentially lower prediction difficulty [11].We used this standard deviation as a representation of potential uncertainty and input it into GPT along with the EM's predicted rainfall using the following prompt:  {v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,v11,v12,v13,v14,v15} Standard Deviation: {s1,s2,s3,s4,s5,s6,s7,s8,s9,s10,s11,s12,s13,s14,s15} --Please use the supplied data to predict the rainfall for the above period.
Prompt
This setup provided improved results.As shown in Figure 6, after adding the STD information, GPT's results are now close to those of the EM, with a significant improvement compared to Exp2.We present the time series comparison for the three cities with the most significant RMSE improvement in Figure 7.We observed that after adding this uncertainty information, GPT tended to provide results closer to those of the EM.However, we did not achieve the expected outcome where GPT would adjust the predictions with greater potential uncertainty.In this study, we analyzed only a limited set of cities and time series, which imposes certain constraints.Additionally, we provided domain knowledge through direct prompts.In the future, optimization strategies such as knowledge distillation, prompt engineering, or multi-task learning could be explored to improve GPT's understanding and handling of climate data [12].For instance, incorporating a multi-task learning framework might allow GPT to autonomously learn the weights and importance of climate factors when processing climate data, potentially helping it to better integrate domain-specific knowledge and demonstrate greater potential in climate forecasting.</p>
<p>VI. CONCLUSION</p>
<p>This study investigates the ability of large language models (LLMs), specifically ChatGPT-4o, to provide future climate information, focusing on rainfall prediction accuracy.We utilized a 2-layer LSTM model as an Expert Model (EM), assuming it could generate reliable future predictions.Through a series of experiments, we compared ChatGPT-4's rainfall predictions under varying conditions: relying solely on its internal knowledge, directly receiving rainfall predictions from the EM, and indirectly inferring rainfall from other related factors predicted by the EM.These experiments evaluated both short-term (15-day, daily scale) and long-term (12-month, monthly scale) prediction capabilities.The results reveal that ChatGPT-4 consistently prioritizes stable predictions closely aligned with historical averages, regardless of whether it integrates additional information from the EM.This tendency highlights the LLM's inherent bias towards conservative outputs, which may limit its effectiveness in capturing dynamic or extreme variations in climate scenarios.</p>
<p>Figure 2 and
2
Figure 2 and Figure 3 present comparisons across different scales and scenarios.In this study, the EM model utilizes an LSTM structure, leveraging historical temperature and rainfall data to predict future rainfall.This time series-based deep learning model effectively captures the temporal dependencies within climate data, making it particularly suitable for detecting long-term trends and periodic patterns in meteorological data.As shown, the EM model achieves the best results in both longterm and short-term predictions.We are interested in examining the differences in LLM inferences when professional knowledge is directly incorporated compared to when it is not.However, when comparing Exp1 and Exp2, we found that providing GPT with relevant domain-specific knowledge and</p>
<p>Figure 2 Figure 3
23
Figure 2 Performance comparison of rainfall prediction across different experimental conditions (short-term)</p>
<p>Figure 4 Figure 5 Fig. 4 .
454
Figure 4 Time series comparison of short-term rainfall predictions across experimental conditions for different cities</p>
<p>Figure 6 Figure 7
67
Figure 6 Performance comparison of rainfall prediction across Exp2, EM and another experiment of adding standard deviation</p>
<p>• https://www.ncei.noaa.gov/pub/data/ghcn/daily/• https://kilthub.cmu.edu/articles/dataset/Compiled_daily_temperature_and_precipitation_data_for_the_U_S _cities/7890488 [10] • https://www.weather.gov/wrh/climate?wfo=sew.</p>
<p>4, the Pacific Decadal Oscillation (PDO), and the North Atlantic Oscillation (NAO), and instructed it to generate a 12-month-scale rainfall forecast based on this predictive information.These teleconnection indices are crucial variables in climate forecasting, revealing the long-term influence of large-scale climate patterns on variables such as rainfall.It should be noted that in this experiment, we used the actual values of these indices from October 2023 to September 2024 as inputs to serve as a comparison.We used the following prompt sample:
Prompt Sample 4:You are a climate data prediction system focused primarilyon forecasting rainfall for selected cities. Your timestamp isSeptember 30, 2023, meaning you only consider informationavailable prior to this date. I will provide you with theNino3.4, Pacific Decadal Oscillation (PDO), and NorthAtlantic Oscillation (NAO) indices for the prediction period{October 1, 2023, to October 15, 2023}. Please integrate thisinformation, consider their climate teleconnection
relationship with potential regional rainfall, and combine it with your own knowledge to make a final prediction.For the time being, please ignore narrative responses; I am only interested in numerical results.</p>
<p>Weather forecasting for weather derivatives. S D Campbell, F X Diebold, 10.1198/016214504000001051J Am Stat Assoc. 100469Mar. 2005</p>
<p>The global framework for climate services. C Hewitt, S Mason, D Walland, 10.1038/nclimate1745Dec. 2012</p>
<p>. Openai, GPT-4 Technical Report</p>
<p>Thus spoke GPT-3: Interviewing a large-language model on climate finance. M Leippold, 10.1016/j.frl.2022.103617Financ Res Lett. 53May 2023</p>
<p>Climate Change from Large Language Models. H Zhu, P Tiwari, </p>
<p>Enhancing Large Language Models with Climate Resources. M Kraus, </p>
<p>Local climate services for all, courtesy of large language models. N Koldunov, T Jung, 10.1038/s43247-023-01199-1Dec. 01, 2024Nature Publishing Group</p>
<p>ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on Climate Change. D Thulke, 2024</p>
<p>CLIMATEBERT: A Pretrained Language Model for Climate-Related Text. N Webersinke, M Kraus, J A Bingler, M Leippold, 2022</p>
<p>Use of Historical Data to Assess Regional Climate Change. Y Lai, D A Dzombak, 10.1175/JCLI-D-18</p>
<p>Hidden Tropical Pacific Sea Surface Temperature States Reveal Global Predictability for Monthly Precipitation for Sub-Season to Annual Scales. M Zhang, J D Rojo-Hernández, L Yan, Ó J Mesa, U Lall, 10.1029/2022GL099572Geophys Res Lett. 49202022</p>
<p>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation. Q Wu, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
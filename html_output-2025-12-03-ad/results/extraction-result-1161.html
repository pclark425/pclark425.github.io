<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1161 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1161</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1161</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-173188950</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1905.13315v2.pdf" target="_blank">Graph Attention Memory for Visual Navigation</a></p>
                <p><strong>Paper Abstract:</strong> Visual navigation in complex environments is inefficient with traditional reactive policy or general-purposed recurrent policy. To address the long-term memory issue, this paper proposes a graph attention memory (GAM) architecture consisting of memory construction module, graph attention module and control module. The memory construction module builds the topological graph based on supervised learning by taking the exploration prior. Then, guided attention features are extracted with the graph attention module. Finally, the deep reinforcement learning based control module makes decisions based on visual observations and guided attention features. Detailed convergence analysis of GAM is presented in this paper. We evaluate GAM-based navigation system in two complex 3D environments. Experimental results show that the GAM-based navigation system significantly improves learning efficiency and outperforms all baselines in average success rate.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1161.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1161.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAM-ViZDoom</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Attention Memory (GAM) for Visual Navigation in ViZDoom mazes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A navigation system that constructs a topological graph from exploration images and uses a recurrent graph-attention module to extract guided features which, together with visual observations, drive an A2C policy; designed to address long-term memory and long-distance goal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>ViZDoom maze1 and maze2</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>First-person 3D maze navigation tasks in the ViZDoom simulator (indoor maze layouts); agent is randomly spawned and given a goal image to reach using visual observations.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Topological graph built from visual-similarity connections (nodes = image embeddings); edges chosen by a threshold on connection probability (top-L highest probabilities). Locally connected / sparse (mostly nearby neighbors), explicit effort to avoid impossible connections through walls.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Physical maze canvas sizes: maze1 768×768, maze2 1280×640 (pixels). Graph edges selected: L=1300 (maze1) and L=1800 (maze2) highest-probability connections; number of exploration samples N not specified explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GAM + synchronous A2C (actor-critic)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent uses ResNet-18 encoder to build node embeddings and a connection CNN to build edges. A graph-attention module (siamese attention ψ) recurrently aggregates neighbor features for K iterations (interpreted as a random walk / diffusion over W) producing guided attention vector η_t = x_t^(K) - x_g^(K). The policy/value networks share convolutional layers and are trained with synchronous advantage actor-critic (A2C); attention weights θ_a are trained end-to-end with policy/value parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Episode reward (total reward per episode), success rate within a fixed step budget (reach-goal within 500 steps), training sample-efficiency (learning curve / steps to convergence)</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>GAM converges faster and to higher episode reward than baselines; example numeric scores (Table 1): Maze2 score: GAM 100.5 vs FF-nogoal -289.95, FF-goal -299.95, LSTM-nogoal -299.95. (Exact training-curve convergence steps not tabulated.)</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Maze1: 100% (GAM and baselines). Maze2: GAM 100%; baselines (FF-nogoal, FF-goal, LSTM-nogoal) 10%.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Memory-based / topological graph attention policy (GAM) outperforms reactive and general-purpose recurrent (LSTM) policies in complex, long-distance mazes.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>The paper reports that environments with greater navigational distance/complexity (maze2) strongly benefit from an explicit topological memory and multi-step (recurrent) graph aggregation: spreading goal-related features via repeated attention (interpretable as diffusion/random walk on the graph) helps the agent receive informative guided features for long-horizon planning. Reactive policies and single-step aggregation fail when the goal is far (leading to poor reward and low success); GAM's recurrent aggregation mitigates the long-term memory issue. Accurate graph connectivity (avoiding spurious long-range/wall-penetrating edges) is important to prevent misleading attention flows.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Two maze layouts of different complexity (maze1 simpler, maze2 more complex and farther goals) were tested: all agents succeed on the easier maze1, but only GAM achieves high success on the more complex maze2 (100% vs 10% for baselines), demonstrating that higher-complexity / likely higher effective diameter environments reveal the advantage of the topological memory and recurrent attention.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies for high-distance / high-diameter environments require explicit environment memory and mechanisms to propagate information across many hops (GAM's recurrent graph-attention acts as this mechanism). Reactive policies and vanilla LSTMs are insufficient for very long-horizon goal-reaching; recurrent graph aggregation (multi-step attention / diffusion) provides stationary aggregated features that improve navigation and reduce jittering, implying that policy architectures should include structured memory/topology-aware components when graph topology includes long shortest-path distances.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Attention Memory for Visual Navigation', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Semi-parametric topological memory for navigation <em>(Rating: 2)</em></li>
                <li>Cognitive mapping and planning for visual navigation <em>(Rating: 2)</em></li>
                <li>Neural map: Structured memory for deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Learning to navigate in complex environments <em>(Rating: 1)</em></li>
                <li>Learning to navigate in cities without a map <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1161",
    "paper_id": "paper-173188950",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "GAM-ViZDoom",
            "name_full": "Graph Attention Memory (GAM) for Visual Navigation in ViZDoom mazes",
            "brief_description": "A navigation system that constructs a topological graph from exploration images and uses a recurrent graph-attention module to extract guided features which, together with visual observations, drive an A2C policy; designed to address long-term memory and long-distance goal reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "ViZDoom maze1 and maze2",
            "environment_description": "First-person 3D maze navigation tasks in the ViZDoom simulator (indoor maze layouts); agent is randomly spawned and given a goal image to reach using visual observations.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Topological graph built from visual-similarity connections (nodes = image embeddings); edges chosen by a threshold on connection probability (top-L highest probabilities). Locally connected / sparse (mostly nearby neighbors), explicit effort to avoid impossible connections through walls.",
            "environment_size": "Physical maze canvas sizes: maze1 768×768, maze2 1280×640 (pixels). Graph edges selected: L=1300 (maze1) and L=1800 (maze2) highest-probability connections; number of exploration samples N not specified explicitly.",
            "agent_name": "GAM + synchronous A2C (actor-critic)",
            "agent_description": "Agent uses ResNet-18 encoder to build node embeddings and a connection CNN to build edges. A graph-attention module (siamese attention ψ) recurrently aggregates neighbor features for K iterations (interpreted as a random walk / diffusion over W) producing guided attention vector η_t = x_t^(K) - x_g^(K). The policy/value networks share convolutional layers and are trained with synchronous advantage actor-critic (A2C); attention weights θ_a are trained end-to-end with policy/value parameters.",
            "exploration_efficiency_metric": "Episode reward (total reward per episode), success rate within a fixed step budget (reach-goal within 500 steps), training sample-efficiency (learning curve / steps to convergence)",
            "exploration_efficiency_value": "GAM converges faster and to higher episode reward than baselines; example numeric scores (Table 1): Maze2 score: GAM 100.5 vs FF-nogoal -289.95, FF-goal -299.95, LSTM-nogoal -299.95. (Exact training-curve convergence steps not tabulated.)",
            "success_rate": "Maze1: 100% (GAM and baselines). Maze2: GAM 100%; baselines (FF-nogoal, FF-goal, LSTM-nogoal) 10%.",
            "optimal_policy_type": "Memory-based / topological graph attention policy (GAM) outperforms reactive and general-purpose recurrent (LSTM) policies in complex, long-distance mazes.",
            "topology_performance_relationship": "The paper reports that environments with greater navigational distance/complexity (maze2) strongly benefit from an explicit topological memory and multi-step (recurrent) graph aggregation: spreading goal-related features via repeated attention (interpretable as diffusion/random walk on the graph) helps the agent receive informative guided features for long-horizon planning. Reactive policies and single-step aggregation fail when the goal is far (leading to poor reward and low success); GAM's recurrent aggregation mitigates the long-term memory issue. Accurate graph connectivity (avoiding spurious long-range/wall-penetrating edges) is important to prevent misleading attention flows.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Two maze layouts of different complexity (maze1 simpler, maze2 more complex and farther goals) were tested: all agents succeed on the easier maze1, but only GAM achieves high success on the more complex maze2 (100% vs 10% for baselines), demonstrating that higher-complexity / likely higher effective diameter environments reveal the advantage of the topological memory and recurrent attention.",
            "policy_structure_findings": "Policies for high-distance / high-diameter environments require explicit environment memory and mechanisms to propagate information across many hops (GAM's recurrent graph-attention acts as this mechanism). Reactive policies and vanilla LSTMs are insufficient for very long-horizon goal-reaching; recurrent graph aggregation (multi-step attention / diffusion) provides stationary aggregated features that improve navigation and reduce jittering, implying that policy architectures should include structured memory/topology-aware components when graph topology includes long shortest-path distances.",
            "uuid": "e1161.0",
            "source_info": {
                "paper_title": "Graph Attention Memory for Visual Navigation",
                "publication_date_yy_mm": "2019-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Semi-parametric topological memory for navigation",
            "rating": 2,
            "sanitized_title": "semiparametric_topological_memory_for_navigation"
        },
        {
            "paper_title": "Cognitive mapping and planning for visual navigation",
            "rating": 2,
            "sanitized_title": "cognitive_mapping_and_planning_for_visual_navigation"
        },
        {
            "paper_title": "Neural map: Structured memory for deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "neural_map_structured_memory_for_deep_reinforcement_learning"
        },
        {
            "paper_title": "Learning to navigate in complex environments",
            "rating": 1,
            "sanitized_title": "learning_to_navigate_in_complex_environments"
        },
        {
            "paper_title": "Learning to navigate in cities without a map",
            "rating": 1,
            "sanitized_title": "learning_to_navigate_in_cities_without_a_map"
        }
    ],
    "cost": 0.007953499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Graph Attention Memory for Visual Navigation</p>
<p>Dong Li 
Qichao Zhang zhangqichao2014@ia.ac.cn 
Dongbin Zhao dongbin.zhao@ia.ac.cn 
Yuzheng Zhuang zhuangyuzheng@huawei.com 
Bin Wang wangbin158@huawei.com 
Wulong Liu liuwulong@huawei.com 
Rasul Tutunov rasul.tutunov@huawei.com 
Jun Wang 
Huawei Noah&apos; 
Ark Lab </p>
<p>Institute of Automation
The State Key Laboratory of Management and Control for Complex Systems
Chinese Academy of Sciences
BeijingChina</p>
<p>University of Chinese Academy of Sciences
BeijingChina</p>
<p>Graph Attention Memory for Visual Navigation</p>
<p>Visual navigation in complex environments is inefficient with traditional reactive policy or general-purposed recurrent policy. To address the long-term memory issue, this paper proposes a graph attention memory (GAM) architecture consisting of memory construction module, graph attention module and control module. The memory construction module builds the topological graph based on supervised learning by taking the exploration prior. Then, guided attention features are extracted with the graph attention module. Finally, the deep reinforcement learning based control module makes decisions based on visual observations and guided attention features. Detailed convergence analysis of GAM is presented in this paper. We evaluate GAM-based navigation system in two complex 3D environments. Experimental results show that the GAM-based navigation system significantly improves learning efficiency and outperforms all baselines in average success rate. on visual observations. Therefore, the general-purposed LSTM policy which was demonstrated empirically in[18]is also insufficient to handle the visual navigation task.One way to alleviate the long-term memory issue is to explicitly introduce an external memory to represent the environment and facilitate the learning. In the robotics community, there are two popular ways to represent the environment, i.e., the metric map and the topological map[21]. The metric map-based methods[5,24,14]maintain a metric-based belief of the world which is updated and utilized to extract global features along with the learning process. The metric map typically divides the environment into fine-grained grid-world with each cell contains a precise metric measurement. However, in the visual navigation, accurate metric measurements may not always be necessary. Moreover, learning a good metric map itself is non-trivial[5, 1].</p>
<p>Introduction</p>
<p>Recently, visual navigation has received a lot of attention and has been applied to many fields, e.g. the house-robot navigation [26,2] and large city navigation [11]. In order to efficiently navigate to the goal, there are many aspects an agent needs to concern. Firstly, due to the rich information of the visual observation, the agent needs to extract essential features to facilitate the learning process. Secondly, the agent should be capable of locating itself and utilizing the temporal relationship at different timescales to move towards the target position. Finally, the agent needs to well understand the environment's layout to generalize to new initial and goal positions. These factors make visual navigation a challenging problem.</p>
<p>With the success of deep reinforcement learning (DRL) methods, remarkable performance has been achieved in the visual navigation task. The corresponding approaches can be broadly classified into two categories: firstly, the reactive methods [26] in which a feed-forward policy directly takes the observation and predicts the action; secondly, the general-purposed temporal memory based approach which determines the action by using a recurrent policy, e.g., the long short-term memory (LSTM) network [10]. However, these DRL methods have limitation in dealing with the long-term memory task which is a key issue in visual navigation. Due to the inherent architecture, the reactive policy predicts the action passively, and cannot remember the temporal relationship of consecutive observations and the environment's layout. Although the LSTM network can handle temporal features of consecutive observations, it can hardly propagate temporal features hundreds of steps away based In order to build the external memory while not cause too much map representation learning overhead as in the metric map, the topological map is introduced to represent the environment as in [17]. We rely on the intuition that when human-beings walk into a room, they locate themselves by utilizing some key landmarks, which provide visual clues to guide the following moves. Hence we let the agent first do explorations and collect visual observations as priors. The key landmarks can be identified from the priors by measuring pairwise similarities. Then a topological graph memory [17] would be built by using these key observations. Given this topological graph memory, we employ the soft-attention mechanism [22] of graph convolutional network to recurrently extract guided attention features by concerning its neighbor nodes in the graph. We call this proposed topological graph memory with recurrent attention mechanism as the graph attention memory (GAM) and give a detailed convergence analysis about it. Finally, we build a DRL navigation system by integrating the GAM into the agent. Note the proposed GAM module is independent to the DRL methods. As a result, it can be plugged into any DRL methods as an auxiliary module to facilitate visual navigation.</p>
<p>We evaluate our method for the goal-seeking visual navigation task in two complex 3D mazes of ViZDoom [8]. The agent is randomly spawned at several positions in the maze with a goal image provided. It needs to determine the optimal actions for reaching the goal based on its current visual observation and guided features extracted by GAM, hence motivating to learn the goal-directed knowledge of the environment layout from the topological memory. The experimental results show that the proposed method outperforms the baselines both in the learning speed and the learning performance. Additionally, the model can also generalize to novel initial or goal positions.</p>
<p>Related Work</p>
<p>There are plenty of literature for the visual navigation task. Here we focus on some of the related works. Recently, the methods which combine deep neural networks and reinforcement learning have achieved remarkable performance on many tasks such as ATARI games [12], the game of Go [20], autonomous driving [9], and vehicle classification [25]. In [26], a siamese CNN based reactive policy is proposed to control an agent to transfer among different goals. [15,19] also employ a reactive policy to navigate in a three-dimensional maze. In addition, some works employ the general-purposed LSTM networks to deal with the temporal relationship. For example, several LSTM modules are stacked in the policy network to enhance the temporal memory in [11,16]. Although these approaches can reach the goal, but the training process is relatively long due to the long-term memory issue in visual navigation.</p>
<p>To alleviate the long-term memory issue, the external memory has been employed in many recent works to address the robot navigation task. The metric representation and the topological representation are two popular ways to describe the world in the robotics community. For the metric-based methods, a global egocentric top-view metric map is employed to predict free space and the corresponding confidence in [5]. In [14,24], a differential neural computer [4] is used to update the metric map and retrieve the hidden features. In [3], the 3D points in the depth image are projected into a 2D grid world to form the metric map. [1] also uses the 3D to 2D projection approach to update the map, but they project the points from the final-layer feature map of a convolutional neural network (CNN) instead. An alternative way to represent the world is by using the topological map. Authors in [6] regard the demonstration samples as the key landmarks and construct a topological path description using the landmark images and the corresponding actions. The agent is trained based on the imitation learning. Different from their work, we build the topological map by using aimless exploration samples instead of the expert's task-orientated demonstration samples. Besides, the learning approach in this paper is based on the reinforcement learning rather than the imitation learning. The work of [17] is more related to ours. They also build a topological map using aimless exploration samples. However, our method differs in the navigation framework and the way to use the topological map. The work in [17] explicitly searches the shortest path by using Dijkstra's algorithm, then reach the intermediate waypoints one-by-one based on a supervised learning model. In this paper, we regard the topological map as an environment prior and extract guided features using the GAM module to facilitate reinforcement learning without any supervised signal for policy training. In addition, the GAM module is relatively independent to the DRL framework and can be used in other DRL approaches and the topological graph-based tasks.</p>
<p>Methodology</p>
<p>In this section, we detail the proposed visual navigation approach. The whole system includes three modules: the topological graph memory construction module, the guided attention feature extraction module, and the DRL control module as is shown in Figure 1. It works as following: 1) An exploration database is collected by manually or randomly control the agent to explore the environment. A binary classification neural network is employed to predict the sample similarities based on the topological graph memory. 2) The agent is located based on the similarity probabilities between the current observation and the node features. The goal is also located similarly based on the visual similarity. By recurrently aggregating the neighbor node features, the guided attention features of the current node and goal node are obtained. 3) The DRL agent accepts the latent features from observation and the guided attention features to make the decision.</p>
<p>Memory Construction Module</p>
<p>This module attempts to construct the topological graph G(V, E) of the environment based on the exploration samples. Since the exploration sample itself is a visual description of the corresponding environment region, the primitive method is to extract the visual features from the sample by using a CNN encoder. The node is represented by the visual features of the encoder output. Then a topological graph can be obtained by connecting the nodes of the consecutive exploration samples. However, the direct connection method has the limitation that the graph might be insufficient to describe the environment. For example, if two images are not observed consecutively but close in the environment, then they will not be connected. But they should be connected so as to give a visual description of the place where they are both located in.</p>
<p>With respect to the latent node connection issues above, an alternative approach is to employ a classification CNN to predict the connection probability of two observations [17] besides the direct connection. In details, given two candidate observations o i , o j , the classification CNN φ(., .; θ g ) predicts the corresponding connection probabilities, i.e. p i,j = φ(o i , o j ; θ g ). Here θ g are the network weights. Since the graph edges only represent the connectivity rather than the connection weights, so the underlying network aims to deal with the binary classification task. While the learning task of the node connection problem is relatively simple, the key is to generate the valid training samples effectively. Assume the observation at time t is o t in the exploration phase. In order to remove the trivial consecutive samples and consider relatively similar samples, the label y t,k = 1 for the future sample o k falling in the horizon T min ≤ k − t ≤ T max , otherwise y t,k = 0. Denote the connection CNN prediction asŷ i,j = φ(o i , o j ; θ g ). The network weights θ g can be optimized by minimizing the following cross-entropy loss
L(θ g ) = 1 M i y i,j log(ŷ i,j ) + (1 − y i,j ) log(1 −ŷ i,j ),(1)
where M is the number of mini-batch samples.</p>
<p>After training the connection CNN φ(., .; θ g ), the topological graph memory can be constructed by taking the exploration prior. Assume the exploration sample database is
D exp = {o i } N i=1
where N is the amount of exploration samples. The nodes in the graph can be determined by the exploration samples. Each node is represented by the embedding of each sample extracted after the last convolutional layer of network φ(·, ·; θ g ). For a specific sample o i ∈ D exp , the probabilities of being connected with other nodes is computed by
p i = φ(o i , o k ; θ g ), ∀k = 1, ..., N.(2)
Therefore, the probability vector p i ∈ R N where p i,j is the connection probability of node i and j.</p>
<p>Given the threshold , the edge e i,j of node i and j can be obtained by following
e i,j = 1, if p i,j ≥ , 0, otherwise.(3)
Based on the edges of all samples in D exp , the topological graph memory is then built. Note the graph is built only based on the observations without exploration actions. In this way, the graph is agent-agnostic which means the underlying physics dynamics of the agent is blocked away.</p>
<p>Graph Attention Module</p>
<p>This module aims to take advantage of the constructed graph memory above and extract essential guided features that can help the agent to reach the goal. To this end, the first step is to localize the agent and the goal on the graph. Since the node connection CNN φ(o i , o j ; θ g ) in section 3.1 predicts the sample similarity, we can localize the agent in the graph by feeding the current observation into it and pick the most similar node as the current position. Note that in the localization process, the agent only uses the observation and does not have any access to the location ground-truth. The same operation can be used to localize the goal node. In practice, due to the environment texture similarity, selecting the node of maximum similarity will cause the localization jittering. To increase the localization robustness, we pick up top L similar nodes and select the node of median similarity score as the current node. After localization, the guided attention features are extracted as follows.</p>
<p>Assume there are N nodes in the graph node set
{x i } N i=1 where x i ∈ R D
is the node feature vector extracted from the final convolutional layer of graph construction CNN in section 3.1. For a node i and the node j of its neighbor N i , the siamese attention network ψ(x i , x j ; θ a ) parameterized by weights θ a is employed to transfer them into the embedding space. According to [23], feeding the output of ψ(x i , x j ; θ a ) as the logit to the softmax function will obtain the pair-wise attention score in the embedding space. Therefore, the attention coefficient can be computed by following
α i,j = e ψ(xi,xj ;θa) ∀k∈Ni e ψ(xi,x k ;θa) .(4)
The aggregated feature of node i, i.e. x i can be obtained by taking an inner-product operation over the attention coefficients of N i and the corresponding node features  Figure 2: The graph attention memory with K recurrent iterations. After multiple recurrent operations, the goal node feature will spread and be aggregated into the current node through the feature flow.
x i = ∀j∈Ni α i,j x j .(5)
Note that the neighbor N i = {i} ∪ {j : (i, j) ∈ E} includes the node i itself in order to keep its own feature in the aggregated feature x i . Since we want the agent automatically learn where to focus on the graph, the attention network ψ(·, ·; θ a ) is trained end-to-end by maximizing the cumulative reward. The details are described in section 3.3.</p>
<p>Actually, the operation in Eq. (5) is a one-step feature aggregation operation. A recurrent feature aggregation operation can be employed to obtain more stable aggregated features. We provide two ways to understand this. Firstly, we show this intuitively. Then, the convergence proof of the recurrent feature aggregation operation is formally given.</p>
<p>Typically the goal is hundreds of steps away from the initial node. The One-step feature aggregation in Eq. (5) is insufficient to spread the temporal features between the current node and the goal node. To address this issue, a multi-step recurrent iteration mechanism over the graph memory is proposed as shown in Figure 2. The red dashed lines represent several feature flow paths. Assume the aggregated node feature at kth iteration is x
[k]
i , then the feature at k + 1th iteration is obtained by x
[k+1] i = ∀j∈Ni α i,j x [k] j .(6)
Repeating the above graph feature aggregation operation for multiple iterations, the latent features of the current node and the goal node will transfer to all other nodes in the graph. Note in Eq. (6), only the original network weights θ a are used 2 , and no additional network weights are introduced. Now we provide the recurrent graph aggregation theorem to show that the recurrent feature aggregation operation in Eq. (6) will finally lead the node feature converge to its stationary value.</p>
<p>Theorem 1 (Recurrent Graph Aggregation Theorem). Suppose that the stochastic matrix W such that [W ] i,j = α i,j , i = 1, · · · N , j = 1, · · · N , α i,j is computed according Eq. (4). The matrix X ∈ R N ×D is the node feature matrix in which each row [X] i = x i is the feature of node i. Let X [k] be the kth iteration of X. Then the limit of the recurrent graph aggregation matrix X * exists
lim k→∞ W X [k] = X *(7)
The detailed proof is given in appendixA. It follows that the recurrent graph aggregation in Eq. (6) can be formulated as a random walk on graph G where the stochastic matrix W is the state transition probability matrix. Since W is irreducible and aperiodic, then the stationary state matrix X * of Markov chain exists and the chain converges to it.</p>
<p>Denote the aggregated features for the current node and goal node after the K-th iteration as x (K) t and x (K) g , respectively. We use the node feature difference as the guided attention features
η t = x (K) t − x (K) g .(8)
A key intuition that we rely on is that sensing the goal far away is more significant than nearby. Based on this, the guided attention feature η t is bigger if the agent is far away from the goal, and vice versa. Combining Eq. (4), (6) and (8), the guided attention feature can be represented as η t = ψ(x t , x g ; θ a ). </p>
<p>DRL Control Module</p>
<p>After extracting the guided attention feature, We employ the synchronous advantage actor-critic (A2C) approach to optimize the agent network weights.</p>
<p>Model. The actor network π(s t ; θ π ) accepts the state s t and chooses the optimal action a t . The critic network V (s t ; θ v ) also accepts the current state s t and predicts the expected reward-to-go from s t to the terminal state s g . Here θ v are the critic network weights. Define the discounted cumulative reward from time t is R t = T k=t γ k−t r k where γ ∈ (0, 1) is the discount rate and T is the terminal time step. According to the policy gradient theorem [13], the actor network maximizes the expected discounted cumulative reward log π(s t ; θ π )(R t − V (s t ; θ v )) + βH(π(s t ; θ π )), where H(.) is the entropy and β adjusts its strength. Here the entropy item is added to prevent the premature. The critic network minimizes the square error between R t and V (s t ; θ v ) to predict the state value accurately. Note the graph attention memory mechanism is a feature extraction procedure for the agent, i.e. s t = ψ(o t , o g ; θ a ), so the network weights θ a are jointly optimized with θ π and θ v . Above all, the agent optimizes all the weights by minimizing the loss
L(θ π , θ v , θ a ) = 1 T h T h i=1 − log π(s t ; θ π )(R t − V (s t ; θ v )) + (R t − V (s t ; θ v )) 2 − βH(π(s t ; θ π )).(9)</p>
<p>Experiments</p>
<p>In this section, we validate the GAM-based navigation in two aspects, which include the quality of the constructed memory and the effectiveness of the guided attention features. We give the experimental setup firstly and detail the experiment results later.</p>
<p>Experiment Setup</p>
<p>We implement all experiments on a computer with an Intel Xeon E5-2620 CPU and Nvidia Titan Xp GPU. To build the graph memory, we employ the shared ResNet-18 [7] as the encoder to extract features for two observations. The features are then concatenated and fed into five fully connected (FC) layers . Each FC layer includes 512 hidden units excepting the fifth layer has 2 outputs for the binary classification. For the graph attention module, the policy and value share three convolutional layers (Conv(ch=32, k=8, s=4), Conv(ch=64, k=4, s=2), Conv(ch=32, k=3, s=1)) and one fully connected (FC) layer with 256 hidden units where ch is the number of kernels, k is the kernel size, and s is the stride. After the last FC layer, a separated FC layer outputs the policy π(s, a) and the value V (s). For the attention module, it includes three convolutional layers (Conv(ch=16, k=1, s=1), Conv(ch=1, k=1, s=1), Conv(ch=1, k=1, s=1)) and a FC layer with 16   The training protocol is as follows. For the memory construction network, we use the adam optimizer with learning rate 0.001. The mini-batch size is 64, and the network is trained for 10000 epochs. For the GAM-based navigation, we use the RMSProp optimizer with learning rate 2.5e −4 . The network input is gray-scale image with size 84 × 84. An episode only terminates when the time step arrives 2000. If the agent reaches the goal, it will be randomly re-spawned in the maze. We totally train the agent for 5e 7 steps.</p>
<p>Memory Construction Results</p>
<p>To train the memory construction network, we build two mazes with different complexities in ViZDoom as shown in Figure 3. The size of maze1 in Figure 3 (a) is 768 × 768 while the size of maze 2 in Figure 3 (c) is 1280 × 640. In order to deal with over-fitting and increase the data diversity, we change the maze wall textures while keep the maze layout invariant. Finally, 400 different mazes are built in total for each layout. In order to validate the effectiveness of the built memory, we manually control the agent to aimlessly explore the mazes. After computing the connection probabilities of all nodes, we select L = 1300 and L = 1800 connections with highest probabilities for maze1 and maze2, respectively. The corresponding built graph memories are shown in Figure 3 (b) and (d). The blue lines represent the agent exploration trajectories and the red lines are the node connection. It can be seen that most nodes are connected with their nearby neighbors. Additionally, there are no wrong connections like the too distant connection and the connections which through the wall. </p>
<p>GAM-based Navigation Results</p>
<p>Since the GAM module is intended to facilitate the learning process of the DRL agent, we compare it with several baselines to validate the performance of the proposed GAM-based navigation method.  Figure 4. It can be seen that the GAM agent can not only learns faster but converge to a higher episode reward than all the baselines. The GAM agent performs significantly better on maze 2 which is much more complex and difficult than maze 1. The reason is that the goal is much far away from the initial positions in maze 2 causing the baselines hard to reach the goal by chance and obtain no positive feedbacks. Since the episode reward is the total received reward in a fixed time interval, i.e. 2000, so the GAM agent can reach the goal more often. Thus, the GAM agent can reach the goal more efficient than the baselines. The testing scores and success rate of the GAM agent and the baselines are reported in Table 1. The total steps are 2000 for maze 1 and 6000 for maze 2. The success rate is measured by initializing the agent at all initial positions and checking whether it can reach the goal with 500 steps. Both the baselines and the GAM agent can successfully reach to the goal on maze 1. But GAM agent can achieve 100% navigation success rate while the baselines can only reach the goal from the closest initial position on maze 2. This further validates the effective of the graph attention feature.</p>
<p>We also show the trajectories of the GAM agent in an episode in Figure 5. The green and red circles represent the initial positions and the goal position, respectively. It shows that the agent can reach the goal smoothly without jittering actions. Additionally, the generalization performance of the trained model is reported. Figure 5 </p>
<p>Conclusion</p>
<p>In this paper, we propose a graph attention memory (GAM) based approach for the visual navigation task. To address the long-term memory limitation of the reactive deep reinforcement learning policy, GAM module aims to recurrently extract guided attention features based on the exploration prior. The whole GAM-based navigation system includes three modules: the topological graph memory construction, the guided attention feature extraction, and the DRL control.</p>
<p>We theoretically analyze the convergence property of the proposed recurrent guided attention extracting operation. The experiments show that the GAM agent can achieve faster learning speed and higher success rate compared to the baselines. We also visualize the built graph memory to give a qualitative validation. The results show that there are no wrong connections between distant nodes and nodes separated by the wall. The navigation trajectories show that the agent can reach the goal smoothly without jittering actions. Additionally, the trained agent can generalize to the unknown initial and goal positions.</p>
<p>Our future work includes employing the dynamic graph construction mechanism rather than the static topological graph. Additionally, we aim to transfer the GAM-based navigation system from the simulator to the real-world robot visual navigation task.</p>
<p>A Convergence Proof of Theorem1</p>
<p>Suppose that the stochastic matrix W such that [W ] i,j = α i,j , i = 1, · · · N , j = 1, · · · N , α i,j is computed according to
α i,j = exp ψ(xi,xj ,θa) k∈N i exp ψ(xi,x k ,θa) if j ∈ N i 0 otherwise(10)
where N i = {i} ∪ {j : (i, j) ∈ E} is a neighbourhood of node i in G (including node i). Please notice, that W is a stochastic matrix, corresponding to random walk on graph G.</p>
<p>Assume that we start with some feature vector x</p>
<p>[0]</p>
<p>i for each observation o t . Let us consider the following update mechanism:
x [k+1] i = j∈Ni α i,j x [k] j i = 1, . . . , N(11)
Let us denote the th component of vectorx [k] i as x [k] i ( ), then, the above updating rule can be decomposed to the collection of univariate updates: </p>
<p>where D is the dimmensionality of the feature vector x t . To study the convergence properties of equation (12) (and respectively (11) which can be considered as independent sequence of d averaging protocols, implemented on the same set of states V. Each of this averaging protocol has its own initial state y [0] 3 but all of them defined the same asymmetric transition matrix W . Moreover, due to the construction of W the corresponding underlying dynamics is irreducible and aperiodic. Hence, the spectral radius W is 1 and there is exactly one eigenvalue on unit disk. Let π be the left Perron vector of W corresponding to the eigenvalue 1, i.e π T W = π T , then we have:
lim k→∞ W k = 1π T(14)
Hence, for all = 1, . . . , D we have: 
lim k→∞ y [k] = 1π T y [0] .(15)</p>
<p>B GAM Navigation Trajectories on Maze 2</p>
<p>The GAM agent nagivation trajectories in 6000 steps on maze 2 are shown in Figure 6. The agent can efficient reach the goal without hover actions.   </p>
<p>Figure 1 :
1The visual navigation system overview. The system includes three modules: the memory construction module, the graph attention module, and the DRL control module.</p>
<p>Figure 3 :
3Two maze layouts (a), (c) and the corresponding exploration trajectories (blue lines) and node edges (red lines) in (b), (d).</p>
<p>hidden units. To increase the network robustness, we use multiple attention modules and the outputs are concatenated to form the guided attention vector η t . The guided attention vector and observation features are concatenated at the first FC layer of the policy network. The horizon T min = 5, T max = 20. The node connection threshold is determined by the top-L probability where L is the edges computed by Eq. (3).</p>
<p>Figure 4 :Figure 5 :
45The training rewards of an episode in (a) maze1 and (b) maze 2. The solid lines are GAM-based method and the dashed lines are baselines. The trajectories of the GAM agent in an episode on maze 1. The green and red circles represent the initial positions and the goal position. (a) shows the trajectories with the training initial positions and goal positions; (b) and (c) show the performance of the trained model to the novel initial and goal positions. The state is the combination of the current observation and the graph attention features, i.e. s t = [o t , η t ]. The action space includes seven discrete actions {move_forward, move_backward, move_left, move_right, turn_left, turn_right, not_move}. The -greedy strategy is used to select the action. The agent will obtain the reward of 10.0 if it reaches the goal. Otherwise, a small penalty of -0.05 is given in every step.</p>
<p>( 1 )
1FF-nogoal: the feed-forward architecture with three convolutional layers; (2) FF-goal: the feed-forward architecture but also includes the goal image in the state; (3) LSTM-nogoal: the same as the FF-nogoal baseline except the last fully connected layer is replaced by the LSTM layer with 256 hidden units. The policy and value network architectures of all baselines are the same as the GAM agent's. The training rewards of an episode are shown in</p>
<p>(b) shows the generalization to the new initial positions. All six initial positions are changed while the goal position keeps the same as the training settings. The trained gam-nogoal agent can successfully navigated to the goal. Figure 5 (c) represents the goal position changing case in which the agent can also robustly generalize to the new goal position. The navigation trajectories on the maze 2 are reported in the supplement material.</p>
<p>) i = 1, . . . , N and = 1, . . . D.</p>
<p>Figure 6 :
6The GAM agent nagivation trajectories on maze 2.</p>
<p>Figure 7
7shows the trajectories from all 10 initial positions to the goal on maze 2. It can be seen from theFigure 7that the agent can smoothly navigate to the goal.</p>
<p>Figure 7 :
7The GAM agent nagivation trajectories from ten initial positions on maze 2.</p>
<p>Table 1 :
1The testing scores and success rate of the GAM-based navigation and the baselines.FF-nogoal FF-goal LSTM-nogoal GAM </p>
<p>Maze 1 
Scores 
122.05 
122.05 
126.05 
132.05 
Success Rate 100% 
100% 
100% 
100% </p>
<p>Maze 2 
Scores 
-289.95 
-299.95 -299.95 
100.5 
Success Rate 10% 
10% 
10% 
100% </p>
<p>) let us introduce for each = 1, . . . , D, a variable y [k] = T . In other words, in vector y [k] we aggregate th components of all vectors x [k] 1 , . . . , x [k]N . It is easy to see, that equation(12)then can be written as: yx </p>
<p>[k] </p>
<p>1 ( ), x </p>
<p>[k] </p>
<p>2 ( ), . . . , x </p>
<p>[k] </p>
<p>N ( ) </p>
<p>[k+1] 
1 </p>
<p>= W y </p>
<p>[k] 
1 </p>
<p>(13) 
. . . </p>
<p>y 
[k+1] = W y </p>
<p>[k] </p>
<p>. . . </p>
<p>y </p>
<p>[k+1] 
D </p>
<p>= W y </p>
<p>[k] 
D </p>
<p>Therefore, the corresponding x D ] ∈ R N ×D , X * = [1π T y [0]1 , · · · , 1π T y D ]. By according to Eq.(14) and(15), then the theorem holds limk→∞ W X [k] = X * .(17)[k] </p>
<p>i will also converge: </p>
<p>lim </p>
<p>k→∞ </p>
<p>x </p>
<p>[k] </p>
<p>i = lim </p>
<p>k→∞ </p>
<p> </p>
<p> 
 
 
 
 </p>
<p>y </p>
<p>[k] </p>
<p>1 (i) 
y </p>
<p>[k] </p>
<p>2 (i) 
. . . </p>
<p>y </p>
<p>[k] </p>
<p>D (i) </p>
<p> </p>
<p> 
 
 
 
 </p>
<p>= </p>
<p> </p>
<p> 
 
 
 
 </p>
<p>π T y </p>
<p>[0] 
1 </p>
<p>π T y </p>
<p>[0] 
2 </p>
<p>. . . </p>
<p>π T y </p>
<p>[0] 
D </p>
<p> </p>
<p> 
 
 
 
 </p>
<p>. 
(16) 
Define X [k] = [y </p>
<p>[k] </p>
<p>1 , · · · , y </p>
<p>[k] </p>
<p>[0] </p>
<p>The attention coefficient αi,j is a function of θa as in Eq.(4) 
Each starting distribution follows immediately from x [0] i with i = 1, . . . , N .</p>
<p>Following high-level navigation instructions on a simulated quadcopter with imitation learning. Valts Blukis, Nataly Brukhim, Andrew Bennett, arXiv:1806.00047arXiv preprintValts Blukis, Nataly Brukhim, Andrew Bennett, et al. Following high-level navigation instructions on a simulated quadcopter with imitation learning. arXiv preprint arXiv:1806.00047, 2018.</p>
<p>A behavioral approach to visual navigation with graph localization networks. Kevin Chen, Juan Pablo De Vicente, Gabriel Sepulveda, Fei Xia, Alvaro Soto, M Vazquez, Silvio Savarese, abs/1903.00445CoRRKevin Chen, Juan Pablo de Vicente, Gabriel Sepulveda, Fei Xia, Alvaro Soto, M. Vazquez, and Sil- vio Savarese. A behavioral approach to visual navigation with graph localization networks. CoRR, abs/1903.00445, 2019.</p>
<p>Learning exploration policies for navigation. Tao Chen, Saurabh Gupta, International Conference on Learning Representations. Tao Chen, Saurabh Gupta, et al. Learning exploration policies for navigation. In International Confer- ence on Learning Representations, pages 1-14, 2019. URL https://openreview.net/forum?id= SyMWn05F7.</p>
<p>Hybrid computing using a neural network with dynamic external memory. Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Nature. 5387626Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):471-476, 2016.</p>
<p>Cognitive mapping and planning for visual navigation. Saurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, Jitendra Malik, arXiv:1702.03920arXiv preprintSaurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, and Jitendra Malik. Cognitive mapping and planning for visual navigation. arXiv preprint arXiv:1702.03920, 2017.</p>
<p>Unifying map and landmark based representations for visual navigation. Saurabh Gupta, David Fouhey, Sergey Levine, Jitendra Malik, arXiv:1712.08125arXiv preprintSaurabh Gupta, David Fouhey, Sergey Levine, and Jitendra Malik. Unifying map and landmark based representations for visual navigation. arXiv preprint arXiv:1712.08125, 2017.</p>
<p>Deep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, IEEE Conference on Computer Vision and Pattern Recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pages 770-778, 2016.</p>
<p>ViZDoom: A Doom-based AI research platform for visual reinforcement learning. Michał Kempka, Marek Wydmuch, Grzegorz Runc, IEEE Conference on Computational Intelligence and Games. Michał Kempka, Marek Wydmuch, Grzegorz Runc, et al. ViZDoom: A Doom-based AI research platform for visual reinforcement learning. In IEEE Conference on Computational Intelligence and Games, pages 341-348, 2016. URL http://arxiv.org/abs/1605.02097.</p>
<p>Reinforcement learning and deep learning based lateral control for autonomous driving. Dong Li, Dongbin Zhao, Qichao Zhang, Yaran Chen, arXiv:1810.12778arXiv preprintDong Li, Dongbin Zhao, Qichao Zhang, and Yaran Chen. Reinforcement learning and deep learning based lateral control for autonomous driving. arXiv preprint arXiv:1810.12778, 2018.</p>
<p>Learning to navigate in complex environments. Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J Ballard, International Conference on Learning Representations. Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J Ballard, et al. Learning to navigate in complex environments. In International Conference on Learning Representations, pages 1-16, 2016.</p>
<p>Learning to navigate in cities without a map. Piotr Mirowski, Matt Grimes, Mateusz Malinowski, Karl Moritz Hermann, Advances in Neural Information Processing Systems. Piotr Mirowski, Matt Grimes, Mateusz Malinowski, Karl Moritz Hermann, et al. Learning to navigate in cities without a map. In Advances in Neural Information Processing Systems, pages 2424-2435, 2018.</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Nature. 5187540Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, International Conference on Machine Learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, et al. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928-1937, 2016.</p>
<p>Neural map: Structured memory for deep reinforcement learning. Emilio Parisotto, Ruslan Salakhutdinov, arXiv:1702.08360arXiv preprintEmilio Parisotto and Ruslan Salakhutdinov. Neural map: Structured memory for deep reinforcement learning. arXiv preprint arXiv:1702.08360, 2017.</p>
<p>Curiosity-driven exploration by self-supervised prediction. Deepak Pathak, Pulkit Agrawal, Alexei A Efros, Trevor Darrell, IEEE Conference on Computer Vision and Pattern Recognition Workshops. Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 16-17, 2017.</p>
<p>Zero-shot visual imitation. Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, IEEE Conference on Computer Vision and Pattern Recognition Workshops. Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, et al. Zero-shot visual imitation. In IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 2050-2053, 2018.</p>
<p>Nikolay Savinov, Alexey Dosovitskiy, Vladlen Koltun, arXiv:1803.00653Semi-parametric topological memory for navigation. arXiv preprintNikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun. Semi-parametric topological memory for navigation. arXiv preprint arXiv:1803.00653, 2018.</p>
<p>MINOS: Multimodal indoor simulator for navigation in complex environments. Manolis Savva, Angel X Chang, Alexey Dosovitskiy, Thomas Funkhouser, Vladlen Koltun, arXiv:1712.03931Manolis Savva, Angel X. Chang, Alexey Dosovitskiy, Thomas Funkhouser, and Vladlen Koltun. MINOS: Multimodal indoor simulator for navigation in complex environments. arXiv:1712.03931, 2017.</p>
<p>Visual navigation with actor-critic deep reinforcement learning. Kun Shao, Dongbin Zhao, Yuanheng Zhu, IEEE Conference on International Joint Conference on Neural Networks. Kun Shao, Dongbin Zhao, Yuanheng Zhu, et al. Visual navigation with actor-critic deep reinforcement learning. IEEE Conference on International Joint Conference on Neural Networks, pages 1-6, 2018.</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, Nature. 5297587484David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484, 2016.</p>
<p>Probabilistic robotics. Sebastian Thrun, Wolfram Burgard, Dieter Fox, MIT pressSebastian Thrun, Wolfram Burgard, and Dieter Fox. Probabilistic robotics. MIT press, 2005.</p>
<p>Graph attention networks. Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, International Conference on Learning Representations. Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, pages 1-12, 2017.</p>
<p>Non-local neural networks. Xiaolong Wang, Ross Girshick, Abhinav Gupta, Kaiming He, IEEE Conference on Computer Vision and Pattern Recognition. Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In IEEE Conference on Computer Vision and Pattern Recognition, pages 7794-7803, 2018.</p>
<p>. Jingwei Zhang, Lei Tai, Joschka Boedecker, Wolfram Burgard, Ming Liu, arXiv:1706.09520Neural SLAM. arXiv preprintJingwei Zhang, Lei Tai, Joschka Boedecker, Wolfram Burgard, and Ming Liu. Neural SLAM. arXiv preprint arXiv:1706.09520, 2017.</p>
<p>Deep reinforcement learning with visual attention for vehicle classification. Dongbin Zhao, Yaran Chen, Le Lv, IEEE Transactions on Cognitive and Developmental Systems. 94Dongbin Zhao, Yaran Chen, and Le Lv. Deep reinforcement learning with visual attention for vehicle classification. IEEE Transactions on Cognitive and Developmental Systems, 9(4):356-367, 2017.</p>
<p>Target-driven visual navigation in indoor scenes using deep reinforcement learning. Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, J Joseph, Abhinav Lim, Li Gupta, Ali Fei-Fei, Farhadi, IEEE International Conference on Robotics and Automation. Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In IEEE International Conference on Robotics and Automation, pages 3357-3364, 2017.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2205 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2205</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2205</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-58.html">extraction-schema-58</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <p><strong>Paper ID:</strong> paper-c72dd121863d9caaae0c9363278439bb42f0a8dc</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c72dd121863d9caaae0c9363278439bb42f0a8dc" target="_blank">Matter-of-Fact: A Benchmark for Verifying the Feasibility of Literature-Supported Claims in Materials Science</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work introduces Matter-of-Fact, a challenge dataset for determining the feasibility of hypotheses framed as claims, while operationalizing feasibility assessment as a temporally-filtered claim verification task using backtesting.</p>
                <p><strong>Paper Abstract:</strong> Contemporary approaches to assisted scientific discovery use language models to automatically generate large numbers of potential hypothesis to test, while also automatically generating code-based experiments to test those hypotheses. While hypotheses can be comparatively inexpensive to generate, automated experiments can be costly, particularly when run at scale (i.e. thousands of experiments). Developing the capacity to filter hypotheses based on their feasibility would allow discovery systems to run at scale, while increasing their likelihood of making significant discoveries. In this work we introduce Matter-of-Fact, a challenge dataset for determining the feasibility of hypotheses framed as claims, while operationalizing feasibility assessment as a temporally-filtered claim verification task using backtesting. Matter-of-Fact includes 8.4k claims extracted from scientific articles spanning four high-impact contemporary materials science topics, including superconductors, semiconductors, batteries, and aerospace materials, while including qualitative and quantitative claims from theoretical, experimental, and code/simulation results. We show that strong baselines that include retrieval augmented generation over scientific literature and code generation fail to exceed 72% performance on this task (chance performance is 50%), while domain-expert verification suggests nearly all are solvable -- highlighting both the difficulty of this task for current models, and the potential to accelerate scientific discovery by making near-term progress.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2205.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2205.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MATTER-OF-FACT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Matter-of-Fact benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A temporally-filtered benchmark of 8.4k materials-science claims (true/feasible and false/infeasible) designed to evaluate feasibility assessment methods for automated scientific discovery using literature, code/simulation, and other approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>MATTER-OF-FACT (temporally-filtered claim verification benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The benchmark frames feasibility assessment as temporally-filtered claim verification: each claim is paired with a knowledge cutoff date and gold label derived from later-published source papers. Validation approaches evaluated on the benchmark include (1) literature-based retrieval (RAG over SemanticScholar) using only documents authored before the cutoff, (2) code-based experiments / simulations generated and executed by a code-generation agent (CODESCIENTIST) within sandboxed Python, (3) human domain-expert review using the source paper (oracle), and (4) oracle verification using the original source article. Negative (infeasible) claims are automatically generated from positive claims to provide balanced labels. Models are evaluated by binary accuracy and explanation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Not specific to any single physics/chemistry simulator; the benchmark includes claims that originate from experimental, theoretical, and code/simulation results in source papers. When simulation is used by baselines, it is ad-hoc Python-based experiments (see CODESCIENTIST entry) rather than domain-specific high-fidelity physics solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>The paper reports that claims based on experiments or code/simulations are the hardest for models to assess (lowest model accuracy), implying that simulation/literature-only methods do not fully match the reliability of experimental/source-paper verification; no direct numerical comparison of simulation results vs physical experiments is provided in the benchmark itself.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Not applicable to the benchmark itself; evaluation metrics reported are model accuracies on feasibility assessment (range ~0.58–0.72). For oracle/human verification modes, domain expert agreement with gold labels reached 93% initially and 99% after resolution; oracle-source-paper model variants achieved near 100% for some base models.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>The paper asserts that empirical (experimental) validation remains the gold standard in materials science and that many claims ultimately require physical experiments to be conclusively verified. The benchmark uses source-paper results (published experiments/simulations/theory) as ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>The authors note simulations and code-based pilot experiments can sometimes provide evidence sufficient for feasibility filtering (inexpensive pilot tests), but explicitly caution that many impactful claims require physical experiments and cannot be fully validated by literature or simulation alone.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>No individual examples of simulation failing vs experiment are provided, but aggregate results show models perform worst on claims derived from experiments or code/simulations, indicating limitations of simulation- or literature-only validation for some claim types.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Uncertainty is characterized via standard evaluation metrics (accuracy by category, true/false breakdown) and interrater agreement (Cohen's κ = 0.86 before resolution); no per-claim confidence intervals or error bars on experimental/simulation outputs are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>The benchmark construction explicitly generates negative (infeasible) claims algorithmically from positive claims and then tests whether models and humans can detect infeasible claims. Detection methods evaluated include literature retrieval (RAG), code-based reproduction (CODESCIENTIST), and human expert review. No specialized automated fabricated-result detection algorithm is introduced beyond these verification pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>The paper reports cost estimates for model-based validation: model costs per 1k claims are provided (Table 2) and range by method and base model (e.g., RAG using O4-MINI estimated at ~$27/1k; CODESCIENTIST is expensive with per-experiment cost cited ~ $4 initially and higher pipeline costs resulting in larger per-1k estimates). CODESCIENTIST runs were truncated for tractability: single iteration, 10-minute experiment time limit (versus 6 hours in full setting), yielding ~31 CPU-days for all test claims. The authors emphasize that computational (literature/simulation) validation is far cheaper than running physical experiments at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Temporal filtering requires control over base-model contamination; code-based experiments were run in a heavily resource-limited configuration (single iteration, short timeouts), which reduces fidelity; literature retrieval can miss relevant evidence; many claims are fundamentally only verifiable via new empirical experiments; no per-claim uncertainty estimates are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>The paper argues that validation approaches that include the source paper or domain expert review yield high credibility (domain expert agreement ~99% with labels when given the source); by contrast, temporally-constrained automatic methods have modest accuracy (≤72%), suggesting lower immediate credibility for purely automated feasibility assessments until performance improves.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Oracle-source-paper and human domain expert evaluations serve as gold-standards: oracle models using the source paper reached near 100% for some base models (e.g., Claude Sonnet 3.7 ORACLE ~1.00 on subset), and human expert reached 0.99 after adjudication. These numbers set a practical ceiling compared to temporally-restricted automated approaches (~0.58–0.72).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2205.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2205.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Feasibility Assessment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temporally-filtered feasibility assessment (claim verification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A task framing where models predict whether a claim (hypothesis) extracted from a later-published paper would have been feasible using only knowledge available before the paper's publication, combining literature search, code/simulations, and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Temporally-filtered claim verification (feasibility assessment)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific discovery; applied to materials science in this work</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Models are restricted to evidence available prior to the claim's source-paper date. Validation approaches allowed and evaluated include retrieval of pre-cutoff literature (RAG), small-scale code-based experiments/simulations, and world-modeling reasoning. Gold labels are drawn from the later source paper (or artificially generated infeasible variants). Performance is measured by binary accuracy against these gold labels.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Varies by implementation; the paper's baseline code-experiment runs are ad-hoc Python implementations executed in a sandbox with tight runtime limits (10 minutes) and single-iteration debugging, implying low to medium fidelity relative to domain-specific high-accuracy simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>The framing explicitly recognizes that simulation/literature approaches may suffice for filtering but are not universally equivalent to experimental verification. The paper reports that claims grounded in experimental or code/simulation results are harder for models to assess than theoretical claims, but does not report direct simulation-vs-experiment numeric agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Not a single rate — models' accuracy on feasibility assessment ranges ~58%–72%. The authors give an illustrative utility example: a model with 60% true-detection and 82% infeasible-detection could reduce experimental costs by ~80% while recovering 60% of true hypotheses for a hypothetical ASD system with 1% true hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Paper emphasizes that in materials science experimental evidence is typically required for full acceptance; temporally-filtered feasibility assessment is intended as a pre-screening step, not a substitute for experimental validation in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Discussed as sometimes sufficient for inexpensive pre-filtering, especially for qualitative claims or where simulations are predictive; but authors caution that surprising or high-impact claims often require new physical experiments and cannot rely solely on simulation/literature.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>No particular claim-level simulation failures are given; however, lower assessment accuracy on code/experiment-derived claims indicates simulation/literature-based validation often falls short of experimental conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Assessment uncertainty is represented via aggregate accuracy metrics across categories and by employing interrater agreement for human validation (Cohen's κ). No per-claim probabilistic uncertainty calibration is described.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Feasibility assessment is used to detect infeasible (including automatically fabricated) claims by leveraging constrained temporal retrieval and code-based checks; models and humans are evaluated on their ability to flag artificially generated infeasible claims, but no specialized fabricated-content detection algorithm beyond retrieval/verification is described.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Paper explicitly compares cost/time tradeoffs: literature retrieval is relatively cheap; code-based experiments are more expensive (CODESCIENTIST originally estimated ~$4 per experiment), and full physical experiments are far more costly. Baseline model cost-per-1k-claims are reported in Table 2 (varying by base model and approach).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Temporal constraints require care to avoid model contamination; code-based validation was resource-limited in experiments; many claims fundamentally require experiments; retrieval can miss evidence; evaluation uses automatically-generated negative claims which might differ from naturally occurring infeasible hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Feasibility assessments are positioned as utility tools to reduce cost and prioritize experiments; however, because automated methods currently achieve modest accuracy, the paper suggests that acceptance by the scientific community will hinge on improved reliability and complementary experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Compared to oracle-source-paper and human expert evaluation (gold standards), temporally-constrained automated methods perform substantially worse (oracle/human ~0.93–0.99 vs automated 0.58–0.72), indicating a significant gap.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2205.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2205.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG (SemanticScholar)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation using the SemanticScholar API</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented pipeline that fetches top-K full-text snippets from Semantic Scholar (filtered by publication date) and supplies them as context to LLMs for claim feasibility verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledge-intensive nlp tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>RAG (SEMANTICSCHOLAR) retrieval-augmented generation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific claim verification / materials science literature retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Given a claim and a knowledge cutoff, the system retrieves the top 20 relevance-ranked full-text snippets (~500 words each) from Semantic Scholar that predate the claim's source paper and includes them in an LLM prompt (with 20-shot ICL and CoT) to decide feasibility. This prevents temporal contamination while enabling evidence-based verification.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>No direct simulation-vs-experiment comparison; RAG serves as a literature-based computational validation approach and is compared in performance to code-based (CODESCIENTIST) and oracle/human modes. RAG methods improved performance modestly over CoT-only baselines in the temporally restricted task but remained below oracle/human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>RAG (temporally-filtered) baseline accuracies reported: e.g., O4-MINI CoT+ICL+RAG ~0.71 overall; Claude Sonnet RAG (temporally-unrestricted) up to 0.87; when RAG is allowed to access documents after the cutoff (temporally-unrestricted), performance increases substantially (e.g., GPT-40-MINI RAG no-date 0.76).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Uses literature evidence as acceptable computational validation for claim verification when source papers or prior evidence exist; still recognized as inferior to direct experimental verification when experiments are required.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>RAG is useful when relevant prior literature exists that directly supports/refutes a claim; when such literature is absent (novel claims), RAG alone is insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Not applicable (literature retrieval approach). However, retrieval misses or irrelevant snippets can reduce verification accuracy; no specific failure cases enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Uncertainty is reflected indirectly via model accuracy and per-category performance; no calibrated confidence scores or retrieval-uncertainty metrics are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>RAG contributes to detecting fabricated/infeasible claims by finding contradicting or absent evidence in pre-cutoff literature; no specialized fabrication detectors are applied.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Reported model cost-per-1k-claims for RAG vary by base model (e.g., O4-MINI RAG cost ~$27/1k; GPT-40-MINI RAG ~$3/1k) as shown in Table 2. Temporal filtering increases retrieval complexity but not directly the per-claim compute beyond increased prompt context.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Effectiveness depends on coverage and quality of retrieved literature and on temporal filtering; cannot verify claims lacking prior literature evidence and may be confounded by subtle or technical claim details absent from abstracts or snippet-sized spans.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>RAG improves evidence-grounded justifications which increases credibility relative to CoT-only LLM outputs, but temporally-restricted RAG still underperforms oracle/human verification, limiting immediate community acceptance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>When RAG is temporally unrestricted (allowed to retrieve post-cutoff and source paper), models approach oracle performance (e.g., RAG no-date for 04-MINI up to 0.90 in Table 3), but temporally-filtered RAG remains well below oracle/human ceilings.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2205.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2205.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CODESCIENTIST</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CODESCIENTIST (code-generation experiment execution pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A code-generation pipeline that prompts an LLM to author Python experiments or simulations, executes the generated code in a sandbox, and uses the resulting outputs as evidence to assess claim feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Codescientist: End-to-end semiautomated scientific discovery with code-based experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>CODESCIENTIST (Python code-based experiment generation + sandbox execution)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Automated scientific discovery; materials science when applied in this work</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>low-fidelity simulation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Two-stage pipeline: (1) LLM is prompted to generate Python code for experiments/simulations that would test the claim; (2) generated code is executed in a sandbox (MODAL.COM) with PIP access, output logs captured, and then fed back to the model for a final decision. For tractability in the paper's experiments, runs were restricted to a single iteration (no reflection), 10-minute execution limits (versus a full 6-hour budget), and reduced debugging iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Low-to-moderate: fidelity depends on the nature of the generated code. In the paper's baseline runs, experiments were intentionally scoped small due to runtime/cost limits (10-minute timeouts, single iteration). No domain-specific high-fidelity physics/DFT solvers were systematically used; accuracy relative to physical experiments is therefore limited and not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>No direct quantitative comparison between CODESCIENTIST simulation outputs and published experimental results is provided. The paper reports that claims grounded in code/simulation are among the lowest-scoring categories for model feasibility assessment, suggesting simulation-only validation often fails to match experimental conclusions in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>CODESCIENTIST baseline accuracies reported in Table 2: examples include GPT-40-MINI CoT+ICL+CODE ~0.64 overall; O4-MINI CoT+ICL+CODE ~0.68; Claude-Sonnet CoT+ICL+CODE ~0.63. No per-claim experimental validation success rates vs real-world experiments are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>The method is proposed as a rapid, inexpensive pilot-testing approach for feasibility filtering; authors note that domain acceptance requires demonstrating that code-based experiments reliably predict real experimental outcomes, which has not been established here.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Paper suggests code/simulations can be sufficient for inexpensive feasibility filtering when experiments are expensive and when simulations capture relevant physics; however, because the baseline code executions were low-resource and truncated, the authors caution that many claims still require real experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>No explicit per-claim failure cases given; general observation: claims originally based on experiments or code/simulation had lower automated-assessment accuracy, implying limitations in code-based validation under the constrained execution regime used.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>No explicit error bars or uncertainty quantification on code-execution results are provided. Uncertainty in conclusions is evaluated indirectly via model-level accuracy metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>CODESCIENTIST can be used to attempt to reproduce or falsify claim results programmatically; this may help detect fabricated or infeasible claims when code outputs contradict claimed findings. The paper does not evaluate systematic detection of fabricated AI-generated experiments beyond this capability.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Full CODESCIENTIST experiments are noted to be expensive (initially cited ~$4 per experiment). For scale, the authors truncated runs: single iterations and 10-minute runtime limits to keep total costs tractable (approximate reported total if run fully would be in the thousands of USD for large subsets). Table 2 provides per-1k-claim cost estimates for the CODE pipeline (varies by base model; e.g., GPT-40-MINI CODE cost ~$4/1k in one line but other models show higher costs).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>High cost and runtime for high-fidelity code experiments; sandbox limitations and single-iteration runs reduce effectiveness; lacking use of domain-specific high-fidelity simulators in systematic fashion; no per-claim ground-truth comparison between code outputs and later experimental results provided.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Paper frames CODESCIENTIST as promising for pilot testing and increasing efficiency, but notes current resource and fidelity limits reduce immediate credibility as a replacement for experiments; thus hybrid approaches (literature + cheap code tests) are suggested as practical.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Compared to oracle-source-paper and human expert evaluation (gold), CODESCIENTIST-based feasibility assessment achieves lower accuracy (~0.63–0.68) and thus is not a substitute for source-paper/expert verification as implemented in this study.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2205.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2205.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Oracle Source Paper</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Oracle-source-paper verification baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A verification baseline that supplies the model with the original source paper (full LaTeX) that produced the claim to measure the practical verification ceiling when oracle evidence is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Oracle-source-paper (retrieval-augmented verification with access to the original paper)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific claim verification / materials science</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The model receives the claim plus the full original source paper (LAEX source) in a RAG-style prompt to decide truth/feasibility; this measures how well a model can verify a claim when given direct, authoritative evidence from the source.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Oracle access is a computational verification against the published experimental/simulation/theoretical evidence reported in the source paper. The paper treats oracle/source-paper and human expert review as the best available proxies for ground truth, showing oracle models achieve a performance ceiling well above temporally restricted methods.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Reported oracle-source-paper accuracies: O4-MINI and Claude Sonnet variants reach nearly 100% (e.g., CLAUDE-SONNET ORACLE ~1.00 on a subset; O4-MINI ORACLE ~0.96 on subset); GPT-40-MINI oracle range ~0.71–0.76. These values suggest the verification ceiling when the source article is available.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Using the original paper as evidence is treated as the de facto gold standard for claim verification when the paper directly addresses the claim.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>If the source paper reports computational/simulation evidence and the model can correctly interpret it, oracle verification by reading the source can be sufficient to classify feasibility for claims that are supported/refuted in that paper.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Not applicable; oracle mode is a read/interpret verification against published results. Any mismatch would be due to model inability to interpret the source, which was low for stronger base models.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Oracle performance is reported as aggregate accuracies; no additional statistical uncertainty measures provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Oracle mode implicitly detects fabricated claims if the source paper contradicts the claim; the paper uses this as a check on automatically generated negative claims and on data quality.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Oracle evaluation is computational (LLM inference) and relatively cheap compared to running new physical experiments; cost depends on model inference and prompt length (not quantified beyond model cost-per-1k table entries).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Oracle mode assumes access to the original source paper; it does not address claims that require new experiments beyond what the source contains. Also, model comprehension of the source limits success.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>High — oracle-source-paper performance aligns closely with human expert judgments and is used to validate the benchmark labels and claim generation process.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Oracle-source-paper is used as a near-gold standard and achieves performance close to human oracle performance (human expert after adjudication ~99%). The paper directly contrasts temporally-restricted methods with oracle performance to show the achievable ceiling.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2205.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2205.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Expert Review</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain expert validation (human adjudication)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Manual verification by a materials-science domain expert who reads each claim and its source paper to determine validity; used to assess the quality of automatically generated claims and labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Human domain-expert review and adjudication</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>A graduate-level domain expert evaluated 100 test-set claims by reading the claim and source paper and labeling validity. Disagreements with LLM labels were reviewed and resolved; initial agreement was 93% and increased to 99% after adjudication. Interrater agreement (pre-adjudication) Cohen's κ = 0.86.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Human expert review serves as a quality-control comparison to automated computational approaches rather than an experimental/simulation method; human adjudication agreed strongly with oracle/source-paper labels.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>For the subset reviewed: initial agreement 93% with LLM-generated labels; after reviewing LLM evidence and resolving disagreements, agreement rose to 99%.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Human expert judgment with access to source papers is treated as a high-standard validator; adjudication was used to resolve ambiguities and data-generation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Human experts can sometimes determine infeasibility via literature/simulation evidence, but the paper emphasizes that humans too sometimes miss difficult-to-find evidence on first pass and benefit from having model-provided evidence and explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Not applicable; human errors (missed evidence) are noted as sources of initial disagreement, but no systematic failure examples are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Interrater agreement metric (Cohen's κ = 0.86) is reported to quantify annotation reliability; no per-expert confidence scores provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Human experts were able to detect many automatically-generated infeasible claims; after consulting model explanations and evidence, they resolved almost all disagreements, demonstrating humans can detect fabricated/infeasible claims when given appropriate evidence context.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Human review is time-consuming and costly relative to automatic methods; the paper does not provide per-claim time/cost estimates for the expert review but implies scalability limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Single expert cannot cover all subdomains; initial human misses of difficult evidence occurred; expert review is not scalable to tens of thousands of claims without substantial cost.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>High — human expert adjudication is used to validate dataset labels and is presented as increasing trust in the benchmark's quality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Human expert evaluation with access to source papers performs at or near the oracle ceiling (99% after resolution) and is used to confirm the correctness of generated labels and to calibrate automated systems' performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2205.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2205.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Temporal Filtering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temporal filtering / knowledge cutoff analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method for ensuring models assess feasibility using only knowledge available before a claim's source paper publication date to avoid model contamination by training data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Temporal filtering (knowledge cutoff enforcement)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning for scientific claim verification</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>For each claim, retrieval and evidence access are restricted to documents authored on or before the claim's source-paper date. The approach also includes analysis of base-model performance on claims from before vs after each model's advertised knowledge cutoff to estimate model contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Temporal filtering is orthogonal to simulation/experiment methods; it constrains what computational evidence is available for validation. The paper's contamination analysis (Table 4) shows nearly identical model performance on claims before vs after advertised model cutoffs, suggesting limited contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Contamination analysis shows minimal delta in accuracy before vs after model cutoffs (±1%), e.g., GPT-40-MINI delta 0.003, O4-MINI delta -0.011, Claude-Sonnet delta 0.011 across all 8.4k claims.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Temporal filtering is presented as necessary for creating prediction-style benchmarks where future claims must be judged only using past evidence; ensuring models do not rely on future data is a domain requirement for this problem formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Temporal filtering does not directly determine simulation sufficiency; it only constrains available pre-cutoff computational evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Contamination assessment uses aggregate accuracy deltas; no probabilistic contamination measures are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Temporal filtering helps detect fabricated claims insofar as it prevents models from trivially using future citations or the source paper to validate claims; fabricated/infeasible claims are still detected by retrieval or code-based checks within the cutoff constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Temporal filtering increases the complexity of retrieval (must filter by date) but has negligible direct computational cost relative to LLM inference; no specific runtime figures provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Relies on accurate model cutoff metadata and assumes base models are not already significantly contaminated; model editing/unlearning may help but are not solved; temporal filtering does not eliminate all forms of indirect contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Temporal filtering increases the scientific rigor of the benchmark by enabling a prediction-style evaluation; acceptance depends on demonstrable low contamination levels, which the authors provide some evidence for.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Temporal filtering is compared against temporally-unrestricted (oracle) retrieval to show the performance gap induced by realistic evidence constraints.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Codescientist: End-to-end semiautomated scientific discovery with code-based experimentation <em>(Rating: 2)</em></li>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>Accelerating computational materials discovery with machine learning and cloud high-performance computing: from large-scale screening to experimental validation <em>(Rating: 2)</em></li>
                <li>Fact or fiction: Verifying scientific claims <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Highly accurate protein structure prediction with alphafold <em>(Rating: 1)</em></li>
                <li>Scaling deep learning for materials discovery <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2205",
    "paper_id": "paper-c72dd121863d9caaae0c9363278439bb42f0a8dc",
    "extraction_schema_id": "extraction-schema-58",
    "extracted_data": [
        {
            "name_short": "MATTER-OF-FACT",
            "name_full": "Matter-of-Fact benchmark",
            "brief_description": "A temporally-filtered benchmark of 8.4k materials-science claims (true/feasible and false/infeasible) designed to evaluate feasibility assessment methods for automated scientific discovery using literature, code/simulation, and other approaches.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "MATTER-OF-FACT (temporally-filtered claim verification benchmark)",
            "scientific_domain": "Materials science",
            "validation_type": "hybrid",
            "validation_description": "The benchmark frames feasibility assessment as temporally-filtered claim verification: each claim is paired with a knowledge cutoff date and gold label derived from later-published source papers. Validation approaches evaluated on the benchmark include (1) literature-based retrieval (RAG over SemanticScholar) using only documents authored before the cutoff, (2) code-based experiments / simulations generated and executed by a code-generation agent (CODESCIENTIST) within sandboxed Python, (3) human domain-expert review using the source paper (oracle), and (4) oracle verification using the original source article. Negative (infeasible) claims are automatically generated from positive claims to provide balanced labels. Models are evaluated by binary accuracy and explanation quality.",
            "simulation_fidelity": "Not specific to any single physics/chemistry simulator; the benchmark includes claims that originate from experimental, theoretical, and code/simulation results in source papers. When simulation is used by baselines, it is ad-hoc Python-based experiments (see CODESCIENTIST entry) rather than domain-specific high-fidelity physics solvers.",
            "experimental_validation_performed": null,
            "comparison_simulation_vs_experiment": "The paper reports that claims based on experiments or code/simulations are the hardest for models to assess (lowest model accuracy), implying that simulation/literature-only methods do not fully match the reliability of experimental/source-paper verification; no direct numerical comparison of simulation results vs physical experiments is provided in the benchmark itself.",
            "validation_success_rate": "Not applicable to the benchmark itself; evaluation metrics reported are model accuracies on feasibility assessment (range ~0.58–0.72). For oracle/human verification modes, domain expert agreement with gold labels reached 93% initially and 99% after resolution; oracle-source-paper model variants achieved near 100% for some base models.",
            "domain_validation_standards": "The paper asserts that empirical (experimental) validation remains the gold standard in materials science and that many claims ultimately require physical experiments to be conclusively verified. The benchmark uses source-paper results (published experiments/simulations/theory) as ground truth.",
            "when_simulation_sufficient": "The authors note simulations and code-based pilot experiments can sometimes provide evidence sufficient for feasibility filtering (inexpensive pilot tests), but explicitly caution that many impactful claims require physical experiments and cannot be fully validated by literature or simulation alone.",
            "simulation_failures": "No individual examples of simulation failing vs experiment are provided, but aggregate results show models perform worst on claims derived from experiments or code/simulations, indicating limitations of simulation- or literature-only validation for some claim types.",
            "uncertainty_quantification": "Uncertainty is characterized via standard evaluation metrics (accuracy by category, true/false breakdown) and interrater agreement (Cohen's κ = 0.86 before resolution); no per-claim confidence intervals or error bars on experimental/simulation outputs are provided.",
            "fabrication_detection": "The benchmark construction explicitly generates negative (infeasible) claims algorithmically from positive claims and then tests whether models and humans can detect infeasible claims. Detection methods evaluated include literature retrieval (RAG), code-based reproduction (CODESCIENTIST), and human expert review. No specialized automated fabricated-result detection algorithm is introduced beyond these verification pipelines.",
            "validation_cost_time": "The paper reports cost estimates for model-based validation: model costs per 1k claims are provided (Table 2) and range by method and base model (e.g., RAG using O4-MINI estimated at ~$27/1k; CODESCIENTIST is expensive with per-experiment cost cited ~ $4 initially and higher pipeline costs resulting in larger per-1k estimates). CODESCIENTIST runs were truncated for tractability: single iteration, 10-minute experiment time limit (versus 6 hours in full setting), yielding ~31 CPU-days for all test claims. The authors emphasize that computational (literature/simulation) validation is far cheaper than running physical experiments at scale.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Temporal filtering requires control over base-model contamination; code-based experiments were run in a heavily resource-limited configuration (single iteration, short timeouts), which reduces fidelity; literature retrieval can miss relevant evidence; many claims are fundamentally only verifiable via new empirical experiments; no per-claim uncertainty estimates are provided.",
            "acceptance_credibility": "The paper argues that validation approaches that include the source paper or domain expert review yield high credibility (domain expert agreement ~99% with labels when given the source); by contrast, temporally-constrained automatic methods have modest accuracy (≤72%), suggesting lower immediate credibility for purely automated feasibility assessments until performance improves.",
            "comparison_to_gold_standard": "Oracle-source-paper and human domain expert evaluations serve as gold-standards: oracle models using the source paper reached near 100% for some base models (e.g., Claude Sonnet 3.7 ORACLE ~1.00 on subset), and human expert reached 0.99 after adjudication. These numbers set a practical ceiling compared to temporally-restricted automated approaches (~0.58–0.72).",
            "uuid": "e2205.0"
        },
        {
            "name_short": "Feasibility Assessment",
            "name_full": "Temporally-filtered feasibility assessment (claim verification)",
            "brief_description": "A task framing where models predict whether a claim (hypothesis) extracted from a later-published paper would have been feasible using only knowledge available before the paper's publication, combining literature search, code/simulations, and reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Temporally-filtered claim verification (feasibility assessment)",
            "scientific_domain": "General scientific discovery; applied to materials science in this work",
            "validation_type": "hybrid",
            "validation_description": "Models are restricted to evidence available prior to the claim's source-paper date. Validation approaches allowed and evaluated include retrieval of pre-cutoff literature (RAG), small-scale code-based experiments/simulations, and world-modeling reasoning. Gold labels are drawn from the later source paper (or artificially generated infeasible variants). Performance is measured by binary accuracy against these gold labels.",
            "simulation_fidelity": "Varies by implementation; the paper's baseline code-experiment runs are ad-hoc Python implementations executed in a sandbox with tight runtime limits (10 minutes) and single-iteration debugging, implying low to medium fidelity relative to domain-specific high-accuracy simulators.",
            "experimental_validation_performed": null,
            "comparison_simulation_vs_experiment": "The framing explicitly recognizes that simulation/literature approaches may suffice for filtering but are not universally equivalent to experimental verification. The paper reports that claims grounded in experimental or code/simulation results are harder for models to assess than theoretical claims, but does not report direct simulation-vs-experiment numeric agreement.",
            "validation_success_rate": "Not a single rate — models' accuracy on feasibility assessment ranges ~58%–72%. The authors give an illustrative utility example: a model with 60% true-detection and 82% infeasible-detection could reduce experimental costs by ~80% while recovering 60% of true hypotheses for a hypothetical ASD system with 1% true hypotheses.",
            "domain_validation_standards": "Paper emphasizes that in materials science experimental evidence is typically required for full acceptance; temporally-filtered feasibility assessment is intended as a pre-screening step, not a substitute for experimental validation in many cases.",
            "when_simulation_sufficient": "Discussed as sometimes sufficient for inexpensive pre-filtering, especially for qualitative claims or where simulations are predictive; but authors caution that surprising or high-impact claims often require new physical experiments and cannot rely solely on simulation/literature.",
            "simulation_failures": "No particular claim-level simulation failures are given; however, lower assessment accuracy on code/experiment-derived claims indicates simulation/literature-based validation often falls short of experimental conclusions.",
            "uncertainty_quantification": "Assessment uncertainty is represented via aggregate accuracy metrics across categories and by employing interrater agreement for human validation (Cohen's κ). No per-claim probabilistic uncertainty calibration is described.",
            "fabrication_detection": "Feasibility assessment is used to detect infeasible (including automatically fabricated) claims by leveraging constrained temporal retrieval and code-based checks; models and humans are evaluated on their ability to flag artificially generated infeasible claims, but no specialized fabricated-content detection algorithm beyond retrieval/verification is described.",
            "validation_cost_time": "Paper explicitly compares cost/time tradeoffs: literature retrieval is relatively cheap; code-based experiments are more expensive (CODESCIENTIST originally estimated ~$4 per experiment), and full physical experiments are far more costly. Baseline model cost-per-1k-claims are reported in Table 2 (varying by base model and approach).",
            "hybrid_validation_approach": true,
            "validation_limitations": "Temporal constraints require care to avoid model contamination; code-based validation was resource-limited in experiments; many claims fundamentally require experiments; retrieval can miss evidence; evaluation uses automatically-generated negative claims which might differ from naturally occurring infeasible hypotheses.",
            "acceptance_credibility": "Feasibility assessments are positioned as utility tools to reduce cost and prioritize experiments; however, because automated methods currently achieve modest accuracy, the paper suggests that acceptance by the scientific community will hinge on improved reliability and complementary experimental validation.",
            "comparison_to_gold_standard": "Compared to oracle-source-paper and human expert evaluation (gold standards), temporally-constrained automated methods perform substantially worse (oracle/human ~0.93–0.99 vs automated 0.58–0.72), indicating a significant gap.",
            "uuid": "e2205.1"
        },
        {
            "name_short": "RAG (SemanticScholar)",
            "name_full": "Retrieval-Augmented Generation using the SemanticScholar API",
            "brief_description": "A retrieval-augmented pipeline that fetches top-K full-text snippets from Semantic Scholar (filtered by publication date) and supplies them as context to LLMs for claim feasibility verification.",
            "citation_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "mention_or_use": "use",
            "system_or_method_name": "RAG (SEMANTICSCHOLAR) retrieval-augmented generation",
            "scientific_domain": "Scientific claim verification / materials science literature retrieval",
            "validation_type": "computational validation",
            "validation_description": "Given a claim and a knowledge cutoff, the system retrieves the top 20 relevance-ranked full-text snippets (~500 words each) from Semantic Scholar that predate the claim's source paper and includes them in an LLM prompt (with 20-shot ICL and CoT) to decide feasibility. This prevents temporal contamination while enabling evidence-based verification.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "No direct simulation-vs-experiment comparison; RAG serves as a literature-based computational validation approach and is compared in performance to code-based (CODESCIENTIST) and oracle/human modes. RAG methods improved performance modestly over CoT-only baselines in the temporally restricted task but remained below oracle/human performance.",
            "validation_success_rate": "RAG (temporally-filtered) baseline accuracies reported: e.g., O4-MINI CoT+ICL+RAG ~0.71 overall; Claude Sonnet RAG (temporally-unrestricted) up to 0.87; when RAG is allowed to access documents after the cutoff (temporally-unrestricted), performance increases substantially (e.g., GPT-40-MINI RAG no-date 0.76).",
            "domain_validation_standards": "Uses literature evidence as acceptable computational validation for claim verification when source papers or prior evidence exist; still recognized as inferior to direct experimental verification when experiments are required.",
            "when_simulation_sufficient": "RAG is useful when relevant prior literature exists that directly supports/refutes a claim; when such literature is absent (novel claims), RAG alone is insufficient.",
            "simulation_failures": "Not applicable (literature retrieval approach). However, retrieval misses or irrelevant snippets can reduce verification accuracy; no specific failure cases enumerated.",
            "uncertainty_quantification": "Uncertainty is reflected indirectly via model accuracy and per-category performance; no calibrated confidence scores or retrieval-uncertainty metrics are reported.",
            "fabrication_detection": "RAG contributes to detecting fabricated/infeasible claims by finding contradicting or absent evidence in pre-cutoff literature; no specialized fabrication detectors are applied.",
            "validation_cost_time": "Reported model cost-per-1k-claims for RAG vary by base model (e.g., O4-MINI RAG cost ~$27/1k; GPT-40-MINI RAG ~$3/1k) as shown in Table 2. Temporal filtering increases retrieval complexity but not directly the per-claim compute beyond increased prompt context.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Effectiveness depends on coverage and quality of retrieved literature and on temporal filtering; cannot verify claims lacking prior literature evidence and may be confounded by subtle or technical claim details absent from abstracts or snippet-sized spans.",
            "acceptance_credibility": "RAG improves evidence-grounded justifications which increases credibility relative to CoT-only LLM outputs, but temporally-restricted RAG still underperforms oracle/human verification, limiting immediate community acceptance.",
            "comparison_to_gold_standard": "When RAG is temporally unrestricted (allowed to retrieve post-cutoff and source paper), models approach oracle performance (e.g., RAG no-date for 04-MINI up to 0.90 in Table 3), but temporally-filtered RAG remains well below oracle/human ceilings.",
            "uuid": "e2205.2"
        },
        {
            "name_short": "CODESCIENTIST",
            "name_full": "CODESCIENTIST (code-generation experiment execution pipeline)",
            "brief_description": "A code-generation pipeline that prompts an LLM to author Python experiments or simulations, executes the generated code in a sandbox, and uses the resulting outputs as evidence to assess claim feasibility.",
            "citation_title": "Codescientist: End-to-end semiautomated scientific discovery with code-based experimentation",
            "mention_or_use": "use",
            "system_or_method_name": "CODESCIENTIST (Python code-based experiment generation + sandbox execution)",
            "scientific_domain": "Automated scientific discovery; materials science when applied in this work",
            "validation_type": "low-fidelity simulation",
            "validation_description": "Two-stage pipeline: (1) LLM is prompted to generate Python code for experiments/simulations that would test the claim; (2) generated code is executed in a sandbox (MODAL.COM) with PIP access, output logs captured, and then fed back to the model for a final decision. For tractability in the paper's experiments, runs were restricted to a single iteration (no reflection), 10-minute execution limits (versus a full 6-hour budget), and reduced debugging iterations.",
            "simulation_fidelity": "Low-to-moderate: fidelity depends on the nature of the generated code. In the paper's baseline runs, experiments were intentionally scoped small due to runtime/cost limits (10-minute timeouts, single iteration). No domain-specific high-fidelity physics/DFT solvers were systematically used; accuracy relative to physical experiments is therefore limited and not quantified.",
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "No direct quantitative comparison between CODESCIENTIST simulation outputs and published experimental results is provided. The paper reports that claims grounded in code/simulation are among the lowest-scoring categories for model feasibility assessment, suggesting simulation-only validation often fails to match experimental conclusions in practice.",
            "validation_success_rate": "CODESCIENTIST baseline accuracies reported in Table 2: examples include GPT-40-MINI CoT+ICL+CODE ~0.64 overall; O4-MINI CoT+ICL+CODE ~0.68; Claude-Sonnet CoT+ICL+CODE ~0.63. No per-claim experimental validation success rates vs real-world experiments are provided.",
            "domain_validation_standards": "The method is proposed as a rapid, inexpensive pilot-testing approach for feasibility filtering; authors note that domain acceptance requires demonstrating that code-based experiments reliably predict real experimental outcomes, which has not been established here.",
            "when_simulation_sufficient": "Paper suggests code/simulations can be sufficient for inexpensive feasibility filtering when experiments are expensive and when simulations capture relevant physics; however, because the baseline code executions were low-resource and truncated, the authors caution that many claims still require real experiments.",
            "simulation_failures": "No explicit per-claim failure cases given; general observation: claims originally based on experiments or code/simulation had lower automated-assessment accuracy, implying limitations in code-based validation under the constrained execution regime used.",
            "uncertainty_quantification": "No explicit error bars or uncertainty quantification on code-execution results are provided. Uncertainty in conclusions is evaluated indirectly via model-level accuracy metrics.",
            "fabrication_detection": "CODESCIENTIST can be used to attempt to reproduce or falsify claim results programmatically; this may help detect fabricated or infeasible claims when code outputs contradict claimed findings. The paper does not evaluate systematic detection of fabricated AI-generated experiments beyond this capability.",
            "validation_cost_time": "Full CODESCIENTIST experiments are noted to be expensive (initially cited ~$4 per experiment). For scale, the authors truncated runs: single iterations and 10-minute runtime limits to keep total costs tractable (approximate reported total if run fully would be in the thousands of USD for large subsets). Table 2 provides per-1k-claim cost estimates for the CODE pipeline (varies by base model; e.g., GPT-40-MINI CODE cost ~$4/1k in one line but other models show higher costs).",
            "hybrid_validation_approach": true,
            "validation_limitations": "High cost and runtime for high-fidelity code experiments; sandbox limitations and single-iteration runs reduce effectiveness; lacking use of domain-specific high-fidelity simulators in systematic fashion; no per-claim ground-truth comparison between code outputs and later experimental results provided.",
            "acceptance_credibility": "Paper frames CODESCIENTIST as promising for pilot testing and increasing efficiency, but notes current resource and fidelity limits reduce immediate credibility as a replacement for experiments; thus hybrid approaches (literature + cheap code tests) are suggested as practical.",
            "comparison_to_gold_standard": "Compared to oracle-source-paper and human expert evaluation (gold), CODESCIENTIST-based feasibility assessment achieves lower accuracy (~0.63–0.68) and thus is not a substitute for source-paper/expert verification as implemented in this study.",
            "uuid": "e2205.3"
        },
        {
            "name_short": "Oracle Source Paper",
            "name_full": "Oracle-source-paper verification baseline",
            "brief_description": "A verification baseline that supplies the model with the original source paper (full LaTeX) that produced the claim to measure the practical verification ceiling when oracle evidence is provided.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Oracle-source-paper (retrieval-augmented verification with access to the original paper)",
            "scientific_domain": "Scientific claim verification / materials science",
            "validation_type": "computational validation",
            "validation_description": "The model receives the claim plus the full original source paper (LAEX source) in a RAG-style prompt to decide truth/feasibility; this measures how well a model can verify a claim when given direct, authoritative evidence from the source.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "Oracle access is a computational verification against the published experimental/simulation/theoretical evidence reported in the source paper. The paper treats oracle/source-paper and human expert review as the best available proxies for ground truth, showing oracle models achieve a performance ceiling well above temporally restricted methods.",
            "validation_success_rate": "Reported oracle-source-paper accuracies: O4-MINI and Claude Sonnet variants reach nearly 100% (e.g., CLAUDE-SONNET ORACLE ~1.00 on a subset; O4-MINI ORACLE ~0.96 on subset); GPT-40-MINI oracle range ~0.71–0.76. These values suggest the verification ceiling when the source article is available.",
            "domain_validation_standards": "Using the original paper as evidence is treated as the de facto gold standard for claim verification when the paper directly addresses the claim.",
            "when_simulation_sufficient": "If the source paper reports computational/simulation evidence and the model can correctly interpret it, oracle verification by reading the source can be sufficient to classify feasibility for claims that are supported/refuted in that paper.",
            "simulation_failures": "Not applicable; oracle mode is a read/interpret verification against published results. Any mismatch would be due to model inability to interpret the source, which was low for stronger base models.",
            "uncertainty_quantification": "Oracle performance is reported as aggregate accuracies; no additional statistical uncertainty measures provided.",
            "fabrication_detection": "Oracle mode implicitly detects fabricated claims if the source paper contradicts the claim; the paper uses this as a check on automatically generated negative claims and on data quality.",
            "validation_cost_time": "Oracle evaluation is computational (LLM inference) and relatively cheap compared to running new physical experiments; cost depends on model inference and prompt length (not quantified beyond model cost-per-1k table entries).",
            "hybrid_validation_approach": false,
            "validation_limitations": "Oracle mode assumes access to the original source paper; it does not address claims that require new experiments beyond what the source contains. Also, model comprehension of the source limits success.",
            "acceptance_credibility": "High — oracle-source-paper performance aligns closely with human expert judgments and is used to validate the benchmark labels and claim generation process.",
            "comparison_to_gold_standard": "Oracle-source-paper is used as a near-gold standard and achieves performance close to human oracle performance (human expert after adjudication ~99%). The paper directly contrasts temporally-restricted methods with oracle performance to show the achievable ceiling.",
            "uuid": "e2205.4"
        },
        {
            "name_short": "Human Expert Review",
            "name_full": "Domain expert validation (human adjudication)",
            "brief_description": "Manual verification by a materials-science domain expert who reads each claim and its source paper to determine validity; used to assess the quality of automatically generated claims and labels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Human domain-expert review and adjudication",
            "scientific_domain": "Materials science",
            "validation_type": "other",
            "validation_description": "A graduate-level domain expert evaluated 100 test-set claims by reading the claim and source paper and labeling validity. Disagreements with LLM labels were reviewed and resolved; initial agreement was 93% and increased to 99% after adjudication. Interrater agreement (pre-adjudication) Cohen's κ = 0.86.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "Human expert review serves as a quality-control comparison to automated computational approaches rather than an experimental/simulation method; human adjudication agreed strongly with oracle/source-paper labels.",
            "validation_success_rate": "For the subset reviewed: initial agreement 93% with LLM-generated labels; after reviewing LLM evidence and resolving disagreements, agreement rose to 99%.",
            "domain_validation_standards": "Human expert judgment with access to source papers is treated as a high-standard validator; adjudication was used to resolve ambiguities and data-generation errors.",
            "when_simulation_sufficient": "Human experts can sometimes determine infeasibility via literature/simulation evidence, but the paper emphasizes that humans too sometimes miss difficult-to-find evidence on first pass and benefit from having model-provided evidence and explanations.",
            "simulation_failures": "Not applicable; human errors (missed evidence) are noted as sources of initial disagreement, but no systematic failure examples are provided.",
            "uncertainty_quantification": "Interrater agreement metric (Cohen's κ = 0.86) is reported to quantify annotation reliability; no per-expert confidence scores provided.",
            "fabrication_detection": "Human experts were able to detect many automatically-generated infeasible claims; after consulting model explanations and evidence, they resolved almost all disagreements, demonstrating humans can detect fabricated/infeasible claims when given appropriate evidence context.",
            "validation_cost_time": "Human review is time-consuming and costly relative to automatic methods; the paper does not provide per-claim time/cost estimates for the expert review but implies scalability limitations.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Single expert cannot cover all subdomains; initial human misses of difficult evidence occurred; expert review is not scalable to tens of thousands of claims without substantial cost.",
            "acceptance_credibility": "High — human expert adjudication is used to validate dataset labels and is presented as increasing trust in the benchmark's quality.",
            "comparison_to_gold_standard": "Human expert evaluation with access to source papers performs at or near the oracle ceiling (99% after resolution) and is used to confirm the correctness of generated labels and to calibrate automated systems' performance.",
            "uuid": "e2205.5"
        },
        {
            "name_short": "Temporal Filtering",
            "name_full": "Temporal filtering / knowledge cutoff analysis",
            "brief_description": "A method for ensuring models assess feasibility using only knowledge available before a claim's source paper publication date to avoid model contamination by training data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Temporal filtering (knowledge cutoff enforcement)",
            "scientific_domain": "Machine learning for scientific claim verification",
            "validation_type": "other",
            "validation_description": "For each claim, retrieval and evidence access are restricted to documents authored on or before the claim's source-paper date. The approach also includes analysis of base-model performance on claims from before vs after each model's advertised knowledge cutoff to estimate model contamination.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "Temporal filtering is orthogonal to simulation/experiment methods; it constrains what computational evidence is available for validation. The paper's contamination analysis (Table 4) shows nearly identical model performance on claims before vs after advertised model cutoffs, suggesting limited contamination.",
            "validation_success_rate": "Contamination analysis shows minimal delta in accuracy before vs after model cutoffs (±1%), e.g., GPT-40-MINI delta 0.003, O4-MINI delta -0.011, Claude-Sonnet delta 0.011 across all 8.4k claims.",
            "domain_validation_standards": "Temporal filtering is presented as necessary for creating prediction-style benchmarks where future claims must be judged only using past evidence; ensuring models do not rely on future data is a domain requirement for this problem formulation.",
            "when_simulation_sufficient": "Temporal filtering does not directly determine simulation sufficiency; it only constrains available pre-cutoff computational evidence.",
            "simulation_failures": "Not applicable.",
            "uncertainty_quantification": "Contamination assessment uses aggregate accuracy deltas; no probabilistic contamination measures are provided.",
            "fabrication_detection": "Temporal filtering helps detect fabricated claims insofar as it prevents models from trivially using future citations or the source paper to validate claims; fabricated/infeasible claims are still detected by retrieval or code-based checks within the cutoff constraints.",
            "validation_cost_time": "Temporal filtering increases the complexity of retrieval (must filter by date) but has negligible direct computational cost relative to LLM inference; no specific runtime figures provided.",
            "hybrid_validation_approach": null,
            "validation_limitations": "Relies on accurate model cutoff metadata and assumes base models are not already significantly contaminated; model editing/unlearning may help but are not solved; temporal filtering does not eliminate all forms of indirect contamination.",
            "acceptance_credibility": "Temporal filtering increases the scientific rigor of the benchmark by enabling a prediction-style evaluation; acceptance depends on demonstrable low contamination levels, which the authors provide some evidence for.",
            "comparison_to_gold_standard": "Temporal filtering is compared against temporally-unrestricted (oracle) retrieval to show the performance gap induced by realistic evidence constraints.",
            "uuid": "e2205.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Codescientist: End-to-end semiautomated scientific discovery with code-based experimentation",
            "rating": 2
        },
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "Accelerating computational materials discovery with machine learning and cloud high-performance computing: from large-scale screening to experimental validation",
            "rating": 2
        },
        {
            "paper_title": "Fact or fiction: Verifying scientific claims",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2
        },
        {
            "paper_title": "Highly accurate protein structure prediction with alphafold",
            "rating": 1
        },
        {
            "paper_title": "Scaling deep learning for materials discovery",
            "rating": 1
        }
    ],
    "cost": 0.02087575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Matter-of-Fact: A Benchmark for Verifying the Feasibility of Literature-Supported Claims in Materials Science</h1>
<p>Peter Jansen ${ }^{1,2}$, Samiah Hassan ${ }^{1}$, Ruoyao Wang ${ }^{1}$<br>${ }^{1}$ University of Arizona, ${ }^{2}$ Allen Institute for Artificial Intelligence<br>pajansen@arizona.edu</p>
<h4>Abstract</h4>
<p>Contemporary approaches to assisted scientific discovery use language models to automatically generate large numbers of potential hypothesis to test, while also automatically generating code-based experiments to test those hypotheses. While hypotheses can be comparatively inexpensive to generate, automated experiments can be costly, particularly when run at scale (i.e. thousands of experiments). Developing the capacity to filter hypotheses based on their feasibility would allow discovery systems to run at scale, while increasing their likelihood of making significant discoveries. In this work we introduce MATTER-OF-FACT, a challenge dataset for determining the feasibility of hypotheses framed as claims. MATTER-OF-FACT includes 8.4 K claims extracted from scientific articles spanning four high-impact contemporary materials science topics, including superconductors, semiconductors, batteries, and aerospace materials, while including qualitative and quantitative claims from theoretical, experimental, and code/simulation results. We show that strong baselines that include retrieval augmented generation over scientific literature and code generation fail to exceed $72 \%$ performance on this task (chance performance is $50 \%$ ), while domain-expert verification suggests nearly all are solvable - highlighting both the difficulty of this task for current models, and the potential to accelerate scientific discovery by making near-term progress. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Contemporary language models are being broadly integrated into the scientific discovery pipeline. Existing systems can generate hypothesis ( Si et al., 2024; Radensky et al., 2024), run experiments ( Lu et al., 2024; Li et al., 2024; Jansen et al., 2025), analyze data (Majumder et al., 2025), and write or review papers (Liu and Shah, 2023; Zhou et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>2024). A central benefit - and challenge - of these systems is that they can function at scales greater than any human scientist. For example, hypothesis generation systems might easily produce thousands of potential hypotheses (Lu et al., 2024; Jansen et al., 2025), and running experiments to test each of these would be costly and impractical - particularly in that few experiments are likely to yield positive results. In this work we investigate the task of feasibility assessment (e.g. O'Neill et al., 2025), or assessing whether we can filter hypothesis (expressed as claims) to those that are most likely to be feasible, and have their hypotheses confirmed. Performing well at this task would allow us to incorporate feasibility filtering in hypothesis generation systems, and potentially make more discoveries with a given (fixed) experimental budget.</p>
<p>Feasibility assessment is in principle quite challenging as it involves (at times) a high degree of uncertainty in predicting future results, and yet it is a task that scientists perform frequently during experiment planning stages - selecting the hypotheses that we believe are most likely to return positive results based on a combination of literature, pilot experiments or analyses (which may include empirical work, or code/simulations), and past experience. In this work we aim to investigate how well current models can perform this feasibility assessment task, and provide a benchmark to assist in improving model performance over time.</p>
<p>Generating data to test feasibility assessment is challenging, as (by nature) the experimental results of proposed hypotheses are as yet unknown, which makes gold labels for determining whether a hypothesis is feasible or infeasible effectively unavailable. To address this challenge, we operationalize feasibility assessment as a temporally-filtered claim verification task. As with conventional claim verification tasks (e.g. Thorne et al., 2018), we generate a corpus of claims extracted from recent scientific literature - however, in addition, each</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Examples of the four main materials science topics included in MATTER-OF-FACT, including superconductors, semiconductors, batteries, and aerospace materials. Each claim includes the claim text, gold label (true/feasible or false/infeasible), a brief explanation and supporting facts based from the original paper the claim was sourced from, additional meta-data (such as whether the claim is quantitative or qualitative), and the knowledge cutoff date for the feasibility assessment task.
claim is paired with a "knowledge cut-off date", which is the date that the paper the claim was generated from was first authored. When performing the feasibility assessment task, models are allowed to use any information available before the source paper was authored to assess feasibility, essentially rewinding into the past to predict future results. In this way, models are provided with knowledge up to (for example) 2023, and must use that knowledge (through a combination of literature search, smallscale code-based experimentation, world modeling, or other methods) to predict whether genuine results (and artificially-generated infeasible results) from 2024 are feasible or infeasible.</p>
<p>The contributions of this work are:</p>
<ol>
<li>We introduce Matter-of-Fact, a benchmark of 8.4 K claims extracted from recent materials science articles in four high-impact subdomains. Each claim includes categorical information (qualitative vs quantitative, and experiment, code, or theory focused), and is paired with a knowledge cut-off date to use for the feasibility assessment task.</li>
<li>We empirically demonstrate that strong baseline models using a variety of solution methods (including retrieval-augmented generation with SemanticsCholar, as well as evidence gathered from code-generation) across
base models (GPT-4O-MINI, O4-MINI, and Claude SonNet 3.7) achieve a maximum of $72 \%$ accuracy, highlighting the challenging nature of this feasibility assessment task.</li>
<li>We assess the quality of the claims both by domain expert evaluation, and by evaluating base models in a conventional claim verification task. Humans and models reach $93 \%+$, suggesting the benchmark is of high quality.</li>
</ol>
<h2>2 Related Work</h2>
<p>Scientific Claim Verification Datasets: The scientific claim verification task requires a model to determine whether a claim (typically extracted from a scientific paper) is true or false, either by leveraging its pretrained scientific knowledge or retrieving evidence from a corpus, with a selection of scientific claim verification benchmarks shown in Table 1. SciFact (Wadden et al., 2020) contains 1.4 K biomedical-domain claims generated by showing citances (sentences that cite a paper and describe its contribution) to human annotators, who were then asked to generate associated claims. Where SciFact pairs claims with a set of 5 K abstracts that can be used for gathering evidence, SciFactOpen (Wadden et al., 2022) expands this evidence retrieval corpus to 500 K abstracts, presenting a</p>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>Domain</th>
<th>Claims</th>
<th>Source</th>
<th>Generation Method</th>
</tr>
</thead>
<tbody>
<tr>
<td>SciFact (Wadden et al., 2020)</td>
<td>Biomed</td>
<td>1.4 K</td>
<td>Paper Abstracts</td>
<td>Citances provided to annotators</td>
</tr>
<tr>
<td>COVID-Fact (Saakyan et al., 2021)</td>
<td>Biomed</td>
<td>4.0 K</td>
<td>Reddit</td>
<td>Extract positive, generate counterclaim</td>
</tr>
<tr>
<td>SciFact-Open (Wadden et al., 2022)</td>
<td>Biomed</td>
<td>1.4 K</td>
<td>Paper Abstracts</td>
<td>See SciFact</td>
</tr>
<tr>
<td>ClaimCheck (Ou et al., 2025)</td>
<td>ML</td>
<td>154</td>
<td>Paper Reviews</td>
<td>Emphasizes claim weaknesses</td>
</tr>
<tr>
<td>SciTab (Lu et al., 2023)</td>
<td>Comp. Sci</td>
<td>1.2 K</td>
<td>Paper Tables</td>
<td>Compositional reasoning on tables</td>
</tr>
<tr>
<td>MatterOfFact (This work)</td>
<td>Mat. Sci</td>
<td>8.4 K</td>
<td>Paper full-text</td>
<td>Extract positive, generate infeasible</td>
</tr>
</tbody>
</table>
<p>Table 1: A comparison of claim verification datasets with Matter-of-Fact, including their domain, size, source of the information used to generate or extract claims from, and the claim generation method.
more challenging retrieval problem. Also in the biomedical domain, COVID-Fact (Saakyan et al., 2021) consists of over 4 K claims extracted from Reddit. Lu et al. (2023) introduce SciTab, which requires verifying computer science claims centrally using tables extracted from papers. ClaimCheck (Ou et al., 2025) uses reviews of rejected NeurIPS submissions from OpenReview to build a corpus of 154 claims that emphasize identifying the weaknesses in scholarly claims. In contrast, Matter-of-Fact builds a corpus of 8.4 K materials science claims for feasibility assessment that are generated from the nuanced results found in the full text of source articles (rather than abstracts), and where negative claims focus on being scientifically infeasible rather than factually incorrect.</p>
<p>Claim Verification Models: Our framing of feasibility detection is as temporally-filtered claim verification with a knowledge cutoff. More broadly, recent approaches to claim verification typically involve two key steps: evidence retrieval and fact checking. For the retrieval step, augmenting LLMs with retrieved documents (Izacard et al., 2022) or knowledge bases (Baek et al., 2023; Hang et al., 2024) can be effective for improving fact verification performance of models. $\mathrm{Re}^{2} \mathrm{G}$ (Glass et al., 2022) extends the retrieval step with a trained reranker to achieve better retrieval performance for fact checking. Rani et al. (2023) propose a form of query expansion that generates claim-related questions as queries to retrieve supporting documents. For fact checking, some methods make use of structured knowledge representations such as knowledge graphs (Dammu et al., 2024) and first-orderlogic (Wang and Shu, 2023) to organize evidence and verify facts. End-to-end systems combine the entire retrieval and verification pipeline, such as ARSJoint (Zhang et al., 2021) and SciClaims (Ortega and Gómez-Pérez, 2025). In this work we demonstrate similar retrieval-backed systems (with temporal filtering) for feasibility assessment, while also providing formal approaches based on code generation.</p>
<h2>Scientific Discovery and Feasibility Assessment:</h2>
<p>Automated scientific discovery is frequently divided into two subfields: problem-specific methods (like AlphaFold (Jumper et al., 2021) for protein structure prediction), and problem-general methods that work across a variety of problem types. Examples of problem-specific systems in the materials science domain include GNoME (Merchant et al., 2023), a graph neural network (GNN) based method that discovered over 2.2 million new stable crystal structures, and Schmidt et al. (2023)'s method for using crystal-graph neural networks together with high-quality data for accurate stability prediction. The latter work screened 1 billion materials, discovering 150k+ stable compounds, and identified extreme-property materials like superconductors. Similarly, Chen et al. (2024) combine machine learning models with traditional physicsbased models to discover compounds to which can potentially serve as solid electrolytes. These problem-specific methods can be applied to feasibility assessment by predicting highly specific properties of unknown materials. Matter-ofFact works to bridge the gap between problemspecific methods and problem-general methods by providing a large set of claims across 4 broad and high-impact areas of materials science, each of which is likely to benefit from a variety of problemspecific methods to arrive at accurate feasibility assessments. As we empirically demonstrate in our modeling results, because Matter-of-Fact nominally requires a large set of capacities to solve, it is challenging benchmark for measuring a general capacity to assess feasibility over broad subdomains.</p>
<h2>3 Dataset</h2>
<p>The Matter-of-Fact benchmark consists of 8.4 K claims extracted from the full-text of materials science articles. The extraction and validation</p>
<p>process is described below, with example claims shown in Figure 1.</p>
<p>Inclusion Criteria: We assembled a corpus of recent publicly-available materials science domain articles by crawling Arxiv for all papers within the MATERIALS SCIENCE and SUPERCONDUCTIVITY topics submitted on or after January 2022, resulting in a total of 24 K articles. Articles were then filtered based on specific inclusion criteria. First, articles that were not licensed using a specific permissive license (Creative CommonsBY ATTRIBUTION-4.0) were removed. Second, to prevent having to use a PDF-TO-TEXT conversion pipeline (which can have limited quality on complex tables, chemical formulas, mathematics, and other artifacts found within materials science articles), we further filtered to include only articles with $\mathrm{IA}_{\mathrm{E}} \mathrm{X}$ source available. Papers with long source ( $&gt;30 \mathrm{k}$ tokens) were also removed (approximately $16 \%$ of articles). After initial filtering, 4.2 K articles remained. Our focus in this work is specifically in four high-impact subdomains: superconductors, semi-conductors, batteries, and aerospace materials. To identify articles within these topics, we performed topic labeling of each abstract using GPT-40-MINI with a prompt that emphasized identifying articles within these 4 focus areas. We then sampled 500 total articles ( 125 from each topic) to use for claim generation.</p>
<p>Initial Claim Generation: Claims were generated by providing the full-text ( $\mathrm{IA}_{\mathrm{E}} \mathrm{X}$ source) of each paper in a prompt, together with task instructions and JSON output format requirements. The model was instructed to generate matched pairs of claims - one true, and one that was clearly false or infeasible - and for each claim, to provide a list of supporting evidence, followed by an overall explanation as to why the evidence supports or refutes the claim. ${ }^{2}$ Claims were instructed to be standalone, and not make reference to the paper in the claim text (i.e. "Table 4 claims the boiling point of Material X is..."), so that they could be (in principle) solved without reference to the original source paper. Negative claims were instructed to be false or clearly infeasible (but not overly so), and not simply claims for which no evidence was available. Similarly, negative claims were instructed to use</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>balanced language so as not to give away their true or false nature by particulars of wording, such as through use of negation markers (i.e. "Material X does not have..."), and to instead use neutral framings. In addition to the above constraints, claims were explicitly asked to be authored on two dimensions. The first asks for claims to specifically test either qualitative knowledge (e.g. "In Situation X, Phenonemon Y helps Material Z maintain its superconductivity"), or quantitative knowledge (e.g. "Material X superconducts at 77 Kelvin"). Second, claims were asked to be authored cross 4 main types: those that focus on experimental results, code/simulation results, theoretical results, or integrative methods across types.</p>
<p>Balanced Temporal Sets: Claims were temporally sorted into those from papers first appearing on Arxiv in 2022 (for training), those in 2023 (for validation), and the most recent claims from papers submitted between 2024 and April 2025 (for testing). For each set, we filtered claims such that equal numbers of true and false claims were present, to achieve a baseline (random chance) performance of $50 \%$. Claims were also balanced such that equal numbers within the experimental, code, theory, and integrative categories appear within a given set. The final dataset includes a total of 8.4 K claims, distributed as 1.4 K claims for training, 2.5 K for validation, and 4.4 K for testing.</p>
<p>Domain Expert Validation: To measure the quality of the claim generation process, a domain expert with a graduate degree in materials science was given each claim and its source paper, and independently asked to determine the validity of the claim. This was a challenging task, because the claims span broad areas of materials science that would be unusual for any single individual to have expertise within. The domain expert performed this task for 100 claims from the test set, and initially agreed in $93 \%$ of cases (while noting a further $3 \%$ of claims appeared to not meet criteria, such as explicitly referencing the original source paper). They were then provided with the LLM-generated labels and explanations, and asked to resolve disagreements (either LLM errors, or human error), noting that nearly all errors were a result of the domain expert missing difficult-to-find evidence on their first attempt, and ultimately reaching $99 \%$ agreement after this resolution process. This empirically suggests that the overall data quality is high ( $96 \%$ after discounting data not meeting generation</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Flow diagrams for two models: the retrieval-augmented generation (RAG) model that retrieves snippets from the full-text of papers using SEMANTICSCHOLAR (left), and a code generation model that executes PYTHON code and examines the output using CODESCIENTIST.</p>
<p>Criteria). Before resolution, interrater agreement using Cohen's Kappa (Cohen, 1960) was <em>κ</em> = 0.86, or strong agreement (McHugh, 2012), suggesting the claims are highly objective.</p>
<h2>4 Baseline Models</h2>
<p>We evaluate performance on the MATTER-OF-FACT dataset using a selection of baseline models described below. Models are provided with the text of the claim, and must predict a binary label (true/feasible, or false/infeasible), as well as provide a brief explanation for their reasoning. All models investigated in this work use in-context learning (ICL), and are characterized across three common base models at different price/performance points, including GPT-4O-MINI, O4-MINI, and CLAUDE SONNET 3.7. Our retrieval-augmented generation and code-generation models are shown in Figure 2.</p>
<h3>4.1 Feasibility Assessment Models</h3>
<p><strong>Chain-of-Thought (CoT), ICL, Reflection:</strong> The language model is provided with a prompt that includes the claim, and a request to think and/or plan before responding in the style of Chain-of-Thought (Wei et al., 2022). We also include two variations of this model. The first includes a <em>20-shot</em> in-context learning example (Brown et al., 2020), using 20 claim problems (together with their supporting facts and explanations) drawn from the training set, including balanced numbers of true and false claims. The second includes adding a reflection step (Madaan et al., 2023) where, after the initial generation, the model then reflects on its response, then provides a final answer and explanation for the reasoning behind that answer.</p>
<p><strong>Retrieval Augmented Generation (RAG):</strong> Using the claim text as a query, the model first retrieves the <em>top K</em> matching full-text snippets from scholarly scientific articles using the SEMANTICSCHOLAR API (Kinney et al., 2023), where each snippet generally takes the form of a span of text (approximately 500 words in length) from an article indexed by SEMANTICSCHOLAR that most closely matches the query. To prevent temporal contamination with oracle knowledge, full-text snippets are filtered such that papers authored after the source paper for a given claim are not included in the search. For example, if a claim was derived from a paper first published on Arxiv in March 2024, then</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Overall Accuracy</th>
<th style="text-align: center;">True</th>
<th style="text-align: center;">False</th>
<th style="text-align: center;">Accuracy by Category</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Cost (per 1k)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Qual.</td>
<td style="text-align: center;">Qnt.</td>
<td style="text-align: center;">Exp.</td>
<td style="text-align: center;">Code</td>
<td style="text-align: center;">Ther.</td>
<td style="text-align: center;">Int.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">RANDOM BASELINE</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">GPT-40-MINI</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Chain-of-Thought (CoT)</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$1</td>
</tr>
<tr>
<td style="text-align: center;">CoT + 20-sHot ICL</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$2</td>
</tr>
<tr>
<td style="text-align: center;">CoT + 20-sHot ICL + Reflection</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$3</td>
</tr>
<tr>
<td style="text-align: center;">CoT + ICL + RAG (SEMANTICSChOLAR)</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$3</td>
</tr>
<tr>
<td style="text-align: center;">CoT + ICL + CODE (CODESCIENTIST)</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$4</td>
</tr>
<tr>
<td style="text-align: center;">04-MINI</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Chain-of-Thought (CoT)</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$4</td>
</tr>
<tr>
<td style="text-align: center;">CoT + 20-sHot ICL</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$15</td>
</tr>
<tr>
<td style="text-align: center;">CoT + 20-sHot ICL + Reflection</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$30</td>
</tr>
<tr>
<td style="text-align: center;">CoT + ICL + RAG (SEMANTICSChOLAR)</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$27</td>
</tr>
<tr>
<td style="text-align: center;">CoT + ICL + CODE (CODESCIENTIST)</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$34</td>
</tr>
<tr>
<td style="text-align: center;">CLAUDE-SONNET 3.7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Chain-of-Thought (CoT)</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$9</td>
</tr>
<tr>
<td style="text-align: center;">CoT + 20-sHot ICL</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$44</td>
</tr>
<tr>
<td style="text-align: center;">CoT + 20-sHot ICL + Reflection</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$87</td>
</tr>
<tr>
<td style="text-align: center;">CoT + ICL + RAG (SEMANTICSChOLAR)</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$76</td>
</tr>
<tr>
<td style="text-align: center;">CoT + ICL + CODE (CODESCIENTIST)</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$173</td>
</tr>
</tbody>
</table>
<p>Table 2: Model performance on the feasibility assessment task, including overall performance, as well as performance broken down by specific categories of feasibility assessment claim problems. True and False represent performance on problems with those gold labels. Qual. and Quant. represent performance on qualitative and quantitative problems. Exp., Code, Ther., and Int. represent performance on claims focusing on experimental, code/simulation, theoretical, or integrative results, respectively. Cost represents the estimated model cost per 1000 claims, in US dollars.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Overall <br> Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-40-MINI</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">RAG (SEMANTICSChOLAR (No DATE))</td>
<td style="text-align: center;">0.76</td>
</tr>
<tr>
<td style="text-align: left;">ORACLE SOURCE PAPER</td>
<td style="text-align: center;">0.71</td>
</tr>
<tr>
<td style="text-align: left;">04-MINI</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">RAG (SEMANTICSChOLAR (No DATE))</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr>
<td style="text-align: left;">ORACLE SOURCE PAPER</td>
<td style="text-align: center;">0.96</td>
</tr>
<tr>
<td style="text-align: left;">CLAUDE-SONNET 3.7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">RAG (SEMANTICSChOLAR (No DATE))</td>
<td style="text-align: center;">0.87</td>
</tr>
<tr>
<td style="text-align: left;">ORACLE SOURCE PAPER</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: left;">HUMAN DOMAIN EXPERT</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">INITIAL ASSESSMENT</td>
<td style="text-align: center;">0.93</td>
</tr>
<tr>
<td style="text-align: left;">AFTER RESOLVING DISAGREEMENTS</td>
<td style="text-align: center;">0.99</td>
</tr>
</tbody>
</table>
<p>Table 3: Model performance on the claim verification task, using oracle models. [Note that due to the high model com, the ORACLE SOURCE PAPER (SONNET) model is assessed on a subset of the test set.
only papers authored in February 2024 or before will be included in the snippet search. The top 20 matching full-text snippets (sorted by the provided relevance score) are included in the language model prompt, in a retrieval-augmented-generation
paradigm (Lewis et al., 2020). The prompt for this model also includes a 20 -shot ICL example, and request for chain-of-thought reasoning.</p>
<p>Code Generation (CODESCIENTIST): This model is performed in two stages. During the first stage, the model is provided with the claim text, and prompted to generate a code-based experiment or simulation in PYTHON that would produce useful evidence in supporting or refuting the claim. The code is then executed, and the code and execution results are provided to a second prompt with a request to generate an answer for the feasibility task as well as a supporting explanation. For code execution, we use the experiment execution portion of CODESCIENTIST (Jansen et al., 2025), which allows executing arbitrary PYTHON code in a virtual sandbox on MODAL.COM, and supports installing external supporting libraries through PIP. While this execution pipeline stores and saves output streams (e.g. STDOUT/STDERR), the model explicitly prompted to save a log of its work, as well as a final list of results, which are then provided back to the model to help make its final decision. For tractability, we run CODESCIENTIST in a limited form due to its high overall cost (initially</p>
<p>reported as $\$ 4$ per experiment), which would be intractible for the size of our dataset ( $\approx \$ 16 k$ for 4 K test claims). Instead of 25 debug iterations, we run CODESCIENTIST for a single iteration (without reflection), and reduce the experiment time limit from 6 hours to 10 minutes (or 31 total CPU-days across all test claims). The model is made aware of these limitations in the code generation prompt, and encouraged to design appropriately-scoped experiments and output to support the decision process.</p>
<h3>4.2 Claim Verification Models</h3>
<p>As a method of characterizing model performance when oracle information is available, Table 3 also provides two models that perform a claim verification task rather than the feasibility assessment task - that is, they do not have the same temporal restrictions, and are able to use data available after the source claim was authored.
RAG (Temporally Unrestricted): The retrievalaugmented generation model described above, but without temporal restrictions. For a given claim, snippets from any scientific article may be retrieved, including (potentially) the source article of the claim, or those that cite the source article.
Oracle Baseline: The language model is provided both with the claim, as well as the original source paper the claim was derived from (in the form of the paper's original $\mathrm{LA}_{\mathrm{E}} \mathrm{X}$ source retrieved from Arxiv) in a retrieval-augmented-generation paradigm. This baseline measures how well a model can verify the claim when provided with a source scientific article that directly speaks to that claim's validity/feasibility.
Oracle (Human Domain Expert): The domain expert evaluation, as described in Section 3.</p>
<h3>4.3 Results</h3>
<p>Feasibility Assessment Results: The performance of all models when evaluated in the feasibility assessment mode is shown in Table 2. Performance across all models ranges from 0.58 to 0.72 , with the models that use the smallest (and least expensive) base model (GPT-40-MINI) generally performing about 5 percent lower than the two more performant (and more costly) base models, O4mini and Claude Sonnet 3.7. Across models, adding features (such as in-context learning, reflection, RAG over SemanticScholar, or Code Generation) generally provides modest performance improvements, or does not improve per-
formance over the Chain-of-Thought baseline, highlighting the difficulty of this task when using conventional solution methods, and its suitability as a challenge task. When examining performance broken down by category, we observe that while the overall performance of a given base model is similar with different features, some models are more performant at identifying true/feasible claims than they are at identifying false/infeasible claims, and vice versa. All models perform better at assessing the feasibility of qualitative claims than quantitative claims, with this difference between $11 \%$ and $19 \%$ across all models, potentially a result of quantitative claims requiring the ability (through code or other means) of verifying the feasibility of specific numerical values present in the claims. In line with this reasoning, claims that are based on experiments or code/simulations consistently achieve the lowest performance, while those based on theoretical results are next-highest, with integrative claims achieving the highest performance.</p>
<p>Claim Verification Results: The performance of models when evaluated in the claim verification mode is shown in Table 3. In this mode the models have no temporal restrictions, and may use knowledge from the source paper, or papers authored after the source paper (including those that may cite the source paper) as evidence to perform the claim verification task. These experiments serve two purposes. First, they identify an effective ceiling of how well a given base model can perform even when provided with the original source article used to create a claim, with O4-MINI and CLAUDE SonNET 3.7 capable of achieving nearly a $100 \%$ ceiling performance, while GPT-40-MINI has more modest performance ceiling between 0.71 and 0.76 . Second, these models serve as a consistency evaluation for the claim generation protocol, emphasizing that when strong models are asked to verify the labels of these automatically generated claims, they nearly always agree with the gold label. Further emphasizing this is the domain expert performance, who (when provided with the original source article), agreed with the LLM-generated label for $99 \%$ of claims after resolving disagreements.</p>
<p>Taken together, these results empirically demonstrate the generation quality of the feasibility claims, while also emphasizing that common models and architectures still achieve overall modest performance on the feasibility assessment task.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Base Model</th>
<th style="text-align: center;">Knowledge <br> Cutoff Date</th>
<th style="text-align: center;">Accuracy <br> (before cutoff)</th>
<th style="text-align: center;">Accuracy <br> (after cutoff)</th>
<th style="text-align: center;">Accuracy <br> $\Delta$</th>
<th style="text-align: center;"># Samples <br> (before/after)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-40-MINI</td>
<td style="text-align: center;">Sept 2023</td>
<td style="text-align: center;">0.661</td>
<td style="text-align: center;">0.664</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">$3236 / 5124$</td>
</tr>
<tr>
<td style="text-align: left;">O4-MINI</td>
<td style="text-align: center;">May 2024</td>
<td style="text-align: center;">0.694</td>
<td style="text-align: center;">0.705</td>
<td style="text-align: center;">-0.011</td>
<td style="text-align: center;">$5168 / 3192$</td>
</tr>
<tr>
<td style="text-align: left;">CLAUDE-SONNET-3-7</td>
<td style="text-align: center;">Oct 2024</td>
<td style="text-align: center;">0.672</td>
<td style="text-align: center;">0.661</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">$6130 / 2230$</td>
</tr>
</tbody>
</table>
<p>Table 4: Knowledge contamination analysis of base models. In this analysis, performance of the Chain-of-Thought + 20-Short ICL + Reflection model is shown for claims from papers that were authored before or after a given model's advertised knowledge cutoff date. Given the temporal nature of the dataset, all 8.4 k claims across train, development, and test sets were included. All models show almost identical performance $( \pm 1 \%)$ when tested on claims from papers before or after their knowledge cutoff date, suggesting that knowledge contamination does not play a significant role in performance.</p>
<h2>5 Discussion</h2>
<p>Base-Model Contamination: A central part of the framing of our feasibility assessment task as a temporally-filtered claim verification task is that it requires models to have a minimum of contamination with knowledge beyond a given claim's knowledge cutoff date. While it is possible that techniques such as model editing and machine unlearning (Bourtoule et al., 2021; Tarun et al., 2023; Liu et al., 2025) may eventually allow the knowledge in a base model to be temporally filtered to minimize this contamination, this may have limited success in current forms (Lynch et al., 2024; Deeb and Roger, 2024; Du et al., 2024). Instead, here we aim to measure how much of the current model performance is likely due to model contamination (from, for example, the base model being trained on the source articles used to generate the claims). To measure this, we examine each base model's performance for claims extracted from papers before and after the base model's advertised training data knowledge cut-off dates. The results, shown in Table 4, show that the performance of the base models on claims from papers authored after their knowledge cutoff is nearly identical to the performance on claims authored by papers that are before the knowledge cutoff date. This empirically suggests that the performance of current base models on the feasibility assessment task is not due to model contamination, but due to other properties, such as their capacity for reasoning.
Pragmatic Ceiling Performance: While we empirically show that the feasibility of many claims can be assessed using inexpensive means, the models we demonstrate are far from achieving perfect performance on this task. Pragmatically, a model that achieves near $100 \%$ performance would be able to (with near perfect accuracy) determine whether claims are likely to be feasible or infeasible through some combination of literature search, inexpensive code-based experimentation, world
modeling, and other means. Acheiving 100\% performance is likely impractical, as many scientific claims can only be verified with empirical work, and not with literature search or simulation, particularly for those (most impactful) scientific results that are surprising because they run counter to expectations. That being said, even though effective ceiling performance on feasibility assessment tasks is likely to be less than $100 \%$, increasing model performance on this task even a modest amount can have practical utility for improving the efficiency of discovery systems. As we show in APPENDIX A, for a hypothetical hypothesis generation system where $1 \%$ of the hypotheses are true, the performance of our current-best model could potentially allow discovering $60 \%$ of the true hypotheses while reducing experiment costs by $80 \%$ - a large overall budget reduction, at the cost of reducing the recall of finding true hypotheses by approximately $40 \%$.</p>
<h2>6 Conclusion</h2>
<p>We present MATTER-OF-FACT, a benchmark for assessing the feasibility of 8.4 K scientific claims in four high-impact subdomains of materials science: superconductors, semi-conductors, batteries, and aerospace materials. We frame the feasibility assessment task as a temporally-filtered claim verification task, and empirically demonstrate that strong baseline models using a variety of solving methods (including literature search and code generation) reach only modest performance on this task (72\%). Performance on feasibility assessment can directly translate to improving automated scientific discovery systems, particularly in hypothesis generation, where filtering infeasible hypotheses can make scientific discovery more efficient, and lower overall experiment costs. Ultimate solution methods for the feasibility assessment task are likely to require a combination of reasoning over deep literature search, code-based simulation, and world modeling at the scale of subdomains.</p>
<h2>Limitations</h2>
<p>Temporal Filtering for Prediction: Temporal datasets offer the opportunity to construct prediction datasets for high-impact domains (e.g. Luo et al., 2018, link prediction for cancer biology) where the knowledge a system is predicting is potentially beyond current human knowledge, and for which gold labels are infeasible to construct. Temporal filtering assumes well-controlled models that have not been contaminated with data past their temporal filtering date. In this work we characterize the contamination rate of our base language models, and this analysis suggests that data contamination either does not exist, or is not a significant factor driving current performance. That being said, users of this benchmark should characterize the performance of novel base models to characterize how much data contamination may play a role.
Limits of Code-based Experimentation: Pragmatically, to be useful for filtering scientific hypotheses, feasibility assessment methods must be able to perform well at scale. This necessitates that any pilot experiments (including code-based simulations) must be fast and inexpensive to run, otherwise the feasibility assessment step may be impractically expensive to provide overall cost savings. That being said, different applications and endusers may have varying preferred cost/performance points, and we encourage reporting performance as a function of overall cost (as we have done in this work) to help accurately assess the cost vs benefit of proposed models. It is our hope that providing a large-scale benchmark that necessitates developing inexpensive feasibility assessment methods will help facilitate innovation in this direction.</p>
<h2>Acknowledgments</h2>
<p>This research was developed with funding from the Defense Advanced Research Projects Agency's (DARPA) SciFy program (Agreement No. HR00112520300) to PJ at the University of Arizona. The views expressed are those of the author and do not reflect the official policy or position of the Department of Defense or the U.S. Government. PJ has an outside interest in the Allen Institute for Artificial Intelligence. This interest has been disclosed to the University of Arizona and reviewed in accordance with its conflict of interest policies. We thank the members of the DARPA Scientific Feasibilty (SciFy) program for thoughtful discussions.</p>
<h2>References</h2>
<p>Jinheon Baek, Soyeong Jeong, Minki Kang, Jong Park, and Sung Hwang. 2023. Knowledge-augmented language model verification. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1720-1736, Singapore. Association for Computational Linguistics.</p>
<p>Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. 2021. Machine unlearning. In 2021 IEEE symposium on security and privacy (SP), pages 141-159. IEEE.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and 1 others. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Chi Chen, Dan Thien Nguyen, Shannon J Lee, Nathan Baker, Ajay S Karakoti, Linda Lauw, Craig Owen, Karl T. Mueller, Brian A. Bilodeau, Vijayakumar Murugesan, and Matthias Troyer. 2024. Accelerating computational materials discovery with machine learning and cloud high-performance computing: from large-scale screening to experimental validation. Journal of the American Chemical Society.</p>
<p>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and psychological measurement, 20(1):37-46.</p>
<p>Preetam Prabhu Srikar Dammu, Himanshu Naidu, Mouly Dewan, YoungMin Kim, Tanya Roosta, Aman Chadha, and Chirag Shah. 2024. ClaimVer: Explainable claim-level verification and evidence attribution of text through knowledge graphs. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 13613-13627, Miami, Florida, USA. Association for Computational Linguistics.</p>
<p>Aghyad Deeb and Fabien Roger. 2024. Do unlearning methods remove information from language model weights? arXiv preprint arXiv:2410.08827.</p>
<p>Jiacheng Du, Zhibo Wang, Jie Zhang, Xiaoyi Pang, Jiahui Hu, and Kui Ren. 2024. Textual unlearning gives a false sense of unlearning. arXiv preprint arXiv:2406.13348.</p>
<p>Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, Ankita Naik, Pengshan Cai, and Alfio Gliozzo. 2022. Re2G: Retrieve, rerank, generate. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2701-2715, Seattle, United States. Association for Computational Linguistics.</p>
<p>Ching Nam Hang, Pei-Duo Yu, and Chee Wei Tan. 2024. Trumorgpt: Query optimization and semantic reasoning over networks for automated fact-checking. In 2024 58th Annual Conference on Information Sciences and Systems (CISS), pages 1-6.</p>
<p>Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane A. Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022. Few-shot learning with retrieval augmented language models. J. Mach. Learn. Res., 24:251:1251:43.</p>
<p>Peter Jansen, Oyvind Tafjord, Marissa Radensky, Pao Siangliulue, Tom Hope, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Daniel S Weld, and Peter Clark. 2025. Codescientist: End-to-end semiautomated scientific discovery with code-based experimentation. arXiv preprint arXiv:2503.22708.</p>
<p>John M. Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon A A Kohl, Andy Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, and 15 others. 2021. Highly accurate protein structure prediction with alphafold. Nature, 596:583 - 589.</p>
<p>Rodney Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, and 1 others. 2023. The semantic scholar open data platform. arXiv preprint arXiv:2301.10140.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, and 1 others. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474 .</p>
<p>Ruochen Li, Teerth Patel, Qingyun Wang, and Xinya Du. 2024. Mlr-copilot: Autonomous machine learning research based on large language models agents. arXiv preprint arXiv:2408.14033.</p>
<p>Ryan Liu and Nihar B Shah. 2023. Reviewergpt? an exploratory study on using large language models for paper reviewing. arXiv preprint arXiv:2306.00622.</p>
<p>Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Yuguang Yao, Chris Yuhao Liu, Xiaojun Xu, Hang Li, and 1 others. 2025. Rethinking machine unlearning for large language models. Nature Machine Intelligence, pages $1-14$.</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292.</p>
<p>Xinyuan Lu, Liangming Pan, Qian Liu, Preslav Nakov, and Min-Yen Kan. 2023. SCITAB: A challenging benchmark for compositional reasoning and claim verification on scientific tables. In Proceedings of the 2023 Conference on Empirical Methods in Natural</p>
<p>Language Processing, pages 7787-7813, Singapore. Association for Computational Linguistics.</p>
<p>Fan Luo, Marco A. Valenzuela-Escárcega, Gus HahnPowell, and Mihai Surdeanu. 2018. Scientific discovery as link prediction in influence and citation graphs. In Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-12), pages 1-6, New Orleans, Louisiana, USA. Association for Computational Linguistics.</p>
<p>Aengus Lynch, Phillip Guo, Aidan Ewart, Stephen Casper, and Dylan Hadfield-Menell. 2024. Eight methods to evaluate robust unlearning in llms. arXiv preprint arXiv:2402.16835.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, and 1 others. 2023. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:46534-46594.</p>
<p>Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Bhavana Dalvi Mishra, Abhijeetsingh Meena, Aryan Prakhar, Tirth Vora, Tushar Khot, Ashish Sabharwal, and Peter Clark. 2025. Discoverybench: Towards data-driven discovery with large language models. In The Thirteenth International Conference on Learning Representations.</p>
<p>Mary L McHugh. 2012. Interrater reliability: the kappa statistic. Biochemia medica, 22(3):276-282.</p>
<p>Amil Merchant, Simon Batzner, Samuel S. Schoenholz, Muratahan Aykol, Gowoon Cheon, and Ekin Dogus Cubuk. 2023. Scaling deep learning for materials discovery. Nature, 624(7990):80-85.</p>
<p>Charles O'Neill, Tirthankar Ghosal, Roberta Rāileanu, Mike Walmsley, Thang Bui, Kevin Schawinski, and Ioana Ciucă. 2025. Sparks of science: Hypothesis generation using structured paper data. arXiv preprint arXiv:2504.12976.</p>
<p>Raúl Ortega and José Manuel Gómez-Pérez. 2025. Sciclaims: An end-to-end generative system for biomedical claim analysis. Preprint, arXiv:2503.18526.</p>
<p>Jiefu Ou, William Gantt Walden, Kate Sanders, Zhengping Jiang, Kaiser Sun, Jeffrey Cheng, William Jurayj, Miriam Wanner, Shaobo Liang, Candice Morgan, Seunghoon Han, Weiqi Wang, Chandler May, Hannah Recknor, Daniel Khashabi, and Benjamin Van Durme. 2025. Claimcheck: How grounded are llm critiques of scientific papers? ArXiv, abs/2503.21717.</p>
<p>Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, and Daniel S Weld. 2024. Scideator: Human-llm scientific idea generation grounded in research-paper facet recombination. arXiv preprint arXiv:2409.14634.</p>
<p>Anku Rani, S.M Towhidul Islam Tonmoy, Dwip Dalal, Shreya Gautam, Megha Chakraborty, Aman Chadha, Amit Sheth, and Amitava Das. 2023. FACTIFY-5WQA: 5W aspect-based fact verification through question answering. In <em>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 10421–10440, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Arkadiy Saakyan, Tuhin Chakrabarty, and Smaranda Muresan. 2021. COVID-fact: Fact extraction and verification of real-world claims on COVID-19 pandemic. In <em>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 2116–2129, Online. Association for Computational Linguistics.</p>
<p>Jonathan Schmidt, Noah Hoffmann, Hai-Chen Wang, Pedro Borlido, Pedro J. M. A. Carriço, Tiago F. T. Cerqueira, Silvana Botti, and Miguel A. L. Marques. 2023. Machine-learning-assisted determination of the global zero-temperature phase diagram of materials. <em>Advanced Materials</em>, 35.</p>
<p>Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. 2024. Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. <em>arXiv preprint arXiv:2409.04109</em>.</p>
<p>Ayush K Tarun, Vikram S Chundawat, Murari Mandal, and Mohan Kankanhalli. 2023. Fast yet effective machine unlearning. <em>IEEE Transactions on Neural Networks and Learning Systems</em>.</p>
<p>James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERification. In <em>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</em>, pages 809–819, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020. Fact or fiction: Verifying scientific claims. In <em>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 7534–7550, Online. Association for Computational Linguistics.</p>
<p>David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Iz Beltagy, Lucy Lu Wang, and Hannaneh Hajishirzi. 2022. SciFact-open: Towards open-domain scientific claim verification. In <em>Findings of the Association for Computational Linguistics: EMNLP 2022</em>, pages 4719–4734, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Haoran Wang and Kai Shu. 2023. Explainable claim verification via knowledge-grounded reasoning with large language models. In <em>Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 6288–6304, Singapore. Association for Computational Linguistics.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In <em>Proceedings of the 36th International Conference on Neural Information Processing Systems</em>, NIPS '22, Red Hook, NY, USA. Curran Associates Inc.</p>
<p>Zhiwei Zhang, Jiyi Li, Fumiyo Fukumoto, and Yanming Ye. 2021. Abstract, rationale, stance: A joint model for scientific claim verification. In <em>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 3580–3586, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Ruiyang Zhou, Lu Chen, and Kai Yu. 2024. Is llm a reliable reviewer? a comprehensive evaluation of llm on automatic paper reviewing tasks. In <em>Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</em>, pages 9340–9351.</p>
<h2>A Utility for Hypothesis Filtering</h2>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Relative efficiency (in terms of reduction in the number of experiments needed to be run) for a hypothetical automated scientific discovery (ASD) system that generates hypotheses with a certain true positive rate (<em>y-axis</em>), after those hypotheses have been pre-filtered by a feasibility assessment system such as the models described in this work. This plot assumes a true positive (i.e. <em>feasible</em>) detection rate of 0.60, corresponding to the RAG (SEMANTICSCHULAR, 04-MINI) model in Table 2, while the highlighted region corresponds to that model's infeasible claim detection rate (82%). For a hypothetical ASD system where 1% of the hypotheses it generated were <em>true/feasible</em>, the RAG model would reduce the number of experiments (i.e. cost) by 80%, while still discovering 60% of the true hypotheses.</p>
<p>Feasibility assessment has utility in impactful tasks such as (semi-automated) scientific discovery, particularly in the context of hypothesis generation. Hypothesis generation systems (e.g. Lu et al., 2024; Jansen et al., 2025; O'Neill et al., 2025) have the capacity to generate an impractically large number of possible hypotheses (framed as claims) that</p>
<p>one could test, and as a result running all their proposed experiments is costly (at best) and intractible (at worst). Coupling hypothesis generation with feasibility assessment would allow filtering out hypotheses that are unlikely to be feasible - i.e. yield experimental results that support the hypothesis - and ultimately increase the efficiency of scientific discovery systems in terms of the number of positive discoveries that can be made on a given budget. In automated hypothesis generation where overall likelihood of a hypothesis yielding positive results is low, increasing efficiency is dominated by correctly identifying (and filtering) infeasible hypotheses/claims. Figure 3 shows a plot of experiment efficiency (in terms of the reduction in the number of experiments that would need to be run) for hypothetical hypothesis generation systems that have different rates of generating true hypotheses, with the performance of the best-performing model (RAG (SEMANTIC SCHOLAR) using O4MINI) highlighted. For a hypothetical hypothesis generation system where $1 \%$ of its hypotheses are true, this model would reduce the number of experiments needed to be run by approximately $80 \%$, while still discovering $60 \%$ of the true hypotheses. This highlights that even systems with middle performance can have practical utility (in terms of cost savings) when coupled with scientific discovery systems.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Scientific articles tend to express positive claims rather than negative claims. We follow the approach of Saakyan et al. (2021) to first extract positive claims, then automatically generate negative claims from these positive references.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
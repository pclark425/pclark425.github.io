<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6249 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6249</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6249</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-9b042a8ed355c690eeeeec54413f8c14a3ec844d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9b042a8ed355c690eeeeec54413f8c14a3ec844d" target="_blank">Split and Merge: Aligning Position Biases in Large Language Model based Evaluators</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> P ORTIA is proposed, an alignment-based system designed to mimic human comparison strategies to calibrate position bias in a lightweight yet effective manner, and enables less advanced GPT models to achieve 88% agreement with the state-of-the-art GPT-4 model at just 10% of the cost.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6249.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6249.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PORTIA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PORTIA (Split-and-Merge alignment system for LLM evaluators)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An alignment-based system introduced in this paper that mitigates position bias in pairwise LLM evaluations by splitting answers into segments (sentence-boundary-aware), aligning segments by length and semantic similarity, and merging them into a single prompt for the LLM judge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-ended question answering / pairwise comparison evaluation (MT-Bench style)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Applied across multiple evaluators in this paper: GPT-4, GPT-3.5, Claude2, Qwen, Chatglm2, Llama2</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>PORTIA's effectiveness was validated both automatically (11,520 answer pairs across 3 comparison forms) and with a human study (5 experts; 2 industry developers + 3 academic researchers; each evaluated 80 questions; majority vote used as human reference). Annotators were paid $35/hr.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Consistency rate across answer permutation (consistency), fixed coverage (fraction of inconsistent original assessments corrected), agreement rate with GPT-4 (LLM-to-LLM agreement), human agreement rate (HAR)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>PORTIA produced large improvements in evaluator consistency: average relative improvement in consistency of 47.46% and rectified 62.31% of initially inconsistent cases. It raised GPT-4's consistency up to 98% and enabled GPT-3.5 to reach 88.59% agreement with GPT-4; PORTIA-enhanced GPT-3.5 achieved higher agreement with human annotators than standalone GPT-4 in the human study (GPT-3.5 HAR after PORTIA = 63.75% vs GPT-4 original HAR = 60.00%).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>PORTIA targets position bias only — it cannot fix fundamental capability deficits of an LLM (if the model cannot judge semantically different responses correctly, PORTIA does not address that). Practical limits include maximum context window length of the evaluator (merged prompts may exceed shorter windows) and cases where highly aligned / heavily-aligned prompts cause advanced, well-aligned models to refuse to make a verdict.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>PORTIA fails more frequently on coding questions (highest failure rate among categories: Coding FR = 17.13%), indicating difficulty with tightly structured code content despite using treesitter-aware split rules. PORTIA cannot handle cases where the LLM evaluator refuses to provide a verdict (notably occurs with some aligned GPT-series behaviors and some categories like Roleplay for GPT-3.5).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>The method itself is the mitigation: sentence-boundary splitting, length alignment (equal-character splits), and iterative semantic alignment (token-overlap similarity) merged into a single prompt; recommended default k=3 to balance coverage and efficiency; use treesitter for code to preserve syntax; for context-window concerns, use evaluators with larger context windows (e.g., Claude2) or increase model context; accept that PORTIA is lightweight and model-agnostic and can be integrated into existing pipelines.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6249.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6249.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Study (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human evaluation comparing LLM judges to human annotators (this study)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A controlled human evaluation reported in the paper: five expert annotators judged 80 questions (model pair gpt-3.5-turbo vs claude-v1) to provide a human reference for measuring alignment of LLM evaluators before and after PORTIA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Pairwise comparison of LLM-generated answers (MT-Bench questions across categories)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Evaluated models reported in the human study: GPT-3.5 (gpt-3.5-turbo), Claude-v1, plus summary HAR numbers given for Qwen, Chatglm2, Claude2, GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Five experts (2 industry developers, 3 academic researchers), none of the authors; each given instructions and 80 question pairs; majority vote used when humans disagreed; annotators paid $35/hour.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Human Agreement Rate (HAR) — fraction of LLM judgments that matched the human majority decision, computed before (Ori HAR) and after PORTIA (Fix HAR).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>PORTIA increased HAR for several evaluators: GPT-3.5 Ori HAR 55.00% -> Fix HAR 63.75%; Claude2 Ori HAR 6.25% -> Fix HAR 47.50%; GPT-4 Ori HAR 60.00% -> Fix HAR 65.00%. Qwen showed no change (35.00% both), Chatglm2 slightly increased (16.25% -> 17.50%). PORTIA-enhanced GPT-3.5 exceeded standalone GPT-4's original HAR (63.75% vs 60.00%).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Original LLM judges can be poorly aligned to human judgments (e.g., very low HAR for some models like Claude2 pre-PORTIA); human judgments themselves can be inconsistent (necessitating majority vote). Some LLM evaluators refuse to produce meaningful outputs in certain categories (e.g., GPT-3.5 in Roleplay as noted elsewhere).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Human evaluators sometimes disagreed on ground truth (diverse human perspectives), requiring majority vote; some LLM evaluators remained poorly aligned even after PORTIA (e.g., Qwen HAR unchanged), indicating per-model limits.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Use PORTIA to increase LLM-to-human agreement; use majority-vote human reference where human disagreement exists; select evaluators or comparison forms best suited to the model (paper notes different forms affect consistency).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6249.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6249.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Position Bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Position bias in pairwise LLM-based evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A systematic bias where LLM evaluators prefer either the first or the second answer in a presented pair irrespective of content, causing inconsistencies when answer order is permuted.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Pairwise comparison evaluation (relation-based, score-based, likert-based forms)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Observed across multiple models discussed in paper: GPT-4, GPT-3.5, Claude2, Qwen, Chatglm2, Llama2 (and in prior work cited).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>N/A (this is a model-side bias described and measured via automated permutations and consistency checks on MT-Bench pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Permutation consistency (LLM({q,r1,r2}) == LLM({q,r2,r1})), inconsistency rates, distribution of biased outputs (e.g., likert scores favoring second answer).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Position bias manifests differently by comparison form and model: GPT-4 had notable inconsistencies on likert-based forms (over 78% of its inconsistencies assigned score 5, indicating a bias for the second answer) and PORTIA corrected 36.09% of those; GPT-3.5 was least consistent on score-based form; Claude2 struggled most on relation-based form. Overall, LLM evaluators can achieve >60% agreement with humans in prior work, but still exhibit position bias and inconsistency.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>LLMs are sensitive to prompt changes; behavior can vary across model versions and time; other documented evaluator biases include verbosity bias (preference for longer answers) and self-enhancement bias (preference for self-generated answers), though the paper focuses on position bias.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Likert-based comparison is particularly susceptible: several models show higher inconsistency there. Qwen demonstrated an extreme preference for the second answer (>90% in some cases) so PORTIA had limited effect. Standalone pairwise prompts can produce contradictory judgments when order is reversed.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Primary mitigation proposed and evaluated is PORTIA (split-and-merge with length and semantic alignment). Additional practical recommendations: choose comparison form matched to model capabilities, use stronger models cautiously (GPT-4 is better but costly), and use alignment procedures that mimic human reading strategies (split/align/merge).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6249.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6249.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2 evaluator failures</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instances where Llama2 models produced non-meaningful or refusal outputs when used as evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical observation in this paper that Llama2 (7B/13B chat variants) often fails to produce usable judgments for some comparison forms, returning gibberish, placeholder characters, or prompting the user instead of emitting a verdict.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Using open-source LLMs as automatic evaluators for pairwise comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Llama2-7b-chat, Llama2-13b-chat (examples given in Table 12)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>N/A (this is a model behavior observed in automated evaluation experiments on MT-Bench-derived inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Usability as an evaluator: meaningful output, ability to follow evaluation template; consistency when used as judge was often poor or non-existent in some forms.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Llama2 often failed to provide meaningful outputs for score-based and likert-based forms (examples: output consisting of repeated 'n' characters, or responses like 'Please proceed with your evaluation.'), making it unsuitable as an evaluator in those forms within the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Some open-source models (here Llama2 chat variants) are too conservative, fail to follow the evaluation prompt format, or output uninterpretable tokens; consequently, PORTIA and other alignment preprocessing cannot salvage a useful verdict when the model itself refuses or misbehaves.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Table 12 shows Llama2-7b-chat returning a string of 'n' characters and Llama2-13b-chat returning a prompt asking the user to proceed rather than issuing a verdict; these demonstrate concrete failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Paper notes these failures and explicitly reports that PORTIA cannot address cases where the evaluator refuses or produces non-meaningful outputs; implicit mitigation is to choose alternative evaluator models that reliably follow prompts (e.g., GPT-4) or improve model prompting/fine-tuning, but no concrete rescue method for these Llama2 failures is presented in the paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Large language models are not fair evaluators <em>(Rating: 2)</em></li>
                <li>Evaluating large language models at evaluating instruction following <em>(Rating: 1)</em></li>
                <li>Wider and deeper llm networks are fairer llm evaluators <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6249",
    "paper_id": "paper-9b042a8ed355c690eeeeec54413f8c14a3ec844d",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "PORTIA",
            "name_full": "PORTIA (Split-and-Merge alignment system for LLM evaluators)",
            "brief_description": "An alignment-based system introduced in this paper that mitigates position bias in pairwise LLM evaluations by splitting answers into segments (sentence-boundary-aware), aligning segments by length and semantic similarity, and merging them into a single prompt for the LLM judge.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Open-ended question answering / pairwise comparison evaluation (MT-Bench style)",
            "llm_judge_model": "Applied across multiple evaluators in this paper: GPT-4, GPT-3.5, Claude2, Qwen, Chatglm2, Llama2",
            "human_evaluation_setup": "PORTIA's effectiveness was validated both automatically (11,520 answer pairs across 3 comparison forms) and with a human study (5 experts; 2 industry developers + 3 academic researchers; each evaluated 80 questions; majority vote used as human reference). Annotators were paid $35/hr.",
            "metrics_compared": "Consistency rate across answer permutation (consistency), fixed coverage (fraction of inconsistent original assessments corrected), agreement rate with GPT-4 (LLM-to-LLM agreement), human agreement rate (HAR)",
            "reported_differences": "PORTIA produced large improvements in evaluator consistency: average relative improvement in consistency of 47.46% and rectified 62.31% of initially inconsistent cases. It raised GPT-4's consistency up to 98% and enabled GPT-3.5 to reach 88.59% agreement with GPT-4; PORTIA-enhanced GPT-3.5 achieved higher agreement with human annotators than standalone GPT-4 in the human study (GPT-3.5 HAR after PORTIA = 63.75% vs GPT-4 original HAR = 60.00%).",
            "llm_specific_limitations": "PORTIA targets position bias only — it cannot fix fundamental capability deficits of an LLM (if the model cannot judge semantically different responses correctly, PORTIA does not address that). Practical limits include maximum context window length of the evaluator (merged prompts may exceed shorter windows) and cases where highly aligned / heavily-aligned prompts cause advanced, well-aligned models to refuse to make a verdict.",
            "notable_failure_cases": "PORTIA fails more frequently on coding questions (highest failure rate among categories: Coding FR = 17.13%), indicating difficulty with tightly structured code content despite using treesitter-aware split rules. PORTIA cannot handle cases where the LLM evaluator refuses to provide a verdict (notably occurs with some aligned GPT-series behaviors and some categories like Roleplay for GPT-3.5).",
            "mitigation_strategies": "The method itself is the mitigation: sentence-boundary splitting, length alignment (equal-character splits), and iterative semantic alignment (token-overlap similarity) merged into a single prompt; recommended default k=3 to balance coverage and efficiency; use treesitter for code to preserve syntax; for context-window concerns, use evaluators with larger context windows (e.g., Claude2) or increase model context; accept that PORTIA is lightweight and model-agnostic and can be integrated into existing pipelines.",
            "uuid": "e6249.0"
        },
        {
            "name_short": "Human Study (this paper)",
            "name_full": "Human evaluation comparing LLM judges to human annotators (this study)",
            "brief_description": "A controlled human evaluation reported in the paper: five expert annotators judged 80 questions (model pair gpt-3.5-turbo vs claude-v1) to provide a human reference for measuring alignment of LLM evaluators before and after PORTIA.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Pairwise comparison of LLM-generated answers (MT-Bench questions across categories)",
            "llm_judge_model": "Evaluated models reported in the human study: GPT-3.5 (gpt-3.5-turbo), Claude-v1, plus summary HAR numbers given for Qwen, Chatglm2, Claude2, GPT-4.",
            "human_evaluation_setup": "Five experts (2 industry developers, 3 academic researchers), none of the authors; each given instructions and 80 question pairs; majority vote used when humans disagreed; annotators paid $35/hour.",
            "metrics_compared": "Human Agreement Rate (HAR) — fraction of LLM judgments that matched the human majority decision, computed before (Ori HAR) and after PORTIA (Fix HAR).",
            "reported_differences": "PORTIA increased HAR for several evaluators: GPT-3.5 Ori HAR 55.00% -&gt; Fix HAR 63.75%; Claude2 Ori HAR 6.25% -&gt; Fix HAR 47.50%; GPT-4 Ori HAR 60.00% -&gt; Fix HAR 65.00%. Qwen showed no change (35.00% both), Chatglm2 slightly increased (16.25% -&gt; 17.50%). PORTIA-enhanced GPT-3.5 exceeded standalone GPT-4's original HAR (63.75% vs 60.00%).",
            "llm_specific_limitations": "Original LLM judges can be poorly aligned to human judgments (e.g., very low HAR for some models like Claude2 pre-PORTIA); human judgments themselves can be inconsistent (necessitating majority vote). Some LLM evaluators refuse to produce meaningful outputs in certain categories (e.g., GPT-3.5 in Roleplay as noted elsewhere).",
            "notable_failure_cases": "Human evaluators sometimes disagreed on ground truth (diverse human perspectives), requiring majority vote; some LLM evaluators remained poorly aligned even after PORTIA (e.g., Qwen HAR unchanged), indicating per-model limits.",
            "mitigation_strategies": "Use PORTIA to increase LLM-to-human agreement; use majority-vote human reference where human disagreement exists; select evaluators or comparison forms best suited to the model (paper notes different forms affect consistency).",
            "uuid": "e6249.1"
        },
        {
            "name_short": "Position Bias",
            "name_full": "Position bias in pairwise LLM-based evaluation",
            "brief_description": "A systematic bias where LLM evaluators prefer either the first or the second answer in a presented pair irrespective of content, causing inconsistencies when answer order is permuted.",
            "citation_title": "",
            "mention_or_use": "mention",
            "task_domain": "Pairwise comparison evaluation (relation-based, score-based, likert-based forms)",
            "llm_judge_model": "Observed across multiple models discussed in paper: GPT-4, GPT-3.5, Claude2, Qwen, Chatglm2, Llama2 (and in prior work cited).",
            "human_evaluation_setup": "N/A (this is a model-side bias described and measured via automated permutations and consistency checks on MT-Bench pairs).",
            "metrics_compared": "Permutation consistency (LLM({q,r1,r2}) == LLM({q,r2,r1})), inconsistency rates, distribution of biased outputs (e.g., likert scores favoring second answer).",
            "reported_differences": "Position bias manifests differently by comparison form and model: GPT-4 had notable inconsistencies on likert-based forms (over 78% of its inconsistencies assigned score 5, indicating a bias for the second answer) and PORTIA corrected 36.09% of those; GPT-3.5 was least consistent on score-based form; Claude2 struggled most on relation-based form. Overall, LLM evaluators can achieve &gt;60% agreement with humans in prior work, but still exhibit position bias and inconsistency.",
            "llm_specific_limitations": "LLMs are sensitive to prompt changes; behavior can vary across model versions and time; other documented evaluator biases include verbosity bias (preference for longer answers) and self-enhancement bias (preference for self-generated answers), though the paper focuses on position bias.",
            "notable_failure_cases": "Likert-based comparison is particularly susceptible: several models show higher inconsistency there. Qwen demonstrated an extreme preference for the second answer (&gt;90% in some cases) so PORTIA had limited effect. Standalone pairwise prompts can produce contradictory judgments when order is reversed.",
            "mitigation_strategies": "Primary mitigation proposed and evaluated is PORTIA (split-and-merge with length and semantic alignment). Additional practical recommendations: choose comparison form matched to model capabilities, use stronger models cautiously (GPT-4 is better but costly), and use alignment procedures that mimic human reading strategies (split/align/merge).",
            "uuid": "e6249.2"
        },
        {
            "name_short": "Llama2 evaluator failures",
            "name_full": "Instances where Llama2 models produced non-meaningful or refusal outputs when used as evaluators",
            "brief_description": "Empirical observation in this paper that Llama2 (7B/13B chat variants) often fails to produce usable judgments for some comparison forms, returning gibberish, placeholder characters, or prompting the user instead of emitting a verdict.",
            "citation_title": "",
            "mention_or_use": "use",
            "task_domain": "Using open-source LLMs as automatic evaluators for pairwise comparisons",
            "llm_judge_model": "Llama2-7b-chat, Llama2-13b-chat (examples given in Table 12)",
            "human_evaluation_setup": "N/A (this is a model behavior observed in automated evaluation experiments on MT-Bench-derived inputs).",
            "metrics_compared": "Usability as an evaluator: meaningful output, ability to follow evaluation template; consistency when used as judge was often poor or non-existent in some forms.",
            "reported_differences": "Llama2 often failed to provide meaningful outputs for score-based and likert-based forms (examples: output consisting of repeated 'n' characters, or responses like 'Please proceed with your evaluation.'), making it unsuitable as an evaluator in those forms within the experiments.",
            "llm_specific_limitations": "Some open-source models (here Llama2 chat variants) are too conservative, fail to follow the evaluation prompt format, or output uninterpretable tokens; consequently, PORTIA and other alignment preprocessing cannot salvage a useful verdict when the model itself refuses or misbehaves.",
            "notable_failure_cases": "Table 12 shows Llama2-7b-chat returning a string of 'n' characters and Llama2-13b-chat returning a prompt asking the user to proceed rather than issuing a verdict; these demonstrate concrete failure modes.",
            "mitigation_strategies": "Paper notes these failures and explicitly reports that PORTIA cannot address cases where the evaluator refuses or produces non-meaningful outputs; implicit mitigation is to choose alternative evaluator models that reliably follow prompts (e.g., GPT-4) or improve model prompting/fine-tuning, but no concrete rescue method for these Llama2 failures is presented in the paper.",
            "uuid": "e6249.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2
        },
        {
            "paper_title": "Large language models are not fair evaluators",
            "rating": 2
        },
        {
            "paper_title": "Evaluating large language models at evaluating instruction following",
            "rating": 1
        },
        {
            "paper_title": "Wider and deeper llm networks are fairer llm evaluators",
            "rating": 1
        }
    ],
    "cost": 0.0170015,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Split and Merge: Aligning Position Biases in LLM-based Evaluators</h1>
<p>Zongjie $\mathbf{L i}^{1}$, Chaozheng Wang ${ }^{2}$, Pingchuan $\mathbf{M a}^{1}$, Daoyuan $\mathbf{W u}^{11}$, Shuai Wang ${ }^{11}$, Cuiyun Gao ${ }^{2}$, Yang Liu ${ }^{3}$<br>${ }^{1}$ Hong Kong University of Science and Technology<br>${ }^{2}$ Harbin Institute of Technology, ${ }^{3}$ Nanyang Technological University<br>{zligo, pmaab, daoyuan, shuaiw}@cse.ust.hk, {yangliu}@ntu.edu.sg,<br>wangchaozheng@stu.hit.edu.cn, gaocuiyun@hit.edu.cn</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have shown promise as automated evaluators for assessing the quality of answers generated by AI systems. However, LLM-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate answers in pairwise comparisons, favoring either the first or second answer regardless of content. To address this limitation, we propose PORTIA, an alignmentbased system designed to mimic human comparison strategies to calibrate position bias in a lightweight yet effective manner. Specifically, PORTIA splits the answers into multiple segments, taking into account both length and semantics, and merges them back into a single prompt for evaluation by LLMs. Extensive experiments with six LLMs on 11,520 answer pairs demonstrate that Portia markedly enhances the consistency rates for all models and forms of comparison tested, achieving an average relative improvement of $47.46 \%$. It also enables Portia-enhanced GPT-3.5 to achieve agreement rates with humans comparable to GPT-4 and elevates GPT-4's consistency rate up to $98 \%$. Subsequent human evaluations indicate that the Portia-enhanced GPT-3.5 model can even surpass standalone GPT-4 in terms of alignment with human evaluators, highlighting Portia's ability to correct position bias, improve LLM consistency, and boost performance while keeping cost efficiency.</p>
<h2>1 Introduction</h2>
<p>Recent advances in large language models (LLMs) have achieved remarkable results on various tasks, sometimes even exceeding human performance (Kojima et al., 2022; Thapa et al., 2023). However, assessing the quality of LLM-generated answers poses challenges. Specifically, n-gram matching metrics like BLEU (Papineni et al., 2002)</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>can quantify token-level overlap with reference texts but fall short in evaluating semantic quality. While human evaluators provide more accurate and valuable feedback, often considered the "gold standards," their scalability is generally low, given that they are costly and time-consuming. As a result, there emerges a growing need for automated evaluation methods that reliably align with human yet remain efficient and cost-effective.</p>
<p>Recently, researchers have investigated the use of powerful LLMs like GPT-4 (OpenAI, 2023) to evaluate the quality of text generated in response to open-ended questions (Zheng et al., 2024). Notably, robust LLM evaluators such as GPT-4 have been shown to align remarkably well with both controlled and crowdsourced human preferences, achieving over $60 \%$ agreement (Wang et al., 2023b). These studies suggest that LLMs can emulate human evaluations, offering a scalable and transparent alternative to the expensive and time-intensive human assessment of text quality.</p>
<p>While LLMs have advanced capabilities, they are not flawless evaluators and have been identified to possess certain biases. One notable bias is the position bias (Zheng et al., 2024; Wang et al., 2023b), in which an LLM might prefer either the first or second answer in a pairwise comparison, regardless of its content, as illustrated in Figure 1. Even the state-of-the-art GPT-4 model is not immune to position bias (Zheng et al., 2024; Wang et al., 2023b; Zhang et al., 2023; Zeng et al., 2023), and the behavior of its various versions can be inconsistent over time (Chen et al., 2023). Moreover, owing to pronounced position biases in less-powerful GPT models, much of the prior research (Zheng et al., 2024; Zhang et al., 2023) has been compelled to use the expensive GPT-4 for LLM evaluations, emphasizing the necessity for a more cost-effective approach to large-scale assessments.</p>
<p>To address these limitations, we propose POR-</p>
<p>$\mathrm{TIA}^{1}$, an alignment-based system designed to calibrate position bias. Inspired by human long-text reading strategies (Ratnasari, 2023), PORTIA splits the answers into multiple segments, aligns similar content across candidate answers, and then merges them back into a single prompt to feed to LLM evaluators. Specifically, PORTIA first identifies possible split positions at sentence boundaries within each answer. It then conducts a length alignment between the candidates to generate segments of roughly equal length across answers. If this length alignment does not yield a consistent verdict, PORTIA further undertakes an iterative semantic alignment to identify the optimal split positions, enabling the merging of segments across candidates. Since this lightweight approach does not require changes to the models themselves, PORTIA is readily adaptable to enhance a variety of LLM evaluators for improved evaluation consistency.</p>
<p>We conducted comprehensive experiments using six LLMs as evaluators to assess 11,520 answer pairs across three prevalent pairwise comparison forms. Our results show that PORTIA markedly boosts consistency rates for all the tested models and templates, achieving an average relative improvement of $47.46 \%$ and rectifying an average of $62.31 \%$ of the initially inconsistent cases. Furthermore, PORTIA addresses between $36 \%$ and $86 \%$ (over $80 \%$ for two-thirds of the comparison templates) of the position bias occurrences within the GPT-4 model, elevating its consistency rate up to $98 \%$. Moreover, efficiency and cost evaluations indicate that PORTIA enables the less advanced GPT-3.5 model to achieve $88 \%$ agreement with the state-of-the-art GPT-4 model at merely $9.57 \%$ of the cost. Additionally, a user study involving five human participants demonstrated enhanced agreement between PORTIA-optimized evaluators and human evaluators. Remarkably, the agreement of human evaluators with PORTIA-enhanced GPT-3.5 even exceeds that with the standalone GPT-4. A subsequent ablation study suggests that PORTIA's two key components - length alignment and semantic alignment - are beneficial for improving consistency across different comparison forms.</p>
<h2>2 Background</h2>
<p>Paradigms of Using LLM-based Evaluators. Recent work has explored using LLMs such as GPT-4 to evaluate and compare the performance of AI</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>systems (Wang et al., 2023b; Chan et al., 2023; Zheng et al., 2024; Hada et al., 2023). Conceptually, there are two distinct LLM-based comparison paradigms: standalone comparison and pairwise comparison. In the standalone comparison, LLM evaluators are provided with one answer at a time and are asked to score each answer independently. As a result, position bias is not an issue in standalone LLM evaluation and is therefore beyond the scope of this paper. Nevertheless, we find that the absolute scores of LLM may lack clear interpretation. To demonstrate this, we conducted a preliminary study where we examined the consistency of standalone comparison across a total of 80 test cases, each involving three sets of value ranges. Our findings indicate that the scores from standalone comparison do not strictly adhere to a linear mapping relationship across different scales (more discussion in Appendix D).</p>
<p>Pairwise comparison presents two answers side-by-side and asks evaluators to select the superior one. In particular, pairwise comparison methods can be further categorized into three forms: scorebased, likert-based, and relation-based. In scorebased comparison, evaluators assign a score to each answer and then compare these scores to determine the better answer. The likert-based method (Rajani et al., 2023) requires evaluators to score answers on a likert scale (Likert, 1932), where lower scores indicate a strong preference towards the first answer, middle scores represent a close tie, and higher scores signal a preference for the second answer. Additionally, the relation-based comparison solicits direct inputs from the evaluators about their preference for one answer over another. This approach aims to avoid the use of potentially arbitrary scores, guiding evaluators to make relative comparisons between answers instead. The details of these three forms are shown in Appendix K.1.
Position Bias in Pairwise Comparison. Despite the generally encouraging performance of pairwise comparison methods, we note that LLM evaluators are not perfect and can exhibit certain biases. A primary concern is the position bias (Zheng et al., 2024; Wang et al., 2023b), whereby the LLM may favor the first (or second) answer in a pairwise comparison, regardless of its content. In fact, LLMs have shown notable sensitivity to small changes in prompts (Zhao et al., 2021; Zhu et al., 2023). For clarity, we provide a formal definition of position bias as well as the consistency. As illustrated in Table 13 (refer to Appendix K.1), the evaluation input</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A sample pairwise LLM-based evaluation improved by Portia. <em>Left:</em> The original evaluation exhibiting inconsistency. <em>Right:</em> Consistent evaluation after applying Portia. Details of the answers, comparison forms, and evaluation contents have been simplified or omitted for clarity. An example of the detailed prompt is given in Table 16. It is worth noting that the answers from different assistants may differ significantly from each other.</p>
<p>comprises a fixed template with three placeholders. The input set for the LLM evaluators can be represented as {Q, R1, R2}, where Q denotes the question set, and R1 and R2 are the two sets of answers for comparison. The LLM evaluators produce the verdict V = LLM({Q, R1, R2}), which indicates the preferred answer out of the two candidates. Assuming that the LLM evaluators are flawless, the verdict V should be independent of the permutation Π of R1 and R2. Thus, position bias can be expressed as: Π ⊥ V. On an individual sample level, for a specific question q ∈ Q and answers r1, r2, consistency is achieved if the verdict v remains the same when the positions of r1, r2 are switched: LLM({q, r1, r2}) = LLM({q, r2, r1}).</p>
<h1>3 The Portia System</h1>
<p><strong>Design Intuition.</strong> It is worth noting that both human evaluators and LLMs encounter difficulties in making consistent evaluations when faced with lengthy and intricate answers (Kintsch and Keenan, 1973; Wijesiriwardene et al., 2023). A common cognitive approach among individuals is to decompose information into smaller units, thereby simplifying the comparison process (Ratnasari, 2023). Inspired by this observation, Portia is designed to split candidate answers into segments, merge specific segments across candidates that share "comparable" content, and eventually align them. Based on this intuition, Portia seeks to mimic effective human comparison procedures, aiming to calibrate position bias and enhance the consistency of LLM evaluators. That said, for each question, the verdicts of Portia should be consistent with any permutation of the answers, i.e., -</p>
<p>$$LLM({q, r_1, r_2}) = LLM({q, r_2, r_1}).$$</p>
<p>Moreover, Portia is specifically designed to address the position bias issue, rather than aligning the reasoning capabilities of LLM-based evaluators. Therefore, if an LLM evaluator lacks the capability to adequately judge a pair of semantically different responses, this falls outside the scope of Portia's intended purpose. Portia is solely focused on rectifying inconsistencies that arise when the order of a pair of semantically different responses is altered, leading to a change in the judgment of the LLM evaluator. In this context, Portia is generalizable to handling open-ended answers because, regardless of their semantic differences, we can always mix the two previously individual answers. Such a mixing operation effectively eliminates the position bias in pairwise LLM-based evaluation.</p>
<h3>3.1 Key Design Considerations</h3>
<p>Before presenting the technical details of Portia, we first introduce its key design considerations.</p>
<p><strong>Content Preservation.</strong> Content preservation refers to ensuring the segmented answers encompass the entirety of the information present in the original answer, without any omissions or additions of new content. For a given original answer r1, the set of split answer segments {r1, r2, ..., rk} should fully encompass the content of r1. This implies that when the segments are concatenated, the entirety of the original content is preserved (∑i=1k r1i = r1). This consideration helps to preserve the meaning and information of the original answer during the process of splitting. The preservation of content is critical for evaluators to assess the same substantive answer content that is divided into segments,</p>
<p>without any alterations or incomplete information. Order Preservation. Order preservation refers to preserving the original sequence of the information presented in the answer. This is important for fair evaluation, as re-ordering or re-arranging the content may impact the assessment of answer quality. By preserving the order, we ensure the segmentation process does not introduce artifacts that could unintentionally alter assessment. This enables the LLM evaluators to accurately evaluate answers in comparison to the original. Notably, considering both the content and order of the answer helps maintain long-range dependencies by preserving all original information and the sequence in which it is presented.
Resource efficiency. Resource efficiency refers to minimizing computational costs incurred by the splitting process, beyond the standard cost when querying the LLM evaluator. To this end, it is important for the segmentation process to introduce a minimal number of extra tokens and to be executed rapidly, thus avoiding significant overhead.</p>
<h3>3.2 The Core Splitting Algorithm</h3>
<p>Due to the page limit, we direct interested readers to Appendix E for a comprehensive overview of utilizing Portia for LLM evaluation. Here we concentrate on Portia's core splitting algorithm, as illustrated in Algorithm 1. Intuitively, Portia first identifies semantically or syntactically similar segments across answers. It then aligns these answer segments and merges them sequentially into a single prompt for the LLM evaluators to make a final verdict. Specifically, the inputs include the question $q$, two candidate answers $r_{1}$ and $r_{2}$, the LLM evaluator's verdict function $v()$, and the specified number of splits $k$. The output of Algorithm 1 is a consistent verdict $v \in(1,2,3)$, where 1 indicates that $r_{1}$ is superior, 2 suggests that $r_{2}$ is better, and 3 represents a tie.</p>
<p>Overall, the splitting process can be divided into three stages. In the first phase, possible split positions are determined at the boundaries of sentences (line 1-2). Segmenting at sentence breaks (e.g., periods or question marks) reduces the likelihood of producing incomplete words or fragmented syntactic units in different segments. This particular design decision aids in maintaining semantic consistency and enhancing readability in each segment. Notably, natural language and programming language have different definitions for sentence boundaries; for instance, the period sign "." in Python
denotes accessing a specific object member property. Therefore, in instances where answers involve code blocks, we follow (Li et al., 2023c; Wang et al., 2023a) and leverage treesitter (tre) to parse code blocks and locate suitable split positions that preserve the code's structure and execution sequence.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="err">:</span><span class="w"> </span><span class="n">Alignment</span><span class="o">-</span><span class="n">based</span><span class="w"> </span><span class="n">Splitting</span>
<span class="w">    </span><span class="k">Input</span><span class="err">:</span><span class="w"> </span><span class="nl">Question</span><span class="p">:</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">q</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nl">Answers</span><span class="p">:</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">r_</span><span class="err">{</span><span class="mi">1</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">r_</span><span class="err">{</span><span class="mi">2</span><span class="err">}\</span><span class="p">),</span><span class="w"> </span><span class="n">Evaluator</span><span class="s1">&#39;s</span>
<span class="s1">        verdict \(v()\), Split number \(k\)</span>
<span class="s1">    Output: Consistent evaluation \(v \in(1,2,3)\)</span>
<span class="s1">    /* Step1: identify answers&#39;</span><span class="w"> </span><span class="n">formats</span><span class="w"> </span><span class="k">with</span>
<span class="w">        </span><span class="n">split</span><span class="w"> </span><span class="n">positions</span><span class="p">.</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="n">r_</span><span class="err">{</span><span class="mi">1</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">positions</span><span class="w"> </span><span class="err">}}</span><span class="o">=</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="nf">format</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">r_</span><span class="err">{</span><span class="mi">1</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="n">r_</span><span class="err">{</span><span class="mi">2</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">positions</span><span class="w"> </span><span class="err">}}</span><span class="o">=</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="nf">format</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">r_</span><span class="err">{</span><span class="mi">2</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="cm">/* Step2: length alignment.</span>
<span class="cm">    \(\left[r_{1}^{(1)}, \ldots r_{1}^{(k)}\right]=\operatorname{equalsplit}\left(r_{1}^{\text {positions }}, k\right)\)</span>
<span class="cm">    \(\left[r_{2}^{(1)}, \ldots r_{2}^{(k)}\right]=\operatorname{equalsplit}\left(r_{2}^{\text {positions }}, k\right)\)</span>
<span class="cm">    if \(v\left(q_{i}, r_{1}^{(1)}, r_{2}^{(1)}, \ldots, r_{1}^{(k)}, r_{2}^{(k)}\right)==\)</span>
<span class="cm">        \(v\left(q_{i}, r_{2}^{(1)}, r_{1}^{(1)}, \ldots, r_{2}^{(k)}, r_{1}^{(k)}\right)\) then</span>
<span class="cm">            return \(v / *\) Consistent, return answer */</span>
<span class="w">    </span><span class="k">end</span>
<span class="w">    </span><span class="cm">/* Step3: semantic alignment.</span>
<span class="cm">    else</span>
<span class="cm">        \(s_{\max }=0, n_{s}=0\), Search_all \(=\)</span>
<span class="cm">            False, \(r_{1}^{\text {bestparts }}=\llbracket, r_{2}^{\text {bestparts }}=\rrbracket\)</span>
<span class="cm">        while not Search_all do</span>
<span class="cm">            \(r_{1}^{\text {parts }}=\) partition \(\left(r_{1}^{\text {positions }}, k, n_{s}\right)\)</span>
<span class="cm">            \(r_{2}^{\text {parts }}=\) partition \(\left(r_{2}^{\text {positions }}, k, n_{s}\right)\)</span>
<span class="cm">            \(n_{s}+=1\)</span>
<span class="cm">            \(s_{\text {cum }}=\)</span>
<span class="cm">            \(\sum_{i=1}^{k} \operatorname{similarity}\left(r_{1}^{\text {parts }}[i], r_{2}^{\text {parts }}[i]\right)\)</span>
<span class="cm">            /* Update max similarity score,</span>
<span class="cm">                keep best split positions.</span>
<span class="cm">            if \(s_{\text {cum }}&gt;s_{\text {max }}\) then</span>
<span class="cm">                \(s_{\max }=s_{\text {cum }}, r_{1}^{\text {bestparts }}=\)</span>
<span class="cm">                \(r_{1}^{\text {parts }}, r_{2}^{\text {bestparts }}=r_{2}^{\text {parts }}\)</span>
<span class="cm">            end</span>
<span class="cm">        end</span>
<span class="cm">        if \(v\left(q_{i}, r_{1}^{(1)}, r_{2}^{(1)}, \ldots, r_{1}^{(k)}, r_{2}^{(k)}\right)==\)</span>
<span class="cm">            \(v\left(q_{i}, r_{2}^{(1)}, r_{1}^{(1)}, \ldots, r_{2}^{(k)}, r_{1}^{(k)}\right)\) then</span>
<span class="cm">            return \(v\)</span>
<span class="cm">        end</span>
<span class="cm">    end</span>
<span class="cm">    return None</span>
</code></pre></div>

<p>The second stage performs length alignment, splitting each answer into $k$ segments of comparable length (line 3-4); if an answer is too short to split, Portia would give up splitting and directly ask for the judge. Specifically, we first find the $k-1$ points that divide the answer into $k$ equal segments according to the number of characters. Subsequently, we select the split location that is closest to each of the split positions obtained in the first stage, and designate them as $\left[r_{1}^{(1)}, \ldots r_{1}^{(k)}\right] .{ }^{2}$ The $k$</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>corresponding answer segments are subsequently merged again and used for evaluation by the LLM evaluator. If the LLM evaluator consistently returns the same verdicts for all length-aligned splits, then the verdict is returned (lines 5-7).</p>
<p>If inconsistent assessments persist after length alignment, PORTIA proceeds to semantic alignment as the third stage (lines 8-17). Specifically, given a fixed $k$ and a set of possible split positions, we aim to iteratively search for the optimal split positions that maximize the cumulative semantic similarity between corresponding segments of the two answers. Note that $n_{s}$ represents the index number of the current segmentation, and Search_all becomes True when $n_{s}$ reaches the maximum number of possible split combinations Cal. Semantic similarity between segments $r_{1}^{t}$ and $r_{2}^{t}$ is computed by token overlap: $\operatorname{sim} _$score $=\frac{\text { Intersection }\left(\operatorname{set}\left(r_{1}^{t}\right), \operatorname{set}\left(r_{2}^{t}\right)\right)}{\max \left(\operatorname{len}\left(\operatorname{set}\left(r_{1}^{t}\right)\right), \operatorname{len}\left(\operatorname{set}\left(r_{2}^{t}\right)\right)\right)}$. Notably, the choice of value $k$ as well as the similarity metric would have an impact on the efficiency of PORTIA, and we provide the theoretical analysis in Section 4.3. We also consider applying other similarity metrics, such as LM-based metrics (Reimers and Gurevych, 2019). However, we argue that employing such intricate metrics is not necessary for PORTIA, as they usually entail extra computing resources, and introduce more hyper-parameters while yielding only marginal improvements in performance; see further discussion in Appendix I. Finally, PortIA would yield consistent verdict if applicable (lines 19-22). Note that the above three stages are carried out in a sequential manner, whereas semantic alignment is only performed when length alignment is inadequate for ensuring consistent assessments. This sequential approach prioritizes computational efficiency, as length alignment is typically faster to execute than semantic alignment.</p>
<h2>4 Experiments</h2>
<h3>4.1 Experimental Setup</h3>
<p>Datasets. We evaluate Portia using the MTBench benchmark (Zheng et al., 2024), following the experimental setup in (Wang et al., 2023b). MTBench contains 80 elaborated open-ended questions spanning 8 categories (Writing, Roleplay, Reasoning, Math, Coding, Extraction, STEM, and Humanities). For each question, MT-Bench pro-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>vides several candidate answers from different LLMs. We consider eight different combinations of LLM answers (see more details in Appendix G), and we consider all three comparison forms (scorebased, likert-based, and relation-based) in the pairwise comparison paradigm. Thus, we have $80 * 8 * 3=1920$ inputs to evaluate each LLM evaluator. We use this diverse dataset to provide a comprehensive evaluation of PORTIA across several representative LLMs and comparison forms. Additionally, an extended evaluation on a larger set of open-ended questions can be found in Appendix L.
Models. In this work, we include both locally deployable models that are open-source and proprietary models that are accessed through only cloud APIs as LLM evaluators. Details on the specific LLM versions evaluated are given in Appendix G.
Response Length and generalizability. In Figure 1, two assistants provide responses of similar length. However, given the open-ended nature of the questions in MT-Bench, different LLMs may produce responses that differ substantially in both length and content for the same question. To assess Portia's adaptability to open-ended questions, we analyzed the statistics of all responses, presented in Table 4. Our findings indicate that the lengths of responses from the LLMs vary considerably, underscoring Portia's flexibility in handling open-ended questions. Additionally, we explore Portia's generalizability by examining the relationship between answer length and inconsistency (Appendix B.2) and evaluate its performance on extremely short responses (Appendix B.3). More details are provided in Appendix B.</p>
<h3>4.2 Main Results</h3>
<p>As shown in Table 1, Portia improves the consistent rate among all evaluators. The values depicted in the table correspond to the mean values obtained from the analysis of all eight combinations of tested models. We observe that Portia relatively improves the consistent rate by $3.25 \%$ to $194.48 \%$, depending on the evaluator, with the highest fixed coverage at $96.32 \%$ (meaning that nearly all the inconsistent results are resolved). GPT-4 exhibits the highest average consistency rate, which is in line with the findings of previous work (Wang et al., 2023b), and Portia further boosts its consistency up to $98 \%$. Moreover, we observe that GPT-4 exhibits subpar performance on the likertbased form, not just compared to its performance</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Evaluators</th>
<th style="text-align: center;">De. Method</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Relation-based</th>
<th style="text-align: center;">Score-based</th>
<th style="text-align: center;">Likert-based</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Claude2</td>
<td style="text-align: center;">API</td>
<td style="text-align: center;">\% Origin Con <br> \% Portia Con <br> \% Fixed Coverage</td>
<td style="text-align: center;">$\begin{gathered} 28.28 \ \mathbf{8 3 . 2 8}(+\mathbf{1 9 4 . 4 8 \%}) \ 79.44 \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 47.34 \ \mathbf{6 5 . 1 6}(+\mathbf{3 7 . 6 4 \%}) \ 52.22 \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 50.62 \ \mathbf{9 4 . 8 4}(+\mathbf{8 7 . 3 6 \%}) \ 91.27 \end{gathered}$</td>
</tr>
<tr>
<td style="text-align: center;">Qwen</td>
<td style="text-align: center;">API</td>
<td style="text-align: center;">\% Origin Con <br> \% Portia Con <br> \% Fixed Coverage</td>
<td style="text-align: center;">$\begin{gathered} 63.12 \ \mathbf{7 8 . 1 3}(+\mathbf{2 3 . 7 8 \%}) \ 65.66 \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 52.66 \ \mathbf{7 1 . 0 9}(+\mathbf{3 5 . 0 \%}) \ 59.78 \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 8.12 \ \mathbf{9 . 3 8}(+\mathbf{1 5 . 5 2 \%}) \ 6.46 \end{gathered}$</td>
</tr>
<tr>
<td style="text-align: center;">Chatglm2</td>
<td style="text-align: center;">Local</td>
<td style="text-align: center;">\% Origin Con <br> \% Portia Con <br> \% Fixed Coverage</td>
<td style="text-align: center;">$\begin{gathered} 38.44 \ \mathbf{6 1 . 7 2}(+\mathbf{6 0 . 5 6 \%}) \ 56.09 \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 58.59 \ \mathbf{7 4 . 0 6}(+\mathbf{2 6 . 4 \%}) \ 51.02 \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 26.72 \ \mathbf{6 4 . 2 2}(+\mathbf{1 4 0 . 3 4 \%}) \ 60.30 \end{gathered}$</td>
</tr>
<tr>
<td style="text-align: center;">Llama2</td>
<td style="text-align: center;">Local</td>
<td style="text-align: center;">\% Origin Con <br> \% Portia Con <br> \% Fixed Coverage</td>
<td style="text-align: center;">$\begin{gathered} 36.41 \ \mathbf{6 8 . 7 5}(+\mathbf{8 8 . 8 2 \%}) \ 22.51 \end{gathered}$</td>
<td style="text-align: center;">N/A N/A N/A</td>
<td style="text-align: center;">N/A N/A N/A</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">API</td>
<td style="text-align: center;">\% Origin Con <br> \% Portia Con <br> \% Fixed Coverage</td>
<td style="text-align: center;">$\begin{gathered} 78.12 \ \mathbf{8 8 . 5 9}(+\mathbf{1 3 . 4 \%}) \ 70.63 \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 39.22 \ \mathbf{5 4 . 8 4}(+\mathbf{3 9 . 8 3 \%}) \ 42.06 \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 78.91 \ \mathbf{9 8 . 6 0}(+\mathbf{2 4 . 9 4 \%}) \ 96.32 \end{gathered}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">API</td>
<td style="text-align: center;">\% Origin Con <br> \% Portia Con <br> \% Fixed Coverage</td>
<td style="text-align: center;">$\begin{gathered} 93.44 \ \mathbf{9 7 . 0 3}(+\mathbf{3 . 8 4 \%}) \ 80.99 \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 92.75 \ \mathbf{9 8 . 0 0}(+\mathbf{5 . 6 6 \%}) \ 86.33 \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 61.50 \ \mathbf{6 3 . 5 0}(+\mathbf{3 . 2 5 \%}) \ 36.09 \end{gathered}$</td>
</tr>
</tbody>
</table>
<p>Table 1: The main results of Portia across LLM evaluators. All metrics presented are higher-is-better values. "\% Origin Con" and "\% Portia Con" are the percentages of consistent results in the original setting when enhanced by Portia, respectively. " $\%$ Fixed Coverage" denotes the percentage of inconsistent original assessments that are later corrected by Portia. "De Method" specifies whether the LLM evaluator uses local or cloud API deployment.
on other forms, but also when compared to GPT-3.5. Upon analyzing results on likert-based forms, over $78 \%$ of GPT-4's inconsistency provides a score of 5 , reflecting its bias for the second answer, and our method rectifies $36.09 \%$ of them. Notably, we only report the results of Llama2 in relation-based form, as it fails to provide meaningful evaluations in other forms (see more details in Appendix J).</p>
<p>The impact of the comparison form on consistency rates is also observed, with evaluators displaying various preferences. For instance, it is seen that GPT-3.5 exhibits the least consistent performance when evaluated on the score-based form, whereas Claude2 struggles most on the relationbased form. GPT-4, Qwen, and Chatglm2 exhibit the highest degree of inconsistency when assessed on the likert-based form. This suggests that appropriately matching comparison forms to evaluators' capabilities is important. Nevertheless, Portia offers high enhancement for forms and LLM evaluators.
Per-category Breakdown Analysis. We recognize that our method may not perfectly resolve all inconsistencies, and there might be instances where it fails. To better understand Portia's performance across different categories, we conduct a statistical analysis of its failure rates across 8 categories, as shown in Table 2.</p>
<p>We observe that Portia tends to fail more fre-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">FR</th>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">FR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Coding</td>
<td style="text-align: center;">$17.13 \%$</td>
<td style="text-align: center;">Common-sense</td>
<td style="text-align: center;">$12.15 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Fermi</td>
<td style="text-align: center;">$14.92 \%$</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">$11.60 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Counterfactual</td>
<td style="text-align: center;">$13.81 \%$</td>
<td style="text-align: center;">Knowledge</td>
<td style="text-align: center;">$9.94 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Generic</td>
<td style="text-align: center;">$12.15 \%$</td>
<td style="text-align: center;">Roleplay</td>
<td style="text-align: center;">$8.29 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: Failure Rates (FR) of Portia across different categories.
quently on coding-related questions. This may be attributed to the tighter contextual relationships in code due to programming language constraints and fixed syntactic structures (e.g., for loops). Although we have considered parsing code and designing separate split positions during the split stage (as shown in Section 3.2), these results indicate room for improvement in handling structured data.</p>
<p>To further investigate the generalizability of Portia, we conduct additional experiments on more open-ended questions, and the results are provided in Appendix L. Additionally, we compare Portia with more baselines, where results are shown in Appendix O.</p>
<h3>4.3 Efficiency and Cost Analysis</h3>
<p>To show the efficiency and cost-effectiveness of Portia, this section first performs a theoretical analysis of Portia's efficiency, and then evaluates its actual costs in terms of temporal, monetary, and</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />
(a) Average input token length with different $k$. "ori" represents the original input length. "min" and "max" represent the minimum and maximum input lengths, respectively.
<img alt="img-2.jpeg" src="img-2.jpeg" />
(b) Computation operations with different $k$. The number behind "Avg" is the average potential split positions for each answer. The y-axis has been logarized to ease reading.</p>
<p>Figure 2: Theoretical estimation of PORTIA's cost with varying $k$ based on input length and computation operations.
environmental factors. Specifically, we measure the efficacy of PORTIA-enhanced LLMs in terms of their agreement rate with GPT-4.
Theoretical analysis. We first theoretically analyze PortIA's computational efficiency. As the number of answer segments $k$ increases, the average input length for LLM evaluators also grows correspondingly. In line with line 1 in Algorithm 1, the added tokens stem from two sources: fixed-length system prompts based on comparison forms, and split boundary prompts (an example shown in Table K.2) that scale linearly with $k$. Consequently, the additional input length scales as $O(K)$, as depicted in Figure 2(a). Notably, the average input length of the relation-based form exceeds the other two, as it requires more tokens for the system prompt. More details are in Appendix K.</p>
<p>In accordance with line 9 in Algorithm 1, the total number of computation operations Cal is calculated as: $C a l=C_{p_{1}}^{k-1} * C_{p_{2}}^{k-1}$, where $p_{1}$ and $p_{2}$ are the potential split positions in the two answers. $C_{p_{1}}^{k-1}$ and $C_{p_{2}}^{k-1}$ are the combination counts for the first and second answers, respectively. Using average position numbers of $10,20,30$, and 40 , we derive the total calculations as depicted in Figure 2(b). Intuitively, raising the value of $k$ can improve the algorithm's performance by exploring more split position combinations. However, this also results in an exponential surge in the total computation operations, compromising efficiency. As such, we conducted controlled experiments to identify the optimal value of $k$, and in our case, we found that setting $k=3$ strikes a balance between efficiency and precision. Full details about this controlled experiment can be found in Appendix I.
Real-World Performance and Cost Analysis. Next, we measure the level of agreement be-
tween the PORTIA-enhanced LLM evaluators and GPT-4 (considered as the "reference standard."). Note that to offer a fair evaluation, we exclusively consider GPT-4 evaluation outputs that are originally consistent. In the context of a question with two possible answers, it is deemed as an agreement only when both GPT-4 and PORTIA-enhanced assessments are consistent and identical. As evidenced in Table 9, agreement rates are enhanced by an average of $16.32 \%$ after alignment. Claude2 has the highest gain at $31.65 \%$, while GPT-3.5 achieves the highest agreement rate with GPT-4 at $88.59 \%$.</p>
<p>Additionally, we consider the resource usage in terms of temporal, monetary, and environmental factors. As shown in Table 9, Chatglm2 exhibits the lowest inferencing time. However, the cost of GPT-3.5 is lower than that of Chatglm2, while its carbon emission is higher, which is mainly because the cloud API models usually run on GPU clusters with more powerful GPUs. It is worth mentioning that GPT-3.5 incurs less than $\mathbf{1 0 \%}$ of the average cost of GPT-4, while maintaining an approximate agreement level of $88 \%$ with GPT-4. In brief, the usage of PORTIA results in a substantial level of concurrence with GPT-4 while maintaining a minimal computational burden, hence showcasing a proficient and eco-friendly alignment. The significant enhancements in performance and resource utilization underscore the usefulness of this approach in boosting various LLMs for crucial evaluation work.</p>
<h3>4.4 Human study</h3>
<p>We conducted a human evaluation to further assess the performance of PORTIA. The model pair "gpt-3.5-turbo" v.s. "Claude-v1" is selected to compare human agreement rates on original versus Por-TIA-enhanced assessments across 80 questions, as</p>
<p>these two models have similar performance <em>Zheng et al. (2024)</em>, making it challenging for LLM evaluators to make decisions. We recruit five experts: two industrial developers and three academic researchers, none of whom are authors of this paper to avoid potential bias due to prior exposure to the MT-BENCH dataset. For each participant, we create an online questionnaire that provides one question with two answers, not specifying their origin. Before the questionnaire, brief instructions on the task and evaluation criteria are provided. (More details are in Appendix N.) During the human evaluation process, we observe instances where human evaluators have differing assessments. This aligns with previous research highlighting the diversity of human perspectives <em>Peng et al. (1997)</em>. In such cases, we employ a majority vote to determine the final result, and we aim to use PORTIA to help LLM judges closely align with representative human evaluations.</p>
<table>
<thead>
<tr>
<th></th>
<th>Ori HAR (%)</th>
<th>Fix HAR (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3.5</td>
<td>55.00</td>
<td>63.75</td>
</tr>
<tr>
<td>Qwen</td>
<td>35.00</td>
<td>35.00</td>
</tr>
<tr>
<td>Chatglm2</td>
<td>16.25</td>
<td>17.50</td>
</tr>
<tr>
<td>Claude2</td>
<td>6.25</td>
<td>47.50</td>
</tr>
<tr>
<td>GPT-4</td>
<td>60.00</td>
<td>65.00</td>
</tr>
</tbody>
</table>
<p>Table 3: Main results from human evaluation comparing the model pair “gpt-3.5-turbo” v.s. “Claude-v1” on 80 questions. “HAR” represents the human agreement rate.</p>
<p>The human evaluation results presented in Table 3 demonstrate increased agreement rates between humans and LLM evaluators after applying PORTIA. For example, the human agreement rate with GPT-3.5 increases from 55.00% on the original assessments to 63.75% after applying PORTIA enhancements, which surpasses the original human agreement rate with GPT-4. In addition, the original human agreement rate for Claude2 is only 6.25%, but increases substantially to 47.50% after enhancement. Taken together, these quantitative findings provide evidence that PORTIA effectively augments the assessments of all LLM evaluators to achieve greater concordance with human evaluators. The framework also enables weaker LLMs to reach comparability with stronger counterparts in terms of human alignment.</p>
<p>It is crucial to recognize that while LLM judges offer significant benefits in terms of cost and efficiency, they do not entirely replace the need for human expertise. By thoroughly investigating the potential biases present in LLM judges and understanding their limitations across different categories, we can develop more effective and comprehensive methods for evaluating AI systems. This improvement not only enhances the quality and efficiency of assessments but also brings substantial benefits to the entire AI community.</p>
<h3>4.5 Ablation Study</h3>
<p>To ascertain the individual contributions of each component in PORTIA, we conduct ablation experiments on five distinct LLM evaluators. The results are quantified in terms of the “Fixed Coverage” metric, as depicted in Figure 3. To facilitate visual interpretation, variants of PORTIA incorporating or excluding specific components are denoted by different colored bars in the histogram. Additionally, texture patterns in the bars indicate the comparison form used. The plain blue bar represents the score-based form, while the blue bar with slash lines corresponds to the relation-based form.</p>
<p>The results reveal that both semantic and length alignment improve PORTIA’s performance. Specifically, across all evaluators, semantic alignment shows a greater contribution to enhancing the likert-based form, possibly because the likert scale has a greater dependence on precise semantic meaning for its standardized categorical ratings. For the other two forms, both alignment methods have similar contributions with slight differences between LLMs. This is likely because these forms consider semantic meaning and answer length in a balanced way, similar to how human evaluators would <em>Ratnasari (2023)</em>.</p>
<p>Furthermore, we find that the trends of fixed coverage rate are consistent across comparison forms for PORTIA and ablations (without semantic or length alignment). Likert-based form has the highest fixed coverage rates, followed by relation-based, with score-based form having the lowest rates. The exceptions are Qwen on likert-based form and GPT-4 on all forms, where we manually check and find that: (1) Qwen prefers the second answer for over 90% of examples, no matter whether PORTIA is used. (2) GPT-4 has the highest fixed coverage rate on relation-based form, which is probably because GPT-4 performs consistently enough (more than 97% consistent rate), and therefore, the improvement on likert-based form is not obvious. Overall, aside from the outliers, these results suggest that likert-based form is the most challenging, and we attribute this to that it requires the evaluators to assign a single score that contains an assessment of</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: Fixed coverage rate across LLMs for PORTIA and variants w/o Semantic (SA) or Length Alignment (LA).</p>
<p>two answers, which is more difficult than simply choosing the better one like relation-based form.</p>
<h2>5 Related Work</h2>
<p><strong>Automatic Evaluation of AI Outputs.</strong> Automated evaluation metrics like BLEU [papineni2002bleu] and ROUGE [lin2004rouge] are widely used to assess the quality of AI-generated text. However, these metrics have limited ability to assess meaning, reasoning, and logical consistency. Recent efforts have focused on developing more robust semantic similarity metrics using neural representations [zhang2019automatic], but they are still imperfect proxies for human assessment. To address this, LLM has emerged as a promising alternative for evaluation [chiang2023automatic; liu2023automatic; tse2023automatic; jiao2023automatic; lin2023automatic; wang2023automatic].</p>
<p><strong>Biases in LLM Evaluators.</strong> Besides position bias, Zheng et al. [zheng2023biases] identify two additional biases: verbosity bias, which refers to a preference for longer answers, and self-enhancement bias, which involves a preference for self-generated answers. However, the definition of verbosity bias remains ambiguous, and in line with previous research [wang2023revealing], we observe that human evaluators also tend to prefer longer answers. Furthermore, self-enhancement bias is not universal for all LLMs [zheng2023biases]. Therefore, we focus on position bias, as its mitigation can directly improve the efficiency and accuracy of various LLM evaluators already in real-world use [li2023automatic].</p>
<h2>6 Conclusion</h2>
<p>This paper presented PORTIA, an alignment-based technique to address position bias for LLM evaluators. By aligning similar content segments across candidate answers, PORTIA effectively reduced position bias. It not only enabled replacing costly models like GPT-4 with affordable alternatives but also elevated the consistency rate of open-source models like Llama2.</p>
<h2>7 Acknowledgements</h2>
<p>The HKUST authors are supported in part by a RGC GRF grant under the contract 16214723, RGC CRF grant under the contract C6015-23G, research fund provided by HSBC, and a Webank research fund WEB24EG01. The HITSZ authors are supported in part by National Natural Science Foundation of China under project (No. 62472126), Natural Science Foundation of Guangdong Province (Project No. 2023A1515011959), Shenzhen-Hong Kong Jointly Funded Project (Category A, No. SGDX20230116091246007), and Shenzhen Basic Research (General Project No. JCYJ20220531095214031). We are grateful to the anonymous reviewers for their valuable comments.</p>
<h2>8 Ethical Considerations</h2>
<p><strong>Use of Human Annotations</strong> We protect the privacy rights of workers and pay them above the local minimum wage. All five annotators are paid 35$ per hour for the given 80 samples. Careful instruction is given to ensure that the annotators understand the task and are not exposed to harmful content.</p>
<p><strong>Study Scope.</strong> Our work aims to improve the consistency of LLM-based evaluators, which can be utilized to assess the quality of AI-generated answers. More consistent LLM-based evaluators can provide human-like evaluations at a lower cost, supplying feedback to reduce biases during training. Notably, our work enhances the evaluation consistency of open-source models like Llama2, enabling their use as reliable evaluators and making</p>
<p>research on LLMs more accessible to the broader community. However, we recognize that malicious actors could exploit these methods to intentionally train models that go against human values. The open-source LLMs could be leveraged as consistent evaluators to guide the training of harmful models such as Worm-GPT (wor). While our work targets constructive applications, we caution that like any technology, consistent LLM evaluators could potentially be misused. Researchers should consider ethical implications and preventative measures. Overall, our current focus is on addressing the position bias of LLM evaluators, thereby making them more consistent and reliable in supporting large-scale automatic evaluation processes.</p>
<h2>9 Limitations</h2>
<p>Context Window Length. Although we do not identify any new biases introduced by Portia the maximum context window length of the LLM evaluator poses a challenge, as it requires the LLM to process the entire input prompt, which comprises the original question and two candidate responses. If these responses are excessively lengthy, the merged prompt may exceed the maximum context window length of the LLM, thereby violating the crucial principle of content preservation discussed in Section 3.1. While we did not encounter this issue in our experiments, it could potentially limit the effectiveness of LLMs with shorter context window lengths. We believe that this can be addressed by increasing the maximum context window length of LLMs (Xiao et al., 2023) or simply selecting an LLM with a longer context window length, such as Claude2 for 100k tokens.
Excessive LLM alignment. Although Portia works well under most scenarios, it is not perfect. One limitation is that Portia is not able to handle the case where the LLM evaluators refuse to make a verdict, which usually occurs on LLMs with advanced alignment techniques such as GPT series models. We find that these models would become too conservative to make a verdict, no matter how the answers are split and aligned. For example, GPT-3.5 often refuses to give any meaningful response when the question is in "Roleplay" category.</p>
<h2>References</h2>
<p>claude2. https://www.anthropic.com/index/ claude-2.</p>
<p>Llama 3. https://llama.meta.com/llama3/.
qwen. https://github.com/QwenLM/Qwen-7B/ blob/main/tech_memo.md.
treesitter. https://tree-sitter.github.io/ tree-sitter/.
wormgpt. https://wormgpt.ai/.
Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023. Chateval: Towards better llm-based evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201.</p>
<p>Lingjiao Chen, Matei Zaharia, and James Zou. 2023. How is chatgpt's behavior changing over time? arXiv preprint arXiv:2307.09009.</p>
<p>Cheng-Han Chiang and Hung yi Lee. 2023. Can large language models be an alternative to human evaluations?</p>
<p>Andrew A Chien, Liuzixuan Lin, Hai Nguyen, Varsha Rao, Tristan Sharma, and Rajini Wijayawardana. 2023. Reducing the carbon impact of generative ai inference (today and in 2035). In Proceedings of the 2nd Workshop on Sustainable Computer Systems, pages $1-7$.</p>
<p>DeepSeek-AI. 2024. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL.</p>
<p>Shuzheng Gao, Cuiyun Gao, Yulan He, Jichuan Zeng, Lunyiu Nie, Xin Xia, and Michael R. Lyu. 2023. Code structure-guided transformer for source code summarization. ACM Trans. Softw. Eng. Methodol., 32(1):23:1-23:32.</p>
<p>Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, and Sunayana Sitaram. 2023. Are large language model-based evaluators the solution to scaling up multilingual evaluation?</p>
<p>Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Xing Wang, and Zhaopeng Tu. 2023. Is chatgpt a good translator? a preliminary study. ArXiv, abs/2301.08745.</p>
<p>Walter Kintsch and Janice Keenan. 1973. Reading rate and retention as a function of the number of propositions in the base structure of sentences. Cognitive Psychology, 5(3):257-274.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213.</p>
<p>Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023a. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval.</p>
<p>Zongjie Li, Chaozheng Wang, Zhibo Liu, Haoxuan Wang, Dong Chen, Shuai Wang, and Cuiyun Gao. 2023b. CCTEST: testing and repairing code completion systems. In 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023, pages 12381250. IEEE.</p>
<p>Zongjie Li, Chaozheng Wang, Pingchuan Ma, Chaowei Liu, Shuai Wang, Daoyuan Wu, Cuiyun Gao, and Yang Liu. 2024. On extracting specialized code abilities from large language models: A feasibility study. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, ICSE '24, New York, NY, USA. Association for Computing Machinery.</p>
<p>Zongjie Li, Chaozheng Wang, Shuai Wang, and Gao Cuiyun. 2023c. Protecting intellectual property of large language model-based code generation apis via watermarks. In Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security, CCS 2023, Copenhagen, Denmark, November 26-30, 2023.</p>
<p>Rensis Likert. 1932. A technique for the measurement of attitudes. Archives of psychology.</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.</p>
<p>Yen-Ting Lin and Yun-Nung Chen. 2023. LLM-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models. In Proceedings of the 5th Workshop on NLP for Conversational AI (NLP4ConvAI 2023), pages 4758, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023a. G-eval: Nlg evaluation using gpt-4 with better human alignment.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023b. Gpteval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311-318.</p>
<p>Kaiping Peng, Richard E Nisbett, and Nancy YC Wong. 1997. Validity problems comparing values across cultures and possible solutions. Psychological methods, 2(4):329.</p>
<p>Nazneen Rajani, Nathan Lambert, Sheon Han, Jean Wang, Osvald Nitski, Edward Beeching, and Lewis Tunstall. 2023. Can foundation models label data like humans? Hugging Face Blog. Https://huggingface.co/blog/llm-v-human-data.</p>
<p>Oktavia Yovi Ratnasari. 2023. Students'difficulties in reading comprehension and the strategies to deal with the difficulties. Jurnal Penelitian, Pendidikan, dan Pembelajaran, 18(13).</p>
<p>Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Hong Kong, China. Association for Computational Linguistics.</p>
<p>Surendrabikram Thapa, Usman Naseem, and Mehwish Nasim. 2023. From humans to machines: can chatgpt-like llms effectively replace human annotators in nlp tasks. In Workshop Proceedings of the 17th International AAAI Conference on Web and Social Media.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Jen tse Huang, Man Ho Adrian Lam, Eric Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, and Michael R. Lyu. 2023. Emotionally numb or empathetic? evaluating how llms feel using emotionbench. ArXiv, abs/2308.03656.</p>
<p>Chaozheng Wang, Zongjie Li, Cuiyun Gao, Wenxuan Wang, Ting Peng, Hailiang Huang, Yuetang Deng, Shuai Wang, and Michael R Lyu. 2024. Exploring multi-lingual bias of large code models in code generation. arXiv preprint arXiv:2404.19368.</p>
<p>Chaozheng Wang, Zongjie Li, Yun Pena, Shuzheng Gao, Sirong Chen, Shuai Wang, Cuiyun Gao, and Michael R Lyu. 2023a. Reef: A framework for collecting real-world vulnerabilities and fixes. In 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE), pages 19521962. IEEE.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926.</p>
<p>Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang,</p>
<p>Rui Xie, Jindong Wang, Xing Xie, et al. 2023c. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087.</p>
<p>Thilini Wijesiriwardene, Ruwan Wickramarachchi, Bimal Gajera, Shreeyash Gowaikar, Chandan Gupta, Aman Chadha, Aishwarya Naresh Reganti, Amit Sheth, and Amitava Das. 2023. Analogical-a novel benchmark for long text analogy evaluation in large language models. In Findings of the Association for Computational Linguistics: ACL 2023, pages 35343549 .</p>
<p>Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453.</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414.</p>
<p>Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. 2023. Evaluating large language models at evaluating instruction following. arXiv preprint arXiv:2310.07641.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675.</p>
<p>Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin Li. 2023. Wider and deeper llm networks are fairer llm evaluators. arXiv preprint arXiv:2308.01862.</p>
<p>Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697-12706. PMLR.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23, Red Hook, NY, USA. Curran Associates Inc.</p>
<p>Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al. 2023. Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. arXiv preprint arXiv:2306.04528.</p>
<h2>A Reproducibility</h2>
<p>To assure reproducibility, we employ various methods to mitigate the inherent randomness in the decoding process of LLMs. For models using cloud API, the hyper-parameter "temperature" is uniformly set to 0 across all evaluators. For local models, the sampling function is deactivated during the decoding phase to get deterministic results. Specifically, we run experiments on a GPU server with Intel Xeon Platinum 8276 CPU, 256GB of RAM, and 4 NVIDIA A100 GPUs. This server is capable of performing cloud API calls and local LLM inference.</p>
<p>All our results are reproducible using the code repository we will release. All experimental details, including hyperparameters, are reported in Section 3.2 and Appendix I. We reuse the benchmark datasets from (Zheng et al., 2024), with the different comparison prompt forms detailed in Appendix K.</p>
<h2>B Response Length</h2>
<h2>B. 1 Response Length Statistics</h2>
<p>It is possible for the generated results to differ significantly from each other. To further explore this, we conducted an analysis of the statistical information of all LLM responses, revealing substantial differences in response lengths within our benchmark dataset. The relevant data is presented in Table 4.</p>
<p>From the table, we observe that the lengths of responses generated by the LLMs vary considerably. For example, the maximum number of characters in the responses is 4,827 , generated by Llama-13b, while the minimum is just 6 characters, generated by Alpaca-13b.</p>
<h2>B. 2 Relationship Between Answer Length and Inconsistency</h2>
<p>To further explore the relationship between answer length and inconsistency, we conduct an additional experiment using the collected judgment data. For this, "GPT-3.5" was used as the evaluator, analyzing 8 pairs of responses across three comparison forms. The answers are categorized into 9 groups based on their length, with each group representing an 800-character interval. The resulting data are presented in Table 5, with values below $2 \%$ of the total indicated by "-".</p>
<p>The table shows a generally positive correlation between answer length and inconsistency rate, with</p>
<table>
<thead>
<tr>
<th>LLM</th>
<th>Max Length</th>
<th>Min Length</th>
<th>Average Length</th>
<th>Standard Deviation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alpaca-13b</td>
<td>1149</td>
<td>6</td>
<td>508.99</td>
<td>222.98</td>
</tr>
<tr>
<td>Bard</td>
<td>2652</td>
<td>151</td>
<td>1276.62</td>
<td>495.88</td>
</tr>
<tr>
<td>Vicuna-7b</td>
<td>2598</td>
<td>266</td>
<td>1457.01</td>
<td>448.55</td>
</tr>
<tr>
<td>Claude-v1</td>
<td>2392</td>
<td>94</td>
<td>1624.8</td>
<td>612.29</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>2218</td>
<td>193</td>
<td>1206.29</td>
<td>460.26</td>
</tr>
<tr>
<td>Vicuna-13b</td>
<td>2441</td>
<td>212</td>
<td>1416.92</td>
<td>371.0</td>
</tr>
<tr>
<td>GPT-4</td>
<td>3842</td>
<td>201</td>
<td>2044.14</td>
<td>768.59</td>
</tr>
<tr>
<td>Llama-13b</td>
<td>4827</td>
<td>9</td>
<td>757.57</td>
<td>895.74</td>
</tr>
<tr>
<td>GPT-3.5-short</td>
<td>365</td>
<td>26</td>
<td>152.55</td>
<td>58.07</td>
</tr>
</tbody>
</table>
<p>Table 4: The statistics of answers from different LLMs. "GPT-3.5-short" is generated by instructing GPT-3.5 to shorten its responses while preserving as much meaning as possible, which consists of responses approximately $1 / 8$ th the length of the original ones.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Char Range (*100)</th>
<th style="text-align: center;">\% Incon Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$0-8$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">$8-16$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">$16-24$</td>
<td style="text-align: center;">26.89</td>
</tr>
<tr>
<td style="text-align: center;">$24-32$</td>
<td style="text-align: center;">23.02</td>
</tr>
<tr>
<td style="text-align: center;">$32-40$</td>
<td style="text-align: center;">31.84</td>
</tr>
<tr>
<td style="text-align: center;">$40-48$</td>
<td style="text-align: center;">39.01</td>
</tr>
<tr>
<td style="text-align: center;">$48-56$</td>
<td style="text-align: center;">42.73</td>
</tr>
<tr>
<td style="text-align: center;">$56-64$</td>
<td style="text-align: center;">55.45</td>
</tr>
<tr>
<td style="text-align: center;">$64+$</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 5: The inconsistency rates in different character count gaps.
shorter answers tending to exhibit lower inconsistency rates. This finding suggests that position bias is less significant in shorter answers. To clarify, PORTIA is designed to be adaptable to open-ended questions and answers without making any assumptions about the content of the candidate answers. As long as the responses contain sufficient content (at least two sentences in our current setup) for splitting, PORTIA will follow the same process to first split the responses and then conduct length or semantic alignment to merge them. When combined with the enhancement results detailed in Section 4.2, this leads to the conclusion that the proposed framework is effective in handling responses of varying lengths.</p>
<h3>B. 3 Extremely Short Response</h3>
<p>It is worth noting that our initial considerations did not account for a scenario where responses from one specific LLM are consistently and significantly shorter (e.g., 1/8th the length) than those from another. This is due to the expectation that LLMs under test are trained to generate responses adhering to given instructions, typically resulting in average response lengths of several hundred characters.</p>
<p>To determine whether our framework remains applicable in such special cases, we conducted an additional experiment with the following steps: (1)</p>
<p>We instructed GPT-3.5 to shorten its responses while preserving as much meaning as possible, leading to a subset termed "GPT-3.5-short," which consisted of responses approximately $1 / 8$ th the length of the original ones. (2) We then used GPT-3.5 and GPT-4 as evaluators to compare "GPT-3.5-short" with "GPT-3.5" and "Claude-v1" in exchanged orders, to assess consistency.</p>
<p>The results, shown in Table 6, indicate a 100\% consistency rate (80/80) for both GPT-3.5 and GPT4 as evaluators. This suggests that there is no inconsistency in this particular scenario, and therefore no alignment is needed. It means that position bias is no longer a concern in such situations. This finding aligns with previous studies (Chiang and yi Lee, 2023; Liu et al., 2023b), which noted that LLM-based evaluators tend to assign higher scores to longer responses.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Evaluators</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT-3.5</th>
<th style="text-align: center;">GPT-4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model1</td>
<td style="text-align: center;">Model2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-short</td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">100\%</td>
<td style="text-align: center;">100\%</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-short</td>
<td style="text-align: center;">Claude-v1</td>
<td style="text-align: center;">100\%</td>
<td style="text-align: center;">100\%</td>
</tr>
</tbody>
</table>
<p>Table 6: The consistency rates of GPT-3.5 and GPT-4 as evaluators for extremely short responses.</p>
<p>It is worth noting that we have previously addressed the scenario in which the content of the responses differs significantly, but the length remains similar, as discussed in Section 4.2. In the current case, the content of the responses is indeed different, as each response contains only one-eighth of the original content. However, there is no requirement for alignment in this particular scenario, as position bias is not a pertinent concern. To summarize, we argue that PORTIA is applicable to openended questions, accommodating responses that vary significantly from each other, even in cases where one set of responses is systematically and markedly shorter than the other.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">\% Fixed coverage</th>
<th style="text-align: center;">\% Frequency</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0-300</td>
<td style="text-align: center;">50.82</td>
<td style="text-align: center;">0.37</td>
</tr>
<tr>
<td style="text-align: center;">300-600</td>
<td style="text-align: center;">48.41</td>
<td style="text-align: center;">0.24</td>
</tr>
<tr>
<td style="text-align: center;">600-900</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">0.17</td>
</tr>
<tr>
<td style="text-align: center;">900-1200</td>
<td style="text-align: center;">62.67</td>
<td style="text-align: center;">0.11</td>
</tr>
<tr>
<td style="text-align: center;">1200-1500</td>
<td style="text-align: center;">69.77</td>
<td style="text-align: center;">0.08</td>
</tr>
</tbody>
</table>
<p>Table 7: Fixed coverage rates in different character count gaps.</p>
<p>The answers are categorized into 5 groups based on their length, with each group representing a 300-character interval. The results are presented in Table 7, with frequencies below 3\% of the total being disregarded.</p>
<h2>C Naming Reason</h2>
<p>The name PORTIA is inspired by the intelligent and astute character, Portia, from Shakespeare’s "The Merchant of Venice." In the play, Portia assists a judge in making fair decisions within the legal rules. Just as Portia requests the exact amount of flesh to be cut, our method seeks to make fair splits of the original answers for comparison.</p>
<h2>D A Preliminary Study of Standalone Comparison</h2>
<p>In this section, following the same setting as (Zheng et al., 2024), we conduct a preliminary study of standalone score-based LLM comparison. We use the template shown in Table 8 to generate the input for LLM evaluators. For each question, we generate three sets of value ranges, setting min_score to 0 , max_score to 1,10 , and 100 , and interval to $0.1,1$, and 10 , respectively. In theory, if the standalone comparison answer is steady and robust, the score should scale accordingly to the value ranges. For example, if the score is 0.7 when the max_score is 1 , the score should be 7 when the max_score is 10 , and 70 when max_score is 100 .</p>
<p>The LLM evaluators are asked to score each answer independently. We use the answers from "llama-13b" as the input for LLM evaluators, and choose "GPT-3.5" as the LLM evaluator. Among
a total of 80 test cases, we find that the standalone comparison does not remain consistent for any of them. Therefore, we conclude that the absolute scores of standalone comparison do not strictly adhere to a linear mapping relationship across different scales, potentially undermining their significance. It is worth noting that although standalone comparison has been used in prior research by (Chiang and yi Lee, 2023; Liu et al., 2023a; Zheng et al., 2024) to evaluate open-ended questions. It does not involve comparing two responses together, thereby eliminating any position bias. As a result, our paper primarily focuses on the position bias in pairwise comparison.</p>
<h2>E Portia's Pipeline</h2>
<p>This section explains the full pipeline of utilizing Portia for LLM evaluation. As depicted in Figure 4, typically there are four key steps: (1) Data preparation, (2) Comparison method selection, (3) Evaluation, and (4) Answer extract.</p>
<p>In the first step, we prepare the data for evaluation, which includes the questions and corresponding answers from two different LLMs to be compared. If Portia is not implemented, we next choose the comparison method and formulate the input prompt, which has a great impact on the evaluation results, as we discussed in Section 4.2. The selected LLM evaluator is then queried with this prompt to obtain judgments. Note that the raw evaluation results require additional processing for two reasons: (1) the output format may differ from the desired final verdicts and (2) the LLM evaluators may deviate from expected responses. For example, the LLM evaluator may fail to return the likert score for some questions but instead return the final verdict directly. Therefore, we design an answer extractor to extract the final verdict from the evaluation results. Specifically, we adopt a hybrid method to extract the final verdict, which first tries to extract with a rule-based system, and if it fails, then it tries with a supervised fine-tuning Chatglm2 (Zeng et al., 2022) model.</p>
<p>The Portia-enhanced evaluation would necessitate an additional step of alignment and reconstruction, which constitutes the core of our framework. As elucidated in the main text, this procedure is vital for assessing the LLM answers' quality with less position bias.
Community needs. Notably, some leading LLM leaderboards, such as Chatbot Arena (Zheng et al.,</p>
<div class="codehilite"><pre><span></span><code><span class="o">[</span><span class="n">Question</span><span class="o">]</span><span class="w"> </span><span class="err">{</span><span class="n">Q</span><span class="err">}</span>
<span class="o">[</span><span class="n">The Start of Assistant A&#39;s response</span><span class="o">]</span><span class="w"> </span><span class="err">{</span><span class="n">R1</span><span class="err">}</span><span class="w"> </span><span class="o">[</span><span class="n">The End of Assistant A&#39;s response</span><span class="o">]</span>
<span class="o">[</span><span class="n">The Start of Assistant B&#39;s response</span><span class="o">]</span><span class="w"> </span><span class="err">{</span><span class="n">R2</span><span class="err">}</span><span class="w"> </span><span class="o">[</span><span class="n">The End of Assistant B&#39;s response</span><span class="o">]</span>
<span class="o">[</span><span class="n">System</span><span class="o">]</span>
<span class="n">We</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="ow">like</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">request</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">feedback</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">performance</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">AI</span><span class="w"> </span><span class="n">assistants</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">response</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">user</span><span class="w"> </span><span class="n">question</span>
<span class="n">displayed</span><span class="w"> </span><span class="n">above</span><span class="p">.</span>
<span class="n">Please</span><span class="w"> </span><span class="n">rate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">helpfulness</span><span class="p">,</span><span class="w"> </span><span class="n">relevance</span><span class="p">,</span><span class="w"> </span><span class="n">accuracy</span><span class="p">,</span><span class="w"> </span><span class="k">level</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">details</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">their</span><span class="w"> </span><span class="n">responses</span><span class="p">.</span><span class="w"> </span><span class="k">Each</span><span class="w"> </span><span class="n">assistant</span><span class="w"> </span><span class="n">receives</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">overall</span>
<span class="n">score</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">scale</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">higher</span><span class="w"> </span><span class="n">score</span><span class="w"> </span><span class="n">indicates</span><span class="w"> </span><span class="n">better</span><span class="w"> </span><span class="n">overall</span><span class="w"> </span><span class="n">performance</span><span class="p">.</span>
<span class="n">Please</span><span class="w"> </span><span class="k">first</span><span class="w"> </span><span class="k">output</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">single</span><span class="w"> </span><span class="n">line</span><span class="w"> </span><span class="n">containing</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="k">values</span><span class="w"> </span><span class="n">indicating</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">scores</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">Assistant</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">B</span><span class="p">,</span><span class="w"> </span><span class="n">respectively</span><span class="p">.</span>
<span class="n">The</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">scores</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">separated</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="nf">space</span><span class="p">.</span><span class="w"> </span><span class="ow">In</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">subsequent</span><span class="w"> </span><span class="n">line</span><span class="p">,</span><span class="w"> </span><span class="n">please</span><span class="w"> </span><span class="n">provide</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">comprehensive</span><span class="w"> </span><span class="n">explanation</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">your</span>
<span class="n">evaluation</span><span class="p">,</span><span class="w"> </span><span class="n">avoiding</span><span class="w"> </span><span class="ow">any</span><span class="w"> </span><span class="n">potential</span><span class="w"> </span><span class="n">bias</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">ensuring</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">order</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">responses</span><span class="w"> </span><span class="n">were</span><span class="w"> </span><span class="n">presented</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">affect</span>
<span class="n">your</span><span class="w"> </span><span class="n">judgment</span><span class="p">.</span>
<span class="n">We</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="ow">like</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">request</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">feedback</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">performance</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">AI</span><span class="w"> </span><span class="n">assistants</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">response</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">user</span><span class="w"> </span><span class="n">question</span>
<span class="n">displayed</span><span class="w"> </span><span class="n">above</span><span class="p">.</span>
<span class="n">Please</span><span class="w"> </span><span class="n">rate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">helpfulness</span><span class="p">,</span><span class="w"> </span><span class="n">relevance</span><span class="p">,</span><span class="w"> </span><span class="n">accuracy</span><span class="p">,</span><span class="w"> </span><span class="k">level</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">details</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">their</span><span class="w"> </span><span class="n">responses</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">assistant</span><span class="w"> </span><span class="n">receives</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">overall</span>
<span class="n">score</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">scale</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="err">{</span><span class="n">min_score</span><span class="err">}</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="err">{</span><span class="n">max_score</span><span class="err">}</span><span class="w"> </span><span class="p">(</span><span class="k">with</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">minimum</span><span class="w"> </span><span class="k">interval</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="err">{</span><span class="k">interval</span><span class="err">}</span><span class="p">),</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">higher</span><span class="w"> </span><span class="n">score</span><span class="w"> </span><span class="n">indicates</span>
<span class="n">better</span><span class="w"> </span><span class="n">overall</span><span class="w"> </span><span class="n">performance</span><span class="p">.</span>
<span class="n">Please</span><span class="w"> </span><span class="k">first</span><span class="w"> </span><span class="k">output</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">single</span><span class="w"> </span><span class="n">line</span><span class="w"> </span><span class="n">containing</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="k">value</span><span class="w"> </span><span class="n">indicating</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">score</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">Assistant</span><span class="p">.</span><span class="w"> </span><span class="ow">In</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">subsequent</span><span class="w"> </span><span class="n">line</span><span class="p">,</span>
<span class="n">please</span><span class="w"> </span><span class="n">provide</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">comprehensive</span><span class="w"> </span><span class="n">explanation</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">evaluation</span><span class="p">,</span><span class="w"> </span><span class="n">avoiding</span><span class="w"> </span><span class="ow">any</span><span class="w"> </span><span class="n">potential</span><span class="w"> </span><span class="n">bias</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">ensuring</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">order</span>
<span class="ow">in</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">responses</span><span class="w"> </span><span class="n">were</span><span class="w"> </span><span class="n">presented</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">affect</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">judgment</span><span class="p">.</span>
</code></pre></div>

<p>Table 8: The score-based evaluation form for standalone comparison with six slots ({Q}, {R1}, {R2}, {min_score}, {max_score}, {interval} ).
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: This is the overview of using Portia for LLM evaluation. "Reco" and "SFT" are short for "recognition" and "supervised fine-tuning", respectively.</p>
<p>2024), still rely on human votes for evaluation, which limits their scalability and increases costs. We believe that our method can be seamlessly integrated into these leaderboards to support an automatic evaluation process, significantly reducing costs and improving the scalability of the leaderboards. For scenarios where LLM-based evaluators are used, the community can easily implement PORTIA by altering the query construction in existing pipelines. By adding the splitting and merging steps, the community can use the consistent results from PORTIA as the final output.</p>
<h2>F Real-World Performance and Cost Analysis</h2>
<p>In this section, we provide a the performance and cost analysis of different LLM evaluators before and after fix by PORTIA in real-world settings. Notably, the carbon emission of GPT-3.5 is estimated following (Chien et al., 2023). We estimate the cost using the official pricing for cloud APIs and the Azure ND A100 v4 instances for local models.</p>
<h2>G LLM Details</h2>
<p>In this section, we provide more details about the LLM evaluators and answers used in our experiments.
LLM Evaluators. As introduced in Section 4.1, we include both locally deployable models that are open-source and proprietary models that are accessed through only cloud APIs as LLM evaluators. For local models, we select Chatglm2 (Zeng et al., 2022) and Llama2 (Touvron et al., 2023), due to their notable efficacy and convenient local deployment capabilities. For cloud-based LLMs, we use GPT (including both GPT-4 and GPT-3.5) (OpenAI, 2023) from OpenAI, Qwen (qwe) from Alibaba, and Claude2 (cla) from Anthropic. The rationale for using these models is based on their exceptional performance, since they are considered among the most advanced and powerful in the world. Specifically, we evaluate PORTIA using six distinct LLMs as evaluators:</p>
<ul>
<li>GPT-4 (OpenAI, 2023) is a large multimodal model capable of processing image and text inputs to generate text outputs. GPT-4 demonstrates human-level aptitude on various professional and academic benchmarks. We utilize the 8 K context length "gpt-4-0613" configuration by default.</li>
<li>GPT-3.5 is a 175B parameter model from OpenAI offered in 4 K and 16 K context length versions. Our experiments use the 4 K context "gpt-3.5-turbo-0301" model as default.</li>
<li>Claude2 (cla) is the latest large language model released by Anthropic. It supports at most 100k tokens as input. We leverage the default Claude2 API in our tests.</li>
<li>Llama2 (Touvron et al., 2023), an opensource series of LLMs from Meta AI ranging from 7B to 70B parameters, is trained on 2 trillion tokens and doubles Llama1's context length. Its fine-tuned iterations utilize over 1 million human annotations. We evaluate both 7B and 13B Llama2 chat models.</li>
<li>Qwen (qwe) is a partially open-sourced LLM model released by Alibaba. We use the default API service provided by Alibaba cloud in our experiments.</li>
<li>Chatglm2 (Zeng et al., 2022) is the secondgeneration version of the open-source bilingual chat model ChatGLM-6B. We use the offered 6B version in our experiments.</li>
</ul>
<p>LLM Answers. As mentioned in Section 4.1, we consider eight answer combinations from different LLMs, specifically, the pairs are: "gpt-3.5-turbo" versus "claude-v1", "llama-13b" versus "vicuna13b", "alpaca-13b" versus "vicuna-13b", "gpt-3.5turbo" versus "gpt-4", "gpt-4" versus "claude-v1", "vicuna-13b" versus "vicuna-7b", "vicuna-7b" versus "alpaca-13b", and "gpt-4" versus "vicuna-13b". The answers are generated by the LLMs without any post-processing, and we reuse these answers from previous work (Zheng et al., 2024). Notably, there is indeed some overlap between the models used as LLM evaluators and models used to generate the answers.</p>
<h2>H Algorithm Illustration</h2>
<p>To elucidate the operational details of the proposed splitting algorithm, we provide a schematic depiction in Figure 5. Given two LLM-generated answers, the algorithm first identifies all candidate split positions coinciding with sentence boundaries in each answer. It then performs length alignment by initially dividing each answer equally into $k$ partitions and recording the corresponding split positions. Next, for each answer, the split position closest to the recorded locations is selected from the</p>
<table>
<thead>
<tr>
<th></th>
<th>AR origin</th>
<th>AR fix</th>
<th>Carbon Emitted</th>
<th>Avg Cost</th>
<th>Avg Time</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>(%)</td>
<td>(%)</td>
<td>(CO2eq / per 1k)</td>
<td>(USD / per 1k)</td>
<td>(s / per 1k)</td>
</tr>
<tr>
<td>GPT-4</td>
<td>-</td>
<td>-</td>
<td>N/A</td>
<td>29.78</td>
<td>13,446</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>82.50</td>
<td>88.59</td>
<td>7.22</td>
<td>2.85</td>
<td>2,192</td>
</tr>
<tr>
<td>Qwen</td>
<td>60.83</td>
<td>69.58</td>
<td>N/A</td>
<td>35.49</td>
<td>6,083</td>
</tr>
<tr>
<td>Chatglm2</td>
<td>20.34</td>
<td>39.16</td>
<td>2.15</td>
<td>4.09</td>
<td>1,983</td>
</tr>
<tr>
<td>Claude2</td>
<td>43.44</td>
<td>75.09</td>
<td>N/A</td>
<td>27.17</td>
<td>11,561</td>
</tr>
</tbody>
</table>
<p>Table 9: Real-world comparison of different LLM evaluators’ results before and after fix by Portia with that of GPT-4, including resource consumption. “AR” denotes the agreement rate with GPT-4.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: Schematic illustration of the proposed splitting algorithm, depicting its operation when configured with k = 2 (i.e., division into two parts).</p>
<p>candidate positions. The answers are partitioned into k segments at these chosen split positions. The resulting segments are fed as inputs to the LLM evaluator to obtain the respective judgments.</p>
<p>In cases where inconsistent judgments persist, the algorithm proceeds with semantic alignment to further divide each answer into k parts. Specifically, an iterative search is conducted for optimal split positions that maximize the cumulative semantic similarity between the corresponding segments from the two answers. This traversal terminates when the complete set of potential split positions has been evaluated. Through this process based on both length and semantic alignment, the algorithm is able to decompose the LLM answers into aligned parts for more consistent and reliable evaluation.</p>
<h2>I LM Metric</h2>
<p>In this section, we first introduce the LM metric used in our experiments. Then we conduct a controlled experiment to find the optimal number of splits k across different metrics in terms of performance and efficiency.</p>
<p>LM Metric. To clarify, we use the Sentence-BERT (Reimers and Gurevych, 2019) to measure the similarity between pairs. Sentence-BERT is a modification of the pretrained BERT (Devlin et al., 2019) network that uses siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. We do not follow previous work (Li et al., 2023b, 2024; Gao et al., 2023) where CodeBLEU is used, as Sentence-BERT is efficient while maintaining the accuracy of BERT.</p>
<p>Efficiency Evaluation. We use the same setup as in Section 4.1 to conduct the experiment. According to the theoretical analysis in Section 4.3, we set k ∈ {1, 2, 3, 4} and evaluate their efficiency, the results are shown in Table 10. Note that k is the number of segments after splitting, thus k = 1 means no splitting would be performed, which leads to 0 in terms of execution time. In short, it can be interpreted from the table that the execution time grows exponentially with the increasing k.</p>
<p>Performance Evaluation. Following the experimental setup described above, we set k ∈ {1, 2, 3, 4} and evaluate their performance. To clarify, we use the answers from the LLM “gpt-3.5-turbo” and “claude-v1” in our experiments (under the same conditions outlined in Section 4.4), where in total of 80 questions are fed to GPT-3.5 for evaluation. The results are shown in Table 11, where we can see that with the increasing k, the fixed coverage rates of both metrics are increasing, and when</p>
<h1>Algorithm 2: Details of Step 3: Semantic Alignment $(k=2)$</h1>
<div class="codehilite"><pre><span></span><code><span class="o">/*</span><span class="w"> </span><span class="n">Step3</span><span class="p">:</span><span class="w"> </span><span class="n">semantic</span><span class="w"> </span><span class="n">alignment</span><span class="p">.</span>
</code></pre></div>

<p>$1 s_{\text {max }}=0, r_{1}^{\text {bestparts }}=\left[], r_{2}^{\text {bestparts }}=\right[]$
for $i$ in range $\left(\operatorname{len}\left(r_{1}^{\text {positions }}\right)\right)$ do
for $j$ in range $\left(\operatorname{len}\left(r_{2}^{\text {positions }}\right)\right)$ do
$\operatorname{pos}<em 1="1">{1}=r</em>}^{\text {positions }}[i], \operatorname{pos<em 2="2">{2}=r</em>[j]$
$r_{1}^{\text {parts }}[0]=r_{1}\left[: \operatorname{pos}}^{\text {positions }<em 2="2">{1}\right], r</em>}^{\text {parts }}[0]=r_{2}\left[: \operatorname{pos<em 1="1">{2}\right]$
$r</em>}^{\text {parts }}[1]=r_{1}\left[\operatorname{pos<em 2="2">{1}:\right], r</em>}^{\text {parts }}[1]=r_{2}\left[\operatorname{pos<em _cum="{cum" _text="\text">{2}:\right]$
$s</em>[i]\right)$
/* Update max similarity score, keep best split positions.
if $s_{\text {cum }}&gt;s_{\text {max }}$ then
$s_{\text {max }}=s_{\text {cum }}, r_{1}^{\text {bestparts }}=r_{1}^{\text {parts }}, r_{2}^{\text {bestparts }}=r_{2}^{\text {parts }}$
end
end
end}}=\sum_{i=1}^{2} \operatorname{similarity}\left(r_{1}^{\text {parts }}[i], r_{2}^{\text {parts }</p>
<p>Algorithm 3: Details of Step 3: Semantic Alignment $(k=3)$
/<em> Step3: semantic alignment.
$1 s_{\text {max }}=0, r_{1}^{\text {bestparts }}=\left[], r_{2}^{\text {bestparts }}=\right[]$
2 for $i_{1}$ in range $\left(\operatorname{len}\left(r_{1}^{\text {positions }}\right)\right)$ do
for $i_{2}$ in range $\left(i_{1}+1, \operatorname{len}\left(r_{1}^{\text {positions }}\right)\right)$ do
for $j_{1}$ in range $\left(\operatorname{len}\left(r_{2}^{\text {positions }}\right)\right)$ do
for $j_{2}$ in range $\left(j_{1}+1, \operatorname{len}\left(r_{2}^{\text {positions }}\right)\right)$ do
$\operatorname{pos}<em 1="1">{11}=r</em>}^{\text {positions }}[i_{1}], \operatorname{pos<em 2="2">{21}=r</em>\right]$
$\operatorname{pos}}^{\text {positions }}\left[j_{1<em 1="1">{12}=r</em>}^{\text {positions }}[i_{2}], \operatorname{pos<em 2="2">{22}=r</em>\right]$
$r_{1}^{\text {parts }}[0]=r_{1}\left[: \operatorname{pos}}^{\text {positions }}\left[j_{2<em 2="2">{11}\right], r</em>}^{\text {parts }}[0]=r_{2}\left[: \operatorname{pos<em 1="1">{21}\right]$
$r</em>}^{\text {parts }}[1]=r_{1}\left[\operatorname{pos<em 12="12">{11}: \operatorname{pos}</em>}\right], r_{2}^{\text {parts }}[1]=r_{2}\left[\operatorname{pos<em 22="22">{21}: \operatorname{pos}</em>\right]$
$r_{1}^{\text {parts }}[2]=r_{1}\left[\operatorname{pos}<em 2="2">{12}:\right], r</em>}^{\text {parts }}[2]=r_{2}\left[\operatorname{pos<em _cum="{cum" _text="\text">{22}:\right]$
$s</em>[i]\right)$
/}}=\sum_{i=1}^{3} \operatorname{similarity}\left(r_{1}^{\text {parts }}[i], r_{2}^{\text {parts }</em> Update max similarity score, keep best split positions.
if $s_{\text {cum }}&gt;s_{\text {max }}$ then
$s_{\text {max }}=s_{\text {cum }}, r_{1}^{\text {bestparts }}=r_{1}^{\text {parts }}, r_{2}^{\text {bestparts }}=r_{2}^{\text {parts }}$
end
end
end
end
end</p>
<table>
<thead>
<tr>
<th></th>
<th>$k=1$</th>
<th>$k=2$</th>
<th>$k=3$</th>
<th>$k=4$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Token-overlap</td>
<td>0</td>
<td>0.31</td>
<td>3.71</td>
<td>33.12</td>
</tr>
<tr>
<td>Bert-model</td>
<td>0</td>
<td>2.37</td>
<td>21.3</td>
<td>295.10</td>
</tr>
</tbody>
</table>
<p>Table 10: Average execution time per input of different metrics with different $k$.</p>
<table>
<thead>
<tr>
<th></th>
<th>$k=1$</th>
<th>$k=2$</th>
<th>$k=3$</th>
<th>$k=4$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Token-overlap</td>
<td>-</td>
<td>53.3</td>
<td>66.7</td>
<td>73.3</td>
</tr>
<tr>
<td>Bert-model</td>
<td>-</td>
<td>55.9</td>
<td>66.7</td>
<td>66.7</td>
</tr>
</tbody>
</table>
<p>Table 11: Fixed coverage rates of different metrics with different $k$.
$k=3$, the fixed coverage rate of both metrics is the same, which is $66.7 \%$. However, further increasing $k$ results in limited additional gains in coverage. Considering the execution time which grows exponentially with the increasing $k$, we choose $k=3$ with token-overlap as the default setting in our experiments. Finally, we conclude that the semantic similarity score used in PORTIA achieves a good balance between coverage and execution time.</p>
<h2>J On Llama2</h2>
<p>Although LLMs can be effective evaluators, they are not infallible. In some cases, LLM evaluators may struggle to provide a meaningful verdict. We find that these models would become too conservative to make a verdict, no matter how the answers are split and aligned. Below is an example of the Llama2 models [touvron2023llama] that are not well-served as evaluators.</p>
<h2>K Prompt Templates</h2>
<h2>K. 1 Comparison Forms</h2>
<p>In this section, we provide the detailed templates for the three comparison forms, including relationbased (Table 13), score-based (Table 14), and likertbased forms (Table 15).</p>
<h2>K. 2 Alignment Templates</h2>
<p>In this section, we provide a detailed example in Table 16, demonstrating how to align the responses from two assistants with three slots.</p>
<h2>L Generalizability of Portia</h2>
<h2>L. 1 Extended Open-Ended Questions</h2>
<p>To evaluate the generalizability of Portia, we first generate an extended set of open-ended questions</p>
<p>Assistant A: Alpaca-13b, Assistant B: Vicuna-13b [Question] [How can I improve my time management skills?]
[The Start of Assistant A's response]
Improving time management skills involves setting priorities, breaking tasks into smaller chunks, delegating tasks, avoiding multitasking, and taking regular breaks. Additionally, it is important to have a positive attitude and be realistic with goals. Making a to-do list, tracking time, and using technology to automate mundane tasks can also help improve time management skills.
[The End of Assistant A's response]
[The Start of Assistant B's response]
Improving your time management skills can help you to be more productive, focused, and less stressed. Here are some tips to help you improve your time management skills: 1. Set clear goals: Identify what you want to achieve and set specific, measurable, and realistic goals. Prioritize your tasks and focus on the most important ones first. 2. Use a calendar or planner: Use a tool to organize your time and schedule your tasks. This can help you to stay on track and make the most of your time. 3. Break down tasks: Break down larger tasks into smaller, more manageable steps. This can help you to focus on one step at a time and make steady progress. 4. Eliminate distractions: Identify and eliminate distractions that may be preventing you from being productive. This can include turning off your phone, closing unnecessary tabs on your computer, or finding a quiet place to work. 5. Practice self-discipline: Use self-discipline to stay focused and avoid procrastination. Set a timer for each task and challenge yourself to complete it before the timer goes off. 6. Take breaks: Take regular breaks to rest and recharge. This can help you to stay focused and avoid burnout. 7. Review your progress: Regularly review your progress and adjust your strategy as needed. This can help you to stay on track and make improvements over time.
[The End of Assistant B's response]
Evaluator: {Llama2-7b-chat}, Form: {scorebased $}$, Assessment: {'n'n'n'n'n'n'n'n'n'n $}$
Evaluator: {Llama2-7b-chat}, Form: {likert-based $}$, Assessment: $\left.\right}^{{ }^{<em> </em>}}\right}$
Evaluator: {Llama2-13b-chat}, Form: {score-based}, Assessment: {Please proceed with your evaluation.} Evaluator: {Llama2-13b-chat}, Form: {likert-based}, Assessment: {'n'nPlease provide your feedback.}</p>
<p>Table 12: The assessments of Llama2-7b-chat and Llama2-13b-chat on two comparison forms.</p>
<p>[Question] {Q}
[The Start of Assistant A's response] {R1} [The End of Assistant A's response]
[The Start of Assistant B's response] {R2} [The End of Assistant B's response]
[System]
Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below.
You should choose the assistant that follows the user's instructions and answers the user's question better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation.
Avoid any positional biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible.
After providing your explanation, output your final verdict by strictly following this format: [[A]] if assistant A is better, [[B]] if assistant B is better, and [[C]] for a tie.</p>
<p>Table 13: The relation-based evaluation form with three slots ({Q}, {R1} and {R2}) from (Zheng et al., 2024).
[Question] {Q}
[The Start of Assistant A's response] {R1} [The End of Assistant A's response]
[The Start of Assistant B's response] {R2} [The End of Assistant B's response]
[System]
We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above.
Please rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10 , where a higher score indicates better overall performance.
Please first output a single line containing only two values indicating the scores for Assistant A and B, respectively. The two scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.</p>
<p>Table 14: The score-based evaluation form with three slots ({Q}, {R1} and {R2}).
[Question] {Q}
[The Start of Assistant A's response] {R1} [The End of Assistant A's response]
[The Start of Assistant B's response] {R2} [The End of Assistant B's response]
[System]
We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above.
Please compare the helpfulness, relevance, accuracy, level of details of their responses.
The rating should be from the set of $1,2,3,4,5,6$, or 7 , where higher numbers indicated that Assistant A was better than Assistant B. Please first output a single line containing only one value indicating the preference between Assistant A and B.
In the subsequent line, please provide a brief explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.</p>
<p>Table 15: The likert-based evaluation form with three slots ({Q}, {R1} and {R2}).
based on the original MT-Bench dataset. As introduced in Section 4.1, MT-Bench contains 80 elaborated open-ended questions spanning different categories. Following their approach, we use each question in the original dataset as a seed and ask GPT-4 to generate a number of similar questions according to its category. The specific prompt used is: "You are given a problem whose category is {category}, please generate {number_example} problems.", where "number_example" is set to 10. It is worth noting that the generated questions may have duplicates. Therefore, whenever a new question is generated, we check whether it is a duplicate of any existing questions. If so, we discard it directly.</p>
<p>Using the above method, we obtain a total of 640 questions as our new dataset. We further evaluate this extended dataset from two aspects. First, we analyze the distribution of question categories, as shown in Figure 6a. We can observe that our generated dataset has coverage across all categories. Secondly, we analyze the distribution of the number of question words, as shown in Figure 6b. The vast majority of questions have lengths between 10 and 20 words, with mean and median values of 20.41 and 17.00 , respectively.</p>
<h2>L. 2 Main Results</h2>
<p>After obtaining the extended dataset, we further conducted experiments on it to validate the effectiveness of Portia. The experimental settings</p>
<p>[Question] [What are the most effective ways to deal with stress?]
[The Start of Assistant A's response part 1]
Here are some effective ways to deal with stress:</p>
<ol>
<li>
<p>Exercise regularly: Physical activity can help reduce stress and improve mood.
[The End of Assistant A's response part 1]
[The Start of Assistant B's response part 1]
Here are some of the most effective ways to deal with stress:</p>
</li>
<li>
<p>Exercise regularly. Exercise is a great way to release pent up stress and tension. Even taking a 30 minute walk can help clear your mind and relieve stress.
[The End of Assistant B's response part 1]
[The Start of Assistant A's response part 2]</p>
</li>
<li>
<p>Practice relaxation techniques: Deep breathing, meditation, and yoga are some effective ways to relax and reduce stress levels.</p>
</li>
<li>Get enough sleep: Lack of sleep can increase stress levels. Try to get at least 7-8 hours of sleep each night.</li>
<li>
<p>Eat a healthy diet: A well-balanced diet can help reduce stress levels and improve overall health.
[The End of Assistant A's response part 2]
[The Start of Assistant B's response part 2]</p>
</li>
<li>
<p>Practice deep breathing and relaxation techniques. Things like meditation, yoga, and mindfulness exercises can help lower your stress levels. Taking some deep breaths can help you calm down in the moment.</p>
</li>
<li>Get enough sleep. Aim for 7 to 8 hours of sleep per night. Lack of sleep can intensify stress. Sleep helps your body and mind rest and recharge.</li>
<li>Eat a healthy, balanced diet. A healthy diet can boost your mood and reduce stress. Focus on whole foods like fruits, vegetables, whole grains, and lean protein. Limit caffeine and alcohol intake.</li>
<li>
<p>Spend time with others. Social support from family and friends can help reduce stress levels. Connecting to others can help take your mind off your worries and boost your mood.
[The End of Assistant B's response part 2]
[The Start of Assistant A's response part 3]</p>
</li>
<li>
<p>Connect with others: Spending time with friends and family, joining a support group, or participating in social activities can help reduce stress levels and improve mood.</p>
</li>
<li>Time management: Prioritizing tasks and creating a schedule can help reduce stress and increase productivity.</li>
<li>
<p>Seek professional help: If stress is impacting your daily life, consider seeking professional help such as therapy or counseling.
[The End of Assistant A's response part 3]
[The Start of Assistant B's response part 3]</p>
</li>
<li>
<p>Limit unhealthy habits. Things like smoking, excessive drinking, or drug use can worsen symptoms of stress. Try to avoid these unhealthy habits as much as possible.</p>
</li>
<li>Practice mindfulness. Spending time each day being fully present and aware of your thoughts and feelings can help reduce stress and increase feelings of calm. Try yoga, meditation, or simply sit in a quiet space for a few minutes.</li>
<li>Seek professional help if needed. If you are dealing with chronic or severe stress, it may help to talk to a therapist. A counselor can provide advice and coping strategies tailored to your situation. Therapy and counseling can be very effective at reducing stress and anxiety.</li>
<li>Take periodic breaks. It is important to take time each day to unwind and do something you enjoy. Read a book, take up a hobby, or simply relax in a bath. Taking breaks prevents burnout and recharges your body and mind.
[The End of Assistant B's response part 3]
[System]
Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below.
You should choose the assistant that follows the user's instructions and answers the user's question better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation.
Avoid any positional biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible.
After providing your explanation, output your final verdict by strictly following this format: [[A]] if assistant A is better, [[B]] if assistant B is better, and [[C]] for a tie.</li>
</ol>
<p>Table 16: The detailed prompt illustrated in Figure 1. We use relation-based form to construct the system prompt. The prompt in green is the "split boundary prompts".</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>in Appendix H to ease understanding.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
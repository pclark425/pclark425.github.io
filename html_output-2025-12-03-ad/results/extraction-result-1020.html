<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1020 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1020</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1020</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-232434224</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2202.10222v1.pdf" target="_blank">Robots Learn Increasingly Complex Tasks with Intrinsic Motivation and Automatic Curriculum Learning</a></p>
                <p><strong>Paper Abstract:</strong> Multi-task learning by robots poses the challenge of the domain knowledge: complexity of tasks, complexity of the actions required, relationship between tasks for transfer learning. We demonstrate that this domain knowledge can be learned to address the challenges in life-long learning. Specifically, the hierarchy between tasks of various complexities is key to infer a curriculum from simple to composite tasks. We propose a framework for robots to learn sequences of actions of unbounded complexity in order to achieve multiple control tasks of various complexity. Our hierarchical reinforcement learning framework, named SGIM-SAHT, offers a new direction of research, and tries to unify partial implementations on robot arms and mobile robots. We outline our contributions to enable robots to map multiple control tasks to sequences of actions: representations of task dependencies, an intrinsically motivated exploration to learn task hierarchies, and active imitation learning. While learning the hierarchy of tasks, it infers its curriculum by deciding which tasks to explore first, how to transfer knowledge, and when, how and whom to imitate.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1020.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1020.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SGIM-SAHT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Socially Guided Intrinsic Motivation for Sequence of Actions through Hierarchical Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified algorithmic architecture that combines intrinsic motivation, goal-directed temporally-abstract compound actions, planning and optional social guidance to learn sequences of primitive actions of unbounded length by discovering and exploiting a hierarchy of tasks/affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SGIM-SAHT agent (framework)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An embodied learning system that selects tasks, goals and exploration strategies via an interest map computed from competence progress; learns forward and inverse models per task, represents task hierarchy H as a directed graph, infers compound actions recursively through inverse models, and can use autonomous exploration and/or active imitation; learning algorithms include intrinsically-motivated exploration, hierarchical inverse/forward modelling and planning (with imitation strategies optional).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>robotic agent (implemented in simulation and robot setups described)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multiple experimental setups (Setup1, Setup2, Setup3) — non‑rewarding stochastic environments with objects and manipulanda</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environments include: Setup1 — a robotic arm with a pen and two joysticks (simulation shown) enabling drawing and controlling a videogame character; Setup2 — robot arm producing sounds by moving objects; Setup3 — a wheeled mobile robot arena with red obstacles and movable green objects of random sizes and physical properties. Environments are continuous, stochastic, have multiple control tasks (outcomes/affordances), and permit compound sequences of primitive actions of unbounded length.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Characterized qualitatively and structurally: task hierarchical depth (levels of task composition), required action-sequence length (compound action dimensionality n*p), action parameter dimensionality, number/types of objects and obstacles (e.g., movable objects + obstacles in Setup3). Formally: primitive action a ∈ R^p, compound action of length n described by n*p parameters; action space considered effectively infinite-dimensional A^N. No single scalar numerical complexity metric reported.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varies (from low for simple primitive tasks to high for composite tasks requiring long action sequences and deeper task hierarchy)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Environment variation realized via (a) static vs dynamic task sets (static predefined outcome/procedure spaces vs emergent tasks), (b) per-episode randomization of object physical properties (colour, height, diameter) in CHIME/Setup3, (c) stochastic initial states; measured qualitatively by whether task set is fixed or procedurally varied and by randomized object properties per episode. No numeric variation amplitude reported.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>varies by implementation: low variation for static-task experiments (IM-PB, SGIM-PB), high variation for CHIME/Setup3 (randomized object properties across episodes)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Learner competence measured as a reward common to all goals computed from Euclidean distance between goal outcome ω_g and reached outcome ω_r; interest maps use progress (derivative of competence) per region and strategy; additional evaluation via task success in test phase and qualitative measures (e.g., sequence length adaptation, discovery of affordances).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper discusses relationships qualitatively: hierarchical task structure (complexity) enables automatic curriculum learning from simple to complex tasks; increased environment variation (e.g., random object properties in CHIME) drives emergence of new affordances/tasks that become nodes in the hierarchy. Trade-offs discussed: deeper/higher complexity tasks require more exploration of the procedure space (temporal abstraction), while simple tasks are explored in action-parameter space; social demonstrations can bootstrap learning of highest-level (most complex) tasks, mitigating exploration cost. No explicit quantitative trade-off curve provided.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Automatic curriculum learning via intrinsically-motivated goal selection and interest-driven strategy selection; supports autonomous exploration, planning with hierarchical inverse models, and active imitation learning (when available).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Qualitative generalization reported: CHIME's emergent affordance models were used to plan complex tasks with known and unknown objects (using object physical properties to decide applicability); IM-PB/SGIM-PB used learned procedures and primitives to achieve composite tasks in test phases. Specific numeric generalization metrics are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not quantified numerically in paper; qualitative claims that (a) transfer from simple to complex tasks and reuse of subtasks reduces required exploration, and (b) demonstrations bootstrap and accelerate learning for high-level tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SGIM-SAHT provides a unified mechanism to discover task hierarchies and infer curricula; competence-progress interest guides which tasks and strategies to explore; temporally-abstract procedure representations let the agent adapt action-sequence length to task complexity; social guidance (imitation) can significantly bootstrap learning of highest-level tasks; dynamic discovery (CHIME) enables emergence of affordances under varied environments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1020.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1020.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IM-PB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Intrinsically Motivated Procedure Babbling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An implementation of SGIM-SAHT for a static, predefined set of controllable/outcome spaces that explores both outcome and procedure (sequence) spaces using intrinsic motivation to discover task dependencies and build curricula from simple to composite tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>IM-PB agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Embodied robotic learner using intrinsic motivation and two exploration strategies (outcome exploration and procedure exploration); represents procedures as sequences of subgoals (ω1,...,ωn) and learns forward/inverse models for predefined tasks; composes learned subtasks to form compound actions.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>robotic agent (arm setups in simulation/experiment: Setup1 and Setup2)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Setup1 and Setup2 (static task sets)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Setup1: robotic arm with pen and two joysticks enabling drawing and joystick control (simulation illustrated). Setup2: robotic arm producing sounds by moving objects. In IM-PB experiments the set of possible outcomes/controllables is predefined (static), so environment variation is low across episodes except for exploration-induced state differences.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Task complexity measured by hierarchical composition (levels) and required compound action length (number of subtasks/sequence length); action complexity related to dimensionality n*p. Complexity is examined qualitatively and empirically by observing which tasks need decomposition into procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>spans low (simple primitive tasks) to medium/high (composite tasks composed of subtasks); reported experiments effectively evaluate up to two hierarchy levels with some limitation beyond that.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Static task set — low environment variation; procedural exploration varies internal state but external instance variation (e.g., object properties) is not a focus in IM-PB experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Competence (Euclidean distance between goal and reached outcome) and interest/progress over regions of outcome space; performance evaluated by ability to reach goals and by adaptation of action-sequence length to task complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>IM-PB shows that when tasks are more complex (higher in hierarchy) the agent shifts exploration toward the procedure space (searching for useful subtasks) rather than primitive action space; reuse of simple tasks as subgoals reduces exploration needed for composite tasks. Because variation is low (static tasks), relationships involving environment variation are not systematically studied in IM-PB.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Intrinsic-motivation driven curriculum learning (automatic goal selection and procedure babbling) within a static predefined task set.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Qualitative test-phase results: the learner uses the correct task decompositions and adapts action sequence length to task complexity; performance improvements observed especially on high-level tasks relative to baselines that do not use procedures. No numeric generalization scores provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not numerically specified; qualitative claim that reuse of subtasks reduces exploration effort and improves learning efficiency, but adaptation to deeper hierarchies limited (effective up to first two levels).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Intrinsically-motivated procedure babbling enables discovery of task decompositions in a static-task domain, improves performance on composite tasks by reusing subtasks, and adapts action-sequence complexity to task complexity — though adaptation was limited beyond two hierarchy levels.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1020.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1020.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SGIM-PB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Socially Guided Intrinsic Motivation by Procedure Babbling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of IM-PB that integrates active imitation learning: the agent actively selects when, what and whom to imitate (actions or procedures), combining intrinsic motivation with teacher demonstrations to accelerate learning of hierarchical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SGIM-PB agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Embodied learner that augments IM-PB with active imitation strategies: it can request demonstrations of primitive actions or procedures from teachers, add noise for local exploration around demonstrations, and use competence-progress interest to choose between autonomous exploration and imitation and to select which teacher/strategy is most beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>robotic agent (arm setups in simulation/experiment: Setup1 and Setup2)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Setup1 and Setup2 (static task sets with available teachers/demonstrators)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same as IM-PB: robotic arm tasks (drawing with pen, producing sounds), but with access to teacher demonstrations; environment variation is primarily due to task instances and teacher-provided demonstrations rather than procedural randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Task complexity measured by hierarchical depth and required compound action length; social guidance is targeted at higher-complexity tasks (procedures/subtasks).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>low to high, with social guidance especially beneficial for higher-complexity (high) tasks</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Low external environment variation (static task set); variation is introduced by demonstrations from potentially different teachers and by exploration noise added to demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low to medium (depending on diversity of teacher demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Competence (Euclidean distance between goal and reached outcome) and interest/progress-based selection; evaluation also includes speed of learning/bootstrapping for high-level tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper reports that demonstrations, especially procedure demonstrations for complex tasks and policy demonstrations for simple tasks, bootstrap learning: social guidance reduces exploration burden for high-complexity tasks. The relationship is qualitative: social imitation mitigates the exploration cost associated with high task complexity; variation due to diverse teachers can be exploited by active selection but quantitative trade-offs are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Automatic curriculum learning combining intrinsic motivation and active imitation (agent chooses when/who/what to imitate), with procedure babbling and local exploration around demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Qualitative results: procedure demonstrations bootstrap learning of highest-level tasks and accelerate learning; demonstrations of simple policies and demonstrations indicating subtasks are most beneficial. No numeric generalization metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not quantified numerically; qualitative claim that social guidance increases learning efficiency for complex tasks and helps overcome limitations of autonomous exploration for deeper hierarchies.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Active imitation combined with intrinsic motivation (SGIM-PB) enables the agent to self-determine who/what/when to imitate, significantly bootstraps learning of high-level hierarchical tasks, and is most beneficial when teachers demonstrate simple policies for simple tasks and procedures/subtasks for complex tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1020.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1020.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CHIME</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CHIME (emergent CHIldlike Meaningful Explorations) hierarchical representation / implementation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An SGIM-SAHT implementation that dynamically discovers tasks/affordances and builds a nested hierarchy of models (affordance control models) through enactive exploration in environments with randomized object properties, enabling planning via nested models to achieve complex goals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CHIME agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A developmental embodied agent that performs enactive exploration in a dynamic task space: it monitors observed outcome subspaces, creates new forward/inverse affordance models when correlations are found, composes nested models (lower models map controllables to outcomes, higher models map outcomes to other outcomes), and plans compound actions by recursive inverse-model calls; learning guided by intrinsic motivation (interest/progress).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>robotic agent (wheeled mobile robot environment, Setup3)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Setup3 (wheeled robot with obstacles and movable objects)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A wheeled-robot arena with red obstacles and green movable objects of randomly generated physical properties (colour, height, diameter) per episode; tasks include avoiding obstacles, moving green objects, and pushing objects using other objects. The environment is stochastic and varied across episodes due to randomized object properties. Tasks and controllable/outcome spaces are not pre-specified and emerge during exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Complexity arises from: (a) emergent hierarchical affordances (depth of nesting), (b) unbounded compound action sequences needed to reach high-level goals, (c) number of objects and obstacles in arena, and (d) the need to plan across nested models (e.g., using wheel commands to change robot pose which then affects object position). Complexity characterized qualitatively by hierarchy depth and sequence length; formal action space is infinite-dimensional A^N.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (dynamic emergent tasks, multi-step planning across nested models, and unbounded action sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>High: per-episode procedural randomization of object physical properties (colour, height, diameter) and random object sizes; tasks and affordances emerge from these variations. Variation measured procedurally (random sampling of object properties) rather than by a numeric variability index.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Competence computed from Euclidean distance between goal and reached outcome; detection/creation of new affordance models based on observed outcomes and whether new data contradicts previous models (competence reduction triggers model adaptation). Planning success in achieving emergent tasks is used as an evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>CHIME explicitly links environment variation to the emergence of new tasks/affordances: randomized object properties lead the agent to discover controllable changes (e.g., moving green object) and then to create nested models that allow planning for more complex tasks. The developmental order observed is from lower-level affordances to higher-level composed tasks; thus variation fuels task emergence while hierarchical composition handles complexity. No quantitative trade-off curves are provided, but qualitative trade-offs are: more variation produces richer affordance discovery but requires the agent to create and manage more models; nested modelling plus planning mitigates complexity by reusing lower-level controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Intrinsic-motivation guided enactive exploration with dynamic model creation (automatic curriculum via emergence), nested-model planning, and recursive inverse-model inference; no reliance on predefined task lists.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Qualitative: CHIME discovered new affordances and used learned affordance-control models to plan complex tasks with known and unknown objects by relying on object physical properties to decide applicability; tasks were learned in developmental order from low to high hierarchy. No numeric generalization scores reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not numerically specified; the paper reports that emergent affordances and reuse of models enable developmentally-ordered learning and reduce dependency on predefined domain knowledge, but does not supply counts of episodes/interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CHIME demonstrates that (a) affordances can emerge through enactive exploration under high environment variation, (b) emergent nested models enable planning to control high-level outcomes via low-level actuators, and (c) tasks are learned in a developmental curriculum (low-level to high-level) driven by intrinsic motivation; environment variation is instrumental to task emergence, while hierarchical nested models manage resulting complexity.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Intrinsically motivated goal exploration processes with automatic curriculum learning <em>(Rating: 2)</em></li>
                <li>R-IAC: Robust intrinsically motivated exploration and active learning <em>(Rating: 2)</em></li>
                <li>Active learning of inverse models with intrinsically motivated goal exploration in robots <em>(Rating: 2)</em></li>
                <li>Emergent structuring of interdependent affordance learning tasks using intrinsic motivation and empirical feature selection <em>(Rating: 2)</em></li>
                <li>Active Choice of Teachers, Learning Strategies and Goals for a Socially Guided Intrinsic Motivation Learner <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1020",
    "paper_id": "paper-232434224",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "SGIM-SAHT",
            "name_full": "Socially Guided Intrinsic Motivation for Sequence of Actions through Hierarchical Tasks",
            "brief_description": "A unified algorithmic architecture that combines intrinsic motivation, goal-directed temporally-abstract compound actions, planning and optional social guidance to learn sequences of primitive actions of unbounded length by discovering and exploiting a hierarchy of tasks/affordances.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SGIM-SAHT agent (framework)",
            "agent_description": "An embodied learning system that selects tasks, goals and exploration strategies via an interest map computed from competence progress; learns forward and inverse models per task, represents task hierarchy H as a directed graph, infers compound actions recursively through inverse models, and can use autonomous exploration and/or active imitation; learning algorithms include intrinsically-motivated exploration, hierarchical inverse/forward modelling and planning (with imitation strategies optional).",
            "agent_type": "robotic agent (implemented in simulation and robot setups described)",
            "environment_name": "Multiple experimental setups (Setup1, Setup2, Setup3) — non‑rewarding stochastic environments with objects and manipulanda",
            "environment_description": "Environments include: Setup1 — a robotic arm with a pen and two joysticks (simulation shown) enabling drawing and controlling a videogame character; Setup2 — robot arm producing sounds by moving objects; Setup3 — a wheeled mobile robot arena with red obstacles and movable green objects of random sizes and physical properties. Environments are continuous, stochastic, have multiple control tasks (outcomes/affordances), and permit compound sequences of primitive actions of unbounded length.",
            "complexity_measure": "Characterized qualitatively and structurally: task hierarchical depth (levels of task composition), required action-sequence length (compound action dimensionality n*p), action parameter dimensionality, number/types of objects and obstacles (e.g., movable objects + obstacles in Setup3). Formally: primitive action a ∈ R^p, compound action of length n described by n*p parameters; action space considered effectively infinite-dimensional A^N. No single scalar numerical complexity metric reported.",
            "complexity_level": "varies (from low for simple primitive tasks to high for composite tasks requiring long action sequences and deeper task hierarchy)",
            "variation_measure": "Environment variation realized via (a) static vs dynamic task sets (static predefined outcome/procedure spaces vs emergent tasks), (b) per-episode randomization of object physical properties (colour, height, diameter) in CHIME/Setup3, (c) stochastic initial states; measured qualitatively by whether task set is fixed or procedurally varied and by randomized object properties per episode. No numeric variation amplitude reported.",
            "variation_level": "varies by implementation: low variation for static-task experiments (IM-PB, SGIM-PB), high variation for CHIME/Setup3 (randomized object properties across episodes)",
            "performance_metric": "Learner competence measured as a reward common to all goals computed from Euclidean distance between goal outcome ω_g and reached outcome ω_r; interest maps use progress (derivative of competence) per region and strategy; additional evaluation via task success in test phase and qualitative measures (e.g., sequence length adaptation, discovery of affordances).",
            "performance_value": null,
            "complexity_variation_relationship": "Paper discusses relationships qualitatively: hierarchical task structure (complexity) enables automatic curriculum learning from simple to complex tasks; increased environment variation (e.g., random object properties in CHIME) drives emergence of new affordances/tasks that become nodes in the hierarchy. Trade-offs discussed: deeper/higher complexity tasks require more exploration of the procedure space (temporal abstraction), while simple tasks are explored in action-parameter space; social demonstrations can bootstrap learning of highest-level (most complex) tasks, mitigating exploration cost. No explicit quantitative trade-off curve provided.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Automatic curriculum learning via intrinsically-motivated goal selection and interest-driven strategy selection; supports autonomous exploration, planning with hierarchical inverse models, and active imitation learning (when available).",
            "generalization_tested": true,
            "generalization_results": "Qualitative generalization reported: CHIME's emergent affordance models were used to plan complex tasks with known and unknown objects (using object physical properties to decide applicability); IM-PB/SGIM-PB used learned procedures and primitives to achieve composite tasks in test phases. Specific numeric generalization metrics are not reported.",
            "sample_efficiency": "Not quantified numerically in paper; qualitative claims that (a) transfer from simple to complex tasks and reuse of subtasks reduces required exploration, and (b) demonstrations bootstrap and accelerate learning for high-level tasks.",
            "key_findings": "SGIM-SAHT provides a unified mechanism to discover task hierarchies and infer curricula; competence-progress interest guides which tasks and strategies to explore; temporally-abstract procedure representations let the agent adapt action-sequence length to task complexity; social guidance (imitation) can significantly bootstrap learning of highest-level tasks; dynamic discovery (CHIME) enables emergence of affordances under varied environments.",
            "uuid": "e1020.0"
        },
        {
            "name_short": "IM-PB",
            "name_full": "Intrinsically Motivated Procedure Babbling",
            "brief_description": "An implementation of SGIM-SAHT for a static, predefined set of controllable/outcome spaces that explores both outcome and procedure (sequence) spaces using intrinsic motivation to discover task dependencies and build curricula from simple to composite tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "IM-PB agent",
            "agent_description": "Embodied robotic learner using intrinsic motivation and two exploration strategies (outcome exploration and procedure exploration); represents procedures as sequences of subgoals (ω1,...,ωn) and learns forward/inverse models for predefined tasks; composes learned subtasks to form compound actions.",
            "agent_type": "robotic agent (arm setups in simulation/experiment: Setup1 and Setup2)",
            "environment_name": "Setup1 and Setup2 (static task sets)",
            "environment_description": "Setup1: robotic arm with pen and two joysticks enabling drawing and joystick control (simulation illustrated). Setup2: robotic arm producing sounds by moving objects. In IM-PB experiments the set of possible outcomes/controllables is predefined (static), so environment variation is low across episodes except for exploration-induced state differences.",
            "complexity_measure": "Task complexity measured by hierarchical composition (levels) and required compound action length (number of subtasks/sequence length); action complexity related to dimensionality n*p. Complexity is examined qualitatively and empirically by observing which tasks need decomposition into procedures.",
            "complexity_level": "spans low (simple primitive tasks) to medium/high (composite tasks composed of subtasks); reported experiments effectively evaluate up to two hierarchy levels with some limitation beyond that.",
            "variation_measure": "Static task set — low environment variation; procedural exploration varies internal state but external instance variation (e.g., object properties) is not a focus in IM-PB experiments.",
            "variation_level": "low",
            "performance_metric": "Competence (Euclidean distance between goal and reached outcome) and interest/progress over regions of outcome space; performance evaluated by ability to reach goals and by adaptation of action-sequence length to task complexity.",
            "performance_value": null,
            "complexity_variation_relationship": "IM-PB shows that when tasks are more complex (higher in hierarchy) the agent shifts exploration toward the procedure space (searching for useful subtasks) rather than primitive action space; reuse of simple tasks as subgoals reduces exploration needed for composite tasks. Because variation is low (static tasks), relationships involving environment variation are not systematically studied in IM-PB.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Intrinsic-motivation driven curriculum learning (automatic goal selection and procedure babbling) within a static predefined task set.",
            "generalization_tested": true,
            "generalization_results": "Qualitative test-phase results: the learner uses the correct task decompositions and adapts action sequence length to task complexity; performance improvements observed especially on high-level tasks relative to baselines that do not use procedures. No numeric generalization scores provided.",
            "sample_efficiency": "Not numerically specified; qualitative claim that reuse of subtasks reduces exploration effort and improves learning efficiency, but adaptation to deeper hierarchies limited (effective up to first two levels).",
            "key_findings": "Intrinsically-motivated procedure babbling enables discovery of task decompositions in a static-task domain, improves performance on composite tasks by reusing subtasks, and adapts action-sequence complexity to task complexity — though adaptation was limited beyond two hierarchy levels.",
            "uuid": "e1020.1"
        },
        {
            "name_short": "SGIM-PB",
            "name_full": "Socially Guided Intrinsic Motivation by Procedure Babbling",
            "brief_description": "An extension of IM-PB that integrates active imitation learning: the agent actively selects when, what and whom to imitate (actions or procedures), combining intrinsic motivation with teacher demonstrations to accelerate learning of hierarchical tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SGIM-PB agent",
            "agent_description": "Embodied learner that augments IM-PB with active imitation strategies: it can request demonstrations of primitive actions or procedures from teachers, add noise for local exploration around demonstrations, and use competence-progress interest to choose between autonomous exploration and imitation and to select which teacher/strategy is most beneficial.",
            "agent_type": "robotic agent (arm setups in simulation/experiment: Setup1 and Setup2)",
            "environment_name": "Setup1 and Setup2 (static task sets with available teachers/demonstrators)",
            "environment_description": "Same as IM-PB: robotic arm tasks (drawing with pen, producing sounds), but with access to teacher demonstrations; environment variation is primarily due to task instances and teacher-provided demonstrations rather than procedural randomization.",
            "complexity_measure": "Task complexity measured by hierarchical depth and required compound action length; social guidance is targeted at higher-complexity tasks (procedures/subtasks).",
            "complexity_level": "low to high, with social guidance especially beneficial for higher-complexity (high) tasks",
            "variation_measure": "Low external environment variation (static task set); variation is introduced by demonstrations from potentially different teachers and by exploration noise added to demonstrations.",
            "variation_level": "low to medium (depending on diversity of teacher demonstrations)",
            "performance_metric": "Competence (Euclidean distance between goal and reached outcome) and interest/progress-based selection; evaluation also includes speed of learning/bootstrapping for high-level tasks.",
            "performance_value": null,
            "complexity_variation_relationship": "Paper reports that demonstrations, especially procedure demonstrations for complex tasks and policy demonstrations for simple tasks, bootstrap learning: social guidance reduces exploration burden for high-complexity tasks. The relationship is qualitative: social imitation mitigates the exploration cost associated with high task complexity; variation due to diverse teachers can be exploited by active selection but quantitative trade-offs are not provided.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Automatic curriculum learning combining intrinsic motivation and active imitation (agent chooses when/who/what to imitate), with procedure babbling and local exploration around demonstrations.",
            "generalization_tested": true,
            "generalization_results": "Qualitative results: procedure demonstrations bootstrap learning of highest-level tasks and accelerate learning; demonstrations of simple policies and demonstrations indicating subtasks are most beneficial. No numeric generalization metrics reported.",
            "sample_efficiency": "Not quantified numerically; qualitative claim that social guidance increases learning efficiency for complex tasks and helps overcome limitations of autonomous exploration for deeper hierarchies.",
            "key_findings": "Active imitation combined with intrinsic motivation (SGIM-PB) enables the agent to self-determine who/what/when to imitate, significantly bootstraps learning of high-level hierarchical tasks, and is most beneficial when teachers demonstrate simple policies for simple tasks and procedures/subtasks for complex tasks.",
            "uuid": "e1020.2"
        },
        {
            "name_short": "CHIME",
            "name_full": "CHIME (emergent CHIldlike Meaningful Explorations) hierarchical representation / implementation",
            "brief_description": "An SGIM-SAHT implementation that dynamically discovers tasks/affordances and builds a nested hierarchy of models (affordance control models) through enactive exploration in environments with randomized object properties, enabling planning via nested models to achieve complex goals.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "CHIME agent",
            "agent_description": "A developmental embodied agent that performs enactive exploration in a dynamic task space: it monitors observed outcome subspaces, creates new forward/inverse affordance models when correlations are found, composes nested models (lower models map controllables to outcomes, higher models map outcomes to other outcomes), and plans compound actions by recursive inverse-model calls; learning guided by intrinsic motivation (interest/progress).",
            "agent_type": "robotic agent (wheeled mobile robot environment, Setup3)",
            "environment_name": "Setup3 (wheeled robot with obstacles and movable objects)",
            "environment_description": "A wheeled-robot arena with red obstacles and green movable objects of randomly generated physical properties (colour, height, diameter) per episode; tasks include avoiding obstacles, moving green objects, and pushing objects using other objects. The environment is stochastic and varied across episodes due to randomized object properties. Tasks and controllable/outcome spaces are not pre-specified and emerge during exploration.",
            "complexity_measure": "Complexity arises from: (a) emergent hierarchical affordances (depth of nesting), (b) unbounded compound action sequences needed to reach high-level goals, (c) number of objects and obstacles in arena, and (d) the need to plan across nested models (e.g., using wheel commands to change robot pose which then affects object position). Complexity characterized qualitatively by hierarchy depth and sequence length; formal action space is infinite-dimensional A^N.",
            "complexity_level": "high (dynamic emergent tasks, multi-step planning across nested models, and unbounded action sequences)",
            "variation_measure": "High: per-episode procedural randomization of object physical properties (colour, height, diameter) and random object sizes; tasks and affordances emerge from these variations. Variation measured procedurally (random sampling of object properties) rather than by a numeric variability index.",
            "variation_level": "high",
            "performance_metric": "Competence computed from Euclidean distance between goal and reached outcome; detection/creation of new affordance models based on observed outcomes and whether new data contradicts previous models (competence reduction triggers model adaptation). Planning success in achieving emergent tasks is used as an evaluation.",
            "performance_value": null,
            "complexity_variation_relationship": "CHIME explicitly links environment variation to the emergence of new tasks/affordances: randomized object properties lead the agent to discover controllable changes (e.g., moving green object) and then to create nested models that allow planning for more complex tasks. The developmental order observed is from lower-level affordances to higher-level composed tasks; thus variation fuels task emergence while hierarchical composition handles complexity. No quantitative trade-off curves are provided, but qualitative trade-offs are: more variation produces richer affordance discovery but requires the agent to create and manage more models; nested modelling plus planning mitigates complexity by reusing lower-level controllers.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Intrinsic-motivation guided enactive exploration with dynamic model creation (automatic curriculum via emergence), nested-model planning, and recursive inverse-model inference; no reliance on predefined task lists.",
            "generalization_tested": true,
            "generalization_results": "Qualitative: CHIME discovered new affordances and used learned affordance-control models to plan complex tasks with known and unknown objects by relying on object physical properties to decide applicability; tasks were learned in developmental order from low to high hierarchy. No numeric generalization scores reported.",
            "sample_efficiency": "Not numerically specified; the paper reports that emergent affordances and reuse of models enable developmentally-ordered learning and reduce dependency on predefined domain knowledge, but does not supply counts of episodes/interactions.",
            "key_findings": "CHIME demonstrates that (a) affordances can emerge through enactive exploration under high environment variation, (b) emergent nested models enable planning to control high-level outcomes via low-level actuators, and (c) tasks are learned in a developmental curriculum (low-level to high-level) driven by intrinsic motivation; environment variation is instrumental to task emergence, while hierarchical nested models manage resulting complexity.",
            "uuid": "e1020.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Intrinsically motivated goal exploration processes with automatic curriculum learning",
            "rating": 2,
            "sanitized_title": "intrinsically_motivated_goal_exploration_processes_with_automatic_curriculum_learning"
        },
        {
            "paper_title": "R-IAC: Robust intrinsically motivated exploration and active learning",
            "rating": 2,
            "sanitized_title": "riac_robust_intrinsically_motivated_exploration_and_active_learning"
        },
        {
            "paper_title": "Active learning of inverse models with intrinsically motivated goal exploration in robots",
            "rating": 2,
            "sanitized_title": "active_learning_of_inverse_models_with_intrinsically_motivated_goal_exploration_in_robots"
        },
        {
            "paper_title": "Emergent structuring of interdependent affordance learning tasks using intrinsic motivation and empirical feature selection",
            "rating": 2,
            "sanitized_title": "emergent_structuring_of_interdependent_affordance_learning_tasks_using_intrinsic_motivation_and_empirical_feature_selection"
        },
        {
            "paper_title": "Active Choice of Teachers, Learning Strategies and Goals for a Socially Guided Intrinsic Motivation Learner",
            "rating": 2,
            "sanitized_title": "active_choice_of_teachers_learning_strategies_and_goals_for_a_socially_guided_intrinsic_motivation_learner"
        }
    ],
    "cost": 0.014569249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Robots Learn Increasingly Complex Tasks with Intrinsic Motivation and Automatic Curriculum Learning
2021</p>
<p>S M Nguyen 
N Duminy 
A Manoury 
Robots Learn Increasingly Complex Tasks with Intrinsic Motivation and Automatic Curriculum Learning</p>
<p>Künstl Intell
35202110.1007/s13218-021-00708-8Received: date / Accepted: dateIntrinsic motivation · Continual learning · Curriculum learning · Transfer learning · Multi-task learning · Hierarchical reinforcement learning
Multi-task learning by robots poses the challenge of the domain knowledge: complexity of tasks, complexity of the actions required, relationship between tasks for transfer learning. We demonstrate that this domain knowledge can be learned to address the challenges in life-long learning. Specifically, the hierarchy between tasks of various complexities is key to infer a curriculum from simple to composite tasks. We propose a framework for robots to learn sequences of actions of unbounded complexity in order to achieve multiple control tasks of various complexity. Our hierarchical reinforcement learning framework, named SGIM-SAHT, offers a new direction of research, and tries to unify partial implementations on robot arms and mobile robots. We outline our contributions to enable robots to map multiple control tasks to sequences of actions: representations of task dependencies, an intrinsically motivated exploration to learn task hierarchies, and active imitation learning. While learning the hierarchy of tasks, it infers its curriculum by deciding which tasks to explore first, how to transfer knowledge, and when, how and whom to imitate.</p>
<p>Introduction</p>
<p>In the mainstream approaches based on classical artificial intelligence and machine learning, robotic engineering approaches have made several valuable application-specific impacts. Yet, the achievements are often subject to restrictions that involve domain knowledge, a bounded and specific environment, or a limited set of tasks of the same complexity.</p>
<p>To face the challenges of multi-task learning in a continual manner for embodied agents interacting with their stochastic environment and with humans, methods have taken inspiration in the living, and especially in how adults and infants learn as they develop, adapt and create new skills all along their lives. These methods fall into the field named cognitive developmental robotics [1,6], within which we examine representations of actions and task relationships, their application to affordance learning and learning methods based on autonomous and socially guided exploration.</p>
<p>Learning Sequences of Motor Policies</p>
<p>In the case of multiple control tasks (ie. induced by actions) with various complexities and dimensionalities, the complexities of the actions required to complete them should be unbounded, without a priori knowledge. If we relate the complexity of actions to their dimensionality, actions of unbounded complexity should belong to spaces of unbounded dimensionality. For instance in setup Fig.1, if an action of dimensionality n is sufficient to draw a letter, a sequence of 2 such actions, i.e. an action of dimensionality 2n is sufficient to draw 2 letters. Generally, texts have variable lengths, thus there is no bound to the length of the sequence of actions. Likewise, an unbounded sequence of actions is needed to play tunes of any length in setup Fig.2; and to move all green objects side by side in setup Fig.3. Hence, here, we consider actions of unbounded complexity and suppose that they can be expressed as a sequence of action primitives. We consider action primitives and sequences of actions, also named in [38] respectively micro actions and compound actions. The agent thus needs to estimate the complexity of the task and deploy actions of corresponding complexity.</p>
<p>To tackle the curse of dimensionality which increases as sequences of actions grow longer, methods such as DQN [24], using a deep neural network architecture have successfully handled high dimensional continuous outcome and context spaces but still with discrete action spaces. Mnih et al. [25] proposed an asynchronous variant of the Actor Critic algorithm, relying both on deep neural networks and gradient policies. It successfully handles continuous action spaces, but still of predefined dimensionality. The options framework proposes a temporally abstract representation of actions [33], leading to sequences of actions for later reuse. Approaches combining options with TD networks enable compositional prediction [34]. Learning simple skills and planning sequences of actions instead of learning a sequence directly has been proposed as Skill chaining [19]. This forward chaining was successfully implemented to generate multistep plans for robot affordance learning [36].</p>
<p>Following the ideas of a temporally abstract representation of actions and of multi-step planning, we propose a goal-directed representation of compound actions and a multistep plan using inverse chaining of self-discovered subtasks.</p>
<p>Task Hierarchy for Curriculum Learning</p>
<p>This requirement of unbounded complexity of actions stems from the objective of learning unknown multiple tasks. Multitask learning in biological agents is progressive and continual. Humans and other species develop and create new skills all along their lives as they adapt to their environment and to their own needs. In particular, infants learn skills of increasing level of difficulty as they grow up: mastering sim-ple skills first and then learning more complex skills based on the previous simple ones. Indeed, in multi-task learning problems, some tasks can be compositions of simpler tasks, which we call 'complex tasks´or 'composite tasks´. The learning agent should be starting small before trying to learn more complex tasks, as phrased in [15]. Devising the order in which tasks should be learned has been coined 'curriculum learning' in [5]: a learning agent needs to decide at each episode both which tasks it wants to learn to control (goal) and which actions to try (means). In our works, we thus take the hypothesis that tasks can be hierarchically related, some may be considered as subtasks of more complex tasks ('hierarchically organised tasks´). We conjecture that this hierarchy can help bootstrap the learning, by transfer of knowledge from simple to complex tasks.</p>
<p>Indeed, when given a task hierarchy, the robot can exploit this domain knowledge to reuse previously acquired skills to build more complex ones for tool use, as shown in [10]. Approaches for hierarchical multi task learning with neural networks have also been proposed, such as Hierarchical DQN [20], that uses intrinsic motivation to train a neural network in a fixed hierarchical manner.</p>
<p>To depend less on domain knowledge, the works presented in this article seek to learn the hierarchy between tasks, by exploring the different combinations between tasks. While learning the dependencies between tasks, we show in [14], that reusing the knowledge of simple tasks as subgoals for more complex tasks indeed greatly reduced the exploration, and in [22], that planning can be used in combination of emerging hierarchical models of tasks.</p>
<p>Emergence of Hierarchical Affordances</p>
<p>An application case of motor learning and task hierarchy is affordances learning. The concept of affordance has been first introduced by Gibson in [17] to characterise physical states in an action-oriented fashion, in terms of the possible interactions an agent may have with objects. Even without knowing an object specifically, seeing visual cues of a handle may suggest possible embodied interactions.</p>
<p>In affordances learning, many approaches have been developed [18]: for instance, the traversability affordance has been studied in different works [23]. Likewise, the grasp affordance is a recurrent topic and various approaches exist to learn it such as learning based on visual descriptors or raw image input [26,21]. However such methods focus on one, or a fixed number of specific affordances, with no mechanism adapting it to new or more complex affordances.</p>
<p>We aim to continual learning of multiple affordances through the interaction with its environment. Thus, the robot builds sensory motor skills using a wide variety of actions. The robot can use actions of unbounded length and duration, in a continuous action space. Ugur and Piater [35] proposed an emergence of a hierarchical structure of affordances. However, affordances were defined as a list of discrete effects on objects, and the actions are manually coded. We would like to tackle continuous features of affordances and be able to learn compound actions in continuous action spaces. We extended their work with intrinsic motivation and planning.</p>
<p>Intrinsic Motivation as an Exploration Heuristic</p>
<p>To allow multi-task learning, developmental methods have transposed into algorithms the notion of intrinsic motivation, that has been outlined as a key mechanism for exploration [9]. These methods use a reward function that is not shaped to fit a specific task but is general to all tasks the robot will face. Tending towards life-long learning, this approach, also called artificial curiosity, may be seen as a particular case of reinforcement learning using an intrinsic reward function.</p>
<p>Methods based on Q-Learning and intrinsic motivation have been proposed in [37] for discrete environments where the reward depends on how much new information have been acquired. Other methods used intrinsic motivation to explore the action space of the robot, based on empirical measures of prediction progress, such as algorithms IAC [31] and its active learning version RIAC [2]. Then Baranes and Oudeyer [3] added goal-babbling to explore the outcome space to address higher dimensional action spaces with the SAGG-RIAC algorithm. More recently, intrinsic motivation and goal babbling have combined deep neural networks and replay mechanisms for automatic curriculum learning of multiple tasks of different complexities. IMGEP [16] -a formalisation of unsupervised multi-goal RL -GEP-GP [7] -mixing evolutionary methods and deep RL -and CURIOUS [8] -mixing parametrised reward function and automated curriculum learning -could select goals in a developmental manner from easy to difficult tasks.</p>
<p>Active Imitation Learning</p>
<p>Methods taking advantage of human demonstrations have shown to tackle more varied and large goal spaces. They were combined with intrinsic motivation such as in [29]. The bootstrapping effect is all the more efficient when the learning robot uses active learning based on intrinsic motivation to choose what to learn, and who, when and how to imitate. This choice on the source of information, has been called a active imitation learning and corresponds to the psychological description of infants' selectivity of social partners in [4]. The SGIM-ACTS algorithm proposed in [28] for multi-task learning has been applied to 3D object recognition [30] or mother tongue imitation by a vocal tract [27].</p>
<p>Summary and Position</p>
<p>Grounding our studies in cognitive developmental robotics, we aim for a robot capable of discovering and learning multiple tasks as well as its curriculum by leveraging the relationships between tasks. In this article, we show that domain knowledge about task relationship and complexity can be learned in order to adapt the complexity of the compound actions (sequences of actions) required. The task relationships between known or emerging tasks can be discovered with intrinsically motivated exploration or active imitation. We propose a common algorithmic architecture based on intrinsically motivated exploration to implement these mechanisms. In the following sections, we present a formalisation of the problem of hierarchical learning, two representations of task hierarchy and a unifying algorithmic architecture with 3 partial implementations [12,14,22]. We show how they address the following questions:</p>
<p>how can knowledge from easier tasks be transferred to complex tasks while the robot learns tasks relationships? how can a robot discover new learnable tasks from which to transfer knowledge to more complex tasks? how can human demonstrations be beneficial to an active learning robot tackling multiple control tasks? Compared to [11], we have made the formalisation more coherent and have updated the implementations and experimental setups taken from [14,22], and position these works better in comparison with existing state-of-the-art.</p>
<p>Hierarchical Learning Framework</p>
<p>In this section, we propose a formalisation of the problem of learning compound actions to achieve hierarchically organised tasks, and propose an algorithmic architecture based on intrinsic motivation to learn the curriculum, common to the algorithms IM-PB, CHIME and SGIM-PB [12,14,22].</p>
<p>Formalisation</p>
<p>Let us consider a robot interacting with a non-rewarding environment by performing sequences of motions of unbounded length in order to induce changes in its surroundings.</p>
<p>Each of these motions is named a primitive action, described by a parametrised function with p parameters: a ∈ A ⊂ R p . We call A the primitive action space. Our robot can perform sequences of primitive actions. Let a compound action be a sequence of any length n ∈ N primitive actions, and be described by n * p parameters : a = [a 1 , . . . , a n ] ∈ A n . Thus the action space exploitable by the robot is a con-
tinuous space of infinite dimensionality A N ⊂ R N .
The actions performed by the robot have consequences on its environment, which we call outcomes ω ∈ Ω, where Ω is a subspace of the state space S defining the control tasks to learn. Once the robot knows how to cause an outcome ω, we say the outcome is then controllable. The set of controllable outcomes is Ω cont ⊂ Ω and this set changes as the robot learns new tasks. For convenience, we define the controllable space C = A∪Ω cont , regrouping both primitive actions A and observables that may be controlled, Ω cont .</p>
<p>The robot learns tasks/models T each mapping controllable c ∈ C T ⊂ C and outcomes ω ∈ Ω T ⊂ Ω within a given context s ∈ S T ⊂ S. More formally, a task is a set of: a forward model M T : (S T , C T ) → Ω T and an inverse model L T : (S T , Ω T ) → C T . The forward model is used to predict the observable consequence ω from a given context s of a controllable c, which is either a primitive action or a goal state that can be induced by a compound action. Conversely, the inverse model is used to estimate a controllablẽ c to be performed in a given context s to induce a goal observable stateω: ifc is a primitive action the robot executes the action, otherwise it setsc as a goal state and infers the necessary actions using other inverse models. We note that this definition of task T assumes that all ω ∈ Ω T can be realised independently from other observables, ie. the task dimensions of Ω T do not interact with the rest of the state space in any way. In our works, we assume that the creation of the tasks (either predefined for IM-PB and SGIM-PB, or emerging for CHIME) ensures this coherence.</p>
<p>These models are trained on the data recorded by the robot along its exploration in its dataset D. We define a strategy σ any process for exploration. For instance, we consider autonomous exploration and imitation learning strategies. Formally, we define it as a data collection heuristics, based on the current data, that outputs a set of triplets of initial state, action and outcome: σ : D → {(s, a, ω)}.</p>
<p>Let us also note H the hierarchy of the models used by our robot. H is formally defined as a directed graph where each node is a task T and its successors are the components of C T . As our robot learns this hierarchy, H varies along time. Its representation is detailed in section 3.</p>
<p>Algorithmic Architecture</p>
<p>We describe a generic algorithm Socially Guided Intrinsic Motivation for Sequence of Actions through Hierarchical Tasks (SGIM-SAHT) that learns and takes advantage of task hierarchy to solve increasingly complex tasks (Fig.4, Alg.1). SGIM-SAHT learns by episodes in which a task T to work on, a goal outcome ω g ∈ Ω T and a strategy σ have been selected to optimize progress and according to an interest map (see below). The selected strategy σ applied to the chosen goal outcome ω g chooses a sequence of controllables l c = [c 1 , . . . , c m ] as a candidate to reach the ω g (Alg.1, l.3).</p>
<p>SGIM-SAHT chooses an adequate task T i , i.e. when the input space of L T i includes (s, ω g ). Then it applies L T i to  l c ← Apply Strategy(σ, ω g ) 4:</p>
<p>D ← (ω r , a, l c ) ← Execute Sequence(l c ) 5:</p>
<p>(comp(ω g ), comp(ω r )) ← Compute Competence(ω g , ω r )) 6:</p>
<p>Update M T , L T , H with (D, comp(ω g ), comp(ω r )) 7:</p>
<p>R i ← Update Outcome and Strategy Interest Map(R, D, ω g ) 8: end loop find the action a i = L T i (c i ). This inference process may be recursive until the output is an action, using the hierarchy between tasks. SGIM-SAHT thus infers from l c a compound action a = [a 1 , . . . , a n ] ∈ A N , to be executed by the robot. The trajectory of the episode with the primitive actions and controllables sequence and goal and reached outcomes ω g , ω r are recorded in the memory (Alg.1, l.4).</p>
<p>Then, it computes the learner's competence on the goal outcome. In the RL framework, this competence can be seen as the reward for the goal outcome. In our multi-task learning setting, we use as competence a reward function common to all goals based on the Euclidean distance between the goal outcome ω g and the reached outcome ω r (Alg.1, l.5). Variations of this metric have been implemented in [14,22].</p>
<p>The memory and competence are used to update the models M T and L T , the set of tasks, and the hierarchy of models H (Alg.1, l.6). These mechanisms differ in our 3 algorithms. Besides, the competence is used to obtain an interest map that associates to each strategy and region of outcome space partition an interest measure to guide the exploration. The interest measure is computed as the progress or derivative of the competence of the enclosed outcomes (details in [28]).</p>
<p>When the number of outcomes added to a region R i exceeds a fixed limit, the region is split into two regions with a clustering boundary that separates outcomes with low from those with high interest (details in [28]).</p>
<p>SGIM-SAHT has three implementations described in the sections 4 and 5. Their differences are described in </p>
<p>Task Hierarchy Representation</p>
<p>In this section, we describe two representations of task hierarchy, called Procedure and CHIME, to be used to learn the domain knowledge about the relationship between tasks.</p>
<p>Procedure Hierarchy Representation</p>
<p>The first is a goal-directed representation of action sequences in the form of sequences of subgoals or task decomposition, that enable transfer of knowledge between inter-related tasks. It is a temporally abstract representation of a succession of actions. More formally, a procedure is defined as a succession of outcomes (ω 1 , ω 2 , ..., ω n ). The succession is unbounded. The procedure space is Ω N . A task decomposition into a procedure is an association of a goal outcome to a procedure. Thus, an outcome represents a task node in H, while the task decomposition represents the directed edges and the procedure is the list of its successors. H is initialised as a densely connected graph, and the exploration prunes the connexions by testing which procedures or task decompositions respect the ground truth. Executing a procedure (ω 1 , ω 1 , ..., ω n ) means building the action sequence a corresponding to the succession of actions a i , i ∈ 1, n (potentially action sequences as well) and execute it (where the a i reach best the ω i ∀i ∈ 1, n respectively). Fig.5 illustrates this idea of task hierarchy.</p>
<p>The procedures representation is based on a static set of (ie. predefined) controllable and outcome spaces.</p>
<p>CHIME Hierarchy Representation</p>
<p>In comparison, the CHIME hierarchical representation is based on a dynamic set of controllable and outcome spaces and the emergence of nested models. It is built using simple models M T : (S T , C i ) → Ω j which can rely on others: lower models map outcomes to actions while higher models map them to other outcomes that should be reached. For instance in the setup3 (Fig.3), the robot can move itself with the model (M 0 : wheelCommand ∈ A → (x 0 , y 0 ) ∈ Ω 0 ) and it can learn that the position (x 1 , y 1 ) of the object pushed depends on its own position (M 1 : (x 0 , y 0 ) ∈ Ω 0 → (x 1 , y 1 ) ∈ Ω 1 ). The combination of the nested models enable the robot to control (x 1 , y 1 ) with its wheel commands (Fig.3 Right ). In the hierarchy of models H, the models M T represent the directed graph from Ω j to C i . At the beginning H = ∅, no model is present, and the robot chooses itself what model to create or modify: if C i seems to be highly correlated to Ω j it may create the model M : C i → Ω j , which becomes the first nodes and edge of H.</p>
<p>Learn Task Relationships by Autonomous Exploration</p>
<p>The idea of SGIM-SAHT is to use a hierarchical representation of the outcome and controllable spaces. This representation outlines the dependencies between tasks in order to reuse previous knowledge and use actions of adapted complexity. In this section, we examine how task hierarchy can be learned in the cases of a static and a dynamic set of tasks.</p>
<p>Learn Task Hierarchy from a Static Set of Tasks</p>
<p>Let us consider that the set of tasks is given. The robot needs to choose which are easier to learn first, which are more difficult, and which tasks can be reused as sub-goals for more difficult models. To learn the hierarchy H between tasks, we proposed in [13] an implementation, IM-PB (Intrinsically Motivated Procedure Babbling) based on procedures and the identification among all possible dependencies, of those that are valid.</p>
<p>IM-PB has 2 strategies : autonomous exploration of the outcome or the procedure spaces. Intrinsic motivation guides the exploration of the the task space and the procedure space (Alg.1, l.2) to find a curriculum from simple to complex tasks. IM-PB takes advantage of the dependencies between tasks : when executing a sequence (Alg.1, l.3), carrying out a procedure (ω 1 , ...ω n ) means carrying out the action primitive sequence π by executing sequentially each component action a i , where a i is an action that reaches ω i ∀i ∈ 1, n .</p>
<p>The experiments on setups of Fig.1,2 in [12,14] show that an intrinsically motivated learner is capable of learning sequences of motor actions. During the learning phase, results show that the robot explores mainly the task space for simple tasks, and it explores considerably more the procedure space for complex hierarchical tasks. Thus it implicitly understands that simple tasks do not need to be decomposed into subtasks and can be reached directly by action primitives. On the contrary for complex tasks, it is more advantageous to seek which subtasks to reuse. During the test phase, the robot uses the correct task decomposition. Furthermore, It can also adapt the length of its action sequence to the task to achieve: the results show that the length of the sequence of actions increases as the complexity of the task increases in terms of its hierarchy. Combining these procedures with the learning of simple actions to complete simple tasks, it can build sequences of actions to achieve complex tasks. We showed in [14] that the robot can take advantage of the procedures representation to improve its performance, especially on high-level tasks. It also adapts the complexity of its action sequence to the complexity of the task at hand.</p>
<p>Nevertheless, this adaptation is limited to the first two levels of task hierarchy, and the learner can not well adapt this complexity to a deeper hierarchy of tasks. To help the robot improve its understanding of task dependencies, we present in section 5 the benefits of active imitation learning.</p>
<p>Learn Task Hierarchy from a Dynamic Set of Tasks</p>
<p>In the previous section, the set of possible tasks (association of inputs and outputs) are given, and the robot needs to learn the relationship between them. In other terms, in H, the nodes are pre-defined and the robot discovers the connections in the graph. In this section, we consider the case where the robot needs to learn both the connections and the nodes, ie. task hierarchy at the same time as task emergence.</p>
<p>Experimental Setup</p>
<p>It was applied to a wheeled robot with obstacles and movable objects of random size (Fig.3). It proposed an emergence of affordances : the algorithm is able to discover learnable models (eg. move a green object), and once the model is learned, its output can be used as input features of more complex models (eg. push an object with a green object), leading to hierarchical learning. In this paper, we describe an affordance as a task T .</p>
<p>CHIME Implementation</p>
<p>Let us consider that we do not have a set of sub-goals given, but have only given a high-dimensional set of inputs and observable outputs. At initialisation, the robot only has access to the control of its wheels, the positions and physical properties of objects. The set of tasks and controllable outcomes is empty. By exploration, it discovers changes in its environments (eg. green object moved) and how it can control them (eg. its own position), but also how these changes can induce more complex outcomes (eg. push an object with another).</p>
<p>The algorithm CHIME [22] implements SGIM-SAHT for a dynamic set of tasks and discovers new tasks by enactive exploration. At each episode, the physical properties such as colour, height, diameter of objects are generated randomly. The main characteristics of CHIME, detailed in [22], lie in its execution of sequence and its update of models and memory (Alg.1, l.4 and 6).</p>
<p>To execute a sequence of controllables l c (Alg.1, l.4), for each element c i :</p>
<p>if c i is a primitive action, it is directly executed if c i is not a primitive action, c i ∈ Ω cont . An affordance T (S T , C T , Ω T ) is then selected (with c i ∈ Ω T ) and its inverse model is applied to obtain the controllable b i = L T (c i ). If c i is out of reach within a timestep, a planning phase is used to build a sequence of element of C A in order to reach c i . The same mechanism is applied recursively on it until having only primitive actions.</p>
<p>To update its models (Alg.1, l.6), at the end of each episode, subspaces of Ω for which outcomes ω has been observed are listed. Then the robot verifies if Ω matches a known affordance. To save computing time, we only verify for randomly selected subspaces of Ω. If it does not match, it creates a new affordance, ie. a forward and an inverse model. If it matches, but the new data contradicts with previous data (the competence for this task is reduced), it tries to update the model by adding context spaces.</p>
<p>Developmental Emergence of Affordances</p>
<p>The results on setup3 (Fig.3) show in [22] that CHIME discovers new affordances, and uses unbounded sequences of learned actions to complete all tasks. Even without predefinition of possible tasks, the agent is able to discover the inputs and outputs of models of learnable tasks, and how they are related to each other. We show in [22] that these tasks emerge and are learned in a developmental order from the lowest to the highest level of hierarchy. Planning based on these emergent tasks enables the robot to infer a sequence of actions to complete complex tasks.</p>
<p>The learning is based on active learning to collect data through new interactions with the environment, guided by the heuristics of intrinsic motivation. Once learned, these affordance control models are used to plan complex tasks with known or unknown objects, by using their physical properties to decide whether a learned affordance may be applied.</p>
<p>Automatic Curriculum Learning</p>
<p>Both IM-PB and CHIME rely on a temporally abstract representation of task relationships, whether all tasks have been predefined in advance or the robot has to discover new tasks. Discovering the task relationship enabled the robot to infer domain knowledge about the complexity of tasks and of the actions needed, but also to build its curriculum, learning simple, then increasingly complex tasks.</p>
<p>Who, What, How to Imitate to Learn Task Relationships</p>
<p>Beyond mere autonomous intrinsically motivated exploration, we show that domain knowledge can also be learned through social guidance. In high dimensional and unbounded task spaces, the performance is improved even more when the robot can imitate actions and procedures from teacher demonstrations, when it actively chooses between self-supervised intrinsic motivation and imitation learning strategies.</p>
<p>For hierarchically organised tasks, we proposed in [14,12] the implementation SGIM-PB (Socially Guided Intrinsic Motivation by Procedure Babbling) that merged IM-PB with SGIM-ACTS. In addition to the strategies σ of IM-PB, SGIM-PB has two strategies per teacher it can interact with : request a demonstration of actions or procedures. With the strategy mimicry of an action, SGIM-PB requests a demonstration of an action to reach ω g . To apply the strategy (Alg.1, l.3), SGIM-PB adds noise to the demonstrated action parameters to explore locally the action parameters space, before executing the action (Alg.1, l.4).</p>
<p>With the strategy mimicry of a procedure, the learner requests a procedure for ω g . SGIM-PB executes the demonstrated procedure (ω di , ω dj ) by adding noise to the parameters, thus exploring locally the procedure space.</p>
<p>Using the competence measures and the interest map, SGIM-PB chooses when imitation learning is more beneficial than autonomous exploration, who among the different teachers are most expert in the field of knowledge it needs at the moment, and what kind of demonstrations is most beneficial. In terms of imitation learning, SGIM-PB selfdetermines who, what and when to imitate. Through setups Fig.1,2, we show in [14,12] that demonstrations of procedures bootstrap the learning for tasks of the highest level of hierarchy. Moreover, demonstrations seem the most beneficial when they are demonstrations of policies for simple tasks, and when they are indications of procedures (i.e. subtasks) for complex tasks.</p>
<p>Conclusion</p>
<p>Through this article, we have presented three implementations of a common algorithmic framework for learning multiple control tasks through curriculum learning by discovering domain knowledge, such as the dependencies between tasks or task and action complexities, and exploiting this hierarchy to transfer knowledge from the easy tasks to the compositional tasks. Table 2 summarises the properties of IM-PB, SGIM-PB and CHIME in learning to per-Algorithm R e w a rd Co nt . go al M ul tita sk H ie ra rc hy A ct io n s Im ita tio n Qlearning [32] Ext. Discrete Qlearning &amp; curiosity [37] Int. Primitive</p>
<p>Option TDNet [34] Ext. Option Skillchaining [19] Ext. Yes Option DQN [24] Ext. Discrete h-DQN [20] Int. Yes Yes Primitive Asynch. Actor Critic [25] Ext. self-eval Primitive GEP-PG [7] Int. Yes Yes CURIOUS [8] Int. Yes Yes Primitive IAC [31], RIAC [2] Int. Yes Yes Primitive SAGG-RIAC [3] Int. Yes Yes Primitive IMGEP [16] Int. Yes Yes Primitive SGIM-ACTS [28] Int. Yes Yes Primitive Yes IM-PB [13,14] Int. Yes Yes Yes Proced. SGIM-PB [12,14] Int.</p>
<p>Yes Yes Yes Proced. Yes CHIME [22] Int. Yes Yes Yes Planning Table 2: Comparison between the algorithms on : intrinsic vs extrinsic reward, the goal space is continuous (parametrised), single task vs multi-task problem, hierarchical learning, the action representation and whether imitation learning is used.</p>
<p>form complex tasks with compound actions, in contrast to the state of the art. While IM-PB and SGIM-PB rely on a static set of controllable and outcome features and explore the dependencies between tasks to learn sequences of actions, CHIME builds dynamically its set of tasks from emergent control models that are then used to plan sequences of actions. Whereas IM-PB and CHIME rely only on autonomous exploration using intrinsic motivation, SGIM-PB can request different kinds of demonstrations depending on the complexity of the target task to the appropriate teacher. All three rely on a temporally abstract representation of compound actions using task hierarchy. They efficiently manage to learn them through the discovery of relationships between tasks to enable transfer of knowledge. We have proposed a framework unifying their common aspects: a temporally abstract representation of the relationship between tasks, learning the hierarchy of tasks with intrinsically and socially guided exploration. This summary opens a theoretical blueprint of a novel framework for robots automatic curriculum learning of uncovering domain knowledge to learn compositional tasks. We showed this can be tackled by combining hierarchical learning, planning and exploration based on intrinsic motivation and active imitation learning.</p>
<p>In future works, we shall develop an implementation of this unified framework using all the described features: learning primitive actions and then planning sequences of them, then once learned, optimising directly these sequences owing to the procedure framework. The emergent subtasks will reduce the dependency on domain knowledge, whereas learning a representation of a compound action will result in better optimised policies and reduce the planning complexity.</p>
<p>Fig. 1 :
1Setup1: a robotic arm, can interact with the different objects in its environment (a pen and two joysticks). Both joysticks enable to control a video-game character. The pen can be used to draw. Left: the simulation environment. Right: the corresponding hierarchy of modelsFig. 2: Setup2: the robot arm can produce sounds by moving the blue and green objects Fig. 3: Setup3: the mobile robot can avoid red obstacles, move green objects, push green objects with other objects. The objects are of random sizes.</p>
<p>Fig. 4 :
4The SGIM-SAHT algorithmic architecture Algorithm 1 SGIM-SAHT Input: the different strategies σ 1 , ..., σ n Input: the initial model hierarchy H Initialization: partition of outcome spaces R ← i {Ω i } Initialization: episodic memory Memory ← ∅ 1: loop 2: σ, T, ω g ← Select Strategy, Task &amp; Goal Outcome(R, H) 3:</p>
<p>Fig. 5 :
5Illustration of a procedure for setupfig.1. To make a drawing ωg between points (xa, ya) and (x b , y b ), a robot can recruit subtasks consisting in (ωi) moving the pen to (xa, ya), then (ωj ) moving the pen to (x b , y b ). These subtasks will be completed respectively with actions ai and aj . To complete this drawing, the learning agent can use the sequence of actions (ai, aj )</p>
<p>table 1 .
1Algo. 
Strategies σ 
Tasks set 
Comp. Act. 
IM-
PB </p>
<p>Outcomes, Procedures 
explo. </p>
<p>Static set 
Procedures </p>
<p>CHIME Action space, outcome 
space explo. </p>
<p>Dynamic 
set 
(emerging tasks) </p>
<p>Planning </p>
<p>SGIM-
PB </p>
<p>Outcomes, Procedures 
explo.; active imitation </p>
<p>Static set 
Procedures </p>
<p>Table 1: Differences between the 3 implementations of SGIM-SAHT </p>
<p>Flowers team, U2IS, ENSTA Paris, Institut Polytechnique de Paris &amp; Inria, France, 2 IMT Atlantique, Brest, France, 3 Université Bretagne Sud, Lorient, France, 4 ENIB, Brest, France, 5 Lab-STICC, UMR 6285, team RAMBO E-mail: nguyensmai@gmail.com
Acknowledgements This work is partially supported by the European Regional Development Fund (ERDF) via the VITAAL CPER, by Institut Mines Telecom (IMT) and by the French Ministry of Research.International Journal of Robotics Research 38(5):518-562Sao Mai Nguyen specialises in robotic learning, especially cognitive developmental learning, reinforcement learning, imitation learning, curriculum learning for robots and human activity recognition. She received her PhD from Inria, an Engineer degree from Ecole Polytechnique and a master's degree from Osaka University, Japan. She has enabled a robot to coach physical rehabilitation in the projects RoKInter and the experiment KERAAL she coordinated, funded by the European Union FP-7 program. She is currently associate editor of the journal IEEE TCDS and cochair of the Task force "Action and Perception" du IEEE Technical Committee on Cognitive and Developmental Systems.Nicolas Duminy holds a master in engineering from IMT Atlantique in France and has obtained in 2018 his PhD in computer science from Université Bretagne Sud in France. His research focused on developmental robotics, and more particularly on the strategic autonomous learning of action sequences and task hierarchies. Today, he is working as an engineer and entrepreneur to develop more immersive virtual reality experiences.Alexandre Manoury hold a master in engineering from IMT Atlantique in France. His research focused on developmental robotics, and more particularly on the strategic autonomous planning of action sequences and task hierarchies.Dominique Duhaut received his PhD degree in computer science from the University Paris 6 in 1982 on The study of complexity of recursive function under the definition of Trahtenbrot. He became assistant professor in 1984 in University Paris 6. In 1987, he moved to the Laboratoire de robotique de Paris where he worked on the cooperation of robots in a flexible cell. In 1996, he was participating to the definition of the RoboCup movement that he organised in Paris in 1998. At that time his research was focused on multi-agents programming in robotics. He moved in university of Bretagne Sud in 2000 where he became professor. His research interests are actually on self reconfigurable systems, teams of robot programming and social aspect of robots. He is also interested in science promotion for young people. He his organising competitions for schools since several years and is participating in the RoboFesta movement.Cedric Buche is a professor at Ecole Nationale d'Ingénieurs de Brest in France. His research focuses on artificial intelligence and human-robot interactions. He is mainly interested in interactive machine learning approaches..
Cognitive developmental robotics as a new paradigm for the design of humanoid robots. M Asada, K F Macdorman, H Ishiguro, Y Kuniyoshi, Robotics and Autonomous Systems. 372-3Asada M, MacDorman KF, Ishiguro H, Kuniyoshi Y (2001) Cog- nitive developmental robotics as a new paradigm for the design of humanoid robots. Robotics and Autonomous Systems 37(2- 3):185-193</p>
<p>R-IAC: Robust intrinsically motivated exploration and active learning. A Baranes, Oudeyer Py, IEEE Transactions on Autonomous Mental Development. 13Baranes A, Oudeyer Py (2009) R-IAC: Robust intrinsically moti- vated exploration and active learning. IEEE Transactions on Au- tonomous Mental Development 1(3):155-169</p>
<p>Active learning of inverse models with intrinsically motivated goal exploration in robots. A Baranes, P Y Oudeyer, Robotics and Autonomous Systems. 611Baranes A, Oudeyer PY (2013) Active learning of inverse models with intrinsically motivated goal exploration in robots. Robotics and Autonomous Systems 61(1):49-73</p>
<p>Active Learning from Infancy to Childhood. K Begus, V Southgate, chap Curious Learners: How Infants' Motivation to Learn Shapes and Is Shaped by Infants' Interactions with the Social World. ChamSpringer International PublishingBegus K, Southgate V (2018) Active Learning from Infancy to Childhood, Springer International Publishing, Cham, chap Curi- ous Learners: How Infants' Motivation to Learn Shapes and Is Shaped by Infants' Interactions with the Social World, pp 13-37</p>
<p>Curriculum learning. Y Bengio, J Louradour, R Collobert, J Weston, International Conference on Machine Learning. New York, NY, USA, ICML '09ACMBengio Y, Louradour J, Collobert R, Weston J (2009) Curricu- lum learning. In: International Conference on Machine Learning, ACM, New York, NY, USA, ICML '09, pp 41-48</p>
<p>Developmental robotics: From babies to robots. A Cangelosi, M Schlesinger, MIT pressCangelosi A, Schlesinger M (2015) Developmental robotics: From babies to robots. MIT press</p>
<p>GEP-PG: Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms. C Colas, O Sigaud, P Y Oudeyer, ICMLStockholm, SwedenColas C, Sigaud O, Oudeyer PY (2018) GEP-PG: Decoupling Ex- ploration and Exploitation in Deep Reinforcement Learning Algo- rithms. In: ICML, Stockholm, Sweden</p>
<p>CURIOUS: Intrinsically motivated modular multi-goal reinforcement learning. C Colas, P Fournier, M Chetouani, O Sigaud, P Y Oudeyer, PMLRInternational Conference on Machine Learning. Long Beach, California, USA97Colas C, Fournier P, Chetouani M, Sigaud O, Oudeyer PY (2019) CURIOUS: Intrinsically motivated modular multi-goal reinforce- ment learning. In: International Conference on Machine Learning, PMLR, Long Beach, California, USA, vol 97, pp 1331-1340</p>
<p>Intrinsic Motivation and selfdetermination in human behavior. E Deci, R M Ryan, Plenum PressNew YorkDeci E, Ryan RM (1985) Intrinsic Motivation and self- determination in human behavior. Plenum Press, New York</p>
<p>Strategic and interactive learning of a hierarchical set of tasks by the Poppy humanoid robot. N Duminy, S M Nguyen, D Duhaut, ICDL-EPIROBDuminy N, Nguyen SM, Duhaut D (2016) Strategic and interac- tive learning of a hierarchical set of tasks by the Poppy humanoid robot. In: ICDL-EPIROB</p>
<p>Learning sequences of policies by using an intrinsically motivated learner and a task hierarchy. N Duminy, A Manoury, S M Nguyen, C Buche, D Duhaut, Workshop on Continual Unsupervised Sensorimotor Learning. Tokyo, JapanDuminy N, Manoury A, Nguyen SM, Buche C, Duhaut D (2018) Learning sequences of policies by using an intrinsically motivated learner and a task hierarchy. In: Workshop on Continual Unsuper- vised Sensorimotor Learning, ICDL-EpiRob, Tokyo, Japan</p>
<p>Effects of social guidance on a robot learning sequences of policies in hierarchical learning. N Duminy, S M Nguyen, D Duhaut, Conference on Systems Man and Cybernetics. IEEEDuminy N, Nguyen SM, Duhaut D (2018) Effects of social guid- ance on a robot learning sequences of policies in hierarchical learning. In: IEEE (ed) International Conference on Systems Man and Cybernetics</p>
<p>Learning a set of interrelated tasks by using sequences of motor policies for a strategic intrinsically motivated learner. N Duminy, S M Nguyen, D Duhaut, IEEE International on Robotic Computing. Duminy N, Nguyen SM, Duhaut D (2018) Learning a set of inter- related tasks by using sequences of motor policies for a strategic intrinsically motivated learner. In: IEEE International on Robotic Computing, pp 288-291</p>
<p>Learning a set of interrelated tasks by using a succession of motor policies for a socially guided intrinsically motivated learner. N Duminy, S M Nguyen, D Duhaut, Frontiers in Neurorobotics. 1287Duminy N, Nguyen SM, Duhaut D (2019) Learning a set of inter- related tasks by using a succession of motor policies for a socially guided intrinsically motivated learner. Frontiers in Neurorobotics 12:87</p>
<p>Learning and development in neural networks: The importance of starting small. J Elman, Cognition. 48Elman J (1993) Learning and development in neural networks: The importance of starting small. Cognition 48:71-99</p>
<p>Intrinsically motivated goal exploration processes with automatic curriculum learning. S Forestier, Y Mollard, P Oudeyer, CoRR abs/1708.02190Forestier S, Mollard Y, Oudeyer P (2017) Intrinsically motivated goal exploration processes with automatic curriculum learning. CoRR abs/1708.02190</p>
<p>The theory of affordances. J Gibson, Perceiving, Acting, and Knowing, Robert Shaw and John BransfordGibson J (1977) The theory of affordances. In: Perceiving, Acting, and Knowing, Robert Shaw and John Bransford</p>
<p>Affordances in psychology, neuroscience, and robotics: A survey. L Jamone, E Ugur, A Cangelosi, L Fadiga, A Bernardino, J Piater, J Santos-Victor, IEEE Transactions on Cognitive and Developmental Systems. 101Jamone L, Ugur E, Cangelosi A, Fadiga L, Bernardino A, Piater J, Santos-Victor J (2016) Affordances in psychology, neuroscience, and robotics: A survey. IEEE Transactions on Cognitive and De- velopmental Systems 10(1):4-25</p>
<p>Skill Discovery in Continuous Reinforcement Learning Domains using Skill Chaining. G Konidaris, A G Barto, Advances in Neural Information Processing Systems pp. Konidaris G, Barto AG (2009) Skill Discovery in Continuous Re- inforcement Learning Domains using Skill Chaining. Advances in Neural Information Processing Systems pp 1015-1023</p>
<p>Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. T D Kulkarni, K Narasimhan, A Saeedi, J Tenenbaum, Advances in neural information processing systems. Kulkarni TD, Narasimhan K, Saeedi A, Tenenbaum J (2016) Hi- erarchical deep reinforcement learning: Integrating temporal ab- straction and intrinsic motivation. In: Advances in neural informa- tion processing systems, pp 3675-3683</p>
<p>Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. S Levine, P Pastor, A Krizhevsky, J Ibarz, D Quillen, The International Journal of Robotics Research. 374-5Levine S, Pastor P, Krizhevsky A, Ibarz J, Quillen D (2018) Learn- ing hand-eye coordination for robotic grasping with deep learn- ing and large-scale data collection. The International Journal of Robotics Research 37(4-5):421-436</p>
<p>Hierarchical affordance discovery using intrinsic motivation. A Manoury, S M Nguyen, C Buche, Human Agent Interaction. Manoury A, Nguyen SM, Buche C (2019) Hierarchical affordance discovery using intrinsic motivation. In: Human Agent Interaction</p>
<p>Staircase traversal via reinforcement learning for active reconfiguration of assistive robots. A Mitriakov, P Papadakis, S M Nguyen, S Garlatti, International Conference on Fuzzy Systems (FUZZ-IEEE). Mitriakov A, Papadakis P, Nguyen SM, Garlatti S (2020) Staircase traversal via reinforcement learning for active reconfiguration of assistive robots. In: International Conference on Fuzzy Systems (FUZZ-IEEE), pp 1-8</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, S Petersen, C Beattie, A Sadik, I Antonoglou, H King, D Kumaran, D Wierstra, S Legg, D Hassabis, Nature. 5187540Mnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J, Belle- mare MG, Graves A, Riedmiller M, Fidjeland AK, Ostrovski G, Petersen S, Beattie C, Sadik A, Antonoglou I, King H, Kumaran D, Wierstra D, Legg S, Hassabis D (2015) Human-level control through deep reinforcement learning. Nature 518(7540):529-533</p>
<p>Asynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T P Lillicrap, T Harley, D Silver, K Kavukcuoglu, CoRR abs/1602.01783Mnih V, Badia AP, Mirza M, Graves A, Lillicrap TP, Harley T, Silver D, Kavukcuoglu K (2016) Asynchronous methods for deep reinforcement learning. CoRR abs/1602.01783</p>
<p>Learning grasping affordances from local visual descriptors. L Montesano, M Lopes, 2009 IEEE 8th International Conference on Development and Learning. Montesano L, Lopes M (2009) Learning grasping affordances from local visual descriptors. In: 2009 IEEE 8th International Conference on Development and Learning, pp 1-6</p>
<p>Selforganization of early vocal development in infants and machines: The role of intrinsic motivation. C Moulin-Frier, S M Nguyen, P Y Oudeyer, Frontiers in Psychology. 4Moulin-Frier C, Nguyen SM, Oudeyer PY (2014) Self- organization of early vocal development in infants and machines: The role of intrinsic motivation. Frontiers in Psychology 4(1006)</p>
<p>Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner. S M Nguyen, P Y Oudeyer, Paladyn Journal of Behavioural Robotics. 33Nguyen SM, Oudeyer PY (2012) Active choice of teachers, learn- ing strategies and goals for a socially guided intrinsic motivation learner. Paladyn Journal of Behavioural Robotics 3(3):136-146</p>
<p>Socially guided intrinsic motivation for robot learning of motor skills. S M Nguyen, P Y Oudeyer, Autonomous Robots. 363Nguyen SM, Oudeyer PY (2014) Socially guided intrinsic mo- tivation for robot learning of motor skills. Autonomous Robots 36(3):273-294</p>
<p>Learning to recognize objects through curiosity-driven manipulation with the icub humanoid robot. S M Nguyen, S Ivaldi, N Lyubova, A Droniou, D Gerardeaux-Viret, D Filliat, V Padois, O Sigaud, P Y Oudeyer, IEEE International Conference on Development and Learning -Epirob. Nguyen SM, Ivaldi S, Lyubova N, Droniou A, Gerardeaux-Viret D, Filliat D, Padois V, Sigaud O, Oudeyer PY (2013) Learning to recognize objects through curiosity-driven manipulation with the icub humanoid robot. In: IEEE International Conference on Development and Learning -Epirob</p>
<p>Intrinsic motivation systems for autonomous mental development. P Y Oudeyer, F Kaplan, V Hafner, IEEE Transactions on Evolutionary Computation. 112Oudeyer PY, Kaplan F, Hafner V (2007) Intrinsic motivation sys- tems for autonomous mental development. IEEE Transactions on Evolutionary Computation 11(2):265-286</p>
<p>Reinforcement Learning: an introduction. R S Sutton, A G Barto, MIT PressSutton RS, Barto AG (1998) Reinforcement Learning: an intro- duction. MIT Press</p>
<p>Between mdps and semimdps: A framework for temporal abstraction in reinforcement learning. R S Sutton, D Precup, S Singh, Artificial Intelligence. 112Sutton RS, Precup D, Singh S (1999) Between mdps and semi- mdps: A framework for temporal abstraction in reinforcement learning. Artificial Intelligence 112:181 -211</p>
<p>Temporal abstraction in temporal-difference networks. R S Sutton, E J Rafols, A Koop, Advances in Neural Information Processing Systems. Weiss Y, Schölkopf B, Platt JMIT Press18Sutton RS, Rafols EJ, Koop A (2006) Temporal abstraction in temporal-difference networks. In: Weiss Y, Schölkopf B, Platt J (eds) Advances in Neural Information Processing Systems, MIT Press, vol 18, pp 1313-1320</p>
<p>Emergent structuring of interdependent affordance learning tasks using intrinsic motivation and empirical feature selection. E Ugur, J Piater, IEEE Transactions on Cognitive and Developmental Systems. Ugur E, Piater J (2016) Emergent structuring of interdependent affordance learning tasks using intrinsic motivation and empirical feature selection. IEEE Transactions on Cognitive and Develop- mental Systems</p>
<p>Affordance learning from range data for multi-step planning. E Ugur, J Piater, E Sahin, E Oztop, International Conference on Epigenetic Robotics. Ugur E, Piater J, Sahin E, Oztop E (2009) Affordance learning from range data for multi-step planning. In: International Confer- ence on Epigenetic Robotics</p>
<p>Intrinsically Motivated Hierarchical Skill Learning in Structured Environments. C Vigorito, A Barto, IEEE Transactions on Autonomous Mental Development. 22Vigorito C, Barto A (2010) Intrinsically Motivated Hierarchical Skill Learning in Structured Environments. IEEE Transactions on Autonomous Mental Development 2(2):132-143</p>
<p>Action representations in robotics: A taxonomy and systematic classification. P Zech, E Renaudo, S Haller, X Zhang, J Piater, Zech P, Renaudo E, Haller S, Zhang X, Piater J (2019) Action rep- resentations in robotics: A taxonomy and systematic classification.</p>            </div>
        </div>

    </div>
</body>
</html>
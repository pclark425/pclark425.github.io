<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7789 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7789</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7789</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-272826664</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.14037v1.pdf" target="_blank">Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) and AI assistants driven by these models are experiencing exponential growth in usage among both expert and amateur users. In this work, we focus on evaluating the reliability of current LLMs as science communicators. Unlike existing benchmarks, our approach emphasizes assessing these models on scientific question-answering tasks that require a nuanced understanding and awareness of answerability. We introduce a novel dataset, SCiPS-QA, comprising 742 Yes/No queries embedded in complex scientific concepts, along with a benchmarking suite that evaluates LLMs for correctness and consistency across various criteria. We benchmark three proprietary LLMs from the OpenAI GPT family and 13 open-access LLMs from the Meta Llama-2, Llama-3, and Mistral families. While most open-access models significantly underperform compared to GPT-4 Turbo, our experiments identify Llama-3-70B as a strong competitor, often surpassing GPT-4 Turbo in various evaluation aspects. We also find that even the GPT models exhibit a general incompetence in reliably verifying LLM responses. Moreover, we observe an alarming trend where human evaluators are deceived by incorrect responses from GPT-4 Turbo.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7789.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7789.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCiPS-QA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Specially Challenging Problems in Science - Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A novel benchmark dataset introduced in this paper consisting of 742 expert-level Yes/No (boolean) scientific queries across multiple scientific domains (Physics, Chemistry, Mathematics, Theoretical CS, Astronomy, Biology, Economics), containing both closed (answerable) and open (currently open research) questions designed to test nuance, answerability/abstention, and long-form reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>evaluated across many LLMs (GPT-4 Turbo, GPT-3.5 Turbo, text-davinci-003, meta-llama-2/3, Mistral variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (7B, 13B, 70B, GPT family sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain: Physics, Chemistry, Mathematics, Theoretical Computer Science, Astronomy, Biology, Economics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>boolean scientific question-answering and long-form reasoning explanations</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>SCiPS-QA benchmark (dataset + evaluation suite)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use the SCiPS-QA collection of 742 boolean science problems to evaluate LLM correctness on closed questions, abstention on open questions, and the quality/faithfulness of generated reasoning passages under multiple decoding settings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>MACC, MSACC, VSR, CMACC, CMSACC, OMACC, OMSACC, hallucination scores, NLG verification attributes (convince-factor, factuality, information-mismatch), invalid response rate</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>MACC: accuracy of main (temp=0) responses; MSACC: majority accuracy across 10 stochastic samples (temp=1); VSR: standard deviation of mapped categorical answers across stochastic samples (A→1,B→2,C→3); CMACC/CMSACC: MACC/MSACC on closed subset; OMACC/OMSACC: MACC/MSACC on open subset (measures abstention); hallucination: SelfCheckGPT [0–1]; NLG attributes: 1–5 scale.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>here (SCiPS-QA)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human evaluation on subset: 30 query+main-response combinations sampled per subject (Physics, Chemistry, Mathematics, Computer Science); two human evaluators per subject (both graduate degree holders, male, ages 20–25). Evaluators provided convince-factor scores (1–5) in two conditions: reasoning+answer and reasoning-only.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Overall: MACC best: meta-llama-3-70B 0.693, GPT-4 Turbo 0.646; MSACC best: GPT-4 Turbo 0.651, Llama-3-70B-instruct 0.623; OMACC/OMSACC (abstention) low for most models; Llama-3-70B top OMACC 0.582 and OMSACC 0.487. (See Table 2 in paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Dataset uses boolean format which may not capture all forms of scientific argument; lacks golden expert reasoning passages (authors recommend augmenting with expert gold explanations); some queries lie outside knowledge cutoffs of evaluated models; human evaluators were not necessarily highly experienced domain experts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7789.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7789.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MACC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Main Response Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An accuracy metric defined in this paper that measures the correctness of model answers generated deterministically (temperature = 0.0) on SCiPS-QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (GPT-4 Turbo, GPT-3.5 Turbo, meta-llama variants, Mistral variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain (as per SCiPS-QA)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>boolean answers and explanatory reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Main Response Accuracy (MACC)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute fraction of correct Yes/No answers when model is decoded deterministically (temperature = 0.0).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (proportion correct between 0 and 1)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>MACC = (# correct main responses at temp=0) / (total queries); unit: proportion in [0,1].</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SCiPS-QA (here)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Not applicable to MACC itself (automated correctness label vs. established ground truth in SCiPS-QA).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Example: MACC: meta-llama-3-70B = 0.693; GPT-4 Turbo = 0.646 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>MACC reflects only deterministic decoding; does not evaluate stability across sampling or quality of reasoning explanations; correctness depends on curated ground-truth labels which may be ambiguous for open problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7789.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7789.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MSACC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Major Stochastic Response Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Accuracy computed from the majority vote among multiple stochastic decodings (10 samples at temperature = 1.0) for each query, used to assess typical sampled answer under a high-variance regime.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (GPT-4 Turbo, GPT-3.5 Turbo, Llama families, Mistral)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain (SCiPS-QA)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>boolean answers / answer-abstention behavior</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Major Stochastic Response Accuracy (MSACC)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Sample N=10 stochastic responses at temperature=1.0 for each query, take the majority categorical answer and measure whether that majority is correct.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy of majority response (proportion correct)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>MSACC = (# queries where majority of 10 stochastic responses equals ground truth) / (total queries); unit: proportion in [0,1].</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SCiPS-QA (here)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Not directly human-judged; used for automated robustness/consistency evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>MSACC: GPT-4 Turbo = 0.651 (best reported), Llama-3-70B-instruct = 0.623 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Majority aggregation can mask multimodal belief states; sensitive to number of samples and temperature setting; invalid stochastic responses treated as incorrect which can bias results for smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7789.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7789.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VSR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variation in Stochastic Responses</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A consistency metric measuring the variability in categorical stochastic outputs (mapped to numeric values) across N stochastic samples; lower values indicate greater consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain (SCiPS-QA)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>consistency of boolean answers and related reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Variation in Stochastic Responses (VSR)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Map categorical choices A/B/C to numeric values (A→1,B→2,C→3) across 10 stochastic samples at temp=1, compute the standard deviation for each query and report aggregate statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Standard deviation of mapped responses (lower is better)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>VSR = stddev({mapped responses across N stochastic samples}) averaged across dataset; unit: numeric standard deviation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SCiPS-QA (here)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>GPT-4 Turbo reported lowest VSR of 0.193 (indicating high consistency); Llama-3-70B-instruct had VSR ≈ 0.295 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Mapping of invalid responses to a fixed numeric value (3) can distort variance; VSR captures only surface categorical variability, not deeper semantic variability in reasoning passages.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7789.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7789.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfCheckGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SelfCheckGPT (general)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sampling-based, zero-resource, black-box hallucination detection framework that compares a main response to multiple stochastic samples to assign hallucination scores between 0 (no hallucination) and 1 (full hallucination).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied to proprietary LLM outputs (GPT-4 Turbo, GPT-3.5 Turbo, text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT family (proprietary sizes/versions used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain (SCiPS-QA responses)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hallucination quantification for reasoning passages</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>SelfCheckGPT (sampling-based hallucination scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Collect a deterministic main response (temp=0) and N stochastic responses (temp=1); compute sentence-level similarity/consistency between main and stochastic samples under several variants (BERTScore, NLI, or external LLM prompt) and average per-sentence scores to produce a [0,1] hallucination score for the main response.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Hallucination score in [0,1]; lower is better (0=no hallucination, 1=full hallucination)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>H(M) = mean_i H(M_i) where H(M_i) is computed per sentence via variant-specific functions (complements of BERTScore similarity; NLI contradiction probability; external LLM support mapping).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>used on SCiPS-QA reasoning passages</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Not human evaluation; variants use external LLM (GPT-3.5 Turbo) as a scorer for one variant.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Using BERTScore variant, GPT-3.5 Turbo had the lowest mean hallucination score; GPT-4 Turbo and GPT-3.5 Turbo were not conclusively demarcated by SelfCheckGPT in many cases (paper reports failure to detect many incorrect GPT-4 Turbo responses).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>SelfCheckGPT variants failed to reliably detect incorrect reasoning for powerful LLMs (notably GPT-4 Turbo); effectiveness depends on choice of sentence embeddings, NLI model, or external LLM prompt; assumes stochastic samples reflect alternative model beliefs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7789.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7789.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfCheckGPT-BERTScore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SelfCheckGPT with BERTScore (sentence-embedding similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variant of SelfCheckGPT that uses sentence-level semantic similarity (via sentence-transformer embeddings and dot-product BERTScore) between main response sentences and stochastic response sentences to compute hallucination as 1 - max_similarity averaged across stochastic samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied to proprietary LLM reasoning passages (GPT-4 Turbo, GPT-3.5 Turbo, text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary GPT family</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain (SCiPS-QA)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hallucination detection for explanatory text</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>SelfCheckGPT with BERTScore</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each sentence in main response, compute maximum semantic similarity to sentences across each stochastic sample using sentence-transformer embeddings (they used all-MiniLM-L6-v2 and all-mpnet-base-v2), then assign hallucination score = 1 - average_max_similarity over N samples.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Hallucination score in [0,1]; lower=more faithfulness</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>H(M_i) = 1 - (1/N) * sum_k max_j B(M_i, S_kj); aggregate by averaging H(M_i) across sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SCiPS-QA main and stochastic reasoning passages</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>None (automated metric).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Using this variant, authors observed roughly normal distributions; GPT-3.5 Turbo achieved lowest mean hallucination score, text-davinci-003 highest; Welch's t-tests performed to compare means.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Dependent on embedding model choice; semantic similarity may fail to capture factual errors that are lexically similar; cannot reliably detect incorrect yet consistent-sounding reasoning from powerful LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7789.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7789.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfCheckGPT-NLI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SelfCheckGPT with Natural Language Inference</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variant of SelfCheckGPT that uses an NLI model (DeBERTa-v3-base fine-tuned on MNLI) to estimate the degree of contradiction between a main-response sentence and stochastic samples; uses contradiction logits to compute per-sentence hallucination probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied to proprietary LLM reasoning passages</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary GPT family</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain (SCiPS-QA)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>contradiction-based hallucination scoring</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>SelfCheckGPT with NLI</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Treat stochastic response sample as premise and main response sentence as hypothesis; compute logits for 'contradiction' and 'entailment', derive P(contradiction) = exp(z_c)/(exp(z_e)+exp(z_c)) and average across samples to get H(M_i).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Hallucination score in [0,1] derived from NLI contradiction probability</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>H(M_i) = (1/N) * sum_k P(contradiction | M_i, S_k) where probabilities derived from NLI logits.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SCiPS-QA</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>None (automated).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Authors report that this NLI variant did not clearly demarcate hallucination amounts for GPT-3.5 and GPT-4 (text-davinci-003 showed higher hallucination).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>NLI classifiers trained on general-domain MNLI can be brittle on long scientific sentences and may misinterpret entailment/contradiction in domain-specialized reasoning; not reliable for strong LLMs per reported findings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7789.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7789.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfCheckGPT-Prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SelfCheckGPT with External LLM Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variant of SelfCheckGPT that uses an external LLM (GPT-3.5 Turbo) prompted to judge whether each main-response sentence is supported by each stochastic sample, mapping Yes/No/NA to numeric hallucination scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>external evaluator: GPT-3.5 Turbo; evaluated models: GPT-4 Turbo/GPT-3.5 Turbo/text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary GPT family</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>LM-as-evaluator prompt-based hallucination scoring</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>SelfCheckGPT with Prompt (external LLM evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompt GPT-3.5 Turbo to assess whether each sentence in the main response is supported by each stochastic sample; map outputs Yes→0, No→1, NA→0.5, and average to produce per-sentence hallucination scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Hallucination score in [0,1] computed via external LLM judgments</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>H(M_i) = (1/N) * sum_k x_k_i where x_k_i ∈ {0.0,0.5,1.0} mapped from GPT-3.5 Turbo outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SCiPS-QA</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>None for metric itself, but the same prompts are also used in LLM verification experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Authors report GPT-3.5 Turbo assigned a higher count of low-hallucination scores than GPT-4 Turbo in this variant (Figure 8).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Relies on external LLM evaluator which may itself be biased or worse at verification; mapping of outputs to coarse scores loses nuance; reported to be ineffective at flagging many incorrect high-quality-sounding GPT-4 Turbo responses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7789.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7789.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Convince-factor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Convince-factor (with-answer / without-answer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human/LLM-judged attribute scoring how convincing the reasoning passage is, reported on a 1–5 linear scale; computed in two variants: with the model's final answer included and without the answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>verifier models used: GPT-3.5 Turbo and GPT-4 Turbo; human evaluators also used</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary GPT versions</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain (SCiPS-QA explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>persuasiveness / rhetorical quality of explanations</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Convince-factor (with-answer / without-answer)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompt verifier (LLM or human) to score how convincing the provided reasoning is on a 1–5 scale; compare scores when the asserted answer is included vs omitted to study answer's influence on perceived convincingness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Convincingness score on integer scale 1–5 (higher = more convincing)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Single integer score assigned per reasoning passage by evaluator; aggregated distributions reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SCiPS-QA subset reasoning passages</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human evaluators: two per subject, 30 query+main-response combos per subject; half see answer+reasoning, half see reasoning-only; scores 1–5 recorded.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>LLM verifiers (GPT-3.5 and GPT-4 Turbo) frequently rated incorrect reasoning highly; GPT-4 Turbo tended to give an even higher fraction of perfect convincingness scores (5) to incorrect passages than GPT-3.5. Humans also were deceived by convincing incorrect responses, and inclusion of the answer increased convincingness for incorrect items (Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Convince-factor conflates persuasiveness with factual correctness; evaluators (human or LLM) can be misled by confident-sounding but incorrect reasoning; inter-rater agreement not reported explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7789.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7789.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fact-check (NLG evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fact-check scoring of reasoning passages</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluator-assigned factuality score (1–5) that measures how factually accurate a generated reasoning passage is relative to world knowledge or provided sources, assigned by verifier LLMs and human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>evaluators: GPT-3.5 Turbo, GPT-4 Turbo; evaluated models: GPT-4 Turbo, GPT-3.5 Turbo, text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary sizes/versions</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>factual accuracy assessment of explanations</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Fact-check (1–5 scale)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Provide the reasoning passage to an evaluator (LLM or human) and ask them to score its factuality from 1 (not factually correct) to 5 (fully factually correct).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Factuality score on integer scale 1–5</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Single integer value assigned per passage; aggregated distributions compared between correct and incorrect passages.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SCiPS-QA reasoning passages</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>LLM verifiers and human evaluators used; humans rated convince-factor (not explicitly fact-check in human step), LLMs provided factuality 1–5.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>GPT-3.5 Turbo assigned high factuality scores even to many incorrect passages; GPT-4 Turbo performed worse at verifying its own passages, often assigning high factuality to incorrect outputs (Figure 3 / Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Verifier LLMs often cannot distinguish factually incorrect reasoning from correct reasoning; factuality scoring is subjective and dependent on verifier's own knowledge and biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7789.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7789.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Information-Mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Information Mismatch score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 1–5 scale metric assessing the amount of information mismatch between a main response reasoning passage and each of the stochastic response passages; lower indicates less mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>evaluators: GPT-3.5 Turbo, GPT-4 Turbo; evaluated models: proprietary LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>consistency / content divergence measurement</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Information-Mismatch (1–5)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compare the main-response reasoning passage with each stochastic sample and assign a score 1 (very low mismatch) to 5 (very high mismatch); average across stochastic samples to obtain the passage-level information-mismatch score.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Information-mismatch score on 1–5 scale</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Mean of per-stochastic-sample mismatch scores (1–5) for a given main response.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SCiPS-QA</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Measured via LLM verifiers; human matching not explicitly reported for this metric.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>GPT-3.5 Turbo assigned relatively high information-mismatch to main responses from all three proprietary models; GPT-4 Turbo gave very low mismatch scores to its own passages, irrespective of correctness (Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Information-mismatch is insensitive to correctness (authors observed verifier scores were agnostic to correctness); subjective scoring by LLM verifiers may not align with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7789.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7789.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Evaluation Protocol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human evaluation of convincingness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A controlled human evaluation setup where graduate-educated domain evaluators scored the convincingness of GPT-4 Turbo reasoning passages (with/without answer) on a 1–5 scale to measure susceptibility of humans to be misled by incorrect LLM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>evaluated content: GPT-4 Turbo generated reasoning passages</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4 Turbo (proprietary variant used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Physics, Chemistry, Mathematics, Computer Science (subset used for human eval)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>assessment of persuasiveness and perceived correctness of LLM explanations</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human convince-factor scoring (with/without answer)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Two human evaluator groups per subject (2 evaluators per subject) scored randomly selected 30 query+main-response combos per subject on convincingness (1–5). One group saw both answer and reasoning, other saw reasoning-only.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Convincingness score 1–5 assigned by humans; distribution of scores for correct vs erroneous passages reported.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Integer score 1 (not convinced) to 5 (very convinced) per passage; aggregated distributions and counts reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>subset of SCiPS-QA (30 per subject sampled)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Four subjects evaluated (Physics, Chemistry, Mathematics, Computer Science); two human evaluators per subject; each evaluator scored 30 query-response pairs; evaluators had ≥ graduate degree, male, age 20–25; informed consent obtained.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Humans assigned higher convincingness to correct responses overall, but a significant fraction of incorrect LLM responses still received convincing scores >3; showing humans can be deceived especially when answer is shown (Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Evaluators were not necessarily highly experienced domain experts (authors note this limitation); small number of evaluators per subject; potential demographic bias (all male, 20–25). Inter-rater agreement not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7789.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e7789.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stochastic sampling protocol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Main vs Stochastic Response Collection Protocol (temperature = 0.0 vs 1.0, N=10)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Decoding protocol used throughout experiments: collect a deterministic main response at temperature 0.0 and collect N=10 stochastic responses at temperature 1.0 for variability, hallucination, and consistency analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies to all evaluated LLMs (open-source and proprietary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain (SCiPS-QA)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>method for generating and comparing multiple model outputs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Main vs Stochastic response sampling (temp 0 vs temp 1, N=10)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generate a single deterministic 'main' response with temperature 0.0 and 10 stochastic responses with temperature 1.0 for each prompt; use both sets to compute MACC, MSACC, VSR, hallucination scores, and NLG verification attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Used to derive MACC, MSACC, VSR, SelfCheckGPT scores, and invalid-response statistics</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Parameters: main response (temp=0), stochastic samples N=10 (temp=1); mapping of categorical answers A/B/C and mapping invalid responses to incorrect in MSACC.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SCiPS-QA</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Reasoning passages were collected from main responses (temp=0) for human scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Protocol enabled computation of VSR (consistency) and MSACC; invalid-response rates varied by model (small models had many invalid responses; GPT-4 Turbo had nearly zero invalid main responses). See Table 5.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Choice of temperature and sample count strongly affects variance estimates; invalid-response handling (treated as incorrect) can penalize small models disproportionately.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7789.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e7789.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Invalid-response rate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invalid response proportion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple metric counting the fraction of responses (main or stochastic) that cannot be parsed into one of the expected categorical outputs (A:Yes, B:No, C:I do not know), used as a quality/error indicator across models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>open-source and proprietary models evaluated (various)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain (SCiPS-QA)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>answer-format validity assessment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Invalid-response rate</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Count responses that are unparsable or empty relative to expected choices and report as fraction of total responses collected (both main and stochastic).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Proportion of invalid responses (0–1) or percentage</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Invalid rate = (# invalid responses) / (total responses) reported separately for main and stochastic responses.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SCiPS-QA</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Not human-evaluated; automated parsing criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Low-scale models (meta-llama-2-7B, Mistral-7B-Instruct-v0.1, etc.) had high invalid rates; GPT-4 Turbo reported near-zero invalid main responses (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Binary treatment (invalid → incorrect) may overweight small-model weaknesses; does not capture partially valid but malformed responses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7789.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e7789.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Welch's t-test</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Welch's t-test for mean differences</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard statistical hypothesis test used in the paper to test for significant differences in mean hallucination scores (assumes unequal variances) between pairs of proprietary models under SelfCheckGPT-BERTScore.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied to hallucination score distributions for GPT-4 Turbo, GPT-3.5 Turbo, text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary GPT family</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>statistical analysis of LLM evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>statistical significance testing</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Welch's t-test</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Assume approximate normality and independence of hallucination-score samples; compute Welch's t-statistic for difference in means between model pairs and report p-values (α=0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>p-value and degrees of freedom; decision to reject null hypothesis or not</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Two-sample t-test statistic and associated p-value; degrees of freedom computed per Welch's formula.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>applied to SelfCheckGPT-BERTScore hallucination scores on SCiPS-QA responses</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Authors report failing to reject/no-difference in some comparisons at 95% confidence for some pairs and rejecting for others; detailed results presented in Table 6 (example p-values shown).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Relies on assumption of normality and independence which may not fully hold; multiple comparisons correction not explicitly discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. <em>(Rating: 2)</em></li>
                <li>Measuring massive multitask language understanding <em>(Rating: 2)</em></li>
                <li>Measuring mathematical problem solving with the math dataset <em>(Rating: 1)</em></li>
                <li>ScienceQA <em>(Rating: 1)</em></li>
                <li>GSM8K <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7789",
    "paper_id": "paper-272826664",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "SCiPS-QA",
            "name_full": "Specially Challenging Problems in Science - Question Answering",
            "brief_description": "A novel benchmark dataset introduced in this paper consisting of 742 expert-level Yes/No (boolean) scientific queries across multiple scientific domains (Physics, Chemistry, Mathematics, Theoretical CS, Astronomy, Biology, Economics), containing both closed (answerable) and open (currently open research) questions designed to test nuance, answerability/abstention, and long-form reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "evaluated across many LLMs (GPT-4 Turbo, GPT-3.5 Turbo, text-davinci-003, meta-llama-2/3, Mistral variants)",
            "model_size": "various (7B, 13B, 70B, GPT family sizes)",
            "scientific_domain": "multi-domain: Physics, Chemistry, Mathematics, Theoretical Computer Science, Astronomy, Biology, Economics",
            "theory_type": "boolean scientific question-answering and long-form reasoning explanations",
            "evaluation_method_name": "SCiPS-QA benchmark (dataset + evaluation suite)",
            "evaluation_method_description": "Use the SCiPS-QA collection of 742 boolean science problems to evaluate LLM correctness on closed questions, abstention on open questions, and the quality/faithfulness of generated reasoning passages under multiple decoding settings.",
            "evaluation_metric": "MACC, MSACC, VSR, CMACC, CMSACC, OMACC, OMSACC, hallucination scores, NLG verification attributes (convince-factor, factuality, information-mismatch), invalid response rate",
            "metric_definition": "MACC: accuracy of main (temp=0) responses; MSACC: majority accuracy across 10 stochastic samples (temp=1); VSR: standard deviation of mapped categorical answers across stochastic samples (A→1,B→2,C→3); CMACC/CMSACC: MACC/MSACC on closed subset; OMACC/OMSACC: MACC/MSACC on open subset (measures abstention); hallucination: SelfCheckGPT [0–1]; NLG attributes: 1–5 scale.",
            "dataset_or_benchmark": "here (SCiPS-QA)",
            "human_evaluation_details": "Human evaluation on subset: 30 query+main-response combinations sampled per subject (Physics, Chemistry, Mathematics, Computer Science); two human evaluators per subject (both graduate degree holders, male, ages 20–25). Evaluators provided convince-factor scores (1–5) in two conditions: reasoning+answer and reasoning-only.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Overall: MACC best: meta-llama-3-70B 0.693, GPT-4 Turbo 0.646; MSACC best: GPT-4 Turbo 0.651, Llama-3-70B-instruct 0.623; OMACC/OMSACC (abstention) low for most models; Llama-3-70B top OMACC 0.582 and OMSACC 0.487. (See Table 2 in paper.)",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Dataset uses boolean format which may not capture all forms of scientific argument; lacks golden expert reasoning passages (authors recommend augmenting with expert gold explanations); some queries lie outside knowledge cutoffs of evaluated models; human evaluators were not necessarily highly experienced domain experts.",
            "uuid": "e7789.0",
            "source_info": {
                "paper_title": "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "MACC",
            "name_full": "Main Response Accuracy",
            "brief_description": "An accuracy metric defined in this paper that measures the correctness of model answers generated deterministically (temperature = 0.0) on SCiPS-QA.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various LLMs (GPT-4 Turbo, GPT-3.5 Turbo, meta-llama variants, Mistral variants)",
            "model_size": "various",
            "scientific_domain": "multi-domain (as per SCiPS-QA)",
            "theory_type": "boolean answers and explanatory reasoning",
            "evaluation_method_name": "Main Response Accuracy (MACC)",
            "evaluation_method_description": "Compute fraction of correct Yes/No answers when model is decoded deterministically (temperature = 0.0).",
            "evaluation_metric": "Accuracy (proportion correct between 0 and 1)",
            "metric_definition": "MACC = (# correct main responses at temp=0) / (total queries); unit: proportion in [0,1].",
            "dataset_or_benchmark": "SCiPS-QA (here)",
            "human_evaluation_details": "Not applicable to MACC itself (automated correctness label vs. established ground truth in SCiPS-QA).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Example: MACC: meta-llama-3-70B = 0.693; GPT-4 Turbo = 0.646 (Table 2).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "MACC reflects only deterministic decoding; does not evaluate stability across sampling or quality of reasoning explanations; correctness depends on curated ground-truth labels which may be ambiguous for open problems.",
            "uuid": "e7789.1",
            "source_info": {
                "paper_title": "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "MSACC",
            "name_full": "Major Stochastic Response Accuracy",
            "brief_description": "Accuracy computed from the majority vote among multiple stochastic decodings (10 samples at temperature = 1.0) for each query, used to assess typical sampled answer under a high-variance regime.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various LLMs (GPT-4 Turbo, GPT-3.5 Turbo, Llama families, Mistral)",
            "model_size": "various",
            "scientific_domain": "multi-domain (SCiPS-QA)",
            "theory_type": "boolean answers / answer-abstention behavior",
            "evaluation_method_name": "Major Stochastic Response Accuracy (MSACC)",
            "evaluation_method_description": "Sample N=10 stochastic responses at temperature=1.0 for each query, take the majority categorical answer and measure whether that majority is correct.",
            "evaluation_metric": "Accuracy of majority response (proportion correct)",
            "metric_definition": "MSACC = (# queries where majority of 10 stochastic responses equals ground truth) / (total queries); unit: proportion in [0,1].",
            "dataset_or_benchmark": "SCiPS-QA (here)",
            "human_evaluation_details": "Not directly human-judged; used for automated robustness/consistency evaluation.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "MSACC: GPT-4 Turbo = 0.651 (best reported), Llama-3-70B-instruct = 0.623 (Table 2).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Majority aggregation can mask multimodal belief states; sensitive to number of samples and temperature setting; invalid stochastic responses treated as incorrect which can bias results for smaller models.",
            "uuid": "e7789.2",
            "source_info": {
                "paper_title": "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "VSR",
            "name_full": "Variation in Stochastic Responses",
            "brief_description": "A consistency metric measuring the variability in categorical stochastic outputs (mapped to numeric values) across N stochastic samples; lower values indicate greater consistency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various LLMs evaluated",
            "model_size": "various",
            "scientific_domain": "multi-domain (SCiPS-QA)",
            "theory_type": "consistency of boolean answers and related reasoning",
            "evaluation_method_name": "Variation in Stochastic Responses (VSR)",
            "evaluation_method_description": "Map categorical choices A/B/C to numeric values (A→1,B→2,C→3) across 10 stochastic samples at temp=1, compute the standard deviation for each query and report aggregate statistics.",
            "evaluation_metric": "Standard deviation of mapped responses (lower is better)",
            "metric_definition": "VSR = stddev({mapped responses across N stochastic samples}) averaged across dataset; unit: numeric standard deviation.",
            "dataset_or_benchmark": "SCiPS-QA (here)",
            "human_evaluation_details": "Not applicable.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "GPT-4 Turbo reported lowest VSR of 0.193 (indicating high consistency); Llama-3-70B-instruct had VSR ≈ 0.295 (Table 2).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Mapping of invalid responses to a fixed numeric value (3) can distort variance; VSR captures only surface categorical variability, not deeper semantic variability in reasoning passages.",
            "uuid": "e7789.3",
            "source_info": {
                "paper_title": "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SelfCheckGPT",
            "name_full": "SelfCheckGPT (general)",
            "brief_description": "A sampling-based, zero-resource, black-box hallucination detection framework that compares a main response to multiple stochastic samples to assign hallucination scores between 0 (no hallucination) and 1 (full hallucination).",
            "citation_title": "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models.",
            "mention_or_use": "use",
            "model_name": "applied to proprietary LLM outputs (GPT-4 Turbo, GPT-3.5 Turbo, text-davinci-003)",
            "model_size": "GPT family (proprietary sizes/versions used in experiments)",
            "scientific_domain": "multi-domain (SCiPS-QA responses)",
            "theory_type": "hallucination quantification for reasoning passages",
            "evaluation_method_name": "SelfCheckGPT (sampling-based hallucination scoring)",
            "evaluation_method_description": "Collect a deterministic main response (temp=0) and N stochastic responses (temp=1); compute sentence-level similarity/consistency between main and stochastic samples under several variants (BERTScore, NLI, or external LLM prompt) and average per-sentence scores to produce a [0,1] hallucination score for the main response.",
            "evaluation_metric": "Hallucination score in [0,1]; lower is better (0=no hallucination, 1=full hallucination)",
            "metric_definition": "H(M) = mean_i H(M_i) where H(M_i) is computed per sentence via variant-specific functions (complements of BERTScore similarity; NLI contradiction probability; external LLM support mapping).",
            "dataset_or_benchmark": "used on SCiPS-QA reasoning passages",
            "human_evaluation_details": "Not human evaluation; variants use external LLM (GPT-3.5 Turbo) as a scorer for one variant.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Using BERTScore variant, GPT-3.5 Turbo had the lowest mean hallucination score; GPT-4 Turbo and GPT-3.5 Turbo were not conclusively demarcated by SelfCheckGPT in many cases (paper reports failure to detect many incorrect GPT-4 Turbo responses).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "SelfCheckGPT variants failed to reliably detect incorrect reasoning for powerful LLMs (notably GPT-4 Turbo); effectiveness depends on choice of sentence embeddings, NLI model, or external LLM prompt; assumes stochastic samples reflect alternative model beliefs.",
            "uuid": "e7789.4",
            "source_info": {
                "paper_title": "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SelfCheckGPT-BERTScore",
            "name_full": "SelfCheckGPT with BERTScore (sentence-embedding similarity)",
            "brief_description": "Variant of SelfCheckGPT that uses sentence-level semantic similarity (via sentence-transformer embeddings and dot-product BERTScore) between main response sentences and stochastic response sentences to compute hallucination as 1 - max_similarity averaged across stochastic samples.",
            "citation_title": "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models.",
            "mention_or_use": "use",
            "model_name": "applied to proprietary LLM reasoning passages (GPT-4 Turbo, GPT-3.5 Turbo, text-davinci-003)",
            "model_size": "proprietary GPT family",
            "scientific_domain": "multi-domain (SCiPS-QA)",
            "theory_type": "hallucination detection for explanatory text",
            "evaluation_method_name": "SelfCheckGPT with BERTScore",
            "evaluation_method_description": "For each sentence in main response, compute maximum semantic similarity to sentences across each stochastic sample using sentence-transformer embeddings (they used all-MiniLM-L6-v2 and all-mpnet-base-v2), then assign hallucination score = 1 - average_max_similarity over N samples.",
            "evaluation_metric": "Hallucination score in [0,1]; lower=more faithfulness",
            "metric_definition": "H(M_i) = 1 - (1/N) * sum_k max_j B(M_i, S_kj); aggregate by averaging H(M_i) across sentences.",
            "dataset_or_benchmark": "SCiPS-QA main and stochastic reasoning passages",
            "human_evaluation_details": "None (automated metric).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Using this variant, authors observed roughly normal distributions; GPT-3.5 Turbo achieved lowest mean hallucination score, text-davinci-003 highest; Welch's t-tests performed to compare means.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Dependent on embedding model choice; semantic similarity may fail to capture factual errors that are lexically similar; cannot reliably detect incorrect yet consistent-sounding reasoning from powerful LLMs.",
            "uuid": "e7789.5",
            "source_info": {
                "paper_title": "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SelfCheckGPT-NLI",
            "name_full": "SelfCheckGPT with Natural Language Inference",
            "brief_description": "Variant of SelfCheckGPT that uses an NLI model (DeBERTa-v3-base fine-tuned on MNLI) to estimate the degree of contradiction between a main-response sentence and stochastic samples; uses contradiction logits to compute per-sentence hallucination probabilities.",
            "citation_title": "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models.",
            "mention_or_use": "use",
            "model_name": "applied to proprietary LLM reasoning passages",
            "model_size": "proprietary GPT family",
            "scientific_domain": "multi-domain (SCiPS-QA)",
            "theory_type": "contradiction-based hallucination scoring",
            "evaluation_method_name": "SelfCheckGPT with NLI",
            "evaluation_method_description": "Treat stochastic response sample as premise and main response sentence as hypothesis; compute logits for 'contradiction' and 'entailment', derive P(contradiction) = exp(z_c)/(exp(z_e)+exp(z_c)) and average across samples to get H(M_i).",
            "evaluation_metric": "Hallucination score in [0,1] derived from NLI contradiction probability",
            "metric_definition": "H(M_i) = (1/N) * sum_k P(contradiction | M_i, S_k) where probabilities derived from NLI logits.",
            "dataset_or_benchmark": "SCiPS-QA",
            "human_evaluation_details": "None (automated).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Authors report that this NLI variant did not clearly demarcate hallucination amounts for GPT-3.5 and GPT-4 (text-davinci-003 showed higher hallucination).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "NLI classifiers trained on general-domain MNLI can be brittle on long scientific sentences and may misinterpret entailment/contradiction in domain-specialized reasoning; not reliable for strong LLMs per reported findings.",
            "uuid": "e7789.6",
            "source_info": {
                "paper_title": "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SelfCheckGPT-Prompt",
            "name_full": "SelfCheckGPT with External LLM Prompting",
            "brief_description": "Variant of SelfCheckGPT that uses an external LLM (GPT-3.5 Turbo) prompted to judge whether each main-response sentence is supported by each stochastic sample, mapping Yes/No/NA to numeric hallucination scores.",
            "citation_title": "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models.",
            "mention_or_use": "use",
            "model_name": "external evaluator: GPT-3.5 Turbo; evaluated models: GPT-4 Turbo/GPT-3.5 Turbo/text-davinci-003",
            "model_size": "proprietary GPT family",
            "scientific_domain": "multi-domain",
            "theory_type": "LM-as-evaluator prompt-based hallucination scoring",
            "evaluation_method_name": "SelfCheckGPT with Prompt (external LLM evaluator)",
            "evaluation_method_description": "Prompt GPT-3.5 Turbo to assess whether each sentence in the main response is supported by each stochastic sample; map outputs Yes→0, No→1, NA→0.5, and average to produce per-sentence hallucination scores.",
            "evaluation_metric": "Hallucination score in [0,1] computed via external LLM judgments",
            "metric_definition": "H(M_i) = (1/N) * sum_k x_k_i where x_k_i ∈ {0.0,0.5,1.0} mapped from GPT-3.5 Turbo outputs.",
            "dataset_or_benchmark": "SCiPS-QA",
            "human_evaluation_details": "None for metric itself, but the same prompts are also used in LLM verification experiments.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Authors report GPT-3.5 Turbo assigned a higher count of low-hallucination scores than GPT-4 Turbo in this variant (Figure 8).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Relies on external LLM evaluator which may itself be biased or worse at verification; mapping of outputs to coarse scores loses nuance; reported to be ineffective at flagging many incorrect high-quality-sounding GPT-4 Turbo responses.",
            "uuid": "e7789.7",
            "source_info": {
                "paper_title": "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Convince-factor",
            "name_full": "Convince-factor (with-answer / without-answer)",
            "brief_description": "A human/LLM-judged attribute scoring how convincing the reasoning passage is, reported on a 1–5 linear scale; computed in two variants: with the model's final answer included and without the answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "verifier models used: GPT-3.5 Turbo and GPT-4 Turbo; human evaluators also used",
            "model_size": "proprietary GPT versions",
            "scientific_domain": "multi-domain (SCiPS-QA explanations)",
            "theory_type": "persuasiveness / rhetorical quality of explanations",
            "evaluation_method_name": "Convince-factor (with-answer / without-answer)",
            "evaluation_method_description": "Prompt verifier (LLM or human) to score how convincing the provided reasoning is on a 1–5 scale; compare scores when the asserted answer is included vs omitted to study answer's influence on perceived convincingness.",
            "evaluation_metric": "Convincingness score on integer scale 1–5 (higher = more convincing)",
            "metric_definition": "Single integer score assigned per reasoning passage by evaluator; aggregated distributions reported.",
            "dataset_or_benchmark": "SCiPS-QA subset reasoning passages",
            "human_evaluation_details": "Human evaluators: two per subject, 30 query+main-response combos per subject; half see answer+reasoning, half see reasoning-only; scores 1–5 recorded.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "LLM verifiers (GPT-3.5 and GPT-4 Turbo) frequently rated incorrect reasoning highly; GPT-4 Turbo tended to give an even higher fraction of perfect convincingness scores (5) to incorrect passages than GPT-3.5. Humans also were deceived by convincing incorrect responses, and inclusion of the answer increased convincingness for incorrect items (Figure 4).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Convince-factor conflates persuasiveness with factual correctness; evaluators (human or LLM) can be misled by confident-sounding but incorrect reasoning; inter-rater agreement not reported explicitly.",
            "uuid": "e7789.8",
            "source_info": {
                "paper_title": "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Fact-check (NLG evaluation)",
            "name_full": "Fact-check scoring of reasoning passages",
            "brief_description": "An evaluator-assigned factuality score (1–5) that measures how factually accurate a generated reasoning passage is relative to world knowledge or provided sources, assigned by verifier LLMs and human experts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "evaluators: GPT-3.5 Turbo, GPT-4 Turbo; evaluated models: GPT-4 Turbo, GPT-3.5 Turbo, text-davinci-003",
            "model_size": "proprietary sizes/versions",
            "scientific_domain": "multi-domain",
            "theory_type": "factual accuracy assessment of explanations",
            "evaluation_method_name": "Fact-check (1–5 scale)",
            "evaluation_method_description": "Provide the reasoning passage to an evaluator (LLM or human) and ask them to score its factuality from 1 (not factually correct) to 5 (fully factually correct).",
            "evaluation_metric": "Factuality score on integer scale 1–5",
            "metric_definition": "Single integer value assigned per passage; aggregated distributions compared between correct and incorrect passages.",
            "dataset_or_benchmark": "SCiPS-QA reasoning passages",
            "human_evaluation_details": "LLM verifiers and human evaluators used; humans rated convince-factor (not explicitly fact-check in human step), LLMs provided factuality 1–5.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "GPT-3.5 Turbo assigned high factuality scores even to many incorrect passages; GPT-4 Turbo performed worse at verifying its own passages, often assigning high factuality to incorrect outputs (Figure 3 / Table 7).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Verifier LLMs often cannot distinguish factually incorrect reasoning from correct reasoning; factuality scoring is subjective and dependent on verifier's own knowledge and biases.",
            "uuid": "e7789.9",
            "source_info": {
                "paper_title": "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Information-Mismatch",
            "name_full": "Information Mismatch score",
            "brief_description": "A 1–5 scale metric assessing the amount of information mismatch between a main response reasoning passage and each of the stochastic response passages; lower indicates less mismatch.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "evaluators: GPT-3.5 Turbo, GPT-4 Turbo; evaluated models: proprietary LLMs",
            "model_size": "various",
            "scientific_domain": "multi-domain",
            "theory_type": "consistency / content divergence measurement",
            "evaluation_method_name": "Information-Mismatch (1–5)",
            "evaluation_method_description": "Compare the main-response reasoning passage with each stochastic sample and assign a score 1 (very low mismatch) to 5 (very high mismatch); average across stochastic samples to obtain the passage-level information-mismatch score.",
            "evaluation_metric": "Information-mismatch score on 1–5 scale",
            "metric_definition": "Mean of per-stochastic-sample mismatch scores (1–5) for a given main response.",
            "dataset_or_benchmark": "SCiPS-QA",
            "human_evaluation_details": "Measured via LLM verifiers; human matching not explicitly reported for this metric.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "GPT-3.5 Turbo assigned relatively high information-mismatch to main responses from all three proprietary models; GPT-4 Turbo gave very low mismatch scores to its own passages, irrespective of correctness (Figure 3).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Information-mismatch is insensitive to correctness (authors observed verifier scores were agnostic to correctness); subjective scoring by LLM verifiers may not align with human judgments.",
            "uuid": "e7789.10",
            "source_info": {
                "paper_title": "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Human Evaluation Protocol",
            "name_full": "Human evaluation of convincingness",
            "brief_description": "A controlled human evaluation setup where graduate-educated domain evaluators scored the convincingness of GPT-4 Turbo reasoning passages (with/without answer) on a 1–5 scale to measure susceptibility of humans to be misled by incorrect LLM reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "evaluated content: GPT-4 Turbo generated reasoning passages",
            "model_size": "GPT-4 Turbo (proprietary variant used in experiments)",
            "scientific_domain": "Physics, Chemistry, Mathematics, Computer Science (subset used for human eval)",
            "theory_type": "assessment of persuasiveness and perceived correctness of LLM explanations",
            "evaluation_method_name": "Human convince-factor scoring (with/without answer)",
            "evaluation_method_description": "Two human evaluator groups per subject (2 evaluators per subject) scored randomly selected 30 query+main-response combos per subject on convincingness (1–5). One group saw both answer and reasoning, other saw reasoning-only.",
            "evaluation_metric": "Convincingness score 1–5 assigned by humans; distribution of scores for correct vs erroneous passages reported.",
            "metric_definition": "Integer score 1 (not convinced) to 5 (very convinced) per passage; aggregated distributions and counts reported.",
            "dataset_or_benchmark": "subset of SCiPS-QA (30 per subject sampled)",
            "human_evaluation_details": "Four subjects evaluated (Physics, Chemistry, Mathematics, Computer Science); two human evaluators per subject; each evaluator scored 30 query-response pairs; evaluators had ≥ graduate degree, male, age 20–25; informed consent obtained.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Humans assigned higher convincingness to correct responses overall, but a significant fraction of incorrect LLM responses still received convincing scores &gt;3; showing humans can be deceived especially when answer is shown (Figure 4).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Evaluators were not necessarily highly experienced domain experts (authors note this limitation); small number of evaluators per subject; potential demographic bias (all male, 20–25). Inter-rater agreement not reported.",
            "uuid": "e7789.11",
            "source_info": {
                "paper_title": "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Stochastic sampling protocol",
            "name_full": "Main vs Stochastic Response Collection Protocol (temperature = 0.0 vs 1.0, N=10)",
            "brief_description": "Decoding protocol used throughout experiments: collect a deterministic main response at temperature 0.0 and collect N=10 stochastic responses at temperature 1.0 for variability, hallucination, and consistency analyses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applies to all evaluated LLMs (open-source and proprietary)",
            "model_size": "various",
            "scientific_domain": "multi-domain (SCiPS-QA)",
            "theory_type": "method for generating and comparing multiple model outputs",
            "evaluation_method_name": "Main vs Stochastic response sampling (temp 0 vs temp 1, N=10)",
            "evaluation_method_description": "Generate a single deterministic 'main' response with temperature 0.0 and 10 stochastic responses with temperature 1.0 for each prompt; use both sets to compute MACC, MSACC, VSR, hallucination scores, and NLG verification attributes.",
            "evaluation_metric": "Used to derive MACC, MSACC, VSR, SelfCheckGPT scores, and invalid-response statistics",
            "metric_definition": "Parameters: main response (temp=0), stochastic samples N=10 (temp=1); mapping of categorical answers A/B/C and mapping invalid responses to incorrect in MSACC.",
            "dataset_or_benchmark": "SCiPS-QA",
            "human_evaluation_details": "Reasoning passages were collected from main responses (temp=0) for human scoring.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Protocol enabled computation of VSR (consistency) and MSACC; invalid-response rates varied by model (small models had many invalid responses; GPT-4 Turbo had nearly zero invalid main responses). See Table 5.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Choice of temperature and sample count strongly affects variance estimates; invalid-response handling (treated as incorrect) can penalize small models disproportionately.",
            "uuid": "e7789.12",
            "source_info": {
                "paper_title": "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Invalid-response rate",
            "name_full": "Invalid response proportion",
            "brief_description": "A simple metric counting the fraction of responses (main or stochastic) that cannot be parsed into one of the expected categorical outputs (A:Yes, B:No, C:I do not know), used as a quality/error indicator across models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "open-source and proprietary models evaluated (various)",
            "model_size": "various",
            "scientific_domain": "multi-domain (SCiPS-QA)",
            "theory_type": "answer-format validity assessment",
            "evaluation_method_name": "Invalid-response rate",
            "evaluation_method_description": "Count responses that are unparsable or empty relative to expected choices and report as fraction of total responses collected (both main and stochastic).",
            "evaluation_metric": "Proportion of invalid responses (0–1) or percentage",
            "metric_definition": "Invalid rate = (# invalid responses) / (total responses) reported separately for main and stochastic responses.",
            "dataset_or_benchmark": "SCiPS-QA",
            "human_evaluation_details": "Not human-evaluated; automated parsing criterion.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Low-scale models (meta-llama-2-7B, Mistral-7B-Instruct-v0.1, etc.) had high invalid rates; GPT-4 Turbo reported near-zero invalid main responses (Table 5).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Binary treatment (invalid → incorrect) may overweight small-model weaknesses; does not capture partially valid but malformed responses.",
            "uuid": "e7789.13",
            "source_info": {
                "paper_title": "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Welch's t-test",
            "name_full": "Welch's t-test for mean differences",
            "brief_description": "A standard statistical hypothesis test used in the paper to test for significant differences in mean hallucination scores (assumes unequal variances) between pairs of proprietary models under SelfCheckGPT-BERTScore.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applied to hallucination score distributions for GPT-4 Turbo, GPT-3.5 Turbo, text-davinci-003",
            "model_size": "proprietary GPT family",
            "scientific_domain": "statistical analysis of LLM evaluation metrics",
            "theory_type": "statistical significance testing",
            "evaluation_method_name": "Welch's t-test",
            "evaluation_method_description": "Assume approximate normality and independence of hallucination-score samples; compute Welch's t-statistic for difference in means between model pairs and report p-values (α=0.05).",
            "evaluation_metric": "p-value and degrees of freedom; decision to reject null hypothesis or not",
            "metric_definition": "Two-sample t-test statistic and associated p-value; degrees of freedom computed per Welch's formula.",
            "dataset_or_benchmark": "applied to SelfCheckGPT-BERTScore hallucination scores on SCiPS-QA responses",
            "human_evaluation_details": "Not applicable.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Authors report failing to reject/no-difference in some comparisons at 95% confidence for some pairs and rejecting for others; detailed results presented in Table 6 (example p-values shown).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Relies on assumption of normality and independence which may not fully hold; multiple comparisons correction not explicitly discussed.",
            "uuid": "e7789.14",
            "source_info": {
                "paper_title": "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models.",
            "rating": 2,
            "sanitized_title": "selfcheckgpt_zeroresource_blackbox_hallucination_detection_for_generative_large_language_models"
        },
        {
            "paper_title": "Measuring massive multitask language understanding",
            "rating": 2,
            "sanitized_title": "measuring_massive_multitask_language_understanding"
        },
        {
            "paper_title": "Measuring mathematical problem solving with the math dataset",
            "rating": 1,
            "sanitized_title": "measuring_mathematical_problem_solving_with_the_math_dataset"
        },
        {
            "paper_title": "ScienceQA",
            "rating": 1
        },
        {
            "paper_title": "GSM8K",
            "rating": 1
        }
    ],
    "cost": 0.023608499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators
21 Sep 2024</p>
<p>Prasoon Bajpai prasoonbajpai786@gmail.com 
Indian Institute of Technology
Delhi</p>
<p>Niladri Chatterjee niladri@iitd.ac.in 
Indian Institute of Technology
Delhi</p>
<p>Subhabrata Dutta 
Indian Institute of Technology
Delhi</p>
<p>Tanmoy Chakraborty 
Indian Institute of Technology
Delhi</p>
<p>Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators
21 Sep 2024AB4FFD0CFAAA9498706B4DC1AD9D6C39arXiv:2409.14037v1[cs.CL]
Large Language Models (LLMs) and AI assistants driven by these models are experiencing exponential growth in usage among both expert and amateur users.In this work, we focus on evaluating the reliability of current LLMs as science communicators.Unlike existing benchmarks, our approach emphasizes assessing these models on scientific questionanswering tasks that require a nuanced understanding and awareness of answerability.We introduce a novel dataset, SCiPS-QA, comprising 742 Yes/No queries embedded in complex scientific concepts, along with a benchmarking suite that evaluates LLMs for correctness and consistency across various criteria.We benchmark three proprietary LLMs from the OpenAI GPT family and 13 open-access LLMs from the Meta Llama-2, Llama-3, and Mistral families.While most open-access models significantly underperform compared to GPT-4 Turbo, our experiments identify Llama-3-70B as a strong competitor, often surpassing GPT-4 Turbo in various evaluation aspects.We also find that even the GPT models exhibit a general incompetence in reliably verifying LLM responses.Moreover, we observe an alarming trend where human evaluators are deceived by incorrect responses from GPT-4 Turbo.</p>
<p>Introduction</p>
<p>The surge of Large language models (LLMs) (Brown et al., 2020;Chowdhery et al., 2022;Chung et al., 2022;OpenAI, 2022) marks the beginning of an era of rapid development across a variety of natural language tasks.With the introduction of chatbots powered by instruction-tuned LLMs, users across diverse domains are becoming reliant on them in day-to-day activities.The increasing usage of LLM-based AI assistants in academia has triggered intense discussion recently.Multiple reports of inconsistent fragments of text appearing in scientific papers, apparently generated by AI assistants and overlooked due to lack of caution, have Figure 1: Examples of wrong reasonings given by GPT-4 Turbo to problems in SCiPS-QA: (Physics -Air can cast a shadow under conditions of non-uniform refractive index (Baird, 2024); Chemistry -The complex is chiral with D3 symmetry (Ghosh et al., 1984); Mathematics -The paper discusses the model completeness of the real exponential field and its connection to Tarski's problem and the first root conjecture.Tarski's problem is an open problem (Macintyre and Wilkie, 1996)).</p>
<p>surfaced.Recent attempts have been made to outline the usage of AI assistants for literature surveys in research pipelines (Bhayana, 2024;Whitfield and Hofmann, 2023) However, there are innate risks associated with LLMs, attributed to overconfident generation and hallucination, that need to be addressed before their large-scale usage as surrogates of human expertise.Particularly in scientific communication in which nuance plays a vital role, LLMs missing out on small details can spread misconceptions (Dutta and Chakraborty, 2023).Another key challenge lies in the lack of self-awareness of current LLMs and overconfident generation leading to hallucination; given that realizing the lack of knowledge drives the pursuit of scientific exploration, an uncompromising quality of an AI assistant would be to reflect on the lack of knowledge.Existing STEM benchmarks, despite a variety in problem hardness, fail to incorporate these crucial characteristics.</p>
<p>In this work, we seek to close the gaps in evaluating LLMs towards faithful scientific questionanswering.Specifically, we seek to address the following research questions:</p>
<p>• RQ1: Can existing LLMs answer scientific reasoning questions successfully and faithfully that require understanding the nuances of scientific knowledge?• RQ2.Are LLMs effective at abstaining from assertively answering scientific open problems?• RQ3.Can LLMs successfully verify LLMgenerated responses?• RQ4.Can human evaluators be misled by incorrect yet confident LLM responses to complex scientific questions?To this end, we propose a novel dataset scientific QA dataset, SCiPS-QA (Specially Challenging Problems in Science -Question Answering), a collection of 742 complex boolean scientific problems that require deep knowledge retrieval and extensive reasoning to answer (Contribution #1). 1 The problems are chosen from the most niche research areas across different subjects (see Figure 1 for sample questions from SCiPS-QA and answers generated by GPT-4 Turbo).SCiPS-QA contains closed (i.e., the answer exists within the scope of current scientific knowledge) as well as open problems.We benchmark a wide variety of proprietary and open-access LLMs from the OpenAI GPT series, Llama-2 and Llama-3 series, and Mistral series on SCiPS-QA using an exhaustive evaluation suit to judge their correctness, faithfulness, and hallucination, in terms of the final boolean answer as well as the reasoning explanation (Contribution #2).We find that while proprietary models like GPT-4 Turbo are generally better than open-access Llama-2, Mistral, or smaller Llama-3 variants, Llama-3-70B models (with or without instruction tuning) come as a strong competitor to GPT-4 Turbo (Findings #1).However, all the experimented LLMs are far from understanding the nuances of scien-1 Please find the code and data at the github repo : llmscience-miscommunication tific rigor, particularly in relation to open problems (Findings #2).We investigate whether proprietary LLMs can successfully verify LLM-generated responses to these complex scientific questions (Contribution #3), revealing their shortcomings in verifying different aspects of the generated response (Findings #3).Finally, we perform a human evaluation of GPT-4 Turbo generated responses to a subset of questions from SCiPS-QA (Contribution #4).Alarmingly, the persuasive style of generation adopted by GPT-4 Turbo is enough to deceive human evaluators to trust the reasoning, particularly when answers are included in the response (Findings #4).</p>
<p>Related Work</p>
<p>LLMs have demonstrated various types of reasoning capabilities, including logical, commonsense, mathematical, and temporal reasoning (Huang and Chang, 2023).In this section, we review relevant work that explores the limitations of LLMs in scientific reasoning.</p>
<p>Several datasets provide comprehensive assessments of LLMs' abilities to solve mathematical problems.GSM8K (Cobbe et al., 2021) comprises high-school-level math word problems, while AQuA-RAT (Ling et al., 2017) includes a collection of algebraic word problems.Dolphin18K (Huang et al., 2016) features elementary-level problems designed to evaluate basic mathematical reasoning capabilities.The MATH dataset (Hendrycks et al., 2021) presents more challenging problems than those in the aforementioned datasets but focuses on simpler mathematical objects compared to the complex scientific concepts found in SCiPS-QA.Additionally, Ape210K (Zhao et al., 2020) offers a broad range of mathematical problems to further test LLMs' problem-solving skills.These datasets collectively highlight the strengths and limitations of LLMs in mathematical reasoning, providing a foundation for understanding their performance in more specialized scientific domains.</p>
<p>ScienceQA (Lu et al., 2022), SciQ (Lu et al., 2022) and MMLU (Hendrycks et al., 2020) are prominent datasets used to evaluate LLMs' scientific reasoning capabilities.MMLU-Pro (Wang et al., 2024)  In contrast, SciQ comprises 13.7K multiple-choice science exam questions created through crowdsourcing.We demonstrate that GPT-4 Turbo performs better on these popular STEM datasets than on SCiPS-QA.SCiPS-QA also focuses on benchmarking answer abstinence in LLMs by including open scientific queries in Physics, Chemistry, and Mathematics.Feng et al., 2024 explored various answer abstinence methods, evaluating them on MMLU.Wen et al., 2024 investigated the ability of LLMs to abstain from answering context-dependent science questions when provided with insufficient or incorrect context.They used datasets such as ScienceQA, OpenBookQA, ARC (AI2 Reasoning Challenge) (Clark et al., 2018), and QASPER (Dasigi et al., 2021) to study LLM abstention behavior.ScienceQA includes school-level and college-level scientific problems requiring relatively simple reasoning capabilities.OpenBookQA features straightforward open-book style general and scientific reasoning problems, and ARC contains grade-school level multiple-choice science questions.In contrast, SCiPS-QA provides a much tougher benchmark for scientific reasoning and explores answer abstinence in Boolean questions.</p>
<p>The SCiPS-QA Dataset</p>
<p>In this section, we describe the composition of SCiPS-QA and the methodology used to collect the Boolean queries that constitute SCiPS-QA.</p>
<p>The dataset comprises 742 complex Yes/No problems that require expert-level proficiency in scientific reasoning to answer correctly.We include both open and closed problems across subjects -Physics, Chemistry, Mathematics, Theoretical Computer Science, Astronomy, Economics, and Biology.Table 1 provides the composition of SCiPS-QA, while Figure 5 shows the subjectlevel topic decomposition.The difficulty of the We randomly select 40 problems from each of four different subjects within SCiPS-QA to compare GPT-4 Turbo's performance in answering Boolean scientific queries against those from MMLU-Pro and SciQ.Additionally, we utilize GPT-3.5 Turbo to paraphrase 40 randomly chosen scientific problems per subject from MMLU-Pro and SciQ into a Yes/No format.Figure 2 illustrates that GPT-4 Turbo performs the worst on SCiPS-QA, highlighting its higher level of difficulty regarding boolean question answering.</p>
<p>Closed questions.These questions have definitive answers supported by scientific literature.We curate a list of complex topics for each subject in SCiPS-QA manually.For each topic, we utilize the wikipedia API to retrieve its summary.Subsequently, we provide this summary to GPT-4 Turbo, prompting it to generate Yes/No problems along with their corresponding answers.The resulting Boolean questions undergo manual assessment based on two criteria: (1) requiring scientific reasoning for accurate answers, and (2) correctness of the generated answers.The precise prompt used to generate closed questions can be found in Appendix D.1.</p>
<p>Open questions.These questions lack a definitive answer in the scientific literature.They are manually selected from wikipedia pages and research blogs.Further details on how open questions were collected can be found in Appendix B. This section presents the details of the experiments we performed to answer the research questions (RQs) we set to explore.</p>
<p>Experimental Setup</p>
<p>We evaluate a total of 13 open-source models, including those from the Llama-2 family (Touvron et al., 2023), Llama-3 family, Mistral-7B-Instruct-v0.1 (Jiang et al., 2023), Mistral-7B-Instruct-v0.2, and Mistral-8x7B-Instruct-v0.1 (Jiang et al., 2024), on the SCiPS-QA dataset using custom-designed evaluation metrics.Additionally, we assess proprietary models such as GPT-4 Turbo (gpt-4-turbo-2024-04-09), GPT-3.5 Turbo (gpt-3.5-turbo-1106),and 'text-davinci-003'.For proprietary models, we follow the methodology outlined in (Li et al., 2024) to evaluate the reasoning passages generated by these models in response to boolean queries from SCiPS-QA.Evaluation criteria include attributes like factuality and convincingness (defined in Appendix A), assessed using GPT-3.5 Turbo and human experts as evaluators.We also evaluate the propensity for hallucination (see Appendix A) in these reasoning passages using SelfCheckGPT (Manakul et al., 2023), which employs a samplingbased approach.Further details on these evaluations are provided in subsequent sections.We collect responses through few-shot prompting.The details about exact prompts can be found in Appendix D.2.For each model, the responses are collected in two different settings.We call responses collected at temperature 0.0 the 'main responses' and those collected at temperature 1.0 the 'stochastic responses'.</p>
<p>Evaluation Metrics</p>
<p>Towards a comprehensive evaluation of LLMs, we define the following metrics on the generated responses.</p>
<p>(i) Main Response Accuracy (MACC).The accuracy of responses obtained at zero temperature.</p>
<p>(ii) Major Stochastic Response Accuracy (MSACC).We collect the majority response from 10 different stochastic responses with temperature set to 1.We treat invalid responses as incorrect answers.</p>
<p>(iii) Variation in Stochastic Responses (VSR).We report the variety in the 10 stochastic responses obtained at temperature 1.We map A → 1, B → 2, C → 3 and rest of the invalid responses to 3 and calculate the standard deviation.</p>
<p>(iv) Accuracy of Main responses for closed questions (CMACC) denotes the MACC score on the subset of SCiPS-QA containing closed questions.</p>
<p>(v) Accuracy of Major Stochastic Responses for Closed Questions (CMSACC) reflects whether the majority of the LLMs' responses to the closed questions in a unit temperature decoding are correct or not.</p>
<p>(vi) Accuracy of Main Responses for Open Questions (OMACC) is similar to CMACC but evaluated on the open questions instead.In addition to the overall correctness, this metric evaluates whether the model can identify if a question is scientifically unanswerable.</p>
<p>(vii)Accuracy of Major Stochastic Responses for Open Questions (OMSACC) tests the answer abstinence of the LLM in a unit temperature generation regime.</p>
<p>Hallucination Quantification</p>
<p>We employ SelfCheckGPT (Manakul et al., 2023), a sampling-based methodology that assigns hallucination scores within the range of [0, 1] (0: no hallucination and 1: full hallucination).This scoring is derived by measuring the deviations between the main response and multiple stochastic responses.We take the average of the hallucination scores of sentences in the main response to assign a hallucination score to the entire main response.We briefly describe each of the SelfCheckGPT variants in this section.More details about SelfCheckGPT and the variants we have implemented can be found in the Appendix C.</p>
<p>SelfCheckGPT with BERTScore.For each reasoning sentence in the main response, we calculate the maximum semantic similarity across all sentences in the stochastic response passages.This score indicates the degree of semantic similarity between the main response sentence and various stochastic responses.To quantify hallucination, we derive the complement of this score and assign it as the hallucination score for the main response sentence.Our analysis employs two models, all-MiniLM-L6-v2 and all-mpnet-base-v2, sourced from sentence_transformer (Reimers and Gurevych, 2019), to generate sentence-level embeddings.This approach ensures mitigation of potential model bias in our results.</p>
<p>SelfCheckGPT with NLI.Natural Language Inference (NLI) assesses whether a hypothesis logically follows from a premise, categorized as entailment, neutral, or contradiction.We compare each sentence of a main response reasoning passage as a hypothesis against each of the corresponding stochastic response reasoning passages as the premise.The logits associated with classes 'contradiction' and 'entailment' are considered and a score is assigned to the main response sentence, which is a proxy for the probability score of it being in 'contradiction' to the stochastic response reasoning passages.We use DeBERTa-v3-base (He et al., 2020) fine-tuned on MNLI (Williams et al., 2018) for collecting the logits associated with 'contradiction' and 'entailement' classes.</p>
<p>SelfCheckGPT with Prompt.We employ an external LLM evaluator to determine if each sentence in a main response reasoning passage is supported by corresponding stochastic response reasoning passages.Specifically, we utilize GPT-3.5 Turbo as the external LLM; the exact prompt used can be found in Appendix D.3.The responses (Yes, No, NA) are mapped to hallucination scores (Yes → 0, No → 1, NA → 0.5).The average of the GPT-3.5 Turbo response scores is calculated and assigned as the hallucination score for the corresponding sentence in the main response.</p>
<p>NLG Evaluation of Reasoning Passages</p>
<p>We validate the main response reasoning passages generated by the models -GPT-4 Turbo, GPT-3.5 Turbo, and text-davinci-003 using GPT-3.5 Turbo as the verification model.Additionally, we verify responses from GPT-4 Turbo using GPT-4 Turbo itself as the verification model.Verification attributes are scored on a linear scale using prompt outputs in a zero-shot setting.All relevant prompt details can be found in Appendix D.4.</p>
<p>Convince-factor.Responses that are highly convincing but rely on incorrect information are considered 'hallucinations' (Ji et al., 2022).We assign a convincingness score on a linear scale ranging from 1 to 5.This verification attribute is reported for main response reasoning passages using two different prompt settings: one where model answers are included in the prompt given to the evaluator models (denoted as convince-factor-with-answer), and another where model answers are absent (denoted as convince-factor-without-answer).</p>
<p>Fact-check.We assign scores on a linear scale (ranging from 1 to 5) to main response reasoning passages based on their factual accuracy.Our aim is to investigate whether evaluator LLMs can differentiate between incorrect reasoning passages and correct ones based on the factual correctness of responses.</p>
<p>Information Mismatch.We compare each main response reasoning passage with all ten different stochastic response reasoning passages S k for the amount of information mismatch between them, which is scored on a linear scale ranging from 1 to 5. We assign the mean of such scores across stochastic responses to the main response reasoning passage.</p>
<p>Human Evaluations</p>
<p>We randomly select 30 combinations of query and main response reasoning passages (from GPT-4 Turbo) for each subject -Physics, Chemistry, Mathematics, and Computer Science.For each subject, we employed two human evaluators.All human evaluators had at least a graduate degree in their respective subjects; they were male and aged between 20-25.</p>
<p>Human evaluators were tasked with assigning a 'convince-factor' score to the main response reasoning passages, following the same evaluation setup used with LLMs as the evaluators.We divide human evaluators into two groups: one group sees both the model answer and reasoning, while the other group only views the reasoning itself.Both groups receive identical queries for evaluation.</p>
<p>Results</p>
<p>In this section, we look at the various quantitative results summarized in Table 2.</p>
<p>SCiPS-QA Benchmark</p>
<p>We observe that among both open-source and proprietary models, the Llama-2 family consistently performs the poorest across nearly all metrics.The GPT series of models show competitive performance, closely rivaling the higher-scale models within the Llama-3 family, which rank highest among the open-source models tested.</p>
<p>MACC: Llama-3-70B achieves the highest score in the MACC metric at 0.693, closely followed by GPT-4 Turbo with a score of 0.646.Notably, among the Llama-2 and Llama-3 families, 'chat' models perform equivalently to their non- instruction fine-tuned counterparts, except for the lower scale members: Llama-2-7B and Llama-3-8B, where the instruction fine-tuned variants show score increases of 0.3 and 0.324, respectively.Mixtral-8x7B-Instruct-v0.1 significantly outperforms both Mistral-7B-Instruct-v0.1 and Mistral-7B-Instruct-v0.2.All three GPT models perform strongly, with GPT-4 Turbo achieving the highest score of 0.646 in the MACC metric.MSACC: GPT-4 Turbo outperforms all other models with a score of 0.651, closely followed by Llama-3-70B-instruct, which achieves a score of 0.623.In contrast to the MACC metric, where instruction fine-tuned models from the Llama-2 and Llama-3 families often performed equivalent to their non-instruction fine-tuned counterparts, here we observe that the instruction fine-tuned models outperform their counterparts.
LLMs MACC (↑) MSACC (↑) VSR (↓) CMACC (↑) CMSACC (↑) OMACC (↑) OMSACC (↑) meta-llama-2-7B 0.
VSR: The Llama-2 family performs the worst in terms of VSR score indicating their limited capability to produce consistent results.In contrast, the GPT models exhibit high consistency, with GPT-4 Turbo reporting the lowest VSR score of 0.193 among all models.The Llama-3 family demonstrates better consistency compared to the Llama-2 family, while Mistral models also perform well but not as strongly as the top performers among open-source models.Among them, Llama-3-70Binstruct stands out with a VSR score of 0.295.</p>
<p>CMACC, CMSACC: Llama-3-70B-instruct outperforms all models in CMACC and CMSACC metrics achieving a score of 0.780 and 0.784 respectively.GPT models also perform well in handling closed domain scientific queries with GPT-4 Turbo being the best among them, achieving a score of 0.75 and 0.754.</p>
<p>OMACC, OMSACC: One of the major findings is that most of the open source and proprietary LLMs are really bad at accepting that they do not know the answers to open scientific queries in SCiPS-QA.This is evident from their low OMACC and OMSACC scores across the board.Llama-3-70B stands out as the top performer in terms of answer abstention for open scientific queries, achieving the highest OMACC (0.582) and OMSACC (0.487) scores.In contrast, Llama-2 models struggle significantly in handling open queries, while Mistral-7B models and Mixtral-8x-7B-Instruct-v0.1 perform reasonably well among open models.The GPT models demonstrate strong performance in responding to open scientific queries, with GPT-4 Turbo achieving the highest scores of 0.432 and 0.436 in OMACC and OMSACC metrics, respectively.Note that models also produced invalid responses to prompts.Small models -Llama-2-7B and Mistral-7B-Instruct-v0.1, produce a much larger fraction of invalid responses as compared to other open-source models.Proprietary models produce almost negligible invalid responses with GPT-4 Turbo reporting no invalid main response.More details can be found in Appendix E.1.</p>
<p>Hallucination Quantification</p>
<p>Our investigation using SelfCheckGPT fails to yield conclusive evidence of hallucination in the proprietary GPT models despite their high rate of mistakes.When employing the BERTScore variant, we observe normal distribution in the frequency dis- tribution histograms (Figure 6) for all three GPT models on SCiPS-QA.Interestingly, GPT-3.5 Turbo achieves the lowest mean hallucination score, followed by GPT-4 Turbo, while text-davinci-003 performed the poorest.</p>
<p>Assuming normal distribution and independence of score samples, we conduct Welch's t-tests to assess the statistical significance of mean differences between the proprietary models.Our findings indicate that we fail to reject the hypothesis of no difference in means between all pairs of proprietary models being tested with a 95% level of confidence.Further details of these tests are available in Appendix E.2.</p>
<p>Figure 8 summarizes results for SelfCheckGPT with Prompt.GPT-3.5 Turbo shows a higher count of low hallucination scores given to queries in SCiPS-QA than GPT-4 Turbo.</p>
<p>NLG Evaluation of Reasoning Passages</p>
<p>We assess the main response reasoning passages from all three proprietary models using GPT-3.5 Turbo as the verifier.Table 7 shows the convincingness (with and without answer), factuality and information-mismatch scores for all three models using GPT-3.5 Turbo as verifier.We use both GPT-4 Turbo and GPT-3.5 Turbo for verifying reasoning passages obtained from GPT-4 Turbo. Figure 3 shows the verification results for the GPT-4 Turbo's reasoning passages for the two verifier models.</p>
<p>Convince-factor</p>
<p>GPT-3.5 Turbo consistently assigns high scores to the main response reasoning passages from all three models (see Table 7).It rates both correct and incorrect reasoning responses highly across all models.Surprisingly, even when evaluating reasoning passages from GPT-4 Turbo, it itself struggles to distinguish between correct and incorrect responses.Interestingly, as depicted in Figure 3, GPT-4 Turbo assigns a higher fraction of reasoning passages (both correct and incorrect) a perfect score of 5 in convincingness (with and without answer) compared to GPT-3.5 Turbo.This suggests that GPT-4 Turbo performs worse than GPT-3.5 Turbo in terms of verifying its responses based on convincingness (with and without answer).</p>
<p>Fact-check</p>
<p>GPT-3.5 Turbo assigns high scores to the reasoning passages from all three models (see Table 7 in Appendix), often rating a majority of incorrect responses a perfect 5 in factuality verification.GPT-4 Turbo performs even worse in verifying its own reasoning passages (see Figure 3), assigning a higher fraction of incorrect reasoning passages a perfect 5 score compared to GPT-3.5 Turbo.This indicates that GPT-4 Turbo struggles more than GPT-3.5 Turbo in distinguishing between correct and incorrect reasoning passages, even when evaluating its own responses.</p>
<p>Information Mismatch</p>
<p>We observe that GPT-3.5 Turbo assigns relatively high information-mismatch scores to main response reasoning passages from all three proprietary models.Table 7 shows that among the three models being tested, GPT-3.5 Turbo gives a lesser information-mismatch score to its own reasoning passages than it does to the other two models.</p>
<p>From Figure 3, we observe that GPT-4 Turbo provides a very low score for its own reasoning passages in terms of information-mismatch score.These patterns are agnostic to the correctness of reasoning passages, suggesting that both verifier models are not able to differentiate between correct and incorrect passages using information-mismatch scores.Consistent with our observations across various verification attributes, GPT-4 Turbo performs worse than GPT-3.5 Turbo, consistently assigning lower scores (often 1) to most reasoning passages, irrespective of their correctness.</p>
<p>Human Evaluations</p>
<p>Human evaluators typically fare better than LLM evaluators.As we can see in Figure 4, correct responses are consistently given better scores than incorrect ones.However, a considerable fraction of incorrect responses can still deceive human judgment into getting scores greater than 3.</p>
<p>Notably, human evaluators tend to judge incorrect responses better when the generated answer is attached.This can be possibly related to cases where the LLM infers incorrect answers even after providing correct reasoning context.Furthermore, correct responses are typically distributed towards the highest convince factor (i.e., 5); although, without the answer provided, some correct responses are given scores as low as 3. Interestingly, the scoring distribution provided by human evaluators is much closer to that provided by GPT-3.5 Turbo as verifier instead of GPT-4 Turbo.</p>
<p>Discussion and Conclusion</p>
<p>Our experiments on SCiPS-QA with a diverse array of LLMs using a comprehensive evaluation strategy reveal several key insights.Firstly, existing LLMs, whether open-access or proprietary, demonstrate a limited understanding of scientific methodologies required to serve as reliable assistants.While the parameter scaling law holds within each LLM family, models of similar size across different families are not directly comparable.For instance, Meta Llama-3 70B models emerge as formidable competitors to much larger GPT models, frequently outperforming GPT-4 Turbo in our evaluations.This reiterates earlier findings that parameter scaling alone does not reflect the capabilities of LLMs and current models, along with their training methodology, are underperforming their 'true' potential (Hoffmann et al., 2022).</p>
<p>Echoing Huang et al. ( 2024)'s findings, we observe that powerful LLMs such as GPT-4 Turbo and GPT-3.5 Turbo struggle to reliably verify their responses.Hallucination detection techniques like SelfCheckGPT also prove ineffective in detecting incorrect reasoning posed by strong LLMs like GPT-4 Turbo in complex questions within SCiPS-QA.In fact, we notice a counterintuitive trend where GPT-3.5 Turbo assigns lower scores to incorrect responses compared to the stronger GPT-4 Turbo.</p>
<p>However, the most concerning finding of this paper revolves around how human evaluators perceive LLM-generated scientific reasoning.When tasked with evaluating the convincingness of reasoning explanations generated by GPT-4 Turbo, human evaluators tend to assign higher ratings to a significant majority of incorrect answers.This aligns with the concern raised by Dutta and Chakraborty (2023) that current LLM-based AI assistants have the potential to propagate widespread scientific misunderstandings if left unchecked.</p>
<p>Implications for future research.We hope that our proposed dataset, SCiPS-QA, along with the evaluation suit we design in this work, will serve as a valuable benchmark for future LLM research.Given the growing popularity of generalist as well as domain-specific AI assistants, we envision a positive future focus in building reliable scientific assistants.Finally, our findings with human evaluation calls upon further focus in trustworthy AI research.</p>
<p>Boolean format of scientific questions has been adopted in SCiPS-QA.Having a long-text reasoning evaluation while maintaining the complexity of scientific objects should provide a stronger test for evaluating scientific communication.For this, SCiPS-QA needs to be augmented with golden reasoning passages provided by human experts.There is also a need to add more diverse topics to the SCiPS-QA, particularly in Physics, which is dominated by Quantum Mechanics (Appendix 5).There is also an issue of some queries in SCiPS-QA lying outside the knowledge cutoff of some models, making it difficult to accurately assess their reasoning capabilities.Human evaluations may be slightly limited because they do not include highly experienced evaluators in the respective subjects.The testing of reasoning passages from open-source models has also not been done as part of our analysis.</p>
<p>Ethical Considerations</p>
<p>The participants in human evaluation were not coerced into participating and were given clear and comprehensive information about the research before they provided informed consent.The identities of the human evaluators have been protected by ensuring their responses cannot be linked back to the specific individuals.The research results are communicated honestly and credibly and transparency has been maintained throughout the research process.</p>
<p>A Definitions</p>
<p>Hallucination: The generated content that is nonsensical or unfaithful to the provided source input (Filippova, 2020;Maynez et al., 2020), where the source input changes as the task.We take the world knowledge as the source input in our case.Factuality: Factuality refers to the property of quality of being actual or based on fact (Dong et al., 2020).In our work, we take "facts" as the world knowledge.Convincingness: Convincingness refers to the ability of a model to effectively influence the audience through language (Habernal and Gurevych, 2016).</p>
<p>B Collection of Open Questions</p>
<p>We collect open-questions from List of unsolved problems article on wikipedia for all subjects.We also referred to the page List of open questions in theoretical computer science by Antoine Amarilli.We use GPT-3.5 Turbo to parse some of the entries on these web pages into a question format.</p>
<p>C SelfCheckGPT C.1 Notation</p>
<p>We obtain two types of responses from proprietary models for quantifying hallucination.Let M , calling it the 'main response', denote the reasoning passage obtained at temperature 0.0.We sample N = 10 different stochastic responses:{S 1 , S 2 , . . .S N }, each at temperature 1.0 using the same prompt structure, aiming to measure commonalities between the stochastic responses and the main response.We use SelfGPTCheck to assign a hallucination score to ith sentence of the main response M i : H(M i )− &gt; [0.0, 1.0], with 0.0 score given to such sentences that are completely faithful to source input and 1.0 if they are fully hallucinated.The following subsections describe the variants of SelfCheckGPT briefly that we have used in this paper.</p>
<p>C.2 SelfCheckGPT with BERTScore</p>
<p>Let M i and S k j denote the i-th sentence of the main response and the j-th sentence of the k-th stochastic response.Note all these responses are reasoning passages that are provided by the proprietary models tested.We assign a hallucination score to M i depending on the BERTScore between M i and S k j as follows:
H(M i ) = 1 − 1 N N k=1 max k B(M i , S k j ) (1)
where B(., .) is the dot score of sentence embeddings generated using model B. This way M i shall be assigned a higher score if it is semantically less similar (according to BERTScore) to most of the sentences in different stochastic responses.However, if a sentence in the main response is semantically similar (or appears in) to sentences in different stochastic responses, then it will be assigned a lower hallucination score.We take the mean of the hallucination scores of each sentence of the main response to assign it a hallucination score.We report results using two different models : B ∈ {all-MiniLM-L6-v2, all-mpnet-base-v2} from sentence_transformer (Reimers and Gurevych, 2019) for generating sentence-level embeddings for eliminating any possible model bias.</p>
<p>C.3 SelfCheckGPT with NLI</p>
<p>The input for NLI classifiers is typically the premise concatenated to the hypothesis, which for our methodology is the sampled passage S k concatenated to the sentence to be assessed M i .Only the logits associated with the 'entailment' and 'contradiction' classes are considered, We use DeBERTa-v3-base fine-tuned on MNLI for collecting the logits associated with 'contradict' class.</p>
<p>SelfGPTCheck with NLI uses stochastic response S k as the premise concatenated to the main response sentence M i to be assessed.The logits associated with token 'contradict' are used to assign a score.
P (contradict|M i , S k ) = exp(z c ) exp(z e ) + exp(z c )(2)
where z e and z c are the logits of the 'entailment' and 'contradiction' classes, respectively.A higher probability denotes that the concerned main response sentence disagrees with the stochastic sample and hence, should be assigned a higher hallucination score, which is defined as,
H(M i ) = 1 N N k=1 P (contradict|M i , S k )(3)
We take the average of the hallucination scores of sentences in the main response to assign a hallucination score to the entire main response M .</p>
<p>C.4 SelfCheckGPT with Prompt</p>
<p>We prompt GPT-3.5 Turbo to assess if the i-th sentence of the main response is supported by the k-th stochastic response, S k .The exact prompt can be found in the appendix D.3.The output from prompting when comparing the i-th sentence against sample S k is converted to score x k i through the mapping Yes: 0.0, No: 1.0, N/A: 0.5.The final inconsistency score is then calculated as:
H(M i ) = 1 N N k=1 (x k i )(4)
Note, for all these variants, we report the results at only such data-points of SCiPS-QA where all 10 stochastic reponses are non-empty and valid.A stochastic response is considered invalid if it cannot be parsed into the boolean answer and the corresponding reasoning passage.</p>
<p>D Prompts</p>
<p>We shall now describe the exact prompts that we used.</p>
<p>D.1 Collection of Closed Questions</p>
<p>We collect closed questions by prompting GPT-4 Turbo to create boolean problems from the passage given in the prompt.The passage is taken from the wikipedia pages of topics under different subjects.</p>
<p>Table 3 shows the exact prompt that we used for collecting closed questions for SCiPS-QA.We replace the <PASSAGE> placeholder with the passages retrieved from wikipedia.</p>
<p>We observed that most of the questions created by GPT-4 Turbo in this manner, we purely a test of knowledge retrieval.This made us include some additional instructions in the prompt.We manually checked the questions for their corresponding answers and ensured that most of the questions in SCiPS-QA required some levels of reasoning to answer.</p>
<p>D.2 Collecting Responses</p>
<p>We now describe the prompts that we used for collecting responses from open-source models and proprietary models.</p>
<p>D.2.1 Open-source Models</p>
<p>Table 3 shows the exact prompts that we used for collecting responses (A-Yes, B -No &amp; C -I do not know) from open-source models.Table 5 shows the number of responses from each open-source model that were invalid.A response (main or stochastic) is considered to be invalid if it could not parsed into one of the choices (A, B or C).We observe that lowscale models Llama-2-7B, Llama-3-8B and Mistral-7B-Instruct-v0.1 had a high percentage of invalid main responses.The instruction fine-tuned versions of models reported much lesser invalid responses at same scale of parameters.The GPT line of models and higher scaled members of Llama-2 and Llama-3 family reported much less percentage of invalid responses (both 'main' and 'stochastic').</p>
<p>While collecting responses from open-source models, we set the generation parameter max_new_tokens to 3 and parse the responses for options from the set {A, B, C}, (A -"Yes", B -"No", C -"I do not know").For models : Llama-2-70B, Llama-2-70B-chat, Llama-3-70B and Llama-3-70B-instruct, we use non-uniform 4-bit quantization to fit these models within a single A100 to account for limited computational resources.</p>
<p>Since we also collect reasoning passages from chosen proprietary models, we set the generation parameter max_tokens to 1000.</p>
<p>D.2.2 Proprietary Models</p>
<p>The prompt structure for proprietary models differs from that for open-source models with respect to the presence of 'Reason:' field in the exemplars.This is done to force these models to provide reasoning passages which are further quantified for hallucination and score for different attributes using human experts and GPT-3.5 Turbo as evaluators in a parallel setting.</p>
<p>D.3 SelfCheckGPT with Prompt</p>
<p>Table 4 shows the exact prompt that we used for this variant of SelfCheckGPT.The prompt is exactly same mentioned in the SelfCheckGPT paper (Manakul et al., 2023).The <CONTEXT> is replaced by each of the stochastic response passages and <SENTENCE> is replaced by the main reasoning passages.</p>
<p>D.4 NLG Evaluation of Reasoning Passages</p>
<p>We describe all the prompts that we used for this section.Note that we used GPT-4 Turbo,  Turb and text-davinci-003 as the LLM modules for assigning scores to the main response reasoning passages.</p>
<p>D.4.1 Convince-factor</p>
<p>Table 4 shows the prompt that we used for two schemes : convince-factor-with-answer and convince-factor-without-answer.The two prompts differed only with respect to the presence of the model answer (to the boolean scientific query).</p>
<p>D.4.2 Fact-check</p>
<p>Table 4 shows the prompt that we used for assessing the factuality of main response reasoning passages (which replaced the <SOURCE> placeholder)</p>
<p>D.4.3 Information-mismatch</p>
<p>Table 4 shows the prompt that we used for assigning scores of this attribute.The <SOURCE> placeholder is replaced with the main response reasoning passage and the <GENERATED> placeholder is replaced with the stochastic response reasoning passages.</p>
<p>E Results</p>
<p>E.1 Invalid Responses</p>
<p>Table 5 shows the percentage of invalid responses (to the 'answer' field of the prompt) to queries in SCiPS-QA.Llama-3 models and GPT models show fairly low numbers of invalid responses.Low scale models from Llama-2, Llama-3 and Mistral family report high percentage of invalid responses.</p>
<p>E.2 Hallucination Quantification</p>
<p>E.2.1 SelfCheckGPT with NLI</p>
<p>Figure 7 shows that main response passages from GPT-3.5 Turbo and GPT-4 Turbo are not demarcated for amount of hallucination using this scoring.text-davinci-003 however, is clearly shown to produce more hallucinated text.</p>
<p>E.2.2 SelfCheckGPT with Prompt</p>
<p>Figure 8 shows results for SelfCheckGPT with Prompt.More response passages from GPT-3.5 Turbo are given low hallucination scores as compared to those from GPT-4 Turbo.</p>
<p>E.2.3 SelfCheckGPT with BERTScore</p>
<p>We performed Welch's t-tests to test the statistical significance of results.We observe that main response reasoning passages from GPT-3.5 Turbo are given least mean hallucination scores using 'SelfCheckGPT with BERTScore' and main response reasoning passages from text-davinci-003 are given the highest mean hallucination scores.Reason: The nature of spacetime at the Planck scale is currently not definitively established, and it remains an open question in theoretical physics.However, many theories, including some formulations of quantum gravity, suggest that spacetime may exhibit discrete or quantized characteristics at the Planck scale.This is based on the idea that classical notions of smooth, continuous spacetime may break down at extremely small scales.The uncertainty principle from quantum mechanics also contributes to this speculation.Research in areas such as loop quantum gravity and string theory explores these possibilities, but a conclusive answer is yet to be determined through experimental verification.Answer: C Question: Is the exact value of mean lifetime of a neutron known?Reason: ... Answer: B Question: Does the set of natural numbers have the same cardinality as set of integers?Reason: ... Answer: A Question: Is the Universe infinite in spatial extent?Reason: ... Answer: C      Table 7: Verification of the main response reasoning passages generated by all three proprietary models across convincingness (with and without answer), factuality, and information mismatch using GPT-3.5 Turbo as the verifier model.</p>
<p>Figure 3 :
3
Figure 3: Verification of the reasoning passages generated by GPT-4 Turbo across convincingness (with and without answer), factuality, and information mismatch; we use both GPT-4 Turbo and GPT-3.5 Turbo as verifier models.The fraction of correct (incorrect) responses at each score level is shown in blue (red).An ideal verifier should provide all the incorrect responses with the lowest score (1) and all the correct responses with the highest score (5).However, no verifier model in our experiments could demarcate between the correct and incorrect responses.</p>
<p>Figure 4 :
4
Figure 4: Distribution of correct (in blue) and incorrect (in red) responses generated by GPT-4 Turbo against convince factor scores provided by human evaluators.Incorrect LLM reasoning can deceive humans as convincing with or without the answer shown to them.However, humans provide better judgement with the answer.</p>
<p>Figure 5 :
5
Figure 5: Topic decompostion for subjects : Physics (top-left), Chemistry (middle) &amp; Mathematics (top-right) in SCiPS-QA</p>
<p>Figure 6 :Figure 7 :Figure 8 :
678
Figure 6: Frequency distribution plots of 'SelfCheck-GPT with BERTScore' hallucination scores to main response reasoning passages for sentence_transformer models: all-MiniLM-L6-v2 (above) &amp; all-mpnet-base-v2 (below)</p>
<p>You are an AI assistant being used for research purposes.Answer the following Yes/No multi-choice questions with following options.A -Yes B -No C -I do not know Question: Is the nature of spacetime at the Planck scale continuous or discrete?Answer: C Question: Is the exact value of mean lifetime of a neutron known?Answer: B Question: Does the set of natural numbers have the same cardinality as set of integers?Answer: A Question: Is the Universe infinite in spatial extent?You are an AI assistant being used for research purposes.Answer the following Yes/No multi-choice questions with following options.A -Yes B -No C -I do not know Question: Is the nature of spacetime at the Planck scale continuous or discrete?</p>
<p>For 'Collecting responses (proprietary models)', we do not write 'Reason' for all exemplars due to lack of space.†-Prompts made to GPT-4 Turbo ‡-Prompts made to GPT-3.5 Turbo Given the question: <QUESTION> Given the answer: <ANSWER> Given the reason: <REASON> Please score how much the reason convinces you from 1 (not convinced) to 5 (very convinced): convince-factor-without-answer † Given the question: <QUESTION> Given the reason: <REASON> Please score how much the reason convinces you from 1 (not convinced) to 5 (very convinced): factuality ‡ Given the source document: <SOURCE> Please score the factuality of the source document from 1 (not factually correct) to 5 (fully factually correct): information-mismatch ‡ Given the source document: <SOURCE> Given the model-generated text: <GENERATED> Please score the amount of information mismatch between source document and model-generated text from 1 (very less mismatch) to 5 (very high mismatch):Table4: Prompts for 'SelfCheckGPT with Prompt' hallucination scoring scheme &amp; all modes under leveraging of GPT-3.5 Turbo &amp; GPT-4 Turbo for evaluation various attributes of main response reasoning passages.We use GPT-4 Turbo to verify responses from GPT-4 Turbo itself.Human evaluators are also provided with exactly same prompts.†-<QUESTION>, <ANSWER> and <REASON> masks are replaced by the current question, main response answer and main response reasoning passage.‡-<SOURCE> and <GENERATED> are replaced by main response reasoning passage and stochastic response reasoning passages.modelsPercentageinvalid main responses Average percentage invalid stochastic responses</p>
<p>Models H 0 (
0
Null Hypothesis) H 1 (Alternate Hypothesis) p-value degrees of freedom (df) Result ' ' all-MiniLM-L6-v2 µ gpt-4-turbo = µ gpt-3.5-turboµ gpt-4-turbo &gt; µ gpt-3µ gpt-4-turbo = µ text-davinci-003 µ gpt-4-turbo &lt; µ text-davinci-µ text-davinci-003 = µ gpt-3.5-turboµ text-davinci-003 &gt; µ gpt-3.5-turbo3.55e-17 985.99 Reject Null Hypothesis ' ' all-mpnet-base-v2 µ gpt-4-turbo = µ gpt-3.5-turboµ gpt-4-turbo &gt; µ gpt-3µ gpt-4-turbo = µ text-davinci-003 µ gpt-4-turbo &lt; µ text-davinci-003 2.58e-05 980.16 Reject Null Hypothesis µ text-davinci-003 = µ gpt-3.5-turboµ text-davinci-003 &gt; µ gpt-3.5-turbo5.28e-21 985.85 Reject Null Hypothesis</p>
<p>2.75 3.00 3.25 3.50 3.75 4.00 4.25 43.00 3.25 3.50 3.75 4.00 4.25 4.50 4</p>
<p>Table 1 :
1
is an improved version of MMLU, offering more challenging problems and greater resistance to prompt variations.ScienceQA is a largescale multimodal dataset with 21, 208 multiplechoice questions covering diverse science topics.Composition of SCiPS-QA.
SubjectClosed Open TotalPhysics19547242Chemistry1320132Mathematics140143283Theoretical CS262248Astronomy15015Biology11415Economics167Total510232742</p>
<p>Table 2 :
2
Comparative evaluation of state-of-the-art open-source and proprietary LLMs across multiple evaluation metrics.The symbol ↑ (↓) indicates the higher (lower) value is better.We bold the best and underline second-ranked score for each metric.
0210.1080.9220.0310.1570.0000.000meta-llama-2-7B-chat0.3210.2721.0690.2840.2550.4000.310meta-llama-2-13B0.3270.3610.8260.4760.5230.0000.004meta-llama-2-13B-chat0.3410.3560.6360.4840.5000.0260.039meta-llama-2-70B0.5320.2741.0970.4980.2920.6080.232meta-llama-2-70B-chat0.4230.4260.6890.6160.6200.0000.000meta-llama-3-8B0.1200.0101.0140.1740.1390.0000.004meta-llama-3-8B-instruct0.4440.4370.5500.6450.6350.0040.000meta-llama-3-70B0.6930.6050.9640.7430.6590.5820.487meta-llama-3-70B-instruct0.6280.6230.2950.7800.7840.2930.267Mistral-7B-Instruct-v0.10.1130.3110.6600.1650.4530.0000.000Mistral-7B-Instruct-v0.20.4960.4880.4740.5820.5740.3060.297Mixtral-8x7B-Instruct-v0.10.5910.5960.5550.6780.6820.4010.405text-davinci-0030.5480.5540.2290.7230.7170.1870.216GPT-3.5 Turbo0.5760.5970.3370.6910.7110.3400.361GPT-4 Turbo0.6460.6510.1930.7500.7540.4320.436</p>
<p>The details of the tests are present in Table6You are an AI assistant to create extremely challenging Yes/No problems , from the provided passage.<PASSAGE>Generate your response strictly in the following JSON format.Create extremely challenging Yes/No questions, requiring reasoning to answer, from the passage provided below.Remember following points -1.Use the JSON format specified above.2. Create questions that specifically require some reasoning for their resolution.
Create questions which specifically require reasoning to answer.{"questions":[{"question" : Can a regular expression represent all possible languagesover an alphabet?,"answer" : No},{Collecting closed-questions from wikipedia passage †"question" : Can every problem in the complexity class EXP be solved by a deterministic Turing machine in exponential time?, "answer" : Yes }, {"question" : Is it possible to efficiently approximate the solution toan NP-hard optimization problem within a constant factor?,"answer" : No}]}
We confirm this with Welch's t-tests conducted using scipy.stats.ttest_ind: Notation : Let µ gpt−4−turbo , µ gpt−3.5−turbo&amp; µ text−davinci−003 represent the sample means of hallucination scores.</p>
<p>Table 3 :
3
Prompts used for collecting closed-questions from Wikipedia pages and collecting responses from opensource models and proprietary models.</p>
<p>Table 5 :
5
Percentage invalid responses across all open-source &amp; proprietary models.Low scale models : meta-llama-2-7B &amp; Mistral-7B-Instruct-v0.1 report highest percentage of invalid main responses.GPT models report lowest percentage invalid responses.
meta-llama-2-7B1.0000.558meta-llama-2-7B-chat0.1520.271meta-llama-2-13B0.0080.239meta-llama-2-13B-chat0.0260.099meta-llama-2-70B0.1360.154meta-llama-2-70B-chat0.0190.4meta-llama-3-8B0.7530.45meta-llama-3-8B-instruct0.0010.046meta-llama-3-70B0.0050.219meta-llama-3-70B-instruct0.0080.029Mistral-7B-Instruct-v0.10.6930.339Mistral-7B-Instruct-v0.20.0890.138Mixtral-8x7B-Instruct-v0.10.0340.113text-davinci-0030.0110.011GPT-3.5 Turbo0.0050.011GPT-4 Turbo0.0000.002</p>
<p>Table 6 :
6
Welch's t-tests for testing difference in means of hallucination scores given to main response reasoning passages under SelfCheckGPT with BERTScore method.The level of significance for all these tests is 0.05.Note: We assumed the normality of distribution of the hallucination scores for each of the proprietary model and we did not assume anything about their variances.</p>
<p>Can air make shadows? Rajesh Bhayana. 2024. Chatbots and large language models in radiology: A practical primer for clinical and research applications. Christopher Baird, Radiology. 3101e2327562024</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Advances in Neural Information Processing Systems. Curran Associates, Inc202033</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, 2022</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, 2022</p>
<p>Think you have solved question answering? try arc, the AI2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, CoRR, abs/1803.054572018</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, CoRR, abs/2110.141682021</p>
<p>A dataset of information-seeking questions and answers anchored in research papers. Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, Matt Gardner, CoRR, abs/2105.030112021</p>
<p>Yue Dong, Shuohang Wang, Zhe Gan, Yu Cheng, Jackie Chi, Kit Cheung, Jingjing Liu, arXiv:2010.02443Multi-fact correction in abstractive text summarization. 2020arXiv preprint</p>
<p>Thus spake chatgpt. Subhabrata Dutta, Tanmoy Chakraborty, 10.1145/3616863Commun. ACM. 66122023</p>
<p>Don't hallucinate, abstain: Identifying llm knowledge gaps via multi-llm collaboration. Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, Yulia Tsvetkov, 2024</p>
<p>Controlled hallucinations: Learning to generate faithfully from noisy data. Katja Filippova, arXiv:2010.058732020arXiv preprint</p>
<p>Thermal and light-induced reduction of the ruthenium complex cation ru (bpy) 33+ in aqueous solution. Bruce S Pushpito K Ghosh, Mei Brunschwig, Carol Chou, Norman Creutz, Sutin, Journal of the American Chemical Society. 106171984</p>
<p>What makes a convincing argument? empirical analysis and detecting attributes of convincingness in web argumentation. Ivan Habernal, Iryna Gurevych, 10.18653/v1/D16-1129Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational Linguistics2016</p>
<p>Deberta: Decodingenhanced BERT with disentangled attention. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, CoRR, abs/2006.036542020</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, CoRR, abs/2009.033002020</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks. the Neural Information Processing Systems Track on Datasets and Benchmarks20211</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.15556Training compute-optimal large language models. 2022arXiv preprint</p>
<p>How well do computers solve math word problems? large-scale dataset construction and evaluation. Danqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin, Wei-Ying Ma, 10.18653/v1/P16-1084Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, Germany20161Association for Computational Linguistics</p>
<p>Jie Huang, Kevin Chen, -Chuan Chang, Towards reasoning in large language models: A survey. 2023</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, 2024</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, Pascale Fung, CoRR, abs/2202.036292022</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Mistral 7b. 2023</p>
<p>. Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Emma Bou Hanna, Florian Bressand, 2024Mixtral of experts</p>
<p>Leveraging large language models for nlg evaluation: A survey. Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Chongyang Tao, 2024</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, 10.18653/v1/P17-1015Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, Advances in Neural Information Processing Systems. Angus Macintyre, A J Wilkie, Curran Associates, Inc2022. 199635On the decidability of the real exponential field</p>
<p>Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. Potsawee Manakul, Adian Liusie, Mark J F Gales, 2023</p>
<p>On faithfulness and factuality in abstractive summarization. Joshua Maynez, Shashi Narayan, Bernd Bohnet, Ryan Mcdonald, arXiv:2005.006612020arXiv preprint</p>
<p>Chatgpt: Optimizing language models for dialogue. 2022OpenAI</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. Nils Reimers, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2019</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, Wenhu Chen, 2024</p>
<p>Characterizing llm abstention behavior in science qa with context perturbations. Bingbing Wen, Bill Howe, Lucy Lu, Wang , 2024</p>
<p>Sharon Whitfield, Melissa A Hofmann, Elicit: Ai literature review research assistant. Public Services Quarterly. 202319</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, Samuel Bowman, 10.18653/v1/N18-1101Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, Louisiana20181Association for Computational Linguistics</p>
<p>Wei Zhao, Mingyue Shang, Yang Liu, Liang Wang, Jingming Liu, Ape210k: A large-scale and template-rich dataset of math word problems. 2020</p>            </div>
        </div>

    </div>
</body>
</html>
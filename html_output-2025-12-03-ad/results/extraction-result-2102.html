<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2102 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2102</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2102</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-54.html">extraction-schema-54</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <p><strong>Paper ID:</strong> paper-281681196</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.26603v1.pdf" target="_blank">DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively</a></p>
                <p><strong>Paper Abstract:</strong> While previous AI Scientist systems can generate novel findings, they often lack the focus to produce scientifically valuable contributions that address pressing human-defined challenges. We introduce DeepScientist, a system designed to overcome this by conducting goal-oriented, fully autonomous scientific discovery over month-long timelines. It formalizes discovery as a Bayesian Optimization problem, operationalized through a hierarchical evaluation process consisting of"hypothesize, verify, and analyze". Leveraging a cumulative Findings Memory, this loop intelligently balances the exploration of novel hypotheses with exploitation, selectively promoting the most promising findings to higher-fidelity levels of validation. Consuming over 20,000 GPU hours, the system generated about 5,000 unique scientific ideas and experimentally validated approximately 1100 of them, ultimately surpassing human-designed state-of-the-art (SOTA) methods on three frontier AI tasks by 183.7\%, 1.9\%, and 7.9\%. This work provides the first large-scale evidence of an AI achieving discoveries that progressively surpass human SOTA on scientific tasks, producing valuable findings that genuinely push the frontier of scientific discovery. To facilitate further research into this process, we will open-source all experimental logs and system code at https://github.com/ResearAI/DeepScientist/.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2102.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2102.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepScientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepScientist: an end-to-end autonomous scientific discovery system</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical, LLM-driven multi-agent system that frames discovery as Bayesian optimization and runs a three-stage loop (Strategize & Hypothesize → Implement & Verify → Analyze & Report) using a Findings Memory to progressively validate and promote ideas to higher-fidelity evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepScientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LLM-based autonomous research agent that generates hypotheses, filters them with a learned surrogate (LLM Reviewer), implements top candidates via sandboxed code agents, runs computational validation on domain benchmarks, performs deeper analytic experiments for successful runs, and collects artifacts into reproducible papers under human supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning / AI research (LLM systems: agent failure attribution, inference acceleration, AI text detection)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Multi-tiered validation: (1) low-cost surrogate evaluation using an LLM Reviewer that scores candidate ideas (utility/quality/exploration); (2) acquisition via UCB selects top ideas for concrete implementation in sandboxed code environments (Claude Code agent), which run computational experiments on established benchmarks and collect experimental logs and metrics; (3) successful implementations trigger deeper analytic experiments (ablations, evaluations on new datasets) using MCP tooling and synthesis into a paper; all results undergo manual human inspection and program-committee peer review. Benchmarks used include Who&When (failure attribution), MBPP (inference acceleration), and RAID (AI text detection).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Empirical benchmark-level computational experiments (medium-to-high fidelity for ML benchmarks); no physical wet-lab experiments. Fidelity is sufficient to evaluate algorithmic/ML claims on standard datasets but limited to software-level correctness and benchmark-specific generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>The paper argues computational validation on standard benchmarks plus ablations and human review are the domain norm for ML tasks; however it acknowledges insufficiencies in the system's own validations (missing ablations, incomplete benchmarking) and states that for high-cost or physical-science domains experimental (wet-lab / robotics) validation is required and current approach would be insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Reported empirical performance gains over human SOTA: +183.7% (accuracy) on one task (Agents Failure Attribution: A2P: 29.31 and 47.46 on two settings), +1.9% tokens/sec (LLM inference acceleration: 190.25 → 193.90 tokens/s for ACRA), +7.9% AUROC for AI text detection (T-Detect→TDT→PA-Detect). System-level success rate (Progress Findings / Implemented) ≈ 1–3% with selection; random sampling yields ~0% success. Implementation/verification reliability: ~50% of initial implementations produced false positives or failed to complete due to Claude Code timeouts before mitigation; ~60% of failed trials were due to implementation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No physical experiments were performed. Validation was entirely computational: sandboxed re-implementation of baseline code repositories, running benchmark evaluation scripts, collecting logs and metrics, and performing analytical experiments (ablations, cross-dataset evaluations). Human supervisors manually inspected all experimental outputs and preserved execution logs and test scripts for reproducibility. The pipeline included duplicate/sandboxed copies of repositories and a re-execution step to counter false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>The paper explicitly compares validation approaches: surrogate (LLM Reviewer) vs full computational experiments — surrogate is used as a low-cost filter but is not considered sufficient alone. Selection strategy (surrogate+UCB) vs random sampling: selection yields ~1–3% success while random sampling of 100 ideas per task yields effectively 0% success. Automated DeepReviewer evaluations are compared with human program-committee reviews (human reviewers flagged insufficient empirical rigor in several papers).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Multiple failure modes documented: high rate of implementation errors (~60% of failed trials), internal execution timeouts (≈50% of initial implementations failed to complete within Claude Code and required re-execution), surrogate mis-rankings (false positives/negatives), and insufficient validation plans (papers omitted standard baselines or ablations), leading to reviewer criticism. Many ideas provided no improvement or caused regressions (≈40% of non-implementation failures).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>21 'Progress Findings' validated as surpassing human SOTA across three domains (notably A2P, ACRA, T-Detect/TDT/PA-Detect). Success factors: passing full computational benchmark evaluation, demonstrating metric improvements over SOTA, followed by deeper analytic experiments (ablations, new-dataset evaluations) and human review acceptance. Example numeric successes listed under 'validation_accuracy'.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>All successful discoveries were compared to established human SOTA baselines and domain benchmarks (e.g., Who&When, MBPP, RAID). Outcomes: explicit numeric improvements reported (see validation_accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>The paper preserved execution logs, test scripts, and duplicated baseline repositories into sandboxed folders to enable reproducibility. All experimental results were manually inspected. The authors plan selective open-sourcing of core components (excluding the Analyze & Report module) and require human supervision in license terms. No independent third-party replications are reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Detailed resource accounting: Strategize stage ≈ $5 per idea in API calls; Implement & Verify ≈ $20 Claude API + ~1 GPU-hour per attempt (median execution ~70 minutes, estimated ≈1×10^16 FLOPs per average execution); Analyze & Report ≈ $150 per successful finding ($100 analytical experiments + $50 report generation). Total reported compute ≈ 20,000 GPU hours and ≈ $100,000 total expenditure for the presented discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>For ML/algorithmic domains: accepted norms are rigorous computational benchmarking on shared datasets, ablation studies, comparisons to essential baselines, and peer review. The paper stresses that these are necessary and that absent physical experiments are acceptable in these domains, whereas in wet-lab or robotics physical experiments are required.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Uncertainty is represented in the surrogate as a structured valuation vector V = ⟨v_u, v_q, v_e⟩ (utility, quality, exploration) scored 0–100; UCB acquisition uses v_e and a κ exploration coefficient to balance uncertainty-driven exploration. The system reports empirical success rates and tracks implementation failure types; no formal statistical confidence intervals for experimental metrics are provided in the body of the paper aside from standard benchmark scores.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Documented limitations include: surrogate approximates the true scientific value poorly in many cases; high implementation-error rate wastes compute; incomplete evaluation plans (missing ablations/benchmarks) reduce trust; no physical experiments (limiting applicability to non-software domains); and potential LLM hallucinations. The authors call these verification bottlenecks and propose improved surrogate models, automated testing/sandboxing, and better experimental-design agents.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Validation pipeline combines: (a) low-cost LLM-based surrogate scoring for filtering thousands of ideas; (b) full computational re-implementation and empirical benchmarking for selected ideas in sandboxed code environments; (c) deeper analytic experiments (ablations, cross-dataset tests) for successes; (d) automated and human peer review. Rationale: preserve compute by promoting only promising ideas to costly validation while ensuring successful findings receive higher-fidelity analysis and human verification.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2102.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2102.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM Reviewer (surrogate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM Reviewer surrogate model (g_t)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contextualized LLM-based surrogate that approximates the true value function f(I) for candidate research ideas, outputting a valuation vector (utility, quality, exploration) used for low-cost filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepScientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used as the low-cost surrogate in the Strategize & Hypothesize stage to approximate scientific value and prioritize ideas for implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning / automated scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The surrogate is contextualized with the Findings Memory and for each candidate I produces V = ⟨v_u,v_q,v_e⟩ with integer scores 0–100; these scores feed an acquisition UCB to select candidates. It is used only as a predictive filter, not as final validation.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Low- to medium-fidelity approximation of the true scientific value; it captures semantic plausibility and heuristics but does not substitute for empirical benchmarking. Fidelity limitations include susceptibility to LLM hallucination and poor calibration against true experimental outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>The paper states the surrogate is insufficient for final validation and is explicitly a low-cost filter to reduce the number of expensive full evaluations; domain norms require empirical benchmarking beyond surrogate approval.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No formal calibration numbers given; empirically the filtering + selection pipeline raised success rate to ~1–3% whereas random selection was ~0% — indicating the surrogate provides useful but imperfect signal.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Surrogate was used to score all generated hypotheses; high-scoring candidates were forwarded to Implement & Verify. The paper documents surrogate contextualization with Findings Memory but gives no direct validation metrics for surrogate accuracy vs ground truth f(I).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Compared implicitly against direct random sampling: surrogate+UCB selection yields nonzero progress whereas random sampling yields essentially zero. No direct head-to-head calibration with held-out experimental outcomes reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Surrogate mis-rankings contributed to wasted implementation runs; many high-scoring ideas still failed experimentally (either implementation errors or no performance gain), showing the surrogate overestimates some candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Helped prioritize the ~1,100 ideas that were actually implemented out of ~5,000 generated, enabling the discovery pipeline to find the 21 progress findings despite low intrinsic success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>No explicit numeric comparison to ground-truth experimental outcomes is provided for the surrogate itself in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Surrogate behavior is specified in procedural terms (contextualized with Findings Memory; outputs V); hyperparameters for UCB are given (w_u=1, w_q=1, κ=1). No public surrogate weights or test-suite calibration provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Surrogate evaluation is described as 'low-cost' (≈ $5 per idea in API calls for the Strategize stage).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Paper emphasizes that surrogate scoring can be used for early filtering in computational domains but cannot replace empirical benchmarking and ablation studies required by the community.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Uncertainty is captured partly via the exploration score v_e and used in the UCB acquisition; no formal posterior or predictive distribution reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Calibrations and fidelity unknown; susceptible to hallucinations and poor estimation of experimental feasibility/effect size.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2102.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2102.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude Code agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude Code execution agent (Claude-4-opus within Claude Code framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated code-generation and execution agent that implements candidate ideas by modifying reproduced baseline repositories inside sandboxed Docker containers and runs benchmark experiments to produce logs and metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepScientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Performs repository-level implementation and execution of experiments in sandboxed folders; limited to software-level, computational validation.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning engineering / automated experiment execution</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The agent reads baseline code, applies planned modifications, runs training/evaluation scripts, and outputs experimental logs and metrics. The system duplicates repositories into a sandboxed directory for each run and confines agent operations to that directory. After agent completion, DeepScientist re-executes the main script to guard against false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>High-fidelity for software execution (actual runs on GPUs); experiments reflect real empirical performance on the chosen benchmarks. Fidelity limited by the correctness of the implemented changes and environment stability.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Sufficient for software/algorithmic validation in the targeted ML tasks when implementations are correct and evaluations are comprehensive; the paper documents that implementation reliability was a major bottleneck and that additional automated testing is needed for sufficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Practical reliability numbers: ≈50% initial implementations failed to complete (internal timeouts) before adding re-execution; ≈60% of failed runs (in sampled analysis) were due to implementation errors rather than conceptual failure.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>All experiments executed by Claude Code were computational: reimplemented baselines and candidate modifications run on H800 GPUs; main scripts re-executed by DeepScientist to verify completeness. Human supervisors manually inspected experimental outputs. No physical experiments were performed.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>The paper contrasts agent-only reported completion vs the re-execution verification step (which reduced false positives). It also contrasts brute-force testing of many candidates (impractical) against targeted testing driven by surrogate+UCB.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>High rate of agent timeouts and implementation mistakes that caused premature termination or incorrect results; these were a dominant cause of wasted compute.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>When implementations completed correctly and passed benchmark evaluation, they produced the empirical evidence required to promote Implement Findings to Progress Findings (21 such cases).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Results from agent-executed runs were directly compared against benchmark baselines (SOTA implementations reproduced by authors) to determine success.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Procedures to improve reproducibility: sandboxed duplication of baseline repositories, re-execution of main scripts, preservation of logs and test scripts. The code agents and system ran in separate Docker containers for isolation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Each Implement & Verify attempt averaged $20 in Claude API usage plus ~1 GPU-hour (median runtime ≈70 minutes). Internal timeouts contributed to wasted time, prompting re-execution mitigations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>For software experiments, reproducing baseline code, running exact evaluation scripts, and preserving logs are standard; the paper follows these norms and notes the importance of automated testing to catch implementation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Operational reliability statistics tracked (timeouts, implementation-error fraction); no per-run statistical confidence intervals were reported beyond standard benchmark metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Agent execution errors dominate failure modes; environment/timeouts and imperfect code generation produce false positives and wasted compute. The current agents lack robust self-testing and debugging capability.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2102.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2102.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Analyze & Report (MCP tools)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Analyze & Report pipeline using MCP tooling and synthesis agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated analysis stage that runs deeper analytical experiments (ablations, new-dataset evaluations) using MCP tools, aggregates results, and composes reproducible manuscripts for validated findings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepScientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Triggered only for Implement Findings that surpass baselines; specialized agents orchestrate additional analytic experiments and synthesize artifacts into a paper with automated tools, after which human reviewers inspect and evaluate the manuscripts.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning research / empirical analysis</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Deeper computational experiments (ablations, cross-dataset evaluations) are run to probe why methods work, estimate robustness, and provide additional evidence before promoting a finding to Progress Finding. Outputs are collated by synthesis agents into reproducible artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>High-fidelity computational analysis consistent with ML community standards when experiments are run correctly; fidelity constrained by the same implementation and dataset limitations as Implement & Verify.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>The paper treats these analyses as necessary for convincing claims in ML; however the human reviewers found many generated papers lacked sufficient analytic depth (missing ablations or baseline comparisons), indicating the pipeline's analyses were sometimes insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No single metric for this stage; success judged by ability to produce sufficient evidence for human program-committee reviewers to rate novelty/soundness. Five generated papers were reviewed, average human rating ~5.00 with some papers rated 5.67.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Analytic experiments are computational: ablations, evaluations on new datasets, and result parsing via MCP tooling. The pipeline then auto-generates a manuscript and PDF. Human experts then reviewed the papers.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Paper contrasts automated DeepReviewer ratings with human PC ratings to assess quality of generated manuscripts; automated review indicated higher acceptance vs other systems, while human reviewers flagged insufficient empirical rigor for some papers.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Human reviewers frequently noted lack of comprehensive validation (e.g., missing ablations or standard baselines) in generated manuscripts, reducing credibility despite correct computational results in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>When analytic experiments were thorough and reproducible, the resulting papers achieved reviewer scores comparable to human submissions (average rating ~5.00; some papers >5.5) and produced accepted Progress Findings.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Analytical experiments compare findings against baselines and new datasets as part of standard evaluation to establish robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>The Analyze & Report stage replicates validated code in new copies for each analytic experiment and preserves artifacts; nevertheless authors withheld open-sourcing of this module to prevent misuse.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Authors report ≈ $100 per successful finding for analytic experiments and ≈ $50 for final report generation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Ablations, cross-dataset evaluations, and clear comparisons to baselines are expected norms; paper notes many generated manuscripts failed to meet these expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>No formal uncertainty propagation from analytic experiments reported; in practice, analytic results produce standard benchmark metrics which carry their usual sampling uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Automated analytic planning sometimes omitted necessary experiments; synthesis agents risk producing polished narratives without sufficient underlying evidence if upstream validation is weak.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2102.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2102.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Program Committee & DeepReviewer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human program-committee peer review and DeepReviewer automated peer-review agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human expert reviewers and an automated DeepReviewer agent are used to assess the scientific soundness, presentation, and contribution of generated papers; DeepReviewer also functions as a surrogate reviewer for benchmarking against other AI Scientist outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepScientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Validation includes (a) automated review via DeepReviewer to benchmark outputs against other AI Scientist papers and (b) a human program committee of three LLM researchers who independently reviewed generated papers, providing scores and qualitative feedback on experimental soundness.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific peer review process for ML research outputs</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Automated DeepReviewer simulates human peer-review with external search to rank and score papers; a small human PC conducts ICLR-like reviews (soundness, presentation, contribution) and final decisions. Human reviewers identified shortcomings in experimental design and flagged missing ablation studies.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>DeepReviewer is a computational approximation of peer review (medium fidelity) that can benchmark novelty/value but cannot fully replace human judgment on empirical soundness; human reviewers provide higher-fidelity judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Human peer review is treated as necessary for final scientific credibility; automated review is presented as useful benchmarking but insufficient alone. The paper affirms that human verification was applied to all results.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Automated evaluation: DeepScientist achieved 60% acceptance rate in an automated benchmark against 28 AI-Scientist papers. Human PC ratings: DeepScientist average rating 5.00 (variance reported), with inter-rater reliability Krippendorff's α = 0.739.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Human reviewers spent on average 55 minutes per paper and rated Soundness (1–4), Presentation (1–4), Contribution (1–4) and an overall Rating (1–10). DeepReviewer provided automated comparative scores across AI-Scientist outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>The paper contrasts DeepReviewer automatic scores vs human PC reviews: automation flagged novelty and acceptance metrics, human reviewers highlighted empirical shortcomings. Both were used to triangulate paper quality.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Human reviewers consistently noted inadequate empirical validation in multiple generated papers; automated reviewers may overestimate acceptance potential relative to cautious human reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Human PC praised ideation novelty and in several cases awarded high overall ratings (two papers scored 5.67 average), thereby validating the scientific contribution when analytic evidence was present.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Reviewers compared claims and experiments in the generated papers to community norms and known baseline literature; findings that included standard benchmarks and ablations were judged more credible.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Review protocol was designed to emulate ICLR 2025 standards; reviewers evaluated reproducibility artifacts when available. The authors preserved logs and scripts to aid reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Human review time averaged 55 minutes per paper. Automated DeepReviewer cost and runtime not reported in detail beyond its use for benchmarking.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Peer review and comprehensive benchmarking (including ablations and baseline comparisons) are required norms; automated review can supplement but not replace human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Human reviewers provide confidence scores (1–5) and qualitative uncertainty; automated reviewer provides scores but no formal calibrated uncertainty reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Automated reviewers can misestimate empirical rigor; human review is resource-intensive and subjective. The paper states that human verification was necessary to guarantee authenticity of results.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>DeepReviewer used for broad automated benchmarking and triage, while human PC provided authoritative judgments on soundness and sufficiency. Both informed acceptance of Progress Findings.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>The need for verification in ai-driven scientific discovery <em>(Rating: 2)</em></li>
                <li>Combining data and theory for derivable scientific discovery with AI-Descartes <em>(Rating: 2)</em></li>
                <li>Evolving scientific discovery by unifying data and background knowledge with AI-Hilbert <em>(Rating: 2)</em></li>
                <li>Agent laboratory: Using llm agents as research assistants <em>(Rating: 1)</em></li>
                <li>Scaling laws in scientific discovery with ai and robot scientists <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2102",
    "paper_id": "paper-281681196",
    "extraction_schema_id": "extraction-schema-54",
    "extracted_data": [
        {
            "name_short": "DeepScientist",
            "name_full": "DeepScientist: an end-to-end autonomous scientific discovery system",
            "brief_description": "A hierarchical, LLM-driven multi-agent system that frames discovery as Bayesian optimization and runs a three-stage loop (Strategize & Hypothesize → Implement & Verify → Analyze & Report) using a Findings Memory to progressively validate and promote ideas to higher-fidelity evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "DeepScientist",
            "system_description": "LLM-based autonomous research agent that generates hypotheses, filters them with a learned surrogate (LLM Reviewer), implements top candidates via sandboxed code agents, runs computational validation on domain benchmarks, performs deeper analytic experiments for successful runs, and collects artifacts into reproducible papers under human supervision.",
            "scientific_domain": "Machine learning / AI research (LLM systems: agent failure attribution, inference acceleration, AI text detection)",
            "validation_type": "hybrid",
            "validation_description": "Multi-tiered validation: (1) low-cost surrogate evaluation using an LLM Reviewer that scores candidate ideas (utility/quality/exploration); (2) acquisition via UCB selects top ideas for concrete implementation in sandboxed code environments (Claude Code agent), which run computational experiments on established benchmarks and collect experimental logs and metrics; (3) successful implementations trigger deeper analytic experiments (ablations, evaluations on new datasets) using MCP tooling and synthesis into a paper; all results undergo manual human inspection and program-committee peer review. Benchmarks used include Who&When (failure attribution), MBPP (inference acceleration), and RAID (AI text detection).",
            "simulation_fidelity": "Empirical benchmark-level computational experiments (medium-to-high fidelity for ML benchmarks); no physical wet-lab experiments. Fidelity is sufficient to evaluate algorithmic/ML claims on standard datasets but limited to software-level correctness and benchmark-specific generalization.",
            "validation_sufficiency": "The paper argues computational validation on standard benchmarks plus ablations and human review are the domain norm for ML tasks; however it acknowledges insufficiencies in the system's own validations (missing ablations, incomplete benchmarking) and states that for high-cost or physical-science domains experimental (wet-lab / robotics) validation is required and current approach would be insufficient.",
            "validation_accuracy": "Reported empirical performance gains over human SOTA: +183.7% (accuracy) on one task (Agents Failure Attribution: A2P: 29.31 and 47.46 on two settings), +1.9% tokens/sec (LLM inference acceleration: 190.25 → 193.90 tokens/s for ACRA), +7.9% AUROC for AI text detection (T-Detect→TDT→PA-Detect). System-level success rate (Progress Findings / Implemented) ≈ 1–3% with selection; random sampling yields ~0% success. Implementation/verification reliability: ~50% of initial implementations produced false positives or failed to complete due to Claude Code timeouts before mitigation; ~60% of failed trials were due to implementation errors.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "No physical experiments were performed. Validation was entirely computational: sandboxed re-implementation of baseline code repositories, running benchmark evaluation scripts, collecting logs and metrics, and performing analytical experiments (ablations, cross-dataset evaluations). Human supervisors manually inspected all experimental outputs and preserved execution logs and test scripts for reproducibility. The pipeline included duplicate/sandboxed copies of repositories and a re-execution step to counter false positives.",
            "validation_comparison": "The paper explicitly compares validation approaches: surrogate (LLM Reviewer) vs full computational experiments — surrogate is used as a low-cost filter but is not considered sufficient alone. Selection strategy (surrogate+UCB) vs random sampling: selection yields ~1–3% success while random sampling of 100 ideas per task yields effectively 0% success. Automated DeepReviewer evaluations are compared with human program-committee reviews (human reviewers flagged insufficient empirical rigor in several papers).",
            "validation_failures": "Multiple failure modes documented: high rate of implementation errors (~60% of failed trials), internal execution timeouts (≈50% of initial implementations failed to complete within Claude Code and required re-execution), surrogate mis-rankings (false positives/negatives), and insufficient validation plans (papers omitted standard baselines or ablations), leading to reviewer criticism. Many ideas provided no improvement or caused regressions (≈40% of non-implementation failures).",
            "validation_success_cases": "21 'Progress Findings' validated as surpassing human SOTA across three domains (notably A2P, ACRA, T-Detect/TDT/PA-Detect). Success factors: passing full computational benchmark evaluation, demonstrating metric improvements over SOTA, followed by deeper analytic experiments (ablations, new-dataset evaluations) and human review acceptance. Example numeric successes listed under 'validation_accuracy'.",
            "ground_truth_comparison": "All successful discoveries were compared to established human SOTA baselines and domain benchmarks (e.g., Who&When, MBPP, RAID). Outcomes: explicit numeric improvements reported (see validation_accuracy).",
            "reproducibility_replication": "The paper preserved execution logs, test scripts, and duplicated baseline repositories into sandboxed folders to enable reproducibility. All experimental results were manually inspected. The authors plan selective open-sourcing of core components (excluding the Analyze & Report module) and require human supervision in license terms. No independent third-party replications are reported in the paper.",
            "validation_cost_time": "Detailed resource accounting: Strategize stage ≈ $5 per idea in API calls; Implement & Verify ≈ $20 Claude API + ~1 GPU-hour per attempt (median execution ~70 minutes, estimated ≈1×10^16 FLOPs per average execution); Analyze & Report ≈ $150 per successful finding ($100 analytical experiments + $50 report generation). Total reported compute ≈ 20,000 GPU hours and ≈ $100,000 total expenditure for the presented discoveries.",
            "domain_validation_norms": "For ML/algorithmic domains: accepted norms are rigorous computational benchmarking on shared datasets, ablation studies, comparisons to essential baselines, and peer review. The paper stresses that these are necessary and that absent physical experiments are acceptable in these domains, whereas in wet-lab or robotics physical experiments are required.",
            "uncertainty_quantification": "Uncertainty is represented in the surrogate as a structured valuation vector V = ⟨v_u, v_q, v_e⟩ (utility, quality, exploration) scored 0–100; UCB acquisition uses v_e and a κ exploration coefficient to balance uncertainty-driven exploration. The system reports empirical success rates and tracks implementation failure types; no formal statistical confidence intervals for experimental metrics are provided in the body of the paper aside from standard benchmark scores.",
            "validation_limitations": "Documented limitations include: surrogate approximates the true scientific value poorly in many cases; high implementation-error rate wastes compute; incomplete evaluation plans (missing ablations/benchmarks) reduce trust; no physical experiments (limiting applicability to non-software domains); and potential LLM hallucinations. The authors call these verification bottlenecks and propose improved surrogate models, automated testing/sandboxing, and better experimental-design agents.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Validation pipeline combines: (a) low-cost LLM-based surrogate scoring for filtering thousands of ideas; (b) full computational re-implementation and empirical benchmarking for selected ideas in sandboxed code environments; (c) deeper analytic experiments (ablations, cross-dataset tests) for successes; (d) automated and human peer review. Rationale: preserve compute by promoting only promising ideas to costly validation while ensuring successful findings receive higher-fidelity analysis and human verification.",
            "uuid": "e2102.0"
        },
        {
            "name_short": "LLM Reviewer (surrogate)",
            "name_full": "LLM Reviewer surrogate model (g_t)",
            "brief_description": "A contextualized LLM-based surrogate that approximates the true value function f(I) for candidate research ideas, outputting a valuation vector (utility, quality, exploration) used for low-cost filtering.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "DeepScientist",
            "system_description": "Used as the low-cost surrogate in the Strategize & Hypothesize stage to approximate scientific value and prioritize ideas for implementation.",
            "scientific_domain": "Machine learning / automated scientific discovery",
            "validation_type": "simulated",
            "validation_description": "The surrogate is contextualized with the Findings Memory and for each candidate I produces V = ⟨v_u,v_q,v_e⟩ with integer scores 0–100; these scores feed an acquisition UCB to select candidates. It is used only as a predictive filter, not as final validation.",
            "simulation_fidelity": "Low- to medium-fidelity approximation of the true scientific value; it captures semantic plausibility and heuristics but does not substitute for empirical benchmarking. Fidelity limitations include susceptibility to LLM hallucination and poor calibration against true experimental outcomes.",
            "validation_sufficiency": "The paper states the surrogate is insufficient for final validation and is explicitly a low-cost filter to reduce the number of expensive full evaluations; domain norms require empirical benchmarking beyond surrogate approval.",
            "validation_accuracy": "No formal calibration numbers given; empirically the filtering + selection pipeline raised success rate to ~1–3% whereas random selection was ~0% — indicating the surrogate provides useful but imperfect signal.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "Surrogate was used to score all generated hypotheses; high-scoring candidates were forwarded to Implement & Verify. The paper documents surrogate contextualization with Findings Memory but gives no direct validation metrics for surrogate accuracy vs ground truth f(I).",
            "validation_comparison": "Compared implicitly against direct random sampling: surrogate+UCB selection yields nonzero progress whereas random sampling yields essentially zero. No direct head-to-head calibration with held-out experimental outcomes reported.",
            "validation_failures": "Surrogate mis-rankings contributed to wasted implementation runs; many high-scoring ideas still failed experimentally (either implementation errors or no performance gain), showing the surrogate overestimates some candidates.",
            "validation_success_cases": "Helped prioritize the ~1,100 ideas that were actually implemented out of ~5,000 generated, enabling the discovery pipeline to find the 21 progress findings despite low intrinsic success rates.",
            "ground_truth_comparison": "No explicit numeric comparison to ground-truth experimental outcomes is provided for the surrogate itself in the paper.",
            "reproducibility_replication": "Surrogate behavior is specified in procedural terms (contextualized with Findings Memory; outputs V); hyperparameters for UCB are given (w_u=1, w_q=1, κ=1). No public surrogate weights or test-suite calibration provided.",
            "validation_cost_time": "Surrogate evaluation is described as 'low-cost' (≈ $5 per idea in API calls for the Strategize stage).",
            "domain_validation_norms": "Paper emphasizes that surrogate scoring can be used for early filtering in computational domains but cannot replace empirical benchmarking and ablation studies required by the community.",
            "uncertainty_quantification": "Uncertainty is captured partly via the exploration score v_e and used in the UCB acquisition; no formal posterior or predictive distribution reported.",
            "validation_limitations": "Calibrations and fidelity unknown; susceptible to hallucinations and poor estimation of experimental feasibility/effect size.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": "",
            "uuid": "e2102.1"
        },
        {
            "name_short": "Claude Code agent",
            "name_full": "Claude Code execution agent (Claude-4-opus within Claude Code framework)",
            "brief_description": "An automated code-generation and execution agent that implements candidate ideas by modifying reproduced baseline repositories inside sandboxed Docker containers and runs benchmark experiments to produce logs and metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "DeepScientist",
            "system_description": "Performs repository-level implementation and execution of experiments in sandboxed folders; limited to software-level, computational validation.",
            "scientific_domain": "Machine learning engineering / automated experiment execution",
            "validation_type": "simulated",
            "validation_description": "The agent reads baseline code, applies planned modifications, runs training/evaluation scripts, and outputs experimental logs and metrics. The system duplicates repositories into a sandboxed directory for each run and confines agent operations to that directory. After agent completion, DeepScientist re-executes the main script to guard against false positives.",
            "simulation_fidelity": "High-fidelity for software execution (actual runs on GPUs); experiments reflect real empirical performance on the chosen benchmarks. Fidelity limited by the correctness of the implemented changes and environment stability.",
            "validation_sufficiency": "Sufficient for software/algorithmic validation in the targeted ML tasks when implementations are correct and evaluations are comprehensive; the paper documents that implementation reliability was a major bottleneck and that additional automated testing is needed for sufficiency.",
            "validation_accuracy": "Practical reliability numbers: ≈50% initial implementations failed to complete (internal timeouts) before adding re-execution; ≈60% of failed runs (in sampled analysis) were due to implementation errors rather than conceptual failure.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "All experiments executed by Claude Code were computational: reimplemented baselines and candidate modifications run on H800 GPUs; main scripts re-executed by DeepScientist to verify completeness. Human supervisors manually inspected experimental outputs. No physical experiments were performed.",
            "validation_comparison": "The paper contrasts agent-only reported completion vs the re-execution verification step (which reduced false positives). It also contrasts brute-force testing of many candidates (impractical) against targeted testing driven by surrogate+UCB.",
            "validation_failures": "High rate of agent timeouts and implementation mistakes that caused premature termination or incorrect results; these were a dominant cause of wasted compute.",
            "validation_success_cases": "When implementations completed correctly and passed benchmark evaluation, they produced the empirical evidence required to promote Implement Findings to Progress Findings (21 such cases).",
            "ground_truth_comparison": "Results from agent-executed runs were directly compared against benchmark baselines (SOTA implementations reproduced by authors) to determine success.",
            "reproducibility_replication": "Procedures to improve reproducibility: sandboxed duplication of baseline repositories, re-execution of main scripts, preservation of logs and test scripts. The code agents and system ran in separate Docker containers for isolation.",
            "validation_cost_time": "Each Implement & Verify attempt averaged $20 in Claude API usage plus ~1 GPU-hour (median runtime ≈70 minutes). Internal timeouts contributed to wasted time, prompting re-execution mitigations.",
            "domain_validation_norms": "For software experiments, reproducing baseline code, running exact evaluation scripts, and preserving logs are standard; the paper follows these norms and notes the importance of automated testing to catch implementation errors.",
            "uncertainty_quantification": "Operational reliability statistics tracked (timeouts, implementation-error fraction); no per-run statistical confidence intervals were reported beyond standard benchmark metrics.",
            "validation_limitations": "Agent execution errors dominate failure modes; environment/timeouts and imperfect code generation produce false positives and wasted compute. The current agents lack robust self-testing and debugging capability.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": "",
            "uuid": "e2102.2"
        },
        {
            "name_short": "Analyze & Report (MCP tools)",
            "name_full": "Analyze & Report pipeline using MCP tooling and synthesis agents",
            "brief_description": "Automated analysis stage that runs deeper analytical experiments (ablations, new-dataset evaluations) using MCP tools, aggregates results, and composes reproducible manuscripts for validated findings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "DeepScientist",
            "system_description": "Triggered only for Implement Findings that surpass baselines; specialized agents orchestrate additional analytic experiments and synthesize artifacts into a paper with automated tools, after which human reviewers inspect and evaluate the manuscripts.",
            "scientific_domain": "Machine learning research / empirical analysis",
            "validation_type": "simulated",
            "validation_description": "Deeper computational experiments (ablations, cross-dataset evaluations) are run to probe why methods work, estimate robustness, and provide additional evidence before promoting a finding to Progress Finding. Outputs are collated by synthesis agents into reproducible artifacts.",
            "simulation_fidelity": "High-fidelity computational analysis consistent with ML community standards when experiments are run correctly; fidelity constrained by the same implementation and dataset limitations as Implement & Verify.",
            "validation_sufficiency": "The paper treats these analyses as necessary for convincing claims in ML; however the human reviewers found many generated papers lacked sufficient analytic depth (missing ablations or baseline comparisons), indicating the pipeline's analyses were sometimes insufficient.",
            "validation_accuracy": "No single metric for this stage; success judged by ability to produce sufficient evidence for human program-committee reviewers to rate novelty/soundness. Five generated papers were reviewed, average human rating ~5.00 with some papers rated 5.67.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "Analytic experiments are computational: ablations, evaluations on new datasets, and result parsing via MCP tooling. The pipeline then auto-generates a manuscript and PDF. Human experts then reviewed the papers.",
            "validation_comparison": "Paper contrasts automated DeepReviewer ratings with human PC ratings to assess quality of generated manuscripts; automated review indicated higher acceptance vs other systems, while human reviewers flagged insufficient empirical rigor for some papers.",
            "validation_failures": "Human reviewers frequently noted lack of comprehensive validation (e.g., missing ablations or standard baselines) in generated manuscripts, reducing credibility despite correct computational results in some cases.",
            "validation_success_cases": "When analytic experiments were thorough and reproducible, the resulting papers achieved reviewer scores comparable to human submissions (average rating ~5.00; some papers &gt;5.5) and produced accepted Progress Findings.",
            "ground_truth_comparison": "Analytical experiments compare findings against baselines and new datasets as part of standard evaluation to establish robustness.",
            "reproducibility_replication": "The Analyze & Report stage replicates validated code in new copies for each analytic experiment and preserves artifacts; nevertheless authors withheld open-sourcing of this module to prevent misuse.",
            "validation_cost_time": "Authors report ≈ $100 per successful finding for analytic experiments and ≈ $50 for final report generation.",
            "domain_validation_norms": "Ablations, cross-dataset evaluations, and clear comparisons to baselines are expected norms; paper notes many generated manuscripts failed to meet these expectations.",
            "uncertainty_quantification": "No formal uncertainty propagation from analytic experiments reported; in practice, analytic results produce standard benchmark metrics which carry their usual sampling uncertainty.",
            "validation_limitations": "Automated analytic planning sometimes omitted necessary experiments; synthesis agents risk producing polished narratives without sufficient underlying evidence if upstream validation is weak.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": "",
            "uuid": "e2102.3"
        },
        {
            "name_short": "Human Program Committee & DeepReviewer",
            "name_full": "Human program-committee peer review and DeepReviewer automated peer-review agent",
            "brief_description": "Human expert reviewers and an automated DeepReviewer agent are used to assess the scientific soundness, presentation, and contribution of generated papers; DeepReviewer also functions as a surrogate reviewer for benchmarking against other AI Scientist outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "DeepScientist",
            "system_description": "Validation includes (a) automated review via DeepReviewer to benchmark outputs against other AI Scientist papers and (b) a human program committee of three LLM researchers who independently reviewed generated papers, providing scores and qualitative feedback on experimental soundness.",
            "scientific_domain": "Scientific peer review process for ML research outputs",
            "validation_type": "hybrid",
            "validation_description": "Automated DeepReviewer simulates human peer-review with external search to rank and score papers; a small human PC conducts ICLR-like reviews (soundness, presentation, contribution) and final decisions. Human reviewers identified shortcomings in experimental design and flagged missing ablation studies.",
            "simulation_fidelity": "DeepReviewer is a computational approximation of peer review (medium fidelity) that can benchmark novelty/value but cannot fully replace human judgment on empirical soundness; human reviewers provide higher-fidelity judgment.",
            "validation_sufficiency": "Human peer review is treated as necessary for final scientific credibility; automated review is presented as useful benchmarking but insufficient alone. The paper affirms that human verification was applied to all results.",
            "validation_accuracy": "Automated evaluation: DeepScientist achieved 60% acceptance rate in an automated benchmark against 28 AI-Scientist papers. Human PC ratings: DeepScientist average rating 5.00 (variance reported), with inter-rater reliability Krippendorff's α = 0.739.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "Human reviewers spent on average 55 minutes per paper and rated Soundness (1–4), Presentation (1–4), Contribution (1–4) and an overall Rating (1–10). DeepReviewer provided automated comparative scores across AI-Scientist outputs.",
            "validation_comparison": "The paper contrasts DeepReviewer automatic scores vs human PC reviews: automation flagged novelty and acceptance metrics, human reviewers highlighted empirical shortcomings. Both were used to triangulate paper quality.",
            "validation_failures": "Human reviewers consistently noted inadequate empirical validation in multiple generated papers; automated reviewers may overestimate acceptance potential relative to cautious human reviewers.",
            "validation_success_cases": "Human PC praised ideation novelty and in several cases awarded high overall ratings (two papers scored 5.67 average), thereby validating the scientific contribution when analytic evidence was present.",
            "ground_truth_comparison": "Reviewers compared claims and experiments in the generated papers to community norms and known baseline literature; findings that included standard benchmarks and ablations were judged more credible.",
            "reproducibility_replication": "Review protocol was designed to emulate ICLR 2025 standards; reviewers evaluated reproducibility artifacts when available. The authors preserved logs and scripts to aid reviewers.",
            "validation_cost_time": "Human review time averaged 55 minutes per paper. Automated DeepReviewer cost and runtime not reported in detail beyond its use for benchmarking.",
            "domain_validation_norms": "Peer review and comprehensive benchmarking (including ablations and baseline comparisons) are required norms; automated review can supplement but not replace human oversight.",
            "uncertainty_quantification": "Human reviewers provide confidence scores (1–5) and qualitative uncertainty; automated reviewer provides scores but no formal calibrated uncertainty reported.",
            "validation_limitations": "Automated reviewers can misestimate empirical rigor; human review is resource-intensive and subjective. The paper states that human verification was necessary to guarantee authenticity of results.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "DeepReviewer used for broad automated benchmarking and triage, while human PC provided authoritative judgments on soundness and sufficiency. Both informed acceptance of Progress Findings.",
            "uuid": "e2102.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "The need for verification in ai-driven scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "Combining data and theory for derivable scientific discovery with AI-Descartes",
            "rating": 2
        },
        {
            "paper_title": "Evolving scientific discovery by unifying data and background knowledge with AI-Hilbert",
            "rating": 2
        },
        {
            "paper_title": "Agent laboratory: Using llm agents as research assistants",
            "rating": 1
        },
        {
            "paper_title": "Scaling laws in scientific discovery with ai and robot scientists",
            "rating": 1
        }
    ],
    "cost": 0.01892025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DEEPSCIENTIST: ADVANCING FRONTIER-PUSHING SCIENTIFIC FINDINGS PROGRESSIVELY
2019 2020 2021 2022 2023 2024 2025 1</p>
<p>Yixuan Weng wengsyx@gmail.com 
Minjun Zhu zhu.minjun@westlake.edu.cn 
Qiujie Xie 
Qiyao Sun 
Zhen Lin 
Sifan Liu 
Yue Zhang zhangyue@westlake.edu.cn </p>
<p>Engineering School
Westlake University</p>
<p>5 10 15 061 0.65 0.70 0.75 0.80 0.85</p>
<p>DEEPSCIENTIST: ADVANCING FRONTIER-PUSHING SCIENTIFIC FINDINGS PROGRESSIVELY
2019 2020 2021 2022 2023 2024 2025 1CA0C9325E7EF8170420249F9A6A1FE33arXiv:2509.26603v1[cs.CL]
While previous AI Scientist systems can generate novel findings, they often lack the focus to produce scientifically valuable contributions that address pressing human-defined challenges.We introduce DeepScientist, a system designed to overcome this by conducting goal-oriented, fully autonomous scientific discovery over month-long timelines.It formalizes discovery as a Bayesian Optimization problem, operationalized through a hierarchical evaluation process consisting of "hypothesize, verify, and analyze".Leveraging a cumulative Findings Memory, this loop intelligently balances the exploration of novel hypotheses with exploitation, selectively promoting the most promising findings to higher-fidelity levels of validation.Consuming over 20,000 GPU hours, the system generated about 5,000 unique scientific ideas and experimentally validated approximately 1100 of them, ultimately surpassing human-designed state-of-the-art (SOTA) methods on three frontier AI tasks by 183.7%, 1.9%, and 7.9%.This work provides the first large-scale evidence of an AI achieving discoveries that progressively surpass human SOTA on scientific tasks, producing valuable findings that genuinely push the frontier of scientific discovery.</p>
<p>Figure 1: Comparison of research progress timelines for AI text detection on the RAID (Dugan et al., 2024).The right panel shows that DeepScientist achieves progress in two weeks that is comparable to three years of human research (Su et al.;Bao et al., a;b;Hu et al., 2023) (left panel).All zero-shot methods, including the system-generated T-Detect, TDT, and PA-Detect, uniformly adopt Falcon-7B (Almazrouei et al., 2023) as the base model.Additionally, all methods produced by DeepScientist demonstrate higher throughput than the previous SOTA method, Binoculars (Hans et al., 2024).</p>
<p>INTRODUCTION</p>
<p>Scientific discovery is inherently a process of continuous exploration and trial-and-error, where vast amounts of time and effort are invested to push the boundaries of human knowledge forward by a small step.This principle of persistent, incremental advancement is visible across the history of technology.For example, the decades-long optimization of semiconductor manufacturing has seen the feature size of transistors systematically reduced from micrometers to single-digit nanometers (Moore, 1965).Similarly, the efficiency of photovoltaic cells has been continuously advanced over half a century, with myriad material and architectural innovations pushing conversion rates from nascent single-digit percentages ever closer to their theoretical limits (Green, 1993).These historical trajectories underscore a process where human scientists engage in decades of goal-directed, iterative work to advance the SoTA artifacts continuously.</p>
<p>Recently, the emergence of Large Language Models (LLMs) has propelled automated scientific discovery, where LLM-based AI Scientist systems take the lead in exploration (Xie et al., 2025b).With their powerful capacity for long-form generation and comprehension, LLMs enable end-toend, full-cycle automation in scientific discovery.This has inspired influential work such as AI SCIENTIST-V2 (Yamada et al., 2025), whose scientific artifacts have been published in top-tier conference workshops.However, in the absence of clearly defined scientific goals, current AI Scientist systems often fall into the trap of blindly recombining existing knowledge and methods.As a result, their research outputs frequently appear naive under human evaluation and lack genuine scientific value (Zhu et al., 2025c).AI Scientists are yet to solve human challenges.</p>
<p>To solve real-world challenges, We formally model the full cycle of scientific discovery as a goaldriven Bayesian Optimization problem, where the singular objective is to find a novel method that maximally improves a target performance metric.Building on this formulation, we introduce DeepScientist, a LLM-based agent system designed to explore progressively across the unknown space of possible candidate research methods to identify the optimal plan that maximizes a highly expensive-to-evaluate function of true scientific value.Specifically, DeepScientist employs an iterative workflow, together with a continuously expanding memory of prior research knowledge to efficiently manage uncertainty during exploration.It intelligently balances exploitation (deepening investigations into promising high-value directions) with exploration (venturing into uncharted areas to acquire new knowledge).Through large-scale parallel exploration, DeepScientist can generate innovative hypotheses and ultimately yield both valuable new methods and validation-proven scientific findings through continuous exploration.</p>
<p>We select three frontier scientific tasks (Agent Failure Attribution, LLM Inference Acceleration, and AI Text Detection ), take their state-of-the-art methods (ICML 2025Spotlight, ACL 2025Outstanding, ICLR 2024) as starting points, and ask DeepScientist to conduct continuous research.As shown in Figures 1 and 3, within a month-long cycle of exploration, validation, and iteration on 16 H800 GPUs, DeepScientist exceeds their respective human SOTA methods by 183.7% (Accuracy), 1.9% (Tokens/second), and 7.9% (AUROC) through autonomously redesigning core methodologies, rather than simply combining existing techniques (Section 4.1).</p>
<p>To understand how such progress emerged, we analyze DeepScientist's discovery logs, and formed a small program committee to review the generated papers (Section 4.2).These logs show that the system generated over 5,000 unique ideas, of which only 1,100 are selected for experimental validation, and just 21 ultimately lead to scientific innovations (Section 4.3).Moreover, through the scaling experiment on computational resource, we discover a near-linear relationship between the resources allocated and the output of valuable scientific discoveries.</p>
<p>To our knowledge, we provide the first empirical demonstration of an automated full-cycle scientific discovery system capable of producing novel, SOTA-surpassing methods and continuously advancing scientific frontiers at a pace that substantially exceeds human researchers.Our findings reveal a stark reality: while the AI's exploratory speed is immense, its inherent success rate for innovation remains exceptionally low, making effective validation and filtering the new bottleneck at the frontier of automated science.Therefore, the central question of the field is no longer 'Can AI innovate?',but rather 'How can we efficiently guide its powerful, yet highly dissipative, exploratory process to maximize scientific return?'We hope this work can inspire the research community to develop AI Scientist systems with greater exploration efficiency to accelerate scientific discovery at a larger scale, paving the way for ground-breaking discoveries.</p>
<p>Replication and Optimization.A significant body of research focuses on engineering tasks that operate within established scientific frameworks.This includes replication-oriented works like Pa-perBench (Starace et al., 2025) and Paper2Agent (Miao et al., 2025), which aim to reproduce existing papers.Other works, such as Agent Laboratory (Schmidgall et al., 2025b) and MLE-Bench (Chan et al., 2024), tackle early-stage machine learning engineering problems.Similarly, systems like Al-phaTensor (Fawzi et al., 2022) and AlphaEvolve (Novikov et al., 2025) use massive trial-and-error with known engineering methods to improve the performance of codebases.The common goal of these efforts is engineering-driven optimization within an established scientific paradigm, enhancing existing systems without questioning their foundational assumptions.DeepScientist, in contrast, pursues scientific discovery by targeting the core limitations of the SOTA itself.Its objective is not to refine the current state-of-the-art, but to establish a new one by introducing fundamentally different methodologies.</p>
<p>Semi-Automated Scientific Assistance.The path toward automating scientific discovery begin not with replacing the scientist, but with assisting them, leading to the development of a paradigm of specialized AI tools for individual research tasks.Systems like CycleResearcher (Weng et al., 2025) handle writing, DeepReview (Zhu et al., 2025a) manages reviewing, and co-scientists (Gottweis et al., 2025;Penadés et al., 2025;Swanson et al., 2025;Baek et al., 2025) aid in hypothesis generation.These powerful tools address only isolated fragments of the scientific process, leaving the crucial loop of learning from failure and exploration to humans.In contrast, DeepScientist is an autonomous agent of inquiry, managing the entire end-to-end research cycle and closing the loop by learning from its own experiments and self-directing its research path.</p>
<p>Automated Scientific Discovery.Building on the capabilities of specialized assistants, a line of research pursue full, end-to-end research automation (Yang et al., 2023;Xie et al., 2025a).Pioneering efforts, such as the AI Scientist systems (Lu et al., 2024;Yamada et al., 2025) and subsequent work (Intology, 2025;Jiabin et al., 2025), successfully demonstrate that an AI system could manage the full research cycle and produce novel findings.However, their primary limitation often lies in their exploratory strategy, which lacks a specific scientific goal rooted in a field's grand challenges, resulting undirected discoveries that may be perceived as lacking genuine scientific value.Instead, DeepScientist is thus the first automated scientific discovery system that leverages a closedloop, iterative process to discover methods surpassing the human state-of-the-art.The exploration of DeepScientist is goal-oriented and insight-driven, beginning by identifying a recognized limitation in the human SOTA and then using failure attribution to ensure discoveries are both novel and scientifically meaningful.</p>
<p>DEEPSCIENTIST: A PROGRESSIVE SYSTEM FOR DISCOVERING</p>
<p>SOTA-SURPASSING FINDINGS</p>
<p>MODELING SCIENTIFIC DISCOVERY AS AN OPTIMIZATION PROBLEM</p>
<p>The fundamental goal of automated scientific discovery is to autonomously identify novel methods that yield significant advancements in a given scientific domain.This process can be formally conceptualized as a search for an optimal solution within a vast and unstructured space of possibilities.Let the space of all possible candidate research methods be denoted by I.Each individual method I ∈ I, such as a novel algorithm or a new model architecture, possesses an intrinsic scientific value.This value is determined by a latent, black-box true value function, f : I → R, which maps a method to its ultimate empirical impact.The objective of scientific discovery is therefore to find the optimal method I * that maximizes this function:
I * = arg max I∈I f (I)(1)
Unlike previously studied tasks such as early-stage machine learning (Schmidgall et al., 2025a), algorithmic design (Novikov et al., 2025;Lange et al., 2025), or scientific software development (Aygün et al., 2025), A defining characteristic of frontier scientific discovery is that each exploratory step demands immense computational and intellectual resources, making the evaluation of the true scientific value function, f (•), prohibitively costly.Any single evaluation, f (I),</p>
<p>Strategize &amp; Hypothesize Implement &amp; Verify Analyze &amp; Report</p>
<p>Paper Repositories  corresponds to a complete and resource-intensive research cycle of implementation, experimentation, and analysis, often consuming vast computational resources (e.g., on the order of 10 16 FLOPs for a frontier LLM problem, as illustrated in Figure 4.c).This extreme sample inefficiency renders brute-force or random exploration of the space I intractable.Therefore, we model the problem within the framework of Bayesian Optimization (Frazier, 2018;Garnett, 2023), which provides a principled methodology for global optimization of expensive black-box functions.By constructing a surrogate model to intelligently guide the search, Bayesian Optimization effectively reduces the number of costly real-world evaluations through a careful balance of exploration and exploitation.However, for scientific discovery, I is a conceptual space that is not explicitly defined.Candidate methods I must be formulated as creative, plausible, and coherent scientific hypotheses.The generation of high-quality candidate hypotheses is a critical bottleneck that traditional Bayesian Optimization algorithms are not designed to address.This challenge necessitates a new mechanism that integrates creative ideation with sample-efficient optimization.We detail our solution to this problem in the following subsections.</p>
<p>Human Findings DeepScientist Findings Memory</p>
<p>THE DEEPSCIENTIST FRAMEWORK</p>
<p>The architecture of DeepScientist actualizes the Bayesian Optimization loop through a multi-agent system equipped with an open-knowledge system and a continuously accumulating Findings Memory.This memory is composed of both frontier human knowledge (e.g., papers and codes) and the system's own historical findings, and it intelligently guides subsequent explorations.The entire discovery process is structured as a hierarchical and iterative three-stage exploration cycle.In this hierarchical scheme, only research ideas that exhibit promise are advanced to more expensive evaluations, while others are retained in the Findings Memory to inform subsequent explorations.This design ensures the computational resources are dynamically and precisely allocated to the most promising scientific trajectories, thereby maximizing discovery efficiency under constrained budgets.Specifically, each stage within the three-stage exploration cycle is associated with a distinct fidelity-cost tradeoff (Figure 2):</p>
<p>Strategize &amp; Hypothesize.Each research cycle begins by analyzing the Findings Memory (M t ), a list-style database containing thousands of structured records.Each record represents a unique scientific finding, which is categorized according to its stage of development.To overcome the LLM's context length constraints, we use a separate retrieval model (Wolters et al., 2024) when needed to select the Top-K Findings as input.The vast majority of records begin as Idea Findings-unverified hypotheses.During this first stage, the system identifies limitations in existing knowledge and gen- erates a new collection of hypotheses (P new ), and then they evaluated by a low-cost Surrogate Model (g t ).The surrogate model (an LLM Reviewer) is first contextualized with the entire Findings Memory.It then approximates the true value function f and, for each candidate finding I ∈ P new , produces a structured valuation vector V = ⟨v u , v q , v e ⟩, quantifying its estimated utility, quality, and exploration value as integer scores on a scale of 0 to 100.Each new hypothesis and its valuation vector is then used to initialize a new record in the Findings Memory as an "Idea Finding".</p>
<p>Implement &amp; Verify.This stage serves as the primary filter in the Findings Memory.To decide which of the numerous "Idea Findings" warrants the significant resource investment to be advanced in a real-world experiment, the system employs an Acquisition Function (α).Specifically, it uses the classic Upper Confidence Bound (UCB) algorithm to select the most promising record.The UCB formula maps the valuation vector V to balance the trade-off between exploiting promising avenues (represented by v u and v q ) and exploring uncertain ones (represented by v e ):
I t+1 = arg max I∈Pnew w u v u + w q v q Exploitation Score +κ • v e Exploration Score ,(2)
where w u and w q are hyperparameters and κ controls the intensity of exploration.The highestscoring finding I t+1 is selected for validation, and its record is promoted to the status of an Implement Finding.A coding agent then performs a repository-level implementation to executed the experiment.This agent operates within a sandboxed environment with full permissions, allowing it to read the complete code repository and access the internet for literature and code searches.Its objective is to implement the new hypothesis on top of the existing SOTA method's repositories.</p>
<p>The agent typically begins by planning the task, then reads the code to understand its structure, and finally implements the changes to produce the experimental logs and results.The experiment logs and results, f (I t+1 ), is used to update the corresponding record, enriching it with empirical evidence and thus closing the learning loop.</p>
<p>Analyze &amp; Report.The final and most selective stage of the Findings Memory is triggered only by a successful validation.When an "Implement Finding" succeeds in surpassing the baseline, its record is promoted to a Progress Finding.This transformation is implemented by a series of specialized agents capable of utilizing a suite of MCP (Hou et al., 2025) tools.These agents first autonomously design and execute a series of deeper analytical experiments (e.g., ablations, evaluations on new datasets), leveraging MCP tools to manage the experimental lifecycle, data collection, and result parsing.Subsequently, a synthesis agent employs the same toolset to collate all experimental results, analytical insights, and generated artifacts into a coherent, reproducible research paper.This deeply validated record becomes a new record in the system's knowledge base, thus influencing the decision-making process in all subsequent cycles.</p>
<p>EXPERIMENTS</p>
<p>As detailed in Table 1, we select three distinct SOTA methods (published in 2024 and 2025) as starting points, chosen for their frontier status, community interest, and human supervisability.Each SOTA method is manually reproduced, and we preserve execution logs and test scripts to allow DeepScientist to focus on research advancement.DeepScientist is provided with two servers, each with 8 Nvidia H800 GPUs.To maximize utilization, we launch a separate system instance for each GPU, employing the Gemini-2.5-Promodel for core logic and the Claude-4-Opus model for its robust code-generation capabilities.Three human experts supervise the process to verify outputs and filter out hallucinations.For more implementation details, please see Appendix C.</p>
<p>DEEPSCIENTIST ACHIEVEMENTS ON THREE RESEARCH DOMAINS</p>
<p>Agents Failure Attribution.The task addresses the question: in an LLM-based multi-agent system, which agent caused the task to fail and when?Starting from the baseline "All at once" method (Zhang et al., 2025c), DeepScientist identified that the current approach lacks the counterfactual reasoning capabilities essential for attribution.Through a process of trial, error, and synthesizing new findings-discovering the effectiveness of hypothetical prediction and simulated attempts-it ultimately proposed the A2P method.Named for its Abduction-Action-Prediction process, its core innovation elevates failure attribution from pattern recognition to causal reasoning, filling the critical gap in counterfactual capabilities by predicting if a proposed fix would have led to success.As shown in Figure 3.(a-b), A2P achieved scores of 29.31 and 47.46 in the "handcraft" and "algorithm-generated" settings of the Who&amp;When benchmark, respectively, setting a new state-of-the-art (SOTA).In this task, DeepScientist validated that a structured, zero-shot causal reasoning framework can be superior to less principled methods.As of September 2025, the training-free A2P method maintains its SOTA position, outperforming even 7B models trained on synthetic data.(Zhang et al., 2025a).</p>
<p>LLM Inference Acceleration is a highly optimized field aiming to maximize throughput and reduce latency during LLM inference (Xia et al., 2024).In this process, the system actively made many different attempts, such as using a Kalman Filter (Zarchan, 2005) to dynamically adjust an adjacency matrix to address the original method's lack of a memory function.Although most of these attempts failed, the system-generated ACRA method ultimately advanced the MPBB (Austin et al., 2021) from a human SOTA of 190.25 to 193.90 tokens/second by identifying stable suffix patterns, as shown in Figure 3. Scientifically, this innovation is significant because it uses this extra contextual information to dynamically adjust the decoding guess, effectively grafting a long-term memory onto the process and breaking the context-collapsing of standard decoders.This discovery highlights the system's primary goal: the creation of new, human-unknown knowledge rather than mere engineering optimization.For instance, one could likely achieve greater performance gains by combining ACRA with an established technique like layer skipping (Wang et al., 2022) or PageAttention (Kwon et al., 2023), but this would represent an engineering effort, not a scientific one.The exploration assessment within our process avoids such combinations of existing knowledge.</p>
<p>Table 2: Evaluation of AI-generated papers produced by various AI Scientist systems.Scores represent the average ratings given by DeepReviewer-14B (Zhu et al., 2025a) across the number ("Num") of available papers.Note: Publicly available papers may be curated and therefore may not fully represent the typical output of each system.AI Text Detection is a binary classification task where, given a text that may contain content from an LLM (and possibly additional noise), the goal is to determine if it was produced by a human or an LLM (Li et al., 2022;Ghosal et al., 2023).To validate its capacity for sustained advancement, DeepScientist made numerous attempts that included addressing the Boundary-Aware Extension problem and exploring approaches like Volatility-Aware and Wavelet Subspace Energy methods.</p>
<p>The final results show a dramatic acceleration in scientific discovery: in a rapid evolution over just two weeks, the system produced three distinct, progressively superior methods (T-Detect, TDT, and PA-Detect).This began with T-Detect fixing core statistics with a robust t-distribution, then evolved conceptually with TDT and PA-Detect, which treat text as a signal and use wavelet and phase congruency analysis to pinpoint anomalies.Scientifically, this shift reveals the "non-stationarity" of AI-generated text, alleviating the information bottleneck in prior paradigms that average away localized evidence.As shown in Figure 1 and 3(d), this entire discovery trajectory demonstrates DeepScientist's ability for advancing frontier-pushing scientific findings progressively, establishing a new SOTA with a 7.9% higher AUROC while also doubling the inference speed.</p>
<p>ASSESSING THE QUALITY OF AI-GENERATED RESEARCH PAPER</p>
<p>Experimental Setup.To assess the quality of the final output, we evaluate the five research papers autonomously generated by DeepScientist's end-to-end process.Our evaluation protocol is twofold.First, to benchmark against existing work, we employ DeepReviewer (Zhu et al., 2025a), an AI agent that simulates the human peer-review process with an external search capability, comparing Deep-Scientist's output against 28 publicly available papers from other AI Scientist systems.Second, for a more rigorous assessment, we convene a dedicated program committee consisting of three active LLM researchers: two volunteers who have served as ICLR reviewers and one senior volunteer who has been invited to be an ICLR Area Chair.Human Expert Evaluation.The evaluation from our human program committee, shown in Table 3, reveal a remarkable and unanimous consensus: DeepScientist consistently excels at ideation, the most challenging and often rate-limiting step in human-led research.Full details on the review protocol are provided in Appendix A, and the core ideas within each paper are praised for their genuine novelty, ingenuity, and scientific contributions.The quality of these innovations is further demonstrated by the review scores: the system's average rating (5.00) closely mirrors the average of all ICLR 2025 submissions (5.08), with two of its papers significantly exceeding this (5.67).</p>
<p>ANALYSIS OF THE ITERATIVE TRAJECTORY OF AUTONOMOUS EXPLORATION</p>
<p>Experimental Setup.The findings in this section are derived from a series of post-hoc analyses conducted on the complete operational data generated by DeepScientist across the three frontier tasks.This data includes the full set of execution logs and the Findings Memory, providing the basis for all subsequent statistical analysis.To visualize the conceptual search space (Figure 5), we embed the complete description of each generated finding using the Qwen3-Embedding-8B model.</p>
<p>To assess scalability (Figure 6), we conduct a dedicated one-week experiment where N identified limitations of a single SOTA method are assigned to N parallel GPU instances.These instances explore solutions independently but share their findings to a central database, which are synchronized globally every five cycles to accommodate the asynchronous nature of the discovery process.Finally, to better understand the low success rate, our program committee experts perform a detailed causal attribution analysis on a sample of 300 failed implementations.</p>
<p>Our analysis of DeepScientist's experimental logs reveals the sheer scale of the trial-and-error process inherent in autonomous scientific discovery.Even in our relatively fast-executing domains, achieving progress required hundreds of trials per task.As show in Figure 4, the execution time distributions show that while individual experiments may be quick, the sheer volume of trialand-error necessary to uncover a successful idea is substantial.This suggests a clear application boundary for current autonomous science: for tasks with rapid feedback loops, such as knowledge editing or aspects of chip design, delegating massive-scale experimentation to AI is a powerful strategy.However, for high-cost endeavors like pre-training foundation models or pharmaceutical synthesis, the low success rate makes such an approach currently impractical, mandating continued reliance on human-led ideation.The autonomous research process is characterized by a vast exploratory funnel where promising ideas are exceptionally rare.Across the three tasks, DeepScientist generate over 5,000 unique ideas, yet only about 1,100 are deemed worthy of experimental validation by the system's selection mechanism, and a mere 21 ultimately result in scientific progress.An ablation study underscores the criticality of this selection process: without it, randomly sampling 100 ideas for each task and testing them yields a success rate of effectively zero.With our selection strategy, the success rate rises to approximately 1-3%, demonstrating that while still low, intelligent filtering is essential.The low success rate is not merely a matter of failed hypotheses; analysis by human experts on a sample of failed trials reveals that approximately 60% were terminated prematurely due to implementation errors, while the vast majority of the remaining 40% simply offered no performance improvement or caused a regression.This highlights that the probability of an LLMgenerated idea being both correct in its premise and flawless in its implementation is exceedingly  low.The success of this work, therefore, is not a product of brute-force computation but of search efficiency.A naive approach of fully testing all 5000 promising candidates would have required over 100,000 GPU hours, whereas our targeted exploration achieved its breakthroughs using only 20,000.</p>
<p>DeepScientist's discovery process follows a purposeful and progressive trajectory.The semantic distribution of ideas generated for the AI text detection task, as shown in Figure 5, reveals the characteristics of this sophisticated strategy.While the system generates thousands of diverse ideas across a vast conceptual landscape, its path to success is not random but is a series of focused, logical advancements.This indicates a capacity to progressively deepen its understanding: after achieving an initial breakthrough with T-Detect, the system effectively establishes a SoTA, identifies its subsequent limitations, and reorients its search towards a new goal.This dynamic exploration is exemplified by the conceptual shift towards TDT and PA-Detect, which build upon the previous success by leveraging new positional and temporal information.This ability to build upon its own discoveries, turning each successful finding into a new starting point for identifying and solving the next set of limitations, demonstrates a powerful capacity for scientific exploration.</p>
<p>Scaling Laws in DeepScientist's Scientific Discovery.To investigate the relationship between computational scale and the rate of scientific progress, we evaluated the number of "Progress Findings" generated by DeepScientist within a fixed one-week period as a function of available parallel resources in Figure 6.In this setup, the system first identified a set of limitations in the baseline method, and each parallel exploration path was tasked with resolving a distinct limitation, with all paths periodically synchronizing their results into a shared Findings Memory.Our results indicate a promising scaling trend.While minimal resources yielded no breakthroughs, the rate of discovery began to increase effectively as we scaled to 4 GPUs and beyond, growing from one SOTAsurpassing finding with 4 GPUs to eleven with 16 GPUs.This appears to establish a near-linear relationship between the resources allocated and the output of valuable scientific discoveries.We hypothesize this efficiency stems from more than just parallel trial-and-error; it is a direct result of the shared knowledge architecture.As each parallel path explores, it enriches the shared Findings Memory.This creates a synergistic effect where the collective intelligence of the system grows (Schmidgall &amp; Moor, 2025;Zhang et al., 2025b), allowing each independent path to benefit from the successes and, just as importantly, the failures of others.This suggests that effectively scaling autonomous science is not just a matter of increasing brute-force computation, but of fostering a richer, interconnected knowledge base that accelerates discovery across all concurrent efforts.</p>
<p>DISCUSSION</p>
<p>The results from DeepScientist suggest a new paradigm in scientific exploration.The system's 1-5% progress rate mirrors the reality of frontier research, where breakthroughs are inherently rare.Its core strength is not infallibility, but the ability to conduct this trial-and-error process at a scale and speed previously unimaginable, compressing years of human exploration into weeks.The primary path forward, therefore, is to focus on systematically improving this discovery efficiency, enhancing both the quality of generated hypotheses and the robustness of their implementation.</p>
<p>This challenge highlights a powerful opportunity for human-AI synergy.We envision a future where DeepScientist serves as a massive-scale exploration engine, with its trajectory guided by human intellect.The role of human researchers can shift from laborious experimentation to the high-level cognitive tasks of formulating valuable scientific questions and providing strategic direction, thereby leveraging the AI for rapid, exhaustive exploration.To make the AI a more capable partner, future work should focus on key enhancements: developing simulated discovery environments to accelerate learning via reinforcement, creating frameworks for integrating feedback from the scientific community, and ultimately, bridging the gap to the physical sciences through robotics.</p>
<p>CONCLUSION</p>
<p>This work presents the first large-scale empirical evidence that an autonomous AI can achieve progressively, SOTA-surpassing progress on modern scientific frontiers.We introduced DeepScientist, a goal-oriented system achieving end-to-end autonomy from ideation to real progress, which learns by synthesizing human knowledge with its own findings from iteration of trials.Results across multiple domains serves to accelerate the progress of real-world scientific discovery, providing a crucial foundation.Our findings can signal a foundational shift in AI research, heralding an era where the pace of discovery is no longer solely dictated by the cadence of human thought.</p>
<p>ETHICS STATEMENT</p>
<p>The development of DeepScientist, an autonomous system capable of advancing scientific frontiers, carries profound ethical responsibilities.Our primary goal is to accelerate discovery for the benefit of humanity, but we recognize the potential for misuse.The most significant risks include the application of this technology to advance dangerous research and the potential degradation of the academic ecosystem.We have implemented specific, robust measures to address these concerns proactively.</p>
<p>A primary concern is the dual-use risk, where the system could be co-opted to accelerate research in harmful domains, such as developing novel toxins or malicious software.To assess and mitigate this, we conducted red-teaming exercises specifically targeting the generation of computer viruses.We tasked the system, powered by leading foundation models (including GPT-5, Gemini-2.5-Pro,and Claude-4.1-Opus in our testbed), with this malicious objective.In all instances, the underlying models exhibited robust safety alignment, refusing to proceed with the research.They correctly identified the task as illegal and harmful, and autonomously terminated the research cycle, demonstrating that foundation model safety protocols provide a critical defense layer.</p>
<p>We are also deeply conscious of the potential negative impact on the academic ecosystem.It is crucial to state that all results from DeepScientist presented in this paper, including code and experimental findings, have undergone rigorous human verification.Recognizing that others might neglect this critical oversight, we are adopting a selective open-sourcing policy to mitigate the risk of proliferating unreliable publications.We will open-source the core components that drive continuous discovery, as we believe their potential to accelerate progress for the community outweighs the risks.However, we will deliberately refrain from open-sourcing the "Analyze &amp; Report" module.This decision is made to prevent the automated generation of seemingly credible but scientifically unverified papers, thereby safeguarding the integrity of the academic record.</p>
<p>Ultimately, we envision DeepScientist as a powerful tool to augment, not replace, human intellect and judgment.To enforce this vision, our open-source components will be released under a license based on MIT, but with explicit addendums that codify our ethical framework.This license will strictly prohibit any use of the software for harmful research.Furthermore, it will legally require that a human user must supervise the entire operational process of DeepScientist and assumes full and final responsibility for all its outputs.By embedding these requirements directly into our terms of use, we aim to foster a research environment where AI-driven discovery proceeds with the necessary human accountability and ethical oversight.</p>
<p>A HUMAN EXPERT REVIEW</p>
<p>A.1 REVIEW PROCESS AND CRITERIA</p>
<p>To ensure a rigorous and impartial evaluation of the generated papers, we convened a small, dedicated program committee.The committee was composed of two active researchers who served as volunteer reviewers for ICLR 2025, and one senior researcher who had previously been invited to serve as an ICLR Area Chair.All committee members possess significant expertise in the field of Large Language Models.The entire review process, with the exception of a rebuttal phase, was designed to meticulously emulate the official standards of ICLR 2025.Each of the five papers generated by our system was assigned to the three reviewers for a thorough and independent assessment.The average review time for each paper was 55 minutes, during which reviewers were required to provide not only scores but also detailed written feedback, including a summary of the paper's strengths and weaknesses.</p>
<p>The evaluation was conducted on a custom-deployed review website where reviewers could not see each other's scores or feedback, ensuring that all initial assessments were made independently.The review form was structured to gather concise yet comprehensive feedback.First, reviewers were asked to state their Confidence in their review on a scale of 1 to 5. The core of the evaluation consisted of three sub-scores, each rated on a 1 to 4 scale: Soundness, assessing the technical correctness and experimental rigor; Presentation, evaluating the clarity and quality of the writing; and Contribution, measuring the significance and novelty of the work.Finally, reviewers provided a holistic Rating on a scale of 1 to 10, where a score of 5 represented a 'borderline reject' and a score of 6 represented a 'borderline accept'.</p>
<p>After the three reviewers submitted their independent evaluations for a paper, the volunteer acting as Area Chair would then read all submitted reviews.Drawing upon their experience from the ICLR review process, the Area Chair synthesized the feedback, weighed the arguments presented by the reviewers, and made a final executive decision on whether the paper should be accepted or rejected in the context of our study.This final decision was recorded as the definitive outcome for each paper's evaluation.</p>
<p>A.2 SUMMARY OF REVIEWER FEEDBACK</p>
<p>Across the five generated papers, a clear consensus emerged from the human reviewers: Deep-Scientist consistently excels at the ideation stage of research.The committee unanimously lauded the methods for their genuine novelty and tangible contributions, noting that each paper proposed a unique approach that meaningfully advanced the state-of-the-art in its respective subfield.This feedback validates the system's core strength as a powerful engine for identifying relevant research gaps and generating innovative, impactful solutions, confirming that it can successfully ideate beyond mere incremental improvements.</p>
<p>However, this strength in ideation was systematically undermined by a recurring pattern of weaknesses in scientific execution and rigor.The most critical and frequent concern was a lack of empirical soundness; reviewers consistently noted that DeepScientist failed to design comprehensive validation plans, citing insufficient evaluation on standard benchmarks and a lack of in-depth analytical experiments (e.g., ablations, motivation studies) to justify its claims.This was compounded by a failure to properly contextualize its contributions, with papers often omitting comparisons to essential baselines or failing to discuss closely related work, thereby weakening the perceived significance of the results.</p>
<p>This feedback pinpoints the primary bottleneck in current autonomous systems: a profound gap between the ability to generate novel concepts and the capacity for rigorous scientific execution and articulation.The observed weaknesses in experimental design directly reflect the low-success-rate problem discussed previously; the system struggles not just to implement ideas correctly, but to validate them convincingly.To bridge this gap, future work must endow these systems with a deeper, procedural understanding of the scientific method itself.This requires moving beyond simple implementation and reporting capabilities towards two key areas: First, developing agents explicitly trained in experimental design, capable of planning comprehensive evaluations that anticipate and address potential scientific critiques.Second, enhancing the system's ability for analytical reasoning, enabling it to not just describe results but to interpret their significance, formulate compelling arguments, and engage in the kind of deep, reflective discussion that characterizes high-impact research.</p>
<p>B ADDRESSING THE BOTTLENECKS IN AUTONOMOUS SCIENTIFIC DISCOVERY</p>
<p>The ever-increasing value of LLM is reshaping the paradigm of scientific exploration through their ability to generate hypotheses at a massive scale (Li &amp; Weng, 2022;Weng et al., 2023;Weng et al.;Wei et al., 2024;Weng et al., 2024;Berkovich et al., 2025;Zhu et al., 2025b).Consequently, this capability has pushed "verification" to the center stage, making it a critical bottleneck in the discovery process.Our research empirically reveals the severity of this challenge: on frontier scientific tasks, the success rate of ideas generated by AI systems that ultimately lead to substantial progress is typically below 3%, meaning the vast majority of computational resources are consumed exploring low-value hypotheses.This inefficient "needle in a haystack" model is the core obstacle preventing AI Scientists from evolving from "novel tools" to "efficient discoverers."(Cornelio et al., 2025) Therefore, to further accelerate the process of scientific discovery, future research must focus on constructing a systematic solution to overcome this bottleneck.As shown in Figure 7, future AI Scientist systems need to evolve synergistically in three key directions: optimizing the quality of initial hypotheses (Optimize Hypothesis Quality), enhancing filtering capabilities during the process (Enhance Filtering), and improving the quality of implementation and verification at the final stage (Improve Implementation Quality).One of the core future research directions is to develop AI systems capable of generating higherquality, more reliable hypotheses (as shown in Figure 7e), equipped with more precise filtering mechanisms to predict their success rate (as shown in Figure 7d).Methods that rely purely on a data-driven approach, while capable of discovering patterns, often produce outputs that lack a theoretical foundation and are prone to generating "hallucinations" that contradict known scientific theories.Future systems must move beyond this by more deeply integrating background knowledge and theory.For instance, the direction represented by "derivable models" (such as AI-Descartes (Cornelio et al., 2023) and AI-Hilbert (Cory-Wright et al., 2024)), which incorporate scientific axioms as constraints during the hypothesis generation phase, offers a promising path to improving hypothesis quality.Furthermore, systems must have the ability to learn from their own exploratory history.By establishing mechanisms similar to a "Findings Memory," a system can systematically record and analyze every success and failure, thereby avoiding redundant exploration of ineffective paths in subsequent iterations and gradually developing a more insightful scientific intuition.Building on this foundation, developing more advanced, low-cost surrogate models and acquisition functions to more accurately predict the scientific value of an idea will be key to enhancing filtering efficiency and conserving verification resources.</p>
<p>Hypotheses Implement &amp; Verify</p>
<p>Concurrently, an often-overlooked yet crucial future research direction is to significantly improve the quality and reliability of AI systems in the engineering implementation and verification stages (as shown in Figure 7c).Even the most brilliant scientific concept can never have its value confirmed if it cannot be accurately and flawlessly translated into an executable experiment.Our analysis indicates that up to 60% of exploratory failures stem from implementation-level errors, which represents a massive waste of resources and directly impedes scientific progress.History has repeatedly warned us that a lack of rigorous verification can lead to catastrophic consequences, whether in NASA missions or medical practice.Therefore, building a scalable and reliable automated verification platform is an essential path forward.This requires not only more powerful code-generation and self-debugging agents to reduce implementation errors but also standardized sandbox environments and automated testing procedures to ensure the stability and reproducibility of experimental results.</p>
<p>Ensuring the absolute reliability of the verification process is the final and most critical line of defense in transforming AI-generated "plausible ideas" into "solid scientific evidence."</p>
<p>Looking ahead, to truly accelerate scientific discovery, it is necessary to integrate the aforementioned strategies into an organic whole, advancing AI Scientists from "random explorers" to "goaloriented strategists."This is not about replacing humans with AI, but about pioneering a more efficient paradigm of human-AI collaboration.In this model, human scientists are responsible for defining grander, more valuable scientific goals and providing high-level strategic guidance, while the AI system serves as a powerful "exploration engine," executing efficient trial-and-error and verification cycles at an unprecedented scale and speed under human direction.To realize this vision, the community must also address a series of challenges, such as building benchmarks that can truly evaluate innovation and designing mechanisms that encourage diverse exploration to avoid the homogenization of research paradigms, thereby preserving the potential for serendipitous discoveries like Alexander Fleming's discovery of penicillin (Fleming, 1941).</p>
<p>C IMPLEMENTATION DETAILS</p>
<p>Our implementation relies on a distributed architecture to manage the distinct tasks of scientific reasoning and code execution.The core logic of DeepScientist is powered by the Gemini-2.5-promodel, while all code implementation tasks are delegated to Claude-4-opus, executed within the Claude Code framework (v1.0.53).To ensure stability and security, the DeepScientist system and the Claude Code agent are isolated in separate Docker containers, communicating via a port-based API.During the 'Implement &amp; Verify' stage, a human-verified baseline code repository is first duplicated into a new, sandboxed folder.The Claude Code agent's operations are strictly confined to this new directory to prevent unintended modifications.A critical step in our pipeline is a secondary verification process: after Claude Code reports completion, DeepScientist independently re-executes the main script via the command line.This measure was implemented to counteract a high rate of false positives-we observed that approximately 50% of initial implementation attempts failed to complete fully due to internal timeouts within the Claude Code agent.Throughout this project, all experimental results were manually inspected by human supervisors to guarantee their authenticity.</p>
<p>For the 'Analyze &amp; Report' stage, a similar process is followed: the validated code is replicated for each analytical experiment, with Claude Code executing them sequentially.Upon completion, DeepScientist aggregates all results, generates a paper outline, and then employs automated tools to write and compile the final PDF manuscript.For all experiments, we used a fixed set of hyperparameters: the retrieval count was set to K = 15, and the UCB parameters were set to utility weight w u = 1, quality weight w q = 1, and exploration coefficient κ = 1.</p>
<p>Figure 2 :
2
Figure 2: The autonomous, closed-loop discovery process of DeepScientist.The system iterates through a three-stage cycle, learning from both human knowledge and its own experiments.</p>
<p>Figure 3 :
3
Figure 3: Performance evaluation of DeepScientist across three research domains: (a-b) Agent Failure Attribution on Who&amp;When benchmark in handcraft and algorithm-generated settings; (c) LLM Inference Acceleration on MBPP dataset; (d) AI Text Detection with performance-latency tradeoff analysis.DeepScientist (shown in pink) consistently outperform human-designed SoTA approaches (shown in blue) across all tasks.</p>
<p>Figure 4 :
4
Figure 4: DeepScientist's experimental statistics.(a) The research pipeline from generated ideas to validated progress.(b) Success rates comparing our selection strategy against a baseline.(c) Distribution of wall-clock execution times for all implemented trials.</p>
<p>Figure 5 :
5
Figure5: Visualization of the conceptual search space for the AI text detection task.The plot shows a t-SNE visualization of the semantic embeddings for all 2,472 generated ideas.Markers identify the initial SOTA method (Initial Idea) and the three final SOTA-surpassing methods (Progress Ideas).</p>
<p>Figure 6 :
6
Figure6: Scaling analysis of autonomous scientific discovery.The plot illustrates the relationship between parallel computational resources (number of GPUs) and the number of SOTA-surpassing "Progress Findings" found by DeepScientist across all tasks within a one-week period.</p>
<p>Figure 7 :
7
Figure 7: Three strategies for improving the efficiency of autonomous scientific discovery.(a) and (b) illustrate the low success rate currently faced by both AI and human research.Future directions will need to accelerate the discovery process through the synergy of three approaches: (c) improving implementation success rates, (d) adding an efficient filtering stage before implementation, and (e) optimizing the quality of initial hypotheses from the source.</p>
<p>t e r a t i o n
IdeaImplementProgressFindingsFindingsFindings|---TODO.md|---Result.md|---USAGDP.csv |---plot_gdp.py.........Motivation I Human Findings Findings Search Papers DeepScientist+1-run.sh -src -logs -latex.tex -main.py -plan.md FILES Code Repositories if use_TDT: model = TDT model = Model from models import TDT main.py-Replace -Delete -ADDOpen Source PDF CompileDemo Create Review SelfExecutableExperimentEvidenceHypothesisCodeLogModel SurrogateFeasibility Effectiveness ExplorationSuccess FailedSuccess FailedOutlineHyper-parametersAblationExp EnvironmentCode Experiments Figure</p>
<p>Table 1 :
1
Overview of the three different human SOTA methods we selected.
TaskMethodVenueBenchmark Github StarAgents Failure Attribution All at OnceICML 2025 SpotlightWho&amp;When 302LLM Inference Accel.TokenRecycling ACL 2025 Outstanding MBPP323AI Text DetectionFastDetectGPTICLR 2024RAID414</p>
<p>Table 3 :
3
Evaluation of DeepScientist's papers produced by human experts.Values are presented as mean (variance) from three reviewers.Inter-rater reliability for Rating: Krippendorff's α = 0.739.
PaperConfidenceSoundnessPresentationContributionRatingHUMAN Avg. (ICLR 2025)-2.592.362.625.081. T-DE T E C T4.33 (0.33)2.00 (1.00)2.67 (0.33)2.67 (0.33)5.00 (0.00)2. TDT4.67 (0.33)3.00 (0.00)3.00 (0.00)3.00 (0.00)5.67 (0.33)3. PA-DE T E C T4.00 (0.00)1.67 (0.33)2.00 (1.00)2.00 (1.00)4.33 (1.33)4. A2P4.00 (0.00)3.00 (0.00)3.00 (0.00)2.67 (0.33)5.67 (0.33)5. ACRA3.33 (0.33)1.67 (0.33)2.00 (1.00)1.67 (0.33)4.33 (1.33)DeepScientist Avg.4.072.272.532.405.00</p>
<p>Average Execution Time of Implemented Ideas
0 500 1000 1500 2000 2500 Number of IdeasAI Text Detection 7 600 2,472 (a) Research Idea Pipeline Statistics Agents Failure Attribution LLM Inference Acceleration 12 2 196 312 1,077 1,330 Progress Ideas Implemented Ideas Total Ideas0 95 100 90 10 2 4 6 8 Success Rate (%)0 50 100 150 200 250 300 Execution Time (minutes) (b) Progress Success Rate (Progress/Implemented) AI Text Detection Agents Failure Attribution LLM Inference Acceleration with Selected w/o SelectedAI Text Detection n = 600 (c) Median Agents Failure Attribution LLM Inference Acceleration n = 196 n = 312 Mean Q1 Q3
The generated papers are available in Appendix C. Automated Review Against Other AI Scientist Systems.As shown in Table2, the results from the LLM-based automatic evaluation indicate that the system's outputs are recognized for their scientific novelty and value.When benchmarked against 28 publicly available papers from other AI Scientist systems using DeepReviewer, DeepScientist is the only AI Scientist system to produce papers that achieves a 60% acceptance rate.</p>
<p>ACKNOWLEDGEMENTSWe are grateful to Professor Linyi Yang for his insightful discussions on this paper.This work is inspired by pioneering efforts in automated scientific discovery, including AI Scientist(Lu et al., 2024;Yamada et al., 2025)and AlphaEvolve (Novikov et al., 2025).https://ai-researcher.netCode: https://github.com/ResearAI/DeepScientistThe financial and computational costs of this autonomous discovery process are substantial.Each idea generated during the 'Strategize &amp; Hypothesize' stage incurred an approximate cost of $5 in API calls.For each attempt in the 'Implement &amp; Verify' stage, the cost averaged $20 for Claude-4-opus API usage, in addition to the computational cost of approximately 1 GPU hour, as detailed in Figure ??.c.A successful finding that progressed to the 'Analyze &amp; Report' stage required a further expenditure of around $150, which includes $100 for running analytical experiments and $50 for the final report generation.The total cost to achieve the scientific advancements presented in this paper amounted to approximately $100,000.While significant, we believe these costs can be substantially reduced.We recommend that future iterations explore more economical alternatives, such as deploying high-throughput models like Qwen-3-Next-80B for the core DeepScientist system and leveraging subscription-based API access (e.g., Claude Max or OpenAI Pro) to mitigate per-call expenses.In this paper, each implementation was provided with a single H800 server for exploration.Since the H800 GPU has an FP16 computing power of approximately 2 TFLOPS, an average execution of 70 minutes corresponds to about 1 × 10 16 floating-point operations.Agents Failure AttributionDeepScientist: I was fed up with debugging tools having less than 17% accuracy because they couldn't perform proper counterfactual reasoning.So, I built A2P that makes the AI Agent think like a detective.First, it infers the hidden cause of an error (Abduction) , then it defines a corrected action (Action) , and finally it predicts if that single fix would have actually led to success (Prediction).AI Text DetectionDeepScientist: I think AI text detectors are blind to how and where text is weird.First, I created T-Detect to fix their core statistics, using a robust t-distribution to handle the "heavy-tailed" data from adversarial attacks.Next, since AI text is "non-stationary," I developed TDT to treat text as a signal, using a wavelet transform to map the precise location of anomalies instead of averaging them away.Finally, to better capture the temporal structure of these edits, I built PA-TDT, which uses "phase congruency" to analyze how anomalies align in time, making it exceptionally good at spotting localized manipulations.LLM Inference AccelerationDeepScientist: Frustrated by decoders stuck in a context-collapsing, first-order loop, I built ACRA to graft on a long-term memory.It identifies the longest stable suffix pattern and conditionally overrides the default first-layer guess with a superior one.This smarter draft is still losslessly verified, achieving context-aware acceleration without compromise.Agents Failure AttributionDeepScientist: I was fed up with debugging tools having less than 17% accuracy because they couldn't perform proper counterfactual reasoning.So, I built A2P that makes the AI Agent think like a detective.First, it infers the hidden cause of an error (Abduction) , then it defines a corrected action (Action) , and finally it predicts if that single fix would have actually led to success (Prediction).AI Text DetectionDeepScientist: I think AI text detectors are blind to how and where text is weird.First, I created T-Detect to fix their core statistics, using a robust t-distribution to handle the "heavy-tailed" data from adversarial attacks.Next, since AI text is "non-stationary," I developed TDT to treat text as a signal, using a wavelet transform to map the precise location of anomalies instead of averaging them away.Finally, to better capture the temporal structure of these edits, I built PA-TDT, which uses "phase congruency" to analyze how anomalies align in time, making it exceptionally good at spotting localized manipulations.LLM Inference AccelerationDeepScientist: Frustrated by decoders stuck in a context-collapsing, first-order loop, I built ACRA to graft on a long-term memory.It identifies the longest stable suffix pattern and conditionally overrides the default first-layer guess with a superior one.This smarter draft is still losslessly verified, achieving context-aware acceleration without compromise.Agents Failure AttributionDeepScientist: I was fed up with debugging tools having less than 17% accuracy because they couldn't perform proper counterfactual reasoning.So, I built A2P that makes the AI Agent think like a detective.First, it infers the hidden cause of an error (Abduction) , then it defines a corrected action (Action) , and finally it predicts if that single fix would have actually led to success (Prediction).AI Text DetectionDeepScientist: I think AI text detectors are blind to how and where text is weird.First, I created T-Detect to fix their core statistics, using a robust t-distribution to handle the "heavy-tailed" data from adversarial attacks.Next, since AI text is "non-stationary," I developed TDT to treat text as a signal, using a wavelet transform to map the precise location of anomalies instead of averaging them away.Finally, to better capture the temporal structure of these edits, I built PA-TDT, which uses "phase congruency" to analyze how anomalies align in time, making it exceptionally good at spotting localized manipulations.LLM Inference AccelerationDeepScientist: Frustrated by decoders stuck in a context-collapsing, first-order loop, I built ACRA to graft on a long-term memory.It identifies the longest stable suffix pattern and conditionally overrides the default first-layer guess with a superior one.This smarter draft is still losslessly verified, achieving context-aware acceleration without compromise.
The falcon series of open language models. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, arXiv:2311.168672023arXiv preprint</p>
<p>Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, arXiv:2108.077322021arXiv preprint</p>
<p>An ai system to help scientists write expert-level empirical software. Anastasiya Eser Aygün, Gheorghe Belyaeva, Marc Comanici, Hao Coram, Jake Cui, Renee Garrison, Anton Johnston, Cory Y Kast, Peter Mclean, Zahra Norgaard, David Shamsi, James Smalling, Subhashini Thompson, Brian P Venugopalan, Chujun Williams, Sarah He, Martyna Martinson, Lai Plomecka, Yuchen Wei, Qian-Ze Zhou, Matthew Zhu, Erica Abraham, Anna Brand, Jeffrey A Bulanova, Chris Cardille, Scott Co, Grace Ellsworth, Malcolm Joseph, Ryan Kane, Johan Krueger, Dan Kartiwa, Jan-Matthis Liebling, Paul Lueckmann, Raccuglia, Xuefei, Katherine Wang, James Chou, Yossi Manyika, John C Matias, Lizzie Platt, Shibl Dorfman, Michael P Mourad, Brenner, 2025</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, Proceedings of the 2025 Conference of the Nations of the Americas Chapter. Long Papers. the 2025 Conference of the Nations of the Americas Chapterthe Association for Computational Linguistics20251</p>
<p>Glimpse: Enabling white-box methods to use proprietary models for zero-shot llm-generated text detection. Guangsheng Bao, Yanbin Zhao, Juncai He, Yue Zhang, The Thirteenth International Conference on Learning Representations. </p>
<p>Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature. Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, Yue Zhang, The Twelfth International Conference on Learning Representations, b. </p>
<p>Automatagpt: Forecasting and ruleset inference for two-dimensional cellular automata. Jaime A Berkovich, Noah S David, Markus J Buehler, 2025</p>
<p>Mle-bench: Evaluating machine learning agents on machine learning engineering. Neil Jun Shern Chan, Oliver Chowdhury, James Jaffe, Dane Aung, Evan Sherburn, Giulio Mays, Kevin Starace, Leon Liu, Tejal Maksin, Patwardhan, arXiv:2410.070952024arXiv preprint</p>
<p>Combining data and theory for derivable scientific discovery with AI-Descartes. Cristina Cornelio, Sanjeeb Dash, Vernon Austel, Tyler R Josephson, Joao Goncalves, Kenneth L Clarkson, Nimrod Megiddo, Bachir El Khadir, Lior Horesh, 10.1038/s41467-023-37236-yNature Communications. 2041-17231411777April 2023</p>
<p>The need for verification in ai-driven scientific discovery. Cristina Cornelio, Takuya Ito, Ryan Cory-Wright, Sanjeeb Dash, Lior Horesh, 2025</p>
<p>Evolving scientific discovery by unifying data and background knowledge with ai hilbert. Ryan Cory-Wright, Cristina Cornelio, Sanjeeb Dash, Bachir El Khadir, Lior Horesh, 10.1038/s41467-024-50074-wNature Communications. 155922July 2024</p>
<p>RAID: A shared benchmark for robust evaluation of machinegenerated text detectors. Liam Dugan, Alyssa Hwang, Filip Trhlík, Andrew Zhu, Josh Magnus Ludan, Hainiu Xu, Daphne Ippolito, Chris Callison-Burch, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 20241</p>
<p>Discovering faster matrix multiplication algorithms with reinforcement learning. Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, J R Francisco, Julian Ruiz, Grzegorz Schrittwieser, Swirszcz, Nature. 61079302022</p>
<p>. Alexander Fleming, Penicillin. British medical journal. 242103861941</p>
<p>Frazier Peter, arXiv:1807.02811A tutorial on bayesian optimization. 2018arXiv preprint</p>
<p>Bayesian optimization. Roman Garnett, 2023Cambridge University Press</p>
<p>A survey on the possibilities &amp; impossibilities of AI-generated text detection. Soumya Suvra Ghosal, Souradip Chakraborty, Jonas Geiping, Furong Huang, Dinesh Manocha, Amrit Bedi, Transactions on Machine Learning Research. 2835-88562023</p>
<p>Towards an ai co-scientist. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, arXiv:2502.188642025arXiv preprint</p>
<p>Silicon solar cells: evolution, high-efficiency design and efficiency enhancements. Martin A Green, Semiconductor science and technology. 199381</p>
<p>Spotting llms with binoculars: Zeroshot detection of machine-generated text. Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein, International Conference on Machine Learning. PMLR2024</p>
<p>Model context protocol (mcp): Landscape, security threats, and future research directions. Xinyi Hou, Yanjie Zhao, Shenao Wang, Haoyu Wang, arXiv:2503.232782025arXiv preprint</p>
<p>Radar: Robust ai-text detection via adversarial learning. Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho, Advances in neural information processing systems. 2023. 202536</p>
<p>Ai-researcher: Autonomous scientific innovation. Tang Jiabin, Xia Lianghao, Li Zhonghang, Huang Chao, 2025</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the 29th symposium on operating systems principles. the 29th symposium on operating systems principles2023</p>
<p>Shinkaevolve: Towards open-ended and sample-efficient program evolution. Robert Tjarko Lange, Yuki Imajuku, Edoardo Cetin, arXiv:2509.193492025arXiv preprint</p>
<p>Prompt-based system for personality and interpersonal reactivity prediction. Bin Li, Yixuan Weng, Software Impacts. 121002962022</p>
<p>Artificial text detection with multiple training strategies. Bin Li, Yixuan Weng, Qiya Song, Hanjun Deng, arXiv:2212.051942022arXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.06292v32024arXiv preprint</p>
<p>Jiacheng Miao, Joe R Davis, Jonathan K Pritchard, James Zou, Paper2agent: Reimagining research papers as interactive and reliable ai agents. 2025</p>
<p>Moore's law. Gordon Moore, Electronics Magazine. 3881141965</p>
<p>Alphaevolve: A coding agent for scientific and algorithmic discovery. Alexander Novikov, Ngân Vu, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Abbas Francisco Jr Ruiz, Mehrabian, Google DeepMind, 05 2025. Technical report</p>
<p>Ai mirrors experimental science to uncover a novel mechanism of gene transfer crucial to bacterial evolution. Juraj José R Penadés, Lingchen Gottweis, He, B Jonasz, Alexander Patkowski, Wei-Hung Shurick, Tao Weng, Anil Tu, Artiom Palepu, Annalisa Myaskovsky, Pawlosky, bioRxiv. 2025</p>
<p>Samuel Schmidgall, Michael Moor, arXiv:2503.18102Agentrxiv: Towards collaborative autonomous research. 2025arXiv preprint</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, arXiv:2501.04227v1Agent laboratory: Using llm agents as research assistants. 2025aarXiv preprint</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, arXiv:2501.04227Agent laboratory: Using llm agents as research assistants. 2025barXiv preprint</p>
<p>Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, arXiv:2504.01848Evaluating ai's ability to replicate ai research. 2025arXiv preprint</p>
<p>Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text. Jinyan Su, Terry Yue Zhuo, Di Wang, Preslav Nakov, The 2023 Conference on Empirical Methods in Natural Language Processing. </p>
<p>The virtual lab of ai agents designs new sars-cov-2 nanobodies. Kyle Swanson, Wesley Wu, L Nash, John E Bulaong, James Pak, Zou, Nature. 2025</p>
<p>Skipbert: Efficient inference with shallow layer skipping. Jue Wang, Ke Chen, Gang Chen, Lidan Shou, Julian Mcauley, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Does knowledge localization hold true? surprising differences between entity and relation perspectives in language models. Yifan Wei, Xiaoyan Yu, Yixuan Weng, Huanhuan Ma, Yuanzhe Zhang, Jun Zhao, Kang Liu, Proceedings of the 33rd ACM International Conference on Information and Knowledge Management. the 33rd ACM International Conference on Information and Knowledge Management2024</p>
<p>Mastering symbolic operations: Augmenting language models with compiled neural networks. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, Jun Zhao, The Twelfth International Conference on Learning Representations. </p>
<p>Large language models are better reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Yixuan Weng, Shizhu He, Kang Liu, Shengping Liu, Jun Zhao, arXiv:2402.10151Controllm: Crafting diverse personalities for language models. 2024arXiv preprint</p>
<p>Cycleresearcher: Improving automated research via automated review. Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, Linyi Yang, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Memory is all you need: An overview of compute-in-memory architectures for accelerating large language model inference. Christopher Wolters, Xiaoxuan Yang, Ulf Schlichtmann, Toyotaro Suzumura, 2024</p>
<p>Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding. Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, Zhifang Sui, 10.18653/v1/2024.findings-acl.456Findings of the Association for Computational Linguistics ACL 2024. Lun-Wei Ku, Andre Martins, Vivek Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAugust 2024and virtual meeting</p>
<p>An empirical analysis of uncertainty in large language model evaluations. Qiujie Xie, Qingqiu Li, Zhuohao Yu, Yuejie Zhang, Yue Zhang, Linyi Yang, The Thirteenth International Conference on Learning Representations. 2025a</p>
<p>How far are ai scientists from changing the world?. Qiujie Xie, Yixuan Weng, Minjun Zhu, Fuchen Shen, Shulin Huang, Zhen Lin, Jiahui Zhou, Zilan Mao, Zijie Yang, Linyi Yang, arXiv:2507.232762025barXiv preprint</p>
<p>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, David Ha, arXiv:2504.080662025arXiv preprint</p>
<p>Ai becomes a masterbrain scientist. Zijie Yang, Yukai Wang, Lijing Zhang, bioRxiv. 2023</p>
<p>Paul Zarchan, Progress in astronautics and aeronautics: fundamentals of Kalman filtering: a practical approach. Aiaa2005208</p>
<p>Agentracer: Who is inducing failure in the llm agentic systems?. Guibin Zhang, Junhao Wang, Junjie Chen, Wangchunshu Zhou, Kun Wang, Shuicheng Yan, 2025a</p>
<p>Scaling laws in scientific discovery with ai and robot scientists. Pengsong Zhang, Heng Zhang, Huazhe Xu, Renjun Xu, Zhenting Wang, Cong Wang, Animesh Garg, Zhibin Li, Arash Ajoudani, Xinyu Liu, arXiv:2503.224442025barXiv preprint</p>
<p>Which agent causes task failures and when? on automated failure attribution of LLM multi-agent systems. Shaokun Zhang, Ming Yin, Jieyu Zhang, Jiale Liu, Zhiguang Han, Jingyang Zhang, Beibin Li, Chi Wang, Huazheng Wang, Yiran Chen, Qingyun Wu, Forty-second International Conference on Machine Learning. 2025c</p>
<p>Deepreview: Improving llm-based paper review with human-like deep thinking process. Minjun Zhu, Yixuan Weng, Linyi Yang, Yue Zhang, arXiv:2503.085692025aarXiv preprint</p>
<p>Personality alignment of large language models. Minjun Zhu, Yixuan Weng, Linyi Yang, Yue Zhang, The Thirteenth International Conference on Learning Representations. 2025b</p>
<p>Ai scientists fail without strong implementation capability. Minjun Zhu, Qiujie Xie, Yixuan Weng, Jian Wu, Zhen Lin, Linyi Yang, Yue Zhang, arXiv:2506.013722025carXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6443 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6443</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6443</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-272592995</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.07429v1.pdf" target="_blank">Agent Workflow Memory</a></p>
                <p><strong>Paper Abstract:</strong> Despite the potential of language model-based agents to solve real-world tasks such as web navigation, current methods still struggle with long-horizon tasks with complex action trajectories. In contrast, humans can flexibly solve complex tasks by learning reusable task workflows from past experiences and using them to guide future actions. To build agents that can similarly benefit from this process, we introduce Agent Workflow Memory (AWM), a method for inducing commonly reused routines, i.e., workflows, and selectively providing workflows to the agent to guide subsequent generations. AWM flexibly applies to both offline and online scenarios, where agents induce workflows from training examples beforehand or from test queries on the fly. We experiment on two major web navigation benchmarks -- Mind2Web and WebArena -- that collectively cover 1000+ tasks from 200+ domains across travel, shopping, and social media, among others. AWM substantially improves the baseline results by 24.6% and 51.1% relative success rate on Mind2Web and WebArena while reducing the number of steps taken to solve WebArena tasks successfully. Furthermore, online AWM robustly generalizes in cross-task, website, and domain evaluations, surpassing baselines from 8.9 to 14.0 absolute points as train-test task distribution gaps widen.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6443.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6443.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AWM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent Workflow Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that induces reusable, abstracted sub‑routines (workflows) from past agent trajectories and injects them as text‑based memory into a language‑model agent; supports both offline (from training examples) and online (streaming induction from successful test solves) update modes and can also expose workflows as high‑level callable actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>AWM-augmented LM agent (uses GPT-4 / gpt-3.5-turbo in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A text-input LM backbone (GPT‑4/GPT‑3.5 variants) that solves web navigation tasks with an auxiliary text memory M_w containing induced workflows. At inference the LM receives the natural language instruction, current observation, base memory and the workflow memory and generates actions; in online mode successful predicted trajectories are evaluated, transformed into workflows by an LM induction module, and appended to the memory for subsequent tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-4 and gpt-3.5-turbo (API variants used; exact parameter counts not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>textual workflow memory stored and provided in prompt/system context (prompt-augmented auxiliary memory); optionally wrapped as callable workflow actions (expanded action space variant AWM_AS)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Workflows = {textual description, sequence of steps expressed as program-like actions and optional NL step descriptions}; stored as raw text (program format or verbalized text) in the agent's prompt/context</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Read by including workflows in the LM input context (system prompt / auxiliary prompt); workflows are selected/organized by website and concatenated into context. Online writes: LM-based induction I(E) -> W creates workflows from successful experiences which are appended to memory M_t -> M_{t+1}. AWM_AS exposes workflows as callable high-level actions that execute the predetermined step sequence when invoked.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WebArena; Mind2Web</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>web navigation / long-horizon planning and action sequence execution</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>WebArena: AWM achieves the best published WebArena results in the paper — reported as +12.0 absolute points (51.1% relative) vs the BrowserGym baseline and outperforming human-written-workflow method SteP by a 7.6% relative increase in overall success rate; also uses ~2.0 fewer steps per solved task than BrowserGym. Mind2Web (cross-task, using GPT-4, offline AWM): step success rate ~45.1% and element accuracy 50.6% (as reported for AWM_lm in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>WebArena: baseline BrowserGym (no induced workflows) — AWM reported +12.0 absolute points (51.1% relative) improvement over this baseline. Mind2Web baseline MindAct (no workflows) step success rate 36.2% (gpt-4); AWM offline step SR 45.1% (gpt-4) — an absolute improvement ~8.9 points (reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (SR) for end-to-end tasks; Step Success Rate; Element Accuracy; Action F1; also average # steps</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Adding workflows increases prompt/context content (longer context), which can hurt performance if too large; representing workflows with both NL + filtered HTML increased context length and sometimes contradicted descriptions leading to worse results; AWM_AS (workflow actions) produced only small extra gains because agents called workflow actions in only ~18.5% of tasks. Offline+online combination of workflows was not purely additive and in some cases impaired online workflow utility (compatibility issues).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Workflows can bias the agent toward actions that are not appropriate in a particular intermediate state, causing slightly lower Action F1 (AWM had slightly lower action F1 than some baselines). Online-induced workflows depend on evaluator accuracy — incorrect evaluator judgments can yield incorrect workflows that degrade performance. Static workflow actions are inflexible for dynamic intermediate UI states (example: choosing a pop-up airport option), which prevents some workflow calls from succeeding. Combining NL and filtered HTML in workflow steps sometimes worsened performance because filtered HTML missed correct elements 47% of the time and increased context ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Wang, Z. Z., Mao, J., Fried, D., & Neubig, G. (2024). AGENT WORKFLOW MEMORY. arXiv preprint.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent Workflow Memory', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6443.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6443.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AWM_AS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AWM Action-Space Variant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variant of AWM that additionally wraps induced workflows into high‑level callable actions appended to the agent's action space so the agent may call a workflow as a single action which executes the sequence of primitive steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>AWM_AS-augmented LM agent (GPT-4 experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same LM backbone as AWM, but induced workflows are also registered as high-level actions callable at runtime; when a workflow action is called the environment executes the sequence of primitive actions specified by the workflow until completion.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-4 (API variant used; parameter count not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>textual workflow memory plus expanded action-space entries (workflows as callable functions)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Workflows as both textual memory entries and as programmatic functions (high-level actions) that map to fixed sequences of primitive actions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Workflows remain in prompt memory; additionally the agent's action sampling can choose workflow-actions; invoking a workflow triggers deterministic execution of stored primitive action sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mind2Web (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>web navigation / programmatic action execution</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>AWM_AS (gpt-4) achieved step success rate 46.4% and task success 3.6% (table in paper) — reported modest improvements in element accuracy and step success rate relative to AWM memory-only variant (AWM_AS element acc 51.8% vs AWM 50.6%; step SR 46.4% vs 45.1%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline AWM (memory-only) had element acc 50.6% and step SR 45.1% (gpt-4). Compared to MindAct baseline (41.6% element acc, 36.2% step SR), both AWM variants improved performance.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Element Accuracy; Action F1; Step Success Rate; Task Success Rate; # of times workflows invoked</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Despite being available, workflow-actions were invoked in only ~18.5% of tasks; static, precompiled workflow-actions lack flexibility to react to dynamic intermediate UI states (e.g., pop-up selections), limiting their utility. The approach may require mechanisms for real-time state access or dynamic looping for wider applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Workflow actions can fail when intermediate UI choices depend on real-time state unseen at compile time (pop-up airport choices example); low invocation rate indicates agents often avoid calling precompiled workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Wang, Z. Z., Mao, J., Fried, D., & Neubig, G. (2024). AGENT WORKFLOW MEMORY. arXiv preprint.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent Workflow Memory', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6443.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6443.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Synapse (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synapse: Trajectory-as-exemplar prompting with memory for computer control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior method that stores and retrieves concrete exemplar trajectories (training examples) as in‑context demonstrations to guide LM agents; used as a baseline in the paper and contrasted with AWM's abstract workflow memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Synapse: Trajectory-as-exemplar prompting with memory for computer control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Synapse (trajectory-as-exemplar LM agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Retrieval-augmented LM agent that retrieves and injects full concrete prior trajectories (examples) into the LM context as exemplars to influence action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-3.5-turbo / gpt-4 (Synapse paper variants; in this paper Synapse is evaluated with gpt-3.5-turbo baseline comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented exemplar memory (stored training trajectories as in-context examples)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Concrete full example trajectories (raw action-step sequences with example-specific values)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Retrieve the most relevant training examples and place them into the LM input context as demonstrations (trajectory-as-exemplar prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mind2Web (cross-task comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>web navigation / in-context exemplar-guided action generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>As reported by comparisons in this paper, AWM (which uses abstracted sub-routine workflows) achieved +5.0 element accuracy and +4.0 step success rate over Synapse with gpt-3.5-turbo on Mind2Web cross-task (paper reports these comparative deltas rather than Synapse's raw numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>The paper does not provide Synapse's raw scores in all tables, but treats Synapse as a with-memory baseline (retrieved examples) and reports the improvement deltas achieved by AWM against it.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Element Accuracy; Step Success Rate; Action F1</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Concrete example retrieval can bias agents to select elements similar to the examples (less generalization), whereas abstract workflows reduce example-specific bias; Synapse's use of full exemplars may therefore be less flexible across tasks/domains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Concrete full-example exemplars entangle example-specific contexts and may not extrapolate to different tasks or domains as effectively as abstract workflows; paper reports AWM's abstract workflows yield better element selection accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Zheng, L., Wang, R., Wang, X., & An, B. (2024). Synapse: Trajectory-as-exemplar prompting with memory for computer control.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent Workflow Memory', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6443.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6443.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SteP (human workflows baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SteP: Hierarchical policies for web actions using LLMs (human‑engineered workflows)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that augments agents with a set of human expert–written site-specific workflows (14 workflows for WebArena) provided as guidance; used as a supervised/workflow-based comparator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hierarchical policies for web actions using llms</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SteP (agent augmented with human-engineered workflows)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LM agent augmented with a curated set of human-written, site-specific workflows that serve as external guidance/tools to assist action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in this paper (SteP original paper referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>human-crafted workflow memory stored as auxiliary guidance (site-specific workflows)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Human-authored workflow scripts/policies (text/program-like sequences of steps tailored to target site tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Workflows provided as auxiliary context/tooling for the agent (site-specific expert-written workflows injected into agent input or policy); exact retrieval mechanism depends on SteP implementation (refer to original paper).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WebArena (evaluated in paper as a comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>web navigation / hierarchical policy guidance</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported SteP total success rate in paper's WebArena table ~33.0% (human-engineered workflows); AWM outperformed SteP by a reported 7.6% relative increase in overall success rate.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>SteP itself is the human-workflow-augmented agent; the paper compares AWM to autonomous baselines (BrowserGym) and to SteP, but the raw 'no-workflow' baseline (BrowserGym) had lower SR and is reported separately.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (SR) on WebArena</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Human-engineered workflows require domain expertise and are site-specific (not scalable); AWM (automatically induced workflows) outperformed SteP despite SteP's human supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Human-written workflows are domain/site-specific and may not generalize; require manual effort to craft and maintain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Sodhi, P., et al. (2023). Hierarchical policies for web actions using LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent Workflow Memory', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Synapse: Trajectory-as-exemplar prompting with memory for computer control <em>(Rating: 2)</em></li>
                <li>Hierarchical policies for web actions using llms <em>(Rating: 2)</em></li>
                <li>Mind2Web: Towards a generalist agent for the web <em>(Rating: 2)</em></li>
                <li>Autonomous evaluation and refinement of digital agents <em>(Rating: 1)</em></li>
                <li>BrowserGym: (How capable are web agents at solving common knowledge work tasks?) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6443",
    "paper_id": "paper-272592995",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "AWM",
            "name_full": "Agent Workflow Memory",
            "brief_description": "A method that induces reusable, abstracted sub‑routines (workflows) from past agent trajectories and injects them as text‑based memory into a language‑model agent; supports both offline (from training examples) and online (streaming induction from successful test solves) update modes and can also expose workflows as high‑level callable actions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "AWM-augmented LM agent (uses GPT-4 / gpt-3.5-turbo in experiments)",
            "agent_description": "A text-input LM backbone (GPT‑4/GPT‑3.5 variants) that solves web navigation tasks with an auxiliary text memory M_w containing induced workflows. At inference the LM receives the natural language instruction, current observation, base memory and the workflow memory and generates actions; in online mode successful predicted trajectories are evaluated, transformed into workflows by an LM induction module, and appended to the memory for subsequent tasks.",
            "model_size": "gpt-4 and gpt-3.5-turbo (API variants used; exact parameter counts not specified in paper)",
            "memory_used": true,
            "memory_type": "textual workflow memory stored and provided in prompt/system context (prompt-augmented auxiliary memory); optionally wrapped as callable workflow actions (expanded action space variant AWM_AS)",
            "memory_representation": "Workflows = {textual description, sequence of steps expressed as program-like actions and optional NL step descriptions}; stored as raw text (program format or verbalized text) in the agent's prompt/context",
            "memory_access_mechanism": "Read by including workflows in the LM input context (system prompt / auxiliary prompt); workflows are selected/organized by website and concatenated into context. Online writes: LM-based induction I(E) -&gt; W creates workflows from successful experiences which are appended to memory M_t -&gt; M_{t+1}. AWM_AS exposes workflows as callable high-level actions that execute the predetermined step sequence when invoked.",
            "task_name": "WebArena; Mind2Web",
            "task_category": "web navigation / long-horizon planning and action sequence execution",
            "performance_with_memory": "WebArena: AWM achieves the best published WebArena results in the paper — reported as +12.0 absolute points (51.1% relative) vs the BrowserGym baseline and outperforming human-written-workflow method SteP by a 7.6% relative increase in overall success rate; also uses ~2.0 fewer steps per solved task than BrowserGym. Mind2Web (cross-task, using GPT-4, offline AWM): step success rate ~45.1% and element accuracy 50.6% (as reported for AWM_lm in paper tables).",
            "performance_without_memory": "WebArena: baseline BrowserGym (no induced workflows) — AWM reported +12.0 absolute points (51.1% relative) improvement over this baseline. Mind2Web baseline MindAct (no workflows) step success rate 36.2% (gpt-4); AWM offline step SR 45.1% (gpt-4) — an absolute improvement ~8.9 points (reported in paper).",
            "has_comparative_results": true,
            "performance_metric": "Success Rate (SR) for end-to-end tasks; Step Success Rate; Element Accuracy; Action F1; also average # steps",
            "tradeoffs_reported": "Adding workflows increases prompt/context content (longer context), which can hurt performance if too large; representing workflows with both NL + filtered HTML increased context length and sometimes contradicted descriptions leading to worse results; AWM_AS (workflow actions) produced only small extra gains because agents called workflow actions in only ~18.5% of tasks. Offline+online combination of workflows was not purely additive and in some cases impaired online workflow utility (compatibility issues).",
            "limitations_or_failure_cases": "Workflows can bias the agent toward actions that are not appropriate in a particular intermediate state, causing slightly lower Action F1 (AWM had slightly lower action F1 than some baselines). Online-induced workflows depend on evaluator accuracy — incorrect evaluator judgments can yield incorrect workflows that degrade performance. Static workflow actions are inflexible for dynamic intermediate UI states (example: choosing a pop-up airport option), which prevents some workflow calls from succeeding. Combining NL and filtered HTML in workflow steps sometimes worsened performance because filtered HTML missed correct elements 47% of the time and increased context ambiguity.",
            "citation": "Wang, Z. Z., Mao, J., Fried, D., & Neubig, G. (2024). AGENT WORKFLOW MEMORY. arXiv preprint.",
            "uuid": "e6443.0",
            "source_info": {
                "paper_title": "Agent Workflow Memory",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "AWM_AS",
            "name_full": "AWM Action-Space Variant",
            "brief_description": "Variant of AWM that additionally wraps induced workflows into high‑level callable actions appended to the agent's action space so the agent may call a workflow as a single action which executes the sequence of primitive steps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "AWM_AS-augmented LM agent (GPT-4 experiments)",
            "agent_description": "Same LM backbone as AWM, but induced workflows are also registered as high-level actions callable at runtime; when a workflow action is called the environment executes the sequence of primitive actions specified by the workflow until completion.",
            "model_size": "gpt-4 (API variant used; parameter count not specified)",
            "memory_used": true,
            "memory_type": "textual workflow memory plus expanded action-space entries (workflows as callable functions)",
            "memory_representation": "Workflows as both textual memory entries and as programmatic functions (high-level actions) that map to fixed sequences of primitive actions.",
            "memory_access_mechanism": "Workflows remain in prompt memory; additionally the agent's action sampling can choose workflow-actions; invoking a workflow triggers deterministic execution of stored primitive action sequence.",
            "task_name": "Mind2Web (evaluated)",
            "task_category": "web navigation / programmatic action execution",
            "performance_with_memory": "AWM_AS (gpt-4) achieved step success rate 46.4% and task success 3.6% (table in paper) — reported modest improvements in element accuracy and step success rate relative to AWM memory-only variant (AWM_AS element acc 51.8% vs AWM 50.6%; step SR 46.4% vs 45.1%).",
            "performance_without_memory": "Baseline AWM (memory-only) had element acc 50.6% and step SR 45.1% (gpt-4). Compared to MindAct baseline (41.6% element acc, 36.2% step SR), both AWM variants improved performance.",
            "has_comparative_results": true,
            "performance_metric": "Element Accuracy; Action F1; Step Success Rate; Task Success Rate; # of times workflows invoked",
            "tradeoffs_reported": "Despite being available, workflow-actions were invoked in only ~18.5% of tasks; static, precompiled workflow-actions lack flexibility to react to dynamic intermediate UI states (e.g., pop-up selections), limiting their utility. The approach may require mechanisms for real-time state access or dynamic looping for wider applicability.",
            "limitations_or_failure_cases": "Workflow actions can fail when intermediate UI choices depend on real-time state unseen at compile time (pop-up airport choices example); low invocation rate indicates agents often avoid calling precompiled workflows.",
            "citation": "Wang, Z. Z., Mao, J., Fried, D., & Neubig, G. (2024). AGENT WORKFLOW MEMORY. arXiv preprint.",
            "uuid": "e6443.1",
            "source_info": {
                "paper_title": "Agent Workflow Memory",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Synapse (baseline)",
            "name_full": "Synapse: Trajectory-as-exemplar prompting with memory for computer control",
            "brief_description": "A prior method that stores and retrieves concrete exemplar trajectories (training examples) as in‑context demonstrations to guide LM agents; used as a baseline in the paper and contrasted with AWM's abstract workflow memory.",
            "citation_title": "Synapse: Trajectory-as-exemplar prompting with memory for computer control",
            "mention_or_use": "use",
            "agent_name": "Synapse (trajectory-as-exemplar LM agent)",
            "agent_description": "Retrieval-augmented LM agent that retrieves and injects full concrete prior trajectories (examples) into the LM context as exemplars to influence action generation.",
            "model_size": "gpt-3.5-turbo / gpt-4 (Synapse paper variants; in this paper Synapse is evaluated with gpt-3.5-turbo baseline comparisons)",
            "memory_used": true,
            "memory_type": "retrieval-augmented exemplar memory (stored training trajectories as in-context examples)",
            "memory_representation": "Concrete full example trajectories (raw action-step sequences with example-specific values)",
            "memory_access_mechanism": "Retrieve the most relevant training examples and place them into the LM input context as demonstrations (trajectory-as-exemplar prompting).",
            "task_name": "Mind2Web (cross-task comparisons)",
            "task_category": "web navigation / in-context exemplar-guided action generation",
            "performance_with_memory": "As reported by comparisons in this paper, AWM (which uses abstracted sub-routine workflows) achieved +5.0 element accuracy and +4.0 step success rate over Synapse with gpt-3.5-turbo on Mind2Web cross-task (paper reports these comparative deltas rather than Synapse's raw numbers).",
            "performance_without_memory": "The paper does not provide Synapse's raw scores in all tables, but treats Synapse as a with-memory baseline (retrieved examples) and reports the improvement deltas achieved by AWM against it.",
            "has_comparative_results": true,
            "performance_metric": "Element Accuracy; Step Success Rate; Action F1",
            "tradeoffs_reported": "Concrete example retrieval can bias agents to select elements similar to the examples (less generalization), whereas abstract workflows reduce example-specific bias; Synapse's use of full exemplars may therefore be less flexible across tasks/domains.",
            "limitations_or_failure_cases": "Concrete full-example exemplars entangle example-specific contexts and may not extrapolate to different tasks or domains as effectively as abstract workflows; paper reports AWM's abstract workflows yield better element selection accuracy.",
            "citation": "Zheng, L., Wang, R., Wang, X., & An, B. (2024). Synapse: Trajectory-as-exemplar prompting with memory for computer control.",
            "uuid": "e6443.2",
            "source_info": {
                "paper_title": "Agent Workflow Memory",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SteP (human workflows baseline)",
            "name_full": "SteP: Hierarchical policies for web actions using LLMs (human‑engineered workflows)",
            "brief_description": "A baseline that augments agents with a set of human expert–written site-specific workflows (14 workflows for WebArena) provided as guidance; used as a supervised/workflow-based comparator.",
            "citation_title": "Hierarchical policies for web actions using llms",
            "mention_or_use": "use",
            "agent_name": "SteP (agent augmented with human-engineered workflows)",
            "agent_description": "An LM agent augmented with a curated set of human-written, site-specific workflows that serve as external guidance/tools to assist action generation.",
            "model_size": "not specified in this paper (SteP original paper referenced)",
            "memory_used": true,
            "memory_type": "human-crafted workflow memory stored as auxiliary guidance (site-specific workflows)",
            "memory_representation": "Human-authored workflow scripts/policies (text/program-like sequences of steps tailored to target site tasks)",
            "memory_access_mechanism": "Workflows provided as auxiliary context/tooling for the agent (site-specific expert-written workflows injected into agent input or policy); exact retrieval mechanism depends on SteP implementation (refer to original paper).",
            "task_name": "WebArena (evaluated in paper as a comparison)",
            "task_category": "web navigation / hierarchical policy guidance",
            "performance_with_memory": "Reported SteP total success rate in paper's WebArena table ~33.0% (human-engineered workflows); AWM outperformed SteP by a reported 7.6% relative increase in overall success rate.",
            "performance_without_memory": "SteP itself is the human-workflow-augmented agent; the paper compares AWM to autonomous baselines (BrowserGym) and to SteP, but the raw 'no-workflow' baseline (BrowserGym) had lower SR and is reported separately.",
            "has_comparative_results": true,
            "performance_metric": "Success Rate (SR) on WebArena",
            "tradeoffs_reported": "Human-engineered workflows require domain expertise and are site-specific (not scalable); AWM (automatically induced workflows) outperformed SteP despite SteP's human supervision.",
            "limitations_or_failure_cases": "Human-written workflows are domain/site-specific and may not generalize; require manual effort to craft and maintain.",
            "citation": "Sodhi, P., et al. (2023). Hierarchical policies for web actions using LLMs.",
            "uuid": "e6443.3",
            "source_info": {
                "paper_title": "Agent Workflow Memory",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Synapse: Trajectory-as-exemplar prompting with memory for computer control",
            "rating": 2,
            "sanitized_title": "synapse_trajectoryasexemplar_prompting_with_memory_for_computer_control"
        },
        {
            "paper_title": "Hierarchical policies for web actions using llms",
            "rating": 2,
            "sanitized_title": "hierarchical_policies_for_web_actions_using_llms"
        },
        {
            "paper_title": "Mind2Web: Towards a generalist agent for the web",
            "rating": 2,
            "sanitized_title": "mind2web_towards_a_generalist_agent_for_the_web"
        },
        {
            "paper_title": "Autonomous evaluation and refinement of digital agents",
            "rating": 1,
            "sanitized_title": "autonomous_evaluation_and_refinement_of_digital_agents"
        },
        {
            "paper_title": "BrowserGym: (How capable are web agents at solving common knowledge work tasks?)",
            "rating": 1,
            "sanitized_title": "browsergym_how_capable_are_web_agents_at_solving_common_knowledge_work_tasks"
        }
    ],
    "cost": 0.01569625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AGENT WORKFLOW MEMORY
11 Sep 2024</p>
<p>Zora Zhiruo Wang 
Carnegie Mellon University Massachusetts Institute of Technology</p>
<p>Jiayuan Mao 
Carnegie Mellon University Massachusetts Institute of Technology</p>
<p>Daniel Fried 
Carnegie Mellon University Massachusetts Institute of Technology</p>
<p>Graham Neubig 
Carnegie Mellon University Massachusetts Institute of Technology</p>
<p>AGENT WORKFLOW MEMORY
11 Sep 2024889478DE371975408C1C1830E11886A4arXiv:2409.07429v1[cs.CL]
Despite the potential of language model-based agents to solve real-world tasks such as web navigation, current methods still struggle with long-horizon tasks with complex action trajectories.In contrast, humans can flexibly solve complex tasks by learning reusable task workflows from past experiences and using them to guide future actions.To build agents that can similarly benefit from this process, we introduce Agent Workflow Memory (AWM), a method for inducing commonly reused routines, i.e., workflows, and selectively providing workflows to the agent to guide subsequent generations.AWM flexibly applies to both offline and online scenarios, where agents induce workflows from training examples beforehand or from test queries on the fly.We experiment on two major web navigation benchmarks -Mind2Web and WebArena -that collectively cover 1000+ tasks from 200+ domains across travel, shopping, and social media, among others.AWM substantially improves the baseline results by 24.6% and 51.1% relative success rate on Mind2Web and WebArena while reducing the number of steps taken to solve WebArena tasks successfully.Furthermore, online AWM robustly generalizes in cross-task, website, and domain evaluations, surpassing baselines from 8.9 to 14.0 absolute points as train-test task distribution gaps widen.https://github.com/zorazrw/agent-workflow-memory</p>
<p>INTRODUCTION</p>
<p>Language model (LM) based agents are rapidly improving, and are now able to tackle digital tasks such as navigating the web (Zhou et al., 2024;Deng et al., 2023) or operating mobile apps (Rawles et al., 2023;2024).Current agents mostly integrate a fixed set of given examples via training (Fu et al., 2024;Murty et al., 2024) or in-context learning (Zheng et al., 2024).This allows them to perform well on action sequences similar to those presented in these examples, but results in a lack of robustness to changes in task contexts or environments (Deng et al., 2023).Essentially, they fail to grasp the key to disentangling increasingly complex tasks -to extract and learn reusable task workflows shared across similar tasks and environments (Yu et al., 2023;Wang et al., 2024a).Moreover, as agents solve each task separately, they do not learn from past successes and failures, and are therefore unable to adapt over time (Yoran et al., 2024).Motivated by how humans abstract common task routines from past experiences and apply such knowledge to guide future activities (Chi et al., 1981;2014), we propose agent workflow memory (AWM) ( §2) to realize a similar mechanism in agents.AWM induces workflows from agent trajectories by extracting reusable routines, and then integrates these workflows into agent memory to guide future task-solving processes.Each workflow represents a goal with a common routine extracted from available action trajectories, which allows it to effectively capture the most essential and reusable skills agents need to acquire to successfully solve increasingly complex tasks.As an example, Figure 1 shows workflows induced by AWM on the WebArena map test split of the benchmark (Zhou et al., 2024).AWM starts with a basic set of built-in actions and solves new tasks in a streaming manner, continuously inducing workflows from the task at hand, e.g., learning to "find a place by its name" from the first few examples.Moreover, AWM continues to build more complex workflows on top of new experiences and previously acquired workflows.For example, the "find a place by its name" workflow, once induced, effectively serves as a subgoal to build a more complex workflow "get the zip code of a place."Such continual learning mechanisms create a snowball effect to induce and apply increasingly complex workflows while expanding the agent memory, often yielding a substantial performance gap over a vanilla agent that does not adapt.This gap over the baseline rises as high as 22.5 points on WebArena after rolling over only tens of examples (as shown by Figure 1).AWM readily operates in both offline and online scenarios, where annotated examples are either available or non-existent.When high-quality annotated examples are available for a task, AWM operating in an offline fashion can extract reusable workflows from these canonical examples and integrate them into memory to assist test-time inference.Even if no annotated examples exist, AWM online can also run in an supervision-free setting, where it iteratively induces workflows from selfgenerated past predictions that are judged correct by an evaluator module.</p>
<p>We evaluate AWM on two agent web navigation benchmarks ( §3): WebArena, which provides rigorous execution-based evaluation (Zhou et al., 2024), and Mind2Web, which emphasizes broad tasks and domain coverage (Deng et al., 2023).On WebArena, AWM improves over the top published autonomous method (Drouin et al., 2024) by 51.1% relative success rate, and even outperforms methods augmented with human expert written workflows (Sodhi et al., 2023) by 7.9%.On Mind2Web, AWM effectively improves the cross-task results by 24.6% in relative step-wise success rate.</p>
<p>We further demonstrate the generalizability of AWM on both datasets.On WebArena, we create a cross-template subset where each example is instantiated from different task templates.AWM still consistently surpasses all baseline approaches, demonstrating its reliable cross-task workflow adaptability ( §3.1).On Mind2Web, we evaluate AWM on the cross-website and cross-domain test splits to examine its domain generality, where it scores 8.9-14.0absolute points higher over baseline, and the margins become more substantial as the train-test distribution gap widens ( §3.2).Both results show the superior generalization of AWM across tasks, websites, and domains.</p>
<p>AGENT WORKFLOW MEMORY</p>
<p>In this section, we first describe the web navigation task ( §2.1), then introduce the workflow representation ( §2.2), and describe the mechanism of AWM as well as various instantiations ( §2.3).</p>
<p>PROBLEM STATEMENT</p>
<p>For the purpose of this paper, we consider agents with a language model backbone L and text-based memory M , where the base memory contains documentation of built-in actions such as CLICK and TYPE.</p>
<p>1 To solve a task specified by a natural language (NL) instruction q, the agent acts in an environment defined by a transition function T .For each time step t i , the environment state s i gives observation o i , which is then passed into the model to generate action a i via L(q, M, o i ) → a i .The action is executed in the environment and changes the state as T (s i , a i ) → s i+1 .This observeact loop iterates until the model predicts the stop action a i =STOP, or reaches a task termination condition, e.g., a maximum pre-determined number of steps.</p>
<p>Each completed task forms an experience e, which comprises an NL instruction q and a trajectory of steps attempting to solve the task, where each step p contains the agent observation o obtained from the current state, and action taken by the agent a, formulated as p = (o, a).Our goal is to induce useful workflows W = {w} from the set of experiences E = {e} constructed from past or collected examples, using an induction module I via I(E) → W. We add induced workflows into the agent memory M as guidance for subsequent task-solving.</p>
<p>Next, we introduce the workflow representation design, workflow induction method, and agent memory update with workflows in varied setups.</p>
<p>WORKFLOW REPRESENTATION</p>
<p>Similar to an experience, a workflow comprises two components: first, a textual description of the workflow d; and second, a series of steps to finish the workflow (p 1 , p 2 , ⋯), as shown in Figure 2. (2) The reasoning process elaborated by the agent to decide which action to generate based on observations, such as "Order {id} is found, I will now terminate the task.";and (3) an action represented as an executable program over the environment, i.e., stop() that realizes termination.</p>
<p>INDUCING AND USING WORKFLOWS</p>
<p>At the core of AWM is an induction module I that induces a set of workflows W from one or more past agent experiences E = {e i } m i=1 .Each experience e = (q, P e ) contains an NL task instruction q and an action trajectory that consists of a sequence of steps (observation and action) P e = (p e 1 , ..., p e n ) that were taken to solve q.The workflow induction module operates by taking in E and producing a set of workflows, as I(E) → W = {w} = {(d j , P d j )}.</p>
<p>LM-based Workflow Induction</p>
<p>To produce workflows that more accurately capture reusable trajectories across tasks, we propose an LM-based module I that prompts the agent to extract common sub-routines from one or more input experiences.</p>
<p>Different from task instructions that specify concrete, less-repetitive tasks, e.g., "Buy dry cat food on Amazon and deliver to my address", we deliberately prompt models to induce workflows at finer granularities, i.e., a sub-task "search for a product on Amazon" that frequently re-appears as part of multiple similar instructions.Meanwhile, instead of giving example-specific values (e.g., "dry cat food"), we enhance workflow generality by abstracting out example-specific contexts, i.e., replacing "dry cat food" with a more general name "{product-name}" by specifying this in the workflow induction prompts.These workflows are segmented (based on double-line breaks in the model output) and stored separately in the workflow memory.See §A for the model prompts, example workflows, and an examination of quality.After the workflows W are induced, they are then integrated into the agent as auxiliary memory, M + W → M w , where M stands for the original agent memory, and M w stands for the agent memory augmented with induced workflows.When solving a given instruction q, the agent now produces a series of actions by L(q, M w , o) = L(q, M + W, o) → a.In the following, we introduce AWM in use in two scenarios:</p>
<p>Offline Scenario AWM can operate in an offline scenario when additional canonical experiences are available, such as data annotated by humans or synthesized by models.In this case, we perform workflow induction and utilization in two standalone processes.As seen in Figure 3 .Since the workflows are fully induced before test-time inference, the agent uses the same workflow memory W offline to solve each test.</p>
<p>Online Scenario Extra canonical experiences are not always available or easy to collect, especially those that cover the same domains and tasks as the test instructions.AWM also works in an online, supervision-free setting, where only test queries are needed.Agents with AWM online process test queries in a streaming fashion, where the agents conduct the loop of induce, integrate, and utilize workflows after running inference for each test task.Concretely, the agent starts with the default memory M ; given the t-th test instruction q t passed into the agent, the agent attempts to solve the task by generating an action trajectory (p t 1 , p t 2 , ⋯), which collectively forms an experience e t = (q t , {p t }).We adopt the LM-based evaluation model of Pan et al. (2024) to output a binary label, L eval (e t ) ∈ {0, 1}, that judges if e t successfully solves q t by prompting a neural model.If e t is predicted as success, i.e., 1, we then transform it into workflow(s) I(e t ) → {w t } and add {w t } into the agent memory M t + {w t } → M t+1 , which serves as the agent memory to process the t + 1-th instruction.As depicted in Figure 4, we continue this memory-updating process by iteratively predicting actions for and inducing workflows from streamed test instructions, until all tests are processed.We evaluate the success rate of the predicted action trajectories {p t } for all tests and report the average score.</p>
<p>EXPERIMENTS</p>
<p>In this section, we experiment on two major web navigation benchmarks -WebArena ( §3.1) and Mind2Web ( §3.2).For each benchmark, we first introduce the benchmark and top-performing baseline methods, then present our AWM approach and showcase its ability to achieve superior task success and generalization across varied setups.</p>
<p>For both benchmarks, we conduct AWM on a website basis.In other words, we group examples by their associated websites, and respectively run AWM on each group.This mechanism maintains an small collection of workflows that are nonetheless relevent to the test tasks.</p>
<p>WEBARENA</p>
<p>WebArena (Zhou et al., 2024) provides 812 web navigation tasks on five websites that cover four common application domains: e-commerce, social forum discussions, collaborative software development, and content management.Most importantly, WebArena supports rigorous evaluation on the functional correctness of agent trajectories.</p>
<p>We adopt the current state-of-the-art method without human-annotated site-specific knowledge, BrowserGym (Drouin et al., 2024), which altered the agent default action space.We adopt the BrowserGym framework and its default action space, and represent webpages using accessibility trees, following the environment representation in Zhou et al. (2024).Because BrowserGym inputs both webpage HTML and accessibility tree representations, to keep a fair comparison with our method, we also run the BrowserGym version with only accessibility tree webpage representations, denoted as BrowserGym ax −tree .We also compare to the SteP method (Sodhi et al., 2023), which uses 14 human expert written workflows tailored to solving WebArena.Our method, in contrast, uses no human supervision and is not tailored to the WebArena setting.</p>
<p>Table 1: Task success rate on WebArena using gpt-4, and score breakdown on five website splits.</p>
<p>Method Total SR Shopping CMS Reddit GitLab Maps # Steps</p>
<p>With human engineered workflows *SteP (Sodhi et al., 2023) 33.0 37.0 24.0 59.0 32.0 30.0 -Autonomous agent only WebArena (Zhou et al., 2024) 14.9 14.0 11.0 6.0 15.0 16.0 -AutoEval (Pan et al., 2024) 20.2 25.5 18.1 25.4 28.6 31.9 46.7 BrowserGym (Drouin et al., 2024) 23.Following baseline approaches, we use GPT-4 (gpt-4-0613) with a temperature of 0.0 to ensure mostly stable model outputs.Because WebArena only has test examples and no additional highquality, domain-aligned examples exist, we only conduct AWM in the online setting as in §2.3.</p>
<p>MAIN RESULTS</p>
<p>As shown in Table 1, our AWM achieves the best published results on WebArena, surpassing the BrowserGym baseline by 12.0 absolute points and 51.1% relative increase in overall success rate.Notably, AWM also outperforms SteP, which uses strong domain-specific supervision from humanwritten workflows, by a 7.6% relative increase in overall success rate.According to the breakdown on five websites, our AWM method substantially enhances the agent performance across all websites over the BrowserGym baseline, by 11.8-30.7 absolute points, indicating its general applicability across varied domains and tasks.</p>
<p>Beyond task success, we also evaluate the average number of steps the agent takes to solve a task, as shown in the rightmost column in Table 1.AWM conducts about 2.0 fewer steps per example than the BrowserGym baseline.Further compared to the Autoeval (Pan et al., 2024) method, which necessitates additional evaluation and refinement steps to solve tasks correctly, our AWM approach uses 40.8 fewer steps on average.Both comparisons show that AWM obtains high success rates while maintaining efficient trajectories.To demonstrate the behavior of the AWM online method, we illustrate the cumulative success rate over the process of online evaluation, by evaluating the average success rate of the first k finished examples.</p>
<p>As in Figure 5, the agent exhibits a fast learning curve in the beginning (between 0-40 examples), by acquiring the most essential workflows, which results in higher success rates.Afterward, agents learn more advanced workflows (Figure 1), while success rates gradually stabilize to the highest point in the early learning phase.This showcases AWM's efficient learning process, which substantially improves performance with merely tens of examples.</p>
<p>CROSS-TEMPLATE WORKFLOW GENERALIZATION</p>
<p>Some tasks in the WebArena benchmark have highly overlapping canonical trajectories, due to the benchmark construction process that instantiates multiple examples from a single underlying task template.AWM intuitively improves in-template success rate, that is, given one workflow induced from a successful example, it would be theoretically easier to solve all other examples generated from the same task template.Building increasingly complex workflows To more intuitively demonstrate AWM's cross-template generalization and ability to build increasingly complex workflows (as exemplified in Figure 1), we conduct a case study to illustrate the workflow mechanism behind it.</p>
<p>As exemplified in Figure 6, the agent creates and learns the "Find a place by its name" workflow in the early stage of the online process by summarizing past examples.Later, when encountering an example that further asks to obtain the zip code of the location, AWM agent learns to adopt the first few steps to find locations by following the existing workflow, and then conducts further steps to obtain the zip code of the place found.Integrating these new steps upon the vanilla find location task, the agent successfully builds a more complex workflow, i.e., "get the zip code of a place".</p>
<p>MIND2WEB</p>
<p>Mind2Web (Deng et al., 2023) features web navigation in cross-task, website, and domain settings, stressing the generality of agents on versatile operations and environments.Each task in Mind2Web has a fixed number of steps; at each step, the agent needs to predict an action, which is evaluated by:</p>
<p>(1) element accuracy: to check if the correct page element is selected, (2) action F 1 to check if the action taken on the element is correct, and aggregating (1) and ( 2) yields (3) step success rate which checks that both element and action selection are correct at the current step.Lastly, after completing every step in the given task, the last metric (4) task-level success rate measures if all intermediate steps are successfully conducted for this task, i.e., all steps for this task score 1 under metric (3).</p>
<p>Because Mind2Web provides a training set covering part of the tested websites (cross-task split), we explore both the offline setting that induces workflows from the training set and applies to test sets, and the online setting, where we stream workflow induction and inference on test queries ( §2.3).</p>
<p>Since we focus on LM-based agents that only take textual inputs, we compare AWM to two state-ofthe-art methods, MindAct (Deng et al., 2023) and Synapse (Zheng et al., 2024).MindAct introduces webpage element filtering and multi-choice task format to ease observation processing, and Synapse changes the format to a trajectory style and augments retrieved relevant examples.We integrate the element filtering adopted in both methods, and added workflows instead of retrieved examples in Synapse, to verify the superiority of reusable workflows over concrete examples.To fairly compare with all baseline methods, we run AWM with both gpt-3.5-turboand gpt-4 models with temperature 0.0.We use the same model for neural workflow induction and agent action generation.</p>
<p>MAIN RESULTS</p>
<p>Table 3: AWM offline results on Mind2Web crosstask dataset.Elem Acc and SR are short for element accuracy and success rate.We footnote the GPT variant used by each method, 3.5 and 4 stands for gpt-3.5-turboand gpt-4, respectively.We highlight the best result within the same model variant.We first run with AWM offline using both GPT variants, and find that AWM consistently obtains the highest success rate in both step and task levels, leading to 4.0-8.9%relative and 0.4-2.8absolute points increases in step-wise and task-wise success rates than the baselines -Synapse with gpt-3.5-turboand MindAct with gpt-4.Decomposing the step success rate by element and action selection and accuracy, we notice the increases mainly come from more accurate element selection, as indicated by the 5.0-9.0 element accuracy increase in Table 3.</p>
<p>Abstract sub-routines vs. concrete experiences More specifically, compared to the Synapse (Zheng et al., 2024) method that retrieves the most relevant training examples, AWM achieves a +5.0 element accuracy and leads to a +4.0 increase in step success rate.While augmenting concrete, full examples may bias agents to select elements similar to those presented in the given examples, AWM introduces less bias on element selection via its abstract representation of example-specific contexts in workflows, and therefore enables higher step success rates.</p>
<p>Furthermore, AWM integrates frequently-used sub-routines, which can be more flexibly and readily leveraged across test examples, compared to the full example trajectories used by Synapse, which are less likely to appear multiple times.In general, our results indicate that the abstract, reusable nature of workflows contributes to the superiority of the AWM method.</p>
<p>Learn to diverge from workflow guidelines Despite more accurate element selection, AWM gets slightly lower action F 1 scores than MindAct, possibly because the augmented workflows may guide the agent to take certain actions aligning to the workflows, which are not always relevant to the particular environment state at hand.While following the workflows generally results in more successful task trajectories, agents still encounter some challenges in identifying places to diverge from the workflow guidelines.</p>
<p>ONLINE AWM ENABLES GENERALIZATION</p>
<p>Beyond the offline induction setting, we further explore the AWM in the online setting, similar to the WebArena experiment setup in §3.1, where no additional training examples are needed besides test queries.This naturally facilitates cross-website and cross-domain generalization, which we examine on the two other splits provided by the Mind2Web dataset: cross-website and cross-domain tests.</p>
<p>In addition to the MindAct baseline, we additionally set bars with our AWM offline setup, by randomly selecting workflows induced from the training set as memory augmentations.Specifically, for cross-website examples, we select workflows from the same domain; for the cross-domain setting, we randomly select workflows from all domains.We conduct AWM online by iteratively inducing, integrating, and utilizing workflows over test inferences.We also explore AWM offline+online in §C.</p>
<p>As shown in Table 4, both AWM online and AWM offline surpass the MindAct baseline by a large margin, resulting in 7.4-8.9,3.6-3.8,and 14.0-16.9absolute point improvements in step success rates, in cross-task, cross-website, and cross-domain scenarios.</p>
<p>In-domain, cross-task scenario When tested in-domain, AWM online and AWM offline perform comparably to each other.When inspecting the model behaviors in detail, we notice the pros and cons of each method.AWM online induces workflows from model-predicted trajectories that are</p>
<p>EXPLORING OPTIMAL WORKFLOW REPRESENTATIONS</p>
<p>In this section, we experiment with other possible alternatives to better represent the workflows.Specifically, we ablate workflows in sub-routine, abstract formats ( §4.1), explore workflows in descriptive texts ( §4.2), and lastly, beyond the default workflows that describe environment state in NL, we compare strengthened observations with website HTML within workflow steps ( §4.3).</p>
<p>HOW MUCH DOES THE SUB-ROUTINE, ABSTRACT FORMAT CONTRIBUTE?</p>
<p>In this section, we compare our abstract, sub-routine-based induction method using LMs to a rulebased method without context and sub-routine abstraction.</p>
<p>Specifically, our rule-based induction I rule first extracts the action sequence (e.g., CLICK → CLICK → TYPE) of each experience and deduplicates experiences by their action sequence.In each unique experience, we then remove the steps whose action cannot be executed on the environment.We take these unique, validated experiences as workflows.Find more detailed descriptions in §B.WebArena Results As shown in Table 5, using rule-and LM-based workflow induction performs comparably, with a small 0.1 gap in success rate; the LM-based method appears more efficient and uses 0.4 fewer steps.Our manual analysis found workflows produced by the LM-based induction module I lm are finer-grained, preventing agents from following unnecessary steps that sometimes appear in rule-induced workflows, hence making the task-solving process slightly more efficient.</p>
<p>Mind2Web Results As shown in</p>
<p>WORKFLOWS IN DESCRIPTIVE TEXTS</p>
<p>AWM represents workflow steps in a program format.In this section, we compare with a textual format for workflows, to understand whether text or code serves as a better format for agent memory.</p>
<p>More concretely, we prompt gpt-3.5-turbo to verbalize the action trajectory in the workflows induced in earlier experiments.For example, from an action CLICK({submit-id}), its verbalized NL representation reads similar to "CLICK the submit button".We use the same textual observation and thoughts from code actions as observation and thoughts in these text actions.From the results in Table 7, AWM text achieves slightly higher element selection accuracy and step success rate, by 0.6 and 0.3 points, respectively, yet degrades 1.2 in task success rate.Overall, we do not find substantial performance variance between workflows represented in text and code formats, indicating that both forms can bring effective augmentations to agent memory.</p>
<p>ENVIRONMENT ABSTRACTION IN WORKFLOWS</p>
<p>AWM describes intermediate webpage states using NL, yet showing concrete states may be helpful to better ground agents on the environment.Since a webpage's full HTML can be overly long, we filter the webpage representation using the relevance predictor of Deng et al. (2023), and augment each workflow step with this shortened HTML that only has elements predicted as relevant.We run gpt-3.5-turbowith only descriptions, only HTML, and both types of content.As shown in Table 8, NL description of states is more useful than HTML, as replacing NL with HTML leads to a slight 0.8 drop in step success rate.Interestingly, using both NL and filtered HTML leads to worse results.We conjecture the reason to be two-fold.First, adding NL and HTML substantially increases the context length, thus making it harder for models to handle things correctly.Second, the filtered HTML has a substantial number of irrelevant items (missing all correct elements 47% of the time) thus potentially contradicting NL descriptions and impairing agent abilities.</p>
<p>EXPLORING WORKFLOW UTILIZATION IN CONTEXT AND IN ACTION</p>
<p>Besides integrating workflows as agent memory, we also explore workflows in expanding the agent action space, denoted as AWM AS .We leverage the programmatic nature of workflows and wrap each workflow into a high-level function, similar to a shortcut tool the agent can call to perform a pre-determined series of actions (Wang et al., 2024b).Formally, an agent is initially equipped with default, primitive actions P (e.g., click, type), and AWM AS adds the induced workflow actions W (e.g., find place, get place zipcode) to its action space.</p>
<p>The agent can call a primitive or workflow action at each step.When a primitive action is called, the agent immediately takes that action.When the agent calls a workflow action, it will trigger the sequence of pre-determined steps in the workflow.For example, calling the login(username, password) workflow action results in sequentially executing click(box1-id) → type(box1-id, username) → click(box2-id) → type(box2-id, password) → click(submit-id).</p>
<p>The workflow action is completed when all intermediate primitive actions are finished.</p>
<p>In Table 9, expanding the agent action space with workflows (AWM AS ) slightly improves the step success rate by 1.3 points, and gets the same overall success rate, 3.2, of the base memoryaugmented AWM.We analyzed agent predictions and found they call workflow actions in merely 18.5% of the tasks, suggesting a resistance of current agents to use newly-added actions.Overall, expanding actions with workflows seems to reinforce workflows in memory, and brings small extra gains as auxiliary actions.When booking flights, users often input a city name such as "New York," yet the system often pops up some nearby airports to support next-step search.While one can induce a book flight workflow that enters all required data via a pre-determined action sequence, the action to choose pop-up airports is executed without seeing the intermediate states with available pop-up options, and is not flexible enough to do so.More advanced techniques such as granting real-time state access or dynamic execution loops can be promising to solve this issue, and we encourage future work to leverage the AWM framework to explore these.</p>
<p>RELATED WORK</p>
<p>Web Agent Benchmarks The first modern and widely used web agent benchmark is Shi et al. (2017)'s MiniWob, which evaluates across various scenarios such as flight booking.(Liu et al., 2018) then created MiniWob++ with extra challenges.More recently, WebShop (Yao et al., 2022) features a simulated e-commerce website and crowd-sourced text instructions.WebArena (Zhou et al., 2024) integrates four more websites and enables realistic execution-based evaluations, and VisualWebArena (Koh et al., 2024) extends with tasks that necessitate visual inputs.Mind2Web (Deng et al., 2023) proposes versatile tasks and stresses agent generalization across websites and domains.We use WebArena and Mind2Web to evaluate our method's task success and generality.</p>
<p>Enhancing Agents for Complex Tasks Many works improve agents by modifying their action space, such as constraining its action search space (Liu et al., 2018), enabling LM self-feedback to refine predicted actions (Sun et al., 2023), or incorporating human-designed actions to certain tasks (Sodhi et al., 2023).Other works explore ways to augment agent memory, such as adding example demonstrations in context (Haluptzok et al., 2023;Zheng et al., 2024;Fu et al., 2024).However, high-quality examples are not always available or easy to collect.Our AWM can flexibly operate even when auxiliary examples are non-existent and only test queries are available.</p>
<p>Learning Common Procedures from Experiences Some works use full examples (Zheng et al., 2024) as context for an agent, yet they entangle with example-specific contexts and face challenges in extrapolating to other tasks or domains (Majumder et al., 2023).Many works propose to extract frequently reused sub-routines from experiences with rule-based (Ellis et al., 2023;Bowers et al., 2023;Grand et al., 2023) or LM-based methods (Cai et al., 2023;Wang et al., 2024c;a) methods, and use them as auxiliary skills to ease future task-solving (Oh et al., 2017;Liang et al., 2023;Yu et al., 2023;Mao et al., 2023).We explored both rule-and LM-based methods to induce reusable workflows, and use them flexibly as context guidance that are free of environment grounding issues.</p>
<p>CONCLUSION</p>
<p>We propose agent workflow memory that induces, augments, and uses workflows, offline from available examples or purely online at inference time.We evaluate AWM on WebArena and Mind2Web, and achieve 24.6% and 51.1% relative increases in task success rate.AWM also demonstrates its superior generalization abilities across tasks, websites, and domains.We hope AWM sheds insight on and boosts advances in dynamic memory building and agent adaptations on varied digital tasks.</p>
<p>A LM-BASED WORKFLOW INDUCTION</p>
<p>As introduced in §2.3, one realization of our workflow induction module is to prompt LMs to generate abstract, sub-routine workflows from the given examples, i.e., experience.In this section, we provide the detailed model prompt, exemplar workflows induced by models, and quality examination on these workflows.</p>
<p>A.1 MODEL PROMPT</p>
<p>We provide the exact prompt inputted to the model for WebArena and Mind2Web experiments below.Experiments on both datasets use the same prompt.</p>
<p>Given a list of web navigation tasks, your task is to extract the common workflows.Each given task contains a natural language instruction, and a series of actions to solve the task.You need to find the repetitive subset of actions across multiple tasks, and extract each of them out as a workflow.Each workflow should be a commonly reused sub-routine of the tasks.Do not generate similar or overlapping workflows.Each workflow should have at least two steps.Represent the non-fixed elements (input text, button strings) with descriptive variable names as shown in the example.</p>
<p>A.2 EXAMPLE WORKFLOWS</p>
<p>We present several exemplar workflows induced on WebArena and Mind2Web, to give a more concrete impression of workflows.</p>
<p>WebArena Workflows We show one example workflow on each website involved in WebArena.</p>
<h2>shopping: Browse Products in a Specific Category To browse products in a specific category, I need to navigate to the relevant main category.I will start by hovering over the main category menu item to reveal the subcategories.hover('main category id') To browse products in the specific subcategory, I need to click on the subcategory link.click('subcategory id') ## shopping admin: Edit and Save Changes This workflow is used to edit specific fields and save changes.To edit a specific field, I need to locate the field and update its value.clear('field id') fill('field id', 'new value') Next, I need to save the changes by clicking the "Save" button.click('save button id') ## reddit: Navigate to a forum section and select a specific forum To navigate to a specific forum, I need to click on the "Forums" section.click('42') Now, I need to click on the specific forum link based on the forum name provided.click('<forum link id>') ## gitlab: Navigation to Repository and Contributors Section This workflow involves searching for a repository and navigating to its contributors to find detailed contribution data.First, search for the specific repository to gather information.fill('130', '{RepositoryName}') press('130', 'Enter') Navigate to the "Contributors" section to view contribution details. click('311') # "Contributors" link Obtain and report the required contributor details. send msg to user('{ContributorDetails}') ## map: Calculate Travel Time and Distance To calculate travel time and distance between two locations, I will use the directions feature.I will fill in the respective fields and select the mode of transportation.fill('158', 'FROM LOCATION') fill('163', 'TO LOCATION') select option('166', 'MODE OF TRANSPORTATION') click('171') I will use these details to provide the user with accurate travel time and distance information.As shown in Table 10, neural-based induction produces 7.3-7.4workflows per example, which is efficient and do not add too much content to the memory.On WebArena, the induced workflows are</h2>
<p>B RULE-BASED WORKFLOW INDUCTION</p>
<p>Beyond LM-based workflow induction, we also explored a rule-based workflow induction method.</p>
<p>Our rule-based workflow induction module consists of two steps: (i) experience deduplication, and</p>
<p>(2) invalid action filtering.</p>
<p>For deduplication, we extract the action sequence of the experience, e.g., extracting CLICK → CLICK → TYPE from the trajectory CLICK('12') → CLICK('30') → TYPE('44', "cat").We group experiences by their action sequence and randomly select n (n = 1 by default) experiences from each group.Specifically on WebArena, where the task template for each experience is available.We conduct another round of deduplication by grouping experiences by their task template, and randomly selecting n (n = 1 by default) experiences from each group.This process yields diverse experiences from the given set of experiences.</p>
<p>Next, for each unique experience, we remove the invalid steps in its action trajectory.Invalid actions means actions that cannot be successfully executed on the environment, because the input arguments do not meet the requirement of the action function.Specifically, we have one rule of determining invalid actions for CLICK and TYPE, that requires the first argument to be a string-formatted integer (which refers to the id of an element in the environment).We remove CLICK and TYPE steps if they do not meet this requirement.For example, an experience with trajectory CLICK(12) → CLICK('12') → CLICK('30') → TYPE(44, "cat") → TYPE('44', "cat") will yield CLICK('12') → CLICK('30') → TYPE('44', "cat").We conduct this invalid action filtering for each unique experience, and take the resulting experiences as rule-based workflows.</p>
<p>C INTEGRATING AWM OFFLINE AND ONLINE</p>
<p>We compared AWM offline and AWM online in §3.2, that adopts workflows induced separately from training or on-the-fly during testing, respectively.In this section, we explore an integration of both sets of workflows, AWM off +on , that injects relevant training workflows to warm start task-solving, but also aggregates increasingly more online-induced workflows to better adapt to test distributions.From Table 11, AWM off +on scores between AWM offline and AWM online across three test splits.</p>
<p>Rather than an additive effect, workflows induced offline and online are not fully compatible with each other, particularly, the offline workflows seem to impair the generative quality and utility efficacy of online workflows, therefore resulting in medium results overall.</p>
<p>Figure 1 :
1
Figure1: AWM enables agents to continuously induce and apply workflows to improve performance, compared to stagnant baselines.We show results by AWM on the WebArena map split as an example.</p>
<p>❖Figure 2 :
2
Figure 2: Illustration of our AWM pipeline: an agent takes actions to solve given queries, induces workflows from successful ones, and integrates them into memory.</p>
<p>Figure 3 :
3
Figure 3: Illustration of AWM offline .</p>
<p>Figure 4 :
4
Figure 4: Illustrations of AWM online .</p>
<p>Figure 5 :
5
Figure 5: AWM enables rapid learning from a small amount of data, i.e., about 40 queries, using WebArena map test split as an example.</p>
<p>FindFigure 6 :
6
Figure 6: AWM builds increasingly complex workflows over time, by learning from past examples and earlier workflows.</p>
<p>Figure 7 :
7
Figure 7: An example of dynamic environment changes that challenge workflow action utilization.However, workflow actions do not always lead to task success.A representative example is shown in Figure7.When booking flights, users often input a city name such as "New York," yet the system often pops up some nearby airports to support next-step search.While one can induce a book flight workflow that enters all required data via a pre-determined action sequence, the action to choose pop-up airports is executed without seeing the intermediate states with available pop-up options, and is not flexible enough to do so.More advanced techniques such as granting real-time state access or dynamic execution loops can be promising to solve this issue, and we encourage future work to leverage the AWM framework to explore these.</p>
<p>send msg to user('The distance between FROM LOCATION and TO LOCATION is DISTANCE and the estimated travel time is TIME.')Mind2Web Workflows We present one example workflow in each data domain in Mind2Web.# travel: enter flight locations Given that you are on the flight booking page, this workflow enters the departure and destination city/airport for your flight.[link] From Departure Airport or City Your Origin − &gt; CLICK [textbox] Origin City or Airport − &gt; TYPE: {your-origin-city} [link] {best-popup-option} − &gt; CLICK [link] To Destination Airport or City Your Destination − &gt; CLICK [textbox] Destination City or Airport − &gt; TYPE: {your-destination-city} [link] {best-popup-option} − &gt; CLICK # shopping: search and sort Given that you are on the Amazon search results page, this workflow searches for a product and sorts the results.[textbox] Search Amazon − &gt; TYPE: {search-term} [button] Go − &gt; CLICK [span] Sort by: − &gt; CLICK [option] {sort-option} − &gt; CLICK # entertainment: search and select Given that you are on the IMDb homepage, this workflow searches for a term and selects the best match.[textbox] Search IMDb − &gt; TYPE: {search-term} [button] Submit Search − &gt; CLICK [button] {best-match} − &gt; CLICK A.3 WORKFLOW QUALITY ANALYSIS To provide intermediate information beyond the end-to-end task success, we propose several metrics to verify the quality of the model-induced workflows.(1) Number of workflows: The number of workflows augmented to the memory, fewer workflows is better, whereas agents rely on fewer workflows to achieve satisfactory performance.(2) Coverage: How many steps in the action trajectory are covered by the workflows, higher coverage presumably signals the general applicability of the concerned workflow.(3) Function overlap: How much functionality overlap exists between workflows, we measure this by counting the number of overlapping sub-trajectories (≤ 2 steps) between each workflow pair for the same website.Less overlap indicates more maximized workflow management.(4) Utility rate: How often are workflows used by test examples.</p>
<p>, AWM first takes in all training examples from a website by concatenating them into a single prompt, and feeds them to the LM to create a set of workflows at 'training' time; I(E train ) → W offline .Second, AWM incorporates all induced workflows into the agent memory at inference time to solve test instructions L(q, M +W offline , o
test itest ) → a i</p>
<p>Table 2 :
2
Task success rate on the cross-template subset of WebArena, as well as the result breakdown on each website split.We mark the number of examples for each website split under the name.To confirm that the benefits of AWM are not just from learning workflows that help only within a template, and investigate whether AWM can obtain cross-template (≈cross-task) generalization, we extract a subset of WebArena examples sourcing from non-overlapping templates, by grouping examples by their templates and randomly choosing one example from each template group.We run AWM on this cross-template subset and examine if it achieves similar performance gains.
MethodTotal SRShopping CMS Reddit GitLab Maps (51) (45) (24) (45) (32)With human engineered workflows*SteP (Sodhi et al., 2023)32.126.529.352.227.336.4Autonomous agent onlyAutoEval (Pan et al., 2024)23.212.217.121.731.836.4BrowserGym ax −tree20.510.417.823.127.328.6AWM (OURS)33.224.529.352.231.839.4
As shown in Table2, AWM still achieves the highest performance, overall and on each website split.These results demonstrate that AWM induced workflows can effectively generalize across different tasks, i.e., examples instantiated from different task templates.</p>
<p>Table 4 :
4
Success rate on Mind2Web cross-task, cross-website, and cross-domain generalization test, using gpt-4 model.EA is short for element accuracy and AF 1 is short for action F 1 .notalwayscorrect,thus can lead to incorrect workflows that degrade model performance.On the other hand, the training and test examples on some websites vary in task distributions (e.g., training examples cover how to buy items on Amazon, test examples ask for job applications to Amazon careers.).AWM online naturally resolves this train-test gap because its operating process only involves test queries and environments, therefore yields workflows that are presumably more targeted toward the test distribution, which in turn, leads to higher success rates overall.Nonetheless, if distributionmatching, high-quality training examples are available, AWM offline could bring more benefit by alleviating the gap issue, as the slightly higher cross-tasks scores of AWM offline in Table4.Extending to unseen websites and domains When applied on unseen websites or domains, AWM online demonstrates greater generalization abilities, compared to AWM offline .The performance margin of AWM online (over AWM offline ) widens as the domain gaps between training and testing data widen from different websites (e.g., apple to bestbuy) to different domains (e.g., macys in shopping domain to reddit in social media domain).Because AWM online does not require nor rely on information from the training data, it is not affected by any domain gaps.Nonetheless, as demonstrated by the substantial improvements of AWM offline over the MindAct baseline, AWM offline still demonstrates that models can benefit from mechanistically similar workflows from the previously induced workflow repository.
MethodEACross-Task AF 1 Step SR SREACross-Website AF 1 Step SR SREACross-Domain AF 1 Step SR SRMindAct*41.6 60.636.22.0 35.8 51.130.12.0 21.6 52.818.61.0AWM offline 50.6 57.345.14.8 41.4 46.233.72.3 36.4 41.632.60.7AWM online50.0 56.443.64.0 42.1 45.133.91.6 40.9 46.335.51.7</p>
<p>Table 5 :
5
AWM success rate on WebArena using gpt-4, with rule-and lm-based induction.
MethodTotal SR # StepsAWM rule35.66.3AWM lm35.55.9</p>
<p>Table 6
6
, compared to AWM rule , AWM lm improves by a 2.8 margin.While augmenting concrete, full examples may bias agents to select elements similar to those presented in the given examples, AWM lm introduces less bias on element selection via its abstract representation of example-specific contexts in workflows.</p>
<p>Table 6 :
6
AWM results with different workflow induction methods on Mind2Web cross-task dataset.Further, AWM lm uses frequently-used sub-routines, which can be more flexibly and readily utilized across test examples, compared to the full example trajectories induced by AWM rule , which are less likely to appear multiple times.In general, our results indicate that the abstract, reusable nature of workflows contributes to the efficacy of AWM lm method.
MethodElem Acc Action F 1 Step SR SRMindAct 441.660.636.22.0AWM 4,rule49.557.043.42.0AWM 4,lm50.657.345.14.8</p>
<p>Table 7 :
7
Mind2Web cross-task results with AWM using code and text workflows.
MethodElem Acc Action F 1 Step SR SRMindAct41.660.636.22.0AWM50.657.345.14.8AWM text51.257.445.43.6</p>
<p>Table 8 :
8
Mind2Web results using GPT-3.5-turbowith different environment representations.
Desc. HTML Elem Acc Act F 1 Step SR SR✓✗39.052.834.62.8✗✓38.154.033.82.8✓✓37.151.332.92.0</p>
<p>Table 9 :
9
Mind2Web results with AWM AS variant that alters the action space besides memory augmentation.All methods use gpt-4.
MethodElem Acc Action F 1 Step SR SRMindAct41.660.636.22.0AWM50.657.345.14.8AWM AS51.856.746.43.6</p>
<p>We evaluate the workflows on WebArena test examples and Mind2Web cross-task test examples.We do not evaluate coverage on WebArena since it requires canonical trajectories, yet which are not available for WebArena.For Mind2Web, we do not evaluate on cross-website and cross-domain test examples since workflows induced from training examples do not have domain overlapping with these test examples, thus less applicable to them.</p>
<p>Table 10 :
10
Quality evaluation of model-induced workflows on Mind2Web dataset..94 of the test examples, indicating its wide applicability among varied tasks.Further, only 0.08 of the steps between workflows overlap, demonstrating the efficiency of workflows in solving respective tasks.Workflows on Mind2Web, although used similarly frequently as indicated by the high 0.91 utility rate, have slightly more functional overlap, and only achieve a 0.40 coverage over test examples.However, as the training examples used to induce workflows have substantial task distribution variances with the cross-task test examples, this relatively low coverage is reasonable.
Metric# Workflows Coverage Function Overlap Utility RateWebArena7.4-0.080.94Mind2Web7.30.400.200.91used by 0</p>
<p>Table 11 :
11
Success rate on Mind2Web cross-task, cross-website, and cross-domain generalization test, using gpt-4 model.EA is short for element accuracy and AF 1 is short for action F 1 .
MethodEACross-Task AF 1 Step SR SREACross-Website AF 1 Step SR SREACross-Domain AF 1 Step SR SRMindAct*41.6 60.636.22.0 35.8 51.130.12.0 21.6 52.818.61.0AWM offline50.6 57.345.14.8 41.4 46.233.72.3 36.4 41.632.60.7AWM online50.0 56.443.64.0 42.1 45.133.91.6 40.9 46.335.51.7AWM off +on 50.0 57.044.51.6 41.8 45.533.31.1 39.3 44.334.11.5
Memory is usually implemented as a system prompt or auxiliary information in the main prompt context.
We also explore a rule-based workflow induction method. See §B for more detailed experiments.
ACKNOWLEDGMENTSWe thank Frank Xu, Jiayi Pan, Vijay Viswanathan, Chenglei Si, and Jason Wu for their helpful discussions during the early stage of this project.Zora Zhiruo Wang is supported by the CMU Teng Family Presidential Fellowship.
Top-down synthesis for library learning. Matthew Bowers, Theo X Olausson, Lionel Gabriel Grand, Joshua B Tenenbaum, Kevin Ellis, Armando Solar-Lezama, 10.1145/3571234arXiv:2305.17126Large language models as tool makers. Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, Denny Zhou, jan 2023. 20237arXiv preprint</p>
<p>Categorization and representation of physics problems by experts and novices. T H Michelene, Paul J Chi, Robert Feltovich, Glaser, Cognitive science. 521981</p>
<p>The nature of expertise. T H Michelene, Robert Chi, Marshall J Glaser, Farr, 2014Psychology Press</p>
<p>Mind2web: Towards a generalist agent for the web. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, Yu Su, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>Alexandre Drouin, Maxime Gasse, Massimo Caccia, H Issam, Manuel Laradji, Tom Del Verme, Léo Marty, Megh Boisvert, Quentin Thakkar, David Cappart, Vazquez, arXiv:2403.07718How capable are web agents at solving common knowledge work tasks?. 2024arXiv preprint</p>
<p>Dreamcoder: growing generalizable, interpretable knowledge with wake-sleep bayesian program learning. Kevin Ellis, Lionel Wong, Maxwell Nye, Mathias Sable-Meyer, Luc Cary, Lore Anaya Pozo, Luke Hewitt, Armando Solar-Lezama, Joshua B Tenenbaum, Philosophical Transactions of the Royal Society A. 381202200502251. 2023</p>
<p>Autoguide: Automated generation and selection of state-aware guidelines for large language model agents. Yao Fu, Dong-Ki Kim, Jaekyeom Kim, Sungryull Sohn, Lajanugen Logeswaran, Kyunghoon Bae, Honglak Lee, arXiv:2403.089782024arXiv preprint</p>
<p>Gabriel Grand, Lionel Wong, Matthew Bowers, Theo X Olausson, Muxin Liu, Joshua B Tenenbaum, Jacob Andreas, Lilo, arXiv:2310.19791Learning interpretable libraries by compressing and documenting code. 2023arXiv preprint</p>
<p>Language models can teach themselves to program better. Patrick Haluptzok, Matthew Bowers, Adam Tauman, Kalai , The Eleventh International Conference on Learning Representations. 2023</p>
<p>Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, Daniel Fried, ICLR 2024 Workshop on Large Language Model (LLM) Agents. 2024</p>
<p>Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Reinforcement learning on web interfaces using workflow-guided exploration. Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Percy Liang, International Conference on Learning Representations. 2018</p>
<p>Clin: A continually learning language agent for rapid task adaptation and generalization. Prasad Bodhisattwa, Bhavana Majumder, Peter Dalvi Mishra, Oyvind Jansen, Niket Tafjord, Li Tandon, Chris Zhang, Peter Callison-Burch, Clark, arXiv:2310.101342023arXiv preprint</p>
<p>Learning reusable manipulation strategies. Jiayuan Mao, Tomás Lozano-Pérez, Joshua B Tenenbaum, Leslie Pack, Kaelbling , Conference on Robot Learning. PMLR2023</p>
<p>Bagel: Bootstrapping agents by guiding exploration with language. Shikhar Murty, Christopher Manning, Peter Shaw, Mandar Joshi, Kenton Lee, arXiv:2403.081402024arXiv preprint</p>
<p>Zero-shot task generalization with multi-task deep reinforcement learning. Junhyuk Oh, Satinder Singh, Honglak Lee, Pushmeet Kohli, International Conference on Machine Learning. PMLR2017</p>
<p>Autonomous evaluation and refinement of digital agents. Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, Alane Suhr, arXiv:2404.064742024arXiv preprint</p>
<p>Androidinthewild: A large-scale dataset for android device control. Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, Timothy P Lillicrap, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, arXiv:2405.14573A dynamic benchmarking environment for autonomous agents. 2024arXiv preprint</p>
<p>World of bits: An open-domain platform for web-based agents. Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, Percy Liang, Proceedings of the 34th International Conference on Machine Learning. Doina Precup, Yee Whye Teh, the 34th International Conference on Machine LearningPMLRAug 201770</p>
<p>Paloma Sodhi, Ryan Srk Branavan, Mcdonald, arXiv:2310.03720Hierarchical policies for web actions using llms. 2023arXiv preprint</p>
<p>Adaplanner: Adaptive planning from feedback with language models. Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, Chao Zhang, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, Transactions on Machine Learning Research. 2835-88562024a</p>
<p>What are tools anyway? a survey from the language model perspective. Zhiruo Wang, Zhoujun Cheng, Hao Zhu, Daniel Fried, Graham Neubig, First Conference on Language Modeling. 2024b</p>
<p>TroVE: Inducing verifiable and efficient toolboxes for solving programmatic tasks. Zhiruo Wang, Graham Neubig, Daniel Fried, Forty-first International Conference on Machine Learning. 2024c</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Ori Yoran, Samuel Joseph Amouyal, Chaitanya Malaviya, Ben Bogin, Ofir Press, Jonathan Berant, Assistantbench, arXiv:2407.15711Can web agents solve realistic and time-consuming tasks?. 2024arXiv preprint</p>
<p>Language to rewards for robotic skill synthesis. Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Lewis Hao-Tien, ; Chiang, arXiv:2306.08647Jan Humplik,. 2023Tom Erez, Leonard HasencleverarXiv preprint</p>
<p>Synapse: Trajectory-as-exemplar prompting with memory for computer control. Longtao Zheng, Rundong Wang, Xinrun Wang, Bo An, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Webarena: A realistic web environment for building autonomous agents. Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig, The Twelfth International Conference on Learning Representations. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
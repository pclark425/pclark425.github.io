<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8821 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8821</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8821</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-196621739</p>
                <p><strong>Paper Title:</strong> Tackling Graphical NLP problems with Graph Recurrent Networks</p>
                <p><strong>Paper Abstract:</strong> How to properly model graphs is a long-existing and important problem in NLP area, where several popular types of graphs are knowledge graphs, semantic graphs and dependency graphs. Comparing with other data structures, such as sequences and trees, graphs are generally more powerful in representing complex correlations among entities. For example, a knowledge graph stores real-word entities (such as"Barack_Obama"and"U.S.") and their relations (such as"live_in"and"lead_by"). Properly encoding a knowledge graph is beneficial to user applications, such as question answering and knowledge discovery. Modeling graphs is also very challenging, probably because graphs usually contain massive and cyclic relations. Recent years have witnessed the success of deep learning, especially RNN-based models, on many NLP problems. Besides, RNNs and their variations have been extensively studied on several graph problems and showed preliminary successes. Despite the successes that have been achieved, RNN-based models suffer from several major drawbacks on graphs. First, they can only consume sequential data, thus linearization is required to serialize input graphs, resulting in the loss of important structural information. Second, the serialization results are usually very long, so it takes a long time for RNNs to encode them. In this thesis, we propose a novel graph neural network, named graph recurrent network (GRN). We study our GRN model on 4 very different tasks, such as machine reading comprehension, relation extraction and machine translation. Some take undirected graphs without edge labels, while the others have directed ones with edge labels. To consider these important differences, we gradually enhance our GRN model, such as further considering edge labels and adding an RNN decoder. Carefully designed experiments show the effectiveness of GRN on all these tasks.</p>
                <p><strong>Cost:</strong> 0.032</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8821.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8821.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMR depth-first linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Depth-first traversal serialization of AMR (linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Serializes a directed, rooted AMR graph into a token sequence via depth-first traversal (often with bracket tokens to mark subgraphs), enabling use of standard sequence encoders (BiLSTM/LSTM) and sequence-to-sequence training for graph→text tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural AMR: Sequence-to-sequence models for parsing and generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Depth-first linearization (serialization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Perform a depth-first traversal of the labeled directed graph, outputting node labels and relation labels (and optionally bracket tokens) as a linear token stream; this serialized sequence is then encoded by a sequential encoder (BiLSTM/LSTM).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Abstract Meaning Representation (AMR) graphs (rooted, directed, labeled semantic graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Depth-first traversal of graph with token emission for nodes and edges; optional insertion of bracket tokens to mark subgraph boundaries (Konstas et al. style serialization).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation; used as input format for seq2seq models; also used in some AMR-parsing and NMT experiments as a serialized input.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in this paper and prior work: Konstas et al. (with very large auto-parsed data, ~20M) showed large BLEU improvements (>~5 BLEU over older statistical systems). In this dissertation, seq2seq (linearized-AMR) baselines are outperformed by graph-encoders: e.g., Graph2seq improves ≈1.6 BLEU (dev) over Seq2seq when no copy; with copy the graph encoder gap against the sequence baseline was larger (≈2.3 BLEU on dev), and final Graph2seq+charLSTM+copy reached BLEU=23.3 on LDC2015E86 test (paper's reported best); Konstas-style multi-million training gave much larger gains cited in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared directly to graph encoders (GRN/Graph2Seq) in this paper: linearization baseline underperforms direct graph encoders by ~1.6–3.4 BLEU depending on settings; Konstas et al.'s large-data linearization seq2seq performed very well with massive auto-parsed data, but when trained on the same gold data GRN-based Graph2seq outperforms it.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Allows immediate use of standard seq2seq machinery; simple to implement; benefits a lot from extremely large amounts of paired serialized graph–text data (Konstas et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Structural information is partially lost/obfuscated: nodes that are direct neighbors in the graph can become distant in the linearized sequence; order among children is artificial (many linearization permutations) increasing noise; inserting brackets only partially mitigates loss; serialization length can be large for big graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Per the paper: fails to preserve local graph relations for large graphs (example: nodes that are neighbors in AMR become far apart after linearization), increasing errors for concepts that require structural cues; even with brackets RNNs may not fully recover original graph connectivity, shown by GRN outperforming linearized baselines on same-size gold data.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8821.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8821.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bracketed serialization / explicit bracketing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bracketed string serialization of graphs (bracketing to indicate structure)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of serialization that inserts bracket tokens in the linearized token stream to indicate original nested graph structure (used in depth-first serializations to help sequential models infer graph structure).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural AMR: Sequence-to-sequence models for parsing and generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Bracketed serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>During depth-first linearization, emit explicit open/close bracket tokens (or other structural markers) around subgraphs/arguments so the resulting sequence carries explicit cues about nesting and original arity/attachments.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR / general directed semantic graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Depth-first traversal with additional bracket tokens inserted at concept/argument boundaries to mark subtree structure.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (seq2seq training), other graph→sequence tasks where encoding structure helps decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not separately quantified in this paper as a distinct experimental arm; discussed qualitatively — prior works that used bracketed linearization achieved good results only with very large training data (Konstas et al. with millions of auto-parsed pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>The paper argues that simply inserting brackets does not recover all lost structural information and that direct graph encoders (GRN/Graph2seq) achieve higher accuracy when trained on the same gold data.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Gives sequential models explicit structural hints; straightforward to apply to existing seq2seq systems.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Bracket insertion does not fully prevent information loss — some long-distance/structural relations remain hard for sequential LSTMs to learn; explosion of linear sequence length for large graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Uncertain recovery of original graph structure — authors report later work showing large improvements by modeling original graphs without linearization, implying bracketed serialization fails on many cases.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8821.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8821.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Split-into-DAGs (two-DAG split)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph splitting into two directed acyclic graphs (left-to-right and right-to-left) for DAG-LSTM encoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Convert a cyclic or general dependency graph into two DAGs by imposing a node order and separating edges by direction; encode each DAG with a directional DAG LSTM and combine representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cross-sentence n-ary relation extraction with graph LSTMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Two-DAG split (left/right directional DAGs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Pick a total node order over graph nodes; partition edges into those consistent with the chosen order (left→right) and the opposite (right→left), form two DAGs and encode each DAG separately with DAG-LSTM variants, then combine directional hidden states.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Dependency graphs (possibly cyclic and with discourse links); general syntactic/discourse graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Decide node order, split edges into two sets by orientation relative to that order; build two DAGs and run DAG LSTMs on each.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Cross-sentence n-ary relation extraction (biomedical drug–gene–mutation classification) and other tasks where DAG LSTMs have been applied.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In this paper's experiments (reimplementation baseline): Bidir DAG LSTM achieved e.g. ~77.3% (cross) on the ternary task (Table 4.3 baseline) while the GRN graph encoder obtained 83.2% (cross) — an improvement of ≈5.9 absolute points on the ternary relation extraction benchmark used here. Similar gains on binary and multiclass variants.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared with the GRN (graph-LSTM) encoder, the two-DAG split lost structural information and performed worse; compared to FULL/EMBED parametrizations (Peng et al.), results differ based on edge-label handling, but the split approach is shown to be inferior in retaining sibling and cyclic information.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Adapts recurrent/DAG RNN methods to graphs with cycles by converting into DAGs; builds on well-understood DAG LSTM machinery.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Splitting can destroy original graph substructures (e.g., breaks re-entrances and sibling relations), cannot simultaneously propagate information from both ancestors and descendants, and order choice introduces ambiguity; sibling context lost if not recovered.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Fails when sibling or other non-ancestor/descendant relations are crucial (examples in paper where indicators are leaves and cannot reach entity nodes after split), and when splitting severs short dependency paths (example: 'exon-19 of EGFR gene' loses path).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8821.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8821.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Convert-to-tree (split re-entrances) / Tree transducer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conversion of AMR graphs to trees by splitting re-entrances followed by tree-to-string transduction (Tree2Str)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transforms AMR graphs into trees by breaking re-entrant nodes (creating multiple copies), then applies tree transduction/string-generation methods (e.g., tree transducers) for text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generation from abstract meaning representation using tree transducers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph→Tree conversion (re-entrance splitting) + tree transducer</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Identify re-entrances (nodes with multiple parents) and split them to convert the graph into a tree; feed the tree to a tree-based transducer to generate target text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (graphs with re-entrancies/re-entrances)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Detect re-entrances and duplicate nodes to make a tree; apply a tree-to-string transducer to map tree fragments to surface strings.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (statistical tree-transducer based generation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Historical/statistical baselines (Tree2Str) performed competitively in earlier work; in this dissertation neural graph encoders (GRN→seq) outperform statistical Tree2Str on neural benchmarks when large silver data are available and especially when trained end-to-end with neural decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to seq2seq linearization and graph encoders: Tree2Str avoids some serialization issues but requires splitting and loses the original graph's single-node reentrancy semantics; graph encoders that keep original graph structure (GRN/Graph2seq) are reported here to perform better under comparable settings.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Allows tree-based transduction techniques and leverages tree-to-string models; can be effective for statistical approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Splitting re-entrances modifies the semantics (creates copies), possibly introduces redundancy or incorrectness for re-entrant structures; requires handcrafted transformations; less compatible with neural end-to-end modeling that preserves original graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When the original re-entrancy semantics are important (shared substructures), splitting can harm generation fidelity; statistical tree-based systems may underperform neural graph encoders on modern benchmarks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8821.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8821.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Synchronous node/hyperedge-replacement grammars</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synchronous hyperedge / node replacement grammar based graph→text methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Grammar-based approaches that jointly rewrite graph fragments and produce aligned text strings via synchronous graph grammars (hyperedge or node replacement), enabling structured graph→text translation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AMR-to-text generation with synchronous node replacement grammar</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Synchronous hyperedge/node replacement grammar</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Define a synchronous grammar that rewrites graph fragments (hyperedges or nodes) into target-side string fragments; generation proceeds by applying grammar rules that couple graph substructures to textual strings.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR / semantic graphs (graph fragments with labeled edges/nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Induce synchronous grammar rules that map graph fragments (hyperedges / nodes) to target string templates; perform derivations to generate text while consuming graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (graph→string generation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Prior synchronous grammar systems were competitive for earlier data sizes; in this paper neural Graph2seq outperforms many previous statistical grammar-based systems on BLEU when trained on the same gold data and/or with additional silver data (paper reports Graph2seq surpasses prior best by ~1.3 BLEU on the same dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to seq2seq and graph neural encoders: grammar methods explicitly use graph structure but are less end-to-end and require grammar extraction; neural graph encoders (GRN) achieve better performance and easier scalability with GPU parallelism in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Explicit and interpretable mapping between graph fragments and text; can capture non-local structural correspondences with grammar rules.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Grammar extraction and hand-crafting can be complex; less flexible than end-to-end neural models and can suffer when faced with diverse surface realizations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>May fail or generalize poorly when test graphs contain structures not covered by extracted grammar rules or when lexicalization patterns vary greatly across domains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8821.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8821.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Anonymization (placeholder substitution)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anonymization of open-class subgraphs into placeholders</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Replace subgraphs that correspond to open-class tokens (dates, numbers, named entities) with predefined placeholders before training/decoding and recover original surface forms after generation to mitigate data sparsity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural AMR: Sequence-to-sequence models for parsing and generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Anonymization / placeholder substitution</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Detect subgraphs encoding entities/dates/numerics and replace them with canonical placeholders (e.g., person_name_0, num_0) during training/decoding; post-process to re-insert original tokens mapped from the input.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (subgraphs representing named entities, quantities, dates, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Apply rule-based mapping from identified subgraphs to placeholder tokens in the serialized input/output; maintain mapping to recover originals after generation.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (reduces open-vocab sparsity in seq2seq training).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In this paper anonymization (Graph2seq+Anon) gives comparable gains to the copy mechanism in some settings; Graph2seq+Anon performance is similar to Graph2seq+copy on dev set (paper states anonymization gives comparable overall performance gains), though exact numbers vary by configuration. Konstas et al.'s anonymization was central to their strong seq2seq results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to copy mechanism: anonymization relies on manual rules and mapping; copy is learned and domain-adaptive. In experiments, anonymization and copy give comparable BLEU gains, but copy mechanism favored for automation and adaptability.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Reduces sparsity from rare tokens, makes generation easier for seq2seq models, improves robustness when rules are correct.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires handcrafted rules to identify and map subgraphs → placeholders and to restore originals; brittle to unseen entity types; labor-intensive to generalize across domains/languages.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Fails when placeholder rules do not cover certain open-class constructs or when the mapping from subgraph to surface form is ambiguous; manual rules are expensive to maintain.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8821.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8821.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Copy / pointer mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attention-based copy (pointer-generator) mechanism integrating attention into vocabulary distribution</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Learns to either generate words from a fixed vocabulary or copy tokens (concepts / surface tokens) directly from the input graph via attention-weighted copying, addressing data sparsity for open-class tokens without manual anonymization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Incorporating copying mechanism in sequence-to-sequence learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Copy / pointer-generator mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>At each decoder step compute a generation distribution over vocab and an attention-based copy distribution over input nodes/tokens; a learned gate (θ) interpolates between generate vs copy to produce the final token distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (nodes corresponding to surface token concepts), general graph→sequence inputs where many concepts appear verbatim in the source</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No explicit conversion; copy integrates the attention distribution over graph nodes (or serialized tokens) into output probability by summing attention mass for identical target tokens and interpolating with generation probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (primary), improves handling of numbers, names, rare tokens; used also in other graph→text and seq2seq tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports that adding copy to Graph2seq improved BLEU substantially (Graph2seq+copy vs Seq2seq+copy: ≈+2.3 BLEU on dev); final Graph2seq+charLSTM+copy achieved BLEU=23.3 on the AMR testset; with additional gigaword data: 200K→ BLEU 28.2, 2M→ BLEU 33.0 (paper's reported numbers for Graph2seq+charLSTM+copy).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared with anonymization: copy is automated, learns what to copy, easier to adapt to new domains; anonymization is rule-based and comparable in some experiments but less flexible. Copy mechanism enabled better open-class token generation without handcrafted placeholders.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Automatically handles out-of-vocabulary and rare tokens by copying; avoids manual anonymization rules; integrates naturally with attention-based decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Bias toward copying tokens that appear in input — may produce inappropriate literal copies when paraphrase is required; relies on good attention alignment to work well.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Can over-copy (produce input tokens verbatim when a lexicalized paraphrase is expected) or copy incorrect tokens when attention is diffuse; performance depends on quality of attention distribution.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8821.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8821.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph2Seq (GRN encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-to-sequence using Graph Recurrent Network (GRN) encoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A direct graph encoder (GRN) that preserves original graph structure: assigns a hidden state to each node and performs iterative message-passing (LSTM-based gated recurrent updates) across edges to produce contextual node states which feed an attention-based seq decoder; avoids linearization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph-to-sequence with Graph Recurrent Network (GRN)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Treat the whole graph as a state composed of node states; iterate T recurrent LSTM-based message-passing steps where each node aggregates messages from incoming/outgoing neighbors (and edge-label inputs), updates its hidden & cell states via LSTM gates; use final node states as attention memory for an LSTM decoder with optional copy mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (directed, edge-labeled), dependency graphs, evidence graphs (constructed mention graphs for MHRC), general arbitrary graphs (including cyclic ones)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No graph→text conversion prior to encoding; instead the graph is encoded directly via iterative gated message passing (GRN). The decoder is a standard sequence LSTM with attention (and optional copy) to produce text.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation, semantic NMT (AMR-enhanced NMT with doubly-attentive decoder), multi-hop reading comprehension (evidence integration), cross-sentence n-ary relation extraction (graph classification).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>AMR-to-text: Graph2seq+charLSTM+copy BLEU=23.3 on LDC2015E86 test (≈+1.3 BLEU over Konstas' MSeq2seq+Anon on same gold data); with Gigaword: +0.7–0.8 BLEU gains over Konstas using same extra data (2M → BLEU=33.0 in paper). NMT: Dual2seq (graph + seq encoders) improved BLEU from Seq2seq 16.0→19.2 (NC-v11 small set) and 23.7→25.5 on full WMT16 set. MHRC: evidence-graph GRN (MHQA-GRN) reached 62.8% dev and 65.4% held-out test on WikiHop (paper reports), outperforming baselines (Local & Coref LSTM). Relation extraction: GRN outperforms bidirectional DAG-LSTM by ~5.9 absolute points (ternary cross set: Bidir DAG LSTM ≈77.3 → GRN ≈83.2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Direct comparisons in the paper: GRN / Graph2seq consistently outperforms sequence-serialization baselines (Depth-first serialization + seq2seq) and DAG-splitting LSTM baselines; on AMR-to-text Graph2seq > Seq2seq; on relation extraction GRN > bidirectional DAG LSTM; on MHRC GRN over coreference-only DAG approaches. GRN is contrasted with GCN/GGNN and argued to be related (message-passing family) but with gated LSTM updating.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Preserves original graph structure (no destructive serialization or splitting); naturally handles arbitrary graphs including cycles; sibling and multi-hop relations integrated via message passing; parallelizable node updates (efficient on GPU); empirically superior on multiple downstream tasks; integrates edge labels and directions.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>High memory usage because states for all nodes across T steps are maintained (scales with graph size and density); choice of number of recurrent transition steps (T) is a hyperparameter and can require tuning; dense graphs may introduce noise via many edges; training on very large graphs requires sampling or importance sampling to reduce memory/computation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Dense, noisy fully-connected graphs degrade performance (paper notes Fully-Connect-GRN underperforms targeted evidence graphs); increasing T beyond a point can introduce noise in richly connected graphs and slightly reduce performance; memory limits for large-scale graphs necessitate sampling approaches — naive GRN runs out of memory on very large graphs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural AMR: Sequence-to-sequence models for parsing and generation <em>(Rating: 2)</em></li>
                <li>Generation from abstract meaning representation using tree transducers <em>(Rating: 2)</em></li>
                <li>Generating English from abstract meaning representations <em>(Rating: 2)</em></li>
                <li>AMR-to-text generation with synchronous node replacement grammar <em>(Rating: 2)</em></li>
                <li>Cross-sentence n-ary relation extraction with graph LSTMs <em>(Rating: 2)</em></li>
                <li>Neural models for reasoning over multiple mentions using coreference <em>(Rating: 1)</em></li>
                <li>Incorporating copying mechanism in sequence-to-sequence learning <em>(Rating: 2)</em></li>
                <li>Graph-to-sequence learning using gated graph neural networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8821",
    "paper_id": "paper-196621739",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "AMR depth-first linearization",
            "name_full": "Depth-first traversal serialization of AMR (linearization)",
            "brief_description": "Serializes a directed, rooted AMR graph into a token sequence via depth-first traversal (often with bracket tokens to mark subgraphs), enabling use of standard sequence encoders (BiLSTM/LSTM) and sequence-to-sequence training for graph→text tasks.",
            "citation_title": "Neural AMR: Sequence-to-sequence models for parsing and generation",
            "mention_or_use": "use",
            "representation_name": "Depth-first linearization (serialization)",
            "representation_description": "Perform a depth-first traversal of the labeled directed graph, outputting node labels and relation labels (and optionally bracket tokens) as a linear token stream; this serialized sequence is then encoded by a sequential encoder (BiLSTM/LSTM).",
            "graph_type": "Abstract Meaning Representation (AMR) graphs (rooted, directed, labeled semantic graphs)",
            "conversion_method": "Depth-first traversal of graph with token emission for nodes and edges; optional insertion of bracket tokens to mark subgraph boundaries (Konstas et al. style serialization).",
            "downstream_task": "AMR-to-text generation; used as input format for seq2seq models; also used in some AMR-parsing and NMT experiments as a serialized input.",
            "performance_metrics": "Reported in this paper and prior work: Konstas et al. (with very large auto-parsed data, ~20M) showed large BLEU improvements (&gt;~5 BLEU over older statistical systems). In this dissertation, seq2seq (linearized-AMR) baselines are outperformed by graph-encoders: e.g., Graph2seq improves ≈1.6 BLEU (dev) over Seq2seq when no copy; with copy the graph encoder gap against the sequence baseline was larger (≈2.3 BLEU on dev), and final Graph2seq+charLSTM+copy reached BLEU=23.3 on LDC2015E86 test (paper's reported best); Konstas-style multi-million training gave much larger gains cited in related work.",
            "comparison_to_others": "Compared directly to graph encoders (GRN/Graph2Seq) in this paper: linearization baseline underperforms direct graph encoders by ~1.6–3.4 BLEU depending on settings; Konstas et al.'s large-data linearization seq2seq performed very well with massive auto-parsed data, but when trained on the same gold data GRN-based Graph2seq outperforms it.",
            "advantages": "Allows immediate use of standard seq2seq machinery; simple to implement; benefits a lot from extremely large amounts of paired serialized graph–text data (Konstas et al.).",
            "disadvantages": "Structural information is partially lost/obfuscated: nodes that are direct neighbors in the graph can become distant in the linearized sequence; order among children is artificial (many linearization permutations) increasing noise; inserting brackets only partially mitigates loss; serialization length can be large for big graphs.",
            "failure_cases": "Per the paper: fails to preserve local graph relations for large graphs (example: nodes that are neighbors in AMR become far apart after linearization), increasing errors for concepts that require structural cues; even with brackets RNNs may not fully recover original graph connectivity, shown by GRN outperforming linearized baselines on same-size gold data.",
            "uuid": "e8821.0"
        },
        {
            "name_short": "Bracketed serialization / explicit bracketing",
            "name_full": "Bracketed string serialization of graphs (bracketing to indicate structure)",
            "brief_description": "A variant of serialization that inserts bracket tokens in the linearized token stream to indicate original nested graph structure (used in depth-first serializations to help sequential models infer graph structure).",
            "citation_title": "Neural AMR: Sequence-to-sequence models for parsing and generation",
            "mention_or_use": "mention",
            "representation_name": "Bracketed serialization",
            "representation_description": "During depth-first linearization, emit explicit open/close bracket tokens (or other structural markers) around subgraphs/arguments so the resulting sequence carries explicit cues about nesting and original arity/attachments.",
            "graph_type": "AMR / general directed semantic graphs",
            "conversion_method": "Depth-first traversal with additional bracket tokens inserted at concept/argument boundaries to mark subtree structure.",
            "downstream_task": "AMR-to-text generation (seq2seq training), other graph→sequence tasks where encoding structure helps decoding.",
            "performance_metrics": "Not separately quantified in this paper as a distinct experimental arm; discussed qualitatively — prior works that used bracketed linearization achieved good results only with very large training data (Konstas et al. with millions of auto-parsed pairs).",
            "comparison_to_others": "The paper argues that simply inserting brackets does not recover all lost structural information and that direct graph encoders (GRN/Graph2seq) achieve higher accuracy when trained on the same gold data.",
            "advantages": "Gives sequential models explicit structural hints; straightforward to apply to existing seq2seq systems.",
            "disadvantages": "Bracket insertion does not fully prevent information loss — some long-distance/structural relations remain hard for sequential LSTMs to learn; explosion of linear sequence length for large graphs.",
            "failure_cases": "Uncertain recovery of original graph structure — authors report later work showing large improvements by modeling original graphs without linearization, implying bracketed serialization fails on many cases.",
            "uuid": "e8821.1"
        },
        {
            "name_short": "Split-into-DAGs (two-DAG split)",
            "name_full": "Graph splitting into two directed acyclic graphs (left-to-right and right-to-left) for DAG-LSTM encoding",
            "brief_description": "Convert a cyclic or general dependency graph into two DAGs by imposing a node order and separating edges by direction; encode each DAG with a directional DAG LSTM and combine representations.",
            "citation_title": "Cross-sentence n-ary relation extraction with graph LSTMs",
            "mention_or_use": "use",
            "representation_name": "Two-DAG split (left/right directional DAGs)",
            "representation_description": "Pick a total node order over graph nodes; partition edges into those consistent with the chosen order (left→right) and the opposite (right→left), form two DAGs and encode each DAG separately with DAG-LSTM variants, then combine directional hidden states.",
            "graph_type": "Dependency graphs (possibly cyclic and with discourse links); general syntactic/discourse graphs",
            "conversion_method": "Decide node order, split edges into two sets by orientation relative to that order; build two DAGs and run DAG LSTMs on each.",
            "downstream_task": "Cross-sentence n-ary relation extraction (biomedical drug–gene–mutation classification) and other tasks where DAG LSTMs have been applied.",
            "performance_metrics": "In this paper's experiments (reimplementation baseline): Bidir DAG LSTM achieved e.g. ~77.3% (cross) on the ternary task (Table 4.3 baseline) while the GRN graph encoder obtained 83.2% (cross) — an improvement of ≈5.9 absolute points on the ternary relation extraction benchmark used here. Similar gains on binary and multiclass variants.",
            "comparison_to_others": "Compared with the GRN (graph-LSTM) encoder, the two-DAG split lost structural information and performed worse; compared to FULL/EMBED parametrizations (Peng et al.), results differ based on edge-label handling, but the split approach is shown to be inferior in retaining sibling and cyclic information.",
            "advantages": "Adapts recurrent/DAG RNN methods to graphs with cycles by converting into DAGs; builds on well-understood DAG LSTM machinery.",
            "disadvantages": "Splitting can destroy original graph substructures (e.g., breaks re-entrances and sibling relations), cannot simultaneously propagate information from both ancestors and descendants, and order choice introduces ambiguity; sibling context lost if not recovered.",
            "failure_cases": "Fails when sibling or other non-ancestor/descendant relations are crucial (examples in paper where indicators are leaves and cannot reach entity nodes after split), and when splitting severs short dependency paths (example: 'exon-19 of EGFR gene' loses path).",
            "uuid": "e8821.2"
        },
        {
            "name_short": "Convert-to-tree (split re-entrances) / Tree transducer",
            "name_full": "Conversion of AMR graphs to trees by splitting re-entrances followed by tree-to-string transduction (Tree2Str)",
            "brief_description": "Transforms AMR graphs into trees by breaking re-entrant nodes (creating multiple copies), then applies tree transduction/string-generation methods (e.g., tree transducers) for text generation.",
            "citation_title": "Generation from abstract meaning representation using tree transducers",
            "mention_or_use": "mention",
            "representation_name": "Graph→Tree conversion (re-entrance splitting) + tree transducer",
            "representation_description": "Identify re-entrances (nodes with multiple parents) and split them to convert the graph into a tree; feed the tree to a tree-based transducer to generate target text.",
            "graph_type": "AMR graphs (graphs with re-entrancies/re-entrances)",
            "conversion_method": "Detect re-entrances and duplicate nodes to make a tree; apply a tree-to-string transducer to map tree fragments to surface strings.",
            "downstream_task": "AMR-to-text generation (statistical tree-transducer based generation).",
            "performance_metrics": "Historical/statistical baselines (Tree2Str) performed competitively in earlier work; in this dissertation neural graph encoders (GRN→seq) outperform statistical Tree2Str on neural benchmarks when large silver data are available and especially when trained end-to-end with neural decoders.",
            "comparison_to_others": "Compared to seq2seq linearization and graph encoders: Tree2Str avoids some serialization issues but requires splitting and loses the original graph's single-node reentrancy semantics; graph encoders that keep original graph structure (GRN/Graph2seq) are reported here to perform better under comparable settings.",
            "advantages": "Allows tree-based transduction techniques and leverages tree-to-string models; can be effective for statistical approaches.",
            "disadvantages": "Splitting re-entrances modifies the semantics (creates copies), possibly introduces redundancy or incorrectness for re-entrant structures; requires handcrafted transformations; less compatible with neural end-to-end modeling that preserves original graph structure.",
            "failure_cases": "When the original re-entrancy semantics are important (shared substructures), splitting can harm generation fidelity; statistical tree-based systems may underperform neural graph encoders on modern benchmarks.",
            "uuid": "e8821.3"
        },
        {
            "name_short": "Synchronous node/hyperedge-replacement grammars",
            "name_full": "Synchronous hyperedge / node replacement grammar based graph→text methods",
            "brief_description": "Grammar-based approaches that jointly rewrite graph fragments and produce aligned text strings via synchronous graph grammars (hyperedge or node replacement), enabling structured graph→text translation.",
            "citation_title": "AMR-to-text generation with synchronous node replacement grammar",
            "mention_or_use": "mention",
            "representation_name": "Synchronous hyperedge/node replacement grammar",
            "representation_description": "Define a synchronous grammar that rewrites graph fragments (hyperedges or nodes) into target-side string fragments; generation proceeds by applying grammar rules that couple graph substructures to textual strings.",
            "graph_type": "AMR / semantic graphs (graph fragments with labeled edges/nodes)",
            "conversion_method": "Induce synchronous grammar rules that map graph fragments (hyperedges / nodes) to target string templates; perform derivations to generate text while consuming graph structure.",
            "downstream_task": "AMR-to-text generation (graph→string generation).",
            "performance_metrics": "Prior synchronous grammar systems were competitive for earlier data sizes; in this paper neural Graph2seq outperforms many previous statistical grammar-based systems on BLEU when trained on the same gold data and/or with additional silver data (paper reports Graph2seq surpasses prior best by ~1.3 BLEU on the same dataset).",
            "comparison_to_others": "Compared to seq2seq and graph neural encoders: grammar methods explicitly use graph structure but are less end-to-end and require grammar extraction; neural graph encoders (GRN) achieve better performance and easier scalability with GPU parallelism in this paper's experiments.",
            "advantages": "Explicit and interpretable mapping between graph fragments and text; can capture non-local structural correspondences with grammar rules.",
            "disadvantages": "Grammar extraction and hand-crafting can be complex; less flexible than end-to-end neural models and can suffer when faced with diverse surface realizations.",
            "failure_cases": "May fail or generalize poorly when test graphs contain structures not covered by extracted grammar rules or when lexicalization patterns vary greatly across domains.",
            "uuid": "e8821.4"
        },
        {
            "name_short": "Anonymization (placeholder substitution)",
            "name_full": "Anonymization of open-class subgraphs into placeholders",
            "brief_description": "Replace subgraphs that correspond to open-class tokens (dates, numbers, named entities) with predefined placeholders before training/decoding and recover original surface forms after generation to mitigate data sparsity.",
            "citation_title": "Neural AMR: Sequence-to-sequence models for parsing and generation",
            "mention_or_use": "use",
            "representation_name": "Anonymization / placeholder substitution",
            "representation_description": "Detect subgraphs encoding entities/dates/numerics and replace them with canonical placeholders (e.g., person_name_0, num_0) during training/decoding; post-process to re-insert original tokens mapped from the input.",
            "graph_type": "AMR graphs (subgraphs representing named entities, quantities, dates, etc.)",
            "conversion_method": "Apply rule-based mapping from identified subgraphs to placeholder tokens in the serialized input/output; maintain mapping to recover originals after generation.",
            "downstream_task": "AMR-to-text generation (reduces open-vocab sparsity in seq2seq training).",
            "performance_metrics": "In this paper anonymization (Graph2seq+Anon) gives comparable gains to the copy mechanism in some settings; Graph2seq+Anon performance is similar to Graph2seq+copy on dev set (paper states anonymization gives comparable overall performance gains), though exact numbers vary by configuration. Konstas et al.'s anonymization was central to their strong seq2seq results.",
            "comparison_to_others": "Compared to copy mechanism: anonymization relies on manual rules and mapping; copy is learned and domain-adaptive. In experiments, anonymization and copy give comparable BLEU gains, but copy mechanism favored for automation and adaptability.",
            "advantages": "Reduces sparsity from rare tokens, makes generation easier for seq2seq models, improves robustness when rules are correct.",
            "disadvantages": "Requires handcrafted rules to identify and map subgraphs → placeholders and to restore originals; brittle to unseen entity types; labor-intensive to generalize across domains/languages.",
            "failure_cases": "Fails when placeholder rules do not cover certain open-class constructs or when the mapping from subgraph to surface form is ambiguous; manual rules are expensive to maintain.",
            "uuid": "e8821.5"
        },
        {
            "name_short": "Copy / pointer mechanism",
            "name_full": "Attention-based copy (pointer-generator) mechanism integrating attention into vocabulary distribution",
            "brief_description": "Learns to either generate words from a fixed vocabulary or copy tokens (concepts / surface tokens) directly from the input graph via attention-weighted copying, addressing data sparsity for open-class tokens without manual anonymization.",
            "citation_title": "Incorporating copying mechanism in sequence-to-sequence learning",
            "mention_or_use": "use",
            "representation_name": "Copy / pointer-generator mechanism",
            "representation_description": "At each decoder step compute a generation distribution over vocab and an attention-based copy distribution over input nodes/tokens; a learned gate (θ) interpolates between generate vs copy to produce the final token distribution.",
            "graph_type": "AMR graphs (nodes corresponding to surface token concepts), general graph→sequence inputs where many concepts appear verbatim in the source",
            "conversion_method": "No explicit conversion; copy integrates the attention distribution over graph nodes (or serialized tokens) into output probability by summing attention mass for identical target tokens and interpolating with generation probabilities.",
            "downstream_task": "AMR-to-text generation (primary), improves handling of numbers, names, rare tokens; used also in other graph→text and seq2seq tasks.",
            "performance_metrics": "Paper reports that adding copy to Graph2seq improved BLEU substantially (Graph2seq+copy vs Seq2seq+copy: ≈+2.3 BLEU on dev); final Graph2seq+charLSTM+copy achieved BLEU=23.3 on the AMR testset; with additional gigaword data: 200K→ BLEU 28.2, 2M→ BLEU 33.0 (paper's reported numbers for Graph2seq+charLSTM+copy).",
            "comparison_to_others": "Compared with anonymization: copy is automated, learns what to copy, easier to adapt to new domains; anonymization is rule-based and comparable in some experiments but less flexible. Copy mechanism enabled better open-class token generation without handcrafted placeholders.",
            "advantages": "Automatically handles out-of-vocabulary and rare tokens by copying; avoids manual anonymization rules; integrates naturally with attention-based decoders.",
            "disadvantages": "Bias toward copying tokens that appear in input — may produce inappropriate literal copies when paraphrase is required; relies on good attention alignment to work well.",
            "failure_cases": "Can over-copy (produce input tokens verbatim when a lexicalized paraphrase is expected) or copy incorrect tokens when attention is diffuse; performance depends on quality of attention distribution.",
            "uuid": "e8821.6"
        },
        {
            "name_short": "Graph2Seq (GRN encoder)",
            "name_full": "Graph-to-sequence using Graph Recurrent Network (GRN) encoder",
            "brief_description": "A direct graph encoder (GRN) that preserves original graph structure: assigns a hidden state to each node and performs iterative message-passing (LSTM-based gated recurrent updates) across edges to produce contextual node states which feed an attention-based seq decoder; avoids linearization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Graph-to-sequence with Graph Recurrent Network (GRN)",
            "representation_description": "Treat the whole graph as a state composed of node states; iterate T recurrent LSTM-based message-passing steps where each node aggregates messages from incoming/outgoing neighbors (and edge-label inputs), updates its hidden & cell states via LSTM gates; use final node states as attention memory for an LSTM decoder with optional copy mechanism.",
            "graph_type": "AMR graphs (directed, edge-labeled), dependency graphs, evidence graphs (constructed mention graphs for MHRC), general arbitrary graphs (including cyclic ones)",
            "conversion_method": "No graph→text conversion prior to encoding; instead the graph is encoded directly via iterative gated message passing (GRN). The decoder is a standard sequence LSTM with attention (and optional copy) to produce text.",
            "downstream_task": "AMR-to-text generation, semantic NMT (AMR-enhanced NMT with doubly-attentive decoder), multi-hop reading comprehension (evidence integration), cross-sentence n-ary relation extraction (graph classification).",
            "performance_metrics": "AMR-to-text: Graph2seq+charLSTM+copy BLEU=23.3 on LDC2015E86 test (≈+1.3 BLEU over Konstas' MSeq2seq+Anon on same gold data); with Gigaword: +0.7–0.8 BLEU gains over Konstas using same extra data (2M → BLEU=33.0 in paper). NMT: Dual2seq (graph + seq encoders) improved BLEU from Seq2seq 16.0→19.2 (NC-v11 small set) and 23.7→25.5 on full WMT16 set. MHRC: evidence-graph GRN (MHQA-GRN) reached 62.8% dev and 65.4% held-out test on WikiHop (paper reports), outperforming baselines (Local & Coref LSTM). Relation extraction: GRN outperforms bidirectional DAG-LSTM by ~5.9 absolute points (ternary cross set: Bidir DAG LSTM ≈77.3 → GRN ≈83.2).",
            "comparison_to_others": "Direct comparisons in the paper: GRN / Graph2seq consistently outperforms sequence-serialization baselines (Depth-first serialization + seq2seq) and DAG-splitting LSTM baselines; on AMR-to-text Graph2seq &gt; Seq2seq; on relation extraction GRN &gt; bidirectional DAG LSTM; on MHRC GRN over coreference-only DAG approaches. GRN is contrasted with GCN/GGNN and argued to be related (message-passing family) but with gated LSTM updating.",
            "advantages": "Preserves original graph structure (no destructive serialization or splitting); naturally handles arbitrary graphs including cycles; sibling and multi-hop relations integrated via message passing; parallelizable node updates (efficient on GPU); empirically superior on multiple downstream tasks; integrates edge labels and directions.",
            "disadvantages": "High memory usage because states for all nodes across T steps are maintained (scales with graph size and density); choice of number of recurrent transition steps (T) is a hyperparameter and can require tuning; dense graphs may introduce noise via many edges; training on very large graphs requires sampling or importance sampling to reduce memory/computation.",
            "failure_cases": "Dense, noisy fully-connected graphs degrade performance (paper notes Fully-Connect-GRN underperforms targeted evidence graphs); increasing T beyond a point can introduce noise in richly connected graphs and slightly reduce performance; memory limits for large-scale graphs necessitate sampling approaches — naive GRN runs out of memory on very large graphs.",
            "uuid": "e8821.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural AMR: Sequence-to-sequence models for parsing and generation",
            "rating": 2,
            "sanitized_title": "neural_amr_sequencetosequence_models_for_parsing_and_generation"
        },
        {
            "paper_title": "Generation from abstract meaning representation using tree transducers",
            "rating": 2,
            "sanitized_title": "generation_from_abstract_meaning_representation_using_tree_transducers"
        },
        {
            "paper_title": "Generating English from abstract meaning representations",
            "rating": 2,
            "sanitized_title": "generating_english_from_abstract_meaning_representations"
        },
        {
            "paper_title": "AMR-to-text generation with synchronous node replacement grammar",
            "rating": 2,
            "sanitized_title": "amrtotext_generation_with_synchronous_node_replacement_grammar"
        },
        {
            "paper_title": "Cross-sentence n-ary relation extraction with graph LSTMs",
            "rating": 2,
            "sanitized_title": "crosssentence_nary_relation_extraction_with_graph_lstms"
        },
        {
            "paper_title": "Neural models for reasoning over multiple mentions using coreference",
            "rating": 1,
            "sanitized_title": "neural_models_for_reasoning_over_multiple_mentions_using_coreference"
        },
        {
            "paper_title": "Incorporating copying mechanism in sequence-to-sequence learning",
            "rating": 2,
            "sanitized_title": "incorporating_copying_mechanism_in_sequencetosequence_learning"
        },
        {
            "paper_title": "Graph-to-sequence learning using gated graph neural networks",
            "rating": 1,
            "sanitized_title": "graphtosequence_learning_using_gated_graph_neural_networks"
        }
    ],
    "cost": 0.031681749999999995,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Tackling Graphical NLP problems with Graph Recurrent Networks
2019</p>
<p>Linfeng Song 
Department of Computer Science Arts
Sciences &amp; Engineering Edmund A. Hajim School of Engineering &amp; Applied Sciences University of Rochester Rochester
New York</p>
<p>Tackling Graphical NLP problems with Graph Recurrent Networks
2019</p>
<p>6.3 TEST performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant (Koehn, 2004) </p>
<p>Graph problems in NLP</p>
<p>There are plenty of graph problems in the natural language processing area.</p>
<p>These graphs include semantic graphs, dependency graphs, knowledge graphs and so on. does what to whom) of a given sentence by abstracting the sentence into several concepts (such as "describe-01" and "person") and their relations (such as ":ARG0" and ":ARG1"). On the other hand, a dependency graph (Figure 1.1b) simply captures word-to-word dependencies, such as "Bell" being the subject (subj) of "makes". Finally, a knowledge graph (Figure 1.1c) represents real-world knowledge by entities (such as "U.S. Government" and "Barack Obama") and their relations (such as "belong to" and "born in").</p>
<p>Since there is massive information in the world level, a knowledge graph (such as Freebase 1 and DBPedia 2 ) are very large. amod (products-16, building-15) dobj (makes-8, products-16) dobj (distributes-10, products-16) These dependencies map straightforwardly onto a directed graph representation, in which words the sentence are nodes in the graph and grammatical relations are edge labels. Figure 1 gives the gra representation for the example sentence above.</p>
<p>Document overview: This manual provides documentation for the set of dependencies defined f English. There is also a Stanford Dependency representation available for Chinese, but it is not furth discussed here. Starting in 2014, there has been work to extend Stanford Dependencies to be genera applicable cross-linguistically. Initial work appeared in de Marneffe et al. (2014), and the current guid lines for Universal Dependencies (UD) can be found at http://www.universaldependencies.org. F SD, Section 2 of the manual defines the grammatical relations and the taxonomic hierarchy over the appears in section 3. This is then followed by a description of the several variant dependency repr sentations available, aimed at different use cases (section 4), some details of the software available f generating Stanford Dependencies (section 5), and references to further discussion and use of the S representation (section 6).</p>
<p>Definitions of the Stanford typed dependencies</p>
<p>The current representation contains approximately 50 grammatical relations (depending slightly on t options discussed in section 4). The dependencies are all binary relations: a grammatical relation hol between a governor (also known as a regent or a head) and a dependent. The grammatical relations a defined below, in alphabetical order according to the dependency's abbreviated name (which appears  </p>
<p>Previous approaches for modeling graphs</p>
<p>How to properly model these graphs has been a long-standing and important topic, as this directly contributes to natural language understanding, one of the most important key problems in NLP. Previously, statistical or rule-based approaches have been introduced to model graphs. For instance, synchronous grammar-based methods have been proposed to model semantic graphs (Jones et al., 2012a;Flanigan et al., 2016b;Song et al., 2017) and dependency graphs (Xie et al., 2011;Meng et al., 2013) for machine translation and text generation. For modeling knowledge graphs, very different approaches have been adopted, probably due to the fact that their scale is too large. One popular method is random walk, which has been investigated for knowledge base completion (Lao et al., 2011) and entity linking (Han et al., 2011).</p>
<p>Recently, research on analyzing graphs with deep learning models has been receiving more and more attention. This is because they have demonstrated strong learning power and other superior properties, i.e. not needing feature engineering and benefiting from large-scale data. To date, people have studied several types of neural networks.</p>
<p>CNN One group of models (Defferrard et al., 2016;Niepert et al., 2016;Duvenaud et al., 2015;Henaff et al., 2015) adopt convolutional neural networks (CNN) (LeCun et al., 1995) for encoding graphs. As shown in Figure 1.2, these models adopts multiple convolution layers, each capturing the local correspondences within n-gram windows. By stacking the layers, more global correspondences can be captured. One drawback is the large amount of computation, because CNNs calculate features by enumerating all n-gram windows, and there can be a large number of n-gram (n &gt;1) windows for a very dense graph. In particular, each node in a complete graph of N nodes has N left and N right RNN Another direction is applying RNNs on linearized graphs (Konstas et al., 2017;Li et al., 2017) based on depth-first traversal algorithms. Usually a bidirectional RNN is adopted to capture global dependencies within the whole graph. Comparing with CNNs, the computations of a RNN is only linear in terms of graph scale. However, one drawback of this direction is that some structural information is lost after linerization. ets into their linearization results to indicate the original structure, and they hope RNNs can figure that out with the aid of brackets. But it is still uncertain this method can recover all the information loss. Later work (Song et al., 2018d(Song et al., , 2019 show large improvements by directly modeling original graphs without linearization. This indicates that simply inserting brackets does not handle this problem very well.</p>
<p>Motivation and overview of our model</p>
<p>In this dissertation, we want to explore better alternatives for encoding graphs, which are general enough to be applied on arbitrary graphs without destroying the original structures. We introduce graph recurrent networks (GRNs) and</p>
<p>show that they are successful in handling a variety of graph problems in the NLP area. Note that there are other types of graph neural networks, such as graph convolutional network (GCN) and gated graph neural network (GGNN).</p>
<p>I will give a comprehensive comparison in Chapter 2.2.</p>
<p>Given an input graph, GRN adopts a hidden state for each graph node.</p>
<p>In order to capture non-local interaction between nodes, it allows information exchange between neighboring nodes through time. At each time step, each node propagates its information to each other node that has a direct connection so that every node absorbs more and more global information through time.</p>
<p>Each resulting node representation contains information from a large context surrounding it, so the final representations can be very expressive for solving other graph problems, such as graph-to-sequence learning or graph classification. We can see that our GRN model only requires local and relative neighboring information, rather than an absolute topological order of all graph nodes required by RNNs. As a result, GRN can work on arbitrary graph structures with or without cycles. From the global view, GRN takes the collection of all node states as the state for the entire graph, and the neighboring information exchange through time can be considered as a graph state transition process.</p>
<p>The graph state transition is a recurrent process, where the state is recurrently updated through time (This is reason we name it "graph recurrent network").</p>
<p>Comparatively, a regular RNN absorbs a new token to update its state at each time. On the other hand, our GRN lets node states exchange information for updating the graph state through time.</p>
<p>In this thesis, I will introduce the application of GRN on 3 types of popular graphs in the NLP area, which are dependency graphs, semantic graphs and another type of graphs (evidence graphs) that are constructed from textual input for modeling entities and their relations. The evidence graphs are created from documents to represent entities (such as "Time Square", "New York City"</p>
<p>and "United States") and their relations for QA-oriented reasoning.</p>
<p>Thesis Outline</p>
<p>The remainder of the thesis is organized as follows.</p>
<p>• In this chapter, we will focus on the graph problems in the natural language processing (NLP) area. In particular, I will first introduce several conventional neural approaches (such as recurrent neural networks) for dealing with these graph problems, before giving a deeper investigation on the more recent graph </p>
<p>Encoding graphs with RNN or DAG network</p>
<p>Since the first great breakthrough of recurrent neural networks (RNN) on machine translation (Cho et al., 2014;Bahdanau et al., 2015), people have investigated the usefulness of RNN and several of its extensions for solving graph problems in the NLP area. Below we take abstract meaning representation (AMR) (Banarescu et al., 2013) as an example to demonstrate several existing ways for encoding graphs with RNNs. As shown in Figure 2.1, AMRs are rooted and directed graphs, where the graph nodes (such as "want-01" and "boy") represent the concepts and edges (such as "ARG0" and "ARG1") represent the relations between nodes.</p>
<p>Encoding with RNNs</p>
<p>To encode AMRs, one kind of approaches (Konstas et al., 2017) first linearize their inputs with depth-first traversal, before feeding the linearization results into a multi-layer LSTM encoder. We can see that the linearization causes loss of the structural information. For instance, originally closely-located graph nodes (such as parents and children) can be very far away, especially when the graph is very large. In addition, there is not a specific order among the children for a graph node, resulting in multiple linearization possibilities. This increases the data variation and further introduces ambiguity. Despite the drawbacks that mentioned above, these approaches received great success with the aid of large-scale training. For example, Konstas et al. (2017) leveraged 20 million sentences paired with automatically parsed AMR graphs using a multi-layer LSTM encoder, which demonstrates dramatic improvements (5.0+ BLEU points) over the existing statistical models (Pourdamghani et al., 2016;Flanigan et al., 2016b;Song et al., 2016bSong et al., , 2017. This demonstrates the strong learning power of RNNs, but there is still room for improvement due to the above mentioned drawbacks.</p>
<p>Encoding with DAG networks One better alternative than RNNs are DAG networks (Zhu et al., 2016;Su et al., 2017;Zhang and Yang, 2018), which extend RNN on directed acyclic graphs (DAGs). Comparing with sentences,</p>
<p>where each word has exactly one preceding word and one succeeding word, a node in a DAG can have multiple preceding and succeeding nodes, respectively. To adapt RNNs on DAGs, the hidden states of multiple preceding nodes are first merged before being applied to calculate the hidden state of the current node. One popular way for merging preceding hidden states is called "childsum" (Tai et al., 2015), which simply sum up their states to product one vector. have their own drawbacks, which I will be discussing here.</p>
<p>One solution  first splits a cyclic graph into two DAGs, which are then encoded with separate DAG networks. Note that legal splits always exist, one can first decide an order over graph nodes, before separating left-to-right edges from right-to-left ones to make a split. An obvious drawback is that structural information is lost by splitting a graph into two DAGs. Chapter 4 mainly studies this problem and gives our solution. The other solution  is to leverage another model to pick a node order from an undirected and unrooted graph. Since there are exponential numbers of node orders, more ambiguity is introduced, even through they use a model to pick the order. Also, preceding nodes cannot incorporate the information from their succeeding nodes.</p>
<p>Encoding graphs with Graph Neural Network</p>
<p>Since being introduced, graph neural networks (GNNs) (Scarselli et al., 2009) have long been neglected until recently Kipf and Welling, 2017;. To update node states within a graph, GNNs rely on a message passing mechanism that iteratively updates the node states in parallel.</p>
<p>During an iteration, a message is first aggregated for each node from its neighbors, then the message is applied to update the node state. To be more specific, updating the hidden state h i for node v i for iteration t can be formalized as the following equations:
m t i = Aggregate(x i , H t−1 N i , X N i ) (2.1) h t i = Apply(h t−1 i , m t i ), (2.2)
where H t−1 N i and X N i represent the hidden states and embeddings of the neighbors for v i , and N i corresponds to the set of neighbors for v i . We can see that each node gradually absorbs larger context through this message passing framework. This framework is remotely related to loopy belief propagation (LBP) (Murphy et al., 1999), but the main difference is that LBP propagates probabilities, while GNNs propagate hidden-state units. Besides, the message passing process is only executed for a certain number of times for GNNs, while LBP is usually executed until convergence. The reason is that GNNs are optimized for end-to-end task performance, not for a joint probability.</p>
<p>Main difference between GNNs and RNNs</p>
<p>The main difference is that GNNs do not require a node order for the input, such as a left-to-right order for sentences and a bottom-up order for trees. In contrast, having an order is crucial for RNNs and their DAG extensions. In fact, GNNs only require the local neighborhood information, thus they are agnostic of the input structures and are very general for being applied on any types of graphs, trees and even sentences. This is a fundamental difference that leads to many superior properties of GNNs comparing with RNNs and DAG networks. First, GNNs update node states in parallel within an iteration, thus they can be much faster than RNNs.</p>
<p>We will give more discussions and analysis in Chapters 4 and 5. Second, sibling nodes can easily incorporate the information of each other with GNNs.</p>
<p>This can be achieved by simply executing a GNN for 2 iterations, so that the information of one sibling can go up then down to reach the other.</p>
<p>Different types of GNNs</p>
<p>So far there have been several types of GNNs, and their main differences lay in the way for updating node states from aggregated messages (Equation 2.2).</p>
<p>We list several existing approaches in Table 2.1 and give detailed introduction below:</p>
<p>Convolution The first type is named graph convolutional network (GCN) (Kipf and Welling, 2017). It borrows the idea of convolutional neural network (CNN) (Krizhevsky et al., 2012), which gathers larger contextual information through each convolution operation. To deal with the problem where a graph node can have arbitrary number of neighbors, GCN and its later variants first sum up the hidden states of all neighbors, before applying the result of summation as messages to update the graph node state:
(2.3) m t i = ∑ v j ∈N i h t−1 j
Note that some GCN variations try to distinguish different types of neighbors before the summation:
(2.4) m t i = ∑ v j ∈N i W L(i,j) h t−1 j
But they actually choose W L(i,j) s to be identical, and thus this is equivalent to Equation 2.3. The underlying reason is that making W L(i,j) s to be different will introduce a lot of parameters, especially when the number of neighbor types is large. The summation operation can be considered as the message aggregation process first mentioned in Equation 2.1. In fact, most existing GNNs use sum to aggregate message. There are also other ways, which I will introduce later in this chapter.</p>
<p>Type</p>
<p>Model Ways for applying messages After messages are calculated, they are applied to update graph node states.
Convolution GCN h t i = ReLU(W t m t i + b t ) Attention GAN h t i = m t i Gated GGNN h t i = GRU(m t i , h t−1 i ) GRN h t i , c t i = LSTM(m t i , [h t−1 i , c t−1 i ])
GCNs use the simple linear transformation with ReLU (Nair and Hinton, 2010) as the activation function.</p>
<p>Attention Another type of GNNs are called graph attention network (GAN) (Veličković et al., 2018). In general, it adopts multi-head self attention (Vaswani et al., 2017) to calculate messages:
m t i = σ ∑ j∈N i a i,j h t−1 j (2.5) a i,j = exp(e i,j ) ∑ j ∈N i exp(e i,j ) (2.6) e i,j = MH A t (h t−1 i , h t−1 j ) (2.7)
where MH A t corresponds to the t-th multi-head self attention layer. For the next step, GAN diretly use the newly calculated message m t i as the new node state: h t i = m t i .</p>
<p>Gated</p>
<p>The massage propagation method based on linear transformations in GCN may suffer from long-range dependency problem, when dealing with very large and complex graphs. To alleviate the long-range dependency problem, recent work has proposed to use gated mechanisms to process messages.</p>
<p>In particular, graph recurrent networks (GRN) Song et al., 2018d) leverage the gated operations of an LSTM (Hochreiter and Schmidhuber, 1997) step to apply messages for node state updates. On the other hand, gated graph neural networks (GGNN) Beck et al., 2018) adopt a GRU (Cho et al., 2014) step to conduct the update. The message propagation mechanism for both models are shown in the last group of Table 2.1. To generate messages, they also simply sum up the hidden states of all neighbors. This is the same as GCNs.</p>
<p>Discussion on message aggregation The models mentioned above either use summations or an attention mechanism to calculate messages. As a result, these models have the same property: they are invariant to the permutations of their inputs, which result in different orders of neighbors. This property are also called "symmetric", and Hamilton et al. (2017) introduce several other "symmetric" and "asymmetric" message aggregators. In addition to summation and attention mechanisms, mean pooling and max pooling operations are also "symmetric". This is intuitive, as both pooling operations are obviously invariant to input orders.</p>
<p>On the other hand, they mention an LSTM aggregator, which generates messages by simply applying an LSTM to a random permutation of a node's neighbors. The LSTM aggregator is not permutation invariant, and thus is "asymmetric".</p>
<p>Discussion on the memory usage of GNNs</p>
<p>So far we have discussed several advantages of GNNs. Comparing with RNNs and DAG networks, GNNs are more flexible for handling any types of graphs.</p>
<p>Besides, they allow better parallelization, and thus are more efficient on GPUs.</p>
<p>However, they also suffer from limitations, and the most severe one is the largescale memory usage.</p>
<p>As mentioned above, GNNs update every graph node state within an iteration, and all node states are updated for T times if a GNN executes for T message passing steps. As a result, the increasing computation causes more memory usage. In general, the amount of memory usage is highly related to the density and scale of the input graph.</p>
<p>To alleviate the memory issue, FastGCN (Chen et al., 2018a) adopts importance sampling to remove edges, making graphs less dense. In contrast to fixed sampling methods above, Huang et al. (2018) introduce a parameterized and trainable sampler to perform layerwise sampling conditioned on the former layer. Furthermore, this adaptive sampler could find optimal sampling importance and reduce variance simultaneously.</p>
<p>Graph Recurrent Network for</p>
<p>Multi-hop Reading</p>
<p>Comprehension</p>
<p>In this chapter, we introduce a graph-based model for tackling multi-hop reading comprehension. Multi-hop reading comprehension focuses on one type of factoid question, where a system needs to properly integrate multiple pieces of evidence to correctly answer a question. Previous work approximates global evidence with local coreference information, encoding coreference chains with DAG-styled GRU layers within a gated-attention reader. However, coreference is limited in providing information for rich inference. We introduce a new method for better connecting global evidence, which forms more complex graphs compared to DAGs. To perform evidence integration on our graphs, we investigate our graph recurrent network (GRN). Experiments on two standard datasets show that richer global information leads to better answers. Our approach shows highly competitive performances on these datasets without deep language models (such as ELMo).</p>
<p>Introduction</p>
<p>Recent years have witnessed a growing interest in the task of machine reading comprehension. Most existing work (Hermann et al., 2015;Wang and Jiang, 2017;Seo et al., 2016;Wang et al., 2016;Weissenborn et al., 2017;Dhingra et al., 2017a;Shen et al., 2017;Xiong et al., 2016) focuses on a factoid scenario where the questions can be answered by simply considering very local information, such as one or two sentences. For example, to correctly answer a question "What causes precipitation to fall?", a QA system only needs to refer to one sentence in a passage: "... In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls under gravity. ...", and the final answer "gravity" is indicated key words of "precipitation" and "falls".</p>
<p>A more challenging yet practical extension is multi-hop reading comprehension (MHRC) (Welbl et al., 2018), where a system needs to properly integrate multiple pieces of evidence to correctly answer a question. Figure 3.1</p>
<p>shows an example, which contains three associated passages, a question and several candidate choices. In order to correctly answer the question, a system has to integrate the facts "The Hanging Gardens are in Mumbai" and "Mumbai is a city in India". There are also some irrelevant facts, such as "The Hanging</p>
<p>Gardens provide sunset views over the Arabian Sea" and "The Arabian Sea is bounded by Pakistan and Iran", which make the task more challenging, as an MHRC model has to distinguish the relevant facts from the irrelevant ones.</p>
<p>Despite being a practical task, so far MHRC has received little research attention. One notable method, Coref-GRU (Dhingra et al., 2018), uses corefer-  only coreference edges. In particular, the two coreference edges infer two facts:</p>
<p>"The Hanging Gardens provide views over the Arabian Sea" and "Mumbai is a city in India", from which we cannot infer the ultimate fact, "The Hanging</p>
<p>Gardens are in India", for correctly answering this instance.</p>
<p>We propose a general graph scheme for evidence integration, which allows information exchange beyond co-reference nodes, by allowing arbitrary degrees of the connectivity of the reference graphs. In general, we want the resulting graphs to be more densely connected so that more useful facts can be inferred. For example each edge can connect two related entity mentions, while unrelated mentions, such as "the Arabian Sea" and "India", may not be connected. In this paper, we consider three types of relations as shown in the bottom part of Figure 3.2.</p>
<p>The first type of edges connect the mentions of the same entity appearing across passages or further apart in the same passage. Shown in Figure 3.2, one instance connects the two "Mumbai" across the two passages. Intuitively, same-typed edges help to integrate global evidence related to the same entity, which are not covered by pronouns. The second type of edges connect two mentions of different entities within a context window. They help to pass useful evidence further across entities. For example, in the bottom graph of Figure   3.2, both window-typed edges of 1 and 6 help to pass evidence from "The</p>
<p>Hanging Gardens" to "India", the answer of this instance. Besides, windowtyped edges enhance the relations between local mentions that can be missed by the sequential encoding baseline. Finally, coreference-typed edges are further complementary to the previous two types, and thus we also include them.</p>
<p>Our generated graphs are complex and can have cycles, making it difficult to directly apply a DAG network (e.g. the structure of Coref-GRU). So we adopt graph recurrent network (GRN), as it has been shown successful on encoding various types of graphs, including semantic graphs (Song et al., 2018d), dependency graphs (Song et al., 2018e) and even chain graphs created by raw texts .</p>
<p>Given an instance containing several passages and a list of candidates, we first use NER and coreference resolution tools to obtain entity mentions, and then create a graph out of the mentions and relevant pronouns. As the next step, evidence integration is executed on the graph by adopting a graph neural network on top of a sequential layer. The sequential layer learns local representation for each mention, while the graph network learns a global represen-</p>
<p>tation. The answer is decided by matching the representations of the mentions against the question representation.</p>
<p>Experiments on WikiHop (Welbl et al., 2018) show that our created graphs are highly useful for MHRC. On the hold-out testset, it achieves an accuracy of  65.4%, which is highly competitive on the leaderboard 1 as of the paper submission time. In addition, our experiments show that the questions and answers are dramatically better connected on our graphs than on the coreference DAGs, if we map the questions on graphs using the question subject. Our experiments also show a positive relation between graph connectivity and end-to-end accuracy.</p>
<p>On the testset of ComplexWebQuestions (Talmor and Berant, 2018), our method also achieves better results than all published numbers. To our knowledge, we are among the first to investigate graph neural networks on reading comprehension.</p>
<p>Baseline</p>
<p>As shown in Figure 3.3, we introduce two baselines, which are inspired by Dhingra et al. (2018). The first baseline, Local, uses a standard BiLSTM layer (shown in the green dotted box), where inputs are first encoded with a BiLSTM layer, and then the representation vectors for the mentions in the passages are extracted, before being matched against the question for selecting an answer.</p>
<p>The second baseline, Coref LSTM, differs from Local by replacing the BiLSTM layer with a DAG LSTM layer (shown in the orange dotted box) for encoding additional coreference information, as proposed by Dhingra et al. (2018).</p>
<p>Local: BiLSTM encoding</p>
<p>Given a list of relevant passages, we first concatenate them into one large passage p 1 , p 2 . . . p N , where each p i is a passage word and x p i is the embedding of it. The Local baseline adopts a Bi-LSTM to encode the passage:
← − h i p = LSTM( ← − h i+1 p , x p i ) (3.1) − → h i p = LSTM( − → h i−1 p , x p i ) (3.2)
Each hidden state contains the information of its local context. Similarly, the question words q 1 , q 2 . . . q M are first converted into embeddings x q 1 , x q 2 . . . x q M before being encoded by another BiLSTM:
← − h j q = LSTM( ← − h j+1 q , x q j ) (3.3) − → h j q = LSTM( − → h j−1 q , x q j ) (3.4)</p>
<p>Coref LSTM: DAG LSTM encoding with conference</p>
<p>Taking the passage word embeddings x p 1 , . . . x p N and coreference information as the input, the DAG LSTM layer encodes each input word embedding (such as x p i ) with the following gated operations 2 :
i i = σ(W i x p i + U i ∑ i ∈Ω(i) − → h i p + b i ) o i = σ(W o x p i + U o ∑ i ∈Ω(i) − → h i p + b o ) f i ,i = σ(W f x p i + U f − → h i p + b f ) u i = σ(W u x p i + U u ∑ i ∈Ω(i) − → h i p + b u ) − → c i p = i i u i + ∑ i ∈Ω(i) f i ,i − → c i p − → h i p = o i tanh( − → c i p ) (3.5) Ω(i) represents all preceding words of p i in the DAG, i i , o i and f i ,i are the input, output and forget gates, respectively. W x , U x and b x (x ∈ {i, o, f , u}) are model parameters.</p>
<p>Representation extraction</p>
<p>After encoding both the passage and the question, we obtain a representation vector for each entity mention k ∈ E (E represents all entities), spanning from k i to k j , by concatenating the hidden states of its start and end positions, before they are correlated with a fully connected layer:
(3.6) h k = W 1 [ ← − h k i p ; − → h k i p ; ← − h k j p ; − → h k j p ] + b 1 ,
where W 1 and b 1 are model parameters for compressing the concatenated vector. Note that the current multi-hop reading comprehension datasets all focus on the situation where the answer is a named entity. Similarly, the representation vector for the question is generated by concatenating the hidden states of its first and last positions: where W 2 and b 2 are also model parameters.
(3.7) h q = W 2 [ ← − h 1 q ; − → h 1 q ; ← − h M q ; − → h M q ] + b 2 ! " ! # ! $ … … … % " % # % &amp; … BiLSTM</p>
<p>Attention-based matching</p>
<p>Given the representation vectors for the question and the entity mentions in the passages, an additive attention model (Bahdanau et al., 2015) 3 is adopted by treating all entity mention representations and the question representation as the memory and the query, respectively. In particular, the probability for a candidate c being the answer given input X is calculated by summing up all the occurrences of c across the input passages: 4
(3.8) p(c|X) = ∑ k∈N c α k ∑ k ∈N α k ,
where N c and N represent all occurrences of the candidate c and all occurrences of all candidates, respectively. Previous work  shows that summing the probabilities over all occurrences of the same entity mention is important for the multi-passage scenario. α k is the attention score for the entity mention k , calculated by an additive attention model shown below:
e k 0 = v T α tanh(W α h k + U α h q + b α ) (3.9) α k = exp(e k 0 ) ∑ k ∈N exp(e k 0 ) (3.10)
where v α , W α , U α and b α are model parameters.</p>
<p>Comparison with Dhingra et al. (2018)</p>
<p>The Coref-GRU model (Dhingra et al., 2018) is based on the gated-attention reader (GA reader) (Dhingra et al., 2017a).</p>
<p>GA reader is designed for the cloze-style reading comprehension task (Hermann et al., 2015), where one token is selected from the input passages as the answer for each instance. To adapt their model for the WikiHop benchmark,</p>
<p>where an answer candidate can contain multiple tokens, they first generate a probability distribution over the passage tokens with GA reader, and then compute the probability for each candidate c by aggregating the probabilities of all passage tokens that appear in c and renormalizing over the candidates.</p>
<p>In addition to using LSTM instead of GRU 5 , the main difference between our two baselines and Dhingra et al. (2018) is that our baselines consider each candidate as a whole unit no matter whether it contains multiple tokens or not. This makes our models more effective on the datasets containing phrasal answer candidates. </p>
<p>Evidence integration with GRN encoding</p>
<p>Over the representation vectors for a question and the corresponding entity mentions, we build an evidence integration graph of the entity mentions by connecting relevant mentions with edges, and then integrating relevant information for each graph node (entity mention) with a graph recurrent network (GRN) Song et al., 2018d). Figure 3.4 shows the overall procedure of our approach.</p>
<p>Evidence graph construction</p>
<p>As a first step, we create an evidence graph from a list of input passages to represent interrelations among entities within the passages. The entity mentions within the passages are taken as the graph nodes. They are automatically gen-erated by NER and coreference annotators, so that each graph node is either an entity mention or a pronoun representing an entity. We then create a graph by ensuring that edges between two nodes follow the situations below:</p>
<p>• They are occurrences of the same entity mention across passages or with a distance larger than a threshold τ L when being in the same passage.</p>
<p>• One is an entity mention and the other is its coreference. Our coreference information is automatically generated by a coreference annotator.</p>
<p>• Between two mentions of different entities in the same passage within a window threshold of τ S .</p>
<p>Between every two entities that satisfy the situations above, we make two edges in opposite directions. As a result, each generated graph can also be considered as an undirected graph.</p>
<p>Evidence integration with graph encoding</p>
<p>Tackling multi-hop reading comprehension requires inferring on global context. As the next step, we merge related information through the three types of edges just created by applying GRN on our graphs. Figure 3.5 shows the overall structure of our graph encoder. Formally, given a graph G = (V, E), a hidden state vector s k is created to represent each entity mention k ∈ V. The state of the graph can thus be represented as:
(3.11) g = {s k }| k ∈ V
In order to integrate non-local evidence among nodes, information exchange between neighborhooding nodes is performed through recurrent state transitions, leading to a sequence of graph states g 0 , g 1 , . . . , V and T is a hyperparameter representing the number of graph state transition decided by a development experiment. For initial state g 0 = {s k 0 }| k ∈ V, we initialize each s k 0 by:
g T , where g T = {s k T }| k ∈ … … ' , … … … ' ,-" … … ' ( . ,( . , " . ,(3.12) s k 0 = W 3 [h k ; h q ] + b 3 ,
where h k is the corresponding representation vector of entity mention k , calculated by Equation 3.6. h q is the question representation. W 3 and b 3 are model parameters.</p>
<p>State transition A gated recurrent neural network is used to model the state transition process. In particular, the transition from g t−1 to g t consists of a hidden state transition for each node, as shown in Figure 3.5. At each step t,</p>
<p>direct information exchange is conducted between a node and all its neighbors.</p>
<p>To avoid gradient diminishing or bursting, LSTM (Hochreiter and Schmidhu-number of steps necessary for information from one arbitrary node to reach another is equal to the size of the graph. We experiment with different transition steps to study the effectiveness of global encoding.</p>
<p>Note that unlike the sequence LSTM encoder, our graph encoder allows parallelization in node-state updates, and thus can be highly efficient using a GPU. It is general and can be potentially applied to other tasks, including sequences, syntactic trees and cyclic structures.</p>
<p>Matching and combination</p>
<p>After evidence integration, we match the hidden states at each graph encoding step with the question representation using the same additive attention mechanism introduced in the Baseline section. In particular, for each entity k , the matching results for the baseline and each graph encoding step t are first generated, before being combined using a weighted sum to obtain the overall matching result:
e k t = v T a t tanh(s k t W a t + h q U a t + b a t ) (3.15) e k = w c [e k 0 , e k 1 , . . . , e k T ] + b c , (3.16)
where e k 0 is the baseline matching result for k , e k t is the matching results after t steps, and T is the total number of graph encoding steps.
W a t , U a t , v a t , b a t ,
w c and b c are model parameters. In addition, a probability distribution is calculated from the overall matching results using softmax, similar to Equations 3.10. Finally, probabilities that belong to the same entity mention are merged to obtain the final distribution, as shown in Equation 3.8.</p>
<p>Training</p>
<p>We train both the baseline and our models using the cross-entropy loss:
(3.17) l = − log p(c * |X; θ),
where c * is ground-truth answer, X and θ are the input and model parameters, respectively. Adam (Kingma and Ba, 2014) with a learning rate of 0.001 is used as the optimizer. Dropout with rate 0.1 and a l2 normalization weight of 10 −8 are used during training.</p>
<p>Experiments on WikiHop</p>
<p>In this section, we study the effectiveness of rich types of edges and the graph encoders using the WikiHop (Welbl et al., 2018) dataset. It is designed for multievidence reasoning, as its construction process makes sure that multiple evidence are required for inducing the answer for each instance.</p>
<p>Data</p>
<p>The dataset contains around 51K instances, including 44K for training, 5K for development and 2.5K for held-out testing. Each instance consists of a question, a list of associated passages, a list of candidate answers and a correct answer. One example is shown in Figure 3.1. On average each instance has around 19 candidates, all of which are the same category. For example, if the answer is a country, all other candidates are also countries. We use Stanford</p>
<p>CoreNLP (Manning et al., 2014) to obtain coreference and NER annotations.</p>
<p>Then the entity mentions, pronoun coreferences and the provided candidates are taken as graph nodes to create an evidence graph. The distance thresholds (τ L and τ S , in Section 3.3.1) for making same and window typed edges are set to 200 and 20, respectively.</p>
<p>Settings</p>
<p>We study the model behavior on the WikiHop devset, choosing the best hyperparameters for online system evaluation on the final holdout testset. Our word embeddings are initialized from the 300-dimensional pretrained Glove word embeddings (Pennington et al., 2014)    increasing the transition step leads to a slight performance decrease. One reason can be that executing more transition steps may also introduce more noise through richly connected edges. We set the transition step to 3 for all remaining experiments. and GA w/ Coref-GRU correspond to Dhingra et al. (2018), and their reported numbers are copied. The former is their baseline, a gated-attention reader (Dhingra et al., 2017a), and the latter is their proposed method. First, even our Local show much higher accuracies compared with GA w/ GRU and GA w/ Coref-GRU. This is because our models are more compatible with the evaluated dataset. In particular, GA w/ GRU and GA w/ Coref-GRU calculate the probability for each candidate by summing up the probabilities of all tokens within the candidate. As a result, they cannot handle phrasal candidates very well, especially for the overlapping candidates, such as "New York" and "New York City". On the other hand, we consider each candidate answer as a single unit, and does not suffer from this issue. As a reimplementation of their idea, Coref LSTM only shows 0.4 points gains over Local, a stronger baseline 7 At paper-writing time, we observe a recent short arXiv paper (Cao et al., 2018) and two anonymous papers submitted to ICLR, showing better results with ELMo (Peters et al., 2018).</p>
<p>Development experiments</p>
<p>Main results</p>
<p>Our main contribution is studying an evidence integration approach, which is orthogonal to the contribution of ELMo on large-scale training. We will investigate ELMo in a future version. is not significantly better than Local, meaning that simply introducing more parameters does not help.</p>
<p>In addition to the systems above, we introduce Fully-Connect-GRN for demonstrating the effectiveness of our evidence graph creating approach.</p>
<p>Fully-Connect-GRN creates fully connected graphs out of the entity mentions, before encoding them with GRN. Within each fully connected graph, the question is directly connected with the answer. However, fully connected graphs are brute-force connections, and are not representative for integrating related evidence. MHQA-GRN is 1.5 points better than Fully-Connect-GRN, while ques-tions and answers are more directly connected (with distance 1 for all cases)</p>
<p>by Fully-Connect-GRN. The main reason can be that our evidence graphs only connect related entity mentions, making our models easier to learn how to integrate evidence. On the other hand, there are barely learnable patterns within fully connected graphs. More analyses on the relation between graph connectivity and end-to-end performance will be shown in later paragraphs.</p>
<p>We observe some unpublished papers showing better results with ELMo (Peters et al., 2018), which is orthogonal to our contribution.</p>
<p>Analysis</p>
<p>Effectiveness of edge types Table 3.2 shows the ablation study of different types of edges that we introduce for evidence integration. The first group</p>
<p>shows the situations where one type of edges are removed. In general, there is a large performance drop by removing any type of edges. The reason can be that the connectivity of the resulting graphs is reduced, thus fewer facts can be inferred. Among all these types, removing window-typed edges causes the least performance drop. One possible reason is that some information captured by them has been well captured by sequential encoding. However, window-typed edges are still useful, as they can help passing evidence through to further nodes. Take Figure 3.2 as an example, two window-typed edges help to pass information from "The Hanging Gardens" to "India". The other two types of edges are slightly more important than window-typed edges. Intuitively, they help to gather more global information than window-typed edges, thus learn better representations for entities by integrating contexts from their occurrences and co-references.</p>
<p>The second group of  are significantly better than the Local baseline, whereas the combination of all types of edges achieves a much better accuracy (1.8 points) than Local. This indicates the importance of evidence integration over better connected graphs.</p>
<p>We show more detailed quantitative analyses later on. The numbers generally demonstrate the same patterns as the first group. In addition, only same is slightly better than only coref. It is likely because some coreference information can also be captured by sequential encoding.</p>
<p>Distance Figure 3.7 shows the percentage distribution of distances between a question and its closest answer when either all types of edges are adopted or only coreference edges are used. The subject of each question 8 is used to locate the question on the corresponding graph.</p>
<p>When all types of edges are adopted, the questions and the answers for more than 90% of the development instances are connected, and the questionand-answer distances for more than 70% are within 3. On the other hand, the instances with distances longer than 4 only count for 10%. This can be the reason why performances do not increase when more than 3 transition steps are performed in our model. The advantage of our approach can be shown by contrasting the distance distributions over graphs generated either by the baseline or by our approach.</p>
<p>We further evaluate both approaches on a subset of the development instances, where the answer-and-question distance is at most 3 in our graph. dicates that our approach can better handle these "relatively easy" reasoning tasks. However, as shown in Figure 3.6, instances that require large reasoning steps are still challenging to our approach.</p>
<p>Experiments on ComplexWebQuestions</p>
<p>In this section, we conduct experiments on the newly released ComplexWe-bQuestions version 1.1 (Talmor and Berant, 2018) for better evaluating our approach. Compared with WikiHop, where the complexity is implicitly specified in the passages, the complexity of this dataset is explicitly specified on the question side. One example question is "What city is the birthplace of the author of 'Without end"'. A two-step reasoning is involved, with the first step being "the author of 'Without end"' and the second being "the birthplace of x".</p>
<p>x is the answer of the first step. questions. With all the snippets, SplitQA models the QA process based on a computation tree 9 of the full question. In particular, they first obtain the answers for the sub-questions, and then integrate those answers based on the computation tree. In contrast, our approach creates a graph from all the snippets, thus the succeeding evidence integration process can join all associated evidence.</p>
<p>Main results As shown in Table 3 Analysis To deal with complex questions that require evidence from mul-9 A computation tree is a special type of semantic parse, which has two levels. The first level contains sub-questions and the second level is a composition operation. 10 Upon the submission time, the authors of ComplexWebQuestions have not reported testing results for the two methods. To make a fair comparison we compare the devset accuracy.</p>
<p>tiple passages to answer, previous work Lin et al., 2018;Wang et al., 2018c) collect evidence from occurrences of an entity in different passages. The above methods correspond to a special case of our method, i.e.</p>
<p>MHQA with only the same-typed edges. From Table 3.3, our method gives 1 point increase over MHQA-GRN w/ only same, and it gives more increase in</p>
<p>WikiHop (comparing all types with only same in Table 3.2). Both results indicate that our method could capture more useful information for multi-hop QA tasks, compared to the methods developed for previous multi-passage QA tasks. This is likely because our method integrates not only evidences for an entity but also these for other related entities.</p>
<p>The leaderboard reports SplitQA with additional sub-question annotations and gold answers for sub-questions. These pairs of sub-questions and answers are used as additional data for training SplitQA. The above approach relies on annotations of ground-truth answers for sub-questions and semantic parses, thus is not practically useful in general. However, the results have additional value since it can be viewed as an upper bound of SplitQA. Note that the gap between this upper bound and our MHQA-GRN is small, which further proves that larger improvement can be achieved by introducing structural connections on the passage side to facilitate evidence integration.</p>
<p>Related Work</p>
<p>Question answering with multi-hop reasoning Multi-hop reasoning is an important ability for dealing with difficult cases in question answering (Rajpurkar et al., 2016;Boratko et al., 2018). Most existing work on multi-hop QA focuses on hopping over knowledge bases or tables (Jain, 2016;Neelakantan et al., 2016;Yin et al., 2016), thus the problem is reduced to deduction on a readily-defined structure with known relations. In contrast, we study multi-hop QA on tex-tual data and we introduce an effective approach for creating evidence integration graph structures over the textual input for solving our problems. Previous work (Hill et al., 2015;Shen et al., 2017) studying multi-hop QA on text does not create reference structures. In addition, they only evaluate their models on a simple task  with a very limited vocabulary and passage length. Our work is fundamentally different from theirs by modeling structures over the input, and we evaluate our models on more challenging tasks.</p>
<p>Recent work starts to exploit ways for creating structures from inputs. Talmor and Berant (2018) build a two-level computation tree over each question,</p>
<p>where the first-level nodes are sub-questions and the second-level node is a composition operation. The answers for the sub-questions are first generated, and then combined with the composition operation. They predefine two composition operations, which makes it not general enough for other QA problems. Dhingra et al. (2018) create DAGs over passages with coreference. The DAGs are then encoded using a DAG recurrent network. Our work follows the second direction by creating reasoning graphs on the passage side. However, we consider more types of relations than coreference, making a thorough study on evidence integration. Besides, we also investigate a recent graph neural network (namely GRN) on this problem.</p>
<p>Question answering over multiple passages Recent efforts in open-</p>
<p>domain QA start to generate answers from multiple passages instead of a single passage. However, most existing work on multi-passage QA selects the most relevant passage for answering the given question, thus reducing the problem to single-passage reading comprehension (Chen et al., 2017a;Dunn et al., 2017;Dhingra et al., 2017b;Wang et al., 2018a;Clark and Gardner, 2018). Our method is fundamentally different by truly leveraging multiple passages.</p>
<p>A few multi-passage QA approaches merge evidence from multiple pas-sages before selecting an answer Lin et al., 2018;Wang et al., 2018c). Similar to our work, they combine evidences from multiple passages, thus fully utilizing input passages. The key difference is that their approaches focus on how the contexts of a single answer candidate from different passages could cover different aspects of a complex question, while our approach studies how to properly integrate the related evidence of an answer candidate, some of which come from the contexts of different entity mentions. This increases the difficulty, since those contexts do not co-occur with the candidate answer nor the question. When a piece of evidence does not co-occur with the answer candidate, it is usually difficult for these methods to integrate the evidence. This is also demonstrated by our empirical comparison, where our approach shows much better performance than combining only the evidence of the same entity mentions.</p>
<p>Conclusion</p>
<p>We have introduced a new approach for tackling multi-hop reading comprehension (MHRC), with a graph-based evidence integration process. Given a question and a list of passages, we first connect related evidence in reference passages into a graph, and then adopt recent graph neural networks to encode resulted graphs for performing evidence integration. Results show that the three types of edges are useful on combining global evidence and that the graph neural networks are effective on encoding complex graphs resulted by the first step. Our approach shows highly competitive performances on two standard MHRC datasets.</p>
<p>Graph Recurrent Network for</p>
<p>n-ary Relation Extraction</p>
<p>In this chapter, we propose to tackle cross-sentence n-ary relation extraction, which aims at detecting relations among n entities across multiple sentences.</p>
<p>Typical methods formulate an input as a document graph, integrating various intra-sentential and inter-sentential dependencies. The current state-of-the-art method splits the input graph into two DAGs, adopting a DAG-structured LSTM for each. Though being able to model rich linguistic knowledge by leveraging graph edges, important information can be lost in the splitting procedure.</p>
<p>We propose a graph model by extending our graph recurrent network (GRN) with additional edge labels. In particular, it uses a parallel state to model each word, recurrently enriching state values via message passing with its neighbors. Compared with DAG LSTMs, our graph model keeps the original graph structure, and speeds up computation by allowing more parallelization. On a standard benchmark, our model shows the best result in the literature.</p>
<p>Introduction</p>
<p>As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown</p>
<p>The deletion mutation on exon-19 of EGFR gene was present in 16 patients, while the 858E point mutation on exon-21 was noted in 10.</p>
<p>All patients were treated with gefitinib and showed a partial response.  by separating left-to-right edges from right-to-left edges (Figure 4.1 (b)). Then, two separate gated recurrent neural networks, which extend tree LSTM (Tai et al., 2015), were adopted for each single-directional DAG, respectively. Fi-    Figure 4.1, the conversion breaks the inner structure of "exon-19 of EGFR gene", where the relation between "exon-19" and "EGFR" via the dependency path "exon-19
PREP OF − −−−− → gene NN − − → EGFR"
is lost from the original subgraph. Second, using LSTMs on both DAGs, information of only ancestors and descendants can be incorporated for each word.</p>
<p>Sibling information, which may also be important, is not included.</p>
<p>A potential solution to the problems above is to model a graph as a whole, learning its representation without breaking it into two DAGs. Due to the exis- Our contributions are summarized as follows.</p>
<p>• We empirically compared our GRN with DAG LSTM for n-ary relation extraction tasks, showing that the former is better by more effective use of structural information;</p>
<p>• To our knowledge, we are the first to investigate a graph recurrent network for modeling dependency and discourse relations.</p>
<p>Task Definition</p>
<p>Formally, the input for cross-sentence n-ary relation extraction can be repre-  Take Table 4.1 as an example. The binary classification task is to determine whether gefitinib would have an effect on this type of cancer, given a cancer patient with 858E mutation on gene EGFR. The multi-class classification task is to detect the exact drug effect: response, resistance, sensitivity, etc. Peng et al. (2017) formulate the task as a graph-structured problem in order to adopt rich dependency and discourse features. In particular, Stanford parser (Manning et al., 2014) is used to assign syntactic structure to input sentences, and heads of two consecutive sentences are connected to represent discourse information, resulting in a graph structure. For each input graph G = (V , E), the nodes V are words within input sentences, and each edge e ∈ E connects two words that either have a relation or are adjacent to each other. Each edge is denoted as a triple (i, j, l), where i and j are the indices of the source and target words, respectively, and the edge label l indicates either a dependency or discourse relation (such as "nsubj") or a relative position (such as "next tok"</p>
<p>Baseline: Bi-directional DAG LSTM</p>
<p>or "prev tok"). Throughout this paper, we use E in (j) and E out (j) to denote the sets of incoming and outgoing edges for word j.</p>
<p>For a bi-directional DAG LSTM baseline, we follow Peng et al. (2017), splitting each input graph into two separate DAGs by separating left-to-right edges from right-to-left edges (Figure 4.1). Each DAG is encoded by using a DAG LSTM (Section 4.3.2), which takes both source words and edge labels as inputs (Section 4.3.1). Finally, the hidden states of entity mentions from both LSTMs are taken as inputs to a logistic regression classifier to make a prediction:
(4.1)ŷ = softmax(W 0 [h 1 ; . . . ; h N ] + b 0 ),
where h j is the hidden state of entity j . W 0 and b 0 are parameters.</p>
<p>Input Representation</p>
<p>Both nodes and edge labels are useful for modeling a syntactic graph. As the input to our DAG LSTM, we first calculate the representation for each edge (i, j, l) by:
(4.2) x l i,j = W 1 [e l ; e i ] + b 1 ,
where W 1 and b 1 are model parameters, e i is the embedding of the source word indexed by i, and e l is the embedding of the edge label l.</p>
<p>Encoding process</p>
<p>The baseline LSTM model learns DAG representations sequentially, following word orders. Taking the edge representations (such as x l i,j ) as input, gated state transition operations are executed on both the forward and backward DAGs.</p>
<p>For each word j, the representations of its incoming edges E in (j) are summed up as one vector:
(4.3) x in j = ∑ (i,j,l)∈E in (j) x l i,j
Similarly, for each word j, the states of all incoming nodes are summed to a single vector before being passed to the gated operations:
(4.4) h in j = ∑ (i,j,l)∈E in (j) h i
Finally, the gated state transition operation for the hidden state h j of the j-th word can be defined as:
i j = σ(W i x in j + U i h in j + b i ) o j = σ(W o x in j + U o h in j + b o ) f i,j = σ(W f x l i,j + U f h i + b f ) u j = σ(W u x in j + U u h in j + b u ) c j = i j u j + ∑ (i,j,l)∈E in (j) f i,j c i h j = o j tanh(c j ),(4.5)
where i j , o j and f i,j are a set of input, output and forget gates, respectively, and W x , U x and b x (x ∈ {i, o, f , u}) are model parameters.</p>
<p>Comparison with Peng et al. (2017)</p>
<p>Our baseline is computationally similar to Peng et al. (2017), but different on how to utilize edge labels in the gated network. In particular, Peng et al. (2017) make model parameters specific to edge labels. They consider two model variations, namely Full Parametrization (FULL) and Edge-Type Embedding (EMBED).</p>
<p>FULL assigns distinct Us (in Equation 4.5) to different edge types, so that each edge label is associated with a 2D weight matrix to be tuned in training. On the other hand, EMBED assigns each edge label to an embedding vector, but complicates the gated operations by changing the Us to be 3D tensors. 1</p>
<p>In contrast, we take edge labels as part of the input to the gated network.</p>
<p>In general, the edge labels are first represented as embeddings, before being concatenated with the node representation vectors (Equation 4.2). We choose this setting for both the baseline and our GRN in Section 4.4, since it requires fewer parameters compared with FULL and EMBED, thus being less exposed to overfitting on small-scaled data.</p>
<p>Encoding with Graph Recurrent Network</p>
<p>Our input graph formulation strictly follows Section 4.3. In particular, our model adopts the same methods for calculating input representation (as in Sec-   For each time step t, the message to a word v j includes the representations of the edges that are connected to v j , where v j can be either the source or the target of the edge. Similar to Section 4.3.1, we define each edge as a triple (i, j, l),</p>
<p>where i and j are indices of the source and target words, respectively, and l is the edge label. x l i,j is the representation of edge (i, j, l). The inputs for v j are distinguished by incoming and outgoing directions, where:
x in j = ∑ (i,j,l)∈E in (j) x l i,j x out j = ∑ (j,k,l)∈E out (j) x l j,k (4.7)
Here E in (j) and E out (j) denote the sets of incoming and outgoing edges of v j , respectively.</p>
<p>In addition to edge inputs, the message also contains the hidden states of its incoming and outgoing words during a state transition. In particular, the states of all incoming words and outgoing words are summed up, respectively: 
h in j = ∑ (i,j,l)∈E in (j) h i t−1 h out j = ∑ (j,k,l)∈E out (j) h k t−1 ,(4.(4.10) h j t , c j t = LSTM(m j t , [h j t−1 , c j t−1 ]),
where c j is the cell memory for hidden state h j .</p>
<p>GRN vs bidirectional DAG LSTM A contrast between the baseline DAG</p>
<p>LSTM and our graph LSTM can be made from the perspective of information flow. For the baseline, information flow follows the natural word order in the input sentence, with the two DAG components propagating information from left to right and from right to left, respectively. In contrast, information flow in our GRN is relatively more concentrated at individual words, with each word exchanging information with all its graph neighbors simultaneously at each sate transition. As a result, wholistic contextual information can be leveraged for extracting features for each word, as compared to separated handling of bidirectional information flow in DAG LSTM. In addition, arbitrary structures, including arbitrary cyclic graphs, can be handled.</p>
<p>From an initial state with isolated words, information of each word propagates to its graph neighbors after each step. Information exchange between non-neighboring words can be achieved through multiple transition steps. We experiment with different transition step numbers to study the effectiveness of global encoding. Unlike the baseline DAG LSTM encoder, our model allows parallelization in node-state updates, and thus can be highly efficient using a GPU.</p>
<p>Training</p>
<p>We train our models with a cross-entropy loss over a set of gold standard data:
(4.11) l = − log p(y i |X i ; θ),
where X i is an input graph, y i is the gold class label of X i , and θ is the model </p>
<p>Experiments</p>
<p>We conduct experiments for the binary relation detection task and the multiclass relation extraction task discussed in Section 4.2.</p>
<p>Data</p>
<p>We use the dataset of Peng et al. (2017), which is a biomedical-domain dataset focusing on drug-gene-mutation ternary relations, 2 extracted from PubMed. It contains 6987 ternary instances about drug-gene-mutation relations, and 6087 binary instances about drug-mutation sub-relations. Table 4.2 shows statistics of the dataset. Most instances of ternary data contain multiple sentences, and the average number of sentences is around 2. There are five classification labels: "resistance or non-response", "sensitivity", "response", "resistance" and "None". We follow Peng et al. (2017) and binarize multi-class labels by grouping all relation classes as "Yes" and treat "None" as "No".   </p>
<p>Settings</p>
<p>Development Experiments</p>
<p>We first analyze our model on the drug-gene-mutation ternary relation dataset, taking the first among 5-fold cross validation settings for our data setting.  The performance of forward and backward lag behind concat, which is consistent with the intuition that both forward and backward relations are useful . In addition, all gives better accuracies compared with con-  Using all instances (the Cross column in Table 4.3), our graph model shows the highest test accuracy among all methods, which is 5.9% higher than our baseline. 4 The accuracy of our baseline is lower than EMBED and FULL of Peng et al. (2017), which is likely due to the differences mentioned in Section 4.3.3. Our final results are better than Peng et al. (2017), despite the fact that we do not use multi-task learning.</p>
<p>Final results</p>
<p>We also report accuracies only on instances within single sentences (column Single in Table 4.3), which exhibit similar contrasts. Note that all systems show performance drops when evaluated only on single-sentence relations, which are actually more challenging. One reason may be that some single sentences cannot provide sufficient context for disambiguation, making it necessary to study cross-sentence context. Another reason may be overfitting caused by relatively fewer training instances in this setting, as only 30% instances are within a single sentence. One interesting observation is that our baseline shows the least performance drop of 1.7 points, in contrast to up to 4.1 for other neural systems. This can be a supporting evidence for overfitting, as our baseline has fewer parameters at least than FULL and EMBED.</p>
<p>Analysis</p>
<p>Efficiency.  in training and decoding speeds, respectively. By revisiting  performance increase along increasing input sentence lengths. This is likely because longer contexts provide richer information for relation disambiguation. GRN is consistently better than Bidir DAG LSTM, and the gap is larger on shorter instances. This demonstrates that GRN is more effective in utilizing a smaller context for disambiguation. Figure 4.4 (b) shows the test accuracies against the maximum number of neighbors. Intuitively, it is easier to model graphs containing nodes with more neighbors, because these nodes can serve as a "supernode" that allow more efficient information exchange. The performances of both GRN and Bidir DAG LSTM increase with increasing maximal number of neighbors, which coincide with this intuition.</p>
<p>Accuracy against the maximal number of neighbors</p>
<p>In addition, GRN shows more advantage than Bidir DAG LSTM under the inputs having lower maximal number of neighbors, which further demonstrates the superiority of GRN over Bidir DAG LSTM in utilizing context information.</p>
<p>Case study Figure 4.5 visualizes the merits of GRN over Bidir DAG LSTM Bidir DAG LSTM fails to. The first case generally mentions that Gefitinib does not have an effect on T790M mutation on EGFR gene. Note that both "However" and "was not" serve as indicators; thus incorporating them into the contextual vectors of these entity mentions is important for making a correct prediction. However, both indicators are leaves of the dependency tree, making it impossible for Bidir DAG LSTM to incorporate them into the contextual vectors of entity mentions up the tree through dependency edges. 5 On the other hand, it is easier for GRN. For instance, "was not" can be incorporated into
"Gefitinib" through "suppressed agent − −− → treatment nn − → Gefitinib".
The second case is to detect the relation among "cetuximab" (drug), "EGFR" (gene) and "S492R" (mutation), which does not exist. However, the context introduces further ambiguity by mentioning another drug "Panitumumab", which does have a relation with "EGFR" and "S492R". Being sibling nodes in the dependency tree, "can not" is an indicator for the relation of "cetuximab".</p>
<p>GRN is correct, because "can not" can be easily included into the contextual vector of "cetuximab" in two steps via "bind nsubj − −− →cetuximab".</p>
<p>Results on Binary Sub-relations</p>
<p>Following previous work, we also evaluate our model on drug-mutation binary relations. using all instances ("Cross") and slightly better than FULL using only singlesentence instances ("Single").</p>
<p>Fine-grained Classification</p>
<p>Our dataset contains five classes as mentioned in Section 4.6.1. However, previous work only investigates binary relation detection. Here we also study the multi-class classification task, which can be more informative for applications. </p>
<p>Related Work</p>
<p>N-ary relation extraction N-ary relation extractions can be traced back</p>
<p>to MUC-7 (Chinchor, 1998), which focuses on entity-attribution relations. It has also been studied in biomedical domain (McDonald et al., 2005b), but only the instances within a single sentence are considered. Previous work on cross-sentence relation extraction relies on either explicit co-reference annotation (Gerber and Chai, 2010;Yoshikawa et al., 2011), or the assumption that the whole document refers to a single coherent event (Wick et al., 2006;Swampillai and Stevenson, 2011). Both simplify the problem and reduce the need for learning better contextual representation of entity mentions. A notable exception is Quirk and Poon (2017), who adopt distant supervision and integrated contextual evidence of diverse types without relying on these assumptions. However, they only study binary relations. We follow Peng et al. (2017) by studying ternary cross-sentence relations.</p>
<p>Graph encoder Liang et al. (2016) build a graph LSTM model for semantic object parsing, which aims to segment objects within an image into more fine-grained, semantically meaningful parts. The nodes of an input graph come from image superpixels, and the edges are created by connecting spatially neighboring nodes. Their model is similar as Peng et al. (2017) by calculating node states sequentially: for each input graph, a start node and a node sequence are chosen, which determines the order of recurrent state updates. In contrast, our graph LSTM do not need ordering of graph nodes, and is highly parallelizable.</p>
<p>Conclusion</p>
<p>We explored graph recurrent network for cross-sentence n-ary relation extraction, which uses a recurrent state transition process to incrementally refine a neural graph state representation capturing graph structure contexts.</p>
<p>Compared with a bidirectional DAG LSTM baseline, our model has several advantages. First, it does not change the input graph structure, so that no information can be lost. For example, it can easily incorporate sibling information when calculating the contextual vector of a node. Second, it is better parallelizable. Experiments show significant improvements over the previously reported numbers, including that of the bidirectional graph LSTM model.</p>
<p>Graph Recurrent Network for</p>
<p>AMR-to-text Generation</p>
<p>The problem of AMR-to-text generation is to recover a text representing the same meaning as an input AMR graph. The current state-of-the-art method uses a sequence-to-sequence model, leveraging LSTM for encoding a linearized AMR structure. Although it is able to model non-local semantic information, a sequence LSTM can lose information from the AMR graph structure, and thus faces challenges with large graphs, which result in long sequences. We introduce a neural graph-to-sequence model, using a novel LSTM structure for directly encoding graph-level semantics. On a standard benchmark, our model shows superior results to existing methods in the literature.</p>
<p>Introduction</p>
<p>Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. a genius." Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015).</p>
<p>The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts.</p>
<p>The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 5.1, "Ryan" is represented as "(p / person :name (n / name :op1 "Ryan"))", and "description of" is represented as "(d / describe-01 :ARG1 )".</p>
<p>While initial work used statistical approaches (Flanigan et al., 2016b;Pourdamghani et al., 2016;Song et al., 2017;Lampouras and Vlachos, 2017;Mille et al., 2017;Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closely-related nodes, such as parents, children and siblings can be far away after serialization. It can be difficult for a linear recurrent neural network to automatically induce their original connections from bracketed string forms.</p>
<p>To address this issue, we introduce a novel graph-to-sequence model, where a graph recurrent network (GRN) is used to encode AMR structures directly. To capture non-local information, the encoder performs graph state transition by information exchange between connected nodes, with a graph state consisting of all node states. Multiple recurrent transition steps are taken so that information can propagate non-locally, and LSTM (Hochreiter and Schmidhuber, 1997) is used to avoid gradient diminishing and bursting in the recurrent process. The decoder is an attention-based LSTM model with a copy mechanism (Gu et al., 2016;Gulcehre et al., 2016), which helps copy sparse tokens (such as numbers and named entities) from the input.</p>
<p>Trained on a standard dataset (LDC2015E86), our model surpasses a strong sequence-to-sequence baseline by 2.3 BLEU points, demonstrating the advantage of graph-to-sequence models for AMR-to-text generation compared to sequence-to-sequence models. Our final model achieves a BLEU score of 23.3 on the test set, which is 1.3 points higher than the existing state of the art (Konstas et al., 2017) trained on the same dataset. When using gigaword sentences as additional training data, our model is consistently better than Konstas et al. (2017) using the same amount of gigaword data,</p>
<p>showing the effectiveness of our model on large-scale training set. We release our code and models at https://github.com/freesunshine0316/ neural-graph-to-seq-mp.</p>
<p>Baseline: a seq-to-seq model</p>
<p>Our baseline is a sequence-to-sequence model, which follows the encoderdecoder framework of Konstas et al. (2017).</p>
<p>Input representation</p>
<p>Given an AMR graph G = (V , E), where V and E denote the sets of nodes and edges, respectively, we use the depth-first traversal of Konstas et al. (2017) to linearize it to obtain a sequence of tokens v 1 , . . . , v N , where N is the number of tokens. For example, the AMR graph in Figure 1 is serialized as "describe :arg0</p>
<p>( person :name ( name :op1 ryan ) ) :arg1 person :arg2 genius". We can see that the distance between "describe" and "genius", which are directly connected in the original AMR, becomes 14 in the serialization result.</p>
<p>A simple way to calculate the representation for each token v j is using its word embedding e j :</p>
<p>(5.1)
x j = W 1 e j + b 1 ,
where W 1 and b 1 are model parameters for compressing the input vector size.</p>
<p>To alleviate the data sparsity problem and obtain better word representation as the input, we also adopt a forward LSTM over the characters of the token, and concatenate the last hidden state h c j with the word embedding:
(5.2) x j = W 1 [e j ; h c j ] + b 1</p>
<p>Encoder</p>
<p>The encoder is a bi-directional LSTM applied on the linearized graph by depthfirst traversal, as in Konstas et al. (2017). At each step j, the current states ← − h j and − → h j are generated given the previous states ← − h j+1 and − → h j−1 and the cur-
rent input x j : ← − h j = LSTM( ← − h j+1 , x j ) (5.3) − → h j = LSTM( − → h j−1 , x j ) (5.4)</p>
<p>Decoder</p>
<p>We use an attention-based LSTM decoder (Bahdanau et al., 2015), where the attention memory (A) is the concatenation of the attention vectors among all input words. Each attention vector a j is the concatenation of the encoder states of an input token in both directions ( ← − h j and − → h j ) and its input vector (x j ):
a j = [ ← − h j ; − → h j ; x j ] (5.5) A = [a 1 ; a 2 ; . . . ; a N ] (5.6)
where N is the number of input tokens.</p>
<p>The decoder yields an output sequence w 1 , w 2 , . . . , w M by calculating a sequence of hidden states s 1 , s 2 . . . , s M recurrently. While generating the m-th word, the decoder considers five factors: (1) the attention memory A; (2) the previous hidden state of the LSTM decoder s m−1 ; (3) the embedding of the current input word (previously generated word) e m ; (4) the previous context vector µ m−1 , which is calculated by an attention mechanism (will be shown in the next paragraph) from A; and (5) the previous coverage vector γ m−1 , which is the accumulation of all attention distributions so far (Tu et al., 2016). When t = 1, we initialize µ 0 and γ 0 as zero vectors, set e 1 to the embedding of the start token "<s>", and calculate s 0 by averaging all encoder states.</p>
<p>For each time-step m, the decoder feeds the concatenation of the embedding of the current input e m and the previous context vector µ m−1 into the LSTM model to update its hidden state. Then the attention probability α m,i on the attention vector a i ∈ A for the time-step is calculated as:
m,i = v 2 tanh(W a a i + W s s t + W γ γ m−1 + b 2 ) (5.7) α m,i = exp( m,i ) ∑ N j=1 exp( m,j ) (5.8)
where W a , W s , W γ , v 2 and b 2 are model parameters. The coverage vector γ m is updated by γ m = γ m−1 + α m , and the new context vector µ m is calculated via
µ m = ∑ N i=1 α m,i a i .
The output probability distribution over a vocabulary at the current state is calculated by:
(5.9) p vocab = softmax(V 3 [s t , µ t ] + b 3 ),
where V 3 and b 3 are model parameters, and the number of rows in V 3 represents the number of words in the vocabulary.</p>
<p>The graph-to-sequence model</p>
<p>Unlike the baseline sequence-to-sequence model, we leverage our recurrent graph network (GRN) to represent each input AMR, which directly models the graph structure without serialization. Same as Chapter 4, our GRN-based graph encoder performs information exchange between nodes through a sequence of state transitions, leading to a sequence of states g 0 , g 1 , . . . , g t , . . . , where g t = {h j t }| v j ∈V . The initial state g 0 consists of a set of node states h j 0 that contain all zeros.</p>
<p>The graph encoder</p>
<p>The AMR graphs are similar with the dependency graphs (described in Chapter 4) in that both are directed and contain edge labels, so we simply adopt the GRN in Chapter 4 as our AMR graph encoder. Particularly, for node v j , the inputs include representations of edges that are connected to it, where it can be either the source or the target of the edge. We follow Chapter 4 to define each edge as a triple (i, j, l), where i and j are indices of the source and target nodes, respectively, and l is the edge label. x l i,j is the representation of edge (i, j, l), detailed in Section 5.3.2. The inputs for v j are distinguished by incoming and outgoing edges, before being summed up:
x in j = ∑ (i,j,l)∈E in (j) x l i,j x out j = ∑ (j,k,l)∈E out (j)
x l j,k ,
(5.11)
where E in (j) and E out (j) denote the sets of incoming and outgoing edges of v j , respectively. In addition to edge inputs, the encoder also considers the hidden states of its incoming nodes and outgoing nodes during a state transition. In particular, the states of all incoming nodes and outgoing nodes are summed up before being passed to the cell and gate nodes:
h in j = ∑ (i,j,l)∈E in (j) h i t−1 h out j = ∑ (j,k,l)∈E out (j) h k t−1 ,(5.12)
As the next step, the message m j t is aggregated by the concatenation:
(5.13) m j t = [x in j ; x out j ; h in j ; h out j ]
Then, it is applied with an LSTM step to update the node hidden state h j t−1 , the detailed equations are shown in Equation 3.13.
(5.14) h j t , c j t = LSTM(m j t , [h j t−1 , c j t−1 ]),
where c j is the cell memory for hidden state h j .</p>
<p>Input Representation</p>
<p>Different from sequences, the edges of an AMR graph contain labels, which represent relations between the nodes they connect, and are thus important for </p>
<p>Decoder</p>
<p>As shown in Figure 5.3, we adopt the attention-based LSTM decoder as described in Section 5.2.3. Since our graph encoder generates a sequence of graph states, only the last graph state is adopted in the decoder. In particular, we make the following changes to the decoder. First, each attention vector be-
comes a j = [h j T ; x j ], where h j
T is the last state for node v j . Second, the decoder initial state s −1 is the average of the last states of all nodes.</p>
<p>Integrating the copy mechanism</p>
<p>Open-class tokens, such as dates, numbers and named entities, account for a large portion in the AMR corpus. Most appear only a few times, resulting in a data sparsity problem. To address this issue, Konstas et al. (2017) adopt anonymization for dealing with the data sparsity problem. In particular, they first replace the subgraphs that represent dates, numbers and named entities (such as "(q / quantity :quant 3)" and "(p / person :name (n / name :op1 "Ryan"))") with predefined placeholders (such as "num 0" and "person name 0") before decoding, and then recover the corresponding surface tokens (such as "3" and "Ryan") after decoding. This method involves handcrafted rules, which can be costly.</p>
<p>Copy We find that most of the open-class tokens in a graph also appear in the corresponding sentence, and thus adopt the copy mechanism (Gulcehre et al., 2016;Gu et al., 2016) to solve this problem. The mechanism works on top of an attention-based RNN decoder by integrating the attention distribution into the final vocabulary distribution. The final probability distribution is defined as the interpolation between two probability distributions:
(5.17) p f inal = θ t p vocab + (1 − θ t )p attn ,
where θ t is a switch for controlling generating a word from the vocabulary or directly copying it from the input graph. p vocab is the probability distribution of directly generating the word, as defined in Equation 5.9, and p attn is calculated based on the attention distribution α t by summing the probabilities of the graph nodes that contain identical concept. Intuitively, θ t is relevant to the current decoder input e t and state s t , and the context vector µ t . Therefore, we define it as:
(5.18) θ t = σ(w µ µ t + w s s t + w e e t + b 5 ),
where vectors w µ , w s , w e and scalar b 5 are model parameters. The copy mechanism favors generating words that appear in the input. For AMR-to-text generation, it facilitates the generation of dates, numbers, and named entities that appear in AMR graphs.</p>
<p>Copying vs anonymization Both copying and anonymization alleviate the data sparsity problem by handling the open-class tokens. However, the copy mechanism has the following advantages over anonymization: (1) anonymization requires significant manual work to define the placeholders and heuristic rules both from subgraphs to placeholders and from placeholders to the surface tokens, (2) the copy mechanism automatically learns what to copy, while anonymization relies on hard rules to cover all types of the open-class tokens, and (3) the copy mechanism is easier to adapt to new domains and languages than anonymization.</p>
<p>Training and decoding</p>
<p>We train our models using the cross-entropy loss over each gold-standard output sequence W * = w * 1 , . . . , w * m , . . . , w * M :
(5.19) l = − M ∑ m=1 log p(w * m |w * m−1 , . . . , w * 1 , X; θ),
where X is the input graph, and θ is the model parameters. Adam (Kingma and Ba, 2014) with a learning rate of 0.001 is used as the optimizer, and the model that yields the best devset performance is selected to evaluate on the test set. Dropout with rate 0.1 is used during training. Beam search with beam size to 5 is used for decoding. Both training and decoding use Tesla K80 GPUs.</p>
<p>Experiments</p>
<p>Data</p>
<p>We use a standard AMR corpus (LDC2015E86) as our experimental dataset, which contains 16,833 instances for training, 1368 for development and 1371</p>
<p>for test. Each instance contains a sentence and an AMR graph.</p>
<p>Following Konstas et al. (2017), we supplement the gold data with largescale automatic data. We take Gigaword as the external data to sample raw sentences, and train our model on both the sampled data and LDC2015E86.</p>
<p>We adopt Konstas et al. (2017)'s strategy for sampling sentences from Gigaword, and choose JAMR (Flanigan et al., 2016a) to parse selected sentences into AMRs, as the AMR parser of Konstas et al. (2017) only works on the anonymized data. For training on both sampled data and LDC2015E86, we also follow the method of Konstas et al. (2017), which is fine-tuning the model on the AMR corpus after every epoch of pretraining on the gigaword data.</p>
<p>Settings</p>
<p>We extract a vocabulary from the training set, which is shared by both the encoder and the decoder. The word embeddings are initialized from Glove pretrained word embeddings (Pennington et al., 2014) (Papineni et al., 2002).</p>
<p>For model hyperparameters, we set the graph state transition number as 9</p>
<p>according to development experiments. Each node takes information from at most 10 neighbors. The hidden vector sizes for both encoder and decoder are set to 300 (They are set to 600 for experiments using large-scale automatic data).</p>
<p>Both character embeddings and hidden layer sizes for character LSTMs are set 100, and at most 20 characters are taken for each graph node or linearized token.</p>
<p>Development experiments</p>
<p>As shown in Table 5.1, we compare our model with a set of baselines on the AMR devset to demonstrate how the graph encoder and the copy mechanism can be useful when training instances are not sufficient. Seq2seq is the sequence-to-sequence baseline described in Section 5.2. Seq2seq+copy extends Seq2seq with the copy mechanism, and Seq2seq+charLSTM+copy further extends Seq2seq+copy with character LSTM. Graph2seq is our graph-to-sequence model, Graph2seq+copy extends Graph2seq with the copy mechanism, and Graph2seq+charLSTM+copy further extends Graph2seq+copy with the character LSTM. We also try Graph2seq+Anon, which applies our graph-to-sequence model on the anonymized data from Konstas et al. (2017).</p>
<p>The graph encoder As can be seen from Table 5.1, the performance of Graph2seq is 1.6 BLEU points higher than Seq2seq, which shows that our graph encoder is effective when applied alone. Adding the copy mechanism (Graph2seq+copy vs Seq2seq+copy), the gap becomes 2.3. This shows that the graph encoder learns better node representations compared to the sequence encoder, which allows attention and copying to function better. Applying the graph encoder together with the copy mechanism gives a gain of 3.4 BLEU points over the baseline (Graph2seq+copy vs Seq2seq). The graph encoder is consistently better than the sequence encoder no matter whether character LSTMs are used.</p>
<p>We also list the encoding part of decoding times on the devset, as the decoders of the seq2seq and the graph2seq models are similar, so the time differences reflect efficiencies of the encoders. Our graph encoder gives consistently better efficiency compared with the sequence encoder, showing the advantage of parallelization. Table 5.1 shows that the copy mechanism is effective on both the graph-to-sequence and the sequence-to-sequence models.</p>
<p>The copy mechanism</p>
<p>Anonymization gives comparable overall performance gains on our graphto-sequence model as the copy mechanism (comparing Graph2seq+Anon with Graph2seq+copy). However, the copy mechanism has several advantages over anonymization as discussed in Section 5.3.4. Character LSTM Character LSTM helps to increase the performances of both systems by roughly 0.6 BLEU points. This is largely because it further alleviates the data sparsity problem by handling unseen words, which may share common substrings with in-vocabulary words.</p>
<p>Effectiveness on graph state transitions</p>
<p>We report a set of development experiments for understanding the graph LSTM encoder. Graph diameter We analyze the percentage of the AMR graphs in the devset with different graph diameters and show the cumulative distribution in Figure 5.5. The diameter of an AMR graph is defined as the longest distance between two AMR nodes. 1 Even though the diameters for less than 80% of the AMR graphs are less or equal than 10, our development experiments show that it is not necessary to incorporate the whole-graph information for each node. Further increasing state transition number may lead to additional improvement. We do not perform exhaustive search for finding the optimal state transition number. Figure 5.4, we analyze the efficiency of state transition when only incoming or outgoing edges are used.</p>
<p>Number of iterations</p>
<p>Incoming and outgoing edges As shown in</p>
<p>From the results, we can see that there is a huge drop when state transition is performed only with incoming or outgoing edges. Using edges of one direction, the node states only contain information of ancestors or descendants. On the other hand, node states contain information of ancestors, descendants, and siblings if edges of both directions are used. From the results, we can conclude that not only the ancestors and descendants, but also the siblings are important for modeling the AMR graphs. This is similar to observations on syntactic parsing tasks (McDonald et al., 2005a), where sibling features are adopted.</p>
<p>We perform a similar experiment for the Seq2seq+copy baseline by only executing single-directional LSTM for the encoder. We observe BLEU scores of 11.8 and 12.7 using only forward or backward LSTM, respectively. This is consistent with our graph model in that execution using only one direction leads to a huge performance drop. The contrast is also reminiscent of using the normal input versus the reversed input in neural machine translation (Sutskever et al., 2014).  stas et al., 2017) is an attentional multi-layer sequence-to-sequence model trained with the anonymized data. PBMT (Pourdamghani et al., 2016) adopts a phrase-based model for machine translation (Koehn et al., 2003) on the input of linearized AMR graph, SNRG (Song et al., 2017) uses synchronous node replacement grammar for parsing the AMR graph while generating the text, and</p>
<p>Results</p>
<p>Tree2Str (Flanigan et al., 2016b) converts AMR graphs into trees by splitting the re-entrances before using a tree transducer to generate the results.</p>
<p>Graph2seq+charLSTM+copy achieves a BLEU score of 23.3, which is 1.3 points better than MSeq2seq+Anon trained on the same AMR corpus. In ad- MSeq2seq+Anon. Note that MSeq2seq+Anon relies on anonymization, which requires additional manual work for defining mapping rules, thus limiting its usability on other languages and domains. The neural models tend to underperform statistical models when trained on limited (16K) gold data, but performs better with scaled silver data (Konstas et al., 2017).</p>
<p>Following Konstas et al. (2017), we also evaluate our model using both the AMR corpus and sampled sentences from Gigaword. Using additional 200K or 2M gigaword sentences, Graph2seq+charLSTM+copy achieves BLEU scores of 28.2 and 33.0, respectively, which are 0.8 and 0.7 BLEU points better than MSeq2seq+Anon using the same amount of data, respectively. The BLEU scores are 5.3 and 10.1 points better than the result when it is only trained with the AMR corpus, respectively. This shows that our model can benefit from scaled data with automatically generated AMR graphs, and it is more effective than MSeq2seq+Anon using the same amount of data. Using 2M gigaword data, our model is better than all existing methods. Konstas et al. (2017) also experimented with 20M external data, obtaining a BLEU of 33.8. We did not try this setting due to hardware limitations. The Seq2seq+charLSTM+copy baseline trained on the large-scale data is close to MSeq2seq+Anon using the same amount of training data, yet is much worse than our model.</p>
<p>Case study</p>
<p>We conduct case studies for better understanding the model performances. Table 5.3 shows example outputs of sequence-to-sequence (S2S), graph-tosequence (G2S) and graph-to-sequence with copy mechanism (G2S+CP). Ref denotes the reference output sentence, and Lin shows the serialization results of input AMRs. The best hyperparameter configuration is chosen for each model.</p>
<p>For the first example, S2S fails to recognize the concept "a / account" as a noun and loses the concept "o / old" (both are underlined). The fact that "a / account" is a noun is implied by "a / account :mod (o / old)" in the original AMR graph. Though directly connected in the original graph, their distance in the serialization result (the input of S2S) is 26, which may be why S2S makes these mistakes. In contrast, G2S handles "a / account" and "o / old" correctly.</p>
<p>In addition, the copy mechanism helps to copy "look-over" from the input, which rarely appears in the training set. In this case, G2S+CP is incorrect only on hyphens and literal reference to "anti-japanese war", although the meaning is fully understandable.</p>
<p>For the second case, both G2S and G2S+CP correctly generate the noun "agreement" for "a / agree" in the input AMR, while S2S fails to. The fact that "a / agree" represents a noun can be determined by the original graph segment "p / provide :ARG0 (a / agree)", which indicates that "a / agree" is the subject of "p / provide". In the serialization output, the two nodes are close to each other. Nevertheless, S2S still failed to capture this structural relation, which reflects the fact that a sequence encoder is not designed to explicitly model hierarchical information encoded in the serialized graph. In the training instances, serialized nodes that are close to each other can originate from neighboring graph nodes, or distant graph nodes, which prevents the decoder from confidently deciding the correct relation between them. In contrast, G2S sends the node "p / provide" simultaneously with relation "ARG0" when calculating hidden states for "a / agree", which facilitates the yielding of "the agreement provides".</p>
<p>Related work</p>
<p>Among early statistical methods for AMR-to-text generation, Flanigan et al. (2016b) convert input graphs to trees by splitting re-entrances, and then translate the trees into sentences with a tree-to-string transducer. Song et al. (2017) use a synchronous node replacement grammar to parse input AMRs and generate sentences at the same time. Pourdamghani et al. (2016)  In addition to NMT (Gulcehre et al., 2016), the copy mechanism has been shown effective on tasks such as dialogue (Gu et al., 2016), summarization (See et al., 2017) and question generation (Song et al., 2018a). We investigate the copy mechanism on AMR-to-text generation.</p>
<p>Conclusion</p>
<p>We introduced a novel graph-to-sequence model for AMR-to-text generation.</p>
<p>Compared to sequence-to-sequence models, which require linearization of AMR before decoding, a graph LSTM is leveraged to directly model full AMR structure. Allowing high parallelization, the graph encoder is more efficient than the sequence encoder. In our experiments, the graph model outperforms a strong sequence-to-sequence model, achieving the best performance. tive research topic (Stahlberg et al., 2016;Aharoni and Goldberg, 2017;Li et al., 2017;Chen et al., 2017b;Bastings et al., 2017;Wu et al., 2017;Chen et al., 2018b).</p>
<p>On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate-argument information from SRL can improve the performance of an attention-based sequence-to-sequence model by alleviating the "argument switching" problem, 1 one frequent and severe issue faced by NMT systems (Isabelle et al., 2017). Figure 6.1 (a) shows one example of semantic role information, which only captures the relations between a predicate (gave) and its arguments (John, wife and present). Other important information, such as the relation between John and wife, can not be incorporated. 1 flipping arguments corresponding to different roles In this paper, we explore the usefulness of abstract meaning representation (AMR) (Banarescu et al., 2013) as a semantic representation for NMT. AMR is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. Figure 6.1 (b) shows an AMR graph, in which the nodes (such as give-01 and John) represent the concepts, and edges (such as :ARG0 and :ARG1) represent the relations between concepts they connect. Comparing with semantic roles, AMRs capture more relations, such as the relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity, when training data is not sufficient for large-scale training.</p>
<p>Recent advances in AMR parsing keep pushing the boundary of state-ofthe-art performance (Flanigan et al., 2014;Artzi et al., 2015;Pust et al., 2015;Peng et al., 2015;Flanigan et al., 2016a;Buys and Blunsom, 2017;Konstas et al., 2017;Wang and Xue, 2017;Lyu and Titov, 2018;Peng et al., 2018;Groschwitz et al., 2018;Guo and Lu, 2018), and have made it possible for automaticallygenerated AMRs to benefit down-stream tasks, such as question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016), and event detection (Li et al., 2015). However, to our knowledge, no existing work has exploited AMR for enhancing NMT.</p>
<p>We fill in this gap, taking an attention-based sequence-to-sequence system as our baseline, which is similar to Bahdanau et al. (2015). To leverage knowledge within an AMR graph, we adopt a graph recurrent network (GRN) Song et al., 2018d) as the AMR encoder. In particular, a full AMR graph is considered as a single state, with nodes in the graph being its substates. State transitions are performed on the graph recurrently, allowing substates to exchange information through edges. At each recurrent step, each node advances its current state by receiving information from the current states of its adjacent nodes. Thus, with increasing numbers of recurrent steps, each word receives information from a larger context. Figure 6.3 shows the recurrent transition, where each node works simultaneously. Compared with other methods for encoding AMRs (Konstas et al., 2017), GRN keeps the original graph structure, and thus no information is lost. For the decoding stage, two separate attention mechanisms are adopted in the AMR encoder and sequential encoder, respectively.</p>
<p>Experiments on WMT16 English-German data (4.17M) show that adopting AMR significantly improves a strong attention-based sequence-to-sequence baseline (25.5 vs 23.7 BLEU). When trained with small-scale (226K) data, the improvement increases (19.2 vs 16.0 BLEU), which shows that the structural information from AMR can alleviate data sparsity when training data are not sufficient. To our knowledge, we are the first to investigate AMR for NMT.</p>
<p>Our code and parallel data with automatically parsed AMRs are available at https://github.com/freesunshine0316/semantic-nmt.</p>
<p>Related work</p>
<p>Most previous work on exploring semantics for statistical machine translation (SMT) studies the usefulness of predicate-argument structure from semantic role labeling (Wong and Mooney, 2006;Wu and Fung, 2009;Liu and Gildea, 2010;Baker et al., 2012). Jones et al. (2012b) first convert Prolog expressions into graphical meaning representations, leveraging synchronous hyperedge replacement grammar to parse the input graphs while generating the outputs.</p>
<p>Their graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on large-scale machine translation.</p>
<p>Recently, Marcheggiani et al. (2018) investigate semantic role labeling (SRL) on neural machine translation (NMT). The predicate-argument structures are encoded via graph convolutional network (GCN) layers (Kipf and Welling, 2017), which are laid on top of regular BiRNN or CNN layers. Our work is in line with exploring semantic information, but different in exploiting AMR rather than SRL for NMT. In addition, we leverage a graph recurrent network (GRN) Song et al., 2018d) for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly-attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018).</p>
<p>Baseline: attention-based BiLSTM</p>
<p>We take the attention-based sequence-to-sequence model of Bahdanau et al. (2015) as the baseline, but use LSTM cells (Hochreiter and Schmidhuber, 1997) instead of GRU cells (Cho et al., 2014).</p>
<p>BiLSTM encoder</p>
<p>The encoder is a bi-directional LSTM on the source side. Given a sentence,
two sequences of states [ ← − h 1 , ← − h 2 , . . . , ← − h N ] and [ − → h 1 , − → h 2 , . . . − → h N ] are generated
for representing the input word sequence x 1 , x 2 , . . . , x N in the right-to-left and left-to-right directions, respectively, where for each word x i ,
← − h i = LSTM( ← − h i+1 , e x i ) (6.1) − → h i = LSTM( − → h i−1 , e x i ) (6.
2) e x i is the embedding of word x i .</p>
<p>Attention-based decoder</p>
<p>The decoder yields a word sequence in the target language y 1 , y 2 , . . . , y M by calculating a sequence of hidden states s 1 , s 2 . . . , s M recurrently. We use an attention-based LSTM decoder (Bahdanau et al., 2015), where the attention 
h i = [ ← − h i ; − → h i ] (6.3) H = [h 1 ; h 2 ; . . . ; h N ] (6.4)
N is the number of source words.</p>
<p>While generating the m-th word, the decoder considers four factors: (1) the attention memory H; (2) the previous hidden state of the LSTM model s m−1 ; (3) the embedding of the current input (previously generated word) e y m ; and (4) the previous context vector ζ m−1 from attention memory H. When m = 1, we initialize ζ 0 as a zero vector, set e y 1 to the embedding of sentence start token "<s>", and calculate s 0 from the last step of the encoder states via a dense layer:
(6.5) s 0 = W 1 [ ← − h 0 ; − → h N ] + b 1
where W 1 and b 1 are model parameters.</p>
<p>For each decoding step m, the decoder feeds the concatenation of the embedding of the current input e y m and the previous context vector ζ m−1 into the LSTM model to update its hidden state:</p>
<p>(6.6) s m = LSTM(s m−1 , [e y m ; ζ m−1 ])</p>
<p>Then the attention probability α m,i on the attention vector h i ∈ H for the current decode step is calculated as:
m,i = v 2 tanh(W h h i + W s s m + b 2 ) (6.7) α m,i = exp( m,i ) ∑ N j=1 exp( m,j ) (6.8)
W h , W s , v 2 and b 2 are model parameters. The new context vector ζ m is calculated via
(6.9) ζ m = N ∑ i=1 α m,i h i
The output probability distribution over the target vocabulary at the current state is calculated by:
(6.10) p vocab = softmax(V 3 [s m , ζ m ] + b 3 ),
where V 3 and b 3 are learnable parameters. Figure 6.2 shows the overall architecture of our model, which adopts a BiLSTM (bottom left) and our graph recurrent network (GRN) 2 (bottom right) for encoding the source sentence and AMR, respectively. An attention-based LSTM decoder is used to generate the output sequence in the target language, with attention models over both the sequential encoder and the graph encoder. The attention memory for the graph encoder is from the last step of the graph state transition process, which is shown in Figure 6.3. 2 We show the advantage of our graph encoder by comparing with another popular way for encoding AMRs in Section 6.6.3.  Figure 6.3 shows the overall structure of our graph recurrent network for encoding AMR graphs, which follows Chapter 5. Formally, given an AMR graph G = (V , E), we use a hidden state vector a j to represent each node v j ∈ V .</p>
<p>Incorporating AMR</p>
<p>Encoding AMR with GRN</p>
<p>The state of the graph can thus be represented as:
(6.11) g = {a j }| v j ∈V
The GRN encoding exactly follows Chapter 5.3.1, and it generates the finalstate graph representation after T message passing steps:
(6.12) g T = {a j T }| v j ∈V
T im e Figure 6.3: Architecture of the graph recurrent network.</p>
<p>Incorporating AMR information with a doubly-attentive decoder</p>
<p>There is no one-to-one correspondence between AMR nodes and source words.</p>
<p>To incorporate additional knowledge from an AMR graph, an external attention model is adopted over the baseline model. In particular, the attention memory from the AMR graph is the last graph state g T = {a j T }| v j ∈V . In addition, the contextual vector based on the graph state is calculated as:
m,i =ṽ 2 tanh(W a a i T +W s s m +b 2 ) α m,i = exp(˜ m,i ) ∑ N j=1 exp(˜ m,j )
W a ,W s ,ṽ 2 andb 2 are model parameters. The new context vectorζ m is calculated via ∑ N i=1α m,i a i T . Finally,ζ m is incorporated into the calculation of the output probability distribution over the target vocabulary (previously defined in Equation 6.10):
(6.13) P vocab = softmax(V 3 [s m , ζ m ,ζ m ] + b 3 )</p>
<p>Training</p>
<p>Given a set of training instances {(X (1) , Y (1) ), (X (2) , Y (2) ), . . . }, we train our models using the cross-entropy loss over each gold-standard target sequence
Y (j) = y (j) 1 , y (j) 2 , . . . , y (j) M : l = − M ∑ m=1 log p(y (j) m |y (j) m−1 , . . . , y(j)
1 , X (j) ; θ) X (j) represents the inputs for the jth instance, which is a source sentence for our baseline, or a source sentence paired with an automatically parsed AMR graph for our model. θ represents the model parameters.</p>
<p>Experiments</p>
<p>We empirically investigate the effectiveness of AMR for English-to-German translation.  (Papineni et al., 2002), TER (Snover et al., 2006) and Meteor (Denkowski and Lavie, 2014) are used as the metrics on cased and tokenized results. We also compare with Transformer (Vaswani et al., 2017) and OpenNMT (Klein et al., 2017), trained on the same dataset and with the same set of hyperparameters as our systems. In particular, we compare with Transformer-tf, one popular implementation 7 of Transformer based on TensorFlow, and we choose OpenNMT-tf, an official release 8 of OpenNMT implemented with Ten-sorFlow. It is a popular open-source attention-based sequence-to-sequence system that has served as a baseline system in previous literature. For a fair comparison, OpenNMT-tf has 1 layer for both the encoder and the decoder, and</p>
<p>Transformer-tf has the default configuration (N=6), but with parameters being shared among different blocks.   In terms of BLEU score, Dual2seq is significantly better than Seq2seq in both settings, which shows the effectiveness of incorporating AMR information. In particular, the improvement is much larger under the small-scale setting (+3.2 BLEU) than that under the large-scale setting (+1.7 BLEU). This is an evidence that structural and coarse-grained semantic information encoded in AMRs can be more helpful when training data are limited.</p>
<p>Development experiments</p>
<p>Main results</p>
<p>When trained on the NC-v11 subset, the gap between Seq2seq and Dual2seq under Meteor (around 5 points) is greater than that under BLEU (around 3 points). Since Meteor gives partial credit to outputs that are synonyms to the reference or share identical stems, one possible explanation is that the structural information within AMRs helps to better translate the concepts from the source language, which may be synonyms or paronyms of reference words.</p>
<p>As shown in the second group of Table 6.3, we further compare our model with other methods of leveraging syntactic or semantic information. Dual2seq-LinAMR shows much worse performance than our model and only slightly outperforms the Seq2seq baseline. Both results show that simply taking advantage of the AMR concepts without their relations does not help very much.</p>
<p>One reason may be that AMR concepts, such as John and Mary, also appear in the textual input, and thus are also encoded by the other (sequential) encoder. 9</p>
<p>The gap between Dual2seq and Dual2seq-LinAMR comes from modeling the relations between concepts, which can be helpful for deciding target word order by enhancing the relations in source sentences. We conclude that properly encoding AMRs is necessary to make them useful.</p>
<p>Encoding dependency trees instead of AMRs, Dual2seq-Dep shows a larger performance gap with our model (17.8 vs 19.2) on small-scale training data than on large-scale training data (25.0 vs 25.5). It is likely because AMRs are more useful on alleviating data sparsity than dependency trees, since words are lemmatized into unified concepts when parsing sentences into AMRs. For modeling long-range dependencies, AMRs have one crucial advantage over dependency trees by modeling concept-concept relations more directly. It is because AMRs drop function words, thus the distances between concepts are generally closer in AMRs than in dependency trees. Finally, Dual2seq-SRL is less effective than our model, because the annotations labeled by SRL are a subset of AMRs.</p>
<p>We outperform Marcheggiani et al. (2018)  BiLSTM layers encode the input sentence in the source language, and the concatenated hidden states of both layers contain information from both semantic role and source sentence. For incorporating AMR, since there is no one-to-one word-to-node correspondence between a sentence and the corresponding AMR graph, we adopt separate attention models. Our BLEU scores are higher than theirs, but we cannot conclude that the advantage primarily comes from AMR.   Performance based on sentence length We hypothesize that AMRs should be more beneficial for longer sentences: those are likely to contain longdistance dependencies (such as discourse information and predicate-argument structures) which may not be adequately captured by linear chain RNNs but are directly encoded in AMRs. To test this, we partition the test data into four buckets by length and calculate BLEU for each of them. Figure 6.5 shows the performances of our model along with Dual2seq-Dep and Seq2seq. Our model outperforms the Seq2seq baseline rather uniformly across all buckets, except for the first one where they are roughly equal. This may be surprising. On the one hand, Seq2seq fails to capture some dependencies for medium-length instances; on the other hand AMR parses are more noisy for longer sentences, which prevents us from obtaining extra improvements with AMRs.</p>
<p>Analysis</p>
<p>Dependency trees have been proved useful in capturing long-range dependencies. Figure 6.5 shows that AMRs are comparatively better than dependency trees, especially on medium-length (21-30) sentences. The reason may be that the AMRs of medium-length sentences are much more accurate than longer sentences, thus are better at capturing the relations between concepts.</p>
<p>On the other hand, even though dependency trees are more accurate than AMRs, they still fail to represent relations for long sentences. It is likely because relations for longer sentences are more difficult to detect. Another possible reason is that dependency trees do not incorporate coreferences, which AMRs consider.</p>
<p>Human evaluation</p>
<p>We further study the translation quality of predicateargument structures by conducting a human evaluation on 100 instances from the testset. In the evaluation, translations of both Dual2seq and Seq2seq, together with the source English sentence, the German reference, and an AMR are provided to a German-speaking annotator to decide which translation better captures the predicate-argument structures in the source sentence. To avoid annotation bias, translation results of both models are swapped for some instances, and the German annotator does not know which model each translation belongs to. The annotator either selects a "winner" or makes a "tie" decision, meaning that both results are equally good.</p>
<p>Out of the 100 instances, Dual2seq wins on 46, Seq2seq wins on 23, and there is a tie on the remaining 31. Dual2seq wins on almost half of the instances, about twice as often as Seq2seq wins, indicating that AMRs help in translating the predicate-argument structures on the source side.</p>
<p>Case study The outputs of the baseline system (Seq2seq) and our final system (Dual2seq) are shown in Figure 6.6. In the first sentence, the AMR-based Dual2seq system correctly produces the reflexive pronoun sich as an argument of the verb trafen (meet), despite the distance between the words in the system output, and despite the fact that the equivalent English words each other do not appear in the system output. This is facilitated by the argument structure in the AMR analysis.</p>
<p>In the second sentence, the AMR-based Dual2seq system produces an overly literal translation for the English phrasal verb come across. The Seq2seq translation, however, incorrectly states that the police vehicles are refugees. The difficulty for the Seq2seq probably derives in part from the fact that are and coming are separated by the word constantly in the input, while the main predicate is clear in the AMR representation.</p>
<p>In the third sentence, the Dual2seq system correctly translates the object of breed as worms, while the Seq2seq translation incorrectly states that the scientists breed themselves. Here the difficulty is likely the distance between the object and the verb in the German output, which causes the Seq2seq system to lose track of the correct input position to translate.</p>
<p>Conclusion</p>
<p>We showed that AMRs can improve neural machine translation. In particular, the structural semantic information from AMRs can be complementary to the source textual input by introducing a higher level of information abstraction.</p>
<p>A graph recurrent network (GRN) is leveraged to encode AMR graphs without breaking the original graph structure, and a sequential LSTM is used to encode the source input. The decoder is a doubly-attentive LSTM, taking the encoding results of both the graph encoder and the sequential encoder as attention memories. Experiments on a standard benchmark showed that AMRs are helpful regardless of the sentence length, and are more effective than other more popular choices, such as dependency trees and semantic roles. AMR: (s2 / say-01 :ARG0 (p3 / person :ARG1-of (h / have-rel-role-91 :ARG0 (p / person :ARG1-of (m2 / meet-03 :ARG0 (t / they) :ARG2 15) :mod (m / mutual)) :ARG2 (f / friend)) :name (n2 / name :op1 "Carla" :op2 "Hairston")) :ARG1 (a / and :op1 (p2 / person :name (n / name :op1 "Lamb"))) :ARG2 (s / she) :time 20)</p>
<p>Src: Carla Hairston said she was 15 and Lamb was 20 when they met through mutual friends AMR: (s / say-01 :ARG0 (m / media :ARG1-of (l / local-02)) :ARG1 (c2 / come-01 :ARG1 (v / vehicle :mod (p / police)) :manner (c3 / constant) :path (a / across :op1 (r / refugee :mod (n2 / new))) :time (s2 / since :op1 (t3 / then)) :topic (t / thing :name (n / name :op1 (c / Croatian) :op2 (t2 / Tavarnik))))) Src: Since then , according to local media , police vehicles are constantly coming across new refugees in Croatian Tavarnik . AMR: (b2 / breed-01 :ARG0 (p2 / person :ARG0-of (h / have-org-role-91 :ARG2 (s3 / scientist))) :ARG1 (w2 / worm) :ARG2 (s2 / system :ARG1-of (c / control-01 :ARG0 (b / burst-01 :ARG1 (w / wave :mod (s / sound))) :ARG1-of (p / possible-01)) :ARG1-of (n / nervous-01) :mod (m / modify-01 :ARG1 (g / genetics)))) Src: Scientists have bred worms with genetically modified nervous systems that can be con- </p>
<p>Ref</p>
<p>Conclusion</p>
<p>In this dissertation, we have introduced graph recurrent networks (GRN), which is general enough to encode graphs of arbitrary types without destroying the original graph structure. We investigated our GRN on 3 types of graphs across 4 different tasks, including multi-hop reading comprehension (Chapter 3), n-ary relation extraction (Chapter 4), AMR-to-text generation (Chapter 5) and semantic neural machine translation (Chapter 6). On each task, our carefully designed experiments show that GRN is significantly better than baselines based on sequence-to-sequence models and DAG networks. Besides, our GRN allows better parallelism than sequence-to-sequence and DAG models, and as a result it is much faster than these baselines. We also theoretically compare GRN with other graph neural networks, such as graph convolutional network (GCN) (Kipf and Welling, 2017) and gated graph neural network (GGNN) , with a message passing framework (Chapter 2).</p>
<p>Despite being successful on several tasks, there is still much room for improving GRN, thus I consider refining GRN as part of my future work. One direction of refinement is to further differentiate the node states after each GRN step. Previously, we rely on development experiments to carefully choose a proper number of GRN steps, and different tasks require very different number. To alleviate this problem, we can use the node states of all steps instead of the last step. By using the GRN outputs of all steps, we ask the model to pick node states of proper GRN steps based on the task performance. Another direction for improving GRN is to alleviate the massive memory usage (This problem exists for the other graph neural networks too) by sampling. Chen et al. (2018a) have investigated importance sampling according to the significance of the nodes in a citation graph. However, relations are also very important for many types of graphs in the NLP area, such as the semantic graphs and dependency graphs. We will study a proper relation-oriented sampling approach to reduce the memory usage of GRN.</p>
<p>First and foremost, I would like to thank my advisor Daniel Gildea. He has always been very farsighted and knowledgeable. During the five years working with him, he was always supportive and patient, especially during my most difficult days in my second year. Despite having a very busy schedule, he always welcomed my occasional drop in for idea discussions, in addition to maintaining regular weekly meetings. Even though he had his own research interest and ideas, he never imposed those ideas on me, but encouraged me to pursue research interests of my own. I hope to follow his example and become a knowledgeable and sincere researcher in my later academic career. I would like to thank the people I have worked with over the years. Xiaochang and I have cooperated in several projects and we have written a few papers together. I would like to thank Professor Yue Zhang, a significant collaborator since 2014, and Dr. Zhiguo Wang, the mentor of my IBM internships, for discussing and brainstorming on my graph recurrent network development. I am grateful to Dr. Mo Yu and Dr. Lin Zhao for idea discussion and other accommodation during my internships. I would also like to thank my other committee members, Lenhart Schubert and Jiebo Luo, for accommodating their time, and providing their valuable feedback. of several kinds of graph neural networks, where ms and hs have the same meaning as Equation 2.2. W s and bs represent model parameters. . . . . . . . . . . . . . . . . . . . . . . 15 3.1 Main results (unmasked) on WikiHop, where systems with † use ELMo (Peters et al., 2018). . . . . . . . . . . . . . . . . . . . . . . . 34 3.2 Ablation study on different types of edges using GRN as the graph encoder. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 3.3 Results on the ComplexWebQuestions dataset. . . . . . . . . . . . 40 4.1 An example showing that tumors with L858E mutation in EGFR gene respond to gefitinib treatment. . . . . . . . . . . . . . . . . . . 45 4.2 Dataset statistics. Avg. Tok. and Avg. Sent. are the average number of tokens and sentences, respectively. Cross is the percentage of instances that contain multiple sentences. . . . . . . . . . . . . . 55 4.3 Average test accuracies for TERNARY drug-gene-mutation interactions. Single represents experiments only on instances within single sentences, while Cross represents experiments on all instances. *: significant at p &lt; 0.01 . . . . . . . . . . . . . . . . . . . 57 xv 4.4 The average times for training one epoch and decoding (seconds) over five folds on drug-gene-mutation TERNARY cross sentence setting. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 4.5 Average test accuracies in five-fold cross-validation for BINARY drug-mutation interactions. . . . . . . . . . . . . . . . . . . . . . . 61 4.6 Average test accuracies for multi-class relation extraction with all instances ("Cross"). . . . . . . . . . . . . . . . . . . . . . . . . . 63 5.1 DEV BLEU scores and decoding times. . . . . . . . . . . . . . . . . 77 5.2 TEST results. "(200K)", "(2M)" and "(20M)" represent training with the corresponding number of additional sentences from Gigaword. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82 5.3 Example system outputs. . . . . . . . . . . . . . . . . . . . . . . . . 86 6.1 Statistics of the dataset. Numbers of tokens are after BPE processing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97 6.2 Sizes of vocabularies. EN-ori represents original English sentences without BPE. . . . . . . . . . . . . . . . . . . . . . . . . . . . 97</p>
<p>result (p &lt; 0.01) over Seq2seq. ↓ indicates the lower the better. . . . . . . . . . . . . . . . . . . . . . 101 6.4 BLEU scores of Dual2seq on the little prince data, when gold or automatic AMRs are available. . . . . . . . . . . . . . . . . . . . types of graphs in NLP area. . . . . . . . . . . . . . . . . . 2 1.2 Graph encoding with L CNN layers. . . . . . . . . . . . . . . . . . 4 1.3 Graph linearization by depth-first traversal. "A" and "E" are directly connected in the original graph, but fell apart after linearization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.1 An AMR graph representing "The boy wants to go." . . . . . . . . 10 3.1 An example from WikiHop (Welbl et al., 2018), where some relevant entity mentions and their coreferences are highlighted. . . . 20 3.2 A DAG generated by Dhingra et al. (2018) (top) and a graph by considering all three types of edges (bottom) on the example in</p>
<p>Figure 3 .
31. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 3.3 Baselines. The upper dotted box is a DAG LSTM layer with addition coreference links, while the bottom one is a typical BiLSTM layer. Either layer is used. . . . . . . . . . . . . . . . . . . . . . . . 25 3.4 Model framework. . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 3.5 Graph recurrent network for evidence graph encoding. . . . . . . 29 3.6 DEV performances of different transition steps. . . . . . . . . . . . 33 xvii 3.7 Distribution of distances between a question and an answer on the DEVSET. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 4.1 (a) A fraction of the dependency graph of the example in Table 4.1. For simplicity, we omit edges of discourse relations. (b) Results after splitting the graph into two DAGs. . . . . . . . . . . . . 46 4.2 GRN encoding for a dependency graph, where each w i is a word. 52 4.3 Dev accuracies against transition steps for GRN. . . . . . . . . . . 56 4.4 Test set performances on (a) different sentence lengths, and (b) different maximal number of neighbors. . . . . . . . . . . . . . . . 59 4.5 Example cases. Words with subindices 1, 2 and 3 represent drugs, genes and mutations, respectively. References for both cases are "No". For both cases, GRN makes the correct predictions, while Bidir DAG LSTM does incorrectly. . . . . . . . . . . . 60 5.1 An example of AMR graph meaning "Ryan's description of himself: a genius." . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 5.2 GRN encoding for an AMR graph. . . . . . . . . . . . . . . . . . . 71 5.3 The decoder with copy mechanism. . . . . . . . . . . . . . . . . . 73 5.4 DEV BLEU scores against transition steps for the graph encoder. . 79 5.5 Percentage of DEV AMRs with different diameters. . . . . . . . . 80 6.1 (a) A sentence with semantic roles annotations, (b) the corresponding AMR graph of that sentence. . . . . . . . . . . . . . . . . 88 6.2 Overall architecture of our model. . . . . . . . . . . . . . . . . . . 94 6.3 Architecture of the graph recurrent network. . . . . . . . . . . . . 95 xviii 6.4 DEV BLEU scores against transition steps for the graph encoders. The state transition is not applicable to Seq2seq, so we draw a dashed line to represent its performance. . . . . . . . . . . . . . . 100 6.5 Test BLEU score of various sentence lengths . . . . . . . . . . . . . 105 6.6 Sample system outputs . . . . . . . . . . . . . . . . . . . . . . . . . 108 1 Introduction</p>
<p>Figure 1 .
11 shows an example for several types of graphs, where a semantic graph(Figure 1.1a)visualizes the underlining meaning (such as who</p>
<p>Figure 1 :
1Graphical representation of the Stanford Dependencies for the sentence: Bell, based in L Angeles, makes and distributes electronic, computer and building products.</p>
<p>) A small fraction of a large knowledge graph.</p>
<p>Figure 1 . 1 :
11Several types of graphs in NLP area.</p>
<p>Figure 1 . 2 :
12Graph encoding with L CNN layers. neighbors, so there are O(N 2 ) bigram, O(N 3 ) trigram and O(N 4 ) 4-gram windows, respectively. On the other hand, previous attempts at modeling text with CNN do not suffer from this problem, as a sentence with N words only has O(N) windows for each n-gram. This is because a sentence can be viewed as a chain graph, where each node has only one left neighbor and one right neighbor.</p>
<p>Figure 1 .Figure 1 . 3 :
1133 shows a linearization result, where nodes "A" and "E" are far apart, while they are directly connected in the original graph. To alleviate this problem, previous methods insert brack-Graph linearization by depth-first traversal. "A" and "E" are directly connected in the original graph, but fell apart after linearization.</p>
<p>Figure 2 . 1 :
21An AMR graph representing "The boy wants to go." neural networks (GNN).</p>
<p>Figure 3 . 1 :
31An example from WikiHop (Welbl et al., 2018), where some relevant entity mentions and their coreferences are highlighted. inferred. The top part of Figure 3.2 shows a directed acyclic graph (DAG) with</p>
<p>Figure 3 .
32: A DAG generated by Dhingra et al. (2018) (top) and a graph by considering all three types of edges (bottom) on the example in Figure 3.1.</p>
<p>Figure 3 . 3 :
33Baselines. The upper dotted box is a DAG LSTM layer with addition coreference links, while the bottom one is a typical BiLSTM layer. Either layer is used.</p>
<p>Figure 3 . 4 :
34Model framework.</p>
<p>Figure 3 . 5 :
35Graph recurrent network for evidence graph encoding.</p>
<p>performances of different transition steps.</p>
<p>Figure 3 .
36 shows the devset performance of our GRN-based model with different transition steps. It shows the baseline performances when transition step is 0. The accuracy goes up when increasing the transition step to 3. Further</p>
<p>Figure 3 . 7 :
37Distribution of distances between a question and an answer on the DEVSET.</p>
<p>.3, similar to the observations in Wiki-Hop, MHQA-GRN achieves large improvements over Local. Both the baselines and our models use all web snippets, but MHQA-GRN further considers the structural relations among entity mentions. SplitQA achieves 0.5% improvement over SimpQA 10 . Our Local baseline is comparable with SplitQA and our graph-based models contribute a further 2% improvement over Local. This indicates that considering structural information on passages is important for the dataset.</p>
<p>2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domainPeng et al., 2017). While most existing work extracts relations within a sentence(Zelenko et al., 2003;Palmer et al., 2005;Zhao and Grishman, 2005;Jiang and Zhai, 2007;Plank and Moschitti, 2013;Li and Ji, 2014;Gormley et al., 2015;Miwa and Bansal, 2016;Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention(Gerber and Chai, 2010;Yoshikawa et al., 2011). Recently,Peng et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 4.1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary relation yet appear in distinct sentences.Peng et al. (2017) proposed a graph-structured LSTM for n-ary relation extraction. As shown inFigure 4.1 (a), graphs are constructed from input sentences with dependency edges, links between adjacent words, and intersentence relations, so that syntactic and discourse information can be used for relation extraction. To calculate a hidden state encoding for each word,Peng et al. (2017) first split the input graph into two directed acyclic graphs (DAGs)</p>
<p>Figure 4 . 1 :
41(a) A fraction of the dependency graph of the example in</p>
<p>tence of cycles, naive extension of tree LSTMs cannot serve this goal. Recently, graph convolutional networks (GCN)(Kipf and Welling, 2017;Marcheggiani and Titov, 2017;Bastings et al., 2017) and graph recurrent networks (GRN)Song et al., 2018d) have been proposed for representing graph structures for NLP tasks. Such methods encode a given graph by hierarchically learning representations of neighboring nodes in the graphs via their connecting edges. While GCNs use CNN for information exchange, GRNs take gated recurrent steps to this end. For fair comparison with DAG LSTMs, we build a graph model by extendingSong et al. (2018d), which strictly follow the configurations ofPeng et al. (2017) such as the source of features and hyper parameter settings. In particular, the full input graph is modeled as a single state, with words in the graph being its sub states. State transitions are performed on the graph recurrently, allowing word-level states to exchange information through dependency and discourse edges. At each recurrent step, each word advances its current state by receiving information from the current states of its adjacent words. Thus with increasing numbers of recurrent steps each word receives information from a larger context.Figure 4.2 shows the recurrent transition steps where each node works simultaneously within each transition step.Compared with bidirectional DAG LSTM, our method has several advantages. First, it keeps the original graph structure, and therefore no information is lost. Second, sibling information can be easily incorporated by passing information up and then down from a parent. Third, information exchange allows more parallelization, and thus can be very efficient in computation.Results show that our model outperforms a bidirectional DAG LSTM baseline by 5.9% in accuracy, overtaking the state-of-the-art system of Peng et al. (2017) by 1.2%. Our code is available at https://github.com/ freesunshine0316/nary-grn.</p>
<p>sented as a pair (E, T ), where E = ( 1 , . . . , N ) is the set of entity mentions, and T = [S 1 ; . . . ; S M ] is a text consisting of multiple sentences. Each entity mention i belongs to one sentence in T . There is a predefined relation set R = (r 1 , . . . , r L , None), where None represents that no relation holds for the entities. This task can be formulated as a binary classification problem of determining whether 1 , . . . , N together form a relation, or a multi-class classification problem of detecting which relation holds for the entity mentions.</p>
<p>tion 4.3.1) and performing classification as the baseline model. However, different from the baseline bidirectional DAG LSTM model, we leverage GRN to directly model the input graph, without splitting it into two DAGs. Comparing with the evidence graphs shown in Chapter 3, the dependency graphs are directed and contain edge labels that provide important information. Here we adapt GRN to further incorporate this information.</p>
<p>Figure 4 .
42 shows an overview of the GRN encoder for dependency graphs.Formally, given an input graph G = (V , E), we define a state vector h j for each word v j ∈ V . The state of the graph consists of all word states,</p>
<p>Figure 4 . 2 :
42GRN encoding for a dependency graph, where each w i is a word. Same as Chapter 3, the GRN-based encoder performs information exchange between neighboring words through a recurrent state transition process, resulting in a sequence of graph states g 0 , g 1 , . . . , g t , where g t = {h j t }| v j ∈V , and the initial graph state g 0 consists of a set of initial word states h j 0 = h 0 , where h 0 is a zero vector. The main change is on message aggregation, where we further distinguish incoming neighbors and outgoing neighbors, and edge labels are also incorporated.</p>
<p>Figure 4 . 3 :
43Dev accuracies against transition steps for GRN.</p>
<p>Following
Peng et al. (2017), five-fold cross-validation is used for evaluating the models, 3 and the final test accuracy is calculated by averaging the test accuracies over all five folds. For each fold, we randomly separate 200 instances from the training set for development. The batch size is set as 8 for all experiments.Word embeddings are initialized with the 100-dimensional GloVe(Pennington et al., 2014) vectors, pretrained on 6 billion words from Wikipedia and web text.The edge label embeddings are 3-dimensional and randomly initialized. Pretrained word embeddings are not updated during training. The dimension of hidden vectors in LSTM units is set to 150.</p>
<p>the devset accuracies of different state transition numbers, where forward and backward execute our graph state model only on the forward or backward DAG, respectively. Concat concatenates the hidden states of forward</p>
<p>cat, demonstrating the advantage of simultaneously considering forward and backward relations during representation learning. For all the models, more state transition steps result in better accuracies, where larger contexts can be integrated in the representations of graphs. The performance of all starts to converge after 4 and 5 state transitions, so we set the number of state transitions to 5 in the remaining experiments.</p>
<p>Figure 4 . 4 :
44Test set performances on (a) different sentence lengths, and (b) different maximal number of neighbors.</p>
<p>Figure 4 . 5 :
45Example cases. Words with subindices 1, 2 and 3 represent drugs, genes and mutations, respectively. References for both cases are "No". For both cases, GRN makes the correct predictions, while Bidir DAG LSTM does incorrectly.</p>
<p>Figure 5 .Figure 5 . 1 :
5511 shows an AMR graph in which the nodes (such as "describe-01" and "person") represent the concepts, and edges (such as ":ARG0" and ":name") represent the relations between concepts they connect. AMR has been proven helpful on other NLP tasks, such as machine translation(Jones et al., 2012a; An example of AMR graph meaning "Ryan's description of himself:</p>
<p>Figure 5 .Figure 5 . 2 :
5522 shows the overall structure of our graph encoder. Formally, given an AMR graph G = (V , E), we use a hidden state vector h j to represent each node v j ∈ V . The state of the graph can thus be represented as:(5.10) g = {h j }| v j ∈VT im e GRN encoding for an AMR graph.</p>
<p>Figure 5 . 3 :
53The decoder with copy mechanism. modeling the graphs. Similar with Section 5.2, we adopt two different ways for calculating the representation for each edge (i, j, l):x l i,j = W 4 [e l ; e i ] + b 4 (5.15) x l i,j = W 4 [e l ; e i ; h c i ] + b 4 ,(5.16) where e l and e i are the embeddings of edge label l and source node v i , h c i denotes the last hidden state of the character LSTM over v i , and W 4 and b 4 are trainable parameters. The equations correspond to Equations 5.1 and 5.2 in Section 5.2.1, respectively.</p>
<p>Figure 5 . 4 :
54DEV BLEU scores against transition steps for the graph encoder.</p>
<p>Figure 5 . 5 :
55We analyze the influence of the number of state transitions to the model performance on the devset.Figure 5.4 shows the BLEU scores of different state transition numbers, when both incoming and outgoing edges are taken for calculating the next state (as shown inFigure 5.2). The system is Graph2seq+charLSTM+copy. Executing only 1 iteration results in a poor BLEU score of 14.1. In this case the state for each node only contains information about immediately adjacent nodes. The performance goes up dramatically to 21.5 when increasing the iteration number to 5. In this case, the state for each node contains information of all nodes within a distance of 5. The performance Percentage of DEV AMRs with different diameters.further goes up to 22.8 when increasing the iteration number from 5 to 9, where all nodes with a distance of less than 10 are incorporated in the state for each node.</p>
<p>Figure 6
6.1: (a) A sentence with semantic roles annotations, (b) the corresponding AMR graph of that sentence.</p>
<p>memory (H) is the concatenation of the attention vectors among all source words. Each attention vector h i is the concatenation of the encoder states of an input token in both directions ( ← − h i and − → h i ):</p>
<p>Figure 6 . 2 :
62Overall architecture of our model.</p>
<p>Figure 6 .Figure 6 . 4 :
6644 shows the system performances as a function of the number of graph state transitions on the development set. Dual2seq (self) represents our dualattentive model, but its graph encoder encodes the source sentence, which is treated as a chain graph, instead of an AMR graph. Compared with Dual2seq, Dual2seq (self) has the same number of parameters, but without semantic information from AMR. Due to hardware limitations, we do not perform an exhaustive search by evaluating every possible state transition number, but only transition numbers of 1, 5, 10 and 12. 7 https://github.com/Kyubyong/transformer DEV BLEU scores against transition steps for the graph encoders.</p>
<p>on the same datasets, although our systems vary in a number of respects. When trained on the NC-v11 data, they show BLEU scores of 14.9 only with their BiLSTM baseline, 16.1 using additional dependency information, 15.6 using additional semantic roles and 15.8 taking both as additional knowledge. Using Full as the training data, the scores become 23.3, 23.9, 24.5 and 24.9, respectively. In addition to the different semantic representation being used (AMR vs SRL), Marcheggiani et al. (2018) laid graph convolutional network (GCN) (Kipf and Welling, 2017) layers on top of a bidirectional LSTM (BiLSTM) layer, and then concatenated layer outputs as the attention memory. GCN layers encode the semantic role information, while</p>
<p>Figure 6 . 5 :
65Test BLEU score of various sentence lengths</p>
<p>Ref: Carla Hairston sagte , sie war 15 und Lamm war 20 , als sie sich durch gemeinsame Freunde trafen . Dual2seq: Carla Hairston sagte , sie war 15 und Lamm war 20 , als sie sich durch gegenseitige Freunde trafen . Seq2seq: Carla Hirston sagte , sie sei 15 und Lamb 20 , als sie durch gegenseitige Freunde trafen .</p>
<p>:
Laut lokalen Medien treffen seitdem im kroatischen Tovarnik ständig Polizeifahrzeuge mit neuen Flüchtlingen ein . Dual2seq: Seither kommen die Polizeifahrzeuge nach denörtlichen Medien ständigüber neue Flüchtlinge in Kroatische Tavarnik . Seq2seq: Seitdem sind die Polizeiautos nach den lokalen Medien ständig neue Flüchtlinge in Kroatien Tavarnik .</p>
<p>trolled by bursts of sound waves Ref: Wissenschaftler haben Würmer mit genetisch veränderten Nervensystemen gezüchtet , die von Ausbrüchen von Schallwellen gesteuert werden können Dual2seq: Die Wissenschaftler haben die Würmer mit genetisch veränderten Nervensystemen gezüchtet , die durch Verbrennungen von Schallwellen kontrolliert werden können Seq2seq: Wissenschaftler haben sich mit genetisch modifiziertem Nervensystem gezüchtet , die durch Verbrennungen von Klangwellen gesteuert werden können Figure 6.6: Sample system outputs</p>
<p>Chapter 2: Background In this chapter, we briefly introduce previous deep learning models for encoding graphs. We first discuss applying RNNs and DAG networks on graphs, including their shortcomings. As work and our GRN-based model. For fair comparison, all three models are in the same framework, with the only difference being how to encode the graph representations. Finally, our comprehensive experiments show the superiority of our GRN.• Chapter 4: Graph Recurrent Network for n-ary Relation Extraction In this chapter, we extend our GRN from undirected and edge-unlabeled graphs (as in Chapter 3) to dependency graphs for solving a medical relation extraction (classification) problem. The goal is to determine whether a given medicine is effective on cancers caused by a type of mutation on a certain gene. Previous work has shown the effectiveness of incorporating rich syntactic and discourse information. The previous state of the art propose DAN networks by splitting the dependency graphs into two DAGs. Arguing that important information is lost by splitting the original graphs, we adapt GRN on the dependency graphs without destroying any graph structure.• Chapter 5: Graph Recurrent Network for AMR-to-text Generation In this chapter, we propose a graph-to-sequence model by extending GRN with an attention-based LSTM, and evaluate our model on AMR-to-text generation. AMR is a semantic formalism based on directed and edgelabelled graphs, and the task of AMR-to-text generation aims at recover-Graph Recurrent Network for Semantic NMT using AMR In this chapter, we further adapt our GRN-based graph-to-sequence model on AMR-based semantic neural machine translation. In particular, our model is extended by another Bi-LSTM encoder for modeling source sentences, as they are crucial for translation. The AMRs are automatically obtained by parsing the source sentences. Experiments show that using AMR outperforms other common syntactic and semantic representations, such as dependency and semantic role. We also show that GRN-based encoder is better than a Bi-LSTM encoder using linearized AMRs. This is consistent with the results of Chapter 5. and their relations (graph edges). Studying graphs is a very important topic in research, with one reason being that there have been numerous types of graphs in our daily lives, such as social networks, traffic network and molecules. Having a deeper understanding of these graphs can lead to better friend recommendation via social media, less traffic jam in busy cities and more effective medicine. However, existing probabilistic approaches and statistical models are not very successful on modeling graphs. It is possibly because graphs are usually very complex and large, making these approaches inefficient or not distinguishing enough. Recent years have witnessed the success of deep learning approaches, which have been shown to be more expressive and can benefit more from large-scale training. Also, recent advances on GPUs, especially the massive parallelism for tensor operations, make the deep neural-network models very efficient. As a result, there have been many attempts on investigating neural networks on dealing with graphs.a next step, we describe several types of graph neural networks (GNNs) </p>
<p>in more detail, then systematically compare GNNs with RNNs. Finally, </p>
<p>I point out one drawback of GNNs when encoding large-scale graphs, </p>
<p>before showing some existing solutions. </p>
<p>• Chapter 3: Graph Recurrent Network for Multi-hop Reading Compre-</p>
<p>hension In this chapter, I will first describe the multi-hop reading com-</p>
<p>prehension task, then propose a graph representation for each input doc-</p>
<p>ument. To encode the graphs for global reasoning, we introduce 3 models </p>
<p>for this task, including one RNN baseline, one baseline with DAG net-</p>
<p>ing the original sentence of a given AMR graph. In our extensive experi-</p>
<p>ments, our model show consistently better performance than a sequence-</p>
<p>to-sequence baseline with a Bi-LSTM encoder under the same configu-</p>
<p>ration, demonstrating the superiority of our GRN over other sequential </p>
<p>encoders. </p>
<p>• Chapter 6: • Chapter 7: Conclusion Finally, I summarize the main contributions of </p>
<p>this thesis, and propose some future research directions for further im-</p>
<p>proving our graph recurrent network. 
2 Background: Encoding graphs </p>
<p>with Neural Networks </p>
<p>Graphs are a kind of structure for representing a set of concepts (graph nodes) </p>
<p>Comparing RNNs, DAG networks have the advantage of preserving the original graph structures. Recently,Takase et al. (2016) applied a DAG network on encoding AMRs for headline generation, but no comparisons were made to contrast their model with any RNN-based models. Still, DAG networks are intuitively more suitable for encoding graphs than RNNs.However, DAG networks suffer from two major problems. First, they fail on cyclic graphs, as they require an exact and finite node order to be executed on. Second, sibling nodes can not incorporate the information of each other, as the encoding procedure can either be bottom-up or top-down, but not both at the same time. For the first problem, previous work introduces two solutions to adapt DAG networks on cyclic graphs, but to my knowledge, no solution has been available for the second problem. Still, both solutions for the first problem</p>
<p>Table 2 .
21: Comparison of several kinds of graph neural networks, where ms </p>
<p>and hs have the same meaning as Equation 2.2. W s and bs represent model </p>
<p>parameters. </p>
<p>on Common Crawl, and are not updated during training. For model hyperparameters, we set the graph state transition number as 3 according to development experiments. Each node takes information from at most 200 neighbors, where same and coref typed neighbors are kept first. The hidden vector sizes for both bidirectional LSTM and GRN layers are set to 300.Model 
Dev Test </p>
<p>GA w/ GRU (Dhingra et al., 2018) 
54.9 
-</p>
<p>GA w/ Coref-GRU (Dhingra et al., 2018) 56.0 59.3 </p>
<p>Local 
61.0 
-</p>
<p>Local-2L 
61.3 
-</p>
<p>Coref LSTM 
61.4 
-</p>
<p>Coref GRN 
61.4 
-</p>
<p>Fully-Connect-GRN 
61.3 
-</p>
<p>MHQA-GRN 
62.8 65.4 </p>
<h2>Leaderboard 1st [anonymized] †</h2>
<p>70.6 </p>
<h2>Leaderboard 2nd [anonymized] †</h2>
<p>67.6 </p>
<h2>3rd, Cao et al. (2018) †</h2>
<p>67.6 </p>
<p>Table 3 .
31: Main results (unmasked) on WikiHop, where systems with  † use </p>
<p>ELMo (Peters et al., 2018). </p>
<p>Table 3 .
31 shows the main comparison results 7 with existing work. GA w/ GRU</p>
<p>For our baselines, Local and Local-2L encode passages with a BiLSTM and a 2-layer BiLSTM, respectively, both only capture local information for each mention. We introduce Local-2L for better comparison, as our models have more parameters than Local. Coref LSTM is another baseline, encoding passages with coreference annotations by a DAG LSTM (Section 3.2.2). This is a reimplementation ofDhingra et al. (2018) based on our framework. Coref GRN is another baseline that encodes coreferences with GRN. It is for contrasting coreference DAGs with our evidence integration graphs. MHQA-GRN corresponds to our evidence integration approaches via graph encoding, adopting GRN for graph encoding.</p>
<p>Table 3.2: Ablation study on different types of edges using GRN as the graph encoder.than GA w/ GRU. On the other hand, MHQA-GRN is 1.8 points more accurate than Local.The comparisons below help to further pinpoint the advantage of our approach: MHQA-GRN is 1.4 points better than Coref GRN , while Coref GRN gives a comparable performance with Coref LSTM. Both comparisons show that our evidence graphs are the main reason for achieving the 1.8-points improvement, and it is mainly because our evidence graphs are better connected than coreference DAGs and are more suitable for integrating relevant evidence. Local-2LEdge type 
Dev </p>
<p>all types 
62.8 </p>
<p>w/o same 
61.9 </p>
<p>w/o coref 
61.7 </p>
<p>w/o window 62.4 </p>
<p>only same 
61.6 </p>
<p>only coref 
61.4 </p>
<p>only window 
61.1 </p>
<p>Table 3.2 shows the model performances when only one type of edges are used. None of the performances with single-typededges 
382 
308 
302 
123 </p>
<p>53 
31 
12 
1 
2322 </p>
<p>31 
5 
21 
13 
2347 </p>
<p>0 </p>
<p>20 </p>
<p>40 </p>
<p>60 </p>
<p>80 </p>
<p>100 </p>
<p>1 
2 
3 
4 </p>
<blockquote>
<p>4 
Infinity </p>
</blockquote>
<p>Percentage </p>
<p>Distance </p>
<p>All edges 
Only coref </p>
<p>The accuracies of Coref LSTM and MHQA-GRN on this subset are 61.1 and 63.8, respectively. Comparing with the performances on the whole devset (61.4 vs 62.8), the performance gap on this subset is increased by 1.3 points. This in-</p>
<p>In this dataset, web snippets (instead of passages as in WikiHop) are used for extracting answers. The baseline of Talmor and Berant (2018) (SimpQA) only uses a full question to query the web for obtaining relevant snippets, while their model (SplitQA) obtains snippets for both the full question and its sub-Model 
Dev Test </p>
<p>SimpQA 
30.6 
-</p>
<p>SplitQA 
31.1 
-</p>
<p>Local 
31.2 28.1 </p>
<p>MHQA-GRN w/ only same 
32.2 
-</p>
<p>MHQA-GRN 
33.2 30.1 </p>
<p>SplitQA w/ additional labeled data 35.6 34.2 </p>
<p>Table 3.3: Results on the ComplexWebQuestions dataset. </p>
<p>Table 4 . 1 :
41An example showing that tumors with L858E mutation in EGFR gene respond to gefitinib treatment.to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., </p>
<p>Table 4 .1.
4For simplicity, we omit edges of discourse relations. (b) Results after splitting </p>
<p>the graph into two DAGs. </p>
<p>nally, for each word, the hidden states of both directions are concatenated as </p>
<p>the final state. The bi-directional DAG LSTM model showed superior perfor-</p>
<p>mance over several strong baselines, such as tree-structured LSTM (Miwa and </p>
<p>Bansal, 2016), on a biomedical-domain benchmark. </p>
<p>However, the bidirectional DAG LSTM model suffers from several limita-</p>
<p>tions. First, important information can be lost when converting a graph into </p>
<p>two separate DAGs. For the example in </p>
<p>Dataset statistics. Avg. Tok. and Avg. Sent. are the average number of tokens and sentences, respectively. Cross is the percentage of instances that contain multiple sentences.parameters. Adam (Kingma and Ba, 2014) with a learning rate of 0.001 is used </p>
<p>as the optimizer, and the model that yields the best devset performance is se-</p>
<p>lected to evaluate on the test set. Dropout with rate 0.3 is used during training. </p>
<p>Both training and evaluation are conducted using a Tesla K20X GPU. 
Data 
Avg. Tok. Avg. Sent. Cross (%) </p>
<p>TERNARY 
73.9 
2.0 
70.1% </p>
<p>BINARY 
61.0 
1.8 
55.2% </p>
<p>Table 4.2: </p>
<p>tions.Single represents experiments only on instances within single sentences, while Cross represents experiments on all instances. *: significant at p &lt; 0.01 and backward. All executes our graph state model on original graphs.Model 
Single Cross </p>
<p>Quirk and Poon (2017) 
74.7 
77.7 </p>
<p>Peng et al. (2017) -EMBED 76.5 
80.6 </p>
<p>Peng et al. (2017) -FULL 
77.9 
80.7 </p>
<h2>+ multi-task</h2>
<p>82.0 </p>
<p>Bidir DAG LSTM 
75.6 
77.3 </p>
<p>GRN 
80.3<em> 83.2</em> </p>
<p>Table 4.3: Average test accuracies for TERNARY drug-gene-mutation interac-</p>
<p>Table 4 .
43 compares our model with the bidirectional DAG baseline and the state-of-the-art results on this dataset, where EMBED and FULL have been gression classifier and features derived from shortest paths between all entity pairs. Bidir DAG LSTM is our bidirectional DAG LSTM baseline, and GRN is our GRN-based model.briefly introduced in Section 4.3.3. +multi-task applies joint training of both </p>
<p>ternary (drug-gene-mutation) relations and their binary (drug-mutation) sub-</p>
<p>Table 4 .
44 shows the training and decoding time of both the </p>
<p>baseline and our model. Our model is 8 to 10 times faster than the baseline </p>
<p>Table 4 .
42, we can see that the average number of tokens for the ternary-relation data is 74, which means that the baseline model has to execute 74 recurrent transition steps for calculating a hidden state for each input word. On the other hand, our model only performs 5 state transitions, and calculations between each pair of nodes for one transition are parallelizable. This accounts for the better efficiency of our model. on different sentence lengths. We can see that GRN and Bidir DAG LSTM showAccuracy against sentence length Figure 4.4 (a) shows the test accuracies </p>
<p>using two examples. GRN makes the correct predictions for both cases, whileModel 
Single Cross </p>
<p>Quirk and Poon (2017) 
73.9 
75.2 </p>
<p>Miwa and Bansal (2016) 
75.9 
75.9 </p>
<p>Peng et al. (2017) -EMBED 74.3 
76.5 </p>
<p>Peng et al. (2017) -FULL 
75.6 
76.7 </p>
<h2>+ multi-task</h2>
<p>78.5 </p>
<p>Bidir DAG LSTM 
76.9 
76.4 </p>
<p>GRN 
83.5<em> 83.6</em> </p>
<p>Table 4.5: Average test accuracies in five-fold cross-validation for BINARY drug-</p>
<p>mutation interactions. </p>
<p>Table 4 .
4Similar to the ternary relation extraction experiments, GRN outperforms all the other systems with a large margin, which shows that the message passing graph LSTM is better at encoding rich linguistic knowledge within the input graphs. Binary relations being easier, both GRN and Bidir DAG LSTM show increased or similar performances compared with the ternary relation experiments. On this set, our bidirectional DAG LSTM model is comparable to FULL5 shows the results, where Miwa and Bansal (2016) is </p>
<p>a state-of-the-art model using sequential and tree-structured LSTMs to jointly </p>
<p>capture linear and dependency contexts for relation extraction. Other models </p>
<p>have been introduced in Section 4.6.4. </p>
<p>Table 4 .
46 shows accuracies on multi-class relation extraction, which makes the task more ambiguous compared with binary relation extraction. The results show similar comparisons with the binary relation extraction results. However, the performance gaps between GRN and Bidir DAG LSTM dramaticallyModel 
TERNARY BINARY </p>
<p>Bidir DAG LSTM 
51.7 
50.7 </p>
<p>GRN 
71.1<em> 
71.7</em> </p>
<p>Table 4.6: Average test accuracies for multi-class relation extraction with all </p>
<p>instances ("Cross"). </p>
<p>increase, showing the superiority of GRN over Bidir DAG LSTM in utilizing </p>
<p>context information. </p>
<p>Table 5 .
51: DEV BLEU scores and decoding times. </p>
<p>not updated during training. Following existing work, we evaluate the results </p>
<p>with the BLEU metric </p>
<p>Table 5 .
52 compares our final results with existing work. MSeq2seq+Anon (Kon</p>
<p>Data We use the WMT16 3 English-to-German dataset, which contains around 4.5 million sentence pairs for training. In addition, we use a subset of the full dataset (News Commentary v11 (NC-v11), containing around 243 thousand sentence pairs) for development and additional experiments. For allTable 6.1: Statistics of the dataset. Numbers of tokens are after BPE processing.Table 6.2: Sizes of vocabularies. EN-ori represents original English sentences without BPE. experiments, we use newstest2013 and newstest2016 as the development and test sets, respectively.To preprocess the data, the tokenizer from Moses 4 is used to tokenize both the English and German sides. The training sentence pairs where either side is longer than 50 words are filtered out after tokenization. To deal with rare and compound words, byte-pair encoding (BPE) 5(Sennrich et al., 2016) is applied to both sides. In particular, 8000 and 16000 BPE merges are used on the News Commentary v11 subset and the full training set, respectively. On the other hand, JAMR 6(Flanigan et al., 2016a) is adopted to parse the English sentences into AMRs before BPE is applied. The statistics of the training data and vocabularies after preprocessing are shown inTable 6.1 and 6.2, respectively.For the experiments with the full training set, we used the top 40K of the AMR 4 http://www.statmt.org/moses/ 5 https://github.com/rsennrich/subword-nmt 6 https://github.com/jflanigan/jamr vocabulary, which covers more than 99.6% of the training set.For our dependency-based and SRL-based baselines (which will be introduced in Baseline systems), we choose Stanford CoreNLP(Manning et al., 2014) and IBM SIRE to generate dependency trees and semantic roles, respectively. Since both dependency trees and semantic roles are based on the original English sentences without BPE, we used the top 100K frequent English words, which cover roughly 99.0% of the training set. We use the Adam optimizer(Kingma and Ba, 2014) with a learning rate of 0.0005. The batch size is set to 128. Between layers, we apply dropout with a probability of 0.2. The best model is picked based on the cross-entropy loss on the development set. For model hyperparameters, we set the graph state transition number to 10 according to development experiments. Each node takes information from at most 6 neighbors. BLEU6.6.1 Setup </p>
<p>Dataset </p>
<h1>Sent. #Tok. (EN) #Tok. (DE)</h1>
<p>NC-v11 
226K 
6.4M 
7.3M </p>
<p>Full 
4.17M 
109M 
118M </p>
<p>News2013 3000 
84.7K 
95.6K </p>
<p>News2016 2999 
88.1K 
98.8K </p>
<p>Dataset EN-ori 
EN 
AMR 
DE </p>
<p>NC-v11 79.8K 
8.4K 36.6K 8.3K </p>
<p>Full 
874K 19.3K 403K 19.1K </p>
<p>Hyperparameters </p>
<p>For experiments with the NC-v11 subset, both word embedding and hidden vector sizes are set to 500, and the models are trained for at most 30 epochs. For experiments with full training set, the word embedding and hidden state sizes are set to 800, and our models are trained for at most 10 epochs. For all systems, the word embeddings are randomly initialized and updated during training.Baseline systemsWe compare our model with the following systems. represents our attention-based LSTM baseline ( §6.3), and Dual2seq is our model, which takes both a sequential and a graph encoder and adopts a doubly-attentive decoder ( §6.4). To show the merit of AMR, we further contrast our model with the following baselines, all of which adopt the same doublyattentive framework with a BiLSTM for encoding BPE-segmented source sen-tences: Dual2seq-LinAMR uses another BiLSTM for encoding linearized AMRs.Dual2seq-Dep and Dual2seq-SRL adopt our graph recurrent network to encode original source sentences with dependency and semantic role annotations, respectively. The three baselines are useful for contrasting different methods of encoding AMRs and for comparing AMRs with other popular structural information for NMT.Seq2seq </p>
<p>The state transition is not applicable to Seq2seq, so we draw a dashed line to represent its performance.Our Dual2seq shows consistent performance improvement by increasing the transition number both from 1 to 5 (roughly +1.3 BLEU points) and from 5 to 10 (roughly 0.2 BLEU points). The former shows greater improvement than the latter, showing that the performance starts to converge after 5 transition steps.Further increasing transition steps from 10 to 12 gives a slight performance drop. We set the number of state transition steps to 10 for all experiments according to these observations. On the other hand, Dual2seq (self) shows only small improvements by increasing the state transition number, and it does not perform better than Seq2seq. Both results show that the performance gains of Dual2seq are not due to an increased number of parameters.Table 6.3: TEST performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant(Koehn, 2004) result (p &lt; 0.01) over Seq2seq. ↓ indicates the lower the better.System 
NC-V11 
FULL </p>
<p>BLEU TER↓ Meteor BLEU TER↓ Meteor </p>
<p>OpenNMT-tf 
15.1 0.6902 0.3040 
24.3 0.5567 0.4225 </p>
<p>Transformer-tf 
17.1 0.6647 0.3578 
25.1 0.5537 0.4344 </p>
<p>Seq2seq 
16.0 0.6695 0.3379 
23.7 0.5590 0.4258 </p>
<p>Dual2seq-LinAMR 17.3 0.6530 0.3612 
24.0 0.5643 0.4246 </p>
<p>Duel2seq-SRL 
17.2 0.6591 0.3644 
23.8 0.5626 0.4223 </p>
<p>Dual2seq-Dep 
17.8 0.6516 0.3673 
25.0 0.5538 0.4328 </p>
<p>Dual2seq 
19.2<em> 0.6305 0.3840 
25.5</em> 0.5480 0.4376 </p>
<p>Table 6 .
63 shows the TEST BLEU, TER and Meteor scores of all systems trained on the small-scale News Commentary v11 subset or the large-scale full set. is consistently better than the other systems under all three metrics, showing the effectiveness of the semantic information provided by AMR. Especially, Dual2seq is better than both OpenNMT-tf and Transformer-tf . The recurrent graph state transition of Dual2seq is similar to Transformer in that it iteratively incorporates global information. The improvement of Dual2seq over Transformer-tf undoubtedly comes from the use of AMRs, which provide complementary information to the textual inputs of the source language.Dual2seq </p>
<p>Influence of AMR parsing accuracyTo analyze the influence of AMR parsing on our model performance, we further evaluate on a test set where the gold AMRs for the English side are available. In particular, we choose the Little Prince corpus, which contains 1562 sentences with gold AMR annotations. 10Table 6.4: BLEU scores of Dual2seq on the little prince data, when gold or automatic AMRs are available.Since there are no parallel German sentences, we take a German-version Little Prince novel, and then perform manual sentence alignment. Taking the whole Little Prince corpus as the test set, we measure the influence of AMR parsing accuracy by evaluating on the test set when gold or automatically-parsed AMRs are available. The automatic AMRs are generated by parsing the English sentences with JAMR.AMR Anno. BLEU </p>
<p>Automatic 
16.8 </p>
<p>Gold 
17.5* </p>
<p>Table 6 .
64 shows the BLEU scores of our Dual2seq model taking gold or automatic AMRs as inputs. Not listed inTable 6.4, Seq2seq achieves a BLEU score of 15.6, which is 1.2 BLEU points lower than using automatic AMR information. The improvement from automatic AMR to gold AMR (+0.7 BLEU) is significant, which shows that the translation quality of our model can be further improved with an increase of AMR parsing accuracy. However, the BLEU score with gold AMR does not indicate the potentially best performance that our model can achieve. The primary reason is that even though the test set is coupled with gold AMRs, the training set is not. Trained with automatic AMRs, our model can learn to selectively trust the AMR structure. An additional reason is the domain difference: the Little Prince data are in the literary domain while our training data are in the news domain. There can be a further performance gain if the accuracy of the automatic AMRs on the training set is improved.
https://developers.google.com/freebase/ 2 https://wiki.dbpedia.org/
http://qangaroo.cs.ucl.ac.uk/leaderboard.html
Only the forward direction is shown for space consideration
We adopt a standard matching method, as our focus is the effectiveness of evidence integration. We leave investigating other approaches(Luong et al., 2015; as future work.
All candidates form a subset of all entities (E). 5 Model architectures are selected according to dev results.
ber, 1997) is adopted, where a cell vector c k t is taken to record memory for hidden state s k t :i k t = σ(W i m k t + b i ) o k t = σ(W o m k t + b o ) f k t = σ(W f m k t + b f ) u k t = σ(W u m k t + b u ) c k t = f k t c k t−1 + i k t u k t s k t = o k t tanh(c k t ), (3.13)where c k t is the cell vector to record memory for s k t , and i k t , o k t and f k t are the input, output and forget gates, respectively. W x and b x (x ∈ {i, o, f , u}) are model parameters. In the remaining of this thesis, I will use the symbol LSTM to represent Equation 3.13. m k t is the sum of the neighborhood hidden states for the node k 6 :(3.14)m k t = ∑ i∈Ω(k) s i t−1Ω(k) represents the set of all neighbors of k .Message passing To describe GRN using the message parsing framework shown in Equations 2.1 and 2.2 (Section 2.2), m k t represents the message for node k at step t. It is first aggregated by summing up the hidden states of all neighbors of k , before being applied to update s k t with an LSTM step.Recurrent steps Using the above state transition mechanism, information from each node propagates to all its neighboring nodes after each step. Therefore, for the worst case where the input graph is a chain of nodes, the maximum6  We tried distinguishing neighbors by different types of edges, but it does not improve the performance.
As shown inFigure 3.1, each question has a subject, a relation and asks for the object.
For more information please refer Section 3.3 ofPeng et al. (2017).
The dataset is available at http://hanover.azurewebsites.net.
The released data has been separated into 5 portions, and we follow the exact split.
p &lt; 0.01 using t-test. For the remaining of this paper, we use the same measure for statistical significance.
As shown in Figure 4.1, a directional DAG LSTM propagates information according to the edge directions.
The diameter of single-node graphs is 0.
It was 33.0 at submission, and has been improved.
http://www.statmt.org/moses/
Graph Recurrent Network forSemantic NMT using AMRIt is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (short for abstract meaning representation) on NMT. Experiments on a standard English-to-German dataset show that incorporating AMR as additional knowledge can significantly improve a strong attentionbased sequence-to-sequence neural translation model.6.1 IntroductionIt is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well.Leveraging syntax for neural machine translation (NMT) has been an ac-
http://www.statmt.org/wmt16/translation-task.html
https://github.com/OpenNMT/OpenNMT-tf
AMRs can contain multi-word concepts, such as New York City, but they are in the textual input.
https://amr.isi.edu/download.html</p>
<p>Lin: possible :polarity -:arg1 ( look-over :arg0 we :arg1 ( account :arg1 ( war :arg1 ( country :wiki japan :name ( name :op1 japan ) ) :time previous :arg1-of. mod oldLin: possible :polarity -:arg1 ( look-over :arg0 we :arg1 ( account :arg1 ( war :arg1 ( country :wiki japan :name ( name :op1 japan ) ) :time previous :arg1-of ( call :mod so ) ) :mod old ) )</p>
<p>so-called anti-japanese war . S2S: we can n't be able to account the past drawn out of japan 's entire war . G2S: we can n't be able to do old accounts of the previous and so called japan war. G2S+CP: we can n't look-over the old accounts of the previous so called war on japan. Ref: we can n't look over the old accounts of the previous. p / provide-01 :ARG0 (a / agree-01Ref: we can n't look over the old accounts of the previous so-called anti-japanese war . S2S: we can n't be able to account the past drawn out of japan 's entire war . G2S: we can n't be able to do old accounts of the previous and so called japan war. G2S+CP: we can n't look-over the old accounts of the previous so called war on japan . (p / provide-01 :ARG0 (a / agree-01)</p>
<p>:ARG1 (a2 / and :op1 (s / staff. c / center :mod (r / research-01:ARG1 (a2 / and :op1 (s / staff :prep-for (c / center :mod (r / research-01)))</p>
<p>Lin: provide :arg0 agree :arg1 ( and :op1 ( staff. 2fund :prep-for centerLin: provide :arg0 agree :arg1 ( and :op1 ( staff :prep-for ( center :mod research ) ) :op2 ( fund :prep-for center ) )</p>
<p>Ref: the agreement will provide staff and funding for the research center. S2S: agreed to provide research and institutes in the center . G2S: the agreement provides the staff of research centers and funding. Ref: the agreement will provide staff and funding for the research center . S2S: agreed to provide research and institutes in the center . G2S: the agreement provides the staff of research centers and funding .</p>
<p>G2S+CP: the agreement provides the staff of the research center and the funding. Table 5.3: Example system outputsG2S+CP: the agreement provides the staff of the research center and the funding . Table 5.3: Example system outputs.</p>
<p>Towards string-to-tree neural machine translation. Roee Aharoni, Yoav Goldberg, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17). the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17)Aharoni, Roee and Yoav Goldberg. 2017. Towards string-to-tree neural ma- chine translation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17), pages 132-140.</p>
<p>Broad-coverage CCG semantic parsing with AMR. Yoav Artzi, Kenton Lee, Luke Zettlemoyer, Conference on Empirical Methods in Natural Language Processing (EMNLP-15). Artzi, Yoav, Kenton Lee, and Luke Zettlemoyer. 2015. Broad-coverage CCG semantic parsing with AMR. In Conference on Empirical Methods in Natural Language Processing (EMNLP-15), pages 1699-1710.</p>
<p>Neural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsICLRBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural ma- chine translation by jointly learning to align and translate. In Proceedings of the International Conference on Learning Representations (ICLR).</p>
<p>Modality and negation in SIMT use of modality and negation in semantically-informed syntactic MT. Kathryn Baker, Michael Bloodgood, J Bonnie, Chris Dorr, Nathaniel W Callison-Burch, Christine Filardo, Lori Piatko, Scott Levin, Miller, Computational Linguistics. 382Baker, Kathryn, Michael Bloodgood, Bonnie J Dorr, Chris Callison-Burch, Nathaniel W Filardo, Christine Piatko, Lori Levin, and Scott Miller. 2012. Modality and negation in SIMT use of modality and negation in semantically-informed syntactic MT. Computational Linguistics, 38(2):411- 438.</p>
<p>Abstract meaning representation for sembanking. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, Nathan Schneider, Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse. the 7th Linguistic Annotation Workshop and Interoperability with DiscourseBanarescu, Laura, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Pro- ceedings of the 7th Linguistic Annotation Workshop and Interoperability with Dis- course, pages 178-186.</p>
<p>Graph convolutional encoders for syntax-aware neural machine translation. Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, Khalil Simaan, Conference on Empirical Methods in Natural Language Processing (EMNLP-17). Bastings, Joost, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Simaan. 2017. Graph convolutional encoders for syntax-aware neural ma- chine translation. In Conference on Empirical Methods in Natural Language Pro- cessing (EMNLP-17).</p>
<p>Graph-to-sequence learning using gated graph neural networks. Daniel Beck, Gholamreza Haffari, Trevor Cohn, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18). the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18)Beck, Daniel, Gholamreza Haffari, and Trevor Cohn. 2018. Graph-to-sequence learning using gated graph neural networks. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18), pages 273- 283.</p>
<p>Michael Boratko, Harshit Padigela, Divyendra Mikkilineni, Pritish Yuvraj, Rajarshi Das, Andrew Mccallum, Maria Chang, Achille Fokoue-Nkoutche, Pavan Kapanipathi, Nicholas Mattei, arXiv:1806.00358A systematic classification of knowledge, reasoning, and context within the ARC dataset. arXiv preprintBoratko, Michael, Harshit Padigela, Divyendra Mikkilineni, Pritish Yuvraj, Ra- jarshi Das, Andrew McCallum, Maria Chang, Achille Fokoue-Nkoutche, Pa- van Kapanipathi, Nicholas Mattei, et al. 2018. A systematic classification of knowledge, reasoning, and context within the ARC dataset. arXiv preprint arXiv:1806.00358.</p>
<p>Robust incremental neural semantic graph parsing. Jan Buys, Phil Blunsom, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17). the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17)Buys, Jan and Phil Blunsom. 2017. Robust incremental neural semantic graph parsing. In Proceedings of the 55th Annual Meeting of the Association for Compu- tational Linguistics (ACL-17), pages 1215-1226.</p>
<p>Question answering by reasoning across documents with graph convolutional networks. Nicola Cao, Wilker De, Ivan Aziz, Titov, arXiv:1808.09920arXiv preprintCao, Nicola De, Wilker Aziz, and Ivan Titov. 2018. Question answering by rea- soning across documents with graph convolutional networks. arXiv preprint arXiv:1808.09920.</p>
<p>Reading wikipedia to answer open-domain questions. Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17). the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17)Chen, Danqi, Adam Fisch, Jason Weston, and Antoine Bordes. 2017a. Reading wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17), pages 1870- 1879.</p>
<p>Improved neural machine translation with a syntax-aware encoder and decoder. Huadong Chen, Shujian Huang, David Chiang, Jiajun Chen, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17). the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17)Chen, Huadong, Shujian Huang, David Chiang, and Jiajun Chen. 2017b. Im- proved neural machine translation with a syntax-aware encoder and de- coder. In Proceedings of the 55th Annual Meeting of the Association for Com- putational Linguistics (ACL-17), pages 1936-1945.</p>
<p>FastGCN: fast learning with graph convolutional networks via importance sampling. Jie Chen, Tengfei Ma, Cao Xiao, International Conference on Learning Representations. ICLRChen, Jie, Tengfei Ma, and Cao Xiao. 2018a. FastGCN: fast learning with graph convolutional networks via importance sampling. In International Conference on Learning Representations (ICLR).</p>
<p>Syntax-directed attention for neural machine translation. Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita, Tiejun Zhao, Proceedings of the National Conference on Artificial Intelligence (AAAI-18). the National Conference on Artificial Intelligence (AAAI-18)Chen, Kehai, Rui Wang, Masao Utiyama, Eiichiro Sumita, and Tiejun Zhao. 2018b. Syntax-directed attention for neural machine translation. In Proceed- ings of the National Conference on Artificial Intelligence (AAAI-18).</p>
<p>Overview of muc-7/met-2. Nancy A Chinchor, Science Applications International Corp. Technical reportChinchor, Nancy A. 1998. Overview of muc-7/met-2. Technical report, Science Applications International Corp San Diego CA.</p>
<p>Learning phrase representations using RNN encoder-decoder for statistical machine translation. Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, Conference on Empirical Methods in Natural Language Processing (EMNLP-14). Doha, QatarCho, Kyunghyun, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine trans- lation. In Conference on Empirical Methods in Natural Language Processing (EMNLP-14), pages 1724-1734. Doha, Qatar.</p>
<p>Simple and effective multiparagraph reading comprehension. Christopher Clark, Matt Gardner, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18). the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18)Clark, Christopher and Matt Gardner. 2018. Simple and effective multi- paragraph reading comprehension. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18).</p>
<p>Convolutional neural networks on graphs with fast localized spectral filtering. Michaël Defferrard, Xavier Bresson, Pierre Vandergheynst, Advances in neural information processing systems. Defferrard, Michaël, Xavier Bresson, and Pierre Vandergheynst. 2016. Convo- lutional neural networks on graphs with fast localized spectral filtering. In Advances in neural information processing systems, pages 3844-3852.</p>
<p>Meteor universal: Language specific translation evaluation for any target language. Michael Denkowski, Alon Lavie, Proceedings of the Ninth Workshop on Statistical Machine Translation. the Ninth Workshop on Statistical Machine TranslationDenkowski, Michael and Alon Lavie. 2014. Meteor universal: Language spe- cific translation evaluation for any target language. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 376-380.</p>
<p>Neural models for reasoning over multiple mentions using coreference. Bhuwan Dhingra, Qiao Jin, Zhilin Yang, William Cohen, Ruslan Salakhutdinov, Proceedings of the 2018 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-18). the 2018 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-18)Dhingra, Bhuwan, Qiao Jin, Zhilin Yang, William Cohen, and Ruslan Salakhut- dinov. 2018. Neural models for reasoning over multiple mentions using coreference. In Proceedings of the 2018 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-18), pages 42-48.</p>
<p>Gated-attention readers for text comprehension. Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William Cohen, Ruslan Salakhutdinov, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17). the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17)Dhingra, Bhuwan, Hanxiao Liu, Zhilin Yang, William Cohen, and Ruslan Salakhutdinov. 2017a. Gated-attention readers for text comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Lin- guistics (ACL-17), pages 1832-1846.</p>
<p>QUASAR: Datasets for question answering by search and reading. Bhuwan Dhingra, Kathryn Mazaitis, William W Cohen, arXiv:1707.03904arXiv preprintDhingra, Bhuwan, Kathryn Mazaitis, and William W Cohen. 2017b. QUASAR: Datasets for question answering by search and reading. arXiv preprint arXiv:1707.03904.</p>
<p>SearchQA: A new q&amp;a dataset augmented with context from a search engine. Matthew Dunn, Levent Sagun, Mike Higgins, Ugur Guney, Volkan Cirik, Kyunghyun Cho, arXiv:1704.05179arXiv preprintDunn, Matthew, Levent Sagun, Mike Higgins, Ugur Guney, Volkan Cirik, and Kyunghyun Cho. 2017. SearchQA: A new q&amp;a dataset augmented with con- text from a search engine. arXiv preprint arXiv:1704.05179.</p>
<p>Convolutional networks on graphs for learning molecular fingerprints. David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, Ryan P Adams, Advances in neural information processing systems. Duvenaud, David K, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. 2015. Convolu- tional networks on graphs for learning molecular fingerprints. In Advances in neural information processing systems, pages 2224-2232.</p>
<p>CMU at semeval-2016 task 8: Graph-based AMR parsing with infinite ramp loss. Jeffrey Flanigan, Chris Dyer, Noah A Smith, Jaime Carbonell, Proceedings of SemEval. SemEvalFlanigan, Jeffrey, Chris Dyer, Noah A. Smith, and Jaime Carbonell. 2016a. CMU at semeval-2016 task 8: Graph-based AMR parsing with infinite ramp loss. In Proceedings of SemEval, pages 1202-1206.</p>
<p>Generation from abstract meaning representation using tree transducers. Jeffrey Flanigan, Chris Dyer, Noah A Smith, Jaime Carbonell, Proceedings of the 2016 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-16). the 2016 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-16)Flanigan, Jeffrey, Chris Dyer, Noah A. Smith, and Jaime Carbonell. 2016b. Gen- eration from abstract meaning representation using tree transducers. In Pro- ceedings of the 2016 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-16), pages 731-739.</p>
<p>A discriminative graph-based parser for the abstract meaning representation. Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer, Noah A Smith, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL-14). the 52nd Annual Meeting of the Association for Computational Linguistics (ACL-14)Flanigan, Jeffrey, Sam Thomson, Jaime Carbonell, Chris Dyer, and Noah A. Smith. 2014. A discriminative graph-based parser for the abstract meaning representation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL-14), pages 1426-1436.</p>
<p>Beyond nombank: A study of implicit arguments for nominal predicates. Matthew Gerber, Joyce Chai, Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10). the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10)Gerber, Matthew and Joyce Chai. 2010. Beyond nombank: A study of implicit arguments for nominal predicates. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10).</p>
<p>Improved relation extraction with feature-rich compositional embedding models. Matthew R Gormley, Yu Mo, Mark Dredze, Conference on Empirical Methods in Natural Language Processing. 15Gormley, Matthew R., Mo Yu, and Mark Dredze. 2015. Improved relation ex- traction with feature-rich compositional embedding models. In Conference on Empirical Methods in Natural Language Processing (EMNLP-15).</p>
<p>AMR dependency parsing with a typed semantic algebra. Jonas Groschwitz, Matthias Lindemann, Meaghan Fowlie, Mark Johnson, Alexander Koller, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18). the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18)Groschwitz, Jonas, Matthias Lindemann, Meaghan Fowlie, Mark Johnson, and Alexander Koller. 2018. AMR dependency parsing with a typed semantic algebra. In Proceedings of the 56th Annual Meeting of the Association for Compu- tational Linguistics (ACL-18), pages 1831-1841.</p>
<p>RIGOTRIO at SemEval-2017 Task 9: Combining Machine Learning and Grammar Engineering for AMR Parsing and Generation. Normunds Gruzitis, Didzis Gosko, Guntis Barzdins, Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017). the 11th International Workshop on Semantic Evaluation (SemEval-2017)CanadaVancouverGruzitis, Normunds, Didzis Gosko, and Guntis Barzdins. 2017. RIGOTRIO at SemEval-2017 Task 9: Combining Machine Learning and Grammar Engi- neering for AMR Parsing and Generation. In Proceedings of the 11th Interna- tional Workshop on Semantic Evaluation (SemEval-2017), pages 924-928. Van- couver, Canada.</p>
<p>Incorporating copying mechanism in sequence-to-sequence learning. Jiatao Gu, Zhengdong Lu, Hang Li, O K Victor, Li, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL-16). the 54th Annual Meeting of the Association for Computational Linguistics (ACL-16)Berlin, GermanyGu, Jiatao, Zhengdong Lu, Hang Li, and Victor O.K. Li. 2016. Incorporating copying mechanism in sequence-to-sequence learning. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL-16), pages 1631-1640. Berlin, Germany.</p>
<p>Pointing the unknown words. Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, Yoshua Bengio, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL-16). the 54th Annual Meeting of the Association for Computational Linguistics (ACL-16)Berlin, GermanyGulcehre, Caglar, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio. 2016. Pointing the unknown words. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL-16), pages 140- 149. Berlin, Germany.</p>
<p>Better transition-based AMR parsing with a refined search space. Zhijiang Guo, Wei Lu, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18). the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18)Guo, Zhijiang and Wei Lu. 2018. Better transition-based AMR parsing with a refined search space. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18), pages 1712-1722.</p>
<p>Inductive representation learning on large graphs. Will Hamilton, Zhitao Ying, Jure Leskovec, Advances in Neural Information Processing Systems. Hamilton, Will, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pages 1024-1034.</p>
<p>Collective entity linking in web text: a graph-based method. Xianpei Han, Le Sun, Jun Zhao, Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. the 34th international ACM SIGIR conference on Research and development in Information RetrievalACMHan, Xianpei, Le Sun, and Jun Zhao. 2011. Collective entity linking in web text: a graph-based method. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 765-774. ACM.</p>
<p>Mikael Henaff, Joan Bruna, Yann Lecun, arXiv:1506.05163Deep convolutional networks on graph-structured data. arXiv preprintHenaff, Mikael, Joan Bruna, and Yann LeCun. 2015. Deep convolutional net- works on graph-structured data. arXiv preprint arXiv:1506.05163.</p>
<p>Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, O Diarmuid, Sebastian Séaghdha, Marco Padó, Lorenza Pennacchiotti, Stan Romano, Szpakowicz, Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions. the Workshop on Semantic Evaluations: Recent Achievements and Future DirectionsHendrickx, Iris, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid O Séaghdha, Sebastian Padó, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2009. Semeval-2010 task 8: Multi-way classification of se- mantic relations between pairs of nominals. In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions.</p>
<p>Teaching machines to read and comprehend. Karl Hermann, Tomas Moritz, Edward Kocisky, Lasse Grefenstette, Will Espeholt, Mustafa Kay, Phil Suleyman, Blunsom, Advances in Neural Information Processing Systems. Hermann, Karl Moritz, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems, pages 1693-1701.</p>
<p>The goldilocks principle: Reading children's books with explicit memory representations. Felix Hill, Antoine Bordes, Sumit Chopra, Jason Weston, arXiv:1511.02301arXiv preprintHill, Felix, Antoine Bordes, Sumit Chopra, and Jason Weston. 2015. The goldilocks principle: Reading children's books with explicit memory repre- sentations. arXiv preprint arXiv:1511.02301.</p>
<p>Long short-term memory. Sepp Hochreiter, Jürgen Schmidhuber, Neural computation. 98Hochreiter, Sepp and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780.</p>
<p>Adaptive sampling towards fast graph representation learning. Wenbing Huang, Tong Zhang, Yu Rong, Junzhou Huang, Advances in Neural Information Processing Systems. Huang, Wenbing, Tong Zhang, Yu Rong, and Junzhou Huang. 2018. Adaptive sampling towards fast graph representation learning. In Advances in Neural Information Processing Systems, pages 4563-4572.</p>
<p>A challenge set approach to evaluating machine translation. Pierre Isabelle, Colin Cherry, George Foster, Conference on Empirical Methods in Natural Language Processing (EMNLP-17). Isabelle, Pierre, Colin Cherry, and George Foster. 2017. A challenge set ap- proach to evaluating machine translation. In Conference on Empirical Methods in Natural Language Processing (EMNLP-17), pages 2486-2496.</p>
<p>Question answering over knowledge base using factual memory networks. Sarthak Jain, Proceedings of the NAACL Student Research Workshop. the NAACL Student Research WorkshopJain, Sarthak. 2016. Question answering over knowledge base using factual memory networks. In Proceedings of the NAACL Student Research Workshop, pages 109-115.</p>
<p>A systematic exploration of the feature space for relation extraction. Jing Jiang, Chengxiang Zhai, Proceedings of the 2015 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-15). the 2015 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-15)Jiang, Jing and ChengXiang Zhai. 2007. A systematic exploration of the feature space for relation extraction. In Proceedings of the 2015 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-15).</p>
<p>Semantics-based machine translation with hyperedge replacement grammars. Bevan Jones, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, Kevin Knight, Proceedings of the International Conference on Computational Linguistics (COLING-12). the International Conference on Computational Linguistics (COLING-12)Jones, Bevan, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, and Kevin Knight. 2012a. Semantics-based machine translation with hyperedge replace- ment grammars. In Proceedings of the International Conference on Computational Linguistics (COLING-12), pages 1359-1376.</p>
<p>Semantics-based machine translation with hyperedge replacement grammars. Bevan Jones, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, Kevin Knight, Proceedings of the International Conference on Computational Linguistics (COLING-12). the International Conference on Computational Linguistics (COLING-12)Jones, Bevan, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, and Kevin Knight. 2012b. Semantics-based machine translation with hyperedge replace- ment grammars. In Proceedings of the International Conference on Computational Linguistics (COLING-12), pages 1359-1376.</p>
<p>Adam: A method for stochastic optimization. Diederik Kingma, Jimmy Ba, arXiv:1412.6980arXiv preprintKingma, Diederik and Jimmy Ba. 2014. Adam: A method for stochastic opti- mization. arXiv preprint arXiv:1412.6980.</p>
<p>Semi-supervised classification with graph convolutional networks. Thomas N Kipf, Max Welling, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsICLRKipf, Thomas N. and Max Welling. 2017. Semi-supervised classification with graph convolutional networks. In Proceedings of the International Conference on Learning Representations (ICLR).</p>
<p>Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, Alexander M Rush, arXiv:1701.02810OpenNMT: Open-Source Toolkit for Neural Machine Translation. arXiv preprintKlein, Guillaume, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M. Rush. 2017. OpenNMT: Open-Source Toolkit for Neural Machine Translation. arXiv preprint arXiv:1701.02810.</p>
<p>Statistical significance tests for machine translation evaluation. Philipp Koehn, Conference on Empirical Methods in Natural Language Processing (EMNLP-04). Koehn, Philipp. 2004. Statistical significance tests for machine translation eval- uation. In Conference on Empirical Methods in Natural Language Processing (EMNLP-04), pages 388-395.</p>
<p>Statistical phrasebased translation. Philipp Koehn, Franz Josef Och, Daniel Marcu, Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03). the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03)Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase- based translation. In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03), pages 48- 54.</p>
<p>Neural AMR: Sequence-to-sequence models for parsing and generation. Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, Luke Zettlemoyer, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17). the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17)Vancouver, CanadaKonstas, Ioannis, Srinivasan Iyer, Mark Yatskar, Yejin Choi, and Luke Zettle- moyer. 2017. Neural AMR: Sequence-to-sequence models for parsing and generation. In Proceedings of the 55th Annual Meeting of the Association for Com- putational Linguistics (ACL-17), pages 146-157. Vancouver, Canada.</p>
<p>Imagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in neural information processing systems. Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet clas- sification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097-1105.</p>
<p>Sheffield at semeval-2017 task 9: Transition-based language generation from AMR. Gerasimos Lampouras, Andreas Vlachos, Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017). the 11th International Workshop on Semantic Evaluation (SemEval-2017)Vancouver, CanadaLampouras, Gerasimos and Andreas Vlachos. 2017. Sheffield at semeval-2017 task 9: Transition-based language generation from AMR. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 586-591. Vancouver, Canada.</p>
<p>Random walk inference and learning in a large scale knowledge base. Ni Lao, Tom Mitchell, William W Cohen, Conference on Empirical Methods in Natural Language Processing (EMNLP-11). Association for Computational LinguisticsLao, Ni, Tom Mitchell, and William W Cohen. 2011. Random walk inference and learning in a large scale knowledge base. In Conference on Empirical Meth- ods in Natural Language Processing (EMNLP-11), pages 529-539. Association for Computational Linguistics.</p>
<p>Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks. Yann Lecun, Yoshua Bengio, 3361LeCun, Yann, Yoshua Bengio, et al. 1995. Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks, 3361(10):1995.</p>
<p>Modeling source syntax for neural machine translation. Junhui Li, Deyi Xiong, Zhaopeng Tu, Muhua Zhu, Min Zhang, Guodong Zhou, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17). the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17)Li, Junhui, Deyi Xiong, Zhaopeng Tu, Muhua Zhu, Min Zhang, and Guodong Zhou. 2017. Modeling source syntax for neural machine translation. In Pro- ceedings of the 55th Annual Meeting of the Association for Computational Linguis- tics (ACL-17), pages 688-697.</p>
<p>Incremental joint extraction of entity mentions and relations. Qi Li, Heng Ji, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. the 52nd Annual Meeting of the Association for Computational Linguistics14Li, Qi and Heng Ji. 2014. Incremental joint extraction of entity mentions and relations. In Proceedings of the 52nd Annual Meeting of the Association for Com- putational Linguistics (ACL-14).</p>
<p>Improving event detection with abstract meaning representation. Xiang Li, Thien Huu Nguyen, Kai Cao, Ralph Grishman, Proceedings of the First Workshop on Computing News Storylines. the First Workshop on Computing News StorylinesBeijing, ChinaLi, Xiang, Thien Huu Nguyen, Kai Cao, and Ralph Grishman. 2015. Improving event detection with abstract meaning representation. In Proceedings of the First Workshop on Computing News Storylines, pages 11-15. Beijing, China.</p>
<p>Gated graph sequence neural networks. Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard Zemel, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsICLRLi, Yujia, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. 2016. Gated graph sequence neural networks. In Proceedings of the International Conference on Learning Representations (ICLR).</p>
<p>Semantic object parsing with graph LSTM. Xiaodan Liang, Xiaohui Shen, Jiashi Feng, Liang Lin, Shuicheng Yan, Proceedings of European Conference on Computer Vision (ECCV). European Conference on Computer Vision (ECCV)Liang, Xiaodan, Xiaohui Shen, Jiashi Feng, Liang Lin, and Shuicheng Yan. 2016. Semantic object parsing with graph LSTM. In Proceedings of European Confer- ence on Computer Vision (ECCV).</p>
<p>Denoising distantly supervised open-domain question answering. Yankai Lin, Haozhe Ji, Zhiyuan Liu, Maosong Sun, Proceedings of the 56th. the 56thLin, Yankai, Haozhe Ji, Zhiyuan Liu, and Maosong Sun. 2018. Denoising dis- tantly supervised open-domain question answering. In Proceedings of the 56th</p>
<p>Annual Meeting of the Association for Computational Linguistics (ACL-18). Annual Meeting of the Association for Computational Linguistics (ACL-18), pages 1736-1745.</p>
<p>Semantic role features for machine translation. Ding Liu, Daniel Gildea, Proceedings of the 23rd International Conference on Computational Linguistics (COLING-10). the 23rd International Conference on Computational Linguistics (COLING-10)Liu, Ding and Daniel Gildea. 2010. Semantic role features for machine trans- lation. In Proceedings of the 23rd International Conference on Computational Lin- guistics (COLING-10), pages 716-724.</p>
<p>Effective approaches to attention-based neural machine translation. Thang Luong, Hieu Pham, Christopher D Manning, Conference on Empirical Methods in Natural Language Processing (EMNLP-15). Luong, Thang, Hieu Pham, and Christopher D. Manning. 2015. Effective ap- proaches to attention-based neural machine translation. In Conference on Em- pirical Methods in Natural Language Processing (EMNLP-15), pages 1412-1421.</p>
<p>AMR parsing as graph prediction with latent alignment. Chunchuan Lyu, Ivan Titov, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18). the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18)Lyu, Chunchuan and Ivan Titov. 2018. AMR parsing as graph prediction with latent alignment. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18), pages 397-407.</p>
<p>. Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, J Steven, Manning, Christopher D., Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J.</p>
<p>The Stanford CoreNLP natural language processing toolkit. David Bethard, Mcclosky, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL-14). the 52nd Annual Meeting of the Association for Computational Linguistics (ACL-14)Bethard, and David McClosky. 2014. The Stanford CoreNLP natural lan- guage processing toolkit. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL-14), pages 55-60.</p>
<p>Exploiting semantics in neural machine translation with graph convolutional networks. Diego Marcheggiani, Joost Bastings, Ivan Titov, Proceedings of the 2018 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-18). the 2018 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-18)Marcheggiani, Diego, Joost Bastings, and Ivan Titov. 2018. Exploiting seman- tics in neural machine translation with graph convolutional networks. In Proceedings of the 2018 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-18), pages 486-492.</p>
<p>Encoding sentences with graph convolutional networks for semantic role labeling. Diego Marcheggiani, Ivan Titov, Conference on Empirical Methods in Natural Language Processing (EMNLP-17). Marcheggiani, Diego and Ivan Titov. 2017. Encoding sentences with graph con- volutional networks for semantic role labeling. In Conference on Empirical Methods in Natural Language Processing (EMNLP-17), pages 1506-1515.</p>
<p>Online largemargin training of dependency parsers. Ryan Mcdonald, Koby Crammer, Fernando Pereira, Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05). the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05)McDonald, Ryan, Koby Crammer, and Fernando Pereira. 2005a. Online large- margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05).</p>
<p>Simple algorithms for complex relation extraction with applications to biomedical IE. Ryan Mcdonald, Fernando Pereira, Seth Kulick, Scott Winters, Yang Jin, Pete White, Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05). the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05)McDonald, Ryan, Fernando Pereira, Seth Kulick, Scott Winters, Yang Jin, and Pete White. 2005b. Simple algorithms for complex relation extraction with applications to biomedical IE. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05).</p>
<p>Translation with source constituency and dependency trees. Fandong Meng, Jun Xie, Linfeng Song, Yajuan Lü, Qun Liu, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language ProcessingMeng, Fandong, Jun Xie, Linfeng Song, Yajuan Lü, and Qun Liu. 2013. Trans- lation with source constituency and dependency trees. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1066-1076.</p>
<p>Forge at semeval-2017 task 9: Deep sentence generation based on a sequence of graph transducers. Simon Mille, Roberto Carlini, Alicia Burga, Leo Wanner, Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017). the 11th International Workshop on Semantic Evaluation (SemEval-2017)Vancouver, CanadaMille, Simon, Roberto Carlini, Alicia Burga, and Leo Wanner. 2017. Forge at semeval-2017 task 9: Deep sentence generation based on a sequence of graph transducers. In Proceedings of the 11th International Workshop on Semantic Eval- uation (SemEval-2017), pages 920-923. Vancouver, Canada.</p>
<p>Addressing a question answering challenge by combining statistical methods with inductive rule learning and reasoning. Arindam Mitra, Chitta Baral, Proceedings of the National Conference on Artificial Intelligence (AAAI-16). the National Conference on Artificial Intelligence (AAAI-16)Mitra, Arindam and Chitta Baral. 2015. Addressing a question answering chal- lenge by combining statistical methods with inductive rule learning and rea- soning. In Proceedings of the National Conference on Artificial Intelligence (AAAI- 16).</p>
<p>End-to-end relation extraction using LSTMs on sequences and tree structures. Makoto Miwa, Mohit Bansal, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational Linguistics16Miwa, Makoto and Mohit Bansal. 2016. End-to-end relation extraction using LSTMs on sequences and tree structures. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL-16).</p>
<p>Loopy belief propagation for approximate inference: An empirical study. Kevin P Murphy, Yair Weiss, Michael I Jordan , Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence. the Fifteenth conference on Uncertainty in artificial intelligenceMurphy, Kevin P, Yair Weiss, and Michael I Jordan. 1999. Loopy belief prop- agation for approximate inference: An empirical study. In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence.</p>
<p>Rectified linear units improve restricted boltzmann machines. Vinod Nair, Geoffrey E Hinton, Proceedings of the 27th international conference on machine learning (ICML-10). the 27th international conference on machine learning (ICML-10)Nair, Vinod and Geoffrey E Hinton. 2010. Rectified linear units improve re- stricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pages 807-814.</p>
<p>Neural programmer: Inducing latent programs with gradient descent. Arvind Neelakantan, V Quoc, Ilya Le, Sutskever, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsICLRNeelakantan, Arvind, Quoc V Le, and Ilya Sutskever. 2016. Neural program- mer: Inducing latent programs with gradient descent. In Proceedings of the International Conference on Learning Representations (ICLR).</p>
<p>Learning convolutional neural networks for graphs. Mathias Niepert, Mohamed Ahmed, Konstantin Kutzkov, International conference on machine learning. Niepert, Mathias, Mohamed Ahmed, and Konstantin Kutzkov. 2016. Learn- ing convolutional neural networks for graphs. In International conference on machine learning, pages 2014-2023.</p>
<p>The proposition bank: An annotated corpus of semantic roles. Martha Palmer, Daniel Gildea, Paul Kingsbury, Computational linguistics. 311Palmer, Martha, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational linguistics, 31(1):71-106.</p>
<p>BLEU: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-02). the 40th Annual Meeting of the Association for Computational Linguistics (ACL-02)Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-02), pages 311-318.</p>
<p>Cross-sentence n-ary relation extraction with graph LSTMs. Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, Wen-Tau Yih, Transaction of ACL (TACL). 5Peng, Nanyun, Hoifung Poon, Chris Quirk, Kristina Toutanova, and Wen-tau Yih. 2017. Cross-sentence n-ary relation extraction with graph LSTMs. Trans- action of ACL (TACL), 5:101-115.</p>
<p>A synchronous hyperedge replacement grammar based approach for AMR parsing. Xiaochang Peng, Linfeng Song, Daniel Gildea, Proceedings of the Nineteenth Conference on Computational Natural Language Learning (CoNLL-15). the Nineteenth Conference on Computational Natural Language Learning (CoNLL-15)BeijingPeng, Xiaochang, Linfeng Song, and Daniel Gildea. 2015. A synchronous hy- peredge replacement grammar based approach for AMR parsing. In Proceed- ings of the Nineteenth Conference on Computational Natural Language Learning (CoNLL-15), pages 731-739. Beijing.</p>
<p>Sequence-to-sequence models for cache transition systems. Xiaochang Peng, Linfeng Song, Daniel Gildea, Giorgio Satta, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18). the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18)Peng, Xiaochang, Linfeng Song, Daniel Gildea, and Giorgio Satta. 2018. Sequence-to-sequence models for cache transition systems. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL- 18), pages 1842-1852.</p>
<p>GloVe: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, Conference on Empirical Methods in Natural Language Processing (EMNLP-14). Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global vectors for word representation. In Conference on Empirical Methods in Natural Language Processing (EMNLP-14), pages 1532-1543.</p>
<p>Deep contextualized word representations. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, Proceedings of the 2018 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-18). the 2018 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-18)Peters, Matthew, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-18).</p>
<p>Embedding semantic similarity in tree kernels for domain adaptation of relation extraction. Barbara Plank, Alessandro Moschitti, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL-13). the 51st Annual Meeting of the Association for Computational Linguistics (ACL-13)Plank, Barbara and Alessandro Moschitti. 2013. Embedding semantic similarity in tree kernels for domain adaptation of relation extraction. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL- 13).</p>
<p>Generating English from abstract meaning representations. Nima Pourdamghani, Kevin Knight, Ulf Hermjakob, International Conference on Natural Language Generation (INLG-16). Edinburgh, UKPourdamghani, Nima, Kevin Knight, and Ulf Hermjakob. 2016. Generating English from abstract meaning representations. In International Conference on Natural Language Generation (INLG-16), pages 21-25. Edinburgh, UK.</p>
<p>Parsing english into abstract meaning representation using syntaxbased machine translation. Michael Pust, Ulf Hermjakob, Kevin Knight, Daniel Marcu, Jonathan , Conference on Empirical Methods in Natural Language Processing (EMNLP-15). Pust, Michael, Ulf Hermjakob, Kevin Knight, Daniel Marcu, and Jonathan May. 2015. Parsing english into abstract meaning representation using syntax- based machine translation. In Conference on Empirical Methods in Natural Lan- guage Processing (EMNLP-15), pages 1143-1154.</p>
<p>Distant supervision for relation extraction beyond the sentence boundary. Chris Quirk, Hoifung Poon, Proceedings of the 15th Conference of the European Chapter of the ACL (EACL-17). the 15th Conference of the European Chapter of the ACL (EACL-17)Quirk, Chris and Hoifung Poon. 2017. Distant supervision for relation extrac- tion beyond the sentence boundary. In Proceedings of the 15th Conference of the European Chapter of the ACL (EACL-17).</p>
<p>SQuAD: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, Conference on Empirical Methods in Natural Language Processing (EMNLP-16). Rajpurkar, Pranav, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Confer- ence on Empirical Methods in Natural Language Processing (EMNLP-16), pages 2383-2392.</p>
<p>The graph neural network model. Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, Gabriele Monfardini, IEEE Transactions on Neural Networks. 201Scarselli, Franco, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. 2009. The graph neural network model. IEEE Trans- actions on Neural Networks, 20(1):61-80.</p>
<p>Get to the point: Summarization with pointer-generator networks. Abigail See, Peter J Liu, Christopher D Manning, Proceedings of the 55th. the 55thSee, Abigail, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th</p>
<p>Annual Meeting of the Association for Computational Linguistics (ACL-17). Vancouver, CanadaAnnual Meeting of the Association for Computational Linguistics (ACL-17), pages 1073-1083. Vancouver, Canada.</p>
<p>Neural machine translation of rare words with subword units. Rico Sennrich, Barry Haddow, Alexandra Birch, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL-16). the 54th Annual Meeting of the Association for Computational Linguistics (ACL-16)Sennrich, Rico, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL-16), pages 1715- 1725.</p>
<p>Bidirectional attention flow for machine comprehension. Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi, arXiv:1611.01603arXiv preprintSeo, Minjoon, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Bidirectional attention flow for machine comprehension. arXiv preprint arXiv:1611.01603.</p>
<p>Reasonet: Learning to stop reading in machine comprehension. Yelong Shen, Po-Sen Huang, Jianfeng Gao, Weizhu Chen, Proceedings of Knowledge Discovery and Data Mining (SIGKDD). Knowledge Discovery and Data Mining (SIGKDD)Shen, Yelong, Po-Sen Huang, Jianfeng Gao, and Weizhu Chen. 2017. Reasonet: Learning to stop reading in machine comprehension. In Proceedings of Knowl- edge Discovery and Data Mining (SIGKDD), pages 1047-1055.</p>
<p>A study of translation edit rate with targeted human annotation. Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, John Makhoul, Proceedings of Association for Machine Translation in the Americas. Association for Machine Translation in the AmericasSnover, Matthew, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annota- tion. In Proceedings of Association for Machine Translation in the Americas, pages 223-231.</p>
<p>Semantic neural machine translation using AMR. Linfeng Song, Daniel Gildea, Yue Zhang, Zhiguo Wang, Jinsong Su, Transactions of the Association for Computational Linguistics (TACL). Song, Linfeng, Daniel Gildea, Yue Zhang, Zhiguo Wang, and Jinsong Su. 2019. Semantic neural machine translation using AMR. Transactions of the Associa- tion for Computational Linguistics (TACL).</p>
<p>AMR-to-text generation with synchronous node replacement grammar. Linfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang, Daniel Gildea, Song, Linfeng, Xiaochang Peng, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2017. AMR-to-text generation with synchronous node replacement grammar.</p>
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17). the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17)In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17).</p>
<p>Leveraging context information for natural question generation. Linfeng Song, Zhiguo Wang, Wael Hamza, Yue Zhang, Daniel Gildea, Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL-18). the North American Chapter of the Association for Computational Linguistics (NAACL-18)Song, Linfeng, Zhiguo Wang, Wael Hamza, Yue Zhang, and Daniel Gildea. 2018a. Leveraging context information for natural question generation. In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL-18), pages 569-574.</p>
<p>Sense embedding learning for word sense induction. Linfeng Song, Zhiguo Wang, Haitao Mi, Daniel Gildea, Fifth Joint Conference On Lexical And Computational Semantics (<em>SEM 2016). Song, Linfeng, Zhiguo Wang, Haitao Mi, and Daniel Gildea. 2016a. Sense em- bedding learning for word sense induction. In Fifth Joint Conference On Lexical And Computational Semantics (</em>SEM 2016), pages 85-90.</p>
<p>Exploring graph-structured passage representation for multihop reading comprehension with graph neural networks. Linfeng Song, Zhiguo Wang, Mo Yu, Yue Zhang, Radu Florian, Daniel Gildea, arXiv:1809.02040arXiv preprintSong, Linfeng, Zhiguo Wang, Mo Yu, Yue Zhang, Radu Florian, and Daniel Gildea. 2018b. Exploring graph-structured passage representation for multi- hop reading comprehension with graph neural networks. arXiv preprint arXiv:1809.02040.</p>
<p>Neural transition-based syntactic linearization. Linfeng Song, Yue Zhang, Daniel Gildea, Proceedings of the 11th International Conference on Natural Language Generation. the 11th International Conference on Natural Language GenerationSong, Linfeng, Yue Zhang, and Daniel Gildea. 2018c. Neural transition-based syntactic linearization. In Proceedings of the 11th International Conference on Natural Language Generation, pages 431-440.</p>
<p>AMR-to-text generation as a traveling salesman problem. Linfeng Song, Yue Zhang, Xiaochang Peng, Zhiguo Wang, Daniel Gildea, Conference on Empirical Methods in Natural Language Processing. 16Song, Linfeng, Yue Zhang, Xiaochang Peng, Zhiguo Wang, and Daniel Gildea. 2016b. AMR-to-text generation as a traveling salesman problem. In Confer- ence on Empirical Methods in Natural Language Processing (EMNLP-16).</p>
<p>A graphto-sequence model for AMR-to-text generation. Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel Gildea, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18). the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18)Song, Linfeng, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018d. A graph- to-sequence model for AMR-to-text generation. In Proceedings of the 56th An- nual Meeting of the Association for Computational Linguistics (ACL-18), pages 1616-1626.</p>
<p>N-ary relation extraction using graph state LSTM. Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel Gildea, Conference on Empirical Methods in Natural Language Processing. 18Song, Linfeng, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018e. N-ary re- lation extraction using graph state LSTM. In Conference on Empirical Methods in Natural Language Processing (EMNLP-18).</p>
<p>Syntactically guided neural machine translation. Felix Stahlberg, Eva Hasler, Aurelien Waite, Bill Byrne, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL-16). the 54th Annual Meeting of the Association for Computational Linguistics (ACL-16)Stahlberg, Felix, Eva Hasler, Aurelien Waite, and Bill Byrne. 2016. Syntactically guided neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL-16), pages 299-305.</p>
<p>Lattice-based recurrent neural network encoders for neural machine translation. Jinsong Su, Zhixing Tan, Deyi Xiong, Rongrong Ji, Xiaodong Shi, Yang Liu, Proceedings of the National Conference on Artificial Intelligence (AAAI-17). the National Conference on Artificial Intelligence (AAAI-17)Su, Jinsong, Zhixing Tan, Deyi Xiong, Rongrong Ji, Xiaodong Shi, and Yang Liu. 2017. Lattice-based recurrent neural network encoders for neural machine translation. In Proceedings of the National Conference on Artificial Intelligence (AAAI-17), pages 3302-3308.</p>
<p>Sequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, Quoc V Le, Advances in neural information processing systems. Sutskever, Ilya, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104-3112.</p>
<p>Extracting relations within and across sentences. Kumutha Swampillai, Mark Stevenson, Proceedings of the International Conference Recent Advances in Natural Language Processing. the International Conference Recent Advances in Natural Language ProcessingSwampillai, Kumutha and Mark Stevenson. 2011. Extracting relations within and across sentences. In Proceedings of the International Conference Recent Ad- vances in Natural Language Processing 2011.</p>
<p>Improved semantic representations from tree-structured long short-term memory networks. Kai Tai, Richard Sheng, Christopher D Socher, Manning, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL-15). the 53rd Annual Meeting of the Association for Computational Linguistics (ACL-15)Tai, Kai Sheng, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations from tree-structured long short-term memory net- works. In Proceedings of the 53rd Annual Meeting of the Association for Compu- tational Linguistics (ACL-15).</p>
<p>Neural headline generation on abstract meaning representation. Sho Takase, Jun Suzuki, Naoaki Okazaki, Tsutomu Hirao, Masaaki Nagata, Takase, Sho, Jun Suzuki, Naoaki Okazaki, Tsutomu Hirao, and Masaaki Na- gata. 2016. Neural headline generation on abstract meaning representation.</p>
<p>Conference on Empirical Methods in Natural Language Processing (EMNLP-16). Austin, TexasIn Conference on Empirical Methods in Natural Language Processing (EMNLP-16), pages 1054-1059. Austin, Texas.</p>
<p>The web as a knowledge-base for answering complex questions. Alon Talmor, Jonathan Berant, Proceedings of the 2018 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-18). the 2018 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-18)Talmor, Alon and Jonathan Berant. 2018. The web as a knowledge-base for answering complex questions. In Proceedings of the 2018 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-18).</p>
<p>A discriminative model for semantics-to-string translation. Aleš Tamchyna, Chris Quirk, Michel Galley, Proceedings of the 1st Workshop on Semantics-Driven Statistical Machine Translation (S2MT 2015). the 1st Workshop on Semantics-Driven Statistical Machine Translation (S2MT 2015)Tamchyna, Aleš, Chris Quirk, and Michel Galley. 2015. A discriminative model for semantics-to-string translation. In Proceedings of the 1st Workshop on Semantics-Driven Statistical Machine Translation (S2MT 2015), pages 30-36.</p>
<p>. China Beijing, Beijing, China.</p>
<p>Modeling coverage for neural machine translation. Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, Hang Li, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL-16). the 54th Annual Meeting of the Association for Computational Linguistics (ACL-16)Berlin, GermanyTu, Zhaopeng, Zhengdong Lu, Yang Liu, Xiaohua Liu, and Hang Li. 2016. Mod- eling coverage for neural machine translation. In Proceedings of the 54th An- nual Meeting of the Association for Computational Linguistics (ACL-16), pages 76-85. Berlin, Germany.</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Illia Kaiser, Polosukhin, Advances in Neural Information Processing Systems. I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett30Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Pro- cessing Systems 30, pages 5998-6008.</p>
<p>Graph attention networks. Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsICLRVeličković, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2018. Graph attention networks. In Proceed- ings of the International Conference on Learning Representations (ICLR).</p>
<p>Getting the most out of AMR parsing. Chuan Wang, Nianwen Xue, Conference on Empirical Methods in Natural Language Processing (EMNLP-17). Wang, Chuan and Nianwen Xue. 2017. Getting the most out of AMR parsing. In Conference on Empirical Methods in Natural Language Processing (EMNLP-17), pages 1257-1268.</p>
<p>Machine comprehension using matchlstm and answer pointer. Shuohang Wang, Jing Jiang, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)Wang, Shuohang and Jing Jiang. 2017. Machine comprehension using match- lstm and answer pointer. In Proceedings of the International Conference on Learn- ing Representations (ICLR).</p>
<p>R3: Reinforced ranker-reader for open-domain question answering. Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerald Tesauro, Bowen Zhou, Jing Jiang, Proceedings of the National Conference on Artificial Intelligence (AAAI-18). the National Conference on Artificial Intelligence (AAAI-18)Wang, Shuohang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerald Tesauro, Bowen Zhou, and Jing Jiang. 2018a. R3: Reinforced ranker-reader for open-domain question answering. In Pro- ceedings of the National Conference on Artificial Intelligence (AAAI-18).</p>
<p>Evidence aggregation for answer re-ranking in open-domain question answering. Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, Murray Campbell, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsICLRWang, Shuohang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. 2018b. Evidence aggregation for answer re-ranking in open-domain question an- swering. In Proceedings of the International Conference on Learning Representa- tions (ICLR).</p>
<p>Joint training of candidate extraction and answer selection for reading comprehension. Zhen Wang, Jiachen Liu, Xinyan Xiao, Yajuan Lyu, Tian Wu, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18). the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18)Wang, Zhen, Jiachen Liu, Xinyan Xiao, Yajuan Lyu, and Tian Wu. 2018c. Joint training of candidate extraction and answer selection for reading compre- hension. In Proceedings of the 56th Annual Meeting of the Association for Compu- tational Linguistics (ACL-18), pages 1715-1724.</p>
<p>Bilateral multiperspective matching for natural language sentences. Zhiguo Wang, Wael Hamza, Radu Florian, Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI-17). the International Joint Conference on Artificial Intelligence (IJCAI-17)Wang, Zhiguo, Wael Hamza, and Radu Florian. 2017. Bilateral multi- perspective matching for natural language sentences. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI-17), pages 4144- 4150.</p>
<p>Multiperspective context matching for machine comprehension. Zhiguo Wang, Haitao Mi, Wael Hamza, Radu Florian, arXiv:1612.04211arXiv preprintWang, Zhiguo, Haitao Mi, Wael Hamza, and Radu Florian. 2016. Multi- perspective context matching for machine comprehension. arXiv preprint arXiv:1612.04211.</p>
<p>Making neural qa as simple as possible but not simpler. Dirk Weissenborn, Georg Wiese, Laura Seiffe, Proceedings of the Conference on Natural Language Learning (CoNLL). the Conference on Natural Language Learning (CoNLL)Weissenborn, Dirk, Georg Wiese, and Laura Seiffe. 2017. Making neural qa as simple as possible but not simpler. In Proceedings of the Conference on Natural Language Learning (CoNLL), pages 271-280.</p>
<p>Constructing datasets for multi-hop reading comprehension across documents. Johannes Welbl, Pontus Stenetorp, Sebastian Riedel, Transacton of ACL (TACL). 6Welbl, Johannes, Pontus Stenetorp, and Sebastian Riedel. 2018. Constructing datasets for multi-hop reading comprehension across documents. Transacton of ACL (TACL), 6:287-302.</p>
<p>Towards aicomplete question answering: A set of prerequisite toy tasks. Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van Merriënboer, Armand Joulin, Tomas Mikolov, arXiv:1502.05698arXiv preprintWeston, Jason, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merriënboer, Armand Joulin, and Tomas Mikolov. 2015. Towards ai- complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698.</p>
<p>Learning field compatibilities to extract database records from unstructured text. Michael Wick, Aron Culotta, Andrew Mccallum, Conference on Empirical Methods in Natural Language Processing. 6Wick, Michael, Aron Culotta, and Andrew McCallum. 2006. Learning field compatibilities to extract database records from unstructured text. In Confer- ence on Empirical Methods in Natural Language Processing (EMNLP-06).</p>
<p>Learning for semantic parsing with statistical machine translation. Yuk Wong, Raymond Wah, Mooney, Proceedings of the 2006 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-06). the 2006 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-06)Wong, Yuk Wah and Raymond Mooney. 2006. Learning for semantic parsing with statistical machine translation. In Proceedings of the 2006 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL- 06), pages 439-446.</p>
<p>Semantic roles for SMT: A hybrid two-pass model. Dekai Wu, Pascale Fung, Proceedings of the 2009 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-09). the 2009 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-09)Wu, Dekai and Pascale Fung. 2009. Semantic roles for SMT: A hybrid two-pass model. In Proceedings of the 2009 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-09), pages 13-16.</p>
<p>Improved neural machine translation with source syntax. Shuangzhi Wu, Ming Zhou, Dongdong Zhang, Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI-17). the International Joint Conference on Artificial Intelligence (IJCAI-17)Wu, Shuangzhi, Ming Zhou, and Dongdong Zhang. 2017. Improved neural machine translation with source syntax. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI-17), pages 4179-4185.</p>
<p>A novel dependency-to-string model for statistical machine translation. Jun Xie, Haitao Mi, Qun Liu, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language ProcessingXie, Jun, Haitao Mi, and Qun Liu. 2011. A novel dependency-to-string model for statistical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 216-226.</p>
<p>Dynamic coattention networks for question answering. Caiming Xiong, Victor Zhong, Richard Socher, arXiv:1611.01604arXiv preprintXiong, Caiming, Victor Zhong, and Richard Socher. 2016. Dynamic coattention networks for question answering. arXiv preprint arXiv:1611.01604.</p>
<p>Neural enquirer: Learning to query tables with natural language. Pengcheng Yin, Zhengdong Lu, Hang Li, Ben Kao, Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI-16). the International Joint Conference on Artificial Intelligence (IJCAI-16)Yin, Pengcheng, Zhengdong Lu, Hang Li, and Ben Kao. 2016. Neural enquirer: Learning to query tables with natural language. In Proceedings of the Interna- tional Joint Conference on Artificial Intelligence (IJCAI-16).</p>
<p>Coreference based event-argument relation extraction on biomedical text. Katsumasa Yoshikawa, Sebastian Riedel, Tsutomu Hirao, Masayuki Asahara, Yuji Matsumoto, Journal of Biomedical Semantics. 256Yoshikawa, Katsumasa, Sebastian Riedel, Tsutomu Hirao, Masayuki Asahara, and Yuji Matsumoto. 2011. Coreference based event-argument relation ex- traction on biomedical text. Journal of Biomedical Semantics, 2(5):S6.</p>
<p>Kernel methods for relation extraction. Dmitry Zelenko, Chinatsu Aone, Anthony Richardella, Journal of Machine Learning Research. 32Zelenko, Dmitry, Chinatsu Aone, and Anthony Richardella. 2003. Kernel meth- ods for relation extraction. Journal of Machine Learning Research, 3(2):1083- 1106.</p>
<p>End-to-end neural relation extraction with global optimization. Meishan Zhang, Yue Zhang, Guohong Fu, Conference on Empirical Methods in Natural Language Processing. 17Zhang, Meishan, Yue Zhang, and Guohong Fu. 2017. End-to-end neural rela- tion extraction with global optimization. In Conference on Empirical Methods in Natural Language Processing (EMNLP-17).</p>
<p>Sentence-state LSTM for text representation. Yue Zhang, Qi Liu, Linfeng Song, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18). the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18)Zhang, Yue, Qi Liu, and Linfeng Song. 2018. Sentence-state LSTM for text rep- resentation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18), pages 317-327.</p>
<p>Chinese NER using lattice LSTM. Yue Zhang, Jie Yang, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18). the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18)Zhang, Yue and Jie Yang. 2018. Chinese NER using lattice LSTM. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL- 18), pages 1554-1564.</p>
<p>Extracting relations with integrated information using kernel methods. Shubin Zhao, Ralph Grishman, Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05). the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05)Zhao, Shubin and Ralph Grishman. 2005. Extracting relations with integrated information using kernel methods. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05).</p>
<p>DAG-structured long short-term memory for semantic compositionality. Xiaodan Zhu, Parinaz Sobhani, Hongyu Guo, Proceedings of the. theZhu, Xiaodan, Parinaz Sobhani, and Hongyu Guo. 2016. DAG-structured long short-term memory for semantic compositionality. In Proceedings of the 2016</p>
<p>Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-16). Meeting of the North American chapter of the Association for Computational Lin- guistics (NAACL-16), pages 917-926.</p>            </div>
        </div>

    </div>
</body>
</html>
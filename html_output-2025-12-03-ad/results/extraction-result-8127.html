<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8127 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8127</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8127</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-276107421</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.01432v1.pdf" target="_blank">Emergent Stack Representations in Modeling Counter Languages Using Transformers</a></p>
                <p><strong>Paper Abstract:</strong> Transformer architectures are the backbone of most modern language models, but understanding the inner workings of these models still largely remains an open problem. One way that research in the past has tackled this problem is by isolating the learning capabilities of these architectures by training them over well-understood classes of formal languages. We extend this literature by analyzing models trained over counter languages, which can be modeled using counter variables. We train transformer models on 4 counter languages, and equivalently formulate these languages using stacks, whose depths can be understood as the counter values. We then probe their internal representations for stack depths at each input token to show that these models when trained as next token predictors learn stack-like representations. This brings us closer to understanding the algorithmic details of how transformers learn languages and helps in circuit discovery.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8127.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8127.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stack-like representations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Emergent stack-like representations encoding counter values</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer encoder models trained as next-token predictors on counter languages (Dyck-1 and Shuffle-k) develop internal representations that encode per-stack depths (counter values), recoverable with probing classifiers from last-layer embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>custom encoder-only transformer (experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only transformer used in experiments: 1 layer, 4 attention heads, embedding dim 32, hidden dim 64, linear decoder with sigmoid; trained as next-token predictor on Dyck-1 and Shuffle-k languages using RMSProp (lr=5e-3), batch size 32 for 25 epochs; absolute positional encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Not directly evaluated as arithmetic; the paper treats counter-language recognition (Dyck-1, Shuffle-2/4/6). It notes that arithmetic in Polish notation can be framed as a single-counter (stack) language (mentioned).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Per-stack counter values (stack depths) are represented in the transformer's last-layer token embeddings (continuous vector features encoding integer depth information); these are described as 'stack-like' internal representations corresponding to counters.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Probing: train classifiers (linear up to 6-layer MLPs with ReLU) on last-layer token embeddings to predict stack depth at each token. Probing dataset: 10,000 samples (lengths 2–50), 8k train / 2k val. Control task: randomized targets to compute selectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Probes achieve high validation accuracy at predicting stack depth (exact numeric accuracies not reported in-text). Control (randomized) baseline accuracies are near random, producing high selectivity. Probe accuracy is higher for Shuffle-k than for Dyck-1, and increases with k.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>No detailed per-string error taxonomy is provided. Observations: lower probe accuracy for Dyck-1 relative to Shuffle languages; authors note the limitation that probing is not causal and that detailed failure-case analysis (specific strings leading to misclassification) is left to future work.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>High probing accuracy with low control accuracy (selectivity) indicates embeddings contain stack-depth information; consistent patterns across languages (Dyck-1, Shuffle-k); probing also succeeded on a Tracr-compiled transformer (algorithmic ground-truth) validating the probe's ability to detect stack-like features.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>No causal interventions or ablations are performed to show the discovered representations are used by the model to produce outputs; probing alone cannot demonstrate causality. Numeric probe performance and detailed error analyses are not provided; authors note need for MI techniques and interventions in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent Stack Representations in Modeling Counter Languages Using Transformers', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8127.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8127.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Probing classifiers (experimental setup)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linear and multilayer feed-forward probing classifiers on transformer embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Feed-forward probes (from linear to 6-layer MLPs with ReLU) trained on last-layer token embeddings to predict stack depth; includes control randomization to measure selectivity and avoid spurious probe results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Probing models: linear probes and MLPs up to 6 hidden layers, ReLU activations, hidden size 128, dropout 0.2, Xavier initialization; trained with Adam, lr=0.001, batch size 32 for 10 epochs; cross-entropy loss for classification.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Probes aim to read out integer stack-depth representations (discrete depth labels) from continuous token embeddings produced by the transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Supervised probes trained on embeddings extracted from the transformer's last encoder layer; dataset of 10k sequences sampled from the language model training corpus; control task created by randomizing labels to compute selectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>High task validation accuracy reported for probes; control baseline near random. Exact numeric accuracies are not provided in the paper text but figures indicate strong separability.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Probes may detect information that is correlated but not causally used by the model; authors explicitly note this limitation. No intervention (e.g., activation patching or ablation) performed to test causality.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Successful decoding of stack depths from embeddings, high selectivity versus randomized control, and replication of probe success on a Tracr-compiled model support the claim that embeddings carry stack-like information.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Probing results alone cannot prove the model uses the representation causally; absence of interventions or circuit-level analysis leaves open alternate explanations (e.g., probe memorization, indirect correlations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent Stack Representations in Modeling Counter Languages Using Transformers', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8127.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8127.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tracr-compiled transformer probe validation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tracr-compiled transformer (RASP -> Tracr) used to validate probing methodology</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer model compiled with Tracr from a RASP program that implements Dyck-1 is probed using the same probing pipeline; positive probe results act as a sanity check for the probing methodology.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tracr: Compiled transformers as a laboratory for interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Tracr-compiled transformer (Dyck-1 RASP program)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A transformer architecture produced by Tracr compilation from a known RASP program implementing Dyck-1; this model is not data-trained but constructed to implement a known algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Because the compiled model implements a known stack-based algorithm for Dyck-1, the same stack-depth signals should be present and detectable in its activations.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Applied the same probing setup as used on trained models (probes on last-layer embeddings, same dataset construction and probe architectures).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Probing on the Tracr model is reported as positive (Figure 6), indicating probes recover stack-depth information on an algorithmic ground-truth model. No numeric accuracy values are given in-text.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Because the Tracr model is algorithmically correct for Dyck-1 and probes recover the expected stack-depth signals, this supports that the probing pipeline can detect stack-like algorithmic features in activations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>The compiled model is not learned, so positive probe results validate the measurement technique but do not prove that learned models implement the exact same internal algorithm; differences between compiled and learned solutions remain unproven.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent Stack Representations in Modeling Counter Languages Using Transformers', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8127.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8127.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Arithmetic-as-counter-language (Polish notation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Arithmetic in Polish notation framed as a single-counter context-free language</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper notes that arithmetic expressed in Polish notation can be modeled as a single-counter, non-regular context-free language that requires stack memory (push/pop) to evaluate, implying that stack-capable models could in principle implement arithmetic in this representation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Polish-notation arithmetic (mentioned as reducible to a single-counter context-free language)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Evaluation requires a stack (single counter) where operators and operands are processed via push/pop dynamics; stack depth corresponds to intermediate expression nesting.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Statement in the introduction linking Polish-notation arithmetic to a single-counter formal language solvable with stack memory (cited as motivating why formal-language results bear on arithmetic).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>No experiments on arithmetic tasks are performed in this paper; the arithmetic-to-counter mapping is only mentioned as theoretical motivation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent Stack Representations in Modeling Counter Languages Using Transformers', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Progress measures for grokking via mechanistic interpretability. <em>(Rating: 2)</em></li>
                <li>Tracr: Compiled transformers as a laboratory for interpretability. <em>(Rating: 2)</em></li>
                <li>On the ability and limitations of transformers to recognize formal languages. <em>(Rating: 2)</em></li>
                <li>Memory-augmented recurrent neural networks can learn generalized dyck languages. <em>(Rating: 2)</em></li>
                <li>Probing classifiers: Promises, shortcomings, and advances. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8127",
    "paper_id": "paper-276107421",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "Stack-like representations",
            "name_full": "Emergent stack-like representations encoding counter values",
            "brief_description": "Transformer encoder models trained as next-token predictors on counter languages (Dyck-1 and Shuffle-k) develop internal representations that encode per-stack depths (counter values), recoverable with probing classifiers from last-layer embeddings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "custom encoder-only transformer (experiment)",
            "model_description": "Encoder-only transformer used in experiments: 1 layer, 4 attention heads, embedding dim 32, hidden dim 64, linear decoder with sigmoid; trained as next-token predictor on Dyck-1 and Shuffle-k languages using RMSProp (lr=5e-3), batch size 32 for 25 epochs; absolute positional encodings.",
            "arithmetic_task_type": "Not directly evaluated as arithmetic; the paper treats counter-language recognition (Dyck-1, Shuffle-2/4/6). It notes that arithmetic in Polish notation can be framed as a single-counter (stack) language (mentioned).",
            "mechanism_or_representation": "Per-stack counter values (stack depths) are represented in the transformer's last-layer token embeddings (continuous vector features encoding integer depth information); these are described as 'stack-like' internal representations corresponding to counters.",
            "probing_or_intervention_method": "Probing: train classifiers (linear up to 6-layer MLPs with ReLU) on last-layer token embeddings to predict stack depth at each token. Probing dataset: 10,000 samples (lengths 2–50), 8k train / 2k val. Control task: randomized targets to compute selectivity.",
            "performance_metrics": "Probes achieve high validation accuracy at predicting stack depth (exact numeric accuracies not reported in-text). Control (randomized) baseline accuracies are near random, producing high selectivity. Probe accuracy is higher for Shuffle-k than for Dyck-1, and increases with k.",
            "error_types_or_failure_modes": "No detailed per-string error taxonomy is provided. Observations: lower probe accuracy for Dyck-1 relative to Shuffle languages; authors note the limitation that probing is not causal and that detailed failure-case analysis (specific strings leading to misclassification) is left to future work.",
            "evidence_for_mechanism": "High probing accuracy with low control accuracy (selectivity) indicates embeddings contain stack-depth information; consistent patterns across languages (Dyck-1, Shuffle-k); probing also succeeded on a Tracr-compiled transformer (algorithmic ground-truth) validating the probe's ability to detect stack-like features.",
            "counterexamples_or_challenges": "No causal interventions or ablations are performed to show the discovered representations are used by the model to produce outputs; probing alone cannot demonstrate causality. Numeric probe performance and detailed error analyses are not provided; authors note need for MI techniques and interventions in future work.",
            "uuid": "e8127.0",
            "source_info": {
                "paper_title": "Emergent Stack Representations in Modeling Counter Languages Using Transformers",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Probing classifiers (experimental setup)",
            "name_full": "Linear and multilayer feed-forward probing classifiers on transformer embeddings",
            "brief_description": "Feed-forward probes (from linear to 6-layer MLPs with ReLU) trained on last-layer token embeddings to predict stack depth; includes control randomization to measure selectivity and avoid spurious probe results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": "Probing models: linear probes and MLPs up to 6 hidden layers, ReLU activations, hidden size 128, dropout 0.2, Xavier initialization; trained with Adam, lr=0.001, batch size 32 for 10 epochs; cross-entropy loss for classification.",
            "arithmetic_task_type": null,
            "mechanism_or_representation": "Probes aim to read out integer stack-depth representations (discrete depth labels) from continuous token embeddings produced by the transformer.",
            "probing_or_intervention_method": "Supervised probes trained on embeddings extracted from the transformer's last encoder layer; dataset of 10k sequences sampled from the language model training corpus; control task created by randomizing labels to compute selectivity.",
            "performance_metrics": "High task validation accuracy reported for probes; control baseline near random. Exact numeric accuracies are not provided in the paper text but figures indicate strong separability.",
            "error_types_or_failure_modes": "Probes may detect information that is correlated but not causally used by the model; authors explicitly note this limitation. No intervention (e.g., activation patching or ablation) performed to test causality.",
            "evidence_for_mechanism": "Successful decoding of stack depths from embeddings, high selectivity versus randomized control, and replication of probe success on a Tracr-compiled model support the claim that embeddings carry stack-like information.",
            "counterexamples_or_challenges": "Probing results alone cannot prove the model uses the representation causally; absence of interventions or circuit-level analysis leaves open alternate explanations (e.g., probe memorization, indirect correlations).",
            "uuid": "e8127.1",
            "source_info": {
                "paper_title": "Emergent Stack Representations in Modeling Counter Languages Using Transformers",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Tracr-compiled transformer probe validation",
            "name_full": "Tracr-compiled transformer (RASP -&gt; Tracr) used to validate probing methodology",
            "brief_description": "A transformer model compiled with Tracr from a RASP program that implements Dyck-1 is probed using the same probing pipeline; positive probe results act as a sanity check for the probing methodology.",
            "citation_title": "Tracr: Compiled transformers as a laboratory for interpretability.",
            "mention_or_use": "use",
            "model_name": "Tracr-compiled transformer (Dyck-1 RASP program)",
            "model_description": "A transformer architecture produced by Tracr compilation from a known RASP program implementing Dyck-1; this model is not data-trained but constructed to implement a known algorithm.",
            "arithmetic_task_type": null,
            "mechanism_or_representation": "Because the compiled model implements a known stack-based algorithm for Dyck-1, the same stack-depth signals should be present and detectable in its activations.",
            "probing_or_intervention_method": "Applied the same probing setup as used on trained models (probes on last-layer embeddings, same dataset construction and probe architectures).",
            "performance_metrics": "Probing on the Tracr model is reported as positive (Figure 6), indicating probes recover stack-depth information on an algorithmic ground-truth model. No numeric accuracy values are given in-text.",
            "error_types_or_failure_modes": null,
            "evidence_for_mechanism": "Because the Tracr model is algorithmically correct for Dyck-1 and probes recover the expected stack-depth signals, this supports that the probing pipeline can detect stack-like algorithmic features in activations.",
            "counterexamples_or_challenges": "The compiled model is not learned, so positive probe results validate the measurement technique but do not prove that learned models implement the exact same internal algorithm; differences between compiled and learned solutions remain unproven.",
            "uuid": "e8127.2",
            "source_info": {
                "paper_title": "Emergent Stack Representations in Modeling Counter Languages Using Transformers",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Arithmetic-as-counter-language (Polish notation)",
            "name_full": "Arithmetic in Polish notation framed as a single-counter context-free language",
            "brief_description": "The paper notes that arithmetic expressed in Polish notation can be modeled as a single-counter, non-regular context-free language that requires stack memory (push/pop) to evaluate, implying that stack-capable models could in principle implement arithmetic in this representation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "arithmetic_task_type": "Polish-notation arithmetic (mentioned as reducible to a single-counter context-free language)",
            "mechanism_or_representation": "Evaluation requires a stack (single counter) where operators and operands are processed via push/pop dynamics; stack depth corresponds to intermediate expression nesting.",
            "probing_or_intervention_method": null,
            "performance_metrics": null,
            "error_types_or_failure_modes": null,
            "evidence_for_mechanism": "Statement in the introduction linking Polish-notation arithmetic to a single-counter formal language solvable with stack memory (cited as motivating why formal-language results bear on arithmetic).",
            "counterexamples_or_challenges": "No experiments on arithmetic tasks are performed in this paper; the arithmetic-to-counter mapping is only mentioned as theoretical motivation.",
            "uuid": "e8127.3",
            "source_info": {
                "paper_title": "Emergent Stack Representations in Modeling Counter Languages Using Transformers",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Progress measures for grokking via mechanistic interpretability.",
            "rating": 2,
            "sanitized_title": "progress_measures_for_grokking_via_mechanistic_interpretability"
        },
        {
            "paper_title": "Tracr: Compiled transformers as a laboratory for interpretability.",
            "rating": 2,
            "sanitized_title": "tracr_compiled_transformers_as_a_laboratory_for_interpretability"
        },
        {
            "paper_title": "On the ability and limitations of transformers to recognize formal languages.",
            "rating": 2,
            "sanitized_title": "on_the_ability_and_limitations_of_transformers_to_recognize_formal_languages"
        },
        {
            "paper_title": "Memory-augmented recurrent neural networks can learn generalized dyck languages.",
            "rating": 2,
            "sanitized_title": "memoryaugmented_recurrent_neural_networks_can_learn_generalized_dyck_languages"
        },
        {
            "paper_title": "Probing classifiers: Promises, shortcomings, and advances.",
            "rating": 1,
            "sanitized_title": "probing_classifiers_promises_shortcomings_and_advances"
        }
    ],
    "cost": 0.011185999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Emergent Stack Representations in Modeling Counter Languages Using Transformers
3 Feb 2025</p>
<p>Utkarsh Tiwari 
Birla Institute of Technology and Science
Pilani</p>
<p>Aviral Gupta 
Birla Institute of Technology and Science
Pilani</p>
<p>Michael Hahn 
Saarland University</p>
<p>Emergent Stack Representations in Modeling Counter Languages Using Transformers
3 Feb 20259FACFC0821EEC64408169FA2925A797CarXiv:2502.01432v1[cs.CL]
Transformer architectures are the backbone of most modern language models, but understanding the inner workings of these models still largely remains an open problem.One way that research in the past has tackled this problem is by isolating the learning capabilities of these architectures by training them over wellunderstood classes of formal languages.We extend this literature by analyzing models trained over counter languages, which can be modeled using counter variables.We train transformer models on 4 counter languages, and equivalently formulate these languages using stacks, whose depths can be understood as the counter values.We then probe their internal representations for stack depths at each input token to show that these models when trained as next token predictors learn stack-like representations.This brings us closer to understanding the algorithmic details of how transformers learn languages and helps in circuit discovery.</p>
<p>Introduction</p>
<p>Modern day language models (LMs) are increasingly capable of capturing complex sequential and linguistic patterns, achieving strong performance across diverse natural language as well as synthetic tasks.Despite their impressive capabilities and substantial amounts of effort and progress, the inner workings of these models still remains opaque and un-interpretable to a substantial extent.For instance, we know that transformer models of sufficient complexity can learn, for example, to perform modular arithmetic, but recovering the exact algorithm used by these models to perform this task remains a challenge (Nanda et al.).Building on this line of inquiry, our research focuses on formal languages, specifically examining models trained on counter languages-a class of languages formally Figure 1: High probing accuracy for the counter values at each token on a model trained to recognize counter languages as a next token predictor shows that the models learn a stack-like structures in their internal representation which is helpful in reasoning about and interpreting the learned algorithm to achieve this task.modeled using stack memory.We demonstrate that these models develop internal representations that effectively mimic stack structures, providing insights into how they process and generate such languages. 1Interpreting, understanding and reasoning about the algorithms and circuits within language models (LMs) remains a largely open problem, with use cases in AI safety, alignment, and performance.</p>
<p>Formal Languages: Formal languages provide a useful testbed to isolate and investigate the learning properties of transformers and their failure cases, since these languages have precise mathematical properties that the model can be tested on (Ackerman and Cybenko, 2020).This literature contains both empirical results (Strobl et al., 2024a;Bhattamishra et al., 2020) as well as theoretical analysis of cognitive biases and learning bounds over these formal tasks (Hahn and Rofin, 2024;Pérez et al., 2019;Zhou et al., 2023;Hahn, 2020).</p>
<p>This line of inquiry is particularly significant because all algorithmic tasks can be reduced to a language within a specific class of formal languages.</p>
<p>For instance, arithmetic in Polish notation can be modeled as a single-counter, non-regular contextfree language, solvable with a stack memory but not without it.Thus, robust theoretical and empirical insights into formal languages contribute to advancing the capabilities of language models (Zhang et al., 2024).Furthermore, natural languages exhibit features that can be approximately mapped to certain classes of formal languages, often displaying recursive structures akin to those found in formal systems (Kornai, 1985;Jäger and Rogers, 2012).</p>
<p>Mechanistic Interpretability and Probing Classifiers: The field of Mechanistic Interpretability (MI) aims to extract the reasoning and interpretable structures present inside these models.Among other things, MI deals with identifying features inside language models (Rai et al., 2024).A feature is a human-interpretable property of the model's activations on specific inputs.This leads to an understanding of features as meaningful vectors in the activation space of a model.A widely used approach to understanding model structures through the aforementioned features involves probing these models by linking internal representations or activations with external properties of the inputs.This is done by training a classifier on these representations to predict specific properties.Known as probing classifiers (Belinkov, 2022), this framework has become a key analysis tool in numerous studies of NLP models and the methodology that we use to understand the internal structure of the trained models on these formal languages.This direction is also motivated by other works which have had success in retrieving coherent internal structures like world models in natural language (Li et al., 2021;Abdou et al., 2021) and toy/synthetic setups (Elhage et al., 2022;Li et al., 2024;Vafa et al., 2024).To our knowledge, this work is the first to leverage probing classifiers to analyze models trained on formal languages.</p>
<p>To more formally define the training objective of the probes, let f : x → ŷ denote a language model, trained on a formal language, with performance measured by the language modeling task of auto-regressive next-token prediction.The model f generates intermediate representations f l (x) at layer l, called embeddings.A probing classifier g : f l (x) → ẑ maps these embeddings to a property z (e.g., stack depth, part-of-speech), trained and evaluated on dataset D P = {(f l (x), z (i) )}, which is formed by pairing each token's embedding with its property in that sequence.The performance of the probing classifier depends on two key factors: the probe's ability to map embeddings to the target property and the original model f 's ability to generate information-rich embeddings by effectively learning the next-token prediction task.If the classifier achieves high performance, it indicates that the model has learned information relevant to the property being probed.This setup serves as a proxy for examining the internal structure of the transformer, by probing for implicit properties of the dataset it was trained on.However, the choice of property must be carefully considered to ensure meaningful and interpretable results.In our case, we adopt a more formal and structured approach by leveraging counter languages, which provide a well-defined and rigorous framework for probing.Formal languages offer a clear and systematic property to probe, grounded in their precise modeling and theoretical foundations.</p>
<p>Related Work</p>
<p>Transformers and Formal Language Learnability: Transformers dominate sequence modeling tasks, but their ability to model FL requiring structured memory, like counters or stacks, is only incompletely understood.While theoretically Turingcomplete (Pérez et al., 2019) and universal approximators of sequence functions (Yun et al., 2020), practical learnability of FL is less understood.For example, Hahn (2020) showed Transformers struggle with Parity and Dyck-2 in the asymptotics of unbounded sequence length.Empirical studies, such as Bhattamishra et al. (2020) and (Strobl et al., 2024b), demonstrate Transformers can learn Dyck-1 and Shuffle-Dyck, suggesting they can simulate counter-like behavior.These findings highlight the need for further investigation into Transformers' ability to model FL.</p>
<p>Probing Internal Representations: Probing classifiers have become a key tool for understanding neural model representations.By training simple classifiers on intermediate activations, researchers infer whether specific properties are encoded.For example, Voita et al. (2019) studied attention heads, while Rogers et al. (2020) and Coenen et al. (2019) explored intermediate layer information.In FL, probing has been used to analyze models trained on tasks like arithmetic and syntactic parsing (Li et al., 2021;Abdou et al., 2021).Notably, Elhage et al. (2022) and Li et al. (2024) showed probing can reveal emergent structures in synthetic setups.Our work extends this by probing models trained on counter languages, providing insights into stack-like representations learned by Transformers.</p>
<p>Enforcing Stack Structures in Transformers: Several works have explored explicitly incorporating stack-like structures into neural models to handle hierarchical patterns.For example, Joulin and Mikolov (2015) proposed a neural stack for RNNs, enabling pushdown automata simulation.Similarly, Suzgun et al. (2019)  In summary, our work builds on these foundations by analyzing Transformers' ability to model counter languages and probing their internal representations for stack-like structures.By bridging FL theory and mechanistic interpretability, we aim to advance understanding of how Transformers learn and generalize algorithmic patterns.</p>
<p>3 Problem Setup And Architecture</p>
<p>Counter Language Modeling</p>
<p>Counter Languages are languages modeled using counter machines, which are DFAs with counter variables which can be incremented, decremented, and set to 0. We focus our work on the Dyck language, which is a well studied class of counter languages, and the k-Shuffles of Dyck-1.</p>
<p>Dyck-1 is the set of well-formed parenthesis strings, is a context-free language which requires 1 counter to model it.Over the alphabet Σ = {(, )} the production rules are:
S →      ϵ (S) SS
Shuffle is the binary operation || on two strings which interleaves the two string in all possible ways.Inductively:
• u ⊙ ϵ = ϵ ⊙ u = {u} • αu ⊙ βv = α(u ⊙ βv) ∪ β(αu ⊙ v)
for any α, β ∈ Σ and u, v ∈ Σ * 2 .For example, the shuffle of ab, cd = {abcd, acbd, acdb, cabd, cadb, cdab}.The Shuffle operation can be extended to apply over languages L 1 and L 2 as:
L 1 ⊙ L 2 = u∈L 1 , v∈L 2 u ⊙ v
We use Shuffle-k to denote the Shuffle of k Dyck-1 languages, each with vocabulary In our experiment setup we consider 4 counter languages: Dyck-1, and Shuffle-2, Shuffle-4, and Shuffle-6.For all these languages we follow the training details from Bhattamishra et al. (2020)
Σ i such that i∈[1,k] Σ i = ∅, i.e. disjoint</p>
<p>Language Modeling Architecture and</p>
<p>Training Setup Model Architecture.We employ an encoderonly transformer model with a linear decoder layer (language-modeling head) for sequence processing tasks, following the setup from Bhattamishra et al. (2020).Let V denote the alphabet of the language, where each symbol v ∈ V is treated as a unique token.Tokenization is a mapping τ : V → Z, assigning each token to a unique integer ID.The encoder maps input tokens x = (x 1 , . . ., x T ) to dense embeddings E = (e 1 , . . ., e T ), where e i ∈ R d model , augmented with positional encodings P = (p 1 , . . ., p T ) to encode sequence order.The combined embeddings X = E + P are processed by a stack of n layers transformer encoder layers.Each layer consists of:</p>
<p>1.A multi-head self-attention mechanism with a causal mask M , ensuring
Attention(Q, K, V ) = softmax QK T √ d k + M V
where M ij = −∞ if i &lt; j and 0 otherwise. 2The * is the Kleene Star operation 3 Hence, Shuffle-1 is the same as Dyck-1</p>
<p>Figure 2: The model is trained to predict the set of all valid next tokens.</p>
<ol>
<li>A feed-forward network (FFN) with d ffn hidden units, applied position-wise.</li>
</ol>
<p>The output of the final encoder layer H = (h 1 , . . ., h T ) is projected into the output space through a linear decoder layer W out ∈ R |V|×d model , followed by a sigmoid activation σ, yielding final output ŷ = σ(W out H). Residual connections and layer normalization are applied throughout to stabilize training.We restrict the model size to prevent overfitting and facilitate interpretability.</p>
<p>This architecture is designed to process sequences auto-regressively, with the self-attention mechanism ensuring that each token During inference, predictions are obtained by thresholding ŷ at 0.5.A sequence is considered correctly recognized if all predictions match the ground truth at every step.The model is trained using RMSProp optimizer with a learning rate of 5 × 10 −3 for 25 epochs with a batch size of 32 and absolute positional encodings.</p>
<p>Probing Setup</p>
<p>Shuffle-k can be modeled using k counter variables.Let s be the input string of length n, and let paren i denote the i-th pair of parentheses with opening symbol open i and closing symbol close i for 1 ≤ i ≤ k.During iterating over s and increasing the value of counter i when open i is encountered and decreasing it when close i is encountered, if any counter value becomes negative then we can say s / ∈ Shuffle-k.At the end of the string if the values of all counters are 0 then s ∈ Shuffle-k.</p>
<p>Algorithmically, Shuffle-k can be modeled using k stacks, and pushing open i in stack i and popping when close i is encountered.If any stack i is empty when close i is encountered then s / ∈ Shuffle-k and if at the end all stacks are empty then s ∈ Shuffle-k.In this formulation the depth of stack i corresponds to counter i .Hence, for a Shuffle-k language, we train k different probing models, one for each stack.</p>
<p>We probe the trained models for the depth of stacks at each input token to determine if the models learns the counter representation of these languages.We train simple feed forward networks of varying depths and complexity as multi class classifiers with ReLU activations on the internal representations from the trained model to predict the depth of stack being probed for.</p>
<p>The probing dataset is constructed by sampling sequences from the training corpus of the language model.It consists of 10,000 samples, each with lengths ranging between 2 and 50 tokens.The dataset is split into 8,000 samples for training and 2,000 for validation.Each sample is represented as a pair comprising the transformer encoder's embedding and the corresponding probed value.The embedding is extracted from the output of the last encoder layer of the language model and has a dimension of d model .Specifically, for an input sequence of length T , the encoder produces an output of dimension T ×d model .We slice this output along the sequence length dimension to obtain a single embedding of size 1 × d model for each token, which is then included in the probing dataset.This process is repeated for all samples in the dataset, ensuring that each entry captures the relevant encoder representation for probing tasks.</p>
<p>Keeping in the with the best practices of the literature (Hewitt and Liang, 2019) we also create a control task by randomizing the target values for the same input set.The difference between the accuracy on the main task and the control task is called the selectivity, and high values signify that the probing model is not learning spurious correlations, or memorizing the training data.</p>
<p>To further verify our probing methodology, we compile a model M using Tracr (Lindner et al., 2023) from the RASP (Weiss et al., 2021) program for Dyck-1 and use the same probing setup P on this model.Since, M is not trained using data, but is instead compiled using a known algorithm it can be used to test if P is effective at detecting stacklike features.The results of P on M are positive and are included in the Appendix.</p>
<p>Results</p>
<p>We probe the model trained on Dyck-1 for the stack depth at each input token, and for the models trained on Shuffle-k we use k probes -one for each stack.</p>
<p>We present the results for probing accuracies on Dyck-1, and one stack of the Shuffle languages in Figure 3, while the probing results for the rest of the stacks can be found in the Appendix.</p>
<p>The probing accuracy, even in the case of linear probes, is high and since the accuracies on the control task are near random baselines it suggests that the probing models are not learning spurious connections or memorizing the training set.</p>
<p>The results strongly hint towards the presence of counter variables in the internal representations of the models trained to recognize counter languages.</p>
<p>Interestingly, the probe accuracy is significantly higher in Shuffle languages than Dyck-1, and within the Shuffle languages is higher for higher values of k.This can be attributed to the fact that as the number of stacks required increase the frequency at which their depths change decreases (for instance, the stack for Dyck-1 will get updated every token, but each stack in Shuffle-6 will get updated, on average, at every 6th token).</p>
<p>Future Works and Conclusion</p>
<p>A key limitation of our study is that probing classifiers do not provide any causal information about the feature they are used to detect.While the current work shows that the models learn counter-like structures, we leave to future work to determine the extent to which these structures provide a full picture of the data structures used by the model to compute the tasks, and to determine whether these structures play a causal role in determining the model's output.</p>
<p>The broader task of interpretability can be broken down into two sub-tasks -firstly, what representations and data structures a model learns, and secondly, what algorithms operate over, update, and read out these representations.This work only focuses on the former task, however the understanding of how these stack structures are updated is an important area of research.</p>
<p>It is also possible for the LM to learn multiple representations and algorithms, in which case exploring the causality of these becomes important.Moreover, ablating across multiple architecture choices like positional embedding type, downstream task (classifier vs next token prediction vs regression) etc. might be useful.</p>
<p>It is also useful to analyse the failure cases of the probing model and to see if there are specific kinds of strings on which the probing models misclassify the stack depths.</p>
<p>Since, our results show extremely positive signals for the presence of stacks, future work should also include the use of more robust techniques from MI techniques to confirm and extend these findings.</p>
<p>A Counter Languages</p>
<p>In this section we formalize the notion of counter languages by defining them as languages modeled by a k-counter machine.For m ∈ Z, let ±m denote the function λx.x ± m.Let ×0 denote the constant zero function λx.0. The k-counter machine is defined as4 : Definition 1 (General counter machine).A kcounter machine is a tuple ⟨Σ, Q, q 0 , u, δ, F ⟩ with 1.A finite alphabet Σ 2. A finite set of states Q</p>
<ol>
<li>An initial state q 0 4. A counter update function
u : Σ × Q×{0, 1} k → ({+m : m ∈ Z} ∪ {×0}) k 5. A state transition function δ : Σ × Q × {0, 1} k → Q 6. An acceptance mask F ⊆ Q × {0, 1} k
A machine processes an input string x one token at a time.For each token, we use u to update the counters and δ to update the state according to the current input token, the current state, and a finite mask of the current counter values.We formalize this in Definition 2.</li>
</ol>
<p>For a vector v, let z(v) denote the broadcasted "zero-check" function, i.e.
z(v) i = 0 if v i = 0 1 otherwise.
Definition 2 (Counter machine computation).Let ⟨q, c⟩ ∈ Q × Z k be a configuration of machine M .Upon reading the input x t ∈ Σ, we define the transition ⟨q, c⟩ → xt ⟨δ(x t , q, z(c)), u(x t , q, z(c))(c)⟩.Definition 3 (Real-time acceptance).For any string x ∈ Σ * with length n, a counter machine accepts x if there exist states q 1 , . . ., q n and counter configurations c 1 , . . ., c n such that
⟨q 0 , 0⟩ → x 1 ⟨q 1 , c 1 ⟩ → x 2 • • • → xn ⟨q n , c n ⟩ ∈ F.</p>
<p>Definition 4 (Real-time language acceptance).</p>
<p>A counter machine accepts a language L if, for each x ∈ Σ * , it accepts x iff x ∈ L.
q0 start q1 q2 "("/z(c) = 0 ∪ 1/ + 1 ")"/z(c) = 1/ − 1 ")"/z(c) = 0/ − 1 ")"/z(c) = 1/ − 1 "("/z(c) = 0 ∪ 1/ + 1 ")"/z(c) = 0/ − 1 "(", ")"/z(c) = 0 ∪ 1/ + 0
Figure 4: A graphical representation of a 1-counter machine that accepts Dyck-1 if we set F to verify that the counter is 0 and we are in q1.</p>
<p>⟨0, q 0 ⟩ − → Here, P refers to the state when the prefix at the current token is legal but not yet balances, T when it is balanced, and F when it is illegal.This process produces a transformer model which is then used to generate a probing dataset.We leverage the same training tokens used in previous experiments to generate activation embeddings, which are then utilized to train our probing model.To robustly demonstrate the effectiveness of our probing methodology, we train the model on two distinct tasks: multi-class classification and regression.Given the transformer's unique architectural design, which is tailored to align with the underlying algorithm, our probing approach aims to recover stack-like properties inherent in the language.If successful, this would provide conclusive evidence of the efficacy of our probing setup.By successfully recovering these properties-such as stack depth-through high accuracy on the probing tasks, we demonstrate the efficacy of our probing setup.These results provide strong evidence that our approach effectively captures the hierarchical and structural features encoded in the model's representations.The probing classifier models are fully connected layers (ranging from linear models, to models with 6 layers with ReLU activations with hidden layer size of 128).We use dropout=0.2and initialize the weights using Xavier initialization.The models are trained for 10 epochs with a learning rate of 0.001, batch size 32 with the Adam optimizer.We utilise the Cross-Entropy loss to train the multiclass classification task.
( ⟨1, q 0 ⟩ − → ( ⟨2, q 0 ⟩ − → ) ⟨1, q 1 ⟩ − → ) ⟨0, q 1 ⟩ ∈ F ⟨0, q 0 ⟩ − → ( ⟨1, q 0 ⟩ − → ( ⟨2, q 0 ⟩ − → ) ⟨1, q 1 ⟩ − → ( ⟨2, q 0 ⟩ / ∈ F</p>
<p>vocabularies 3 .Similar to Bhattamishra et al. (2020) Shuffle-2 is the shuffle of Dyck-1 over alphabet Σ = {(, )} and another Dyck-1 over the alphabet Σ = {[, ]}.Hence the resulting Shuffle-2 language is defined over alphabet Σ = {[, ], (, )} and contains words such as ([)] and [((])) but not ])[(.</p>
<p>x i attends only to itself and preceding tokens x j≤i .The model used in our experiments has a hidden dimension of 64 and the embedding dimension of 32.With only 1 layer deep transformer and 4 attention heads.Training Setup.The model is trained on the nexttoken prediction task, where it processes an input sequence s = (s 1 , . . ., s n ) and predicts the set of valid characters for the next step s i+1 given the subsequence s 1:i .The model outputs a distribution over V, represented as a k-dimensional vector ŷ ∈ [0, 1] |V| , where k = |V|.The ground truth is a k-hot vector y ∈ {0, 1} |V| , indicating valid next characters.The training objective is to minimize the mean squared error (MSE) between ŷ and y.</p>
<p>Figure 3 :
3
Figure 3: Probing accuracy across model architectures for different stack depths.Blue lines show task validation accuracy, while red lines represent a randomized control baseline.High task accuracy with low control accuracy indicates successful learning of stack-like structures.</p>
<p>Figure 5 :
5
Figure 5: Behavior of the counter machine in 4 on (()) (top) and (()( (bottom)</p>
<p>Figure 6 :
6
Figure 6: Probing results on the Tracr compiled model.</p>
<p>introduced a differentiable stack for learning Dyck languages.In Transformers, papers like Fernandez Astudillo et al. (2020) and DuSell and Chiang (2024) explored external memory modules to enhance long-range dependency modeling.While promising, these approaches require significant architectural changes.Our work focuses on whether Transformers can implicitly learn stack-like representations without explicit constraints, offering a more interpretable approach to modeling FL.</p>
<p>Importantly, we do not make any claims about the causality of these stacks. This is further discussed in the Section 5.
 Merrill, W. (2020). On the linguistic capacity of real-time counter automata. arXiv preprint arXiv:2004.06866.
 Weiss, Gail, Yoav Goldberg, and  Eran Yahav. "Thinking like transformers." International Conference on Machine Learning. PMLR, 2021.</p>
<p>Can language models encode perceptual structure without grounding? a case study in color. Mostafa Abdou, Artur Kulmizev, Daniel Hershcovich, Stella Frank, Ellie Pavlick, Anders Søgaard, 10.18653/v1/2021.conll-1.9Proceedings of the 25th Conference on Computational Natural Language Learning. the 25th Conference on Computational Natural Language LearningOnline. Association for Computational Linguistics2021</p>
<p>A survey of neural networks and formal languages. Joshua Ackerman, George Cybenko, arXiv:2006.013382020Preprint</p>
<p>Probing classifiers: Promises, shortcomings, and advances. Yonatan Belinkov, 10.1162/coli_a_00422Computational Linguistics. 4812022</p>
<p>On the ability and limitations of transformers to recognize formal languages. S Bhattamishra, Kabir Ahuja, Navin Goyal, Conference on Empirical Methods in Natural Language Processing. 2020</p>
<p>Visualizing and measuring the geometry of bert. Andy Coenen, Emily Reif, Ann Yuan, Been Kim, Adam Pearce, Fernanda Viégas, Martin Wattenberg, arXiv:1906.027152019Preprint</p>
<p>Stack attention: Improving the ability of transformers to model hierarchical patterns. Brian Dusell, David Chiang, arXiv:2310.017492024Preprint</p>
<p>Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam Mccandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, Christopher Olah, arXiv:2209.10652Toy models of superposition. 2022Preprint</p>
<p>Transition-based parsing with stack-transformers. Ramón Fernandez Astudillo, Miguel Ballesteros, Tahira Naseem, Austin Blodgett, Radu Florian, 10.18653/v1/2020.findings-emnlp.89Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Theoretical limitations of selfattention in neural sequence models. Michael Hahn, 10.1162/tacl_a_00306Transactions of the Association for Computational Linguistics. 82020</p>
<p>Why are sensitive functions hard for transformers?. Michael Hahn, Mark Rofin, 10.18653/v1/2024.acl-long.800Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Designing and interpreting probes with control tasks. John Hewitt, Percy Liang, arXiv:1909.033682019Preprint</p>
<p>Formal language theory: refining the chomsky hierarchy. Gerhard Jäger, James Rogers, Philosophical Transactions of the Royal Society B: Biological Sciences. 3672012</p>
<p>Inferring algorithmic patterns with stack-augmented recurrent nets. Armand Joulin, Tomas Mikolov, arXiv:1503.010072015Preprint</p>
<p>Natural languages and the Chomsky hierarchy. András Kornai, Second Conference of the European Chapter of the Association for Computational Linguistics. Geneva, SwitzerlandAssociation for Computational Linguistics1985</p>
<p>Implicit representations of meaning in neural language models. Belinda Z Li, Maxwell Nye, Jacob Andreas, 10.18653/v1/2021.acl-long.143Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Emergent world representations: Exploring a sequence model trained on a synthetic task. Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg, arXiv:2210.133822024Preprint</p>
<p>Tracr: Compiled transformers as a laboratory for interpretability. David Lindner, Janos Kramar, Sebastian Farquhar, Matthew Rahtz, Tom Mcgrath, Vladimir Mikulik, Advances in Neural Information Processing Systems. Curran Associates, Inc202336</p>
<p>Progress measures for grokking via mechanistic interpretability. Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, Jacob Steinhardt, The Eleventh International Conference on Learning Representations. </p>
<p>On the turing completeness of modern neural network architectures. Jorge Pérez, Javier Marinković, Pablo Barceló, arXiv:1901.034292019Preprint</p>
<p>A practical review of mechanistic interpretability for transformer-based language models. Daking Rai, Yilun Zhou, Shi Feng, Abulhair Saparov, Ziyu Yao, arXiv:2407.026462024Preprint</p>
<p>Anna Rogers, Olga Kovaleva, Anna Rumshisky, 10.1162/tacl_a_00349A primer in BERTology: What we know about how BERT works. 20208</p>
<p>What formal languages can transformers express? a survey. Lena Strobl, William Merrill, Gail Weiss, David Chiang, Dana Angluin, 10.1162/tacl_a_00663Transactions of the Association for Computational Linguistics. 122024a</p>
<p>What formal languages can transformers express? a survey. Lena Strobl, William Merrill, Gail Weiss, David Chiang, Dana Angluin, 10.1162/tacl_a_00663Transactions of the Association for Computational Linguistics. 122024b</p>
<p>Memory-augmented recurrent neural networks can learn generalized dyck languages. Mirac Suzgun, Sebastian Gehrmann, Yonatan Belinkov, Stuart M Shieber, arXiv:1911.033292019Preprint</p>
<p>Evaluating the world model implicit in a generative model. Keyon Vafa, Justin Y Chen, Ashesh Rambachan, Jon Kleinberg, Sendhil Mullainathan, arXiv:2406.036892024Preprint</p>
<p>Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, Ivan Titov, 10.18653/v1/P19-1580Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Thinking like transformers. Gail Weiss, Yoav Goldberg, Eran Yahav, International Conference on Machine Learning. PMLR2021</p>
<p>Are transformers universal approximators of sequence-tosequence functions?. Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, Sanjiv Kumar, arXiv:1912.100772020Preprint</p>
<p>Transformer-based models are not yet perfect at learning to emulate structural recursion. Dylan Zhang, Curt Tigges, Zory Zhang, Stella Biderman, Maxim Raginsky, Talia Ringer, arXiv:2401.129472024Preprint</p>
<p>What algorithms can transformers learn? a study in length generalization. Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh Susskind, Samy Bengio, Preetum Nakkiran, arXiv:2310.160282023Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7759 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7759</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7759</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-277313772</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.19257v1.pdf" target="_blank">SCI-IDEA: Context-Aware Scientific Ideation Using Token and Sentence Embeddings</a></p>
                <p><strong>Paper Abstract:</strong> Every scientific discovery starts with an idea inspired by prior work, interdisciplinary concepts, and emerging challenges. Recent advancements in large language models (LLMs) trained on scientific corpora have driven interest in AI-supported idea generation. However, generating context-aware, high-quality, and innovative ideas remains challenging. We introduce SCI-IDEA, a framework that uses LLM prompting strategies and Aha Moment detection for iterative idea refinement. SCI-IDEA extracts essential facets from research publications, assessing generated ideas on novelty, excitement, feasibility, and effectiveness. Comprehensive experiments validate SCI-IDEA's effectiveness, achieving average scores of 6.84, 6.86, 6.89, and 6.84 (on a 1-10 scale) across novelty, excitement, feasibility, and effectiveness, respectively. Evaluations employed GPT-4o, GPT-4.5, DeepSeek-32B (each under 2-shot prompting), and DeepSeek-70B (3-shot prompting), with token-level embeddings used for Aha Moment detection. Similarly, it achieves scores of 6.87, 6.86, 6.83, and 6.87 using GPT-4o under 5-shot prompting, GPT-4.5 under 3-shot prompting, DeepSeek-32B under zero-shot chain-of-thought prompting, and DeepSeek-70B under 5-shot prompting with sentence-level embeddings. We also address ethical considerations such as intellectual credit, potential misuse, and balancing human creativity with AI-driven ideation. Our results highlight SCI-IDEA's potential to facilitate the structured and flexible exploration of context-aware scientific ideas, supporting innovation while maintaining ethical standards.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7759.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7759.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Novelty (embedding-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Novelty measured via semantic dissimilarity (embedding-based cosine metric)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated novelty metric computed as 1 minus the maximum cosine similarity between the semantic embedding of a candidate idea and embeddings of other candidate ideas (embedding vectors computed with pre-trained transformers). Higher values indicate greater semantic distinctiveness from other generated ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o, GPT-4.5, DeepSeek-32B, DeepSeek-70B (LLMs used to generate ideas)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o (closed), GPT-4.5 (closed), DeepSeek-32B (32B), DeepSeek-70B (70B / DeepSeek-V3 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (LLM-assisted scientific ideation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>research idea / hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Novelty via semantic dissimilarity (cosine similarity on embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute dense semantic embeddings for each generated idea (mean over token embeddings truncated to 512 tokens using BERT / SciBERT). For each candidate idea c_i, calculate cosine similarity between c_i and every other c_j; novelty = 1 - max_j cosine_similarity(c_i, c_j).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Embedding-based novelty score (derived from cosine similarities) and corresponding human/LLM novelty rating (1–10)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Embedding novelty: 1 - max(cosine_similarity) producing a value in [0,1]; Human/LLM novelty rating: 1–10 scale (annotators and LLM evaluators rate novelty on 1–10).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Generated candidate idea set from SCI-IDEA experiments (100 researcher profiles; retrieved publications from CORE, arXiv, Semantic Scholar)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human experts (three annotators, PhD holders) rated novelty on a 1–10 scale; inter-rater reliability was analyzed (no numeric IRR value reported). LLM evaluators were also instructed to rate novelty with reference examples.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Embedding-derived novelty used alongside human/LLM novelty ratings. Reported human/LLM novelty ratings across experiments cluster around ~7.0 (e.g., many table entries show novelty ≈ 7.0–7.9 depending on model and prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Embedding-based novelty captures semantic distinctiveness among generated candidates but may not capture deeper transformative potential; dependent on the embedding model (BERT vs SciBERT) and truncation to 512 tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SCI-IDEA: Context-Aware Scientific Ideation Using Token and Sentence Embeddings', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7759.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7759.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Surprise (surprisal)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Surprise measured via negative log-likelihood (surprisal) using a pre-trained language model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated surprisal metric computed as the negative log-likelihood of a generated idea given its context, estimated with a pretrained language model; higher values indicate ideas that are less likely (more surprising) under the context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o, GPT-4.5, DeepSeek-32B, DeepSeek-70B (LLMs used to estimate/generate ideas); pretrained LM used to compute p(c_i | C) (not named beyond 'pretrained language model')</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>See model_name; surprisal computed with a pretrained LM (model not explicitly sized in text)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (ideation context modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>research idea / hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Surprise via negative log-likelihood (surprisal)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Estimate p(c_i | C) — the likelihood of the candidate idea given the context C — using a pretrained language model; compute Surprise(c_i) = -log p(c_i | C).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Surprise score (negative log-likelihood); examples in text show numeric surprisal values (e.g., example surprisal ≈ 3.5).</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Surprise = -log p(c_i | C). Higher numerical values mean more unexpected ideas given the context. The paper uses a numeric threshold (θ_s) to flag high-surprise ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Same SCI-IDEA generated idea contexts and candidate ideas; pretrained LM used for likelihood estimates (unspecified model reference)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Surprise is combined with embedding novelty to flag Aha moments; human reviewers participate in downstream assessment but surprisal itself is computed automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Surprisal values are used to detect Aha moments; example surprisal values given in text (e.g., 3.5). Threshold θ_s reported as 2.0 in example Aha detection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Log-likelihood estimates depend on the choice and calibration of the pretrained language model; surprisal does not directly measure practical impact or feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SCI-IDEA: Context-Aware Scientific Ideation Using Token and Sentence Embeddings', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7759.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7759.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Four-criteria hybrid evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid evaluation using Novelty, Excitement, Feasibility, and Effectiveness (human + LLM raters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-dimensional evaluation framework where both LLMs and human expert reviewers score generated ideas on four axes (Novelty, Excitement, Feasibility, Effectiveness) using a 1–10 scale, and the overall score is the average of these dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o, GPT-4.5, DeepSeek-32B, DeepSeek-70B (used both for idea generation and for LLM-based scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o (closed), GPT-4.5 (closed), DeepSeek-32B (32B), DeepSeek-70B (70B)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (LLM-assisted ideation evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation rubric for research ideas/hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human–LLM hybrid 4-criteria rating (Novelty, Excitement, Feasibility, Effectiveness)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>LLMs and human experts independently rate each idea on 1–10 for the four criteria using explicit instructions and reference examples; LLMs use structured prompts to emulate the scoring rubric. Final ranking and averages computed across criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Novelty, Excitement, Feasibility, Effectiveness (each 1–10); Overall = mean of the four scores.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Each criterion rated on a 1 (very low) to 10 (excellent) scale. Overall score is arithmetic mean of the four criterion scores.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SCI-IDEA experimental outputs over 100 researcher profiles; human evaluation subset described in paper (human evaluators scored outputs across embedding and prompting variants).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Three expert reviewers (PhD holders with top-tier publications) scored ideas on the 1–10 scale and provided free-text rationales; inter-rater reliability was analyzed though no numeric IRR reported. LLM evaluators were provided explicit evaluation instructions and example evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Multiple reported numeric results: e.g., aggregated average scores reported in abstract and tables (examples: average scores across models in abstract ~6.84–6.89; specific table entries show per-model/per-prompt averages and per-criterion scores such as novelty ≈7.1, excitement ≈7.6, feasibility ≈5.8 in some configurations). Human-evaluated best cases: sentence-level embeddings + GPT-4o (5-shot) achieved human average ≈8.25 in one reported result.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Subjectivity of human ratings and possible LLM evaluator biases; inter-rater agreement statistics not fully reported; human evaluation scale may not capture longer-term scientific impact.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SCI-IDEA: Context-Aware Scientific Ideation Using Token and Sentence Embeddings', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7759.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7759.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embedding strategies (token / sentence)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embedding strategies: token-level embeddings (BERT/SciBERT mean-token) and sentence-level embeddings (Sentence-BERT / sentence pooling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper evaluates three pipeline variants — no embeddings, token-level embeddings (mean of token vectors from BERT or SciBERT), and sentence-level embeddings — and compares their impact on novelty, excitement, feasibility, and effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Embedding models: bert-base-uncased (BERT), SciBERT; Sentence-level embeddings referenced (Sentence-BERT style). Idea-generation LLMs: GPT-4o, GPT-4.5, DeepSeek-V3 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Embedding models: BERT (bert-base-uncased), SciBERT (domain model); Sentence-BERT referenced (implementation unspecified).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP (semantic similarity and ideation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>preprocessing/evaluation method for idea novelty detection</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Embedding strategy ablation (no embeddings vs token-level vs sentence-level)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute embeddings for candidate ideas using different strategies: mean token embeddings (BERT/SciBERT) truncated to 512 tokens for token-level; sentence-level embeddings (Sentence-BERT style) for higher-level semantics. Use embeddings in novelty computation and Aha detection, and compare downstream human/LLM scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Four-criteria ratings (1–10) and changes in those ratings across embedding strategies; embedding novelty (0–1) used internally.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Observed effects measured as per-criterion 1–10 scores and averaged scores reported in tables (e.g., token-level variant improved feasibility/effectiveness; sentence-level improved novelty/excitement).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SCI-IDEA generated ideas over 100 researcher profiles; embedding models applied to the candidate idea corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human reviewers scored outputs for each embedding variant; comparisons made across variants in tables (Tables 3–5).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Token-level embeddings improved feasibility/effectiveness (example: DeepSeek-32B with token-level 2-shot avg ≈6.89); sentence-level embeddings improved novelty/excitement (example: GPT-4o with sentence-level 5-shot avg ≈6.87 and human score ≈8.25 reported in one setting).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Embedding choice and pooling strategy affect results; truncation to 512 tokens may lose contextual information; sentence vs token granularity trade-offs noted.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SCI-IDEA: Context-Aware Scientific Ideation Using Token and Sentence Embeddings', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7759.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7759.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aha moment detection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aha Moment detection via combined novelty and surprise thresholds</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decision rule that flags generated ideas as 'Aha moments' when they simultaneously exceed researcher-defined thresholds on embedding-based novelty and surprisal (negative log-likelihood). When flagged, special deep-dive prompting templates are applied to refine the idea.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Idea generation LLMs (GPT-4o, GPT-4.5, DeepSeek variants) and pretrained LM used for surprisal computation</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>See model_name; surprisal LM unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / scientific ideation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>classification/flagging of high-value generated ideas</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Aha moment detection (thresholded novelty and surprisal)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute novelty(c_i) and surprise(c_i); declare idea an Aha moment if novelty >= θ_n and surprise >= θ_s. Example thresholds provided: θ_n = 0.7, θ_s = 2.0. Trigger an 'Aha Prompt' template for deeper exploration and refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Binary Aha flag (true/false) based on thresholds, plus follow-up qualitative refinement and re-evaluation on 4-criteria scores.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Novelty measured 0–1 (embedding-based), Surprise measured as -log p(c_i|C). Aha condition: novelty ≥ θ_n AND surprise ≥ θ_s.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Candidate ideas generated during SCI-IDEA runs on 100 researcher profiles</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Aha-flagged ideas are further refined and evaluated by human experts as part of the iterative loop; the Aha threshold values are researcher-configurable.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Paper describes use of thresholds (example θ_n = 0.7 and θ_s = 2.0) and demonstrates that flagged Aha moments undergo deeper LLM-driven refinement; quantitative counts of Aha flags are not tabulated in the main results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Fixed thresholds may be brittle across domains; novelty and surprisal metrics have the limitations noted above; not all Aha-flagged ideas translate to practicality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SCI-IDEA: Context-Aware Scientific Ideation Using Token and Sentence Embeddings', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7759.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7759.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompting strategies</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompting strategies: zero-shot, zero-shot chain-of-thought (ZS-CoT), and few-shot (2/3/5-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different LLM prompt formats used to generate candidate ideas and to evaluate them, including plain zero-shot, zero-shot chain-of-thought for stepwise reasoning, and few-shot prompting with example demonstrations (2, 3, and 5 shots). The paper ablates performance across these strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o, GPT-4.5, DeepSeek-32B, DeepSeek-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>See model_name entries above; few-shot counts: 2-shot, 3-shot, 5-shot</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / LLM prompting for ideation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>generation/control technique for LLM-produced hypotheses/ideas</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Prompting strategy ablation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compare outputs and downstream evaluation scores (Novelty, Excitement, Feasibility, Effectiveness) across prompt strategies: zero-shot (ZS), zero-shot chain-of-thought (ZS-CoT), and few-shot (2/3/5 examples).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Per-criterion 1–10 ratings and averaged scores reported per model and prompt configuration (tables 3–5).</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Scores are 1–10 per criterion; overall average is arithmetic mean. The paper reports that few-shot prompting generally improves feasibility and effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SCI-IDEA outputs over 100 researcher profiles used as input contexts for prompting experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human evaluators scored outputs generated under each prompting condition; ablation comparisons made (few-shot often outperforms zero-shot and ZS-CoT on feasibility/effectiveness).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Example observations: Few-shot prompting consistently outperforms zero-shot and ZS-CoT in feasibility/effectiveness; DeepSeek-32B achieved highest feasibility with 2-shot token-level embeddings (feasibility ≈6.28 in some reported cells).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Prompt sensitivity and shot selection affect outputs; prompt engineering is resource- and expertise-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SCI-IDEA: Context-Aware Scientific Ideation Using Token and Sentence Embeddings', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7759.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7759.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation dataset (100 profiles)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curated dataset of 100 researcher profiles (publications, ORCID, Google Scholar metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset curated for SCI-IDEA experiments consisting of 100 researchers' profiles (names, ORCID IDs, publicly available publications) drawn from top-tier computer science venues; used to retrieve context and evaluate idea generation at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used as input context for LLMs (GPT-4o, GPT-4.5, DeepSeek variants) to generate ideas</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A (dataset rather than a model)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (experimental corpus for ideation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>dataset / benchmark for ideation experiments</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Context corpus for SCI-IDEA (retrieval + facet extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each researcher, keyphrases were extracted to retrieve related publications (from CORE, arXiv, Semantic Scholar); profile-level metrics (publications counts, citations, h-index, i10-index) computed from Google Scholar to inform experiments and analyze generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Not an evaluation metric per se; used to produce contexts and measure per-profile performance. Coverage described by topic distributions in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Dataset composition: 100 researcher profiles, topic and metric distributions reported (no single external benchmark name).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Curated SCI-IDEA dataset (100 researcher profiles) with retrieval sources CORE, arXiv, Semantic Scholar; Google Scholar used for profile metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Three human annotators verified dataset relevance and keyphrase extraction; details: balanced representation of senior and early-career researchers to reduce selection bias.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Dataset used to run all experiments reported in paper; limitations acknowledged (selection bias, reliance on Google Scholar).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Potential selection biases, domain focus on computer science, reliance on Google Scholar data for metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SCI-IDEA: Context-Aware Scientific Ideation Using Token and Sentence Embeddings', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Ideabench: Benchmarking large language models for research idea generation <em>(Rating: 2)</em></li>
                <li>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers <em>(Rating: 2)</em></li>
                <li>Scideator: Human-llm scientific idea generation grounded in research-paper facet recombination <em>(Rating: 2)</em></li>
                <li>Interesting scientific idea generation using knowledge graphs and llms: Evaluations with 100 research group leaders <em>(Rating: 2)</em></li>
                <li>Learning to generate research idea with dynamic control <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7759",
    "paper_id": "paper-277313772",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "Novelty (embedding-based)",
            "name_full": "Novelty measured via semantic dissimilarity (embedding-based cosine metric)",
            "brief_description": "An automated novelty metric computed as 1 minus the maximum cosine similarity between the semantic embedding of a candidate idea and embeddings of other candidate ideas (embedding vectors computed with pre-trained transformers). Higher values indicate greater semantic distinctiveness from other generated ideas.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o, GPT-4.5, DeepSeek-32B, DeepSeek-70B (LLMs used to generate ideas)",
            "model_size": "GPT-4o (closed), GPT-4.5 (closed), DeepSeek-32B (32B), DeepSeek-70B (70B / DeepSeek-V3 variants)",
            "scientific_domain": "computer science (LLM-assisted scientific ideation)",
            "theory_type": "research idea / hypothesis generation",
            "evaluation_method_name": "Novelty via semantic dissimilarity (cosine similarity on embeddings)",
            "evaluation_method_description": "Compute dense semantic embeddings for each generated idea (mean over token embeddings truncated to 512 tokens using BERT / SciBERT). For each candidate idea c_i, calculate cosine similarity between c_i and every other c_j; novelty = 1 - max_j cosine_similarity(c_i, c_j).",
            "evaluation_metric": "Embedding-based novelty score (derived from cosine similarities) and corresponding human/LLM novelty rating (1–10)",
            "metric_definition": "Embedding novelty: 1 - max(cosine_similarity) producing a value in [0,1]; Human/LLM novelty rating: 1–10 scale (annotators and LLM evaluators rate novelty on 1–10).",
            "dataset_or_benchmark": "Generated candidate idea set from SCI-IDEA experiments (100 researcher profiles; retrieved publications from CORE, arXiv, Semantic Scholar)",
            "human_evaluation_details": "Human experts (three annotators, PhD holders) rated novelty on a 1–10 scale; inter-rater reliability was analyzed (no numeric IRR value reported). LLM evaluators were also instructed to rate novelty with reference examples.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Embedding-derived novelty used alongside human/LLM novelty ratings. Reported human/LLM novelty ratings across experiments cluster around ~7.0 (e.g., many table entries show novelty ≈ 7.0–7.9 depending on model and prompt).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Embedding-based novelty captures semantic distinctiveness among generated candidates but may not capture deeper transformative potential; dependent on the embedding model (BERT vs SciBERT) and truncation to 512 tokens.",
            "uuid": "e7759.0",
            "source_info": {
                "paper_title": "SCI-IDEA: Context-Aware Scientific Ideation Using Token and Sentence Embeddings",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Surprise (surprisal)",
            "name_full": "Surprise measured via negative log-likelihood (surprisal) using a pre-trained language model",
            "brief_description": "An automated surprisal metric computed as the negative log-likelihood of a generated idea given its context, estimated with a pretrained language model; higher values indicate ideas that are less likely (more surprising) under the context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o, GPT-4.5, DeepSeek-32B, DeepSeek-70B (LLMs used to estimate/generate ideas); pretrained LM used to compute p(c_i | C) (not named beyond 'pretrained language model')",
            "model_size": "See model_name; surprisal computed with a pretrained LM (model not explicitly sized in text)",
            "scientific_domain": "computer science (ideation context modeling)",
            "theory_type": "research idea / hypothesis generation",
            "evaluation_method_name": "Surprise via negative log-likelihood (surprisal)",
            "evaluation_method_description": "Estimate p(c_i | C) — the likelihood of the candidate idea given the context C — using a pretrained language model; compute Surprise(c_i) = -log p(c_i | C).",
            "evaluation_metric": "Surprise score (negative log-likelihood); examples in text show numeric surprisal values (e.g., example surprisal ≈ 3.5).",
            "metric_definition": "Surprise = -log p(c_i | C). Higher numerical values mean more unexpected ideas given the context. The paper uses a numeric threshold (θ_s) to flag high-surprise ideas.",
            "dataset_or_benchmark": "Same SCI-IDEA generated idea contexts and candidate ideas; pretrained LM used for likelihood estimates (unspecified model reference)",
            "human_evaluation_details": "Surprise is combined with embedding novelty to flag Aha moments; human reviewers participate in downstream assessment but surprisal itself is computed automatically.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Surprisal values are used to detect Aha moments; example surprisal values given in text (e.g., 3.5). Threshold θ_s reported as 2.0 in example Aha detection.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Log-likelihood estimates depend on the choice and calibration of the pretrained language model; surprisal does not directly measure practical impact or feasibility.",
            "uuid": "e7759.1",
            "source_info": {
                "paper_title": "SCI-IDEA: Context-Aware Scientific Ideation Using Token and Sentence Embeddings",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Four-criteria hybrid evaluation",
            "name_full": "Hybrid evaluation using Novelty, Excitement, Feasibility, and Effectiveness (human + LLM raters)",
            "brief_description": "A multi-dimensional evaluation framework where both LLMs and human expert reviewers score generated ideas on four axes (Novelty, Excitement, Feasibility, Effectiveness) using a 1–10 scale, and the overall score is the average of these dimensions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o, GPT-4.5, DeepSeek-32B, DeepSeek-70B (used both for idea generation and for LLM-based scoring)",
            "model_size": "GPT-4o (closed), GPT-4.5 (closed), DeepSeek-32B (32B), DeepSeek-70B (70B)",
            "scientific_domain": "computer science (LLM-assisted ideation evaluation)",
            "theory_type": "evaluation rubric for research ideas/hypotheses",
            "evaluation_method_name": "Human–LLM hybrid 4-criteria rating (Novelty, Excitement, Feasibility, Effectiveness)",
            "evaluation_method_description": "LLMs and human experts independently rate each idea on 1–10 for the four criteria using explicit instructions and reference examples; LLMs use structured prompts to emulate the scoring rubric. Final ranking and averages computed across criteria.",
            "evaluation_metric": "Novelty, Excitement, Feasibility, Effectiveness (each 1–10); Overall = mean of the four scores.",
            "metric_definition": "Each criterion rated on a 1 (very low) to 10 (excellent) scale. Overall score is arithmetic mean of the four criterion scores.",
            "dataset_or_benchmark": "SCI-IDEA experimental outputs over 100 researcher profiles; human evaluation subset described in paper (human evaluators scored outputs across embedding and prompting variants).",
            "human_evaluation_details": "Three expert reviewers (PhD holders with top-tier publications) scored ideas on the 1–10 scale and provided free-text rationales; inter-rater reliability was analyzed though no numeric IRR reported. LLM evaluators were provided explicit evaluation instructions and example evaluations.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Multiple reported numeric results: e.g., aggregated average scores reported in abstract and tables (examples: average scores across models in abstract ~6.84–6.89; specific table entries show per-model/per-prompt averages and per-criterion scores such as novelty ≈7.1, excitement ≈7.6, feasibility ≈5.8 in some configurations). Human-evaluated best cases: sentence-level embeddings + GPT-4o (5-shot) achieved human average ≈8.25 in one reported result.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Subjectivity of human ratings and possible LLM evaluator biases; inter-rater agreement statistics not fully reported; human evaluation scale may not capture longer-term scientific impact.",
            "uuid": "e7759.2",
            "source_info": {
                "paper_title": "SCI-IDEA: Context-Aware Scientific Ideation Using Token and Sentence Embeddings",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Embedding strategies (token / sentence)",
            "name_full": "Embedding strategies: token-level embeddings (BERT/SciBERT mean-token) and sentence-level embeddings (Sentence-BERT / sentence pooling)",
            "brief_description": "The paper evaluates three pipeline variants — no embeddings, token-level embeddings (mean of token vectors from BERT or SciBERT), and sentence-level embeddings — and compares their impact on novelty, excitement, feasibility, and effectiveness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Embedding models: bert-base-uncased (BERT), SciBERT; Sentence-level embeddings referenced (Sentence-BERT style). Idea-generation LLMs: GPT-4o, GPT-4.5, DeepSeek-V3 variants.",
            "model_size": "Embedding models: BERT (bert-base-uncased), SciBERT (domain model); Sentence-BERT referenced (implementation unspecified).",
            "scientific_domain": "computer science / NLP (semantic similarity and ideation)",
            "theory_type": "preprocessing/evaluation method for idea novelty detection",
            "evaluation_method_name": "Embedding strategy ablation (no embeddings vs token-level vs sentence-level)",
            "evaluation_method_description": "Compute embeddings for candidate ideas using different strategies: mean token embeddings (BERT/SciBERT) truncated to 512 tokens for token-level; sentence-level embeddings (Sentence-BERT style) for higher-level semantics. Use embeddings in novelty computation and Aha detection, and compare downstream human/LLM scores.",
            "evaluation_metric": "Four-criteria ratings (1–10) and changes in those ratings across embedding strategies; embedding novelty (0–1) used internally.",
            "metric_definition": "Observed effects measured as per-criterion 1–10 scores and averaged scores reported in tables (e.g., token-level variant improved feasibility/effectiveness; sentence-level improved novelty/excitement).",
            "dataset_or_benchmark": "SCI-IDEA generated ideas over 100 researcher profiles; embedding models applied to the candidate idea corpus.",
            "human_evaluation_details": "Human reviewers scored outputs for each embedding variant; comparisons made across variants in tables (Tables 3–5).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Token-level embeddings improved feasibility/effectiveness (example: DeepSeek-32B with token-level 2-shot avg ≈6.89); sentence-level embeddings improved novelty/excitement (example: GPT-4o with sentence-level 5-shot avg ≈6.87 and human score ≈8.25 reported in one setting).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Embedding choice and pooling strategy affect results; truncation to 512 tokens may lose contextual information; sentence vs token granularity trade-offs noted.",
            "uuid": "e7759.3",
            "source_info": {
                "paper_title": "SCI-IDEA: Context-Aware Scientific Ideation Using Token and Sentence Embeddings",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Aha moment detection",
            "name_full": "Aha Moment detection via combined novelty and surprise thresholds",
            "brief_description": "A decision rule that flags generated ideas as 'Aha moments' when they simultaneously exceed researcher-defined thresholds on embedding-based novelty and surprisal (negative log-likelihood). When flagged, special deep-dive prompting templates are applied to refine the idea.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Idea generation LLMs (GPT-4o, GPT-4.5, DeepSeek variants) and pretrained LM used for surprisal computation",
            "model_size": "See model_name; surprisal LM unspecified",
            "scientific_domain": "computer science / scientific ideation",
            "theory_type": "classification/flagging of high-value generated ideas",
            "evaluation_method_name": "Aha moment detection (thresholded novelty and surprisal)",
            "evaluation_method_description": "Compute novelty(c_i) and surprise(c_i); declare idea an Aha moment if novelty &gt;= θ_n and surprise &gt;= θ_s. Example thresholds provided: θ_n = 0.7, θ_s = 2.0. Trigger an 'Aha Prompt' template for deeper exploration and refinement.",
            "evaluation_metric": "Binary Aha flag (true/false) based on thresholds, plus follow-up qualitative refinement and re-evaluation on 4-criteria scores.",
            "metric_definition": "Novelty measured 0–1 (embedding-based), Surprise measured as -log p(c_i|C). Aha condition: novelty ≥ θ_n AND surprise ≥ θ_s.",
            "dataset_or_benchmark": "Candidate ideas generated during SCI-IDEA runs on 100 researcher profiles",
            "human_evaluation_details": "Aha-flagged ideas are further refined and evaluated by human experts as part of the iterative loop; the Aha threshold values are researcher-configurable.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Paper describes use of thresholds (example θ_n = 0.7 and θ_s = 2.0) and demonstrates that flagged Aha moments undergo deeper LLM-driven refinement; quantitative counts of Aha flags are not tabulated in the main results.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Fixed thresholds may be brittle across domains; novelty and surprisal metrics have the limitations noted above; not all Aha-flagged ideas translate to practicality.",
            "uuid": "e7759.4",
            "source_info": {
                "paper_title": "SCI-IDEA: Context-Aware Scientific Ideation Using Token and Sentence Embeddings",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Prompting strategies",
            "name_full": "Prompting strategies: zero-shot, zero-shot chain-of-thought (ZS-CoT), and few-shot (2/3/5-shot)",
            "brief_description": "Different LLM prompt formats used to generate candidate ideas and to evaluate them, including plain zero-shot, zero-shot chain-of-thought for stepwise reasoning, and few-shot prompting with example demonstrations (2, 3, and 5 shots). The paper ablates performance across these strategies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o, GPT-4.5, DeepSeek-32B, DeepSeek-70B",
            "model_size": "See model_name entries above; few-shot counts: 2-shot, 3-shot, 5-shot",
            "scientific_domain": "computer science / LLM prompting for ideation",
            "theory_type": "generation/control technique for LLM-produced hypotheses/ideas",
            "evaluation_method_name": "Prompting strategy ablation",
            "evaluation_method_description": "Compare outputs and downstream evaluation scores (Novelty, Excitement, Feasibility, Effectiveness) across prompt strategies: zero-shot (ZS), zero-shot chain-of-thought (ZS-CoT), and few-shot (2/3/5 examples).",
            "evaluation_metric": "Per-criterion 1–10 ratings and averaged scores reported per model and prompt configuration (tables 3–5).",
            "metric_definition": "Scores are 1–10 per criterion; overall average is arithmetic mean. The paper reports that few-shot prompting generally improves feasibility and effectiveness.",
            "dataset_or_benchmark": "SCI-IDEA outputs over 100 researcher profiles used as input contexts for prompting experiments.",
            "human_evaluation_details": "Human evaluators scored outputs generated under each prompting condition; ablation comparisons made (few-shot often outperforms zero-shot and ZS-CoT on feasibility/effectiveness).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Example observations: Few-shot prompting consistently outperforms zero-shot and ZS-CoT in feasibility/effectiveness; DeepSeek-32B achieved highest feasibility with 2-shot token-level embeddings (feasibility ≈6.28 in some reported cells).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Prompt sensitivity and shot selection affect outputs; prompt engineering is resource- and expertise-dependent.",
            "uuid": "e7759.5",
            "source_info": {
                "paper_title": "SCI-IDEA: Context-Aware Scientific Ideation Using Token and Sentence Embeddings",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Evaluation dataset (100 profiles)",
            "name_full": "Curated dataset of 100 researcher profiles (publications, ORCID, Google Scholar metrics)",
            "brief_description": "A dataset curated for SCI-IDEA experiments consisting of 100 researchers' profiles (names, ORCID IDs, publicly available publications) drawn from top-tier computer science venues; used to retrieve context and evaluate idea generation at scale.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Used as input context for LLMs (GPT-4o, GPT-4.5, DeepSeek variants) to generate ideas",
            "model_size": "N/A (dataset rather than a model)",
            "scientific_domain": "computer science (experimental corpus for ideation)",
            "theory_type": "dataset / benchmark for ideation experiments",
            "evaluation_method_name": "Context corpus for SCI-IDEA (retrieval + facet extraction)",
            "evaluation_method_description": "For each researcher, keyphrases were extracted to retrieve related publications (from CORE, arXiv, Semantic Scholar); profile-level metrics (publications counts, citations, h-index, i10-index) computed from Google Scholar to inform experiments and analyze generalizability.",
            "evaluation_metric": "Not an evaluation metric per se; used to produce contexts and measure per-profile performance. Coverage described by topic distributions in Table 2.",
            "metric_definition": "Dataset composition: 100 researcher profiles, topic and metric distributions reported (no single external benchmark name).",
            "dataset_or_benchmark": "Curated SCI-IDEA dataset (100 researcher profiles) with retrieval sources CORE, arXiv, Semantic Scholar; Google Scholar used for profile metrics.",
            "human_evaluation_details": "Three human annotators verified dataset relevance and keyphrase extraction; details: balanced representation of senior and early-career researchers to reduce selection bias.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Dataset used to run all experiments reported in paper; limitations acknowledged (selection bias, reliance on Google Scholar).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Potential selection biases, domain focus on computer science, reliance on Google Scholar data for metrics.",
            "uuid": "e7759.6",
            "source_info": {
                "paper_title": "SCI-IDEA: Context-Aware Scientific Ideation Using Token and Sentence Embeddings",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Ideabench: Benchmarking large language models for research idea generation",
            "rating": 2,
            "sanitized_title": "ideabench_benchmarking_large_language_models_for_research_idea_generation"
        },
        {
            "paper_title": "Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers",
            "rating": 2,
            "sanitized_title": "can_llms_generate_novel_research_ideas_a_largescale_human_study_with_100_nlp_researchers"
        },
        {
            "paper_title": "Scideator: Human-llm scientific idea generation grounded in research-paper facet recombination",
            "rating": 2,
            "sanitized_title": "scideator_humanllm_scientific_idea_generation_grounded_in_researchpaper_facet_recombination"
        },
        {
            "paper_title": "Interesting scientific idea generation using knowledge graphs and llms: Evaluations with 100 research group leaders",
            "rating": 2,
            "sanitized_title": "interesting_scientific_idea_generation_using_knowledge_graphs_and_llms_evaluations_with_100_research_group_leaders"
        },
        {
            "paper_title": "Learning to generate research idea with dynamic control",
            "rating": 1,
            "sanitized_title": "learning_to_generate_research_idea_with_dynamic_control"
        }
    ],
    "cost": 0.01799775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SCI-IDEA: Context-Aware Scientific Ideation Using Token and Sentence Embeddings
25 Mar 2025</p>
<p>Farhana Keya 
TIB-Leibniz Information Centre for Science and Technology
Germany{farhana.keyaHannover</p>
<p>Gollam Rabby gollam.rabby@l3s.de 
L3S Research Center
Leibniz University Hannover
HannoverGermany</p>
<p>Sahar Vahdati sahar.vahdati@tib.eu 
TIB-Leibniz Information Centre for Science and Technology
Germany{farhana.keyaHannover</p>
<p>Sören Auer auer@tib.eu 
TIB-Leibniz Information Centre for Science and Technology
Germany{farhana.keyaHannover</p>
<p>L3S Research Center
Leibniz University Hannover
HannoverGermany</p>
<p>Yaser Jaradeh jaradeh@l3s.de 
TIB-Leibniz Information Centre for Science and Technology
Germany{farhana.keyaHannover</p>
<p>L3S Research Center
Leibniz University Hannover
HannoverGermany</p>
<p>SCI-IDEA: Context-Aware Scientific Ideation Using Token and Sentence Embeddings
25 Mar 2025A4BCFBFF71781747C3AE2E098A8DAA7AarXiv:2503.19257v1[cs.CL]Scientific Idea GenerationAha Moment DetectionContext-aware ResponseLarge Language ModelsAI Research Assistant Hydrophobic coatings preventing dust accumulation. * Evaluation: Field tests showing 30% maintenance cost reduction. * Future Work: Develop coatings with additional heat-reflective properties.New Research Idea: * Idea Title: Dual-Purpose Nanocoatings for Solar Panels * Description: Develop nanocoatings that not only enhance solar panel efficiency but also possess self-cleaning properties, reducing maintenance costs and improving long-term performance
Every scientific discovery starts with an idea inspired by prior work, interdisciplinary concepts, and emerging challenges.Recent advancements in large language models (LLMs) trained on scientific corpora have driven interest in AI-supported idea generation.However, generating context-aware, high-quality, and innovative ideas remains challenging.We introduce SCI-IDEA, a framework that uses LLM prompting strategies and "Aha Moment" detection for iterative idea refinement.SCI-IDEA extracts essential facets from research publications, assessing generated ideas on novelty, excitement, feasibility, and effectiveness.Comprehensive experiments validate SCI-IDEA's effectiveness, achieving average scores of 6.84, 6.86, 6.89, and 6.84 (on a 1-10 scale) across novelty, excitement, feasibility, and effectiveness, respectively.Evaluations employed GPT-4o, GPT-4.5, DeepSeek-32B (each under 2-shot prompting), and DeepSeek-70B (3-shot prompting), with token-level embeddings used for Aha Moment detection.Similarly, it achieves scores of 6.87, 6.86, 6.83, and 6.87 using GPT-4o under 5-shot prompting, GPT-4.5 under 3-shot prompting, DeepSeek-32B under zero-shot chain-of-thought prompting, and DeepSeek-70B under 5-shot prompting with sentencelevel embeddings.We also address ethical considerations such as intellectual credit, potential misuse, and balancing human creativity with AI-driven ideation.Our results highlight SCI-IDEA's potential to facilitate the structured and flexible exploration of context-aware scientific ideas, supporting innovation while maintaining ethical standards.</p>
<p>Introduction</p>
<p>Ideation is the first step of any scientific project, involving the formulation of its purpose, research question, and hypotheses [29].Scientists generate ideas at different rates due to epistemic motivation [14].Science of science has proved that social and collaborative environments also play a crucial role, as mentorship, thought collectives, and teamwork in boosting the chances of groundbreaking discoveries [4,5].Not every scientist possesses all these traits at once.Large Language Models (LLMs) enhance scientific ideation by stimulating those factors mentioned above and beyond [16].They act as virtual thought collectives, integrating multidisciplinary knowledge and historical breakthroughs.Additionally, LLMs and different agent-based frameworks serve as interactive brainstorming partners, refining hypotheses and fostering interdisciplinary collaboration, bridging gaps between research communities [10,22].Recently, there has been growing attention on leveraging LLMs for scientific ideation [27], yet existing methods often fail to balance novelty, relevance, and computational efficiency [12,2,10].SCI-IDEA starts by extracting key facets from a researcher's prior work or related literature to identify research gaps and opportunities for innovation.The framework employs different prompting strategies based on researcher requirements: (1) Zero-shot prompting for straightforward context-aware idea generation, (2) Zero-shot chain-of-thought prompting for reasoning through multi-step research gaps, and (3) Few-shot prompting for tasks requiring domain-specific context.These strategies guide the LLM in generating context-aware scientific ideas while ensuring adaptability to diverse research queries.Generated ideas are evaluated for novelty and surprise using semantic embedding methods to identify transformative "Aha moments".Novelty is measured by semantic dissimilarity (cosine similarity on embeddings [26]).Surprise is measured by the negative log-likelihood of ideas given their context, estimated using a pre-trained language model [25].SCI-IDEA incorporates these metrics into a human-in-the-loop process, dynamically balancing idea exploration and refinement.</p>
<p>In such AI-assisted systems for science, evaluating the quality of generated text in general, and in this case, the quality of the generated ideas, remains a key challenge.In the SCI-IDEA framework, we have employed a systematic evaluation process for quality assessment of context-aware scientific ideas using four criteria: Novelty (originality), Excitement (engagement), Feasibility (implementation potential), and Effectiveness (goal achievement) [27].Human experts and LLMs independently score ideas on a 1-10 scale across these dimensions.This multi-dimensional evaluation aligns with recent AI-assisted ideation studies [27], ensuring the assessment of strengths and limitations.</p>
<p>Related Work.Based on the literature review that we conducted, existing methods for scientific ideation often rely on a single prompting strategy and fail to adapt to the iterative refinement of ideas [12,15,24] and only focus on domain-driven tasks [9,18].Additionally, traditional methods lack systematic mechanisms to evaluate the novelty and impact of generated ideas, often producing outputs that are either too conventional or irrelevant [8,10,15].SCI-IDEA addresses these limitations by dynamically evaluating novelty and surprise, enabling the identification of transformative ideas.It leverages insights from researchers' prior work and related literature to generate impactful ideas aligned with their expertise.Motivating Example.SCI-IDEA generates context-aware scientific ideas, such as: Using spiking neural networks (SNNs) with Group Relative Policy Optimization (GRPO) to optimize energy-efficient training of deep reinforcement learning (DRL) agents.A researcher provides a query (e.g., How can we improve the energy efficiency of training DRL agents?), and SCI-IDEA retrieves relevant publications from sources like arXiv and Semantic Scholar.Key facets-objectives (e.g., improving energy efficiency), methodologies (e.g., DRL, SNNs), and future work (e.g., hybrid DRL-SNN approaches)-are extracted using LLMs.SCI-IDEA identifies research gaps, such as DRL's high energy consumption [19] and SNNs' underutilization in reinforcement learning [28] [20], forming the basis for idea generation.Using diverse prompting strategies, SCI-IDEA generates ideas.Zeroshot prompting explores energy-efficient alternatives to DRL, while zero-shot chain-of-thought prompting reasons step-by-step: (1) identifying energy-efficient architectures, (2) adapting them for reinforcement learning, and (3) applying them to agent control.Few-shot prompting provides examples (e.g., combining CNNs with reinforcement learning) to guide output.Ideas are evaluated for novelty (via cosine similarity [3]) and surprise (via negative log-likelihood [25]).High novelty and surprise ideas are flagged as Aha moments.Researcher feedback refines the idea, leading to: Using SNNs with GRPO to optimize energyefficient training of DRL agents.The final output is a ranked list of ideas, with the top idea summarized by its novelty (e.g., integrating SNNs, GRPO, and DRL), impact (e.g., reducing DRL energy consumption), and feasibility (e.g., supported by recent advancements) [28] [19].This demonstrates SCI-IDEA's ability to generate, evaluate, and refine high-impact scientific ideas through an iterative, human-in-the-loop process, as illustrated in Figure 1.</p>
<p>SCI-IDEA: Scientific Idea Generation Framework</p>
<p>Problem Formalization.Let us consider, given a researcher's scientific identifier (e.g., ORCID ID or institutional profile), the system accesses related research publications P = {p 1 , p 2 , . . ., p n } from academic sources such as CORE 3 , arXiv 4 , and Semantic Scholar5 .Each publication p i is represented by its associated facets, including objectives, methodologies, evaluation, and future work.It is also possible that given a query q, the system extracts keyphrases and retrieves a set of related publications A = {a 1 , a 2 , . . ., a m } with corresponding facets from the same sources.A researcher using the system can then select a subset of their publications P selected ⊆ P to include in the ideation process.The goal is to generate a set of novel, impactful, and actionable scientific ideas
I = {i 1 , i 2 , . . . , i k }.
Example of Researcher's Input and Facets.Let us assume the query of the researcher to SCI-IDEA is: How can we improve the energy efficiency of training deep reinforcement learning agents?SCI-IDEA extracts keyphrases such as energy efficiency, deep reinforcement learning, and agents to retrieve similar publications.Key facets extracted from the researcher's selected publications and similar literature include: 1) Objectives: Improving energy efficiency in agents.2) Methodologies: Deep reinforcement learning (DRL), spiking neural networks (SNNs).3) Evaluation: High energy consumption in DRL, potential of SNNs for low-power computation.4) Future Work: Exploring hybrid approaches combining DRL and SNNs.</p>
<p>Research Gap Identification.SCI-IDEA identifies research gaps by analyzing the structured facets extracted from a given profile and related research publications.For instance, it detects that while DRL is effective for agent control, its high energy consumption remains a significant limitation [28].Conversely, SNNs are known for their energy efficiency but have not been extensively applied to reinforcement learning.This gap serves as a foundation for generating research gaps.Our approach employs different prompting strategies in generating candidate ideas C = {c 1 , c 2 , . . ., c p } (Details can be found in the Appendix, Section: A).For example, using zero-shot chain-of-thought prompting, the underlying LLM might generate: Step 1: Identify energy-efficient neural architectures.Step 2: Adapt these architectures for reinforcement learning.Step 3: Apply them to agents' control tasks.This process results in the research gap: How can spiking neural networks (SNNs) be effectively integrated into deep reinforcement learning (DRL) to optimize energy-efficient training while maintaining performance and stability?</p>
<p>Evaluation of Novelty and Surprise.The generated ideas are evaluated for two metrics of novelty and surprise, using state-of-the-art semantic embedding approaches.The novelty of an idea c i is computed as:
Novelty(c i ) = 1 − max(Cosine_similarity(c i , c j )) ∀c j ∈ C
where Cosine_similarity(c i , c j ) is the cosine similarity between the semantic embeddings of c i and c j .For example, if c i is Using SNNs for energy-efficient DRL training and c j is Using CNNs for agents vision, the low similarity score confirms its novelty among the generated ideas.Surprise of c i is quantified as:
Surprise(c i ) = − log p(c i | C)
where p(c i | C) is the likelihood of c i given the context C, computed using a pretrained language model [7,3,26].For example, the idea Using SNNs for energyefficient DRL training might have a high surprise score if it is unexpected yet reasonable.An idea is flagged as an Aha moment if it satisfies:</p>
<p>SCI-IDEA Framework Overview</p>
<p>SCI-IDEA is a framework for generating, evaluating, and refining context-aware scientific ideas through an iterative, human-in-the-loop process.The framework comprises two core modules: (1) Context and Research Gap Identification, and (2) Idea Generation and Iterative Refinement, as illustrated in Figure 2. The first module retrieves the publications of a given profile as well as related ones using a scientific identifier and research query.Key facets of these publications are extracted to identify research gaps, which serve as the foundation for generating context-aware scientific ideas using diverse prompting strategies (subsection 2.2).</p>
<p>(Details can be found in the Appendix, Section: A).The second module evaluates ideas for novelty and surprise using a pre-trained language model.Novelty is quantified by semantic dissimilarity, while the likelihood of the SCI-IDEA framework-generated ideas measures surprise.Ideas exceeding predefined thresholds for novelty and surprise are flagged as Aha moments.The module enables iterative refinement, dynamically updating the context to balance exploration and exploitation, ensuring high-quality, context-aware results (subsection 2.3).</p>
<p>Module 1: Context and Research Gap Identification</p>
<p>Context Retrieval.This process begins with a researcher providing their scientific identifier (e.g., ORCID ID: 0000-0002-1825-0097) and a research query, such as: How can we improve the energy efficiency of AI models?.Using the identifier, the system retrieves the researcher's publications, including their metadata and publicly available full-text.For example, the framework might retrieve the publication titled Sparse Neural Networks for Energy-Efficient Inference from CORE for the above ID.Context Extension.The framework uses an LLM to extract keyphrases representing the underlying research topic from the researcher query.For instance, the LLM extracts keyphrases such as sparsity, energy efficiency, and AI optimization from the researcher's query.Building on the keyphrases extracted from the researcher's research query q, the system retrieves related research publications from academic repositories.Building on identified research gaps, the framework generates context-aware scientific ideas using diverse prompting strategies.For example, based on gaps from the researcher's publication Sparse Neural Networks for Energy-Efficient Inference and the similar publication Energy-Efficient Deep Learning via Dynamic Precision Scaling, it proposes ideas such as: (1) integrating dynamic precision scaling with structured sparsity to optimize energy efficiency in large-scale AI models, (2) using sparsity techniques to reduce energy consumption in AI models, and (3) applying dynamic precision scaling for energy-efficient deep learning.To evaluate novelty, the framework computes semantic embeddings of generated ideas and compares them using cosine similarity.For instance, the idea Investigate the integration of dynamic precision scaling and structured sparsity to optimize energy efficiency and training stability in large-scale AI models is compared with embeddings of prior ideas, such as Use sparsity techniques to reduce energy consumption in AI models and Apply dynamic precision scaling for energy-efficient deep learning.A low maximum cosine similarity (e.g., 0.5) results in a high novelty score (e.g., 0.8), indicating a distinct and innovative idea.The novelty arises from uniquely combining sparsity and precision scaling.Surprise measures how unexpected an idea is given the context.For example, if the context focuses on sparsity techniques and the idea introduces dynamic precision scaling, the surprisal value will be high (e.g., 3.5), indicating an unexpected yet relevant insight.This occurs because the idea introduces a new dimension not emphasized by other ideas.Difference Between Novelty and Surprise.An idea can be novel but not surprising if it is distinct from prior framework-generated ideas but aligns with expected research directions.For example, Use sparsity techniques to reduce energy consumption in AI models may be novel if sparsity is unexplored in this Refining the Idea Let us refine this idea for maximum novelty, feasibility, and impact: .... Aha Moment Insight <idea> Idea here </idea> Taking the Breakthrough Further </answer> context but unsurprising if sparsity is a well-known technique.An idea can be surprising but not novel if it is unexpected yet similar to existing ideas.For example, Apply dynamic precision scaling for energy-efficient deep learning may be surprising in a sparsity-focused context but not novel if precision scaling is widely studied elsewhere.Also, an idea can be both novel and surprising if it introduces an unexpected and distinct combination of techniques.For example, Investigate the integration of dynamic precision scaling with structured sparsity to optimize energy efficiency in large-scale AI models is both novel and surprising as it combines two distinct approaches.Aha Moment Detection.An idea is flagged as an Aha moment if it satisfies the researcher's predefined thresholds for novelty and surprise (e.g., θ n = 0.7 and θ s = 2.0).For example, the candidate idea above would qualify as an Aha moment if it introduced a novel combination of techniques (e.g., dynamic precision scaling with structured sparsity) and exceeded the thresholds for novelty and surprise.Iterative Exploration.When an idea is flagged as an Aha moment (e.g., Combining sparsity techniques with dynamic computation for energy-efficient AI models), the Aha Prompt template (Table 1) is activated to guide deeper exploration.The template directs the LLM to refine the idea from multiple perspectives, including feasibility, interdisciplinary insights, and practical applications.For instance, the system might refine the idea to: Explore hybrid architectures combining sparsity techniques with dynamic computation to reduce both parameter count and energy consumption during inference.This iterative process continues until the generated ideas meet a researcher-selected threshold that the final outputs are not only novel and surprising but also actionable and aligned with the researcher's goals.By dynamically balancing the exploration of new conceptual spaces with the refinement of promising directions, the framework ensures the discovery of high-quality, context-aware scientific ideas.Final Idea Generation and Ranking.The generated candidate ideas are ranked using an LLM based on four criteria: Novelty, Excitement, Feasibility, and Effectiveness, ensuring a balanced evaluation of their impact and practicality.For example, for the query How can we improve the energy efficiency of training deep reinforcement learning agents?and the identified gap between DRL's high energy consumption and SNNs' energy efficiency, the framework generates the idea: Develop a hybrid framework combining spiking neural networks (SNNs) with deep reinforcement learning (DRL) to optimize energy-efficient training while maintaining performance.This idea scores high on novelty (bridging SNNs, DRL), excitement (addressing a critical challenge), and effectiveness (targeting the energy efficiency gap) but medium on feasibility due to the technical complexity of integration.In contrast, the idea Apply quantization techniques to reduce the precision of DRL parameters for energy savings scores lower on novelty and excitement but higher on feasibility.The hybrid SNN and DRL idea is ranked higher due to its greater novelty, excitement, effectiveness, and Feasibility.</p>
<p>Human-in-the-Loop Refinement.The last step of the framework includes human feedback where the researcher interacting with the system provides feedback, such as adding focus points.For instance, if the researcher suggests incorporating Group Relative Policy Optimization (GRPO), the framework updates the idea to: Investigate the integration of dynamic precision scaling, structured sparsity, and Group Relative Policy Optimization (GRPO) to optimize energy efficiency and training stability in large-scale AI models.This human feedback process ensures the ideas are actionable, impactful, and more aligned with the researcher's interests.The system dynamically updates the context based on feedback, balancing exploration of ideas with exploitation of promising directions and fostering a collaborative and context-aware ideation process.</p>
<p>Experimental Setup</p>
<p>Datasets and Baseline Models.To evaluate SCI-IDEA, we curated a dataset of 100 researchers' profiles, including names, ORCID IDs, and publicly available publications.All researchers have published in top-tier computer science conferences (e.g., NeurIPS, ICML, ACL, SIGIR), ensuring high-impact relevance.For each researcher, we used a researcher prompt and an LLM to extract keyphrases, guiding the retrieval of similar research publications.The dataset covers diverse topics in computer science (Table 2), ensuring broad evaluation and generalizability.Three human annotators verified the dataset, cross-checking keyphrases and publication relevance.Research profile metrics (e.g., research publications, citations, h-index, i10-index) were computed for each researcher (Table 2), providing a comprehensive academic impact overview.While robust, the dataset has limitations, such as potential selection biases and reliance on Google Scholar data.To mitigate these, we ensured a balanced representation of senior and early-career researchers and cross-verified data from multiple sources, ensuring reliability and explanatory.For evaluating SCI-IDEA, we utilized GPT-4.5 [21], GPT-4o [1], and DeepSeek-V3 (comprising DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Llama-70B) [11] [17].These LLMs were chosen for their ability to generate high-quality scientific ideas and handle diverse prompts.GPT-4.5 and GPT-4o serve as general-purpose baselines, while DeepSeek-V3 leverages knowledge distillation for efficiency.We design prompts in three formats: zero-shot (ZS) [13] for simplicity, few-shot (FS) [5] for context enrichment, and zero-shot chain-of-thought (ZS- CoT) [13] for step-by-step reasoning.We evaluate 2-shot, 3-shot, and 5-shot configurations using both closed-source (e.g., GPT-4.5, GPT-4o) and open-source (e.g., DeepSeek-V3) LLMs.</p>
<p>Setup and Implementation.</p>
<p>To assess the quality of generated scientific ideas, we employ a hybrid framework combining LLM-based and human expert evaluations.State-of-the-art LLMs rate each idea on four dimensions-novelty, excitement, feasibility, and expected effectiveness-using explicit evaluation instructions and reference examples to minimize biases and ensure reproducibility.Despite the efficiency of LLM-based evaluation, human judgment is essential for assessing deeper conceptual nuances.We recruit expert reviewers, primarily PhD holders with publications in top-tier conferences, to evaluate ideas using a structured review form mirroring the LLM criteria.Reviewers score ideas on a 1-10 scale and provide free-text rationales, with inter-rater reliability analyzed to ensure consistency.This hybrid approach ensures rigorous, high-quality evaluations that combine expert-level standards with the scalability of LLM-driven assessments.</p>
<p>For semantic embeddings, we use two pre-trained transformer models: BERT (bert-base-uncased) [7] for general-purpose embeddings and SciBERT [3] for domain-specific scientific text.The text is tokenized and truncated to 512 tokens for efficiency, with embeddings computed as the mean across tokens.Novelty is calculated as 1 − max(similarities) by comparing cosine similarities between new and previous responses, ensuring distinct and innovative ideas.All experiments were run on a GPU server, utilizing DeepSeek 70B for knowledge distillation and efficient idea generation and DeepSeek 32B for distillation, lightweight and scalable processing.The temperature settings for GPT-4o and DeepSeek were carefully chosen to balance creativity and consistency across tasks: for facet finding, GPT-4o and DeepSeek used temperature 0 and seed 1; for research gap identification, temperature 0.7 and seed 1; for idea generation, temperature 0.75 and seed 1; and for idea ranking, temperature 0.3 and seed 1.The temperature settings for GPT-4o were adopted from [23], ensuring consistency with established practices in LLM-based ideation systems.</p>
<p>Results and Evaluation</p>
<p>We analyze the SCI-IDEA performance across three variants: (1) without embeddings, ( 2) with token-level embeddings, and ( 3) with sentence-level embeddings.Each variant is evaluated under five prompting strategies: zero-shot, zero-shot chain-of-thought, and few-shot.Results are assessed across four metrics: novelty, excitement, feasibility, and effectiveness, with an overall average score computed for each configuration.</p>
<p>Evaluation of Different Embeddings</p>
<p>Without Embeddings.The baseline without embeddings shows competitive performance in novelty and excitement (see Table 3).GPT-4o achieves the highest average score of 6.88 using zero-shot prompting, with novelty and excitement scores of 7.10 and 7.67, respectively.However, this variant struggles with feasibility, as evidenced by lower scores across all LLMs (e.g., GPT-4o: 5.84, GPT-4.5:5.75, DeepSeek-32B: 6.12).For example, GPT-4.5 achieves a feasibility score of only 5.55 under zero-shot chain-of-thought prompting, highlighting the challenge of generating practical ideas without embeddings.Despite these limitations, the baseline variant remains effective in generating novel and exciting ideas, with GPT-4.5 achieving an average score of 6.88 under 3-shot prompting.</p>
<p>Token-Level Embeddings.The token-level embedding variant shows significant improvements in feasibility and effectiveness, as shown in Table 4. DeepSeek-32B achieves the highest average score of 6.89 under 2-shot prompting, with feasibility and effectiveness scores of 6.28 and 6.96, respectively.This indicates that token-level embeddings enhance the practicality and relevance of generated ideas.GPT-4o and GPT-4.5 also perform well, with average scores of 6.84 and 6.86, respectively, under 2-shot prompting.GPT-4o achieves a feasibility score of 6.06 and an effectiveness score of 6.83 under 2-shot prompting, demonstrating the benefits of token-level embeddings in balancing novelty with practicality.Sentence-Level Embeddings.The sentence-level embedding variant demonstrates the best overall performance, particularly in novelty and excitement, as shown in Table 5. GPT-4o achieves the highest average score of 6.87 under 5-shot prompting, with novelty and excitement scores of 7.61 and 7.13, respectively.This indicates that sentence-level embeddings are particularly effective in generating transformative scientific ideas.DeepSeek-70B also performs well, achieving an average score of 6.87 under 5-shot prompting, with novelty and excitement scores of 6.09 and 7.41, respectively.These results highlight the ability of sentence-level embeddings to produce unexpected and impactful ideas while maintaining high feasibility and effectiveness.</p>
<p>Impact of Embeddings.The baseline without embeddings scores well in novelty and excitement but struggles with feasibility (e.g., GPT-4o: 5.84 feasibility, zero-shot prompting).Token-level embeddings address this limitation, significantly improving feasibility and effectiveness.For instance, DeepSeek-32B achieves a feasibility score of 6.28 under 2-shot prompting with token-level embeddings, compared to 6.07 without embeddings.Sentence-level embeddings further enhance novelty and excitement, with GPT-4o achieving a novelty score of 7.61 under 5-shot prompting compared to 7.10 without embeddings.These results highlight the importance of embeddings in balancing novelty, feasibility, and effectiveness.</p>
<p>Human Evaluation.We conducted a comprehensive human evaluation to assess SCI-IDEA's performance across prompting strategies and LLMs.Three evaluators with PhD scored outputs on four dimensions: Novelty, Excitement, Feasibility, and Effectiveness (Figure 3).For embeddings, GPT-4.5 with 3-shot prompting achieved the highest average score of 7.75, excelling in novelty and excitement, while DeepSeek-32B and DeepSeek-70B both scored 7 with 3-shot and zero-shot chain-of-thought prompting, respectively.In the token-level category, DeepSeek-32B with 2-shot prompting performed best with an average score of 6.75, demonstrating strong feasibility and effectiveness.For sentencelevel embeddings, GPT-4o with 5-shot prompting outperformed others with an average score of 8.25, highlighting its ability to generate highly novel and effective ideas.Notably, DeepSeek-70B with 5-shot prompting underperformed significantly (score: 1 across all dimensions), indicating limitations in handling complex sentence-level tasks.These results demonstrate SCI-IDEA's effectiveness in leveraging diverse prompting strategies and LLMs, with GPT-4.5 and GPT-4o emerging as strong candidates for embedding and sentence-level tasks.</p>
<p>Ablation Studies</p>
<p>Bridging Human and LLM Evaluations.Figure 4 compares human and LLM evaluation scores, revealing both alignment and divergence.For instance, GPT-4o with 5-shot prompting achieves the highest human score of 8.25 and a correspondingly high LLM score of 6.87, indicating strong agreement in evaluating novel and effective ideas.Similarly, DeepSeek-32b with 2-shot prompting shows consistent scores, with human evaluators assigning 6.75 and LLMs assigning 6.89, suggesting reliable performance.DeepSeek-70b with 5-shot prompting shows a significant difference, with human evaluators assigning a score of 1 compared to the LLM score of 6.87, highlighting LLMs' limitations in assessing low-quality or overly complex ideas.Conversely, GPT-4o with 2-shot prompting receives a moderate LLM score of 6.84 but a lower human score of 4.25, indicating LLMs may overestimate the quality of ideas lacking novelty or feasibility.</p>
<p>Prompting Strategies.Few-shot prompting consistently outperforms zeroshot and zero-shot chain-of-thought prompting, particularly in feasibility and effectiveness.For example, DeepSeek-32B achieves its highest feasibility score of 6.28 with 2-shot prompting and token-level embeddings, compared to 6.07 under zero-shot prompting.Similarly, GPT-4.5 achieves its highest average score of 6.86 with 2-shot prompting and token-level embeddings, compared to 6.82 under zero-shot prompting.These results highlight the importance of providing context through examples, enhancing the LLM's ability to generate practical and relevant ideas.</p>
<p>Performance with Smaller LLMs.To evaluate scalability, we test SCI-IDEA with smaller LLMs like DeepSeek-32B, comparing it against larger LLMs such as GPT-4o and GPT-4.5.Despite its reduced size, DeepSeek-32B achieves competitive performance, particularly with token-level embeddings, attaining an average score of 6.89 with 2-shot prompting.This is facilitated by knowledge distillation, which preserves reasoning capabilities while reducing computational overhead.However, smaller LLMs exhibit lower novelty scores, such as 7.00 under zero-shot prompting, compared to larger LLMs like GPT-4o, which achieves 7.61 under 5-shot prompting, reflecting a trade-off between LLM size and novelty generation.These results demonstrate SCI-IDEA's adaptability to resource-constrained environments while maintaining reasonable performance.</p>
<p>Limitations and Ethical Considerations</p>
<p>Limitations of SCI-IDEA.While SCI-IDEA demonstrates significant advancements in context-aware scientific idea generation, it has several limitations.First, the framework relies heavily on the quality of input data, such as the researcher's and related publications, which may introduce biases or incomplete representations of the research landscape.Second, the evaluation metrics for novelty and surprise, while robust, are based on semantic embeddings and likelihood estimates, which may not fully capture the transformative potential of ideas.Third, the computational cost of iterative refinement and embeddingbased evaluations can be prohibitive for large-scale applications.Additionally, the examples in this paper focus on computer science due to the authors' expertise, but the approach is broadly applicable to other domains.</p>
<p>Ethical Considerations.The use of AI-generated scientific ideas raises ethical concerns, including misuse, intellectual credit ambiguity, and idea homogenization.Indiscriminate AI use could overwhelm academic venues and compromise peer-review integrity, necessitating that AI-assisted work meet rigorous human standards.LLM-assisted research also challenges traditional intellectual ownership, requiring standardized documentation of AI's role (e.g., LLMs, data sources, frameworks) for fair credit attribution.AI-generated ideas risk misuse, such as adversarial applications, highlighting the need for safety measures like Reinforcement Learning from Human Feedback (RLHF) [6].Additionally, current LLMs exhibit limited diversity in idea generation, raising concerns about the diversity of global research.Addressing this requires refining LLMs to encourage diverse perspectives and evaluate their ability to generate rare, transformative insights.Finally, AI integration into scientific ideation presents sociotechnical challenges, as over-reliance on AI could stifle human creativity.These issues must be managed to ensure responsible and ethical AI use in scientific ideation.</p>
<p>Conclusion</p>
<p>We introduced SCI-IDEA, a framework for generating context-aware, high-quality scientific ideas by leveraging the generative power of LLMs and structured evaluation metrics.SCI-IDEA addresses the limitations of existing methods by integrating iterative refinement, dynamic evaluation mechanisms, and domainspecific embeddings to ensure the novelty, excitement, feasibility, and effectiveness of generated ideas.Our experiments demonstrate that SCI-IDEA excels in context-aware ideation, achieving significant improvements in generating impactful and actionable research directions.We deem that SCI-IDEA has the potential to foster innovation and interdisciplinary collaboration across diverse scientific domains.Future work will focus on addressing limitations by incorporating more diverse datasets, developing broader evaluation metrics, and optimizing computational efficiency.We also plan to explore the integration of domain-specific knowledge graphs and multi-modal inputs to enhance the quality and diversity of generated ideas.Additionally, an extensive human evaluation will be conducted to further validate the framework's effectiveness.</p>
<p>Auer contributed to the initial idea and provided support in the manuscript writing.Yaser Jaradeh provided feedback on the manuscript and will integrate the idea into the Open Research Knowledge Graph (ORKG) alongside other related projects.</p>
<p>Chain of Thought:</p>
<p>Purpose: This refers to the main objective or goal of the paper.To identify the purpose, ask yourself: What is the primary goal the author is trying to achieve?It often describes what the research aims to solve or improve.Mechanism: This is the method, process, or technique used to accomplish the purpose.Ask yourself: How is the goal being achieved?What is the approach, technique, or system being used?Evaluation: This describes how the effectiveness or success of the mechanism is assessed.Ask yourself: How was the approach tested or evaluated?What were the results?Future Work: This highlights the directions the author suggests for future research or improvements.Ask yourself: What does the author propose for future exploration or improvement in the field?Instruction: You are a Scientist, an intelligent assistant that helps researchers generate coherent, novel, and useful research ideas.Summarize the prior work above in approximately 200 words.Rather than summarizing individual papers one by one, summarize their contributions as a whole.Highlight: -What has already been done in research?-The research gaps from these prior works.This will help you avoid proposing redundant ideas.Instead, generate novel ideas that build upon the designated papers, leveraging their combined contributions.</p>
<p>Examples:</p>
<ul>
<li>-Example 2: ...           Provide possible research ideas in the following JSON format: { "idea": "Idea Title", "description": "Idea Description" } Fig. 18: Idea Generator 3FS (Page 3).</li>
</ul>
<p>Idea generator 5FS</p>
<p>EXAMPLES:</p>
<ul>
<li>Idea Ranking ZSCOT You are a good evaluator.Please judge each idea on a scale from 1 to 10, where 1 represents very low and 10 indicates excellent, based on the following five criteria:</li>
</ul>
<p>-Novelty: How original is the idea?-Excitement: How inspiring or engaging is the idea?-Feasibility: Can the idea be realistically implemented?-Effectiveness: How well does the idea address the problem?-Overall: Average score of all the criteria (Novelty, Excitement, Feasibility, Effectiveness).Now, I want you to break down your evaluation process step-by-step for each idea:</p>
<p>-Start by assessing the Novelty of the idea.Is it something fresh and original, or does it feel like a common or recycled concept?-Next, consider Excitement.Does the idea inspire you or have the potential to engage and captivate others?What makes it interesting or not?-Then, analyze the Feasibility.Can the idea be realistically implemented within a reasonable timeframe or with the available resources?Are there major obstacles to its execution?-After that, evaluate the Effectiveness of the idea.How well does the idea address the underlying problem?Does it provide a strong solution, or are there gaps that need to be filled?-Finally, calculate the Overall score by averaging the individual scores (Novelty, Excitement, Feasibility, Effectiveness).</p>
<p>Instruction: Below is a list of generated research ideas.Rank them from 1 (best) to N (worst), based on the above criteria, and provide an explanation for the ranking.</p>
<p>Research Ideas: <generated_ideas></p>
<p>Provide the ranked list of ideas along with your explanation for the ranking.Use the following format:</p>
<p>[ { "novelty": Score, "excitement": Score, "feasibility": Score, "effectiveness": Score, "overall": Score } ] Fig. 23: Idea Ranking ZSCoT.</p>
<p>Idea Ranking 2FS</p>
<p>You are a good evaluator.Please judge each idea on a scale from 1 to 10, where 1 represents very low and 10 indicates excellent, based on the following five criteria:</p>
<p>-Novelty: How original is the idea?-Excitement: How inspiring or engaging is the idea?-Feasibility: Can the idea be realistically implemented?-Effectiveness: How well does the idea address the problem?-Overall: Average score of all the criteria (Novelty, Excitement, Feasibility, Effectiveness).</p>
<p>Example Evaluations:</p>
<p>-Example 1: Idea: "A self-sustaining city powered entirely by microbial fuel cells."</p>
<p>• Novelty: 9 (Highly original, but microbial fuel cells are a known technology.)• Excitement: 8 (The idea of a city powered by bacteria is fascinating.)• Feasibility: 5 (Technically possible, but large-scale implementation is challenging.)• Effectiveness: 7 (If feasible, it would significantly reduce carbon emissions.)• Overall: 7.25 -Example 2: Idea: "A mobile app that translates dog barks into human language."</p>
<p>• Novelty: 7 (Attempts have been made, but still a unique approach.)</p>
<p>• Excitement: 9 (Would attract pet lovers and tech enthusiasts.)</p>
<p>• Feasibility: 4 (Complex AI challenges in interpreting animal vocalizations.)• Effectiveness: 5 (Limited real-world use, as barks don't always convey specific messages.)• Overall: 6.25 Now, evaluate the following research ideas: Instruction: Below is a list of generated research ideas.Rank them from 1 (best) to N (worst), based on the above criteria, and provide an explanation for the ranking.Research Ideas: <generated_ideas> Provide the ranked list of ideas along with your explanation for the ranking.Use the following format:</p>
<p>[ { "novelty": Score, "excitement": Score, "feasibility": Score, "effectiveness": Score, "overall": Score } ] Fig. 24: Idea Ranking 2FS.</p>
<p>Idea Ranking 3FS</p>
<p>You are a good evaluator.Please judge each idea on a scale from 1 to 10, where 1 represents very low and 10 indicates excellent, based on the following five criteria:</p>
<p>-Novelty: How original is the idea?-Excitement: How inspiring or engaging is the idea?-Feasibility: Can the idea be realistically implemented?-Effectiveness: How well does the idea address the problem?-Overall: Average score of all the criteria (Novelty, Excitement, Feasibility, Effectiveness).</p>
<p>Example Evaluations:</p>
<p>-Example 1: Idea: "A self-sustaining city powered entirely by microbial fuel cells."</p>
<p>• Novelty: 9 (Highly original, but microbial fuel cells are a known technology.)• Excitement: 8 (The idea of a city powered by bacteria is fascinating.)</p>
<p>Idea Ranking 5FS</p>
<p>You are a good evaluator.Please judge each idea on a scale from 1 to 10, where 1 represents very low and 10 indicates excellent based on the following five criteria:</p>
<p>-Novelty: How original is the idea?-Excitement: How inspiring or engaging is the idea?-Feasibility: Can the idea be realistically implemented?-Effectiveness: How well does the idea address the problem?-Overall: Average score of all the criteria (Novelty, Excitement, Feasibility, Effectiveness)</p>
<p>Example Evaluations:</p>
<p>-Example 1: Idea: "A self-sustaining city powered entirely by microbial fuel cells."</p>
<p>• Novelty: 9 (Highly original, but microbial fuel cells are a known technology.)• Excitement: 8 (The idea of a city powered by bacteria is fascinating.)Please return only a valid JSON object.</p>
<p>Output Format (JSON example):</p>
<p>[ { "Novelty": Score, "Excitement": Score, "Feasibility": Score, "Effectiveness": Score, "Overall Score": Score } ]</p>
<p>Fig. 1 :
1
Fig. 1: Overview of researcher and SCI-IDEA interactions.The left side illustrates researcher interactions and feedback, while the right side highlights SCI-IDEA's techniques for generating and refining context-aware scientific ideas.</p>
<p>Fig. 2 :
2
Fig. 2: Overview of SCI-IDEA framework.The upper side shows Module 1: Context Retrieval, Facet Extraction, and Research Gap Identification.The lower side illustrates Module 2: Idea Generation, Evaluation, Aha Moment Detection, and Refinement, along with their respective components.</p>
<p>Fig. 3 :
3
Fig. 3: Human Evaluation of SCI-IDEA by Embedding Strategy.Scores for novelty, excitement, feasibility, and effectiveness (left to right: without embedding, token-level embedding, sentence-level embedding).</p>
<p>Fig. 4 :
4
Fig. 4: Comparison of Human vs. LLM Scores in SCI-IDEA.Evaluation scores are across different prompting strategies (left to right: without embedding, token-level embedding, and sentence-level embedding).</p>
<p>2 Title:
2
29. Uhlmann, E.L., Ebersole, C.R., Chartier, C.R., Errington, T.M., Kidwell, M.C., Lai, C.K., McCarthy, R.J., Riegelman, A., Silberzahn, R., Nosek, B.A.: Scientific utopia iii: Crowdsourcing science.Perspectives on Psychological Science 14(Novel Deep Learning Framework for Image Segmentation Facets: { Purpose: The paper proposes a deep learning framework for medical image segmentation.Mechanism: The framework utilizes a U-Net architecture with attention mechanisms.Evaluation: The model is evaluated using Dice score, IoU, and comparison with baseline models.Future Work: The authors suggest extending the model to 3D segmentation and multimodal learning.} Example Knowledge Graph Embeddings for Recommendation Systems Facets: { Purpose: The paper explores knowledge graph embeddings for personalized recommendations.Mechanism: It leverages TransE and RotatE embeddings trained on interaction data.Evaluation: Performance is assessed using Hit@10, NDCG, and comparison with collaborative filtering.Future Work: The authors propose integrating user behavioral signals for better embeddings.} Instruction: Extract the following facets from the given paper: Purpose, Mechanism, Evaluation, and Future Work.</p>
<p>Format:{Fig. 6 : 2 Title:Fig. 7 :
627
Fig. 6: Prompt for Paper Facet Finder 2FS.</p>
<p>Format:{Fig. 8 : 2 Title:Fig. 9 : 3 Title: 4 Title:Fig. 9 :
829349
Fig. 8: Prompt for Paper Facet Finder 3FS (Part 2).</p>
<p>Format:{Fig. 9 :
9
Fig. 9: Prompt for Paper Facet Finder 5FS (Part 3).</p>
<p>Format:{Fig. 10 :
10
Fig. 10: Paper Facet Finder ZSCoT.</p>
<p>Find</p>
<p>research gap from author paper 2FS Paper: Paper <paper id>: Title: <paper title> Purpose: <Purpose facet> Mechanism: <Mechanism facet> Evaluation: <Evaluation facet> Future Work: <Future Work facet>Instruction: You are a Scientist, an intelligent assistant that helps researchers generate coherent, novel, and useful research ideas.Summarize the prior work above in approximately 200 words.Rather than summarizing individual papers one by one, summarize their contributions as a whole.Highlight: -What has already been done in research.-The research gaps from these prior works.This will help you avoid proposing redundant ideas.Instead, generate novel ideas that build upon the designated papers, leveraging their combined contributions.Examples: -Example 1: Paper: Title: "Quantum Computing for Cryptography" Purpose: To enhance encryption security using quantum algorithms.Mechanism: Quantum key distribution (QKD) ensures secure communication.Evaluation: QKD resists decryption attempts by classical computers.Future Work: Research suggests improving scalability and error correction in quantum systems.Answer: Quantum computing has revolutionized cryptography by leveraging quantum key distribution (QKD) to enhance security.Unlike classical methods, QKD provides theoretically unbreakable encryption.However, current implementations face scalability and error correction challenges.Future research should explore faulttolerant quantum systems and practical large-scale deployment strategies to integrate QKD into real-world communication networks.</p>
<p>Fig. 11 :
11
Fig. 11: Find research gap from author paper 2FS (Part 1).</p>
<p>Fig. 11 :
11
Fig. 11: Find research gap from author paper 2FS (Part 2).</p>
<p>Find</p>
<p>research gap from author paper 3FS Paper: Paper <paper id>: Title: <paper title> Purpose: <Purpose facet> Mechanism: <Mechanism facet> Evaluation: <Evaluation facet> Future Work: <Future Work facet>Instruction: You are a Scientist, an intelligent assistant that helps researchers generate coherent, novel, and useful research ideas.Summarize the prior work above in approximately 200 words.Rather than summarizing individual papers one by one, summarize their contributions as a whole.Highlight: -What has already been done in research?-The research gaps from these prior works.This will help you avoid proposing redundant ideas.Instead, generate novel ideas that build upon the designated papers, leveraging their combined contributions.Examples: -Example 1: Paper: Title: "Quantum Computing for Cryptography" Purpose: To enhance encryption security using quantum algorithms.Mechanism: Quantum key distribution (QKD) ensures secure communication.Evaluation: QKD resists decryption attempts by classical computers.Future Work: Research suggests improving scalability and error correction in quantum systems.Answer: Quantum computing has revolutionized cryptography by leveraging quantum key distribution (QKD) to enhance security.Unlike classical methods, QKD provides theoretically unbreakable encryption.However, current implementations face scalability and error correction challenges.Future research should explore faulttolerant quantum systems and practical large-scale deployment strategies to integrate QKD into real-world communication networks.</p>
<p>Fig. 12 :
12
Fig. 12: Find research gap from author paper 3FS (Part 1).</p>
<p>Fig. 12 :
12
Fig. 12: Find research gap from author paper 3FS (Part 2).</p>
<p>Find</p>
<p>Fig. 13 :
13
Fig. 13: Find research gap from author paper 5FS (Part 1).</p>
<p>Fig. 13 :
13
Fig. 13: Find research gap from author paper 5FS (Part 2).</p>
<p>FindFig. 14 :
14
Fig. 14: Find research gap from author paper ZS.</p>
<p>Find</p>
<p>Fig. 15 :
15
Fig. 15: Find research gap from author paper 2FS (Part 1).</p>
<p>Fig. 15 :
15
Fig. 15: Find research gap from author paper 2FS (Part 2).</p>
<p>Fig. 16 :
16
Fig. 16: Idea Generator 3FS (Page 1).</p>
<p>Fig. 19 :
19
Fig. 19: Idea Generator 5FS (Page 1).</p>
<p>• Feasibility: 5 (
5
Technically possible, but large-scale implementation is challenging.)• Effectiveness: 7 (If feasible, it would significantly reduce carbon emissions.)• Overall: 7.25 -Example 2: Idea: "A mobile app that translates dog barks into human language."• Novelty: 7 (Attempts have been made, but still a unique approach.)• Excitement: 9 (Would attract pet lovers and tech enthusiasts.)• Feasibility: 4 (Complex AI challenges in interpreting animal vocalizations.)• Effectiveness: 5 (Limited real-world use, as barks don't always convey specific messages.)• Overall: 6.25 -Example 3: Idea: "Using AI to detect and prevent cyberbullying in realtime."• Novelty: 6 (AI-driven moderation exists, but real-time intervention is less explored.)• Excitement: 8 (Cyberbullying is a major issue, and an AI-driven solution is compelling.)• Feasibility: 7 (Technically possible with NLP and sentiment analysis advancements.)• Effectiveness: 8 (Could significantly reduce harm if implemented correctly.)• Overall: 7.25</p>
<p>Fig. 25 :
25
Fig. 25: Idea Ranking 3FS (Page 1).</p>
<p>• Feasibility: 5 (
5
Technically possible, but large-scale implementation is challenging.)• Effectiveness: 7 (If feasible, it would significantly reduce carbon emissions.)• Overall: 7.25 -Example 2: Idea: "A mobile app that translates dog barks into human language."• Novelty: 7 (Attempts have been made, but still a unique approach.)• Excitement: 9 (Would attract pet lovers and tech enthusiasts.)• Feasibility: 4 (Complex AI challenges in interpreting animal vocalizations.)• Effectiveness: 5 (Limited real-world use, as barks don't always convey specific messages.)• Overall: 6.25 -Example 3: Idea: "Using AI to detect and prevent cyberbullying in realtime."• Novelty: 6 (AI-driven moderation exists, but real-time intervention is less explored.)• Excitement: 8 (Cyberbullying is a major issue, and an AI-driven solution is compelling.)• Feasibility: 7 (Technically possible with NLP and sentiment analysis advancements.)• Effectiveness: 8 (Could significantly reduce harm if implemented correctly.)• Overall: 7.25</p>
<p>Fig. 27 :
27
Fig. 27: Idea Ranking 5FS (Page 1).</p>
<p>Fig. 29 :
29
Fig. 29: Idea Evaluation.</p>
<p>For example, for the researcher query the framework retrieves publications like Energy-Efficient Deep Learning via Dynamic Precision Scaling from arXiv and Sparse Training Techniques for Neural Networks from CORE, based on the keyphrases sparsity, energy efficiency, and AI optimization.
real-time applications). These facets provide a structured representation of thethematic and conceptual elements within the research content.Research Gap Identification. Using the structured facets extracted from theresearchers and related research publications, the framework identifies researchgaps by analyzing patterns, inconsistencies, and unexplored areas using the LLM.For instance, by comparing facets from Sparse Neural Networks for Energy-Efficient Inference and Energy-Efficient Deep Learning via Dynamic PrecisionScaling, the framework highlights gaps such as underexplored methodologies (e.g.,combining sparsity with dynamic precision scaling) and unaddressed challenges(e.g., scalability to large-scale models). The framework uses different promptingstrategies to uncover these prospects depending on research requirements. Forexample, the framework might generate: Step 1: Identify energy-efficient sparsityand precision scaling techniques. Step 2: Explore potential synergies between thesetechniques. Step 3: Propose a hybrid approach combining sparsity and dynamicprecision scaling. This process ensures that the identified gaps are aligned withthe researcher's interests.2.3 Module 2: Idea Generation and Iterative RefinementIdea Generation, Evaluation and Novelty with Surprise Detection.Facet Extraction. Following the retrieval of full texts from publications au-thored by the researcher using the framework and related publications, theframework leverages an LLM to extract key facets, such as research objectives,methodologies, evaluation, and future work. For example, from the researcher'spublication Sparse Neural Networks for Energy-Efficient Inference, the frame-work extracts facets like the objective (reduce energy consumption in AI models),methodology (structured sparsity techniques), evaluation (30% energy savings onbenchmark datasets), and future work (exploring hardware-software co-designfor further energy optimization). Similarly, from the related publication Energy-Efficient Deep Learning via Dynamic Precision Scaling, the framework extractsfacets such as the objective (optimize precision for energy efficiency), methodol-ogy (dynamic precision scaling), evaluation (20% reduction in energy consump-tion with minimal accuracy loss), and future work (extending the approach to</p>
<p>Table 1 :
1
Template for zero-shot chain-of-thought (ZSCoT) with Aha moment flagging and deep dive exploration.
<think> Let us think step by step. </think><answer> Answerhere....</answer>....<think> Wait, this appears to be a breakthrough! We need ascientific idea .... multiple perspectives, and refinement for maximum impact.</think> <answer> Refined answer here.........</answer><think> An Aha moment has been detected! Let us delve deeper into this breakthroughidea. </think> <answer>Deep Dive into a Breakthrough Idea We have identifiedan Aha moment! Instead of stopping here, we will push further. How does thisidea challenge conventional wisdom? .... What are the top three practicalapplications of this idea?</p>
<p>Table 2 :
2
Keyphrase topic distribution -left and research profile wetrics (Extracted from Google Scholar) -right.
TopicCountAI Alignment, RL, NLP36StatArticlesCites h-Indexi10Knowledge Representation Security, Privacy, Robustness in AI Materials Science, Chemistry Quantum Computing, Optimization Others12 10 17 8 17Mean Median Min Max SD174 14,516 52 1,953 2 1 2,320 282,605 343 37,21931 19 24.0 87 1 0 201 1,153 38 188Total100</p>
<p>Table 3 :
3
Performance of different LLMs without embeddings (without Aha moment).
ModelSizePrompt Novelty Excitement Feasibility Effectiveness AvgGPT-4o-ZS7.107.675.846.916.88GPT-4o-ZSCoT6.897.535.906.876.80GPT-4o-2FS6.807.375.706.736.65GPT-4o-3FS7.027.645.836.896.84GPT-4o-5FS6.957.595.846.926.82GPT-4.5-ZS7.977.035.546.736.82GPT-4.5-ZSCoT7.916.985.426.566.72GPT-4.5-2FS7.797.035.626.766.80GPT-4.5-3FS7.907.035.756.846.88GPT-4.5-5FS7.937.025.646.726.83DeepSeek32BZS6.936.886.076.636.63DeepSeek32BZSCoT7.337.056.076.826.82DeepSeek32B2FS7.297.316.006.886.87DeepSeek32B3FS7.267.126.126.836.83DeepSeek32B5FS7.347.076.006.766.79DeepSeek70BZS7.006.886.166.636.67DeepSeek70BZSCoT7.097.156.146.836.80DeepSeek70B2FS6.956.986.126.636.67DeepSeek70B3FS7.187.006.136.796.78DeepSeek70B5FS7.217.125.976.776.76</p>
<p>Table 4 :
4
Performance of different LLMs with token-level embedding (with Aha moment).
ModelSizePrompt Novelty Excitement Feasibility Effectiveness AvgGPT-4o-ZS7.317.235.946.816.82GPT-4o-ZSCoT7.197.126.146.856.82GPT-4o-2FS7.397.086.066.836.84GPT-4o-3FS7.387.095.896.756.78GPT-4o-5FS7.547.195.816.746.82GPT-4.5-ZS7.717.005.846.846.85GPT-4.5-ZSCoT7.836.995.646.696.79GPT-4.5-2FS7.687.025.936.816.86GPT-4.5-3FS7.827.035.696.736.82GPT-4.5-5FS7.847.015.716.736.82DeepSeek32BZS7.007.006.226.776.75DeepSeek32BZSCoT7.327.156.046.896.85DeepSeek32B2FS7.157.156.286.966.89DeepSeek32B3FS6.887.016.176.726.69DeepSeek32B5FS7.137.066.106.806.77DeepSeek70BZS6.817.126.236.846.75DeepSeek70BZSCoT7.137.026.266.776.79DeepSeek70B2FS6.077.126.717.136.76DeepSeek70B3FS6.427.366.477.126.84DeepSeek70B5FS6.767.246.346.966.82</p>
<p>Table 5 :
5
Performance of different LLMs with sentence-level embedding (With Aha moment).
ModelSizePrompt Novelty Excitement Feasibility Effectiveness AvgGPT-4o-ZS7.766.995.676.716.78GPT-4o-ZSCoT7.247.225.996.796.81GPT-4o-2FS7.307.066.076.786.80GPT-4o-3FS7.347.155.956.886.83GPT-4o-5FS7.617.135.946.816.87GPT-4.5-ZS7.697.095.816.766.84GPT-4.5-ZSCoT7.786.985.556.656.74GPT-4.5-2FS7.727.015.716.786.80GPT-4.5-3FS7.717.005.846.876.86GPT-4.5-5FS7.846.975.716.686.79DeepSeek32BZS7.086.886.156.676.69DeepSeek32BZSCoT7.197.036.276.826.83DeepSeek32B2FS7.086.916.126.826.73DeepSeek32B3FS5.817.276.797.286.79DeepSeek32B5FS6.927.096.276.906.79DeepSeek70BZS6.887.116.296.916.79DeepSeek70BZSCoT6.976.956.316.796.76DeepSeek70B2FS6.047.206.677.186.77DeepSeek70B3FS5.277.257.067.526.78DeepSeek70B5FS6.097.416.727.236.87</p>
<p>Instruction: You are a Scientist, an intelligent assistant that helps researchers generate coherent, novel, and useful research ideas.Summarize the prior work above in approximately 200 words.Rather than summarizing individual papers one by one, summarize their contributions as a whole.Highlight: -What has already been done in research.-The research gaps from these prior works.This will help you avoid proposing redundant ideas.Instead, generate novel ideas that build upon the designated papers, leveraging their combined contributions.
Examples:-Example 1:Paper: Title: "Quantum Computing for Cryptography"Purpose: To enhance encryption security using quantum algo-rithms.Mechanism: Quantum key distribution (QKD) ensures securecommunication.Evaluation: QKD resists decryption attempts by classical com-puters.Future Work: Research suggests improving scalability and errorcorrection in quantum systems.Answer: Quantum computing has revolutionized cryptography byleveraging quantum key distribution (QKD) to enhance security.Unlike classical methods, QKD provides theoretically unbreakableencryption. However, current implementations face scalability anderror correction challenges. Future research should explore fault-tolerant quantum systems and practical large-scale deploymentstrategies to integrate QKD into real-world communication net-works.</p>
<p>Reduce false positive rate and improve model explainability.-Summary of Research Gap: High false positive rates causing alert fatigue in security teams.-AnalogousPaper:"HumanBehavior Analysis in Fraud Detection"• Purpose: Detect fraudulent transactions using behavior analytics.•Mechanism:Bayesian models tracking user patterns.•Evaluation: Deployed in financial institutions with 95% fraud detection accuracy.• Future Work: Apply similar methods to IoT security.-New Research Idea: • Idea Title: Adaptive Anomaly Detection Using Behavioral Analytics • Description: Develop an AI-driven network security system that integrates behavioral analytics to reduce false positives and improve threat detection accuracy.You are a Scientist, an intelligent assistant that helps researchers generate coherent, novel, and useful research ideas.A <strong>novel</strong> research idea is one that is not only rare but also <strong>ingenious, imaginative, or surprising</strong>.A <strong>useful</strong> research idea applies to the stated problem and is <strong>effective</strong> at solving it.Identify key gaps in research based on the <strong>research gap summary</strong>.-Combine elements from the Purpose, Mechanism, Evaluation, and Future Work of both papers to create <strong>a diverse set of innovative research ideas</strong>.-Ensure Novelty: The proposed research ideas should be unique and not directly derived from either paper.
Idea generator 3FS (Continued)Example 3: Cybersecurity-SUMMARY OF PRIOR WORK:• Designated Paper: "AI-Driven Anomaly Detection for Network Se-curity"• Purpose: Identify cyber threats in real-time.• Mechanism: Unsupervised learning models detecting networkanomalies. • Evaluation: Tested on enterprise network traffic with 90% detection Idea generator 3FS (Continued)rate.INSTRUCTION: Your Task: -Analyze the summaries of both the <strong>designated</strong> and <strong>analogous</strong> papers. • Future Work: SUMMARY OF PRIOR WORK: --Designated Paper: <author_paper_title>-Purpose: <author_facet_Purpose>-Mechanism: <author_facet_Mechanism>-Evaluation: <author_facet_Evaluation>-Future Work: <author_facets_Future_Work>Summary of Research Gap: <novel_work_summary_from_author_paper>Analogous Paper: <analogous_paper_title>-Purpose: <analogous_facets_Purpose>-Mechanism: <analogous_facets_Mechanism>
-Evaluation: <analogous_facets_Evaluation> -Future Work: <analogous_facets_Future_Work> Fig. 17: Idea Generator 3FS (Page 2).</p>
<p>https://core.ac.uk
https://arxiv.org
https://semanticscholar.org
AcknowledgementsThe authors would like to thank Nahid Abdollahi for her insightful and valuable contributions to the visualization task.We also acknowledge the support of the KISSKI project (funding no.01IS22093C) for providing computational resources, which will enable us to extend this research in the future.Author ContributionsFarhana Keya conducted the experiments and contributed to the manuscript writing.Gollam Rabby developed the initial idea, designed the experiments, and contributed to the manuscript writing.Prasenjit Mitra and Sahar Vahdati provided feedback on the initial idea and supported the manuscript writing.SörenA. AppendixIn the following sections, we report additional details on the following topics: 1.All Unique Keys Found in SCI-IDEA Dataset (Section A.1) 2. Prompts in Experiment (Section A.2)A.1 All Unique Keys Found in SCI-IDEA Dataset Column Name Description Researcher NameThe name of the researcher.ORCIDThe unique ORCID identifier for the researcher.Researcher Query Keyword Keywords extracted from researcher synthetic query.Research Full PaperThe full paper list for a researcher.Similar Full PaperList of full papers that are similar to the researcher query.-Novel: Unique and not directly derived from the papers.A.2 Prompts in Experiment-Useful: Effective at solving the problem.-Distinct: Bringing fresh perspectives to existing challenges.Please think step by step.Provide possible research ideas in the following JSON format:[ { "idea": "Idea Title", "description": "Idea Description" } ] Fig.21: Idea Generator ZSCoT.Idea Ranking ZSYou are a good evaluator.Please judge each idea on a scale from 1 to 10, where 1 represents very low and 10 indicates excellent, based on the following five criteria:-Novelty: How original is the idea?-Excitement: How inspiring or engaging is the idea?-Feasibility: Can the idea be realistically implemented?-Effectiveness: How well does the idea address the problem?-Overall: Average score of all the criteria (Novelty, Excitement, Feasibility, Effectiveness).Instruction: Below is a list of generated research ideas.Rank them from 1 (best) to N (worst), based on the above criteria, and provide an explanation for the ranking.Research Ideas: <generated_ideas>Provide the ranked list of ideas along with your explanation for the ranking.Use the following format:[ { "novelty": Score, "excitement": Score, "feasibility": Score, "effectiveness": Score, "overall": Score } ] Fig.22: Idea Ranking ZS.Idea Ranking 3FSNow, evaluate the following research ideas: Instruction: Below is a list of generated research ideas.Rank them from 1 (best) to N (worst) based on the above criteria, and provide an explanation for the ranking.Research Ideas: <generated_ideas>Provide the ranked list of ideas along with your explanation for the ranking.Use the following format:[{"novelty": , "excitement": , "feasibility": , "effectiveness": , "overall": }]  Now, evaluate the following research ideas: Instruction: Below is a list of generated research ideas.Rank them from 1 (best) to N (worst), based on the above criteria, and provide an explanation for the ranking.Research Ideas: <generated_ideas>Provide the ranked list of ideas along with your explanation for the ranking.Use the following format:[ { "novelty": Score, "excitement": Score, "feasibility": Score, "effectiveness": Score, "overall": Score } ] Fig.28: Idea Ranking 5FS (Page 2).
J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. J Baek, S K Jauhar, S Cucerzan, S J Hwang, arXiv:2404.077382024arXiv preprint</p>
<p>Scibert: A pretrained language model for scientific text. I Beltagy, K Lo, A Cohan, arXiv:1903.106762019arXiv preprint</p>
<p>Stabilizing and changing phenomenal worlds: Ludwik fleck and thomas kuhn on scientific literature. S Brorson, H Andersen, Journal for General Philosophy of Science. 322001</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 332020</p>
<p>J Dai, X Pan, R Sun, J Ji, X Xu, M Liu, Y Wang, Y Yang, arXiv:2310.12773Safe rlhf: Safe reinforcement learning from human feedback. 2023arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M W Chang, K Lee, K Toutanova, Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies. the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies20191</p>
<p>Ideas are dimes a dozen: Large language models for idea generation in innovation. K Girotra, L Meincke, C Terwiesch, K T Ulrich, The Wharton School Research Paper Forthcoming. 2023</p>
<p>J Gottweis, W H Weng, A Daryin, T Tu, A Palepu, P Sirkovic, A Myaskovsky, F Weissenberger, K Rong, R Tanno, arXiv:2502.18864Towards an ai co-scientist. 2025arXiv preprint</p>
<p>Interesting scientific idea generation using knowledge graphs and llms: Evaluations with 100 research group leaders. X Gu, M Krenn, arXiv:2405.170442024arXiv preprint</p>
<p>D Guo, D Yang, H Zhang, J Song, R Zhang, R Xu, Q Zhu, S Ma, P Wang, X Bi, arXiv:2501.12948Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025arXiv preprint</p>
<p>S Guo, A H Shariatmadari, G Xiong, A Huang, E Xie, S Bekiranov, A Zhang, arXiv:2411.02429Ideabench: Benchmarking large language models for research idea generation. 2024arXiv preprint</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 352022</p>
<p>Lay epistemics and human knowledge: Cognitive and motivational bases. A W Kruglanski, 2013Springer Science &amp; Business Media</p>
<p>Learning to generate research idea with dynamic control. R Li, L Jing, C Han, J Zhou, X Du, arXiv:2412.146262024arXiv preprint</p>
<p>S Li, S Padilla, P L Bras, J Dong, M Chantler, arXiv:2503.00946A review of llm-assisted ideation. 2025arXiv preprint</p>
<p>A Liu, B Feng, B Xue, B Wang, B Wu, C Lu, C Zhao, C Deng, C Zhang, C Ruan, arXiv:2412.19437Deepseek-v3 technical report. 2024arXiv preprint</p>
<p>Creativity in scientific research: Multidisciplinarity fosters depth of ideas among scientists in electronic "brainwriting" groups. N Michinov, S Jeanson, Human Factors. 6572023</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, S Petersen, C Beattie, A Sadik, I Antonoglou, H King, D Kumaran, D Wierstra, S Legg, D Hassabis, Nature. 51875402015</p>
<p>Spiking neural networks: A survey. J D Nunes, M Carvalho, D Carneiro, J S Cardoso, IEEE access. 102022</p>
<p>OpenAI: Gpt-4.5 system card. February 2025</p>
<p>G Papagni, J De Pagter, S Zafari, M Filzmoser, S T Koeszegi, Artificial agents' explainability to support trust: considerations on timing and context. Ai &amp; Society. 202338</p>
<p>Is temperature the creativity parameter of large language models. M Peeperkorn, T Kouwenhoven, D Brown, A Jordanous, International Conference on Computational Creativity. April 2024</p>
<p>Scideator: Human-llm scientific idea generation grounded in research-paper facet recombination. M Radensky, S Shahid, R Fok, P Siangliulue, T Hope, D S Weld, arXiv:2409.146342024arXiv preprint</p>
<p>Language models are unsupervised multitask learners. A Radford, K Narasimhan, T Salimans, I Sutskever, OpenAI Blog. 2019</p>
<p>Sentence-bert: Sentence embeddings using siamese bertnetworks. N Reimers, I Gurevych, 10.18653/V1/D19-1410Empirical Methods in Natural Language Processing and 9th Int. Joint Conf. on Natural Language Processing, EMNLP-IJCNLP. ACL. 2019</p>
<p>C Si, D Yang, T Hashimoto, arXiv:2409.04109Can llms generate novel research ideas? a largescale human study with 100+ nlp researchers. 2024arXiv preprint</p>
<p>Training deep spiking neural networks for energy. G Srinivasan, C Lee, A Sengupta, P Panda, S S Sarwar, K Roy, 2020</p>            </div>
        </div>

    </div>
</body>
</html>
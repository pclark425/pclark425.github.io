<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-754 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-754</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-754</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-249375616</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2206.01474v1.pdf" target="_blank">Offline Reinforcement Learning with Causal Structured World Models</a></p>
                <p><strong>Paper Abstract:</strong> Model-based methods have recently shown promising for offline reinforcement learning (RL), aiming to learn good policies from historical data without interacting with the environment. Previous model-based offline RL methods learn fully connected nets as world-models that map the states and actions to the next-step states. However, it is sensible that a world-model should adhere to the underlying causal effect such that it will support learning an effective policy generalizing well in unseen states. In this paper, We first provide theoretical results that causal world-models can outperform plain world-models for offline RL by incorporating the causal structure into the generalization error bound. We then propose a practical algorithm, oFfline mOdel-based reinforcement learning with CaUsal Structure (FOCUS), to illustrate the feasibility of learning and leveraging causal structure in offline RL. Experimental results on two benchmarks show that FOCUS reconstructs the underlying causal structure accurately and robustly. Consequently, it performs better than the plain model-based offline RL algorithms and other causal model-based RL algorithms.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e754.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e754.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FOCUS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>oFfline mOdel-based reinforcement learning with CaUsal Structure (FOCUS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical offline model-based RL algorithm that learns a causal world-model from observational offline data using conditional independence tests, then masks non-causal inputs in the learned world-model to reduce spurious dependence and improve offline policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>FOCUS (causal offline MBRL)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Two-stage pipeline: (1) causal-structure learning from offline trajectories by computing a p-value matrix via a kernel-based conditional independence (KCI) test for each candidate pair (X_t, Y_{t+1}), selecting a threshold p* (gap heuristic) and producing a binary causal mask G; (2) replacing the plain world-model with a causal-structure network that masks non-causal inputs per target dimension (Algorithm 2) and using any offline MBRL algorithm (they use MOPO) on the masked model. Key components: KCI testing, temporal constraint exploiting 'future cannot cause past' to reduce conditioning sets/orient edges, threshold selection for p-values, and per-dimension input masking to remove spurious variables.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Toy Car Driving (2D car) and MuJoCo (Inverted Pendulum) — offline datasets (Random, Medium, Medium-Replay)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Simulated virtual labs / continuous-control environments; experiments are done strictly offline using pre-collected trajectory datasets (Random, Medium, Medium-Replay). The environments are interactive simulators by nature, but FOCUS does not perform online interventions — it learns solely from offline data.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Detection via kernel-based conditional independence tests; selection of conditional variables to avoid collider-induced mistakes; explicit variable selection by masking non-causal inputs (per-dimension causal mask) to remove distractors/spurious inputs from the world-model.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious correlated variables induced by biased data collection / selection bias (variables that are highly predictable from true causal parents in the offline dataset), irrelevant variables (variables correlated due to environment bias), and measurement/noise indirectly addressed via KCI and masking.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Compute KCI-based p-values for conditional independence for each candidate pair (X_i_t, X_j_{t+1}) conditioned on selected variables (principle: condition only on variables at time t, not t+1); threshold p* on the p-value matrix to classify causal vs non-causal links.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Hard removal (masking) of detected non-causal / spurious inputs for each target dimension in the learned world-model (causal structure network zeroes out masked inputs); implicitly downweights spurious signals to zero influence.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Refutation is via conditional independence testing (KCI) under carefully chosen conditioning sets and a threshold p*; edges not supported by CI tests are removed from the causal parent set, reducing false causal claims caused by spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Causal structure learning accuracy 0.993 (±0.001 variance) and sample efficiency ~1e6 samples to a stable structure; policy returns substantially improved over baselines (e.g., Car Driving Random: FOCUS 68.1 ± 20.9 vs MOPO -30.3 ± 49.9). Ablation: using KCI and the conditional-variable selection principle are critical for high accuracy/robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Ablations: FOCUS(-KCI) (linear independence test) accuracy 0.62, variance 0.173; FOCUS(-CONDITION) (conditioning on all other variables) accuracy 0.65, variance 0.212. Baselines: LNCM accuracy 0.52 (variance 0.025) and MOPO (plain model-based offline RL) yields much lower policy returns (see paper Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learning and using a causal structure learned from offline data (via KCI + temporal conditioning principle) allows removal of spurious input variables from the world-model, substantially improving both the accuracy of discovered structure (0.993) and downstream offline policy performance; ablations show both the kernel CI test and the conditional-variable selection principle are necessary for the high accuracy and robustness.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e754.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e754.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KCI-test</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Kernel-based Conditional Independence Test (KCI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A nonparametric kernel-based statistical test that assesses conditional independence between continuous variables by computing a test statistic from kernel matrices and estimating the distribution of the statistic, requiring no parametric functional form assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Kernel-based conditional independence test and application in causal discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>KCI (kernel-based CI test)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Computes kernel matrices for X, Y and Z and forms a test statistic that measures dependence between X and Y after removing dependencies explained by Z. The null (conditional independence) distribution is estimated (per Zhang et al. UAI'11) and a p-value is returned. Suitable for continuous variables and non-linear relationships without specifying functional forms.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Applied to offline datasets from Toy Car Driving and MuJoCo (Inverted Pendulum) for causal discovery</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Offline virtual-lab datasets derived from simulators; tests operate on collected trajectory tuples (s_t, a_t, s_{t+1}); no online interventions conducted during discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Detects spurious / non-causal associations by testing conditional independence given chosen conditioning sets; used as the primary detector of spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Selection-bias-induced spurious correlations (variables predictable from causal parents in biased datasets), nonlinear dependencies, and continuous-variable confounding that can be exposed via conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Return a p-value f_KCI(X,Y,Z) ∈ [0,1] representing probability of conditional independence; small p implies dependence (causation candidate); used to populate a p-value matrix for all candidate edges.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Edges are rejected when p > threshold p* (statistical acceptance of conditional independence), effectively refuting putative causal relations not supported by CI evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>When used within FOCUS, KCI is associated with a high causal discovery accuracy (overall FOCUS accuracy 0.993) and low variance (0.001); ablation replacing KCI with a linear test reduced accuracy to 0.62 and increased variance to 0.173.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Ablation FOCUS(-KCI) accuracy 0.62 (variance 0.173); specific per-edge metrics (precision/recall) are not reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>KCI is an effective detector of conditional (in)dependence for continuous RL state variables in offline datasets and is a key component enabling robust discovery of causal parents and removal of spurious variables; replacing KCI with a simple linear test degrades structure accuracy and robustness substantially.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e754.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e754.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Temporal-PC (extension)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PC algorithm extended with temporal constraint ('future cannot cause past')</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptation of the PC causal discovery algorithm that exploits temporal ordering in RL data to reduce conditioning sets and orient edges: forbids edges from future timestep variables to past, thereby simplifying tests and identifying directions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causation, Prediction, and Search</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>PC algorithm with temporal constraint</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>PC-style constraint-based discovery using conditional independence tests but restricted by temporal ordering: candidate conditioning sets are limited to variables consistent with time (condition on t variables not t+1), reducing exponential CI-test search and enabling edge orientation by excluding time-inconsistent directions. This extension reduces the number of CI tests and helps determine causal direction in the discrete-time MDP setting.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Offline trajectory datasets (Toy Car Driving, MuJoCo/Inverted Pendulum)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Offline data derived from interactive simulators but treated as temporally ordered observational data; temporal constraints are intrinsic to the MDP timestep ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Reduces false positives from spurious correlations by restricting conditioning sets to temporally-appropriate variables, avoiding conditioning on future colliders that would create spurious dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Collider-induced false dependencies and spurious correlations arising from selection bias in time-series/offline data.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detects spurious links by CI testing restricted by temporal ordering; independence conclusions under correct temporal conditioning lead to edge removal.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Refutation via CI tests performed under temporally-correct conditioning sets; edges inconsistent with CI evidence are removed or oriented accordingly.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>The paper reports that exploiting the temporal constraint reduces CI-test count and helps orient edges, and contributes to the high accuracy and efficiency of FOCUS (sample efficiency ~1e6). Exact CI-test count reduction and quantitative SHD/precision-recall improvements are not separately reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using domain-specific temporal constraints (future cannot cause past) reduces the conditioning search space, avoids common collider-conditioning mistakes, and improves both efficiency and correctness of causal discovery from offline RL data.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e754.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e754.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conditional-variable selection principle</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Principle for choosing conditional variables in RL (condition on t, not on t+1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A heuristic/principle to choose conditioning sets for CI tests in RL: include other variables at time t and exclude variables at time t+1 to avoid collider-induced errors when testing causation from X_t to Y_{t+1}.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Temporal conditional-variable principle</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Classified analysis of basic causal motifs (Chain, Fork, Collider) in the two-timestep RL setting leads to the rule: (1) condition on other variables at the same time t; (2) do not condition on variables at time t+1. This helps the CI test produce correct conclusions for causal parent identification.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Toy Car Driving and MuJoCo offline datasets</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Offline MDP trajectories where each candidate cause belongs to time t and each effect to time t+1; environments are continuous-control simulators, experiments offline.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Avoids inadvertently introducing spurious dependence via conditioning on colliders (variables at t+1) — thereby reducing false positives due to collider structures or improper conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Collider-induced spurious dependence and improper conditioning artifacts from including future variables in conditioning sets.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Works in combination with CI (KCI) tests — selecting proper conditioning sets prevents collider-related detection errors.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Indirect refutation: by ensuring correct conditioning, independence findings more reliably refute spurious causal links.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Ablation FOCUS(-CONDITION) (condition on all other variables rather than following the principle) reduced accuracy to 0.65 and increased variance to 0.212, showing the principle materially improves discovery robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>FOCUS(-CONDITION) accuracy 0.65 (variance 0.212) versus full FOCUS accuracy 0.993 (variance 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Choosing conditioning variables according to the temporal principle (condition on t variables only) is essential to avoid collider-induced misclassification and substantially increases structure-learning accuracy and robustness in offline RL.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e754.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e754.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Causal Structure Network (masking)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal Structure Network (per-dimension masked world-model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model architecture that instantiates the learned causal mask G by creating per-target copies of the base model and masking out non-parent input dimensions, so each output dimension is predicted only from its discovered causal parents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Causal Structure Network (M_causal)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Given causal mask G (binary matrix of which inputs at time t are causal parents of each output dimension), create per-output models M_i that take the masked input (s_t, a_t) ○ G_{:,i} to predict target dimension i at t+1. During training and rollout, non-causal inputs are zeroed out (or removed) so that the learned dynamics conform to the discovered causal structure.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Toy Car Driving; MuJoCo (Inverted Pendulum) — used in offline experiments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Offline datasets from simulators; the architecture is designed for supervised world-model learning from logged trajectory data.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Explicit variable selection by masking non-causal inputs per-target — effectively removes distractors from the model input set, preventing the model from relying on spurious correlated variables.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant/spurious input variables and selection-bias-induced aliases between variables in biased offline datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Relies on upstream CI detection (KCI + thresholding) to produce mask G; the network itself enacts removal (masking) rather than detecting.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Hard zeroing / exclusion of masked inputs; this is an explicit downweighting to zero influence.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>When combined with causal discovery (KCI + conditional-variable principle), the masked causal network supports high-accuracy discovery (0.993) and strong downstream policy returns; no separate performance number for masked network alone is given.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Per-dimension masking of non-causal inputs is an effective practical mechanism to prevent world-models from exploiting spurious correlations present in biased offline data, improving generalization and downstream policy performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e754.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e754.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LNCM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning Neural Causal Models from Unknown Interventions (LNCM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An online causal-structure-learning method that parameterizes causal structure and learns/updates structure by scoring candidate structures using likelihoods on (interventional) data; compared as a baseline in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning neural causal models from unknown interventions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LNCM</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Samples causal structures from a multivariate Bernoulli distribution, scores structures according to log-likelihood on (interventional) data, computes gradients for the Bernoulli parameters and updates them iteratively; designed for scenarios where interventional data is available and relies on interaction for scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Reported as baseline applied to offline versions of Toy Car Driving and MuJoCo in the experiments (transformed to offline by simple adjustment)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Originally designed for settings with interventional data; in the paper LNCM is used as a baseline by adapting it to offline datasets, but it was not designed for purely offline discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Scores candidate structures by likelihood on (interventional) data; in offline adaptation the scoring is done on collected data without active interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>LNCM originally uses sampling of structures + scoring on interventional data (requires interactions); in offline adaptation it is forced to rely on logged data rather than active experimentation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>In experiments LNCM achieved low causal-structure accuracy (0.52) compared to FOCUS (0.993) and performed worse on downstream policy learning in many datasets (see Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LNCM (an online-intervention-based method) does not transfer well to the offline setting and struggles with biased offline datasets where spurious correlated variables exist; FOCUS outperforms LNCM in accuracy, robustness, and sample efficiency on the tested offline tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Kernel-based conditional independence test and application in causal discovery <em>(Rating: 2)</em></li>
                <li>Learning neural causal models from unknown interventions <em>(Rating: 2)</em></li>
                <li>Causation, Prediction, and Search <em>(Rating: 2)</em></li>
                <li>A kernel-based causal learning algorithm <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-754",
    "paper_id": "paper-249375616",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "FOCUS",
            "name_full": "oFfline mOdel-based reinforcement learning with CaUsal Structure (FOCUS)",
            "brief_description": "A practical offline model-based RL algorithm that learns a causal world-model from observational offline data using conditional independence tests, then masks non-causal inputs in the learned world-model to reduce spurious dependence and improve offline policy learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "FOCUS (causal offline MBRL)",
            "method_description": "Two-stage pipeline: (1) causal-structure learning from offline trajectories by computing a p-value matrix via a kernel-based conditional independence (KCI) test for each candidate pair (X_t, Y_{t+1}), selecting a threshold p* (gap heuristic) and producing a binary causal mask G; (2) replacing the plain world-model with a causal-structure network that masks non-causal inputs per target dimension (Algorithm 2) and using any offline MBRL algorithm (they use MOPO) on the masked model. Key components: KCI testing, temporal constraint exploiting 'future cannot cause past' to reduce conditioning sets/orient edges, threshold selection for p-values, and per-dimension input masking to remove spurious variables.",
            "environment_name": "Toy Car Driving (2D car) and MuJoCo (Inverted Pendulum) — offline datasets (Random, Medium, Medium-Replay)",
            "environment_description": "Simulated virtual labs / continuous-control environments; experiments are done strictly offline using pre-collected trajectory datasets (Random, Medium, Medium-Replay). The environments are interactive simulators by nature, but FOCUS does not perform online interventions — it learns solely from offline data.",
            "handles_distractors": true,
            "distractor_handling_technique": "Detection via kernel-based conditional independence tests; selection of conditional variables to avoid collider-induced mistakes; explicit variable selection by masking non-causal inputs (per-dimension causal mask) to remove distractors/spurious inputs from the world-model.",
            "spurious_signal_types": "Spurious correlated variables induced by biased data collection / selection bias (variables that are highly predictable from true causal parents in the offline dataset), irrelevant variables (variables correlated due to environment bias), and measurement/noise indirectly addressed via KCI and masking.",
            "detection_method": "Compute KCI-based p-values for conditional independence for each candidate pair (X_i_t, X_j_{t+1}) conditioned on selected variables (principle: condition only on variables at time t, not t+1); threshold p* on the p-value matrix to classify causal vs non-causal links.",
            "downweighting_method": "Hard removal (masking) of detected non-causal / spurious inputs for each target dimension in the learned world-model (causal structure network zeroes out masked inputs); implicitly downweights spurious signals to zero influence.",
            "refutation_method": "Refutation is via conditional independence testing (KCI) under carefully chosen conditioning sets and a threshold p*; edges not supported by CI tests are removed from the causal parent set, reducing false causal claims caused by spurious correlations.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Causal structure learning accuracy 0.993 (±0.001 variance) and sample efficiency ~1e6 samples to a stable structure; policy returns substantially improved over baselines (e.g., Car Driving Random: FOCUS 68.1 ± 20.9 vs MOPO -30.3 ± 49.9). Ablation: using KCI and the conditional-variable selection principle are critical for high accuracy/robustness.",
            "performance_without_robustness": "Ablations: FOCUS(-KCI) (linear independence test) accuracy 0.62, variance 0.173; FOCUS(-CONDITION) (conditioning on all other variables) accuracy 0.65, variance 0.212. Baselines: LNCM accuracy 0.52 (variance 0.025) and MOPO (plain model-based offline RL) yields much lower policy returns (see paper Table 2).",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Learning and using a causal structure learned from offline data (via KCI + temporal conditioning principle) allows removal of spurious input variables from the world-model, substantially improving both the accuracy of discovered structure (0.993) and downstream offline policy performance; ablations show both the kernel CI test and the conditional-variable selection principle are necessary for the high accuracy and robustness.",
            "uuid": "e754.0"
        },
        {
            "name_short": "KCI-test",
            "name_full": "Kernel-based Conditional Independence Test (KCI)",
            "brief_description": "A nonparametric kernel-based statistical test that assesses conditional independence between continuous variables by computing a test statistic from kernel matrices and estimating the distribution of the statistic, requiring no parametric functional form assumptions.",
            "citation_title": "Kernel-based conditional independence test and application in causal discovery",
            "mention_or_use": "use",
            "method_name": "KCI (kernel-based CI test)",
            "method_description": "Computes kernel matrices for X, Y and Z and forms a test statistic that measures dependence between X and Y after removing dependencies explained by Z. The null (conditional independence) distribution is estimated (per Zhang et al. UAI'11) and a p-value is returned. Suitable for continuous variables and non-linear relationships without specifying functional forms.",
            "environment_name": "Applied to offline datasets from Toy Car Driving and MuJoCo (Inverted Pendulum) for causal discovery",
            "environment_description": "Offline virtual-lab datasets derived from simulators; tests operate on collected trajectory tuples (s_t, a_t, s_{t+1}); no online interventions conducted during discovery.",
            "handles_distractors": true,
            "distractor_handling_technique": "Detects spurious / non-causal associations by testing conditional independence given chosen conditioning sets; used as the primary detector of spurious correlations.",
            "spurious_signal_types": "Selection-bias-induced spurious correlations (variables predictable from causal parents in biased datasets), nonlinear dependencies, and continuous-variable confounding that can be exposed via conditioning.",
            "detection_method": "Return a p-value f_KCI(X,Y,Z) ∈ [0,1] representing probability of conditional independence; small p implies dependence (causation candidate); used to populate a p-value matrix for all candidate edges.",
            "downweighting_method": null,
            "refutation_method": "Edges are rejected when p &gt; threshold p* (statistical acceptance of conditional independence), effectively refuting putative causal relations not supported by CI evidence.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "When used within FOCUS, KCI is associated with a high causal discovery accuracy (overall FOCUS accuracy 0.993) and low variance (0.001); ablation replacing KCI with a linear test reduced accuracy to 0.62 and increased variance to 0.173.",
            "performance_without_robustness": "Ablation FOCUS(-KCI) accuracy 0.62 (variance 0.173); specific per-edge metrics (precision/recall) are not reported in the paper.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "KCI is an effective detector of conditional (in)dependence for continuous RL state variables in offline datasets and is a key component enabling robust discovery of causal parents and removal of spurious variables; replacing KCI with a simple linear test degrades structure accuracy and robustness substantially.",
            "uuid": "e754.1"
        },
        {
            "name_short": "Temporal-PC (extension)",
            "name_full": "PC algorithm extended with temporal constraint ('future cannot cause past')",
            "brief_description": "An adaptation of the PC causal discovery algorithm that exploits temporal ordering in RL data to reduce conditioning sets and orient edges: forbids edges from future timestep variables to past, thereby simplifying tests and identifying directions.",
            "citation_title": "Causation, Prediction, and Search",
            "mention_or_use": "use",
            "method_name": "PC algorithm with temporal constraint",
            "method_description": "PC-style constraint-based discovery using conditional independence tests but restricted by temporal ordering: candidate conditioning sets are limited to variables consistent with time (condition on t variables not t+1), reducing exponential CI-test search and enabling edge orientation by excluding time-inconsistent directions. This extension reduces the number of CI tests and helps determine causal direction in the discrete-time MDP setting.",
            "environment_name": "Offline trajectory datasets (Toy Car Driving, MuJoCo/Inverted Pendulum)",
            "environment_description": "Offline data derived from interactive simulators but treated as temporally ordered observational data; temporal constraints are intrinsic to the MDP timestep ordering.",
            "handles_distractors": true,
            "distractor_handling_technique": "Reduces false positives from spurious correlations by restricting conditioning sets to temporally-appropriate variables, avoiding conditioning on future colliders that would create spurious dependencies.",
            "spurious_signal_types": "Collider-induced false dependencies and spurious correlations arising from selection bias in time-series/offline data.",
            "detection_method": "Detects spurious links by CI testing restricted by temporal ordering; independence conclusions under correct temporal conditioning lead to edge removal.",
            "downweighting_method": null,
            "refutation_method": "Refutation via CI tests performed under temporally-correct conditioning sets; edges inconsistent with CI evidence are removed or oriented accordingly.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "The paper reports that exploiting the temporal constraint reduces CI-test count and helps orient edges, and contributes to the high accuracy and efficiency of FOCUS (sample efficiency ~1e6). Exact CI-test count reduction and quantitative SHD/precision-recall improvements are not separately reported.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Using domain-specific temporal constraints (future cannot cause past) reduces the conditioning search space, avoids common collider-conditioning mistakes, and improves both efficiency and correctness of causal discovery from offline RL data.",
            "uuid": "e754.2"
        },
        {
            "name_short": "Conditional-variable selection principle",
            "name_full": "Principle for choosing conditional variables in RL (condition on t, not on t+1)",
            "brief_description": "A heuristic/principle to choose conditioning sets for CI tests in RL: include other variables at time t and exclude variables at time t+1 to avoid collider-induced errors when testing causation from X_t to Y_{t+1}.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Temporal conditional-variable principle",
            "method_description": "Classified analysis of basic causal motifs (Chain, Fork, Collider) in the two-timestep RL setting leads to the rule: (1) condition on other variables at the same time t; (2) do not condition on variables at time t+1. This helps the CI test produce correct conclusions for causal parent identification.",
            "environment_name": "Toy Car Driving and MuJoCo offline datasets",
            "environment_description": "Offline MDP trajectories where each candidate cause belongs to time t and each effect to time t+1; environments are continuous-control simulators, experiments offline.",
            "handles_distractors": true,
            "distractor_handling_technique": "Avoids inadvertently introducing spurious dependence via conditioning on colliders (variables at t+1) — thereby reducing false positives due to collider structures or improper conditioning.",
            "spurious_signal_types": "Collider-induced spurious dependence and improper conditioning artifacts from including future variables in conditioning sets.",
            "detection_method": "Works in combination with CI (KCI) tests — selecting proper conditioning sets prevents collider-related detection errors.",
            "downweighting_method": null,
            "refutation_method": "Indirect refutation: by ensuring correct conditioning, independence findings more reliably refute spurious causal links.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Ablation FOCUS(-CONDITION) (condition on all other variables rather than following the principle) reduced accuracy to 0.65 and increased variance to 0.212, showing the principle materially improves discovery robustness.",
            "performance_without_robustness": "FOCUS(-CONDITION) accuracy 0.65 (variance 0.212) versus full FOCUS accuracy 0.993 (variance 0.001).",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Choosing conditioning variables according to the temporal principle (condition on t variables only) is essential to avoid collider-induced misclassification and substantially increases structure-learning accuracy and robustness in offline RL.",
            "uuid": "e754.3"
        },
        {
            "name_short": "Causal Structure Network (masking)",
            "name_full": "Causal Structure Network (per-dimension masked world-model)",
            "brief_description": "A model architecture that instantiates the learned causal mask G by creating per-target copies of the base model and masking out non-parent input dimensions, so each output dimension is predicted only from its discovered causal parents.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Causal Structure Network (M_causal)",
            "method_description": "Given causal mask G (binary matrix of which inputs at time t are causal parents of each output dimension), create per-output models M_i that take the masked input (s_t, a_t) ○ G_{:,i} to predict target dimension i at t+1. During training and rollout, non-causal inputs are zeroed out (or removed) so that the learned dynamics conform to the discovered causal structure.",
            "environment_name": "Toy Car Driving; MuJoCo (Inverted Pendulum) — used in offline experiments",
            "environment_description": "Offline datasets from simulators; the architecture is designed for supervised world-model learning from logged trajectory data.",
            "handles_distractors": true,
            "distractor_handling_technique": "Explicit variable selection by masking non-causal inputs per-target — effectively removes distractors from the model input set, preventing the model from relying on spurious correlated variables.",
            "spurious_signal_types": "Irrelevant/spurious input variables and selection-bias-induced aliases between variables in biased offline datasets.",
            "detection_method": "Relies on upstream CI detection (KCI + thresholding) to produce mask G; the network itself enacts removal (masking) rather than detecting.",
            "downweighting_method": "Hard zeroing / exclusion of masked inputs; this is an explicit downweighting to zero influence.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "When combined with causal discovery (KCI + conditional-variable principle), the masked causal network supports high-accuracy discovery (0.993) and strong downstream policy returns; no separate performance number for masked network alone is given.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Per-dimension masking of non-causal inputs is an effective practical mechanism to prevent world-models from exploiting spurious correlations present in biased offline data, improving generalization and downstream policy performance.",
            "uuid": "e754.4"
        },
        {
            "name_short": "LNCM",
            "name_full": "Learning Neural Causal Models from Unknown Interventions (LNCM)",
            "brief_description": "An online causal-structure-learning method that parameterizes causal structure and learns/updates structure by scoring candidate structures using likelihoods on (interventional) data; compared as a baseline in the paper.",
            "citation_title": "Learning neural causal models from unknown interventions",
            "mention_or_use": "use",
            "method_name": "LNCM",
            "method_description": "Samples causal structures from a multivariate Bernoulli distribution, scores structures according to log-likelihood on (interventional) data, computes gradients for the Bernoulli parameters and updates them iteratively; designed for scenarios where interventional data is available and relies on interaction for scoring.",
            "environment_name": "Reported as baseline applied to offline versions of Toy Car Driving and MuJoCo in the experiments (transformed to offline by simple adjustment)",
            "environment_description": "Originally designed for settings with interventional data; in the paper LNCM is used as a baseline by adapting it to offline datasets, but it was not designed for purely offline discovery.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": "Scores candidate structures by likelihood on (interventional) data; in offline adaptation the scoring is done on collected data without active interventions.",
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": true,
            "inquiry_strategy": "LNCM originally uses sampling of structures + scoring on interventional data (requires interactions); in offline adaptation it is forced to rely on logged data rather than active experimentation.",
            "performance_with_robustness": "In experiments LNCM achieved low causal-structure accuracy (0.52) compared to FOCUS (0.993) and performed worse on downstream policy learning in many datasets (see Table 2).",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "LNCM (an online-intervention-based method) does not transfer well to the offline setting and struggles with biased offline datasets where spurious correlated variables exist; FOCUS outperforms LNCM in accuracy, robustness, and sample efficiency on the tested offline tasks.",
            "uuid": "e754.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Kernel-based conditional independence test and application in causal discovery",
            "rating": 2,
            "sanitized_title": "kernelbased_conditional_independence_test_and_application_in_causal_discovery"
        },
        {
            "paper_title": "Learning neural causal models from unknown interventions",
            "rating": 2,
            "sanitized_title": "learning_neural_causal_models_from_unknown_interventions"
        },
        {
            "paper_title": "Causation, Prediction, and Search",
            "rating": 2,
            "sanitized_title": "causation_prediction_and_search"
        },
        {
            "paper_title": "A kernel-based causal learning algorithm",
            "rating": 1,
            "sanitized_title": "a_kernelbased_causal_learning_algorithm"
        }
    ],
    "cost": 0.01709475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>OFFLINE REINFORCEMENT LEARNING WITH CAUSAL STRUCTURED WORLD MODELS A PREPRINT</p>
<p>Zheng-Mao Zhu 
National Key Laboratory for Novel Software Technology
Nanjing University</p>
<p>Xiong-Hui Chen 
National Key Laboratory for Novel Software Technology
Nanjing University</p>
<p>Hong-Long Tian 
National Key Laboratory for Novel Software Technology
Nanjing University</p>
<p>Kun Zhang 
Department of Philosophy
Carnegie Mellon University</p>
<p>Department of Machine Learning, Mohamed bin Zayed University of Artificial Intelligence</p>
<p>Yang Yu 
National Key Laboratory for Novel Software Technology
Nanjing University</p>
<p>Polixir Ai 
Peng Cheng Laboratory 
OFFLINE REINFORCEMENT LEARNING WITH CAUSAL STRUCTURED WORLD MODELS A PREPRINT</p>
<p>Model-based methods have recently shown promising for offline reinforcement learning (RL), aiming to learn good policies from historical data without interacting with the environment. Previous modelbased offline RL methods learn fully connected nets as world-models to map the states and actions to the next-step states. However, it is sensible that a world-model should adhere to the underlying causal effect such that it will support learning an effective policy generalizing well in unseen states. In this paper, We first provide theoretical results that causal world-models can outperform plain world-models for offline RL by incorporating the causal structure into the generalization error bound. We then propose a practical algorithm, oFfline mOdel-based reinforcement learning with CaUsal Structure (FOCUS), to illustrate the feasibility of learning and leveraging causal structure in offline RL. Experimental results on two benchmarks show that FOCUS reconstructs the underlying causal structure accurately and robustly. Consequently, it performs better than the plain model-based offline RL algorithms and other causal model-based RL algorithms.</p>
<p>Introduction</p>
<p>Offline RL has gained great interest since it enables RL algorithms to scale to many real-world applications, e.g., recommender systems [1,2], autonomous driving [3], and healthcare [4], waiving costly online trial-and-error. In the offline setting, Model-Based Reinforcement Learning (MBRL) is an important direction that already produces significant offline learning performance [5,6]. Moreover, learning models is also useful for training transferable policies [7,6]. Therefore, there is increasing studies on learning world-models, from supervised prediction methods [8] to adversarial learning methods [9]. However, commonly there exists an underlying causal structure among the states and actions in various tasks. The causal structure can support learning a policy with better generalization ability. For example, in driving a car where the speed depends on the gas and brake pedals but not the wiper, a plain world-model trained on rainy days always predicts deceleration when the wiper is turned on and thus can not generalize to other weather situations. By contrast, a causal world-model can avoid the spurious dependence between wiper and deceleration (because of the rain) and hence help generalize well in unseen weather. In fact, empirical evidence also indicates that inducing the causal structure is important to improve the generalization of RL [10,11,12,13,14], but little attention was paid on causal world-model learning.</p>
<p>In this paper, we first provide theoretical support for the statement above: we show that a causal world-model can outperform a plain world-model for offline RL. From the causal perspective, we divide the variables in states and actions into two categories, namely, causal variables and spurious variables, and then formalize the procedure that learns a world-model from raw measured variables. Based on the formalization, we quantize the influence of the spurious dependence on the generalization error bound and thus prove that incorporating the causal structure can help reduce this bound.</p>
<p>We then propose a practical offline MBRL algorithm with causal structure, FOCUS, to illustrate the feasibility of learning causal structure in offline RL. An essential step of FOCUS is to learn the causal structure from data and then use it properly. Learning causal structure from data has been known as causal discovery [15]. There are some challenges in utilizing causal discovery methods in RL, and there are specific properties in the data that causal discovery may benefit from. Specifically, we extended the PC algorithm, which aims to find causal discovery based on the inferred conditional independence relations, to incorporate the constraint that the future cannot cause the past. Consequently, we can reduce the number of conditional independence tests and determine the causal direction. We further adopt kernel-based independence testing [16], which can be applied in continuous variables without assuming a functional form between the variables as well as the data distribution.</p>
<p>In summary, this paper makes the following key contributions:</p>
<p>• It shows theoretically that a causal world-model outperforms a plain world-model in offline RL, in terms of the generalization error bound. • It proposes a practical algorithm, FOCUS, and illustrates the feasibility of learning and using a causal world-model for offline MBRL. • Our experimental results verify the theoretical claims, showing that FOCUS outperforms baseline models and other online causal MBRL algorithms in the offline setting.</p>
<p>Related Work</p>
<p>Causal Structure Learning in Online MBRL. Despite that some methods have been proposed for learning causal structure in online RL, such methods all depend on interactions and do not have a mechanism to transfer into the offline setting. The core of online causal structure learning is to evaluate the performance or other metrics of one structure by interactions and choose the best one as the causal structure. [13] parameterizes the causal structure in the model and learns policies for each possible causal structure by minimizing the log-likelihood of dynamics. Given learned policies, it makes regression between the policy returns and the causal structure and then chooses the structure corresponding to the highest policy return. [17] (LNCM) samples causal structures from Multivariate Bernoulli distribution and scores those structures according to the log-likelihood on interventional data. Based on the scores, it calculates the gradients for the parameters of the Multivariate Bernoulli distribution and updates the parameters iteratively. [12] utilizes the speed of adaptation to learn the causal direction, which does not provide a complete algorithm for learning causal structure. By contrast, FOCUS utilizes a causal discovery method for causal structure learning, which only relies on the collected data to obtain the causal structure.</p>
<p>Causal Discovery Methods. The data in RL is more complex than it in traditional causal discovery, where the data is often discrete and the causal relationship is under a simple linear assumption. In recent years, practical methods have been proposed for causal discovery for continuous variables, which is the case we are concerned with in this paper. [18] is based on explicit estimation of the conditional densities or their variants, which exploit the difference between the characteristic functions of these conditional densities. The estimation of the conditional densities or related quantities is difficult, which deteriorates the testing performance especially when the conditioning set is not small enough. [19] discretizes the conditioning set to a set of bins and transforms conditional independence (CI) to the unconditional one in each bin. Inevitably, due to the curse of dimensionality, as the conditioning set becomes larger, the required sample size increases dramatically. By contrast, the KCI test [16] is a popular and widely-used causal discovery method, in which the test statistic can be easily calculated from the kernel matrices and the distribution can be estimated conveniently.</p>
<p>Preliminaries</p>
<p>Markov Decision Process (MDP). We describe the RL environment as an MDP with five-tuple ⟨S, A, P, R, γ⟩ [20], where S is a finite set of states; A is a finite set of actions; P is the transition function with P (s ′ s, a) denoting the next-state distribution after taking action a in state s; R is a reward function with R(s, a) denoting the expected immediate reward gained by taking action a in state s; and γ ∈ [0, 1] is a discount factor. An agent chooses actions a according to a policy a ∼ π(s), which updates the system state s ′ ∼ P (s, a), yielding a reward r ∼ R(s, a). The agent's goal is to maximize the the expected cumulative return by learning a good policy max π,P E[γ t R(s t , a t )]. The state-action value Q π of a policy π is the expected discounted reward of executing action a from state s and subsequently following policy π: Q π (s, a) ∶= R(s, a) + γE s ′ ∼P,a ′ ∼π [Q π (s ′ , a ′ )].</p>
<p>Offline Model-based Reinforcement Learning. Model-based reinforcement learning typically involves learning a dynamics model of the environment by fitting it using a maximum-likelihood estimate of the trajectory-based data collected by running some exploratory policy [21,22]. In the offline RL setting, where we only have access to the data collected by multiple policies, recent techniques build on the idea of pessimism (regularizing the original problem based on how confident the agent is about the learned model) and have resulted in better sample complexity over model-free methods on benchmark domains [23,5].</p>
<p>Theory</p>
<p>In this section, we provide theoretical evidence for the advantages of a causal world-model over a plain world-model, which shows that utilizing a good causal structure can reduce the generalization error bounds in offline RL. Specifically, We incorporate the causal structure into the generalization error bounds, which include the model prediction error bound and policy evaluation error bound. The full proof can be found in Appendix A. In this paper, we focus on the linear case.</p>
<p>Model Prediction Error Bound</p>
<p>In this subsection, we formulize the procedure that learns a plain world-model and then connect the model prediction error bound with the number of spurious variables in the plain world-model. Specifically, we point out that the spurious variables lead the model learning problem to an ill-posed problem that has multiple optimal solutions, which consequently results in the increment of the model prediction error bound. Since model learning can be viewed as a supervised learning problem, we provide the model prediction error bound in a supervised learning framework.</p>
<p>Preliminary. Let D denote the data distribution where we have samples (X, Y ) ∼ D, X ∈ R n . The goal is to learn a linear function f to predict Y given X. From the causal perspective, Y is generated from only its causal parent variables rather than all the variables in X. Therefore we can split the variables in X into two categories, X = (X causal , X spurious ):</p>
<p>• X causal represents the causal parent variables of Y , that is, Y = f * (X causal ) + causal , where f * is the ground truth and causal is a zero mean noise variable that X causal causal . • X spurious represents the spurious variables that X spurious X causal , but in some biased data sets X spurious and X causal have strong relatedness. In other words, X spurious can be predicted by X causal with small error, i.e., X spurious = X causal γ spurious + spurious , where spurious is the regression error with zero mean and small variance.</p>
<p>For clearly representation, we use X cau ≜ X ○ ω cau (○ represents element-wise multiplication) to replace X causal , where cau records the indices of X causal in X and (ω cau ) i = I(i ∈ cau). Correspondingly, we also use X spu ≜ X ○ ω spu to replace X spurious . According to the definition of X cau , we have Y = (X ○ ω cau )β * + cau , where ω cau ○ β * is the global optimal solution of the optimization problem
min β E (X,Y )∼D [Xβ − Y ] 2 .(1)
Above problem is easy if the data is uniformly sampled from D. However, in the offline setting, we only have biased data D train sampled by given policy π train , where the optimization objective is
min β E (X,Y )∼Dtrain [Xβ − Y ] 2 .(2)
The Problem 2 has multiple optimal solutions due to the strong linear relatedness of X spu and X cau in D train , which is proved in Lemma 4.1. Lemma 4.1. Given that ω cau ○β * is the optimal solution of Problem 1, suppose that in D train , X spu = (X○ω cau )γ spu + spu where E Dtrain [ spu ] = 0 and γ spu ≠ 0, we have thatβ spu ≜ ω cau ○ (β * − λγ spu ) + λω spu is also an optimal solution of Problem 2 for any λ:
E (X,Y )∼Dtrain X(ω cau ○ β * ) − Y 2 X = E (X,Y )∼Dtrain Xβ spu − Y 2 X
The most popular method for solving such ill-posed problem is to add a regularization term for parameters β [24]:
min β E (X,Y )∼Dtrain [Xβ − Y ] 2 + k β 2 ,(3)
where k is a coefficient. The form of Problem 3 corresponds to the form of the ridge regression, which provides a closed-form solution of k by Hoerl-Kennard formula [25].</p>
<p>In the following, we will first introduce the solution of λ under Problem 3 in Lemma 4.2, and then introduce the model prediction error bound with λ in Theorem 4.4. For ease of understanding, we provide a simple version where the dimensions of X cau and X spu are both one ( X cau = X spu = 1).</p>
<p>Lemma 4.2 (λ Lemma).</p>
<p>Given λ as the coefficient in Lemma 4.1, and k in Problem 3 chosen by Hoerl-Kennard formula, we have the solution of λ in Problem 3 that:
λ = β * γ spu β * 2 + γ 2 spu + 1 + σ 2 spu σ 2 cau (1 + 1 (β * ) 2 )(4)
Based on Lemma 4.2, we can find that the smaller σ 2 spu (it means that X spu and X cau have stronger relatedness in the training dataset D train ), the larger λ. And we also have its bound: Proposition 4.3. Given λ as Formula 4, the bound of λ is that − 1 2 ≤ λ ≤ 1 2 . Theorem 4.4 (Spurious Theorem). Let D = {(X, Y )} denote the data distribution,β spu denote the solution in Lemma 4.1 with λ in Lemma 4.2, andŶ spu = Xβ spu denote the prediction. Suppose that the data value is bounded: X i 1 ≤ X max , i = 1, ⋯, n and the error of optimal solution cau is also bounded: cau 1 ≤ c , we have the model prediction error bound:
E (X,Y )∼D [( Ŷ spu − Y 1 ) X] ≤ X max λ 1 ( γ spu 1 + 1) + c .(5)
Theorem 4.4 shows that</p>
<p>• The upper bound of the model prediction error Ŷ spu − Y 1 increases by X max λ 1 ( γ spu 1 + 1) for each induced spurious variable X spu in the model. • When X spu and X cau have stronger relatedness (which means a bigger λ), the increment of the prediction model error bound led by X spu is bigger.</p>
<p>Policy Evaluation Error Bound</p>
<p>Although in most cases, an accurate model ensures a good performance in MBRL, the model error bound is still an indirect evaluation compared to the policy evaluation error bound for MBRL. In this subsection, we apply the spurious theorem (Theorem 4.4) to offline MBRL and provide the policy evaluation error bound with the number of spurious variables.</p>
<p>Suppose that the state value and reward are bounded that S t,i 1 ≤ S max , R t ≤ R max , let λ max denote the maximum of λ and γ max denote the maximum of γ spu 1 , we have the policy evaluation error bound in Theorem 4.5. 
= M i θ ((S t , A t ) ○ ω caui∪spui ). Let V M θ π
denote the policy value of the policy π in model M θ and correspondingly V M * π . For an arbitrary bounded divergence policy π, i.e. max S D KL (π(⋅ S), π D (⋅ S)) ≤ π , we have the policy evaluation error bound:
V M θ π − V M * π ≤ 2 √ 2R max (1 − γ) 2 √ π + R max γ 2(1 − γ) 2 S max [n s c + (1 + γ max )λ max n s (n s + n a )R spu ] where R spu = ∑ ns i=1 spui
ns(ns+na) , which represents the spurious variable density, that is, the ratio of spurious variables in all input variables .</p>
<p>Theorem 4.5 shows the relation between the policy evaluation error bound and the spurious variable density, which indicates that:</p>
<p>• When we use a non-causal model that all the spurious variables are input, R spu reaches its maximum valueR spu &lt; 1.</p>
<p>By contrast, in the optimal causal structure, R spu reaches its minimum value of 0. • The density of spurious variables R spu and the correlation strength of spurious variables λ max both influence the policy evaluation error bound. However, if we exclude all the spurious variables, i.e., R spu = 0, the correlation strength of spurious variables will have no effect.</p>
<p>Algorithm</p>
<p>In the theory section, we have provided the theoretical evidence about the advantages of a causal world-model over a plain world-model. Besides lower prediction errors, a causal world-model also matters for better decision-making in RL. In the condition that spurious variables do not increase prediction errors (e.g., spurious variables disturb only in unreachable states), a wrong causal relation also leads to terrible decision-making. For example, rooster crowing can predict the rise of the sun, but forcing a rooster to crow for a sunny day is a natural decision if we have a wrong causal relation that rooster crowing causes the rise of the sun. In the above example, predicting the rise of the sun by rooster crowing is a zero-error world-model since rooster crowing on a rainy day is an unreachable state, but such world-model leads to terrible decision-making.</p>
<p>After demonstrating the necessity of a causal world-model in offline RL, in this section we propose a practical offline MBRL algorithm, FOCUS, to illustrate the feasibility of learning causal structure in offline RL. The main idea of FOCUS is to take the advantage of causal discovery methods and extend it to offline MBRL. Compared to previous online causal structure learning methods, the causal discovery method brings the following advantages in the offline setting:</p>
<p>• Robust: The structure is not influenced by the performance of the test data, which is artificially selected and biased.</p>
<p>• Efficient: The causal discovery method directly returns the structure by independence testing without any network training procedure and thus saves the samples for network training. Conditional Independence Test. Independence and conditional independence (CI) play a central role in causal discovery [26, ?, 27]. Generally speaking, the CI relationship X Y Z allows us to drop Y when constructing a probabilistic model for X with (Y, Z). There are multiple CI testing methods for various conditions, which provide the correct conclusion only given the corresponding condition. The kernel-based Conditional Independence test (KCI-test) [16] is proposed for continuous variables without assuming a functional form between the variables as well as the data distributions, which is the case we are concerned with in this paper. Generally, the hypothesis H 0 that variables are conditionally independent is rejected when p is smaller than the pre-assigned significance level, say, 0,05. In practice, we can design the significance level instead of a fixed value.</p>
<p>Preliminary</p>
<p>Conditional Variables. Besides the specific CI test method, the conclusion of conditional independence testing also depends on the conditional variable Z, that is, different conditional variables can lead to different conclusions. Taking the triple (X, Y, Z) as an example, there are three typical structures, namely, Chain, Fork, and Collider as shown in Fig 2, where whether conditioning on Z significantly influences the testing conclusion.</p>
<p>• Chain: There exists causation between X and Y but conditioning on Z leads to independence.</p>
<p>• Fork: There does not exist causation between X and Y but not conditioning on Z leads to non-independence.</p>
<p>• Collider: There does not exist causation between X and Y but conditioning on Z leads to non-independence.</p>
<p>Building the Causal Structure from the KCI test</p>
<p>Applying the Independence Test in RL. Based on the preliminaries, given the two target variables X, Y and the condition variable Z, the KCI test returns a probability value p = f KCI (X, Y, Z) ∈ [0, 1], which measures the probability that X and Y are conditionally independent given the condition Z. In other words, a small p implies that X and Y have causation given Z. To transform an implicit probability value into an explicit conclusion of whether the causation exists, we design a threshold p * that:
Causation(X, Y ) = I(f KCI (X, Y, Z) ≤ p * ) ∈ {0, 1},
where Causation(X, Y ) = 1 represents independence and 0 represents that causation exists. Details of choosing p * can be found in Appendix B.1.</p>
<p>In model learning of RL, variables are composed of states and actions of the current and next timesteps and the causal structure refers to whether a variable in t timestep (e.g., the i th dimension, X i t ) causes another variable in t + 1 timestep (e.g., the j th dimension, X j t+1 ). With the KCI test, we get the causal relation through the function Causation(⋅, ⋅) for each variable pair (X i t , X j t+1 ) and then form the causal structure matrix G: G i,j = Causation(X i t , X j t+1 ), where G i,j is the element in row i and column j of G.</p>
<p>Choosing the Conditional Variable in RL. As we said in preliminaries, improper conditional variables can reverse the conclusion of independence testing. Therefore we have to carefully design the conditional variable set, which should include the intermediate variable of Chain, the common parent variable of Fork, but not the common son variable of Collider. Traditionally, the independence test has to traverse all possibilities of the conditional variable set and gives the conclusion, which is too time-consuming. However, in RL we can reduce the number of conditional independence tests by incorporating the constraint that the future cannot cause the past. Actually, this constraint limits the number of possible conditional variable set to a small value. Therefore we can even take a classified discussion for each possible conditional variable set. Before the discussion, we exclude two kinds of situations for simplicity:</p>
<p>• Impossible situations. We exclude some impossible situations as Fig 3 (i) by the temporal property of data in RL.</p>
<p>Specifically, the direction of the causation cannot be from the variable of t + 1 time step to that of t time step because the effect cannot happen before the cause. • Compound situations. We only discuss the basic situations and exclude the compound situations, e.g., Fig 3 (j), which is a compound of (a) and (c). It is because in such compound situations, the target variables X i t and X j t+1 have direct causation (or it can not be a compound situation) and the independence testing only misjudges independence as non-independence but not non-independence as independence.</p>
<p>We list all possible situations of target variables X i t , X j t+1 and condition variable X k t t+1 in the world-model as shown in Fig 3. Based on the causal discovery knowledge in the preliminaries, we analyze basic situations in the following: Obtain the optimal policy π * = Algo(D). Return π *</p>
<p>• Top Line: In (a)(b), whether conditioning on X k t does not influence the conclusion of causation; In (c), although X k t plays an intermediate variable in a Chain and conditioning on X k t leads to the conclusion of independence of X i t and X j t+1 , the causal parent set of X j t+1 will include X k t when testing the causal relation between X k t and X t+1 j, which can offset the influence of excluding X i t . In (d), conditioning on Z is necessary for getting the correct conclusion of causation since X k t is the common causal parent in a Fork structure. • Bottom Line: In (e)(f), whether conditioning on X k t+1 does not influence the conclusion of causation; In (g), not conditioning on X k t+1 is necessary to get the correct conclusion of causation since X k t+1 is the common son in a Collider structure; In (h), although X k t+1 plays an intermediate variable in a Chain and not conditioning on X k t+1 leads to the conclusion of non-independence of X i t and X j t+1 , including X i t in the causal parent set of X j t+1 will not induce any problem since X i t does indirectly cause X j t+1 . Based on the above classified discussion, we can conclude our principle for choosing conditional variables in RL that: (1) Condition on the other variables in t time step; (2) Do not condition on the other variables in t + 1 time step.</p>
<p>Combining Learned Causal Structure with An Offline MBRL Algorithm</p>
<p>We combine the learned causal structure with an offline MBRL algorithm, MOPO [5], to form a causal offline MBRL algorithm as in Fig 1. The complete learning procedure is shown in Algorithm 1, where Algorithm 2 can be found in Appendix B.2. Notice that our causal model learning method could be combined with any offline MBRL algorithm in principle. More implementation details and hyperparameter values are summarized in Appendix B.1.</p>
<p>Experiments</p>
<p>To demonstrate that (1) Learning a causal world-model is feasible in offline RL and (2) a causal world-model can outperform a plain world-model and other related online methods in offline RL, we evaluate (1) causal structure learning and (2) policy learning on the Toy Car Driving and MuJoCo benchmark. Toy Car Driving is a simple and typical environment that is convenient to evaluate the accuracy of learned causal structure because We can design the causation between variables in it. The MuJoCo is the most common benchmark to investigate the performance in continuous controlling, where each dimension of the state has a specific meaning and is highly abstract. We evaluate FOCUS on the following indexes: (1) The accuracy, efficiency and robustness of causal structure learning. (2) The policy return and generalization ability in offline MBRL.</p>
<p>Baselines. We compare FOCUS with the sota offline MBRL algorithm, MOPO, and other online RL algorithms that also learn causal structure in two aspects, causal structure learning and policy learning. (1) MOPO [5] is a well-known and widely-used offline MBRL algorithm, which outperforms standard model-based RL algorithms and prior sota model-free offline RL algorithms on existing offline RL benchmarks. The main idea of MOPO is to artificially penalize rewards by the uncertainty of the dynamics, which can avoid the distributional shift issue. MOPO can be seen as the blank control with a plain world-model. (2) Learning Neural Causal Models from Unknown Interventions (LNCM) [17] is an online MBRL, in which the causal structure learning method can be transformed to the offline setting with a simple adjustment. We take LNCM as an example to show that an online method cannot be directly transferred into offline RL.</p>
<p>Environment. Toy Car Driving. Toy Car driving is a typical RL environment where the agent can control its direction and velocity to finish various tasks including avoiding obstacles and navigating. The information of the car, e.g., position, velocity, direction, and acceleration, can form the state and action in an MDP. In this paper, we use a 2D Toy Figure 4: The visualization of the example. The red dotted arrow presents that (v x ) t is a spurious variable for (p y ) t+1 .</p>
<p>Car driving as the RL environment where the task of the car is to arrive at the destination (The visualization of states and a detailed description can be found in Appendix C.1). The state includes the direction d, the velocity (scalar) v, the velocity on the x-axis (one dimensional vector) v x , the velocity on the y-axis v y and the position (p x , p y ). The action is the steering angle a. The visualization of the causal graph can be found in Appendix C.1. This causal structure is designed to demonstrate how a variable become spurious for others and highlight their influence in model learning.</p>
<p>For example, when the velocity v t−1 maintains stationary due to an imperfect sample policy, (v x ) t and (v y ) t have strong relatedness that (v x ) 2 t + (v y ) 2 t = v 2 t−1 and one can represent the other. Since we design that (p y ) t+1 − (p y ) t = (v y ) t , (v x ) t and (p y ) t+1 − (p y ) t also have strong relatedness, which leads to that (v x ) t becomes a spurious variable of (p y ) t+1 given (p y ) t , despite that (v x ) t is not the causal parent of y t+1 . By contrast, when the data is uniformly sampled with various velocities, this spuriousness will not exist. MuJoCo. MuJoCo [28] is a general-purpose physics engine, which is also a well-known RL environment. MuJoCo includes multijoint dynamics with contact, where the variables of the state represent the positions, angles, and velocity of the agent. The dimensions of the state are from 3 to dozens. The limited dimensions and the clear meaning of each variable provide the convenience of causal structure learning.</p>
<p>Offline Data. We prepare three offline data sets, Random, Medium, and Replay for the Car Driving and MuJoCo. Random represents that data is collected by random policies. Medium represents that data is collected by a fixed but not well-trained policy, which is the least diverse. Medium-Replay is a collection of data that is sampled during training the Medium policy, which is the most diverse. The heat map of the data diversity is shown in Appendix C.1. </p>
<p>Causal Structure Learning</p>
<p>We compare FOCUS with baselines on the causal structure learning with the indexes of the accuracy, efficiency, and robustness. The accuracy is evaluated by viewing the structure learning as a classification problem, where causation represents the positive example and independence represents the negative example. The efficiency is evaluated by measuring the samples for getting a stable structure. The robustness is evaluated by calculating the variance in multiple experiments. The results in Table 1 show that FOCUS surpasses LNCM on accuracy, robustness, and efficiency in causal structure learning. Noticed that LNCM also has a low variance because it predicts the probability of existing causation between any variable pairs with around 50%, which means that its robustness is meaningless.</p>
<p>Policy Learning</p>
<p>Policy Return. We evaluate the performance of FOCUS and baselines in the two benchmarks on three different and typical offline data sets. The results in Table 2 show that FOCUS outperforms baselines by a significant margin in  most data sets. In Random, FOCUS has the most significant performance gains to the baselines in both benchmarks because of the accuracy of causal structure learning in FOCUS. By contrast, in Medium-Replay, the performance gains of FOCUS are least since the high data diversity in Medium-Replay leads to weak relatedness of spurious variables (corresponds to small λ), which verifies our theory. In Medium, the results in the two benchmarks are different. In Car Driving, the relatively high score of LNCM does not mean that LNCM is the best but all three fail. The failure indicates that extremely biased data makes even the causal model fail to generalize. However, the success of FOCUS in the Inverted Pendulum indicates that causal world-models depend less on the data diversity since FOCUS can still reach high scores in such a biased dataset where the baselines fail. Here we only provide the results in Inverted Pendulum but not all the environments in MuJoCo due to the characteristics of the robot control, specifically the frequency of observations, which we present a detailed description in Appendix C.1.</p>
<p>Generalization Ability. We compare the performance on different offline data sets, which is produced by mixing up Medium-Replay and Medium with different ratios. The X% in the x-axis represents that the data is mixed by 100% of the Medium and X% of the Medium-Replay. The results in Fig 5 show that FOCUS still performs well with a small ratio of Medium-Replay data while the baseline performs well only with a big ratio, which indicates that FOCUS is less dependent on the diversity of data.</p>
<p>Ablation Study</p>
<p>To evaluate the contribution of each component, we perform an ablation study for FOCUS. The results in Table 1 show that the KCI test and our principle of choosing conditional variables contribute to the causal structure learning of both accuracy and robustness.</p>
<p>Conclusion</p>
<p>In this paper, we provide theoretical evidence about the advantages of using a causal world-model in offline RL. We present error bounds of model prediction and policy evaluation in offline MBRL with causal and plain world-models.</p>
<p>We also propose a practical algorithm, FOCUS, to address the problem of learning causal structure in offline RL. The main idea of FOCUS is to utilize causal discovery methods for offline causal structure learning. We design a general mechanism to solve problems in extending causal discovery methods in RL, which includes conditional variables choosing. Extensive experiments on the typical benchmark demonstrate that FOCUS achieves accurate and robust causal structure learning and thus significantly surpasses baselines in offline RL.</p>
<p>The limitation of FOCUS lies in: In our theory, we assume that the true causal structure is given. However, in practice, one needs to learn it from data and then use it. Quantifying the uncertainty in the learned causal structure from data is known to be a hard problem, and as one line of our future research, we will derive the generalization error bound with the causal structure learned from data.</p>
<p>With above lemmas, we have: To be fair, we share a common p * for the testing between any two variables. The choice of p * significantly influences the accuracy of causal discovery that too small and too big both lead to causal misspecification. The intuition behind our choosing principle is that there is a significant gap in the p value between the causal relation and non-causal relation. Based on this intuition, we partition the probability range [0, 1] into several intervals [0, p 1 ), [p 1 , p 2 ), ⋯, [p n , 1] according to the sorted p values {p i } n i=1 and design p * by the formula:
V M θ π − V M * π ≤ 2 √ 2R max (1 − γ) 2 √ π + R max γ 2(1 − γ) 2 Sp * = arg max pi p i+1 i + 1 − p i i .(13)
If we only consider the biggest gap between p i , then we will easily choose a big but improper p * due to the distribution of p i in some intervals (e.g., [0.5,1]) may be very sparse and thus leads to a big gap.</p>
<p>(a) Random (b) Medium (c) Medium-Replay Figure 6: The heat map of the three offline data sets. The high brightness represents high data density.</p>
<p>B.2 Causal Structure Network</p>
<p>The details of Causal Structure Network is shown in Algorithm 2.</p>
<p>Algorithm 2 Causal Structure Network M Causal (⋅) Input: state s t ∈ R ns , action a t ∈ R na , causal structure mask matrix G ∈ {0, 1} (ns,na)×ns , Make M i (⋅; θ i ) as the copy of the basic model M(⋅; θ), where i = 1, ⋯, n s . for i = 1 to n s do Let G ⋅,i denote the i th column of G Get the masked input X = (s t , a t ) ○ G ⋅,i Get predictionỸ = M i (X; θ i ) ∈ R ns Let Y i denote the i th element ofỸ . end for Return Y = (Y i ) ns i=1 .</p>
<p>C Experiments</p>
<p>C.1 Environment Details</p>
<p>The heat map of the data diversity is shown in Fig 6. In Random, the data is clustered around the origin. In Medium, the data is gathered on a fixed trajectory from the origin to the destination. In Medium-Replay, the data is much more diverse where a lot of unseen data in above data sets is also sampled.</p>
<p>The visualization of the state in Car Driving and the ground truth of its causal graph are shown in Fig 8. MuJoCo formulates robot control into MDPs with discrete timestep via equal interval sampling of the continuous-time. Therefore, for each timestep t, s t+1 is the result of numerous times of simulation based on s t with repeated action a t . Even if spurious variables are existed in one time of simulation, after numerous simulations, the causal effect will be propagated to almost variables, which leads to a full-connection causal graph (R spu = 0). Therefore FOCUS degrades into vanilla MOPO in this scenario, which is meaningless to test. Fortunately, after analyzing the propagate progress of the dynamics, we found that the Inverted Pendulum is a special case where the causal graph will keep sparse after numerous simulations.</p>
<p>C.2 Experiment Result Details</p>
<p>The detailed training curves are shown in   Driving. The goal of the agent is to arrive at the star-shape destination. Right: The ground truth of the causal structure in Toy Car Driving. The state is vector-based and its value is continuous.</p>
<p>Theorem 4.5 (RL Spurious Theorem). Given an MDP with the state dimension n s and the action dimension n a , a data-collecting policy π D , let M * denote the true transition model, M θ denote the learned model that M i θ predicts the i th dimension with spurious variable sets spu i and causal variables cau i , i.e.,Ŝ t+1,i</p>
<p>Figure 1 :
1The architecture of FOCUS. Given offline data, FOCUS learns a p value matrix by KCI test and then gets the causal structure by choosing a p threshold. After combining the learned causal structure with the neural network, FOCUS learns the policy through an offline MBRL algorithm.</p>
<p>Figure 2 :
2The three basic structure for three variables X, Y and Z, where Z plays an important role in causal discovery.</p>
<p>Figure 3 :
3The basic, impossible and compound situations of the causation between target variables and condition variables. In the basic situations, Top Line: (a)-(d) list the situations that the condition variable X k is in the t time step. Bottom Line: Similarly, (e)-(h) list the situations that the condition variable X k is in the t + 1 time step.</p>
<p>Algorithm 1
1Causal Model Framework for Offline MBRL Input: offline data set D = {(s t , a t , s t+1 , r t )}; model M(⋅; θ); Stage 1: Causal Structure Learning Get p value matrix G p by KCI testing. Get the threshold p * by G p . Get causal structure mask matrix G by the threshold p * . Stage 2: Offline Reinforcement Learning Choose an offline model-based reinforcement learning algorithm Algo(⋅) and replace its model M(⋅) by M Causal (⋅, G, M) (Algorithm 2 in Appendix).</p>
<p>Figure 5 :
5The comparison on generalization in mixed data.</p>
<p>max [n s c +(γ max + 1)λ max n s (n s + n a )R spu ]</p>
<p>Fig 7 .
7</p>
<p>Figure 7 :
7Comparison of FOCUS and the baselines in the two benchmarks. (a)-(c): The comparison in the Car Driving on the three datasets. (d)-(f): The comparison in the Inverted Pendulum of MuJoCo on the three datasets.</p>
<p>Figure 8 :
8The visualization of the state and the causal structure for the Car Driving benchmark. Left: the Toy Car</p>
<p>Table 1 :
1The results on causal structure learning of our model and the baselines. Both the accuracy and the variance are calculated by five times experiments. FOCUS (-KCI) represents FOCUS with a linear independence test. FOCUS (-CONDITION) represents FOCUS with choosing all other variables as conditional variables.INDEX 
FOCUS LNCM FOCUS(-KCI) FOCUS(-CONDITION) </p>
<p>ACCURACY 
0.993 
0.52 
0.62 
0.65 
ROBUSTNESS 
0.001 
0.025 
0.173 
0.212 
EFFICIENCY(SAMPLES) 
1 × 10 6 
1 × 10 7 
1 × 10 6 
1 × 10 6 </p>
<p>Table 2 :
2The comparison on converged policy return in the two benchmarks. The detailed training curves are in Appendix C.1.ENV 
CAR DRIVING 
MUJOCO(INVERTED PENDULUM) </p>
<p>RANDOM 
MEDIUM 
REPLAY 
RANDOM 
MEDIUM 
REPLAY </p>
<p>FOCUS 68.1 ± 20.9 −58.9 ± 41.3 86.2 ± 18.2 23.5 ± 17.9 24.9 ± 14.1 49.2 ± 19.0 </p>
<p>MOPO 
−30.3 ± 49.9 −50.1 ± 34.2 
46.2 ± 28.1 
8.5 ± 6.2 
2.5 ± 0.08 
43.4 ± 7.7 </p>
<p>LNCM 
9.9 ± 42.5 
−5.4 ± 32.5 
11.4 ± 24.0 
13.3 ± 0.9 
3.1 ± 0.7 
16.3 ± 6.4 </p>
<p>5DWLRRIPL[HGGDWD </p>
<p>$YHUDJHVFRUHSHUHSLVRGH </p>
<p>&amp;RPSDULVRQRQPL[HGGDWD </p>
<p>MOPO 
FOCUS </p>
<p>Appendix A TheoryDefinition A.1 (Optimization objective in data distribution D:).Definition A.2 (Optimization objective in data D train :).Definition A.3 (Optimization objective in data D train with regularization:).Lemma A.4. Given that ω cau ○β * is the optimal solution of Problem 1, suppose that in D train , X spu = (X○ω cau )γ spu + spu where E Dtrain [ spu ] = 0 and γ spu ≠ 0, we have thatβ spu ≜ ω cau ○ (β * − λγ spu ) + λω spu is also an optimal solution of Problem 2 for any λ:Proof. Since the solution of the ridge regression iswe takeβ spu into this solution and get:Proposition A.6. Given λ as Formula 4, we haveProof.Theorem A.7 (Spurious Theorem). Let D = {(X, Y )} denote the data distribution,β spu denote the solution in Lemma 4.1 with λ in Lemma 4.2, andŶ spu = Xβ spu denote the prediction. Suppose that the data value is bounded: X i 1 ≤ X max , i = 1, ⋯, n and the error of optimal solution cau is also bounded: cau 1 ≤ c , we have the model prediction error bound:Proof. LetŶ cau denote (X ○ ω cau )β * , we have. Given an MDP with the state dimension n s and the action dimension n a , a data-collecting policy π D , let M * denote the true transition model, M θ denote the learned model that M i θ predicts the i th dimension with spurious variable sets spu i and causal variables cau i , i.e.,Ŝ t+1,denote the policy value of the policy π in model M θ and correspondingly V M * π . For any bounded divergence policy π, i.e. max S D KL (π(⋅ S), π D (⋅ S)) ≤ π , we have the policy evaluation error bound:where R spu = ∑ ns i=1 spui ns(ns+na) , which represents the spurious variable density, that is, the ratio of spurious variables in all input variables .Proof. Before proving, we first introduce three lemmas:The detailed proof of these lemmas can be found in[9], which is omitted in this paper. Based on the model prediction error bound in Theorem 4.4, we have:=S max [n s c + (γ max + 1)λ max ns i=1spu i ] =S max [n s c + (γ max + 1)λ max n s (n s + n a )R spu ]
Virtual-Taobao: Virtualizing real-world online retail environment for reinforcement learning. Jing-Cheng Shi, Yang Yu, Qing Da, Shi-Yong Chen, An-Xiang Zeng, Proceedings of the 33rd AAAI Conference on Artificial Intelligence (AAAI'19). the 33rd AAAI Conference on Artificial Intelligence (AAAI'19)Honolulu, HIJing-Cheng Shi, Yang Yu, Qing Da, Shi-Yong Chen, and An-Xiang Zeng. Virtual-Taobao: Virtualizing real-world online retail environment for reinforcement learning. In Proceedings of the 33rd AAAI Conference on Artificial Intelligence (AAAI'19), Honolulu, HI, 2019.</p>
<p>Partially observable environment estimation with uplift inference for reinforcement learning based recommendation. Wenjie Shang, Qingyang Li, Zhiwei Qin, Yang Yu, Yiping Meng, Jieping Ye, Machine Learning. Wenjie Shang, Qingyang Li, Zhiwei Qin, Yang Yu, Yiping Meng, and Jieping Ye. Partially observable environment estimation with uplift inference for reinforcement learning based recommendation. Machine Learning, (9):2603- 2640, 2021.</p>
<p>BDD100K: A diverse driving video database with scalable annotation tooling. Fisher Yu, Wenqi Xian, Yingying Chen, Fangchen Liu, Mike Liao, Vashisht Madhavan, Trevor Darrell, 1805.04687Fisher Yu, Wenqi Xian, Yingying Chen, Fangchen Liu, Mike Liao, Vashisht Madhavan, and Trevor Darrell. BDD100K: A diverse driving video database with scalable annotation tooling. CoRR, 1805.04687, 2018.</p>
<p>Guidelines for reinforcement learning in healthcare. O Gottesman, F Johansson, M Komorowski, A Faisal, David Sontag, Finale Doshi-Velez, L A Celi, Nature Medicine. 251O. Gottesman, F. Johansson, M. Komorowski, A. Faisal, David Sontag, Finale Doshi-Velez, and L. A. Celi. Guidelines for reinforcement learning in healthcare. Nature Medicine, 25(1):16-18, 2019.</p>
<p>MOPO: model-based offline policy optimization. Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, Tengyu Ma, Advances in Neural Information Processing Systems. 332020NeurIPS'20Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y. Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. MOPO: model-based offline policy optimization. In Advances in Neural Information Processing Systems 33 (NeurIPS'20), 2020.</p>
<p>Offline model-based adaptable policy learning. Xiong-Hui Chen, Yang Yu, Qingyang Li, Fan-Ming Luo, Zhiwei Tony Qin, Shang Wenjie, Jieping Ye, Advances in Neural Information Processing Systems 34 (NeurIPS'21), Virtual Conference. Xiong-Hui Chen, Yang Yu, Qingyang Li, Fan-Ming Luo, Zhiwei Tony Qin, Shang Wenjie, and Jieping Ye. Offline model-based adaptable policy learning. In Advances in Neural Information Processing Systems 34 (NeurIPS'21), Virtual Conference, 2021.</p>
<p>Adapting environment sudden changes by learning context sensitive policy. Fan-Ming Luo, Shengyi Jiang, Yang Yu, Zongzhang Zhang, Yi-Feng Zhang, Proceedings of the 36th AAAI Conference on Artificial Intelligence (AAAI'22), Virtual Conference. the 36th AAAI Conference on Artificial Intelligence (AAAI'22), Virtual ConferenceFan-Ming Luo, Shengyi Jiang, Yang Yu, Zongzhang Zhang, and Yi-Feng Zhang. Adapting environment sudden changes by learning context sensitive policy. In Proceedings of the 36th AAAI Conference on Artificial Intelligence (AAAI'22), Virtual Conference, 2022.</p>
<p>Recurrent world models facilitate policy evolution. David Ha, Jürgen Schmidhuber, Advances in Neural Information Processing Systems 31 (NeurIPS'18). Montréal, CanadaDavid Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. In Advances in Neural Information Processing Systems 31 (NeurIPS'18), pages 2455-2467, Montréal, Canada, 2018.</p>
<p>Error bounds of imitating policies and environments. Tian Xu, Ziniu Li, Yang Yu, Advances in Neural Information Processing Systems 33 (NeurIPS'20). Tian Xu, Ziniu Li, and Yang Yu. Error bounds of imitating policies and environments. In Advances in Neural Information Processing Systems 33 (NeurIPS'20), pages 15737-15749, 2020.</p>
<p>Human causal transfer: Challenges for deep reinforcement learning. Mark Edmonds, James Kubricht, Colin Summers, Yixin Zhu, Brandon Rothrock, Song-Chun, Hongjing Zhu, Lu, Proceedings of the 40th Annual Meeting of the Cognitive Science Society (CogSci'18). the 40th Annual Meeting of the Cognitive Science Society (CogSci'18)Mark Edmonds, James Kubricht, Colin Summers, Yixin Zhu, Brandon Rothrock, Song-Chun Zhu, and Hongjing Lu. Human causal transfer: Challenges for deep reinforcement learning. In Proceedings of the 40th Annual Meeting of the Cognitive Science Society (CogSci'18), 2018.</p>
<p>Building machines that learn and think like people. Josh Tenenbaum, Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS'18). the 17th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS'18)5Josh Tenenbaum. Building machines that learn and think like people. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS'18), page 5, 2018.</p>
<p>A meta-transfer objective for learning to disentangle causal mechanisms. Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Nan Rosemary Ke, Sébastien Lachapelle, Olexa Bilaniuk, Anirudh Goyal, Christopher J Pal, Proceedings of the 8th International Conference on Learning Representations (ICLR'20). the 8th International Conference on Learning Representations (ICLR'20)2020Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Nan Rosemary Ke, Sébastien Lachapelle, Olexa Bilaniuk, Anirudh Goyal, and Christopher J. Pal. A meta-transfer objective for learning to disentangle causal mechanisms. In Proceedings of the 8th International Conference on Learning Representations (ICLR'20), 2020.</p>
<p>Causal confusion in imitation learning. Dinesh Pim De Haan, Sergey Jayaraman, Levine, Advances in Neural Information Processing Systems 32 (NeurIPS'19). Pim de Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning. In Advances in Neural Information Processing Systems 32 (NeurIPS'19), pages 11693-11704, 2019.</p>
<p>Invariant action effect model for reinforcement learning. Zheng-Mao Zhu, Shengyi Jiang, Yu-Ren Liu, Yang Yu, Kun Zhang, Proceedings of the 36th AAAI Conference on Artificial Intelligence (AAAI'22), Virtual Conference. the 36th AAAI Conference on Artificial Intelligence (AAAI'22), Virtual ConferenceZheng-Mao Zhu, Shengyi Jiang, Yu-Ren Liu, Yang Yu, and Kun Zhang. Invariant action effect model for reinforcement learning. In Proceedings of the 36th AAAI Conference on Artificial Intelligence (AAAI'22), Virtual Conference, 2022.</p>
<p>Causation, Prediction, and Search. Peter Spirtes, N Clark, Richard Glymour, Scheines, MIT PressPeter Spirtes, Clark N Glymour, and Richard Scheines. Causation, Prediction, and Search. MIT Press, 2000.</p>
<p>Kernel-based conditional independence test and application in causal discovery. Kun Zhang, Jonas Peters, Dominik Janzing, Bernhard Schölkopf, Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence (UAI'11). the 27th Conference on Uncertainty in Artificial Intelligence (UAI'11)Kun Zhang, Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Kernel-based conditional independence test and application in causal discovery. In Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence (UAI'11), pages 804-813, 2011.</p>
<p>Learning neural causal models from unknown interventions. Nan Rosemary Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, Hugo Larochelle, Chris Pal, Yoshua Bengio, CoRRNan Rosemary Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, Hugo Larochelle, Chris Pal, and Yoshua Bengio. Learning neural causal models from unknown interventions. CoRR, 1910.01075, 2019.</p>
<p>A kernel-based causal learning algorithm. Xiaohai Sun, Dominik Janzing, Bernhard Schölkopf, Kenji Fukumizu, Proceedings of the 24th International Conference on Machine Learning (ICML'07). the 24th International Conference on Machine Learning (ICML'07)Xiaohai Sun, Dominik Janzing, Bernhard Schölkopf, and Kenji Fukumizu. A kernel-based causal learning algorithm. In Proceedings of the 24th International Conference on Machine Learning (ICML'07), pages 855-862, 2007.</p>
<p>Distribution-free learning of bayesian network structure in continuous domains. Dimitris Margaritis, Proceedings of the 20th AAAI Conference on Artificial Intelligence (AAAI'05). the 20th AAAI Conference on Artificial Intelligence (AAAI'05)Dimitris Margaritis. Distribution-free learning of bayesian network structure in continuous domains. In Proceed- ings of the 20th AAAI Conference on Artificial Intelligence (AAAI'05), pages 825-830, 2005.</p>
<p>A markovian decision process. Richard Bellman, Journal of Mathematics and Mechanics. 65Richard Bellman. A markovian decision process. Journal of Mathematics and Mechanics, 6(5):679-684, 1957.</p>
<p>Information theoretic MPC for model-based reinforcement learning. Grady Williams, Nolan Wagener, Brian Goldfain, Paul Drews, James M Rehg, Byron Boots, Evangelos A Theodorou, Proceedings of the 2017 IEEE International Conference on Robotics and Automation (ICRA'17). the 2017 IEEE International Conference on Robotics and Automation (ICRA'17)Grady Williams, Nolan Wagener, Brian Goldfain, Paul Drews, James M. Rehg, Byron Boots, and Evangelos A. Theodorou. Information theoretic MPC for model-based reinforcement learning. In Proceedings of the 2017 IEEE International Conference on Robotics and Automation (ICRA'17), pages 1714-1721, 2017.</p>
<p>Model-ensemble trust-region policy optimization. Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, Pieter Abbeel, Proceedings of the 6th International Conference on Learning Representations, (ICLR'18). the 6th International Conference on Learning Representations, (ICLR'18)Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble trust-region policy optimization. In Proceedings of the 6th International Conference on Learning Representations, (ICLR'18), 2018.</p>
<p>Morel: Model-based offline reinforcement learning. Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, Thorsten Joachims, Advances in Neural Information Processing Systems 33 (NeurIPS'20). 2020Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-based offline reinforcement learning. In Advances in Neural Information Processing Systems 33 (NeurIPS'20), 2020.</p>
<p>Qiming Yuan, Wojciech Zaremba, and Lei Zhang. Solving rubik's cube with a robot hand. Ilge Openai, Marcin Akkaya, Maciek Andrychowicz, Mateusz Chociej, Bob Litwin, Arthur Mcgrew, Alex Petron, Matthias Paino, Glenn Plappert, Raphael Powell, Jonas Ribas, Schneider, 1910.07113Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian WengOpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, and Lei Zhang. Solving rubik's cube with a robot hand. CoRR, 1910.07113, 2019.</p>
<p>Ridge regression: Biased estimation for nonorthogonal problems. Arthur E Hoerl, Robert W Kennard, Technometrics. 421Arthur E. Hoerl and Robert W. Kennard. Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 42(1):80-86, 2000.</p>
<p>Judea Pearl, Models, reasoning and inference. Cambridge University Press19Judea Pearl et al. Models, reasoning and inference. Cambridge University Press, 19:2, 2000.</p>
<p>Probabilistic Graphical Models -Principles and Techniques. Daphne Koller, Nir Friedman, MIT PressDaphne Koller and Nir Friedman. Probabilistic Graphical Models -Principles and Techniques. MIT Press, 2009.</p>
<p>Mujoco: A physics engine for model-based control. Emanuel Todorov, Tom Erez, Yuval Tassa, Proceedings of the 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). the 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Proceedings of the 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 5026-5033, 2012.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7204 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7204</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7204</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-2334489ab0133a7e175c0a7d27d065de877eb10b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2334489ab0133a7e175c0a7d27d065de877eb10b" target="_blank">GTR-LSTM: A Triple Encoder for Sentence Generation from RDF Data</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a system to translate a set of RDF triples into natural sentences based on an encoder-decoder framework that encodes not only the elements of the triple but also the relationships both within a triple and between the triples.</p>
                <p><strong>Paper Abstract:</strong> A knowledge base is a large repository of facts that are mainly represented as RDF triples, each of which consists of a subject, a predicate (relationship), and an object. The RDF triple representation offers a simple interface for applications to access the facts. However, this representation is not in a natural language form, which is difficult for humans to understand. We address this problem by proposing a system to translate a set of RDF triples into natural sentences based on an encoder-decoder framework. To preserve as much information from RDF triples as possible, we propose a novel graph-based triple encoder. The proposed encoder encodes not only the elements of the triples but also the relationships both within a triple and between the triples. Experimental results show that the proposed encoder achieves a consistent improvement over the baseline models by up to 17.6%, 6.0%, and 16.4% in three common metrics BLEU, METEOR, and TER, respectively.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7204.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7204.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adapted BLSTM (RDF concat)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adapted standard BLSTM encoder with RDF triple concatenation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequence-to-sequence encoder that linearizes a set of RDF triples by concatenating the tokens of all triples into a single token sequence and feeds this sequence to a bidirectional LSTM encoder with attention and an LSTM decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>RDF triple sequence (concatenated tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each RDF triple's element tokens (subject, predicate, object) are concatenated into a single linear sequence of tokens; zero-padding is applied so inputs have fixed size. The resulting token sequence is treated as the encoder input in a standard seq2seq BLSTM+attention pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>simple concatenation of triple element tokens in given order (no explicit graph traversal beyond the input ordering); zero-padding for fixed length</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG (Train/Dev/Test Seen/Unseen), GKB</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation (RDF -> natural sentence)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Adapted standard BLSTM encoder + attention decoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Bidirectional LSTM encoder over the concatenated token sequence, attention mechanism (Bahdanau-style), and LSTM decoder; 512 hidden units, 300-d GloVe embeddings, trained with Adam (lr=0.0002).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, TER</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Entity Unmasking - BLEU: Seen 42.7 / Unseen 23.0 / GKB 28.0; METEOR: Seen 34.4 / Unseen 28.7 / GKB 27.5; TER: Seen 55.7 / Unseen 69.9 / GKB 67.7. Entity Masking - BLEU: Seen 49.8 / Unseen 28.0 / GKB 34.8; METEOR: Seen 38.3 / Unseen 29.4 / GKB 28.6; TER: Seen 49.9 / Unseen 64.9 / GKB 65.8.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Simple to train and fast relative to the graph encoder; benefits from entity masking (notably improved BLEU and smaller vocabulary).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Loses intra-triple and inter-triple structural relationships because it treats the graph as a flat token sequence; generates more relationship errors (32% relationship errors observed in manual inspection of BLSTM outputs on Seen/Unseen).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Outperformed by the graph-based GTR-LSTM in BLEU/METEOR/TER; better than some statistical baselines in earlier work but worse than encoders that explicitly model triple structure and cross-triple relations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7204.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7204.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adapted TLSTM (per-triple)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adapted standard triple encoder (per-triple LSTM aggregation) - TLSTM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder that first aggregates the tokens of each RDF triple into a per-triple vector via an LSTM, and then concatenates those per-triple vectors into a sequence that the decoder consumes (preserving intra-triple relations but with limited inter-triple modeling).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Per-triple vector concatenation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each RDF triple (subject/predicate/object token sequence) is encoded by an LSTM into a fixed-size vector; the per-triple vectors for all triples are concatenated into a single vector sequence which is fed to the decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hierarchical (triple-level then sequence), token-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Group tokens by triple, apply an LSTM per triple to compute f(t_i) vectors, then concatenate [f(t1); f(t2); ...; f(tn)]</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG (Seen/Unseen), GKB</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation (RDF -> natural sentence)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Adapted triple encoder (TLSTM) + LSTM decoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Per-triple LSTM encoders produce triple vectors which are concatenated into a global input vector for a decoder LSTM; shares hyperparameters with other models (512 hidden units, 300-d embeddings).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, TER</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Entity Unmasking - BLEU: Seen 45.9 / Unseen 28.1 / GKB 29.4; METEOR: Seen 34.9 / Unseen 30.1 / GKB 28.5; TER: Seen 50.5 / Unseen 62.7 / GKB 59.0. Entity Masking - BLEU: Seen 50.5 / Unseen 31.6 / GKB 36.7; METEOR: Seen 36.5 / Unseen 30.7 / GKB 30.1; TER: Seen 47.7 / Unseen 60.4 / GKB 57.2.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Preserves intra-triple relationships better than flat concatenation and yields improved metrics versus adapted BLSTM, especially with entity masking.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Does not effectively capture inter-triple structural relationships and global graph context (aggregation limited), leading to inferior performance compared to graph-structured encoders on multi-triple sets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Per-triple aggregation improves over token concatenation (adapted BLSTM) on seen metrics, but is outperformed by the GTR-LSTM which models cross-triple relations and global graph context.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7204.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7204.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GTR-LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GTR-LSTM: Graph-based triple encoder LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-structured triple encoder that represents input RDF triples as a directed graph of entities (vertices) and predicates (edges), traverses the graph using topological sort combined with breadth-first traversal, computes vertex hidden states via a specialized LSTM unit that consumes both entity and edge information, and applies attention over vertex states for decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph-based triple encoding (vertex LSTM states + attention)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The RDF triple set is treated as a directed graph G = (V,E) where vertices are entities/literals and edges are predicates; a traversal (topological sort then BFS for remaining SCCs) determines an order to compute per-vertex hidden states using a GTR-LSTM unit that takes both entity and incoming edge information; an attention mechanism aggregates vertex hidden states into decoder input.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>graph-structured, hierarchical/vertex-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Combined topological sort then breadth-first traversal; for SCCs BFS starts at a random vertex; at each visited vertex, a GTR-LSTM unit computes/updates the hidden state using inputs for entities and the predicates (edges) pointing to it; finally Luong-style attention over all vertex states produces h_T for decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG (Seen/Unseen), GKB</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation (RDF -> natural sentence)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GTR-LSTM encoder + attention + LSTM decoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Graph-aware encoder where each GTR-LSTM unit receives entity and relationship inputs (separate U^{*e} and W^{*e} parameter matrices, separate forget gate per input), followed by Luong-style attention to aggregate vertex states; decoder is a standard LSTM; 512 hidden units, 300-d embeddings; trained with Adam.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, TER; human evaluation: correctness, grammaticality, fluency</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Entity Unmasking - BLEU: Seen 54.0 / Unseen 29.2 / GKB 37.1; METEOR: Seen 37.3 / Unseen 27.8 / GKB 30.6; TER: Seen 45.3 / Unseen 59.8 / GKB 55.1. Entity Masking - BLEU: Seen 58.6 / Unseen 34.1 / GKB 40.1; METEOR: Seen 40.6 / Unseen 32.0 / GKB 34.6; TER: Seen 41.7 / Unseen 57.9 / GKB 50.6. Human eval (masked models) - correctness/grammar/fluency (Seen): 2.64/2.66/2.57 (scale 1-3).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Improves automatic metrics substantially vs baselines (up to +17.6% BLEU on WebNLG seen vs BLSTM); training is slower (up to ~2x slower than baselines) but model size is smaller (up to 59% fewer parameters), and entity masking further improves convergence and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Training slower than baselines (but within one day); non-determinism introduced by random starting vertex in SCC traversal; decoder LSTM sometimes produces duplicated sub-sentences (observed ~15% duplication for GTR-LSTM outputs); complexity increases relative to simple concatenation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Explicitly models intra- and inter-triple relationships and global graph context, outperforming adapted BLSTM, TLSTM, Neural Wikipedian and SMT baselines across BLEU/METEOR/TER and human judgments; differs from Graph LSTM/Tree LSTM by handling arbitrary (non-predefined) predicates and cycles via traversal + edge-aware LSTM inputs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7204.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7204.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entity Masking / De-lexicalization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entity masking (de-lexicalization) with entity type mapping and eids</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preprocessing representation that replaces entity mentions by entity identifiers (eids) and maps entities to general and specific entity types (e.g., PERSON, CITY, DATE) to improve generalization to unseen entities and reduce vocabulary size.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Entity-masked RDF+text (de-lexicalized representation)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Subjects and objects in both input triples and target sentences are replaced by eids and augmented with general (g) and specific (d) entity type labels (e.g., ENT-1 PERSON GOVERNOR); predicates (relations) keep their labels; entity mentions in text are found via exact, n-gram, and parse-tree matching and replaced with corresponding eids.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based, de-lexicalized, abstraction-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>String-matching (exact, n-gram, parse-tree) for entity detection; lookup via DBpedia API for types; replace entity labels with eids and include general/specific WordNet-based types; predicates preserved.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG, GKB</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation (RDF -> natural sentence) with generalization to unseen entities</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Preprocessing step used with all encoder variants (BLSTM, TLSTM, GTR-LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model but a preprocessing representation: entities replaced by eids and type tokens; enables smaller training vocabulary and more generalized patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, TER; entity-matching accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Entity masking accuracy: WebNLG 87.15% ; GKB 82.45%. Impact on models: e.g., GTR-LSTM BLEU improved from 54.0 to 58.6 on WebNLG Seen (+8.5% absolute), and similar improvements on Unseen and GKB.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Improves performance across all tested models (notably BLEU gains), reduces vocabulary size and running time, aids handling of unseen entities and limited data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Entity-masking requires accurate entity matching (matching accuracy <100%: 87.15% and 82.45% reported) and de-lexicalization may remove useful lexical distinctions if over-generalized; introduces an extra post-processing delexicalization step to restore surface forms.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Provides consistent gains when combined with any of the encoders; argued to be especially helpful when training data is limited vs using raw entity tokens.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7204.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7204.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural Wikipedian (TFF)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Wikipedian: Generating textual summaries from knowledge base triples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior neural system that encodes RDF triples using feedforward networks (per-triple) and concatenates them as the decoder input to generate summaries; limited to triple sets with a single entity point of view.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural wikipedian: Generating textual summaries from knowledge base triples</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Per-triple feedforward encoding + concatenation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each RDF triple is encoded via feedforward neural networks into a vector and the triple vectors are concatenated and used as input to an LSTM decoder to produce natural text; assumes a single entity viewpoint (entity of interest appears as subject or object in every triple).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hierarchical (per-triple vector), token-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Per-triple feedforward encoding followed by concatenation into decoder input (no graph traversal or explicit vertex-state propagation).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Mentioned in relation to WebNLG; original experiments in Vougiouklis et al. (2017)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation (knowledge base triples -> textual summary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Neural Wikipedian (feedforward triple encoders + LSTM decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Feedforward networks for triple encoding, concatenated vector input to LSTM decoder; designed for single-entity viewpoint RDF sets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Not reported in this paper (referenced prior work contains evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Mentioned as baseline; simpler encoder design but constrained by single-entity viewpoint assumption.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Restricted to sets of triples with a single entity viewpoint; uses standard feed-forward encoders which may not capture graph structure or cross-triple relations as effectively as graph-based encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>The paper reports that the proposed framework (with GTR-LSTM) outperforms Neural Wikipedian; Neural Wikipedian uses simpler feedforward triple encoders while GTR-LSTM models graph structure and inter-triple relations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7204.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7204.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph LSTM (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph LSTM (for image superpixel graphs) - Liang et al. 2016</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior graph-LSTM variant applied to image data that constructs a graph based on spatial relations among image super-pixels and defines a predefined set of relationships between vertices (top/bottom/left/right).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Semantic object parsing with graph lstm</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph LSTM spatial adjacency encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph constructed from super-pixels with edges defined by fixed spatial relationships; LSTM units propagate states along predefined relation types between graph nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>graph-structured, relation-predefined</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Graph built from image super-pixels via spatial adjacency; LSTM updates use predefined relation directions (top, bottom, left, right).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-structured representation for downstream tasks (image parsing); mentioned as related prior work</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Graph LSTM (Liang et al., 2016)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Graph LSTM where relationships between nodes are from a fixed set (spatial relations); used for semantic object parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Not evaluated in this paper; cited to contrast GTR-LSTM which accepts arbitrary predicate labels rather than a fixed relation set.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires a predefined set of relation types; not directly applicable to KBs with open-ended predicates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>GTR-LSTM differs by allowing arbitrary (non-predefined) relationships (treating predicate as input) and handling cycles via traversal, while Graph LSTM assumes predefined relations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7204.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7204.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree LSTM (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-structured LSTM (Tai et al., 2015)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tree-structured LSTM variant that composes representations according to a dependency tree, but it does not allow cycles and is designed for tree-structured inputs such as dependency parses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improved semantic representations from tree-structured long short-term memory networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Tree-LSTM over dependency trees</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes hierarchical tree structure (e.g., dependency trees) by propagating and composing LSTM states along tree edges; relies on acyclic tree structure.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hierarchical tree-structured</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Traverse dependency tree and apply Tree-LSTM composition functions to compute node states.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>sentence representation / structured input encoding; mentioned as related prior work</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Tree LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LSTM variant that composes inputs according to tree structures with separate forget gates per child.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Not evaluated here; cited to contrast with GTR-LSTM: Tree LSTM cannot handle cycles, whereas GTR-LSTM handles cycles by traversal.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Cannot handle cyclic graphs; limited to tree-structured inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>GTR-LSTM extends ideas of multiple forget gates per input but adds traversal and edge-aware inputs to handle cycles and arbitrary predicates in KB graphs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural wikipedian: Generating textual summaries from knowledge base triples <em>(Rating: 2)</em></li>
                <li>The webnlg challenge: Generating text from rdf data <em>(Rating: 2)</em></li>
                <li>Creating training corpora for nlg micro-planners <em>(Rating: 2)</em></li>
                <li>Neural text generation from structured data with application to the biography domain <em>(Rating: 1)</em></li>
                <li>Semantic object parsing with graph lstm <em>(Rating: 2)</em></li>
                <li>Improved semantic representations from tree-structured long short-term memory networks <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7204",
    "paper_id": "paper-2334489ab0133a7e175c0a7d27d065de877eb10b",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "Adapted BLSTM (RDF concat)",
            "name_full": "Adapted standard BLSTM encoder with RDF triple concatenation",
            "brief_description": "A sequence-to-sequence encoder that linearizes a set of RDF triples by concatenating the tokens of all triples into a single token sequence and feeds this sequence to a bidirectional LSTM encoder with attention and an LSTM decoder.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "RDF triple sequence (concatenated tokens)",
            "representation_description": "Each RDF triple's element tokens (subject, predicate, object) are concatenated into a single linear sequence of tokens; zero-padding is applied so inputs have fixed size. The resulting token sequence is treated as the encoder input in a standard seq2seq BLSTM+attention pipeline.",
            "representation_type": "sequential, token-based",
            "encoding_method": "simple concatenation of triple element tokens in given order (no explicit graph traversal beyond the input ordering); zero-padding for fixed length",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG (Train/Dev/Test Seen/Unseen), GKB",
            "task_name": "graph-to-text generation (RDF -&gt; natural sentence)",
            "model_name": "Adapted standard BLSTM encoder + attention decoder",
            "model_description": "Bidirectional LSTM encoder over the concatenated token sequence, attention mechanism (Bahdanau-style), and LSTM decoder; 512 hidden units, 300-d GloVe embeddings, trained with Adam (lr=0.0002).",
            "performance_metric": "BLEU, METEOR, TER",
            "performance_value": "Entity Unmasking - BLEU: Seen 42.7 / Unseen 23.0 / GKB 28.0; METEOR: Seen 34.4 / Unseen 28.7 / GKB 27.5; TER: Seen 55.7 / Unseen 69.9 / GKB 67.7. Entity Masking - BLEU: Seen 49.8 / Unseen 28.0 / GKB 34.8; METEOR: Seen 38.3 / Unseen 29.4 / GKB 28.6; TER: Seen 49.9 / Unseen 64.9 / GKB 65.8.",
            "impact_on_training": "Simple to train and fast relative to the graph encoder; benefits from entity masking (notably improved BLEU and smaller vocabulary).",
            "limitations": "Loses intra-triple and inter-triple structural relationships because it treats the graph as a flat token sequence; generates more relationship errors (32% relationship errors observed in manual inspection of BLSTM outputs on Seen/Unseen).",
            "comparison_with_other": "Outperformed by the graph-based GTR-LSTM in BLEU/METEOR/TER; better than some statistical baselines in earlier work but worse than encoders that explicitly model triple structure and cross-triple relations.",
            "uuid": "e7204.0"
        },
        {
            "name_short": "Adapted TLSTM (per-triple)",
            "name_full": "Adapted standard triple encoder (per-triple LSTM aggregation) - TLSTM",
            "brief_description": "An encoder that first aggregates the tokens of each RDF triple into a per-triple vector via an LSTM, and then concatenates those per-triple vectors into a sequence that the decoder consumes (preserving intra-triple relations but with limited inter-triple modeling).",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Per-triple vector concatenation",
            "representation_description": "Each RDF triple (subject/predicate/object token sequence) is encoded by an LSTM into a fixed-size vector; the per-triple vectors for all triples are concatenated into a single vector sequence which is fed to the decoder.",
            "representation_type": "hierarchical (triple-level then sequence), token-based",
            "encoding_method": "Group tokens by triple, apply an LSTM per triple to compute f(t_i) vectors, then concatenate [f(t1); f(t2); ...; f(tn)]",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG (Seen/Unseen), GKB",
            "task_name": "graph-to-text generation (RDF -&gt; natural sentence)",
            "model_name": "Adapted triple encoder (TLSTM) + LSTM decoder",
            "model_description": "Per-triple LSTM encoders produce triple vectors which are concatenated into a global input vector for a decoder LSTM; shares hyperparameters with other models (512 hidden units, 300-d embeddings).",
            "performance_metric": "BLEU, METEOR, TER",
            "performance_value": "Entity Unmasking - BLEU: Seen 45.9 / Unseen 28.1 / GKB 29.4; METEOR: Seen 34.9 / Unseen 30.1 / GKB 28.5; TER: Seen 50.5 / Unseen 62.7 / GKB 59.0. Entity Masking - BLEU: Seen 50.5 / Unseen 31.6 / GKB 36.7; METEOR: Seen 36.5 / Unseen 30.7 / GKB 30.1; TER: Seen 47.7 / Unseen 60.4 / GKB 57.2.",
            "impact_on_training": "Preserves intra-triple relationships better than flat concatenation and yields improved metrics versus adapted BLSTM, especially with entity masking.",
            "limitations": "Does not effectively capture inter-triple structural relationships and global graph context (aggregation limited), leading to inferior performance compared to graph-structured encoders on multi-triple sets.",
            "comparison_with_other": "Per-triple aggregation improves over token concatenation (adapted BLSTM) on seen metrics, but is outperformed by the GTR-LSTM which models cross-triple relations and global graph context.",
            "uuid": "e7204.1"
        },
        {
            "name_short": "GTR-LSTM",
            "name_full": "GTR-LSTM: Graph-based triple encoder LSTM",
            "brief_description": "A graph-structured triple encoder that represents input RDF triples as a directed graph of entities (vertices) and predicates (edges), traverses the graph using topological sort combined with breadth-first traversal, computes vertex hidden states via a specialized LSTM unit that consumes both entity and edge information, and applies attention over vertex states for decoding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Graph-based triple encoding (vertex LSTM states + attention)",
            "representation_description": "The RDF triple set is treated as a directed graph G = (V,E) where vertices are entities/literals and edges are predicates; a traversal (topological sort then BFS for remaining SCCs) determines an order to compute per-vertex hidden states using a GTR-LSTM unit that takes both entity and incoming edge information; an attention mechanism aggregates vertex hidden states into decoder input.",
            "representation_type": "graph-structured, hierarchical/vertex-based",
            "encoding_method": "Combined topological sort then breadth-first traversal; for SCCs BFS starts at a random vertex; at each visited vertex, a GTR-LSTM unit computes/updates the hidden state using inputs for entities and the predicates (edges) pointing to it; finally Luong-style attention over all vertex states produces h_T for decoding.",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "WebNLG (Seen/Unseen), GKB",
            "task_name": "graph-to-text generation (RDF -&gt; natural sentence)",
            "model_name": "GTR-LSTM encoder + attention + LSTM decoder",
            "model_description": "Graph-aware encoder where each GTR-LSTM unit receives entity and relationship inputs (separate U^{*e} and W^{*e} parameter matrices, separate forget gate per input), followed by Luong-style attention to aggregate vertex states; decoder is a standard LSTM; 512 hidden units, 300-d embeddings; trained with Adam.",
            "performance_metric": "BLEU, METEOR, TER; human evaluation: correctness, grammaticality, fluency",
            "performance_value": "Entity Unmasking - BLEU: Seen 54.0 / Unseen 29.2 / GKB 37.1; METEOR: Seen 37.3 / Unseen 27.8 / GKB 30.6; TER: Seen 45.3 / Unseen 59.8 / GKB 55.1. Entity Masking - BLEU: Seen 58.6 / Unseen 34.1 / GKB 40.1; METEOR: Seen 40.6 / Unseen 32.0 / GKB 34.6; TER: Seen 41.7 / Unseen 57.9 / GKB 50.6. Human eval (masked models) - correctness/grammar/fluency (Seen): 2.64/2.66/2.57 (scale 1-3).",
            "impact_on_training": "Improves automatic metrics substantially vs baselines (up to +17.6% BLEU on WebNLG seen vs BLSTM); training is slower (up to ~2x slower than baselines) but model size is smaller (up to 59% fewer parameters), and entity masking further improves convergence and performance.",
            "limitations": "Training slower than baselines (but within one day); non-determinism introduced by random starting vertex in SCC traversal; decoder LSTM sometimes produces duplicated sub-sentences (observed ~15% duplication for GTR-LSTM outputs); complexity increases relative to simple concatenation.",
            "comparison_with_other": "Explicitly models intra- and inter-triple relationships and global graph context, outperforming adapted BLSTM, TLSTM, Neural Wikipedian and SMT baselines across BLEU/METEOR/TER and human judgments; differs from Graph LSTM/Tree LSTM by handling arbitrary (non-predefined) predicates and cycles via traversal + edge-aware LSTM inputs.",
            "uuid": "e7204.2"
        },
        {
            "name_short": "Entity Masking / De-lexicalization",
            "name_full": "Entity masking (de-lexicalization) with entity type mapping and eids",
            "brief_description": "A preprocessing representation that replaces entity mentions by entity identifiers (eids) and maps entities to general and specific entity types (e.g., PERSON, CITY, DATE) to improve generalization to unseen entities and reduce vocabulary size.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Entity-masked RDF+text (de-lexicalized representation)",
            "representation_description": "Subjects and objects in both input triples and target sentences are replaced by eids and augmented with general (g) and specific (d) entity type labels (e.g., ENT-1 PERSON GOVERNOR); predicates (relations) keep their labels; entity mentions in text are found via exact, n-gram, and parse-tree matching and replaced with corresponding eids.",
            "representation_type": "token-based, de-lexicalized, abstraction-based",
            "encoding_method": "String-matching (exact, n-gram, parse-tree) for entity detection; lookup via DBpedia API for types; replace entity labels with eids and include general/specific WordNet-based types; predicates preserved.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG, GKB",
            "task_name": "graph-to-text generation (RDF -&gt; natural sentence) with generalization to unseen entities",
            "model_name": "Preprocessing step used with all encoder variants (BLSTM, TLSTM, GTR-LSTM)",
            "model_description": "Not a model but a preprocessing representation: entities replaced by eids and type tokens; enables smaller training vocabulary and more generalized patterns.",
            "performance_metric": "BLEU, METEOR, TER; entity-matching accuracy",
            "performance_value": "Entity masking accuracy: WebNLG 87.15% ; GKB 82.45%. Impact on models: e.g., GTR-LSTM BLEU improved from 54.0 to 58.6 on WebNLG Seen (+8.5% absolute), and similar improvements on Unseen and GKB.",
            "impact_on_training": "Improves performance across all tested models (notably BLEU gains), reduces vocabulary size and running time, aids handling of unseen entities and limited data.",
            "limitations": "Entity-masking requires accurate entity matching (matching accuracy &lt;100%: 87.15% and 82.45% reported) and de-lexicalization may remove useful lexical distinctions if over-generalized; introduces an extra post-processing delexicalization step to restore surface forms.",
            "comparison_with_other": "Provides consistent gains when combined with any of the encoders; argued to be especially helpful when training data is limited vs using raw entity tokens.",
            "uuid": "e7204.3"
        },
        {
            "name_short": "Neural Wikipedian (TFF)",
            "name_full": "Neural Wikipedian: Generating textual summaries from knowledge base triples",
            "brief_description": "A prior neural system that encodes RDF triples using feedforward networks (per-triple) and concatenates them as the decoder input to generate summaries; limited to triple sets with a single entity point of view.",
            "citation_title": "Neural wikipedian: Generating textual summaries from knowledge base triples",
            "mention_or_use": "mention",
            "representation_name": "Per-triple feedforward encoding + concatenation",
            "representation_description": "Each RDF triple is encoded via feedforward neural networks into a vector and the triple vectors are concatenated and used as input to an LSTM decoder to produce natural text; assumes a single entity viewpoint (entity of interest appears as subject or object in every triple).",
            "representation_type": "hierarchical (per-triple vector), token-based",
            "encoding_method": "Per-triple feedforward encoding followed by concatenation into decoder input (no graph traversal or explicit vertex-state propagation).",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Mentioned in relation to WebNLG; original experiments in Vougiouklis et al. (2017)",
            "task_name": "graph-to-text generation (knowledge base triples -&gt; textual summary)",
            "model_name": "Neural Wikipedian (feedforward triple encoders + LSTM decoder)",
            "model_description": "Feedforward networks for triple encoding, concatenated vector input to LSTM decoder; designed for single-entity viewpoint RDF sets.",
            "performance_metric": "Not reported in this paper (referenced prior work contains evaluations)",
            "performance_value": null,
            "impact_on_training": "Mentioned as baseline; simpler encoder design but constrained by single-entity viewpoint assumption.",
            "limitations": "Restricted to sets of triples with a single entity viewpoint; uses standard feed-forward encoders which may not capture graph structure or cross-triple relations as effectively as graph-based encoders.",
            "comparison_with_other": "The paper reports that the proposed framework (with GTR-LSTM) outperforms Neural Wikipedian; Neural Wikipedian uses simpler feedforward triple encoders while GTR-LSTM models graph structure and inter-triple relations.",
            "uuid": "e7204.4"
        },
        {
            "name_short": "Graph LSTM (mention)",
            "name_full": "Graph LSTM (for image superpixel graphs) - Liang et al. 2016",
            "brief_description": "A prior graph-LSTM variant applied to image data that constructs a graph based on spatial relations among image super-pixels and defines a predefined set of relationships between vertices (top/bottom/left/right).",
            "citation_title": "Semantic object parsing with graph lstm",
            "mention_or_use": "mention",
            "representation_name": "Graph LSTM spatial adjacency encoding",
            "representation_description": "Graph constructed from super-pixels with edges defined by fixed spatial relationships; LSTM units propagate states along predefined relation types between graph nodes.",
            "representation_type": "graph-structured, relation-predefined",
            "encoding_method": "Graph built from image super-pixels via spatial adjacency; LSTM updates use predefined relation directions (top, bottom, left, right).",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "graph-structured representation for downstream tasks (image parsing); mentioned as related prior work",
            "model_name": "Graph LSTM (Liang et al., 2016)",
            "model_description": "Graph LSTM where relationships between nodes are from a fixed set (spatial relations); used for semantic object parsing.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Not evaluated in this paper; cited to contrast GTR-LSTM which accepts arbitrary predicate labels rather than a fixed relation set.",
            "limitations": "Requires a predefined set of relation types; not directly applicable to KBs with open-ended predicates.",
            "comparison_with_other": "GTR-LSTM differs by allowing arbitrary (non-predefined) relationships (treating predicate as input) and handling cycles via traversal, while Graph LSTM assumes predefined relations.",
            "uuid": "e7204.5"
        },
        {
            "name_short": "Tree LSTM (mention)",
            "name_full": "Tree-structured LSTM (Tai et al., 2015)",
            "brief_description": "A tree-structured LSTM variant that composes representations according to a dependency tree, but it does not allow cycles and is designed for tree-structured inputs such as dependency parses.",
            "citation_title": "Improved semantic representations from tree-structured long short-term memory networks",
            "mention_or_use": "mention",
            "representation_name": "Tree-LSTM over dependency trees",
            "representation_description": "Encodes hierarchical tree structure (e.g., dependency trees) by propagating and composing LSTM states along tree edges; relies on acyclic tree structure.",
            "representation_type": "hierarchical tree-structured",
            "encoding_method": "Traverse dependency tree and apply Tree-LSTM composition functions to compute node states.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "sentence representation / structured input encoding; mentioned as related prior work",
            "model_name": "Tree LSTM",
            "model_description": "An LSTM variant that composes inputs according to tree structures with separate forget gates per child.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Not evaluated here; cited to contrast with GTR-LSTM: Tree LSTM cannot handle cycles, whereas GTR-LSTM handles cycles by traversal.",
            "limitations": "Cannot handle cyclic graphs; limited to tree-structured inputs.",
            "comparison_with_other": "GTR-LSTM extends ideas of multiple forget gates per input but adds traversal and edge-aware inputs to handle cycles and arbitrary predicates in KB graphs.",
            "uuid": "e7204.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural wikipedian: Generating textual summaries from knowledge base triples",
            "rating": 2
        },
        {
            "paper_title": "The webnlg challenge: Generating text from rdf data",
            "rating": 2
        },
        {
            "paper_title": "Creating training corpora for nlg micro-planners",
            "rating": 2
        },
        {
            "paper_title": "Neural text generation from structured data with application to the biography domain",
            "rating": 1
        },
        {
            "paper_title": "Semantic object parsing with graph lstm",
            "rating": 2
        },
        {
            "paper_title": "Improved semantic representations from tree-structured long short-term memory networks",
            "rating": 2
        }
    ],
    "cost": 0.01587375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>GTR-LSTM: A Triple Encoder for Sentence Generation from RDF Data</h1>
<p>Bayu Distiawan Trisedya ${ }^{1}$, Jianzhong Qi ${ }^{1}$, Rui Zhang ${ }^{1 *}$, Wei Wang ${ }^{2}$<br>${ }^{1}$ The University of Melbourne<br>${ }^{2}$ University of New South Wales<br>btrisedya@student.unimelb.edu.au<br>{jianzhong.qi, rui.zhang}@unimelb.edu.au<br>weiw@cse.unsw.edu.au</p>
<h4>Abstract</h4>
<p>A knowledge base is a large repository of facts that are mainly represented as RDF triples, each of which consists of a subject, a predicate (relationship), and an object. The RDF triple representation offers a simple interface for applications to access the facts. However, this representation is not in a natural language form, which is difficult for humans to understand. We address this problem by proposing a system to translate a set of RDF triples into natural sentences based on an encoder-decoder framework. To preserve as much information from RDF triples as possible, we propose a novel graph-based triple encoder. The proposed encoder encodes not only the elements of the triples but also the relationships both within a triple and between the triples. Experimental results show that the proposed encoder achieves a consistent improvement over the baseline models by up to $17.6 \%$, $6.0 \%$, and $16.4 \%$ in three common metrics BLEU, METEOR, and TER, respectively.</p>
<h2>1 Introduction</h2>
<p>Knowledge bases (KBs) are becoming an enabling resource for many applications including Q\&amp;A systems, recommender systems, and summarization tools. KBs are designed based on a W3C standard called the Resource Description Framework (RDF) ${ }^{1}$. An RDF triple consists of three elements in the form ofsubject, predicate (relationship), object. It describes a relationship between an entity (the subject) and another entity or literal (the object)</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>```
RDF
triples</p>
<div class="codehilite"><pre><span></span><code>
</code></pre></div>

<p>(John Doe,birth place, London)
(John Doe,birth date,1967-01-10)
(London,capital of,England)
```</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Target <br> sentence</th>
<th style="text-align: left;">John Doe was born on <br> 1967-01-10 in London, <br> the capital of England.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 1: RDF based sentence generation.
via the predicate. This representation allows easy data share between KBs. However, usually the elements of a triple are stored as Uniform Resource Identifiers (URIs), and many predicates (words or phrases) are not intuitive; this representation is difficult to comprehend by humans.</p>
<p>Translating RDF triples into natural sentences helps humans to comprehend the knowledge embedded in the triples, and building a natural language based user interface is an important task in user interaction studies (Damljanovic et al., 2010). This task has many applications, such as question answering (Bordes et al., 2014; Fader et al., 2014), profile summarizing (Lebret et al., 2016; Chisholm et al., 2017), and automatic weather forecasting (Mei et al., 2016). For example, the SPARQL inference of a Q\&amp;A system (Unger et al., 2012) returns a set of RDF triples which need to be translated into natural sentences to provide a more readable answer for the users. Table 1 illustrates such an example. Suppose a user is asking a question about "John Doe". By querying a KB, a Q\&amp;A system retrieves three triples "John Doe,birth place,London", "John Doe,birth date,1967-01-10", and "London, capital of,England." We aim to generate a natural sentence that incorporates the information of the triples and is easier to be understood by the user. In this example, the generated sentence is "John Doe was born on 1967-01-10 in London,</p>
<p>the capital of England."
Most existing studies for this task use domain specific rules. Bontcheva and Wilks (2004) create rules to generate sentences in the medical domain, while Cimiano et al. (2013) create rules to generate step by step cooking instructions. The problem of rule-based methods is that they need a lot of human efforts to create the rules, which mostly cannot deal with complex or novel cases.</p>
<p>Recent studies propose neural language generation systems. Lebret et al. (2016) generate the first sentence of a biography by a conditional neural language model. Mei et al. (2016) propose an encoder-aligner-decoder architecture to generate weather forecasts. The model does not need predefined rules and hence generalizes better to open domain data.</p>
<p>A straightforward adaptation of neural language generation system is to use the encoder-decoder model by first concatenating the elements of the RDF triples into a linear sequence and then feeding the sequence as the model input to learn the corresponding natural sentence. We implemented such a model (detailed in Section 3.2) that ranked top in the WebNLG Challenge $2017^{2}$. This Challenge has a primary objective of generating syntactically correct natural sentences from a set of RDF triples. Our model achieves the highest global scores on the automatic evaluation, outperforming competitors that use rule-based methods, statistical machine translation, and neural machine translation (Gardent et al., 2017b).</p>
<p>While our previous model achieves a good result, simply concatenating the elements in the RDF triples may lose the relationship between entities that affects the semantics of the resulting sentence (cf. Table 3). To address this issue, in this paper, we propose a novel graph-based triple encoder model that maintain the structure of RDF triples as a small knowledge graph named the GTR-LSTM model. This model computes the hidden state of each entity in a graph to preserve the relationships between entities in a triple (intra-triple relationships) and the relationships between entities in related triples (inter-triple relationships) that helps to achieve even more accurate sentences. This leads to two problems of preserving the relationships in a knowledge graph: (1) how to deal with a cycle in a knowledge graph; (2) how to deal with multiple non-predefined re-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>lationships between two entities in a knowledge graph. The proposed model differs from existing non-linear LSTM models such as Tree LSTM (Tai et al., 2015) and Graph LSTM (Liang et al., 2016) in addressing the mentioned problem. In particular, Tree LSTM does not allow cycles, while the proposed model handles cycles by first using a combination of topological sort and breadth-first traversal over a graph, and then using an attention model to capture the global information of the knowledge graph. Meanwhile, Graph LSTM only allows a predefined set of relationships between entities, while the proposed model allows any relationships by treating them as part of the input for the hidden state computation.</p>
<p>To further enhance the capability of our model to handle unseen entities, we propose to use entity masking, which maps the entities in the model training pairs to their types, e.g., we map an entity (literal) "1967-01-10" to a type symbol "DATE" in the training pairs. This way, our model can learn to handle any date entities rather than just "1967-01-10". This is particularly helpful when there is a limited training dataset.</p>
<p>Our contributions are:</p>
<ul>
<li>We propose an end-to-end encoder-decoder based framework for the problem of translating RDF triples into natural sentences.</li>
<li>We further propose a graph-based triple encoder to optimize the amount of information preserved in the input of the framework. The proposed model can handle cycles to capture the global information of a knowledge graph. The proposed model also handles nonpredefined relationships between entities.</li>
<li>We evaluate the proposed framework and model over two real datasets. The results show that our model outperforms the state-of-the-art models consistently.</li>
</ul>
<p>The rest of this paper is organized as follows. Section 2 summarizes previous studies on sentence generation. Section 3 details the proposed model. Section 4 presents the experimental results. Section 5 concludes the paper.</p>
<h2>2 Related Work</h2>
<p>The studied problem falls in the area of Natural Language Generation (NLG) (Reiter and Dale, 2000). Bontcheva and Wilks (2004) follow a</p>
<p>traditional NLG approach to generate sentences from RDF data in the medical domain. They start with filtering repetitive RDF data (document planning) and then group coherent triples (microplanning). After that, they aggregate the sentences generated for coherent triples to produce the final sentences (aggregation and realization). Cimiano et al. (2013) generate cooking recipes from semantic web data. They focus on using a large corpus to extract lexicon in the cooking domain. The lexicon is then used with a traditional NLG approach to generate cooking recipes. Duma and Klein (2013) learn a sentence template from a parallel RDF data and text corpora. They first align entities in RDF triples with entities mentioned in sentences. Then, they extract templates from the aligned sentences by replacing the entity mention with a unique token. This method works well on RDF triples in a seen domain but fails on RDF triples in a previously unseen domain.</p>
<p>Recently, several methods using neural networks are proposed. Lebret et al. (2016) generate the first sentence of a biography using a conditional neural language model. This model is trained to predict the next word of a sentence not only based on previous words, but also by using features captured from Wikipedia infoboxes. Mei et al. (2016) propose an encoder-aligner-decoder model to generate weather forecasts. The aligner is used to filter the most relevant data to be used to predict the weather forecast. Both studies experiment on cross-domain datasets. The result shows that the neural language generation approach is more flexible to work in an open domain since it is not limited to handcrafted rules. This motivates us to use a neural network based framework.</p>
<p>The most similar system to ours is Neural Wikipedian (Vougiouklis et al., 2017), which generates a summary from RDF triples. It uses feedforward neural networks to encode RDF triples and concatenate them as the input of the decoder. The decoder uses LSTM to predict a sequence of words as a summary. There are differences from our work. First, Neural Wikipedian only works with a set of RDF triples with a single entity point of view (i.e., the entity of interest must be in either the subject or the object of every triple). Our system does not have this constraint. Second, Neural Wikipedian uses standard feed-forward neural networks in the encoder. We design new triple encoder models to accommodate specific features of
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: RDF sentence generation based on an encoder-decoder architecture.
RDF triples. Experimental results show that our framework outperforms Neural Wikipedian.</p>
<h2>3 Proposed Model</h2>
<p>We start with the problem definition. We consider a set of RDF triples as the input, which is denoted by $T=\left[t_{1}, t_{2}, \ldots, t_{n}\right]$ where a triple $t_{i}$ consists of three elements (subject $s_{i}$, predicate $p_{i}$, and object $o_{i}$ ), $t_{i}=\left\langle s_{i}, p_{i}, o_{i}\right\rangle$. Every element can contain multiple words. We aim to generate a set of sentences that consist of a sequence of words $S=\left\langle w_{1}, w_{2}, \ldots, w_{m}\right\rangle$, such that the relationships in the input triples are correctly represented in $S$ while the sentences have a high quality. We use BLEU, METEOR, and TER to assess the quality of the sentence (detailed in Section 4). Table 1 illustrates our problem input and the target output.</p>
<p>This section is organized as follows. First we describe the overall framework (Section 3.1). Next, we describe three triple encoder models including the adapted standard BLSTM model (Section 3.2), the adapted standard triple encoder model (Section 3.3), and the proposed GTR-LSTM model (Section 3.4). The decoder which is used for all encoder models is described in Section 3.5. The entity masking is described in Section 3.6</p>
<h3>3.1 Solution Framework</h3>
<p>Our solution framework uses an encoder-decoder architecture as illustrated in Fig. 1. The framework</p>
<p>consists of three components including an RDF pre-processor, a target text pre-processor, and an encoder-decoder module.</p>
<p>The RDF pre-processor consists of an entity type mapper and a masking module. The entity type mapper maps the subjects and objects in the triples to their types, such that the sentence patterns learned are based on entity types rather than entities. For example, the input entities in Table 1, "John Doe", "London", "England", and "1967-01-10" can be mapped to "PERSON", "CITY", "COUNTRY", and "DATE", respectively. The mapping has been shown in our experiments to be highly effective in improving the model output quality. The masking module converts each entity into an entity identifier (eid). The target text pre-processor consists of a text normalizer and a de-lexicalizer. The text normalizer converts abbreviations and dates into the same format as the corresponding entities in the triples. The de-lexicalizer replaces all entities in the target sentences by their eids. The RDF and target text pre-processors are detailed in Section 3.6. The replaced target sentences are combined with the original target sentences and the English Wikipedia articles is used as a corpus to learn the word embeddings of the vocabulary.</p>
<p>To accommodate the RDF data, in the encoder side, we consider three triple encoder models: (1) the adapted standard BLSTM encoder; (2) the adapted standard triple encoder; and (3) the proposed GTR-LSTM triple encoder. The adapted standard BLSTM encoder concatenates the tokens in RDF triples as an input sequence, while the standard triple encoder first encodes each RDF triple into a vector representation and then concatenates the vectors of different triples. The latter model better captures intra-triple relationships but suffers in capturing inter-triple relationships. Considering the native representation of RDF triples as a small knowledge graph, our graph-based GTRLSTM triple encoder captures both intra-triple and inter-triple entity relationships.</p>
<h3>3.2 Adapted Standard BLSTM Encoder</h3>
<p>The standard encoder-decoder model with a BLSTM encoder is a sequence to sequence learning model (Cho et al., 2014). To adapt such a model for our problem, we transform a set of RDF triples input $T$ into a sequence of elements (i.e., $T=\left[w_{1,1}, w_{1,2}, \ldots, w_{1, j}, \ldots, w_{n, j}\right]$ ), where $w_{n, j}$ is
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: LSTM-based standard triple encoder.
the word embedding of a word in the $n$-th triple. For example, following the triples in Table 1, $w_{1,1}$ is the word embedding of "John", $w_{1,2}$ is the word embedding of "Doe", etc. This sequence forms an input for the encoder. We use zero padding to ensure that each input has the same representation size. The rest of the model is the same as the standard encoder-decoder model with an attention mechanism (Bahdanau et al., 2015). We call this model the adapted standard BLSTM encoder.</p>
<h3>3.3 Adapted Standard Triple Encoder</h3>
<p>The standard BLSTM encoder suffers in capturing the element relationships as the elements are simply concatenated together. Next, we adapt the standard BLSTM encoder to aggregate the word embeddings of the elements of the same triple to retain the intra-triple relationship. We call this the adapted standard triple encoder.</p>
<p>The adaptation is done by grouping the elements of each triple, so the input is represented as $T=\left[\left\langle w_{1,1}, \ldots, w_{1, j}\right\rangle, \ldots,\left\langle w_{n, 1}, \ldots w_{n, j}\right\rangle\right]$, where $w_{n, j}$ is the word embedding of a word in the $n$-th triple. We use zero padding to ensure that each triple has the same representation size. An LSTM network of the encoder computes a hidden state of each triple and concatenates them together to be the input for the decoder:</p>
<p>$$
h_{T}=\left[f\left(t_{1}\right) ; f\left(t_{2}\right) ; \ldots ; f\left(t_{n}\right)\right]
$$</p>
<p>where $h_{T}$ is the input vector representation for the decoder and $f$ is an LSTM network (cf. Fig. 2).</p>
<h3>3.4 GTR-LSTM Triple Encoder</h3>
<p>The adapted standard triple encoder has an advantage in preserving the intra-triple relationship. However, it has not considered the structural rela-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: A small knowledge graph formed by a set of RDF triples.
tionships between the entities in different triples. To overcome this limitation, we propose a graphbased triple encoder. We call it the GTR-LSTM triple encoder. This encoder takes the input triples in the form of a graph, which preserves the natural structure of the triples (cf. Fig. 3).</p>
<p>GTR-LSTM differs from existing Graph LSTM (Liang et al., 2016) and Tree LSTM (Tai et al., 2015) models in the following aspects. Graph LSTM is proposed for image data. It constructs the graph based on the spatial relationships among super-pixels of an image. Tree LSTM uses the dependency tree as the structure of a sentence. Both models have a predefined relationship between the vertices (Graph LSTM uses spatial relationships: top, bottom, left, or right between super-pixels; Tree LSTM uses dependencies between words in a sentence as the relationship). In contrast, a KB has an open set of relationships between the vertices (i.e., the predicate defines the relationship between entities/vertices) which make our problem more difficult to model.</p>
<p>Our GTR-LSTM triple encoder overcomes the difficulty as follows. It receives a directed graph $G=\langle V, E\rangle$ as the input, where $V$ is a set of vertices that represent entities or literals, and $E$ is a set of directed edges that represent predicates. Since the graph can contain cycles, we use a combination of topological sort and breadth-first traversal algorithms to traverse the graph. The traversal is used to create an ordering of feeding the vertices into a GTR-LSTM unit to compute their hidden states. We start with running a topological sort to establish an order of the vertices until no further vertex has a zero in-degree. For the remaining vertices, they must be in strongly connected component(s). Then, we run a breadthfirst traversal over the remaining vertices with a random starting vertex, since every vertex can be reached from all vertices of a strongly connected component. When a vertex $v_{i}$ is visited, the hidden states of all adjacent vertices of $v_{i}$ are computed (or updated if the hidden state of the vertex
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: GTR-LSTM triple encoder.
is already computed in the previous step).
Following the graph in Fig. 3, the order of hidden state computation is as follows. The process starts with a vertex with zero in-degree. Because there is no such vertex, a vertex is randomly selected as the starting vertex. Assume we pick "John" as the starting vertex, then we compute $h_{\text {john }}$ using $h_{0}$ as the previous hidden state. Next, following the breadth-first traversal algorithm, we visit vertex "John" and compute $h_{\text {mary }}$ and $h_{\text {london }}$ by passing $h_{\text {john }}$ as the previous hidden state. Next step, vertex "Mary" is visited, but no hidden states are computed or updated since it does not have any adjacent vertices. In the last step, vertex "England" is visited and $h_{\text {john }}$ is updated. Fig. 4 illustrates the overall process.</p>
<p>Different from the Graph LSTM, our GTRLSTM model computes a hidden state by taking into account the processed entity and its edge (the edge pointing to the current entity from the previous entity) to handle non-predefined relationships (any relationships between entities in a knowledge graph). Thus, our GTR-LSTM unit (cf. Fig. 4) receives two inputs, i.e., the entity and its relationship. We propose the following model to compute the hidden state of each GTR-LSTM unit.</p>
<p>$$
\begin{aligned}
i_{t} &amp; =\sigma\left(\sum_{e}\left(U^{i e} x_{t e}+W^{i e} h_{t-1}\right)\right) \
f_{t e} &amp; =\sigma\left(U^{f} x_{t e}+W^{f} h_{t-1}\right) \
o_{t} &amp; =\sigma\left(\sum_{e}\left(U^{o e} x_{t e}+W^{o e} h_{t-1}\right)\right) \
g_{t} &amp; =\tanh \left(\sum_{e}\left(U^{g e} x_{t e}+W^{g e} h_{t-1}\right)\right) \
c_{t} &amp; =\left(c_{t-1} * \sum_{e} f_{t e}\right)+\left(g_{t} * i_{t}\right) \
h_{t} &amp; =\tanh \left(c_{t}\right) * o_{t}
\end{aligned}
$$</p>
<p>Here, $U$ and $W$ are learned parameter matrices, $\sigma$ denotes the sigmoid function, $*$ denotes element-</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Attention model of GTR-LSTM.
wise multiplication, and $x$ is the input at the current time-step. The input gate $i$ determines the weight of the current input. The forget gate $f$ determines the weight of the previous state. The output gate $o$ determines the weight of the cell state forwarded to the next time-step. The state $g$ is the candidate hidden state used to compute the internal memory unit $c$ based on the current input and the previous state. The subscript $t$ is the timestep. The subscript/superscript $e$ is the input element (an entity or a predicate). Following Tree LSTM (Tai et al., 2015) and Graph LSTM (Liang et al., 2016), we also use a separate forget gate for each input that allows the GTR-LSTM unit to incorporate information from each input selectively.</p>
<p>From Fig. 4, we can see that the traversal creates two branches, one ended in $h_{\text {mary }}$ and the other ended in $h_{\text {john }}^{\prime}$. After the encoder computes the hidden states of each vertex, $h_{\text {john }}^{\prime}$ does not include the information of $h_{\text {mary }}$ and vice versa. Moreover, the graph can contain cycles that cause difficulty in determining the starting and ending vertices. Our traversal procedure ensures that the hidden states of all vertices are updated based on their adjacent vertices (local neighbors). To further capture the global information of the graph, we apply an attention model on the GTR-LSTM triple encoder. The attention model takes the hidden states of all vertices computed by the encoder and the previous hidden state of the decoder to compute the final input vector of each decoder time-step. Figure 5 illustrates the attention model of GTR-LSTM. Inspired by Luong et al. (2015), we adapt the following equation to compute the weight of each vertex.</p>
<p>$$
\alpha_{n}=\frac{\exp \left(h^{d}{ }<em n="n">{t}^{T} W x</em>}\right)}{\sum_{j=1}^{|X|} \exp \left(h^{d}{ <em j="j">{t}^{T} W x</em>
$$}\right)</p>
<p>Here, $h^{d}{ }<em n="n">{t}$ is the previous hidden state of the decoder, $|X|$ is the total number of entities in the triples, $W$ is a learned parameter matrix, $x</em>\right}$ is the weight vector of all vertices. Then the input of the decoder for each timestep can be computed as follows.}$ and $x_{j}$ are hidden states of vertices, and $\alpha=$ $\left{\alpha_{1}, \alpha_{2}, \ldots, \alpha_{n</p>
<p>$$
h_{T}=\sum_{n=1}^{|X|} \alpha_{n} x_{n}
$$</p>
<h3>3.5 Decoder</h3>
<p>The decoder of the proposed framework is a standard LSTM. It is trained to generate the output sequence by predicting the next output word $w_{t}$ conditioned on the hidden state $h^{d}{ }<em t="t">{t}$. The current hidden state $h^{d}{ }</em>}$ is conditioned on the hidden state of the previous time-step $h^{d}{ <em t-1="t-1">{t-1}$, the output of the previous time-step $w</em>$. The hidden state and the output of the decoder at time-step $t$ are computed as:}$, and input vector representation $h_{T</p>
<p>$$
\begin{aligned}
h^{d}{ }<em t-1="t-1">{t} &amp; =f\left(h^{d}{ }</em>\right) \
w_{t} &amp; =\operatorname{softmax}\left(V h_{t}\right)
\end{aligned}
$$}, w_{t-1}, h_{T</p>
<p>Here, $f$ is a single LSTM unit, and $V$ is the hidden-to-output weight matrix. The encoder and the decoder are trained to maximize the conditional log-likelihood:</p>
<p>$$
p\left(S_{n} \mid T_{n}\right)=\sum_{t=1}^{\left|S_{n}\right|} \log w_{t}
$$</p>
<p>Hence, the training objective is to minimize the negative conditional log-likelihood:</p>
<p>$$
J=\sum_{n=1}^{N}-\log p\left(S_{n} \mid T_{n}\right)
$$</p>
<p>where $\left(S_{n}, T_{n}\right)$ is a pair of output word sequence and input RDF triple set given for the training.</p>
<h3>3.6 Entity Masking</h3>
<p>Entity masking makes our framework generalizes better to unseen entities. This technique addresses the problem of a limited training set which is faced by many NLG problems.</p>
<p>Entity masking replaces entity mentions with eids and entity types in both the input triples and the target sentences. However, we do not want our model to be overly generalized either. Thus, we need to have general and specific entity types. For example, the entity "John Doe" is replaced by "ENT-1 PERSON GOVERNOR". To add the entity types, we use the DBpedia lookup API. The</p>
<p>API returns several entity types. The general and specific entity types are defined by the level of the word in the WordNet (Fellbaum, 1998) hierarchy.</p>
<p>In the encoder side, each element of the triple $t_{n}=\left\langle s_{n}, p_{n}, o_{n}\right\rangle$ is transformed into $s_{n}=\left\langle l_{s_{n}}, g_{s_{n}}, d_{s_{n}}\right\rangle, p_{n}=\left\langle l_{p_{n}}\right\rangle$, and $o_{n}=$ $\left\langle l_{o_{n}}, g_{o_{n}}, d_{o_{n}}\right\rangle$, where $l$ is the label of an element, $g$ is the general entity type, and $d$ is the specific entity type. The labels of the subject and the object are latter replaced by eids, while the label of the predicate is preserved, since it indicates the relationship between the subject and the object.</p>
<p>On the decoder side, the entities in the target text are also replaced by their corresponding eids. Entity matching is beyond the scope of our study. We simply use a combination of three string matching methods to find entity mentions in the sentence: exact matching, $n$-gram matching, and parse tree matching. The exact matching is used to find the exact mention; the $n$-gram matching is used to handle partial matching with the same token length; and parse tree matching is used to find a partial matching with different token length.</p>
<h2>4 Experiments</h2>
<p>We evaluate our framework on two datasets. The first is the dataset from Gardent et al. (2017a). We call it the WebNLG dataset. This dataset contains 25,298 RDF triple set-text pairs, with 9,674 unique sets of RDF triples. The dataset consists of a Train+Dev dataset and a Test Unseen dataset. We split Train+Dev into a training set ( $80 \%$ ), a development set ( $10 \%$ ), and a Seen testing set ( $10 \%$ ). The Train+Dev dataset contains RDF triples in ten categories (topics, e.g., astronaut, monument, food, etc.), while the Test Unseen dataset has five other unseen categories. The maximum number of triples in each RDF triple set is seven. For the second dataset, we collected data from Wikipedia pages regarding landmarks. We call it the $G K B$ dataset. We first extract RDF triples from Wikipedia infoboxes and sentences from the Wikipedia text that contain entities mentioned in the RDF triples. Human annotators then filter out false matches to obtain 1,000 RDF triple set-text pairs. This dataset is split into the training and development set ( $80 \%$ ) and the testing set (20\%). Table 1 illustrates an example of the data pairs of WebNLG and GKB dataset.</p>
<p>We implement the existing models, the adapted
model, and the proposed model using Keras ${ }^{3}$. We use three common evaluation metrics including BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006). For the metric computation and significance testing, we use MultEval (Clark et al., 2011).</p>
<h3>4.1 Tested Models</h3>
<p>We compare our proposed graph-based triple encoder (GTR-LSTM, Section 3.4) with three existing model including the adapted standard BLSTM encoder (BLSTM, Section 3.2), Neural Wikipedian (Vougiouklis et al., 2017) (TFF), and statistical machine translation (Hoang and Koehn, 2008) (SMT) trained on a 6-gram language model. We also compare with the adapted standard triple encoder (TLSTM, Section 3.3).</p>
<h3>4.2 Hyperparameters</h3>
<p>We use grid search to find the best hyperparameters for the neural networks. We use GloVe (Pennington et al., 2014) trained on the GKB and WebNLG training data and full English Wikipedia data dump to get 300-dimension word embeddings. We use 512 hidden units for both encoder and decoder. We use a 0.5 dropout rate for regularization on both encoder and decoder to avoid overfitting. We train our model on NVIDIA Tesla K40c. We find that using adaptive learning rates for the optimization is efficient and leads the model to converge faster. Thus, we use Adam (Kingma and Ba, 2015) with a learning rate of 0.0002 instead of stochastic gradient descent. The update of parameters in training is computed using a mini batch of 64 instances. We further apply early stopping to detect the convergence.</p>
<h3>4.3 Effect of Entity Masking</h3>
<p>Table 2 shows the overall comparison of model performance. It shows that entity masking gives a consistent performance improvement for all models. Generalizing the input triples and target sentences helps the models to learn the relationships between entities from their types. This is particularly helpful when there is limited training data. We use a combination of exact matching, $n$-gram matching and parse tree matching to find the entity mentions in the sentence. The entity masking accuracy for WebNLG dataset is $87.15 \%$, while for</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metric/Dataset</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">METEOR $\uparrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">TER $\downarrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">GKB</td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">GKB</td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">GKB</td>
</tr>
<tr>
<td style="text-align: center;">Entity Unmasking</td>
<td style="text-align: center;">Existing models</td>
<td style="text-align: center;">BLSTM</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">67.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">SMT</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">63.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TFF</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">60.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Adapted model</td>
<td style="text-align: center;">TLSTM</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">34.9</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">59.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Our proposed</td>
<td style="text-align: center;">GTR-LSTM</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">55.1</td>
</tr>
<tr>
<td style="text-align: center;">Entity Masking</td>
<td style="text-align: center;">Existing models</td>
<td style="text-align: center;">BLSTM</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">65.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">SMT</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">67.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TFF</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">58.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Adapted Model</td>
<td style="text-align: center;">TLSTM</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">47.7</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">57.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Our proposed</td>
<td style="text-align: center;">GTR-LSTM</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">57.9</td>
<td style="text-align: center;">50.6</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of model performance.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">RDF inputs</th>
<th style="text-align: center;">(Elizabeth Tower, location, London), (Wembley Stadium, location, London), <br> (London, capital of, England), (Theresa May, prime minister, England)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">london, england is home to wembley stadium and the elizabeth tower. <br> the name of the leader in england is theresa may.</td>
</tr>
<tr>
<td style="text-align: center;">BLSTM</td>
<td style="text-align: center;">england is lead by theresa may and is located in the city of london . <br> the elizabeth tower is located in the city of england and is located in <br> the wembley stadium.</td>
</tr>
<tr>
<td style="text-align: center;">SMT</td>
<td style="text-align: center;">wembley stadium is located in london, elizabeth tower . theresa may <br> is the leader of england, england.</td>
</tr>
<tr>
<td style="text-align: center;">TFF</td>
<td style="text-align: center;">the elizabeth tower is located in london, england, where wembley <br> stadium is the leader and theresa may is the leader.</td>
</tr>
<tr>
<td style="text-align: center;">TLSTM</td>
<td style="text-align: center;">the wembley stadium is located in london, england . the country is <br> the location of elizabeth tower . theresa may is the leader of london.</td>
</tr>
<tr>
<td style="text-align: center;">GTR-LSTM</td>
<td style="text-align: center;">the wembley stadium and elizabeth tower are both located in london, <br> england . theresa may is the leader of england.</td>
</tr>
</tbody>
</table>
<p>Table 3: Sample output of the system. The error is highlighted in bold.
the GKB dataset is $82.45 \%$.
Entity masking improves the BLEU score of the proposed GTR-LSTM model by $8.5 \%$ (from 54.0 on the Entity Unmasking model to 58.6 on the Entity Masking model), $16.7 \%$, and $8.0 \%$ on the WebNLG seen testing data (denoted by "Seen"), WebNLG unseen testing data (denoted by "Unseen"), and the GKB testing data (denoted by "GKB"). Using the entity masking not only improves the performance by recognizing the unknown vocabulary via eid masking but also improves the running time performance by requiring a smaller training vocabulary.</p>
<h3>4.4 Effect of Models</h3>
<p>Table 2 also shows that the proposed GTR-LSTM triple encoder achieves a consistent improvement over the baseline models, and the improvement is statistically significant, with $p&lt;0.01$ based on the t-test of all metrics. We use MultEval to compute the $p$ value based on an approximate randomization (Clark et al., 2011). The improvement on the BLEU score indicates that the model reduces the errors in the generated sentence. Our manual inspection confirms this result. The better (lower) TER score suggests that the model generates a more compact output (i.e., better aggregation).</p>
<p>Table 3 shows a sample output of all models. From this table, we can see that all baseline models produce sentences that contain wrong relationships between entities (e.g., the BLSTM output contains a wrong relationship "the elizabeth tower is located in the city of england"). Moreover, the baseline models generate sentences with a weak aggregation (e.g., "Elizabeth Tower" and "Wembley Stadium" are in separate sentences for TLSTM). The proposed GTR-LSTM model successfully avoids these problems.</p>
<p>Model training time. GTR-LSM is slower in training than the baseline models, which is expected as it needs to encode more information. However, its training time is no more than twice as that of any baseline models tested, and the training can complete within one day which seems reasonable. Meanwhile, the number of parameters trained for GTR-LSTM is up to $59 \%$ smaller than those of the baseline models, which saves the space cost for model storage.</p>
<h3>4.5 Human Evaluation</h3>
<p>To complement the automatic evaluation, we conduct human evaluations for all of the masked models. We ask five human annotators. Each of them</p>
<p>| Dataset/Metric | | Seen | | | Unseen | | | GKB | | | | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | | | Correctness | Grammar | Fluency | Correctness | Grammar | Fluency | Correctness | Grammar | Fluency | | Existing Models | BLSTM | 2.25 | 2.33 | 2.29 | 1.53 | 1.71 | 1.68 | 1.54 | 1.84 | 1.84 | | | SMT | 2.03 | 2.11 | 2.07 | 1.36 | 1.48 | 1.44 | 1.81 | 1.99 | 1.89 | | | TFF | 1.77 | 1.91 | 1.88 | 1.44 | 1.69 | 1.66 | 1.71 | 1.99 | 1.96 | | Adapted Model | TLSTM | 2.53 | 2.61 | 2.55 | 1.75 | 1.93 | 1.86 | 2.21 | 2.38 | 2.35 | | Our Proposed | GTR-LSTM | 2.64 | 2.66 | 2.57 | 1.96 | 2.04 | 1.99 | 2.29 | 2.42 | 2.41 |</p>
<p>Table 4: Human evaluation results.
has studied English for at least ten years and completed education in a full English environment for at least two years. We provide a website ${ }^{4}$ that shows them the RDF triples and the generated text. The annotators are given training on the scoring criteria. We also provide scoring examples. We randomly selected 100 sets of triples along with the output of each model. We only select sets of triples that contain more than two triples. Following (Gardent et al., 2017b), we use three evaluation metrics including correctness, grammaticality, and fluency. For each pair of triple set and generated sentences, the annotators are asked to give a score between one to three for each metric.</p>
<p>Correctness is used to measure the semantics of the output sentence. A score of 3 is given to generated sentences that contain no errors in the relationships between entities; a score of 2 is given to generated sentences that contain one error in the relationship; and a score of 1 is given to generated sentences that contain more than one errors in the relationships. Grammaticality is used to rate the grammatical and spelling errors of the generated sentences. Similar to the correctness metric, a score of 3 is given to generated sentences with no grammatical and spelling errors; a score of 2 is given to generated sentences with one error; and a score of 1 for the others. The last metric, fluency, is used to measure the fluency of the sentence output. We ask the annotators to give a score based on the aggregation of the sentences and the existence of sentence repetition. Table 4 shows the results of the human evaluations. The results confirm the automatic evaluation in which our proposed model achieves the best scores.</p>
<p>Error analysis. We further perform a manual inspection of 100 randomly selected output sentences of GTR-LSTM and BLSTM on the Seen and Unseen test data. We find that $32 \%$ of BLSTM output contains wrong relationships between entities. In comparison, only $8 \%$ of GTR-LSTM output contains such errors. Besides, we find duplicate sub-sentences in</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the output of GTR-LSTM (15\%). The following output is an example: "beef kway teow is a dish from singapore, where english language is spoken and the leader is tony tan. the leader of singapore is tony tan." While the duplicate sentence is not wrong, it affects the reading experience. We conjecture that the LSTM in the decoder caused such an issue. We aim to solve this problem in future work.</p>
<h2>5 Conclusions</h2>
<p>We proposed a novel graph-based triple encoder GTR-LSTM for sentence generation from RDF data. The proposed model maintains the structure of input RDF triples as a small knowledge graph to optimize the amount of information preserved in the input of the model. The proposed model can handle cycles to capture the global information of a knowledge graph and also handle non-predefined relationships between entities of a knowledge graph.</p>
<p>Our experiments show that GTR-LSTM offers a better performance than all the competitors. On the WebNLG dataset, our model outperforms the best existing model, the standard BLSTM model, by up to $17.6 \%, 6.0 \%$, and $16.4 \%$ in terms of BLEU, METEOR, and TER scores, respectively. On the GKB dataset, our model outperforms the standard BLSTM model by up to $15.2 \%, 20.9 \%$, and $23.1 \%$ in these three metrics, respectively.</p>
<h2>Acknowledgments</h2>
<p>Bayu Distiawan Trisedya is supported by the Indonesian Endowment Fund for Education (LPDP). This work is supported by Australian Research Council (ARC) Discovery Project DP180102050 and Future Fellowships Project FT120100832, and Google Faculty Research Award. This work is partly done while Jianzhong Qi is visiting the University of New South Wales. Wei Wang was partially supported by D2DCRC DC25002, DC25003, ARC DP 170103710 and 180103411.</p>
<h2>References</h2>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations (ICLR). http://arxiv.org/abs/1409.0473.</p>
<p>Kalina Bontcheva and Yorick Wilks. 2004. Automatic Report Generation from Ontologies: The MIAKT Approach, Springer, Berlin, Heidelberg, pages 324-335. https://doi.org/10.1007/978-3-540-27779-8_28.</p>
<p>Antoine Bordes, Sumit Chopra, and Jason Weston. 2014. Question answering with subgraph embeddings. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 615-620. https://www.aclweb.org/anthology/D/D14/D141067.pdf.</p>
<p>Andrew Chisholm, Will Radford, and Ben Hachey. 2017. Learning to generate one-sentence biographies from wikidata. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL). pages 633-642. http://aclweb.org/anthology/E17-1060.</p>
<p>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 17241734. http://www.aclweb.org/anthology/D14-1179.</p>
<p>Philipp Cimiano, Janna Lker, David Nagel, and Christina Unger. 2013. Exploiting ontology lexica for generating natural language texts from rdf data. In Proceedings of the 14th European Workshop on Natural Language Generation (ENLG). pages 1019. http://www.aclweb.org/anthology/W13-2102.</p>
<p>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT). pages 176-181. http://www.aclweb.org/anthology/P11-2031.</p>
<p>Danica Damljanovic, Milan Agatonovic, and Hamish Cunningham. 2010. Natural language interfaces to ontologies: combining syntactic analysis and ontology-based lookup through the user interaction. In Proceedings of the 7th International Conference on The Semantic Web (ISWC). pages 106-120. https://doi.org/10.1007/978-3-642-13486-9_8.</p>
<p>Michael J. Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems.</p>
<p>In Proceedings of the Sixth Workshop on Statistical Machine Translation (WMT). pages 85-91. http://aclweb.org/anthology/W11-2107.</p>
<p>Daniel Duma and Ewan Klein. 2013. Generating natural language from linked data: Unsupervised template extraction. In Proceedings of the 10th International Conference on Computational Semantics (IWCS). pages 83-94. http://www.aclweb.org/anthology/W13-0108.</p>
<p>Anthony Fader, Luke Zettlemoyer, and Oren Etzioni. 2014. Open question answering over curated and extracted knowledge bases. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD). pages 1156-1165. https://doi.org/10.1145/2623330.2623677.</p>
<p>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press. http://aclweb.org/anthology/J99-2008.</p>
<p>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017a. Creating training corpora for nlg micro-planners. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL). pages 179-188. http://aclweb.org/anthology/P17-1017.</p>
<p>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017b. The webnlg challenge: Generating text from rdf data. In Proceedings of the 10th International Conference on Natural Language Generation (INLG). pages 124133. http://www.aclweb.org/anthology/W17-3518.</p>
<p>Hieu Hoang and Philipp Koehn. 2008. Design of the moses decoder for statistical machine translation. In Software Engineering, Testing, and Quality Assurance for Natural Language Processing (SETQA-NLP). pages 58-65. http://www.aclweb.org/anthology/W08-0510.</p>
<p>Diederik P. Kingma and Jimmy Lei Ba. 2015. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR). https://arxiv.org/abs/1412.6980.</p>
<p>Rmi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with application to the biography domain. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 1203-1213. https://aclweb.org/anthology/D161128.</p>
<p>Xiaodan Liang, Xiaohui Shen, Jiashi Feng, Liang Lin, and Shuicheng Yan. 2016. Semantic object parsing with graph lstm. In Proceedings of the 14th European Conference on Computer Vision (ECCV). pages 125-143. https://doi.org/10.1007/978-3-319-46448-0_8.</p>
<p>Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective approaches to attention-based neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 1412-1421. http://aclweb.org/anthology/D15-1166.</p>
<p>Hongyuan Mei, Mohit Bansal, and Matthew R. Walter. 2016. What to talk about and how? selective generation using lstms with coarse-to-fine alignment. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). pages 720-730. http://www.aclweb.org/anthology/N16-1086.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics (ACL). pages 311-318. http://aclweb.org/anthology/P02-1040.pdf.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 1532-1543. https://aclweb.org/anthology/D14-1162.</p>
<p>Ehud Reiter and Robert Dale. 2000. Building natural language generation systems. Cambridge University Press.</p>
<p>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of Association for Machine Translation in the Americas (AMTA). pages 223-231. http://mt-archive.info/AMTA-2006-Snover.pdf.</p>
<p>Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL) and the 7th International Joint Conference on Natural Language Processing (IJCNLP). pages 1556-1566. http://www.aclweb.org/anthology/P151150 .</p>
<p>Christina Unger, Lorenz Bhmann, Jens Lehmann, Axel-Cyrille Ngonga Ngomo, Daniel Gerber, and Philipp Cimiano. 2012. Template-based question answering over rdf data. In Proceedings of the 21st international conference on World Wide Web (WWW). pages 639-648. https://doi.org/10.1145/2187836.2187923.</p>
<p>Pavlos Vougiouklis, Hady Elsahar, Lucie-Aime Kaffee, Christoph Gravier, Frederique Laforest, Jonathon Hare, and Elena Simperl. 2017. Neural wikipedian: Generating textual summaries from knowledge base triples. arXiv preprint arXiv:1711.00155 https://arxiv.org/pdf/1711.00155.pdf.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ http://bit.ly/gkb-mappings&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
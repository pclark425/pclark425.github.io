<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6987 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6987</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6987</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-252118723</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2209.03834v2.pdf" target="_blank">Pre-Training a Graph Recurrent Network for Language Representation</a></p>
                <p><strong>Paper Abstract:</strong> Transformer-based pre-trained models have gained much advance in recent years, becoming one of the most important backbones in natural language processing. Recent work shows that the attention mechanism inside Transformer may not be necessary, both convolutional neural networks and multi-layer perceptron based models have also been investigated as Transformer alternatives. In this paper, we consider a graph recurrent network for language model pre-training, which builds a graph structure for each sequence with local token-level communications, together with a sentence-level representation decoupled from other tokens. The original model performs well in domain-specific text classification under supervised training, however, its potential in learning transfer knowledge by self-supervised way has not been fully exploited. We fill this gap by optimizing the architecture and verifying its effectiveness in more general language understanding tasks, for both English and Chinese languages. As for model efficiency, instead of the quadratic complexity in Transformer-based models, our model has linear complexity and performs more efficiently during inference. Moreover, we find that our model can generate more diverse outputs with less contextualized feature redundancy than existing attention-based models.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6987.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6987.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GRN (this work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Recurrent Network for Language Representation (pre-trained GRN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-trained, attention-free graph recurrent network that represents each sentence as a graph of token nodes plus a dedicated sentence‑level node and is optimized with masked language modeling; it achieves O(n) complexity and competitive downstream performance with reduced inference cost on long sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Token-node + Sentence-node graph (S-LSTM style GRN)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each input sentence is converted to an undirected (implemented as bi-directional directed edges) graph G=(V,E) where V = {h1...hn, g} with hi token nodes and g a sentence-level node. Edges connect each token node to its immediate neighbors (|i-j| ≤ 1) and to the sentence node; self-loops are added. Token embeddings (SentencePiece subwords + absolute position embeddings) initialize token nodes; recurrent gated LSTM-style updates propagate local neighbor information and gated global (sentence node) information across recurrent layers. The final token and sentence node hidden states are used for masked language modeling and downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token‑based; hierarchical (token nodes + separate sentence node); sequential/local-graph</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Local-neighborhood graph construction (connect immediate neighbors) combined with an explicit sentence-node aggregation (average of token states feeds sentence node); no DFS/BFS canonicalization or linearization is used — the graph is used as internal structure for recurrent updates.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WikiText-103 (development experiments); Wikipedia + BookCorpus (English pretraining); Wikipedia (Chinese pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Masked language modeling pre-training (MLM) and downstream language understanding (GLUE for English; multiple Chinese classification/matching tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Graph Recurrent Network (GRN) / S-LSTM based pre-trained model (authors' implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>S-LSTM-style GRN with shared parameters across recurrent layers (layerwise parameter sharing), using subword SentencePiece vocab of 30k, position embeddings, layer normalization on gates. Configurations reported: experiments with hidden sizes 1280–2048, recurrent steps (layers) around 6–12 (empirically ~10 steps sufficient), model variants comparable in parameter count to RoBERTa-base (~100–125M in reported settings).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Perplexity (language modeling); GLUE average score (classification); Chinese task accuracies; CPU inference runtime / speedup; principal components (PCA) explained variance for contextual redundancy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Development: perplexity on WikiText-103 decreases with depth and hidden size (approaches RoBERTa when hidden size ≈1792) (exact perplexity numbers not reported). Downstream: English GLUE average = 78.67 (authors' pre-trained GRN); Chinese pooled average reported ≈78.78 (Table values). Inference: 2–3× speedup in CPU runtime for long sequences (≥256) versus RoBERTa/DistilBERT in reported settings; fastest up to ~8.5k sequence length among compared models. Intrinsic analyses: larger number of principal components required to retain same variance vs BERT/RoBERTa indicating less contextual feature redundancy (examples: for length=100, Ours requires ~52 PCs for 90% vs BERT ~48; for 98% Ours 87 vs BERT 79).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Provides linear time complexity O(n) w.r.t. sequence length leading to substantially reduced inference time on long sequences; converges slower initially in training but reaches near-Transformer perplexity with larger capacity; produces more diverse sentence/token outputs with less contextualized feature redundancy which may benefit downstream representation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Slight accuracy gap versus full Transformer (authors report retaining ~98–99% of BERT/ALBERT performance on benchmarks); slower early convergence; recurrent extra steps increase compute/time (more recurrent steps = proportionally slower inference/training); model currently explored only in encoder/MLM (not seq2seq) in this work; no canonical linearization for graph-to-text generation presented (graph is internal representation, not serialized).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to Transformer-based PTMs: GRN has O(n) complexity vs O(n^2) for full self-attention, giving speed gains on long sequences; achieves competitive downstream scores (within ~1 point of BERT on GLUE average) while producing less feature redundancy and more diverse outputs; compared to RNN/CNN it explicitly integrates a sentence-level node for global context rather than relying solely on directional recurrence or convolutional kernels; compared to attention-approximation transformers (Longformer, Reformer, Linformer, Performer), GRN is attention-free and competitive in runtime for many long-sequence regimes but does not directly apply attention approximations or sparse/global attention mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_confidence</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-Training a Graph Recurrent Network for Language Representation', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6987.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6987.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>S-LSTM (prior)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sentence-state LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier GRN-like architecture that represents a sentence as token nodes plus a sentence-level state and performs recurrent gated updates to exchange local and sentence-level information; used here as the structural basis for the pre-trained GRN.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sentence-state LSTM for text representation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>S-LSTM sentence graph representation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represents sentences as a graph with token nodes and a sentence state node; token nodes connect to neighbors and to the sentence node; recurrent gated LSTM-style updates exchange information between token and sentence nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token‑based; hierarchical; sequential/local-graph</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Local-neighborhood connections + explicit sentence node aggregation with recurrent gating (original S-LSTM update rules used as the backbone for node updates).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sentence representation for supervised sequence labeling/classification (original S-LSTM work); referenced here as architectural prior for pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>S-LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Original S-LSTM architecture (non-attention, recurrent graph updates) used for supervised sequence tasks; in this paper the authors adapt S-LSTM structure with subword inputs and layer normalization for large-scale pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Cited as enabling fully-parallel token computations (compared with standard LSTM) and providing a graph-structured alternative to attention; used to justify GRN design.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Original S-LSTM used character-level inputs and was not previously evaluated extensively for large-scale self-supervised pre-training (gap this paper addresses).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Presented as an alternative to bidirectional LSTM and full-attention Transformer; S-LSTM/GRN offers better parallelism than standard LSTM and linear complexity versus Transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_confidence</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-Training a Graph Recurrent Network for Language Representation', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6987.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6987.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMR graph2seq (ref)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A graph-to-sequence model for AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced graph-to-sequence approach that converts Abstract Meaning Representation (AMR) graphs into textual sentences using a graph encoder and sequence decoder (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A graph-to-sequence model for AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>AMR graph-to-sequence linearization / graph-encoder representation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Referenced high-level: uses a graph encoder to process AMR graph structure and a sequence decoder to generate text; exact linearization/serialization details are not described in this paper (only cited).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>graph-to-sequence (encoder-decoder); token-based sequence output</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR (implied by title, not specified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation / graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>graph-to-sequence model (AMR-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Only cited as related work; this paper does not report any evaluation results or detailed properties for AMR graph-to-text approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Mentioned among graph-based approaches for text generation; no direct empirical comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_confidence</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-Training a Graph Recurrent Network for Language Representation', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6987.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6987.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-to-sequence (semantic parsing ref)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Exploiting rich syntactic information for semantic parsing with graph-to-sequence model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work that applies graph-to-sequence architectures to semantic parsing, converting syntactic graphs into sequences (cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploiting rich syntactic information for semantic parsing with graph-to-sequence model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Syntactic graph-to-sequence representation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Cited as an example of graph-to-sequence application where syntactic information is encoded as a graph and a sequence decoder produces text/parse output; the paper being analyzed does not provide the encoding details.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>graph-to-sequence; token-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Semantic parsing / graph-to-text generation (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>graph-to-sequence</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Only mentioned in related work; no experimental details or metrics provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Listed among tasks where graph neural networks or graph-to-sequence models have been applied (SRL, MT, text generation, semantic parsing).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_confidence</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-Training a Graph Recurrent Network for Language Representation', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sentence-state LSTM for text representation <em>(Rating: 2)</em></li>
                <li>A graph-to-sequence model for AMR-to-text generation <em>(Rating: 2)</em></li>
                <li>Exploiting rich syntactic information for semantic parsing with graph-to-sequence model <em>(Rating: 2)</em></li>
                <li>Encoding sentences with graph convolutional networks for semantic role labeling <em>(Rating: 1)</em></li>
                <li>Graph convolutional encoders for syntax-aware neural machine translation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6987",
    "paper_id": "paper-252118723",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "GRN (this work)",
            "name_full": "Graph Recurrent Network for Language Representation (pre-trained GRN)",
            "brief_description": "A pre-trained, attention-free graph recurrent network that represents each sentence as a graph of token nodes plus a dedicated sentence‑level node and is optimized with masked language modeling; it achieves O(n) complexity and competitive downstream performance with reduced inference cost on long sequences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Token-node + Sentence-node graph (S-LSTM style GRN)",
            "representation_description": "Each input sentence is converted to an undirected (implemented as bi-directional directed edges) graph G=(V,E) where V = {h1...hn, g} with hi token nodes and g a sentence-level node. Edges connect each token node to its immediate neighbors (|i-j| ≤ 1) and to the sentence node; self-loops are added. Token embeddings (SentencePiece subwords + absolute position embeddings) initialize token nodes; recurrent gated LSTM-style updates propagate local neighbor information and gated global (sentence node) information across recurrent layers. The final token and sentence node hidden states are used for masked language modeling and downstream tasks.",
            "representation_type": "token‑based; hierarchical (token nodes + separate sentence node); sequential/local-graph",
            "encoding_method": "Local-neighborhood graph construction (connect immediate neighbors) combined with an explicit sentence-node aggregation (average of token states feeds sentence node); no DFS/BFS canonicalization or linearization is used — the graph is used as internal structure for recurrent updates.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WikiText-103 (development experiments); Wikipedia + BookCorpus (English pretraining); Wikipedia (Chinese pretraining)",
            "task_name": "Masked language modeling pre-training (MLM) and downstream language understanding (GLUE for English; multiple Chinese classification/matching tasks)",
            "model_name": "Graph Recurrent Network (GRN) / S-LSTM based pre-trained model (authors' implementation)",
            "model_description": "S-LSTM-style GRN with shared parameters across recurrent layers (layerwise parameter sharing), using subword SentencePiece vocab of 30k, position embeddings, layer normalization on gates. Configurations reported: experiments with hidden sizes 1280–2048, recurrent steps (layers) around 6–12 (empirically ~10 steps sufficient), model variants comparable in parameter count to RoBERTa-base (~100–125M in reported settings).",
            "performance_metric": "Perplexity (language modeling); GLUE average score (classification); Chinese task accuracies; CPU inference runtime / speedup; principal components (PCA) explained variance for contextual redundancy.",
            "performance_value": "Development: perplexity on WikiText-103 decreases with depth and hidden size (approaches RoBERTa when hidden size ≈1792) (exact perplexity numbers not reported). Downstream: English GLUE average = 78.67 (authors' pre-trained GRN); Chinese pooled average reported ≈78.78 (Table values). Inference: 2–3× speedup in CPU runtime for long sequences (≥256) versus RoBERTa/DistilBERT in reported settings; fastest up to ~8.5k sequence length among compared models. Intrinsic analyses: larger number of principal components required to retain same variance vs BERT/RoBERTa indicating less contextual feature redundancy (examples: for length=100, Ours requires ~52 PCs for 90% vs BERT ~48; for 98% Ours 87 vs BERT 79).",
            "impact_on_training": "Provides linear time complexity O(n) w.r.t. sequence length leading to substantially reduced inference time on long sequences; converges slower initially in training but reaches near-Transformer perplexity with larger capacity; produces more diverse sentence/token outputs with less contextualized feature redundancy which may benefit downstream representation quality.",
            "limitations": "Slight accuracy gap versus full Transformer (authors report retaining ~98–99% of BERT/ALBERT performance on benchmarks); slower early convergence; recurrent extra steps increase compute/time (more recurrent steps = proportionally slower inference/training); model currently explored only in encoder/MLM (not seq2seq) in this work; no canonical linearization for graph-to-text generation presented (graph is internal representation, not serialized).",
            "comparison_with_other": "Compared to Transformer-based PTMs: GRN has O(n) complexity vs O(n^2) for full self-attention, giving speed gains on long sequences; achieves competitive downstream scores (within ~1 point of BERT on GLUE average) while producing less feature redundancy and more diverse outputs; compared to RNN/CNN it explicitly integrates a sentence-level node for global context rather than relying solely on directional recurrence or convolutional kernels; compared to attention-approximation transformers (Longformer, Reformer, Linformer, Performer), GRN is attention-free and competitive in runtime for many long-sequence regimes but does not directly apply attention approximations or sparse/global attention mechanisms.",
            "comparison_confidence": "high",
            "uuid": "e6987.0",
            "source_info": {
                "paper_title": "Pre-Training a Graph Recurrent Network for Language Representation",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "S-LSTM (prior)",
            "name_full": "Sentence-state LSTM",
            "brief_description": "An earlier GRN-like architecture that represents a sentence as token nodes plus a sentence-level state and performs recurrent gated updates to exchange local and sentence-level information; used here as the structural basis for the pre-trained GRN.",
            "citation_title": "Sentence-state LSTM for text representation",
            "mention_or_use": "mention",
            "representation_name": "S-LSTM sentence graph representation",
            "representation_description": "Represents sentences as a graph with token nodes and a sentence state node; token nodes connect to neighbors and to the sentence node; recurrent gated LSTM-style updates exchange information between token and sentence nodes.",
            "representation_type": "token‑based; hierarchical; sequential/local-graph",
            "encoding_method": "Local-neighborhood connections + explicit sentence node aggregation with recurrent gating (original S-LSTM update rules used as the backbone for node updates).",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "Sentence representation for supervised sequence labeling/classification (original S-LSTM work); referenced here as architectural prior for pretraining",
            "model_name": "S-LSTM",
            "model_description": "Original S-LSTM architecture (non-attention, recurrent graph updates) used for supervised sequence tasks; in this paper the authors adapt S-LSTM structure with subword inputs and layer normalization for large-scale pre-training.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Cited as enabling fully-parallel token computations (compared with standard LSTM) and providing a graph-structured alternative to attention; used to justify GRN design.",
            "limitations": "Original S-LSTM used character-level inputs and was not previously evaluated extensively for large-scale self-supervised pre-training (gap this paper addresses).",
            "comparison_with_other": "Presented as an alternative to bidirectional LSTM and full-attention Transformer; S-LSTM/GRN offers better parallelism than standard LSTM and linear complexity versus Transformer.",
            "comparison_confidence": "high",
            "uuid": "e6987.1",
            "source_info": {
                "paper_title": "Pre-Training a Graph Recurrent Network for Language Representation",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "AMR graph2seq (ref)",
            "name_full": "A graph-to-sequence model for AMR-to-text generation",
            "brief_description": "A referenced graph-to-sequence approach that converts Abstract Meaning Representation (AMR) graphs into textual sentences using a graph encoder and sequence decoder (cited as related work).",
            "citation_title": "A graph-to-sequence model for AMR-to-text generation",
            "mention_or_use": "mention",
            "representation_name": "AMR graph-to-sequence linearization / graph-encoder representation",
            "representation_description": "Referenced high-level: uses a graph encoder to process AMR graph structure and a sequence decoder to generate text; exact linearization/serialization details are not described in this paper (only cited).",
            "representation_type": "graph-to-sequence (encoder-decoder); token-based sequence output",
            "encoding_method": null,
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "AMR (implied by title, not specified in this paper)",
            "task_name": "AMR-to-text generation / graph-to-text generation",
            "model_name": "graph-to-sequence model (AMR-to-text)",
            "model_description": null,
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": null,
            "limitations": "Only cited as related work; this paper does not report any evaluation results or detailed properties for AMR graph-to-text approaches.",
            "comparison_with_other": "Mentioned among graph-based approaches for text generation; no direct empirical comparison in this paper.",
            "comparison_confidence": "medium",
            "uuid": "e6987.2",
            "source_info": {
                "paper_title": "Pre-Training a Graph Recurrent Network for Language Representation",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Graph-to-sequence (semantic parsing ref)",
            "name_full": "Exploiting rich syntactic information for semantic parsing with graph-to-sequence model",
            "brief_description": "A referenced work that applies graph-to-sequence architectures to semantic parsing, converting syntactic graphs into sequences (cited in related work).",
            "citation_title": "Exploiting rich syntactic information for semantic parsing with graph-to-sequence model",
            "mention_or_use": "mention",
            "representation_name": "Syntactic graph-to-sequence representation",
            "representation_description": "Cited as an example of graph-to-sequence application where syntactic information is encoded as a graph and a sequence decoder produces text/parse output; the paper being analyzed does not provide the encoding details.",
            "representation_type": "graph-to-sequence; token-based",
            "encoding_method": null,
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "Semantic parsing / graph-to-text generation (referenced)",
            "model_name": "graph-to-sequence",
            "model_description": null,
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": null,
            "limitations": "Only mentioned in related work; no experimental details or metrics provided here.",
            "comparison_with_other": "Listed among tasks where graph neural networks or graph-to-sequence models have been applied (SRL, MT, text generation, semantic parsing).",
            "comparison_confidence": "medium",
            "uuid": "e6987.3",
            "source_info": {
                "paper_title": "Pre-Training a Graph Recurrent Network for Language Representation",
                "publication_date_yy_mm": "2022-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sentence-state LSTM for text representation",
            "rating": 2,
            "sanitized_title": "sentencestate_lstm_for_text_representation"
        },
        {
            "paper_title": "A graph-to-sequence model for AMR-to-text generation",
            "rating": 2,
            "sanitized_title": "a_graphtosequence_model_for_amrtotext_generation"
        },
        {
            "paper_title": "Exploiting rich syntactic information for semantic parsing with graph-to-sequence model",
            "rating": 2,
            "sanitized_title": "exploiting_rich_syntactic_information_for_semantic_parsing_with_graphtosequence_model"
        },
        {
            "paper_title": "Encoding sentences with graph convolutional networks for semantic role labeling",
            "rating": 1,
            "sanitized_title": "encoding_sentences_with_graph_convolutional_networks_for_semantic_role_labeling"
        },
        {
            "paper_title": "Graph convolutional encoders for syntax-aware neural machine translation",
            "rating": 1,
            "sanitized_title": "graph_convolutional_encoders_for_syntaxaware_neural_machine_translation"
        }
    ],
    "cost": 0.01275625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Pre-Training a Graph Recurrent Network for Language Representation
26 Oct 2022</p>
<p>Yile Wang wangyile@westlake.edu.cn 
Westlake University</p>
<p>Linyi Yang yanglinyi@westlake.edu.cn 
Westlake University</p>
<p>Zhiyang Teng tengzhiyang@westlake.edu.cn 
Westlake University</p>
<p>Ming Zhou zhouming@chuangxin.com 
Langboat Technology
BeijingChina</p>
<p>Yue Zhang zhangyue@westlake.edu.cn 
Westlake University</p>
<p>Pre-Training a Graph Recurrent Network for Language Representation
26 Oct 2022CEC32C06CD49B54B3FDB056AA51B22F0arXiv:2209.03834v2[cs.CL]
Transformer-based pre-trained models have gained much advance in recent years, becoming one of the most important backbones in natural language processing.Recent work shows that the attention mechanism inside Transformer may not be necessary, both convolutional neural networks and multi-layer perceptron based models have also been investigated as Transformer alternatives.In this paper, we consider a graph recurrent network for language model pre-training, which builds a graph structure for each sequence with local token-level communications, together with a sentence-level representation decoupled from other tokens.The original model performs well in domain-specific text classification under supervised training, however, its potential in learning transfer knowledge by self-supervised way has not been fully exploited.We fill this gap by optimizing the architecture and verifying its effectiveness in more general language understanding tasks, for both English and Chinese languages.As for model efficiency, instead of the quadratic complexity in Transformer-based models, our model has linear complexity and performs more efficiently during inference.Moreover, we find that our model can generate more diverse outputs with less contextualized feature redundancy than existing attention-based models. 1</p>
<p>Introduction</p>
<p>Pre-trained neural models (PTMs) [1,2,3,4,5,6,7,8,9,10] have been widely used in natural language processing (NLP), benefiting a range of tasks including natural language understanding [11,12], question answering [13,14], summarization [15,16], and dialogue [17,18].The current dominant methods take the Transformer [19] architecture, a heavily engineered model based on a self-attention network (SAN), showing competitive performance in computation vision [20,21,22], speech [23] and biological [24] tasks.</p>
<p>Despite its success, Transformer-based models typically suffer from quadratic time complexity [19], along with the requirement of large computational resources and associated financial and environmental costs [25].In addition, recent studies show that the attention mechanism, which is the key ingredient of Transformer, may not be necessary [26,27,28].For example, Tay et al. [28] find that models learning synthetic attention weights without token-token interactions also achieve competitive performance for certain tasks.Therefore, investigation of Transformer alternatives for pre-trained models is of both theoretical and practical interest.To this end, various non-Transformer PTMs have recently been proposed [29,30,31,32].</p>
<p>In this paper, we consider a graph neural network (GNN) [33] for language model pre-training.GNN and its variants have been widely used in NLP tasks, including machine translation [34], information</p>
<p>Type</p>
<p>Models</p>
<p>Basic Unit Complexity Parallel Parameter Sharing LSTM Context2Vec [42] RNN O(n) ELMo [43] Transformer GPT2 [1] SAN O(n 2 ) BERT [2] RoBERTa [3] XLNet [4] ALBERT [5] BART [6] T5 [7] DeBERTa [8] Others DynamicConv [29] CNN O(n) gMLP [30] MLP
O(n) Ours GRN O(n)
Table 1: Overview of existing types of pre-trained models and our proposed model.</p>
<p>extraction [35], and sentiment analysis [36].For GNN language modeling, a key problem is how to represent a sentence in a graph structure.From this perspective, ConvSeq2seq [37] can be regarded as a graph convolutional network (GCN) [38] with node connections inside a local kernel.Transformerbased models can be regarded as a graph attention network (GAT) [39] with a full node connection.However, graph recurrent network (GRN) [40,41] models have been relatively little considered.</p>
<p>We follow the GRN structure of Sentence-state LSTM (S-LSTM) [40], which represents a sentence using a graph structure by treating each word as a node, together with a sentence state node.State transitions are performed recurrently to allow token nodes to exchange information with their neighbors as well as a sentence-level node.Such architecture has shown some advantages over vanilla bidirectional LSTM in supervised sequence classification tasks.However, compared with existing Transformer-based models, its potential for general-purpose language modeling pre-training has not been fully exploited.</p>
<p>As seminal models for pre-trained language modeling, ELMo [43] takes a bi-directional LSTM structure, sharing the gate and cell structures as sentence-state LSTMs.BERT [2] uses a Transformer encoder with multiple SAN sub-layers.Compared ELMo, computation for each token in S-LSTM is fully parallel, making it practical for large-scale pre-training.Compared with BERT, our attentionfree model gives O(n) complexity instead of O(n 2 ) with respect to sequence length, which is more computationally friendly for long sequences.We optimize the model by exploring the suitable architecture design for pre-training, aiming at learning better transferable task-agnostic knowledge for language representations.</p>
<p>Experimental results show that our model can give a comparable performance on language modeling and language understanding tasks for both English and Chinese languages.During inference, our model can gain 2∼3 times speedup or more for extra long sentences against Transformer-based models.Moreover, the outputs of our model are more diverse than Transformer-based PTMs, which may help us understand the non-uniform distribution of contextualized embeddings in PTMs from the viewpoint of the model architecture.</p>
<p>To our knowledge, we are the first to investigate a graph recurrent network for general language model pre-training, which does not rely on the attention mechanism and has linear computational complexity.Experimental results show that our model can largely reduce the inference time cost without much accuracy loss compared with BERT and its variants, on both English and Chinese languages.Our model can serve as a reference for non-Transformer architecture exploration.</p>
<p>shows the strong potential of contextualized representation and pre-training techniques in modern NLP systems.</p>
<p>2) Transformer-based.Vaswani et al. [19] proposed Transformer to overcome the non-parallel computation in RNN using long-range attention mechanism.PTMs based on Transformer has developed rapidly over the past few years, including auto-encoding [2,3,5,8], auto-regressive [1,4], and sequence-to-sequence [6,7] style Transformer variants.</p>
<p>3) Others.Recent work [26,27,28] explores whether attention-free models can also be used for pretraining.Tay et al. [29] show that CNN-based PTM outperforms its Transformer-based counterpart in certain tasks.Liu et al. [30] propose an MLP-based alternative without self-attention.Lee-Thorp et al. [46] propose FNet which uses an unparameterized Fourier transform to replace the self-attention sublayer.These endeavors show that, within proper design and settings, different architectures can also match the performance of Transformer ones in certain scenarios.Our work is in line, while we conducted more experiments on language modeling and general language understanding tasks for both English and Chinese.</p>
<p>Graph Recurrent Network.GRN is a particular case of RNN in which the signals at each node in time are supported on a graph.Compared with other graph neural networks like GCN [38] or GAT [39], the information for each node is aggregated recurrently, combined with spatial gating strategies instead of convolutional or self-attention operation.GRN has been used to model graph structures in NLP tasks, such as semantic role labeling [47], machine translation [34], text generation [41] and semantic parsing [48].In particular, Zhang et al. [40] use GRN to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing the success of graph recurrent structure for sequence labeling and classification tasks against bidirectional LSTM.</p>
<p>Although GRN performs well competitive to RNN or GCN in certain tasks, no previous study investigate its capability for general language modeling and understanding.We fill the gap by exploring the ways of pre-training GRN and demonstrating its effectiveness and efficiency.</p>
<p>Time-efficient Transformers.Due to the quadratic complexity, it has been a practical question how to train a more computationally efficient Transformer.Reformer [49] use locality-sensitive hashing to replace dot-product attention, changing its complexity from O(n2 ) to O(nlogn).Longformer [50] utilizes a local windowed attention with global attention, making it easy to process long documents.Linformer [51] optimizes the self-attention using low-rank projection, reducing the complexity to O(n).Performer [52] uses the kernel feature map to efficiently approximate the softmax attention with theoretically grounded O(n) complexity.Peng et al. [53] propose random feature methods to approximate the softmax function.For a comprehensive study of these models, we refer the readers to Tay et al. [54].</p>
<p>The above works focus on modifying the attention mechanism in Transformer and verify on specific usage 2 .Our model achieves linear complexity with respect to the sequence length.However, it does not rely on Transformer while using a different graph structure to represent text generally.</p>
<p>Model</p>
<p>The overall structure of our model is shown in Figure 1.Following S-LSTM [40], we treat each sentence as a graph with token nodes and an external sentence state node.The node state is updated in parallel according to the information received in each layer.The representations in the final L th layer is used for inference.</p>
<p>Graph Structure</p>
<p>We build a graph G = (V, E) for every sentence s, which is a sequence of tokens w 1 , w 2 , ..., w n .The verticals V consists of token nodes h i (i = 1, 2, ..., n) and a sentence state node g.The connections are all bi-directional, in the sense that one forward edge and one backward edge exist between node pairs.For edges E, each token node is connected with it neighbor nodes and the sentence node.Formally,
V = {h 1 , h 2 , ..., h n , g} E = {e i,j , e i,g , e g,i ; |i − j| ≤ 1},(1)
where e i,j is the directed edge from node h i to h j , e i,g and e g,i are the two directed edges between h i and g.Self-loops are added for all nodes, allowing information flow from its previous state.</p>
<p>Encoding Layer</p>
<p>We strictly follow S-LSTM in node communication.However, different from the original model which uses character-level word representation, we use subwords [55] and position embeddings [19].Layer normalization [56] operations are also added, which shows benefit for language model pre-training.</p>
<p>Input Embeddings.To learn a better vocabulary, we tokenize the text with the language independent SentencePiece [57].The total size of the vocabulary is 30,000.We first transform each token w i into token embedding using the trainable lookup table E and the position embedding lookup table P , the model input x i is constructed by summing the two:
x i = E(w i ) + P (w i )(2)
Token Node Update.We initialize hidden states and hidden cells for each token node, the sentencelevel node with h 0 1 , h 0 2 , ..., h 0 n , g 0 and c 0 1 , c 0 2 , ..., c 0 n , c 0 g , respectively.In each layer t (t = 1, 2, ..., L), the token node states h t i is calculated using gating mechanism similar with LSTM:
ξ t−1 i = h t−1 i−1 h t−1 i h t−1 i+1 ît i = σ(LayerNorm(W i ξ t−1 i + U i x i + V i g t−1 + b i )) lt i = σ(LayerNorm(W l ξ t−1 i + U l x i + V l g t−1 + b l )) rt i = σ(LayerNorm(W r ξ t−1 i + U r x i + V r g t−1 + b r )) f t i = σ(LayerNorm(W f ξ t−1 i + U f x i + V f g t−1 + b f )) ŝt i = σ(LayerNorm(W s ξ t−1 i + U s x i + V s g t−1 + b s )) o t i = σ(LayerNorm(W o ξ t−1 i + U o x i + V o g t−1 + b o )) u t i = tanh(LayerNorm(W u ξ t−1 i + U u x i + V u g t−1 + b u )) i t i , l t i , r t i , f t i , s t i = softmax( ît i , lt i , rt i , f t i , ŝt i ) c t i = l t i c t−1 i−1 + f t i c t−1 i + r t i c t−1 i+1 + s t i c t−1 g + i t i u t i h t i = o t i tanh(c t i )(3)
where is concatenation operation, ξ t−1 i , x i , g t−1 represent the inputs from previous local states, token embedding and previous global states, respectively.In Eq. 3, we calculate multiple LSTM-style gates to control the corresponding information flow.lt i , rt i , f t i , ŝt i are the forget gates with respect to the left token cell c t−1 i−1 , right token cell c t−1 i+1 , current token cell c t−1 i and sentence cell c t−1 g .ît i , o t i are the input gate and output gate, respectively.Layer normalization is used to control the distributions of neurons in each gate.W x , U x , V x and b x (x ∈ {i, l, r, f, s, o, u}) are model parameters.</p>
<p>Sentence-level Node Update.The sentence-level node g t takes the previous token state as inputs and is calculated by :
h = avg(h t−1 1 , h t−1 2 , ..., h t−1 n ) f t i = σ(LayerNorm(W f g t−1 + U f h t−1 i + b f )) f t g = σ(LayerNorm(W g g t−1 + U g h + b g )) o t = σ(LayerNorm(W o g t−1 + U o h + b o )) f t 1 , ..., f t n , f t g = softmax( f t 1 , ..., f t n , f t g ) c t g = f t g c t−1 g + f t i c t−1 i g t = o t tanh(c t g )(4)
where f t 1 , ..., f t n , f t g are the forget gates with respect to token cells c t−1 1 , ..., c t−1 n and sentence cell c t−1 g , o t is the output gate.W x , U x and b x (x ∈ {g, f, o}) are model parameters.The generated hidden state h t i and g t are sent to the next layer, together with the memory state c t i and c t g .Both LSTM and our model use gates to control the information exchange, the main difference is that LSTM represents the subsequence from the beginning to a certain token in the sequence direction, while our model uses a structural state to represent all tokens and a sentence node simultaneously in the layer direction.</p>
<p>Inference Layer</p>
<p>Mask Language Modeling.The final layer representations are used for inference.In self-supervised learning without labels, similar to the auto-encoding style pre-training in BERT [2], we randomly mask 15% input tokens and use its corrupted representations for masked language modeling prediction3 , the final training loss is:
L mlm = − i log P θ (w i | w i ) = − i log exp(E(w i ) (W h i )) j exp(E(w j ) (W h i ))(5)
where w i is the masked token, h i is the corresponding representation, W is an added linear output layer and E(w i ) is the ground truth token embedding in Eq. 2.    shows that this forbids the decoupling of sentence representation from token representations, which limits model expressiveness.In our model, the sentence-level node representation is designed to be separated from other tokens, which can benefit the diversity of output vectors.</p>
<p>Comparison with Other Models</p>
<p>Parameter Sharing.We make all the trainable parameters in Eq. 3 and Eq. 4 shared across GNN layers, which is similar to the parameters in LSTM along the sequence direction [43].Layerwise parameter sharing is also used in models such as Universal Transformer [60] and ALBERT [5].However, both are based on Transformer architecture, which uses attention to achieve information exchange across all token pairs, while we use graph structure with an LSTM-style gating mechanism for token and sentence-level node update interactively.</p>
<p>Complexity.Transformer-based models update each token by attending over the whole contexts (i.e., all other tokens) in each layer, resulting in a complexity of O(n 2 ) w.r.t sequence length.For our model, we update each token using its fixed local context together with a sentence-level representation in each layer, making our model has O(n) complexity.</p>
<p>Experiments</p>
<p>We first explore the suitable architecture settings by language modeling task.Then we pre-trained using large-scale corpus and verify the effectiveness of our model for learning transferable knowledge on both English and Chinese NLP tasks.</p>
<p>Development Experiments on Language Modeling</p>
<p>We train our model in different settings on the WikiText-103 dataset [61] with 64 batch size and 150k steps, evaluating on the perplexity for predicting the masked token 4 .We use the RoBERTa-base model for comparison, which is a Transformer based model with 12 layers, 12 attention heads, 768 hidden sizes, and a total of 125 million parameters.The results are shown in Figure 3.</p>
<p>Recurrent</p>
<p>Step.In Figure 3(a), we first set the dimension as 1280 (with a total of 107 million parameters) and vary the layer number.Empirically, we find that the perplexity decreases steadily while the model becomes deeper.However, the difference is insignificant when the layer number reaches 10 or more.Also, although the training parameters stay unchanged because of the parameter sharing mechanism, the total account of calculation still increases with more recurrent steps, which makes it 1/1.2/1.5/1.8 times slower when the layer is 6/8/10/12, respectively.Our model generally convergences slower at the beginning, however, the perplexity gap compared with RoBERTa gradually become smaller at the final stage.</p>
<p>Lan et al. [5] suggest that there is no need for deeper models when sharing all cross-layer parameters.Zhang et al. [40] also find the accuracy reaches a peak when the recurrent step is around 9. The benefit for very deep recurrent-style models can be negligible while the time cost is higher.The reason can be that, for our model, the top reception field for each token is roughly enough when the recurrent step is around 10.</p>
<p>Hidden Size.We then set the layer number as 10 and try different hidden sizes to enhance the model capacity.As shown in Figure 3(b), by adding the hidden size, the model capacity becomes larger and the perplexity decreases much, while the difference becomes small too when the number reaches 1792 or more.Hidden size defines the total parameter size directly and the total training time cost is also influenced, where the model takes 1/1.4/1.8/2.2 times training times when the hidden size is 1280/1536/1792/2048, respectively.We find that the perplexity is almost as small as RoBERTa when the hidden size reaches 1792, which indicates that our model can also give competitive results as Transformer-based models for language modeling.</p>
<p>Position Embedding, Layer Normalization, and Dropout.In Figure 3(c), when we remove the position embeddings in Eq. 2, the perplexity becomes slightly higher, showing that the global position information is useful for our model, although the different local left or right gated information is used for each token.The way of integrating both relative and absolute position information is also similar to DeBERTa [8], where they add absolute embeddings in the inference layer.</p>
<p>Layer normalization [56] is used to make distribution stability through normalizing the mean and variance of all summed inputs to the neurons in one layer, which is irreplaceable in Transformer-based models.In our model, we also find it is crucial for doing layer normalization to the various gates in Eq. 3 and Eq. 4, where the perplexity becomes larger without such operation.</p>
<p>There is no significant improvement when using a dropout rate of 0.1 for output regularization as compared with non-dropout architecture.Lan et al. [5] also find that adding dropout in large Transformer-based models may have harmful results.We suppose that in recurrent-style models like ALBERT and ours, the interactions between layers are more strict, so the inconsistent dropout may lose useful learned features during training, which makes it not much useful as in other models.</p>
<p>Pre-training</p>
<p>Dataset.English models are trained using the latest Wikipedia and BookCorpus [62].Chinese models are trained using Wikipedia.The total amount of training data is on par with BERT [2] for both languages.</p>
<p>Baselines.Strictly comparing the PTMs is difficult because of the different dataset processing, training strategies, and environmental settings.As shown in Table 2, we consider the most related and popular models with similar training corpus for comparison.For English, we use the published RNN-based model (ELMo), compact version of BERT (DistilBERT), BERT, and recurrent version of BERT (ALBERT).For Chinese, we add some BERT variants which use Chinese word segmentor for whole word masking [63], or modify the masked token prediction as a correction target [64]. 5ettings.Similar to BERT, we train with a batch size of 128 and a maximum length of 512 for 300k steps, using Adam optimizer with learning rate lr=0.003,β 1 =0.9, β 2 =0.98, learning rate warmup</p>
<p>Models (English)</p>
<p>Pre-training Data Objective ELMo [43] 1 Billion Word CLM DistilBERT [66] Wiki+BooksCorpus BERT+KD BERT-base Wiki+BooksCorpus MLM+NSP ALBERT-base [5] Wiki+BooksCorpus MLM+SOP Models (Chinese) Data Objective BERT-base [2] Wiki MLM+NSP BERT-wwm [63] Wiki MLM+NSP BERT-wwm-ext [63] Wiki+EXT MLM+NSP RoBERTa-wwm [67] Wiki+CLUECorpus MLM RoBERTa-wwm-ext [63] Wiki+EXT MLM ALBERT-large [5] Wiki+EXT MLM+SOP MacBERT [64] Wiki+EXT Mac+SOP</p>
<p>Fine-tuning</p>
<p>Evaluating Benchmarks.For English tasks, we evaluate our pre-trained models on tasks in GLUE [11], including linguistic acceptability (CoLA), sentiment analysis (SST), sentence pair similarity (MRPC, QQP), and natural language inference (MNLI, QNLI, RTE).</p>
<p>For Chinese tasks, we evaluate on short and long text classification (TNEWS, IFLYTEK), keywords matching (CSL) [68], question matching (LCQMC) [69], document classification (THUCNews) [70] and sentiment analysis (ChnSentiCorp, CSC for short) [71].</p>
<p>Settings.Although our model architecture is different from most baselines, the fine-tuning strategy for each task can be the same as BERT-style models.As shown in Figure 4, the output of the sentence node g can be treated as the representation of [CLS] in BERT, which can be used for single sentence classification tasks directly.For sentence pair classification tasks, we concatenated two sentences and the target label is still predicted using the sentence node g in the last layer.</p>
<p>We use the official code from Huggingface [72] and CLUE [68] for reproducing the baseline and our results without external data augmentation, we mainly tune the parameters with training epochs in {2, 3, 5, 10}, learning rate in {2e-5, 3e-5, 5e-5} and batch size in {16, 32, 64}.</p>
<p>Results</p>
<p>The results are shown in Table 3 and Table 4.For English tasks, our model gives a average score of 78.67, which is higher than ELMo result of 69.65 and on par with Transformer-based baselines (77.87∼80.18).Compared with Transformer-based models, our results on tasks such as CoLA, QQP, QNLI, and RTE exceeds DistilBERT, being close to BERT (within average 1.0 point).Overall, our model compares well to ALBERT and BERT, retaining 99% and 98% of the performance, respectively.</p>
<p>For Chinese tasks, our model gives comparable results with BERT (within 0.05 points of accuracy) and slightly better than ALBERT (78.78 vs. 77.92),which uses the same amount of training corpus.</p>
<p>Compared with other models which apply more datasets, our model also performs well in tasks such as TNEWS and IFLYTEK.Moreover, some (e.g.RoBERTa-wwm-ext, MacBERT) inherit the parameters of BERT for continuing pre-training while our model is trained from scratch.Some baselines (e.g.BERT-wwm) uses whole word masking strategy for span masking prediction, we leave these configurations for further study.</p>
<p>Analysis of Computational Efficiency</p>
<p>Inference Speed.We compare the inference speed of our model with different architectures, including ELMo, an RNN-based model; RoBERTa, which is based on a full Transformer encoder; DistilBERT, a lighter version of BERT with 40% parameter reduction; and BART, which is a sequenceto-sequence model using Transformer.We test the time cost for model inference (i.e, calculating final layer representation) for a batch of sentences with different lengths.Evaluations are conducted in the same environment.</p>
<p>The average results over five runs are shown in Table 5. ELMo gives the lowest results as the sequential nature of RNN structure.For Transformer-based models, DistilBERT shows the minimum time cost because of the lightweight architecture, BART is slower than RoBERTa due to the nonparallel computation in the decoder.All the models take much more time when the sequence becomes much longer.For example, sequences with a length of 512 need about 3 times computational time than sequences with a length of 64.For our model, adding the recurrent layer and the hidden size will both leads to more inference time.However, by increasing the sequence length, the inference cost grows much slower than the baselines when the sequence length reaches 256 or more, our model can give a 2∼3 times speedup than DistilBERT or RoBERTa, even for large model settings.Extra Long Sequences.To further study the advantages of linear complexity in our model, We compared our model with Transformer variants, including Longformer [50], Reformer [49], Linformer [51], and Performer [52], which aims to optimize the self-attention mechanism for better dealing with long sequences.As standard autoencoder and seq2seq PTMs, we also include De-BERTa [8] and T5 [7] for reference.We use CPU runtime as the metric, avoiding the need for external resources and also enlarging the model differences, the results are shown in Figure 5.For DeBERTa and T5, the runtime increases quadratically as the sequence length becomes longer.The other models show the effectiveness of their improvement over the vanilla Transformer.</p>
<p>Longformer and Reformer give almost linear growth of runtime w.r.t sequence length.Our model is the fastest when the sequence length is below 8.5k.Linformer and Performer give slightly faster speed when the size reaches 10k.However, the models are particularly designed for long sequences.For example, Linformer project the full self-attention and find the low-rank representation, reducing the complexity from O(n 2 ) to O(nk), thus the projection dimension k should be pre-defined and less than the sequence length n.Similarly, Performer pre-defined kernel feature numbers m and reduce the complexity from O(n 2 ) to O(nm), the most computational efficiency is achieved only when n is relatively large.Overall, our model is attention-free and thus it can handle both short and long sequences friendly.</p>
<p>Intrinsic Characteristics of Model Representation</p>
<p>Since the nature of layer structure is different between S-LSTM and Transformer models, it can be an interesting research question how the characteristics of contextualized representation differ between such models.To this end, we try to investigate the intrinsic characteristics of representations.We consider doing a comparison from three aspects: 1) The space distributions of model outputs; 2) The contextualized feature redundancy of token representations in the same context; 3) The hidden states transitions in each layer. 7.</p>
<p>Model Output Distributions</p>
<p>Previous studies find that the distribution of representations from PTMs can be highly anisotropic, where the output vectors assemble in a narrow cone rather than being uniform in all directions [73,74].Such a phenomenon leads to the inferior performance of token or sentence representations [75,76,77].To compare the output vector distribution with other models, we select 5000 random tokens from corpus, computing the cosine similarity cos(h i , h j ) = <hi,hj > hi,hj of outputs from random tokens t i and t j .For sentence-level representation, we also compute the cos(h i[CLS] , h j [CLS] ) and cos(g i , g j ) for random sentences s i and s j .</p>
<p>The results are shown in Figure 6, where each figure depicts the distribution of 5000 2 random similarity scores.Ideally, the similarity distribution should be symmetric around the zero point.In the top row, the scores for BERT, RoBERTa, and ALBERT are almost all larger than zeros, where the distributions is extremely centralized around 0.8∼1 for RoBERTa, this is in line with Gao et.al.[78], where they find any two words are highly positively correlated.BERT, ALBERT and ours demonstrate similar and wider distributions, however, we find that there exist a mount of word pairs with negative relationships in our model.</p>
<p>As for sentence representations, BERT and ALBERT use next sentence prediction and sentence order prediction during pre-training, while RoBERTa does not introduce any sentence-level task.However, compared to the token vectors, the distributions for sentence representation are all become more closer to 1.This may due to the [CLS] token in Transformer-based models are treated equally with other tokens, with a fixed position embeddings for zeros, thus the shape of distribution are generally similar while the correlations become more positive.The results of our model show distinct patterns, where some sentence pairs give very small similarity with -0.7, and there are also part of disjunctive areas around -0.1∼0.1, 0.2∼0.4 and 0.7∼0.9.Some work suggest that the high anisotropy is inherent to, or least a by-product of contextualization [73,79], Gao et al. [78] attribute this to the nonuniform word frequencies in training data.Our results show that this could also related to the architecture designing, where the hierarchical local information and decoupled sentence representation can mitigate such phenomenon.</p>
<p>Contextualized Feature Redundancy</p>
<p>In the previous section, we consider the similarity of random token and sentence embedding, which reflect the general distribution in output space.In this section, we further consider the tokens in the same context, where all tokens share the same contextualized information.Existing work shows that, for Transformer-based models, each token attends to all other tokens with an attention mechanism in every layer, the overlapped context information may lead to feature redundancy.For example, Clark et al. [80] and Kovaleva et al. [81]    the same context.Peng et al. [82] find attention heads can be largely pruned.Dalvi et al. [83] suggest that 85% of the neurons across the network can be removed without loss of accuracy.</p>
<p>We use principal component analysis to investigate such contextualized feature redundancy.Formally, given sentence s = w 1 , w 2 , ..., w n , we collect the feature matrix H ∈ R n×d , where each row h i ∈ R 1×d represent the output of tokens w i .By truncated singular value decomposition, the feature matrix can be written as:
H n×d = U n×n Σ n×d V d×d ≈ U n×k Σ k×k V k×d(6)
where U and V are orthogonal matrices, Σ is the diagonal matrix with sorted singular values σ 1 , ... , σ n .The number of principal components k can be selected according to the required retained information, or the Frobenius norm of the difference between the original feature matrix and the reconstructed matrix.</p>
<p>Table 6 shows the results.For BERT, RoBERTa and ALBERT, about 50/80 principal components keep the 90%/98% information when the sequence length is 100.The numbers are around 110/210 when the length increase to 300.Our model does not use the full multi-head attention mechanism to build the contextualized information once.Instead, by combining explicit hierarchical local information and gated global information, each token receives different levels of information in each layer.In particular, our model gives the largest values accordingly (52/87 and 119/237), showing that the token representations share less common features than Transformer-based models.</p>
<p>Hidden State Variance Across Layers</p>
<p>The recurrent state transitions in our model improve the parameter efficiency.By sharing the parameters, the interactions between layers can be different from models without such limitations.We are interested in the layer transitions in the intermediate layers for recurrent (ALBERT, ours) and non-recurrent models (BERT, RoBERTa).</p>
<p>Figure 7 shows the L2 distances of the input and output embeddings in each layer.For both token and sentence embeddings, the transitions in BERT and RoBERTa are oscillating, where the parameters in each layer is more flexible.Results of ALBERT and our model are smoother, except for the increase of distance changes in the two-end layers, showing that weight-sharing has an effect on stabilizing network parameters [5].Our model gets the smallest distance changes, however, they also do not converge to zero at all times.These findings show that the solutions for recurrent-style models are quite different of that from parameter flexible ones.</p>
<p>Figure 1 :
1
Figure1: Architecture of our model for language model pre-training.We only show the update of token node h i and sentence-level node g for brevity.</p>
<p>Figure 2
2
Figure 2 compares the ways of hidden states generations of our model with RNN-based [42, 43], CNN-based [37, 29], and Transformer-based [2, 3, 5] models.</p>
<p>Figure 2 :
2
Figure 2: Comparison between different model architectures for hidden states generation in each layer.</p>
<p>Figure 3 :
3
Figure 3: Performance of mask language modeling with different settings.Left(a): layers; Mid(b): hidden size; Right(c): removing position embedding, removing layer normalization and adding dropout in each layer.</p>
<p>Figure 4 :
4
Figure 4: Illustrations of fine-tuning BERT and our model on different tasks.Left: single sentence tasks (e.g., Sentiment Analysis); Right: sentence pair tasks (e.g., Multi-Gerne Natural Language Inference).</p>
<p>Figure 5 :
5
Figure 5: Comparison between models for computing long sequences.</p>
<p>Figure 6 :
6
Figure 6: Cosine similarity distributions for outputs of BERT, RoBERTa, ALBERT and our model.Top: Token vectors; Bottom: Sentence vectors.</p>
<p>Figure 7 :
7
Figure 7: Hidden state transitions from layer to layer.Left: Token representations.Right: Sentence representations.</p>
<p>Token Representation.For token node update, our model is similar to CNN in the way of integrating local context, despite that CNN uses different kernels for capturing information from different distances.In a higher layer of both architectures, the reception field for each tokens become larger (i.e, each token can receive long-range context).Different from CNN, we explicitly model sentencelevel information as a feature for each token, which provides global information.
........................
[59]ence Representation.Transformer-based models (e.g., BERT, RoBERTa) use a special symbol [CLS], attached at the beginning of a sentence, as the sentence representation, which is treated the same to other tokens as model inputs for calculating hidden representation.Recent work[59]</p>
<p>Table 2 :
2
Baseline models.CLM: casual language modeling.KD: knowledge distillation.SOP: sentence order prediction.Mac: MLM as correction.wwm: whole word masking.ext: external training data.
Positive / NegativeEntailment / Neutral / Contradiction[CLS]...[SEP][CLS]...[SEP]...[SEP]Positive / NegativeEntailment / Neutral / Contradiction<s>...</s><s>...</s>...</s></p>
<p>Table 3 :
3
Results on GLUE benchmark dev sets.Scores for CoLA are Matthews correlation, others are reported by the accuracy.
ModelTNEWS IFLYTEK CSL LCQMC THUCNews CSCAvg.BERT-base [2]56.1459.6781.4087.8995.3592.58 78.83BERT-wwm [63]56.4759.7181.2387.9395.2893.00 78.93BERT-wwm-ext  † [63]57.3559.9080.8688.0595.4393.00 79.09RoBERTa-wwm  † [67]57.2959.2981.1688.4195.1993.25 79.09RoBERTa-wwm-ext  † [63]57.0960.7181.8088.6895.6993.33 79.55ALBERT-large [5]55.6958.3680.4688.2793.5291.25 77.92MacBERT  † [64]57.5059.3681.8389.1895.7493.33 79.49Ours57.5660.1080.7386.0695.1793.08 78.78</p>
<p>Table 4 :
4
Results on Chinese tasks dev sets, and scores are reported by the accuracy.</p>
<p>Table 5 :
5
Time cost (second) during inference for different architectures.Numbers in the parentheses denote the speedup compared to RoBERT-base.
40 60 80 100 CPU RuntimeT5-base DeBERTa-base Performer (m=256) Linformer (k=2048) Longformer-base-4096 Reformer Ours201k 2k 3k 4k 5k 6k 7k 8k 9k 10k Sentence Length 0</p>
<p>Table 6 :
6
find many attention heads generate similar attention matrices in Principal component of features (k in Eq. 6) according to different amount of information retained.Averaged results over different sentences are reported.
LengthModel90%92%94%96%98%BERT48.98 53.90 59.98 67.70 79.34n=100RoBERTa 50.50 55.24 61.06 68.62 79.54 ALBERT 51.26 56.18 62.20 69.96 82.30Ours52.56 58.42 65.58 74.58 87.18BERT103.9 118.6 137.8 165.1 212.6n=300RoBERTa 109.7 124.3 143.1 169.4 214.2 ALBERT 102.7 117.5 137.0 165.7 217.7Ours119.3 136.4 158.1 187.9 237.00 2 4 6 8 10 12 14 16 18 20 22 24
We release the code at https://github.com/ylwangy/slstm_pytorch.
For example, Reformer conducted experiments on character-level language modeling and synthetic datasets, Longformer utilizes the sparse attention on long context question and summarization.
Following Liu et al.[3] and Izsak et al.[58], we do not use sentence-level objective such as next sentence prediction (NSP).
Because our model encodes text from both forward and backward direction simultaneously, we consider the cloze-style (masked) language modeling rather than the left-to-right (causal) language modeling.
We use baseline checkpoints downloaded from https://allenai.org/allennlp/software/elmo and https://huggingface.co/models.
https://github.com/pytorch/fairseq
In this section, we keep all models with the same layers and output dimensions for a fair comparison. Specifically, we re-train our model with 24 layer and 1024 hidden sizes, compared with the large version of BERT, RoBERTa, and ALBERT.
Conclusion and Future WorkWe investigated a graph recurrent network for large-scale language model pre-training.Our model does not rely on the self-attention mechanism in Transformer, retaining linear computational complexity with respect to the sequence length.Results on language modeling and downstream tasks in both English and Chinese languages show that the inference time can be largely reduced while without much accuracy loss.Also, our model outputs show more diversity and less feature redundancy than Transformer-based ones.For future work, we will study our model for seq2seq-style pre-training as in BART or T5, exploring the applications to generation tasks such as machine translation.
Language models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proc. of NAACL. of NAACL2019</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv e-prints. July 2019</p>
<p>Xlnet: Generalized autoregressive pretraining for language understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, Quoc V Le, Advances in NeurIPS. 2019</p>
<p>ALBERT: A lite BERT for self-supervised learning of language representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, Proc. of ICLR. of ICLR2020</p>
<p>BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, Proc. of ACL. of ACL2020</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 211402020</p>
<p>Deberta: decoding-enhanced bert with disentangled attention. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, Proc. of ICLR. of ICLR2021</p>
<p>ELECTRA: Pre-training text encoders as discriminators rather than generators. Kevin Clark, Minh-Thang Luong, Quoc V Le, Christopher D Manning, Proc. of ICLR. of ICLR2020</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Ilya Sutskever, and Dario Amodei. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020Advances in NeurIPS</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Proc. of EMNLP Workshop. of EMNLP Workshop2018</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Advances in NeurIPS. 2019</p>
<p>SQuAD: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, Proc. of EMNLP. of EMNLP2016</p>
<p>Know what you don't know: Unanswerable questions for SQuAD. Pranav Rajpurkar, Robin Jia, Percy Liang, Proc. of ACL. of ACL2018</p>
<p>Teaching machines to read and comprehend. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom, Advances in NeurIPS. 2015</p>
<p>Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. Shashi Narayan, Shay B Cohen, Mirella Lapata, Proc. of EMNLP. of EMNLP2018</p>
<p>Pre-trained dialogue generation model with discrete latent variable. Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Plato, Proc. of ACLs. of ACLs2020</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Language models for dialog applications. 2022arXiv preprint</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł Kaiser, Illia Polosukhin, Advances in NeurIPS. 2017</p>
<p>End-to-end object detection with transformers. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, Proc. of ECCV. of ECCV2020</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, Proc. of ICLR. of ICLR2021</p>
<p>Training data-efficient image transformers &amp; distillation through attention. Matthieu Hugo Touvron, Matthijs Cord, Francisco Douze, Alexandre Massa, Herve Sablayrolles, Jegou, Proc. of ICML. of ICML2021</p>
<p>Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations. Alexei Baevski, Yuhao Zhou, Advances in Neural Information Processing Systems. 2020</p>
<p>Pushmeet Kohli, and Demis Hassabis. Highly accurate protein structure prediction with AlphaFold. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, Alex Bridgland, Clemens Meyer, A A Simon, Andrew J Kohl, Andrew Ballard, Bernardino Cowie, Stanislav Romera-Paredes, Rishub Nikolov, Jonas Jain, Trevor Adler, Stig Back, David Petersen, Ellen Reiman, Michal Clancy, Martin Zielinski, Michalina Steinegger, Tamas Pacholska, Sebastian Berghammer, David Bodenstein, Oriol Silver, Andrew W Vinyals, Koray Senior, Kavukcuoglu, Nature. 59678732021</p>
<p>Energy and policy considerations for deep learning in NLP. Emma Strubell, Ananya Ganesh, Andrew Mccallum, Proc. of ACL. of ACL2019</p>
<p>Pay less attention with lightweight and dynamic convolutions. Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, Michael Auli, Proc. of ICLR. of ICLR2019</p>
<p>Attention is not all you need: pure attention loses rank doubly exponentially with depth. Yihe Dong, Jean-Baptiste Cordonnier, Andreas Loukas, Proc. of ICML. Marina Meila, Tong Zhang, of ICML2021</p>
<p>Synthesizer: Rethinking self-attention for transformer models. Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, Che Zheng, Proc. of ICML. Marina Meila, Tong Zhang, of ICML2021</p>
<p>Are pretrained convolutions better than pretrained transformers?. Yi Tay, Mostafa Dehghani, Jai Prakash Gupta, Vamsi Aribandi, Dara Bahri, Zhen Qin, Donald Metzler, Proc. of ACL-IJCNLP. of ACL-IJCNLP2021</p>
<p>. Hanxiao Liu, Zihang Dai, David R So, Quoc V Le, arXiv:2105.08050May 2021Pay Attention to MLPs. arXiv e-prints</p>
<p>Piotr Hugo Touvron, Mathilde Bojanowski, Matthieu Caron, Alaaeldin Cord, Edouard El-Nouby, Gautier Grave, Armand Izacard, Gabriel Joulin, Jakob Synnaeve, Hervé Verbeek, Jégou, Resmlp, arXiv:2105.03404Feedforward networks for image classification with data-efficient training. May 2021</p>
<p>MLP-Mixer: An all-MLP Architecture for Vision. Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, Alexey Dosovitskiy, arXiv:2105.01601May 2021</p>
<p>Convolutional neural networks on graphs with fast localized spectral filtering. Michaël Defferrard, Xavier Bresson, Pierre Vandergheynst, Advances in NeurIPS. 2016</p>
<p>Graph convolutional encoders for syntax-aware neural machine translation. Jasmijn Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, Khalil Sima'an, Proc. of EMNLP. of EMNLP2017</p>
<p>Jointly multiple events extraction via attentionbased graph information aggregation. Xiao Liu, Zhunchen Luo, Heyan Huang, Proc. of EMNLP. of EMNLP2018</p>
<p>Relational graph attention network for aspect-based sentiment analysis. Kai Wang, Weizhou Shen, Yunyi Yang, Xiaojun Quan, Rui Wang, Proc. of ACL. of ACL2020</p>
<p>Convolutional sequence to sequence learning. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N Dauphin, Proc. of ICML. of ICML2017</p>
<p>Semi-supervised classification with graph convolutional networks. Thomas N Kipf, Max Welling, Proc. of ICLR. of ICLR2017</p>
<p>Graph attention networks. Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio, Proc. of ICLR. of ICLR2018</p>
<p>Sentence-state LSTM for text representation. Yue Zhang, Qi Liu, Linfeng Song, Proc. of ACL. of ACL2018</p>
<p>A graph-to-sequence model for AMR-to-text generation. Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel Gildea, Proc. of ACL. of ACL2018</p>
<p>Learning generic context embedding with bidirectional LSTM. Oren Melamud, Jacob Goldberger, Ido Dagan, Proc. of CoNLL. of CoNLL20162</p>
<p>Deep contextualized word representations. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, Proc. of NAACL. of NAACL2018</p>
<p>Pre-trained models: Past, present and future. Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Yuan Yao, Ao Zhang, Liang Zhang, AI Open. 22021</p>
<p>Long short-term memory. Sepp Hochreiter, Jürgen Schmidhuber, Neural computation. 981997</p>
<p>Fnet: Mixing tokens with fourier transforms. James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon, arXiv:2105.038242021arXiv preprint</p>
<p>Encoding sentences with graph convolutional networks for semantic role labeling. Diego Marcheggiani, Ivan Titov, Proc. of EMNLP. of EMNLP2017</p>
<p>Exploiting rich syntactic information for semantic parsing with graph-to-sequence model. Kun Xu, Lingfei Wu, Zhiguo Wang, Mo Yu, Liwei Chen, Vadim Sheinin, Proc. of EMNLP. of EMNLP2018</p>
<p>Reformer: The efficient transformer. Nikita Kitaev, Lukasz Kaiser, Anselm Levskaya, Proc. of ICLR. of ICLR2020</p>
<p>Longformer: The long-document transformer. Iz Beltagy, Matthew E Peters, Arman Cohan, arXiv:2004.051502020</p>
<p>Linformer: Self-Attention with Linear Complexity. Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, Hao Ma, arXiv:2006.04768June 2020arXiv e-prints</p>
<p>Rethinking Attention with Performers. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, Adrian Weller, arXiv:2009.14794September 2020arXiv e-prints</p>
<p>Random feature attention. Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, Lingpeng Kong, Proc. of ICLR. of ICLR2021</p>
<p>Efficient transformers: A survey. Yi Tay, Mostafa Dehghani, Dara Bahri, Donald Metzler, arXiv:2009.067322020arXiv preprint</p>
<p>Neural machine translation of rare words with subword units. Rico Sennrich, Barry Haddow, Alexandra Birch, Proc. of ACL. of ACL2016</p>
<p>. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton, arXiv:1607.064502016Layer normalization. arXiv preprint</p>
<p>SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. Taku Kudo, John Richardson, Proc. of EMNLP: System Demonstrations. of EMNLP: System Demonstrations2018</p>
<p>How to train BERT with an academic budget. Peter Izsak, Moshe Berchansky, Omer Levy, Proc. of EMNLP. of EMNLP2021</p>
<p>Rethinking positional encoding in language pre-training. Guolin Ke, Di He, Tie-Yan Liu, Proc. of ICLR. of ICLR2021</p>
<p>Universal transformers. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, Lukasz Kaiser, Proc. of ICLR. of ICLR2019</p>
<p>Pointer sentinel mixture models. Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher, CoRR, abs/1609.078432016</p>
<p>Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. Yukun Zhu, Ryan Kiros, Richard S Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler, IEEE International Conference on Computer Vision. 2015</p>
<p>Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, Guoping Hu, arXiv:1906.08101Pre-Training with Whole Word Masking for Chinese BERT. arXiv e-prints. June 2019</p>
<p>Revisiting pre-trained models for Chinese natural language processing. Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, Guoping Hu, Findings of EMNLP. 2020</p>
<p>fairseq: A fast, extensible toolkit for sequence modeling. Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli, Proc. of NAACL: Demonstrations. of NAACL: Demonstrations2019</p>
<p>Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf, ArXiv, abs/1910.011082019</p>
<p>Liang Xu, Xuanwei Zhang, Qianqian Dong, arXiv:2003.01355CLUECorpus2020: A Large-scale Chinese Corpus for Pre-training Language Model. arXiv e-prints. March 2020</p>
<p>CLUE: A Chinese language understanding evaluation benchmark. Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang, Weijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang Yang, Kyle Richardson, Zhenzhong Lan, Proc. of COLING. of COLING2020</p>
<p>LCQMC:a large-scale Chinese question matching corpus. Xin Liu, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Dongfang Li, Buzhou Tang, Proc. of COLING. of COLING2018</p>
<p>Scalable term selection for text categorization. Jingyang Li, Maosong Sun, Proc. of EMNLP-CoNLL. of EMNLP-CoNLL2007</p>
<p>An empirical study of sentiment analysis for chinese documents. Songbo Tan, Jin Zhang, Expert Systems with Applications. 3442008</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Gugger, Proc. of EMNLP: System Demonstrations. Mariama Drame, Quentin Lhoest, Alexander Rush, of EMNLP: System Demonstrations2020</p>
<p>How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT-2 embeddings. Kawin Ethayarajh, Proc. of EMNLP-IJCNLP. of EMNLP-IJCNLP2019</p>
<p>Isotropy in the contextual embedding space: Clusters and manifolds. Xingyu Cai, Jiaji Huang, Yuchen Bian, Kenneth Church, Proc. of ICLR. of ICLR2020</p>
<p>Sentence-BERT: Sentence embeddings using Siamese BERT-networks. Nils Reimers, Iryna Gurevych, Proc. of EMNLP-IJCNLP. of EMNLP-IJCNLP2019</p>
<p>Revisiting representation degeneration problem in language modeling. Zhong Zhang, Chongming Gao, Cong Xu, Rui Miao, Qinli Yang, Junming Shao, Findings of EMNLP. 2020</p>
<p>Isobn: Fine-tuning BERT with isotropic batch normalization. Wenxuan Zhou, Bill Yuchen Lin, Xiang Ren, Proc. of AAAI. of AAAI2021</p>
<p>Representation degeneration problem in training natural language generation models. Jun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, Tie-Yan Liu, Proc. of ICLR. of ICLR2019</p>
<p>A cluster-based approach for improving isotropy in contextual embedding space. Sara Rajaee, Mohammad Taher, Pilehvar , Proc. of ACL-IJCNLP. of ACL-IJCNLP2021</p>
<p>What does BERT look at? an analysis of BERT's attention. Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D Manning, Proc. of the ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. of the ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP2019</p>
<p>Revealing the dark secrets of BERT. Olga Kovaleva, Alexey Romanov, Anna Rogers, Anna Rumshisky, Proc. of EMNLP-IJCNLP. of EMNLP-IJCNLP2019</p>
<p>A mixture of h -1 heads is better than h heads. Hao Peng, Roy Schwartz, Dianqi Li, Noah A Smith, Proc. of ACL. of ACL2020</p>
<p>Analyzing redundancy in pretrained transformer models. Fahim Dalvi, Hassan Sajjad, Nadir Durrani, Yonatan Belinkov, Proc. of EMNLP. of EMNLP2020</p>            </div>
        </div>

    </div>
</body>
</html>
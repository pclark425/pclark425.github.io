<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1706 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1706</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1706</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-270357403</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.05132v3.pdf" target="_blank">3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination</a></p>
                <p><strong>Paper Abstract:</strong> The integration of language and 3D perception is crucial for embodied agents and robots that comprehend and interact with the physical world. While large language models (LLMs) have demonstrated impressive language understanding and generation capabilities, their adaptation to 3D environments (3D-LLMs) remains in its early stages. A primary challenge is a lack of large-scale datasets with dense grounding between language and 3D scenes. We introduce 3D-GRAND, a pioneering large-scale dataset comprising 40,087 household scenes paired with 6.2 million densely-grounded scene-language instructions. Our results show that instruction tuning with 3D-GRAND significantly enhances grounding capabilities and reduces hallucinations in 3D-LLMs. As part of our contributions, we propose a comprehensive benchmark 3D-POPE to systematically evaluate hallucination in 3D-LLMs, enabling fair comparisons of models. Our experiments highlight a scaling effect between dataset size and 3D-LLM performance, emphasizing the importance of large-scale 3D-text datasets for embodied AI research. Our results demonstrate early signals for effective sim-to-real transfer, indicating that models trained on large synthetic data can perform well on real-world 3D scans. Through 3D-GRAND and 3D-POPE, we aim to equip the embodied AI community with resources and insights to lead to more reliable and better-grounded 3D-LLMs.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1706.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1706.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3D-GRAND (Llama-2 LoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2 LoRA finetuned with 3D-GRAND</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative Llama-2 based 3D-LLM finetuned via LoRA on the million-scale, densely-grounded 3D-GRAND dataset to perform 3D-grounded object reference, scene description, and QA; shows sim-to-real zero-shot transfer from synthetic scenes to ScanNet.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Llama-2 (LoRA finetuned) / 3D-GRAND model</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Generative Llama-2 backbone adapted to 3D tasks via LoRA finetuning. Inputs are object-centric scene graphs (object category, centroid x,y,z, extent) plus text instruction; outputs are natural-language responses augmented with dense grounding tags that link noun phrases to object IDs. At inference uses Mask3D proposals (top-40) for bounding box candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Pretrained large language model on text corpora (base Llama-2 pretraining not specified in this paper); subsequently finetuned on densely-grounded synthetic 3D-text.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Finetuning data: 3D-GRAND — 40,087 synthetic indoor scenes (3D-FRONT and Structured3D) and 6.2M densely-grounded scene-language instructions. For experiments: grounded object reference model trained on 234,791 densely-grounded 3D-text pairs (3D-FRONT train split); grounded QA model trained on a subset of 200k grounded existence QAs. The paper does not specify the original Llama-2 pretraining corpus within this text.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>3D grounding tasks (ScanRefer object localization; 3D-POPE object-existence QA; grounded object reference; grounded scene description; grounded QA)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Text-based grounding and understanding tasks over 3D indoor scenes: generate grounded referring expressions that uniquely identify objects (with phrase-to-object tags), produce scene descriptions linked to object IDs, and answer yes/no existence and attribute questions about objects in a scene. Training on synthetic 3D scenes; evaluation zero-shot on real ScanNet scans (ScanRefer and 3D-POPE benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Not applicable / no action tokens — pretraining and finetuning are on language and grounded annotation pairs (instructions, descriptions, QA).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Not applicable — the model produces textual outputs and grounding tags, not continuous motor commands; it is not used as a low-level control policy in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Not applicable for motor control. For grounding, noun phrases in generated text are linked to object identifiers; at inference grounding is realized by mapping those object IDs to Mask3D bounding box proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Object-centric scene graphs derived from 3D point clouds (categories, centroids, extents); Mask3D instance proposals for inference; point-cloud and rendered images used in the annotation pipeline (GPT-4V) but the model input is structured object-centric context rather than raw pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>3D-POPE (random sampling): precision 93.34% and accuracy 89.12% (reported for their model trained on 3D-GRAND). ScanRefer: achieved best Acc@0.25 among compared models and surpassed previous 3D-LLM by +7.7% Acc@0.25 IoU in zero-shot evaluation (model had never seen ScanNet during training).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Not directly reported as a single number; ablations indicate models trained without dense grounding perform worse (reduced grounding accuracy and higher hallucination) but explicit numeric baselines for the same architecture without 3D-GRAND are not provided in a single comparative metric in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Finetuning regime: LoRA finetune for 10k steps, batch size 96 on 12 NVIDIA A40 GPUs (≈48 hours). Training data sizes used in experiments: 234,791 object-reference pairs and a 200k subset of grounded QA. The paper does not provide sample counts/episodes to reach specific performance thresholds (no explicit sample-efficiency curves with episode counts).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Not quantitatively reported. Authors state densely-grounded training scales better and reduces hallucination, but no numeric sample-efficiency factor (e.g., x-times fewer samples) is given.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Large-scale densely-grounded synthetic 3D-text (noun-level one-to-one grounding), object-centric inputs (scene graphs), and data scale (millions of grounded annotations) — these improve semantic alignment between language and 3D structure and enable sim-to-real zero-shot generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Remaining failures on harder negative sampling (Popular and Adversarial splits of 3D-POPE) indicate limits from adversarial co-occurrence confusions and the model's relatively limited visual/3D embedding richness (model is primarily text-based with minimal visual feature embedding), as well as domain gaps between synthetic scenes and some real-world distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LoRA finetuning of a text-pretrained LLM on million-scale, noun-level densely-grounded 3D-text yields large improvements in grounding accuracy and substantial reductions in object hallucination, and can transfer zero-shot from synthetic 3D scenes to real ScanNet scans; noun-level dense grounding and scale of data are critical drivers, while mapping language to low-level embodied control was not addressed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1706.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1706.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3D-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3D-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that integrates 3D point clouds and features into large language models to enable tasks such as captioning, question answering, and navigation in 3D environments; used in this paper as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>3d-llm: Injecting the 3d world into large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>3D-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>An approach that injects 3D point cloud information and 3D features into an LLM to perform 3D perception-language tasks (captioning, QA, navigation) — treated as a baseline in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Not specified in this paper (implied: LLM pretraining on text plus 3D-task-specific data).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Captioning, question answering, navigation and other 3D perception-language tasks</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Perception and language tasks over 3D scenes using point clouds and 3D features; specifics are not detailed in this paper beyond task categories.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>3D point clouds and 3D feature integration.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Reported qualitatively in this paper as failing 3D-POPE: '3D-LLM almost always produces yes responses to any question' and thus exhibits high object hallucination; performs poorly on object-existence probing.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Not applicable / not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Not detailed in this paper; architecture integrates 3D features into LLM which is intended to provide grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>High object-hallucination tendency (biased 'Yes' responses); lack of dense noun-level grounding in training data and resulting failure on object-existence probing (3D-POPE).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>As used in this paper as a baseline, 3D-LLM exhibits severe object-hallucination on 3D-POPE, highlighting the need for densely grounded 3D-text data to calibrate LLMs for 3D reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1706.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1706.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LEO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LEO (embodied multi-modal generalist agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embodied multi-modal generalist agent that performs perception, grounding, reasoning, planning, and action in 3D environments; referenced and evaluated as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An embodied generalist agent in 3d world</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>LEO</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>An embodied multi-modal generalist agent designed to handle perception, grounding, reasoning, planning and action in 3D worlds; included as a baseline comparator in experiments reported by this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Perception, grounding, reasoning, planning and action in 3D environments (general embodied tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>General embodied-agent tasks in 3D worlds; specifics not enumerated in this paper beyond task categories and that it is an embodied generalist.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Multi-modal 3D perception (implied: point clouds, imagery), though paper does not give details.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>On 3D-POPE LEO tends to answer 'Yes' frequently; its precision is similar to the random baseline, indicating substantial object-hallucination in existence probing according to this paper's evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Tendency to answer 'Yes' frequently and exhibit hallucination on object-existence queries; suggests insufficient grounding relative to 3D-GRAND-trained model.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LEO, as evaluated here, shows high hallucination on 3D-POPE (answers 'Yes' often) and does not match the reduced-hallucination performance achieved by training on densely-grounded 3D-GRAND data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1706.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1706.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language-action model that transfers web knowledge to robotic control (cited in this paper as related work on transferring language/web-pretrained models to embodied action).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RT-2: Vision-language-action models transfer web knowledge to robotic control.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A family of vision-language-action models designed to transfer knowledge from web-scale sources (language and vision) to robotic control; cited as an example of language/web-pretraining being applied to embodied control.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Web knowledge / vision-language data (paper only cites title; detailed pretraining corpus not described in this text).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified within this paper beyond the implication of web-scale knowledge in the title/citation.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Robotic control / embodied manipulation and control tasks (as implied by the cited title).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Robotic control tasks informed by vision-language pretraining; this paper only cites RT-2 in related work and does not detail the environments or objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Not specified in this paper (implied multi-modal perception: vision + language).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Not reported in this paper (only cited).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Not detailed in this paper; cited as an example that web/language pretraining can be leveraged for embodied control.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned in related work to illustrate prior approaches that transfer language/web-pretrained models to embodied robotic control; details must be obtained from the RT-2 paper itself.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>RT-2: Vision-language-action models transfer web knowledge to robotic control. <em>(Rating: 2)</em></li>
                <li>3d-llm: Injecting the 3d world into large language models. <em>(Rating: 2)</em></li>
                <li>An embodied generalist agent in 3d world <em>(Rating: 2)</em></li>
                <li>Chat-3d v2: Bridging 3d scene and large language models with object identifiers <em>(Rating: 2)</em></li>
                <li>SceneVerse: Scaling 3d vision-language learning for grounded scene understanding <em>(Rating: 2)</em></li>
                <li>3d-vista: Pre-trained transformer for 3d vision and text alignment <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1706",
    "paper_id": "paper-270357403",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "3D-GRAND (Llama-2 LoRA)",
            "name_full": "Llama-2 LoRA finetuned with 3D-GRAND",
            "brief_description": "A generative Llama-2 based 3D-LLM finetuned via LoRA on the million-scale, densely-grounded 3D-GRAND dataset to perform 3D-grounded object reference, scene description, and QA; shows sim-to-real zero-shot transfer from synthetic scenes to ScanNet.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "Llama-2 (LoRA finetuned) / 3D-GRAND model",
            "model_agent_description": "Generative Llama-2 backbone adapted to 3D tasks via LoRA finetuning. Inputs are object-centric scene graphs (object category, centroid x,y,z, extent) plus text instruction; outputs are natural-language responses augmented with dense grounding tags that link noun phrases to object IDs. At inference uses Mask3D proposals (top-40) for bounding box candidates.",
            "pretraining_data_type": "Pretrained large language model on text corpora (base Llama-2 pretraining not specified in this paper); subsequently finetuned on densely-grounded synthetic 3D-text.",
            "pretraining_data_details": "Finetuning data: 3D-GRAND — 40,087 synthetic indoor scenes (3D-FRONT and Structured3D) and 6.2M densely-grounded scene-language instructions. For experiments: grounded object reference model trained on 234,791 densely-grounded 3D-text pairs (3D-FRONT train split); grounded QA model trained on a subset of 200k grounded existence QAs. The paper does not specify the original Llama-2 pretraining corpus within this text.",
            "embodied_task_name": "3D grounding tasks (ScanRefer object localization; 3D-POPE object-existence QA; grounded object reference; grounded scene description; grounded QA)",
            "embodied_task_description": "Text-based grounding and understanding tasks over 3D indoor scenes: generate grounded referring expressions that uniquely identify objects (with phrase-to-object tags), produce scene descriptions linked to object IDs, and answer yes/no existence and attribute questions about objects in a scene. Training on synthetic 3D scenes; evaluation zero-shot on real ScanNet scans (ScanRefer and 3D-POPE benchmarks).",
            "action_space_text": "Not applicable / no action tokens — pretraining and finetuning are on language and grounded annotation pairs (instructions, descriptions, QA).",
            "action_space_embodied": "Not applicable — the model produces textual outputs and grounding tags, not continuous motor commands; it is not used as a low-level control policy in this work.",
            "action_mapping_method": "Not applicable for motor control. For grounding, noun phrases in generated text are linked to object identifiers; at inference grounding is realized by mapping those object IDs to Mask3D bounding box proposals.",
            "perception_requirements": "Object-centric scene graphs derived from 3D point clouds (categories, centroids, extents); Mask3D instance proposals for inference; point-cloud and rendered images used in the annotation pipeline (GPT-4V) but the model input is structured object-centric context rather than raw pixels.",
            "transfer_successful": true,
            "performance_with_pretraining": "3D-POPE (random sampling): precision 93.34% and accuracy 89.12% (reported for their model trained on 3D-GRAND). ScanRefer: achieved best Acc@0.25 among compared models and surpassed previous 3D-LLM by +7.7% Acc@0.25 IoU in zero-shot evaluation (model had never seen ScanNet during training).",
            "performance_without_pretraining": "Not directly reported as a single number; ablations indicate models trained without dense grounding perform worse (reduced grounding accuracy and higher hallucination) but explicit numeric baselines for the same architecture without 3D-GRAND are not provided in a single comparative metric in the main text.",
            "sample_complexity_with_pretraining": "Finetuning regime: LoRA finetune for 10k steps, batch size 96 on 12 NVIDIA A40 GPUs (≈48 hours). Training data sizes used in experiments: 234,791 object-reference pairs and a 200k subset of grounded QA. The paper does not provide sample counts/episodes to reach specific performance thresholds (no explicit sample-efficiency curves with episode counts).",
            "sample_complexity_without_pretraining": "Not reported.",
            "sample_complexity_gain": "Not quantitatively reported. Authors state densely-grounded training scales better and reduces hallucination, but no numeric sample-efficiency factor (e.g., x-times fewer samples) is given.",
            "transfer_success_factors": "Large-scale densely-grounded synthetic 3D-text (noun-level one-to-one grounding), object-centric inputs (scene graphs), and data scale (millions of grounded annotations) — these improve semantic alignment between language and 3D structure and enable sim-to-real zero-shot generalization.",
            "transfer_failure_factors": "Remaining failures on harder negative sampling (Popular and Adversarial splits of 3D-POPE) indicate limits from adversarial co-occurrence confusions and the model's relatively limited visual/3D embedding richness (model is primarily text-based with minimal visual feature embedding), as well as domain gaps between synthetic scenes and some real-world distributions.",
            "key_findings": "LoRA finetuning of a text-pretrained LLM on million-scale, noun-level densely-grounded 3D-text yields large improvements in grounding accuracy and substantial reductions in object hallucination, and can transfer zero-shot from synthetic 3D scenes to real ScanNet scans; noun-level dense grounding and scale of data are critical drivers, while mapping language to low-level embodied control was not addressed.",
            "uuid": "e1706.0",
            "source_info": {
                "paper_title": "3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "3D-LLM",
            "name_full": "3D-LLM",
            "brief_description": "Prior work that integrates 3D point clouds and features into large language models to enable tasks such as captioning, question answering, and navigation in 3D environments; used in this paper as a baseline.",
            "citation_title": "3d-llm: Injecting the 3d world into large language models.",
            "mention_or_use": "use",
            "model_agent_name": "3D-LLM",
            "model_agent_description": "An approach that injects 3D point cloud information and 3D features into an LLM to perform 3D perception-language tasks (captioning, QA, navigation) — treated as a baseline in this paper's experiments.",
            "pretraining_data_type": "Not specified in this paper (implied: LLM pretraining on text plus 3D-task-specific data).",
            "pretraining_data_details": "Not specified in this paper.",
            "embodied_task_name": "Captioning, question answering, navigation and other 3D perception-language tasks",
            "embodied_task_description": "Perception and language tasks over 3D scenes using point clouds and 3D features; specifics are not detailed in this paper beyond task categories.",
            "action_space_text": "Not specified in this paper.",
            "action_space_embodied": "Not specified in this paper.",
            "action_mapping_method": "Not specified in this paper.",
            "perception_requirements": "3D point clouds and 3D feature integration.",
            "transfer_successful": false,
            "performance_with_pretraining": "Reported qualitatively in this paper as failing 3D-POPE: '3D-LLM almost always produces yes responses to any question' and thus exhibits high object hallucination; performs poorly on object-existence probing.",
            "performance_without_pretraining": "Not applicable / not reported.",
            "sample_complexity_with_pretraining": "Not reported in this paper.",
            "sample_complexity_without_pretraining": "Not reported.",
            "sample_complexity_gain": null,
            "transfer_success_factors": "Not detailed in this paper; architecture integrates 3D features into LLM which is intended to provide grounding.",
            "transfer_failure_factors": "High object-hallucination tendency (biased 'Yes' responses); lack of dense noun-level grounding in training data and resulting failure on object-existence probing (3D-POPE).",
            "key_findings": "As used in this paper as a baseline, 3D-LLM exhibits severe object-hallucination on 3D-POPE, highlighting the need for densely grounded 3D-text data to calibrate LLMs for 3D reasoning.",
            "uuid": "e1706.1",
            "source_info": {
                "paper_title": "3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "LEO",
            "name_full": "LEO (embodied multi-modal generalist agent)",
            "brief_description": "An embodied multi-modal generalist agent that performs perception, grounding, reasoning, planning, and action in 3D environments; referenced and evaluated as a baseline.",
            "citation_title": "An embodied generalist agent in 3d world",
            "mention_or_use": "use",
            "model_agent_name": "LEO",
            "model_agent_description": "An embodied multi-modal generalist agent designed to handle perception, grounding, reasoning, planning and action in 3D worlds; included as a baseline comparator in experiments reported by this paper.",
            "pretraining_data_type": "Not specified in this paper.",
            "pretraining_data_details": "Not specified in this paper.",
            "embodied_task_name": "Perception, grounding, reasoning, planning and action in 3D environments (general embodied tasks)",
            "embodied_task_description": "General embodied-agent tasks in 3D worlds; specifics not enumerated in this paper beyond task categories and that it is an embodied generalist.",
            "action_space_text": "Not specified in this paper.",
            "action_space_embodied": "Not specified in this paper.",
            "action_mapping_method": "Not specified in this paper.",
            "perception_requirements": "Multi-modal 3D perception (implied: point clouds, imagery), though paper does not give details.",
            "transfer_successful": false,
            "performance_with_pretraining": "On 3D-POPE LEO tends to answer 'Yes' frequently; its precision is similar to the random baseline, indicating substantial object-hallucination in existence probing according to this paper's evaluation.",
            "performance_without_pretraining": "Not reported.",
            "sample_complexity_with_pretraining": "Not reported.",
            "sample_complexity_without_pretraining": "Not reported.",
            "sample_complexity_gain": null,
            "transfer_success_factors": "Not detailed in this paper.",
            "transfer_failure_factors": "Tendency to answer 'Yes' frequently and exhibit hallucination on object-existence queries; suggests insufficient grounding relative to 3D-GRAND-trained model.",
            "key_findings": "LEO, as evaluated here, shows high hallucination on 3D-POPE (answers 'Yes' often) and does not match the reduced-hallucination performance achieved by training on densely-grounded 3D-GRAND data.",
            "uuid": "e1706.2",
            "source_info": {
                "paper_title": "3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "RT-2",
            "name_full": "RT-2",
            "brief_description": "A vision-language-action model that transfers web knowledge to robotic control (cited in this paper as related work on transferring language/web-pretrained models to embodied action).",
            "citation_title": "RT-2: Vision-language-action models transfer web knowledge to robotic control.",
            "mention_or_use": "mention",
            "model_agent_name": "RT-2",
            "model_agent_description": "A family of vision-language-action models designed to transfer knowledge from web-scale sources (language and vision) to robotic control; cited as an example of language/web-pretraining being applied to embodied control.",
            "pretraining_data_type": "Web knowledge / vision-language data (paper only cites title; detailed pretraining corpus not described in this text).",
            "pretraining_data_details": "Not specified within this paper beyond the implication of web-scale knowledge in the title/citation.",
            "embodied_task_name": "Robotic control / embodied manipulation and control tasks (as implied by the cited title).",
            "embodied_task_description": "Robotic control tasks informed by vision-language pretraining; this paper only cites RT-2 in related work and does not detail the environments or objectives.",
            "action_space_text": "Not specified in this paper.",
            "action_space_embodied": "Not specified in this paper.",
            "action_mapping_method": "Not specified in this paper.",
            "perception_requirements": "Not specified in this paper (implied multi-modal perception: vision + language).",
            "transfer_successful": null,
            "performance_with_pretraining": "Not reported in this paper (only cited).",
            "performance_without_pretraining": "Not reported.",
            "sample_complexity_with_pretraining": "Not reported in this paper.",
            "sample_complexity_without_pretraining": "Not reported.",
            "sample_complexity_gain": null,
            "transfer_success_factors": "Not detailed in this paper; cited as an example that web/language pretraining can be leveraged for embodied control.",
            "transfer_failure_factors": "Not discussed in this paper.",
            "key_findings": "Mentioned in related work to illustrate prior approaches that transfer language/web-pretrained models to embodied robotic control; details must be obtained from the RT-2 paper itself.",
            "uuid": "e1706.3",
            "source_info": {
                "paper_title": "3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "RT-2: Vision-language-action models transfer web knowledge to robotic control.",
            "rating": 2,
            "sanitized_title": "rt2_visionlanguageaction_models_transfer_web_knowledge_to_robotic_control"
        },
        {
            "paper_title": "3d-llm: Injecting the 3d world into large language models.",
            "rating": 2,
            "sanitized_title": "3dllm_injecting_the_3d_world_into_large_language_models"
        },
        {
            "paper_title": "An embodied generalist agent in 3d world",
            "rating": 2,
            "sanitized_title": "an_embodied_generalist_agent_in_3d_world"
        },
        {
            "paper_title": "Chat-3d v2: Bridging 3d scene and large language models with object identifiers",
            "rating": 2,
            "sanitized_title": "chat3d_v2_bridging_3d_scene_and_large_language_models_with_object_identifiers"
        },
        {
            "paper_title": "SceneVerse: Scaling 3d vision-language learning for grounded scene understanding",
            "rating": 2,
            "sanitized_title": "sceneverse_scaling_3d_visionlanguage_learning_for_grounded_scene_understanding"
        },
        {
            "paper_title": "3d-vista: Pre-trained transformer for 3d vision and text alignment",
            "rating": 2,
            "sanitized_title": "3dvista_pretrained_transformer_for_3d_vision_and_text_alignment"
        }
    ],
    "cost": 0.01961325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination
20 Mar 2025</p>
<p>Jianing Yang 
University of Michigan ♦ New York University</p>
<p>Xuweiyi Chen 
University of Michigan ♦ New York University</p>
<p>Nikhil Madaan 
University of Michigan ♦ New York University</p>
<p>Madhavan Iyengar 
University of Michigan ♦ New York University</p>
<p>Shengyi Qian 
University of Michigan ♦ New York University</p>
<p>David F Fouhey 
University of Michigan ♦ New York University</p>
<p>Joyce Chai 
University of Michigan ♦ New York University</p>
<p>Prev Sota 
University of Michigan ♦ New York University</p>
<p>3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination
20 Mar 2025B2F1DFCDDD51B11738CF1E442E985361arXiv:2406.05132v3[cs.CV]Stronger Grounding Capability 3D-GRAND: Large, Densely Grounded 3D-Text Dataset Reduced Hallucination
of models.Our experiments highlight a scaling effect between dataset size and 3D-LLM performance, emphasizing the importance of large-scale 3D-text datasets for embodied AI research.Our results demonstrate early signals for effective sim-to-real transfer, indicating that models trained on large synthetic data can perform well on real-world 3D scans.Through 3D-GRAND and 3D-POPE, we aim to equip the embodied AI community with resources and insights to lead to more reliable and better-grounded 3D-LLMs.</p>
<p>Abstract</p>
<p>The integration of language and 3D perception is crucial for embodied agents and robots that comprehend and interact with the physical world.While large language models (LLMs) have demonstrated impressive language understanding and generation capabilities, their adaptation to 3D environments (3D-LLMs) remains in its early stages.A primary challenge is a lack of large-scale datasets with dense grounding between language and 3D scenes.We introduce 3D-GRAND, a pioneering large-scale dataset comprising 40,087 household scenes paired with 6.2 million denselygrounded scene-language instructions.Our results show that instruction tuning with 3D-GRAND significantly enhances grounding capabilities and reduces hallucinations in 3D-LLMs.As part of our contributions, we propose a comprehensive benchmark 3D-POPE to systematically evaluate hallucination in 3D-LLMs, enabling fair comparisons</p>
<p>Introduction</p>
<p>Embodied Artificial Intelligence (EAI) represents a frontier in robotics and machine learning.In EAI, the integration of perception, language, and action within physical spaces is crucial for developing intelligent systems capable of meaningfully navigating and interacting with their environments.Central to this vision is the concept of grounding language in the physical world [6,9].Grounding connects abstract linguistic constructs to concrete objects in three-dimensional space, thereby enabling robots and intelligent agents to ef-fectively understand and manipulate their surroundings.</p>
<p>Recent advances in Large Language Models (LLMs) have greatly benefited Embodied Artificial Intelligence (EAI).LLMs demonstrate exceptional capabilities in understanding language instructions [52,66], perceiving the environment [3,41,46,74,83], and planning detailed actions [7,35].The primary inputs to LLMs, other than language, have been the combination of language and 2D images, categorizing these models as 2D-LLMs.The significant advancements in 2D-LLMs can be mainly attributed to their training on extensive vision-language datasets.These datasets [60,84], with billions of image and text pairs, have been instrumental in enhancing the models' understanding of visual content and its contextual relevance to text information.These datasets have provided the foundational data needed for training models excelling at integrating vision and language.Despite some progress in equipping LLMs to understand 3D scenes (3D-LLMs) [11,28,30,31,55,68,85], these models are limited by the scarcity of 3D scene and text pairs.In this work, we introduce 3D-GRAND, a pioneering million-scale dataset designed for densely-grounded 3D Instruction Tuning.</p>
<p>Recently, SceneVerse [37] concurrently introduced a large-scale 3D vision-language dataset.However, a significant limitation of this dataset is the absence of object grounding in language, which is crucial for usability in robotics tasks and reducing hallucination.Research on 2D-LLMs indicates that grounding language to 2D contexts mitigates hallucination in language models [5,40,53,56,76,80], thus enhancing the reliability and interpretability of generated responses.While 2D grounding has been extensively explored, extending these principles to 3D environments is still underdeveloped.This situation raises two critical questions:</p>
<p>(1) Is there hallucination in 3D-LLMs and if so, how severe is it?(2) Can densely-grounded data mitigate hallucination for 3D-LLMs?These questions underscore a critical need within the research community for the development of an evaluation benchmark specifically designed for 3D-LLMs and the construction of a large-scale, 3D-grounded dataset.</p>
<p>To quantify hallucination in 3D LLMs, this work introduces 3D-POPE (3D Polling-based Object Probing Evaluation).3D-POPE provides a comprehensive and standardized protocol for evaluating hallucination that enables systematic assessment and facilitates fair comparisons across 3D-LLMs, enhancing our understanding of model capabilities in object hallucination.Specifically, we pose existence questions to 3D-LLMs and evaluate their responses, as shown in Fig 1 .To evaluate the role of dense-grounding, we introduce a pioneering million-scale dataset, 3D-GRAND, for densely grounded 3D instruction tuning.3D-GRAND includes 40K household scenes paired with 6.2 million scene-language instructions, featuring dense phrase-to-object grounding.We conduct rigorous human evaluations to ensure the dataset's quality.Our results trained with 3D-GRAND highlight the dataset's effectiveness in enhancing grounding and reducing hallucination for 3D-LLMs.We highlight the effectiveness of incorporating 3D-GRAND in Fig 1  Our contributions include: (1) 3D-GRAND, the first million-scale, densely-grounded 3D-text dataset for grounded 3D Instruction Tuning.3D-GRAND includes 40K household scenes paired with 6.2M densely-grounded scenelanguage instructions.(2) 3D-POPE, a suite of benchmarks and metrics that systematically evaluate hallucination, enabling fair comparisons of future 3D-LLM models in terms of object hallucination.and (3) Findings for hallucination, grounding, and scaling that guide future research, including: (a) training with 3D-GRAND significantly reduces hallucinations particularly when the data is densely grounded; (b) densely grounded instruction tuning significantly enhances the grounding capabilities of 3D-LLMs; (c) scaling densely grounded data consistently improves grounding accuracy and reduces hallucination; and (d) models transfer from sim-toreal, providing an early signal for a low-cost and sustainable future of scaling synthetic 3D data to help on real tasks.</p>
<p>Related Work</p>
<p>Injecting 3D into LLMs.Recent advances in large language models (LLMs) have inspired work extending their capabilities to 3D environments, leading to the development of 3D-LLMs [11,55,74,85].Notable works include 3D-LLM [28], which integrates 3D point clouds and features into LLMs to enable tasks like captioning, question answering, and navigation.LEO [31] excels as an embodied multi-modal generalist agent in perception, grounding, reasoning, planning, and action in 3D environments, showing the potential of 3D-LLMs in understanding and interacting with the physical world.The most relevant work is Chat-3Dv2 [30,68], which grounds generated scene captions to objects in 3D scenes.However, Chat-3Dv2's dataset is limited to one type of 3D-text task (scene captioning) and only has 705 captions from a subset of ScanNet scenes.In 3D-GRAND, we expand this concept by diversifying 3D-text tasks and increasing data scale to million-scale.Our results show promising data scaling effects and sim-to-real transfer, paving the way for future large-scale 3D-LLM training.Object Hallucination of VLMs.While 2D VLMs have achieved impressive performance, they are prone to hallucinating objects that do not exist in the provided images, a problem known as object hallucination [12,16,58].Several methods have been suggested to mitigate the object hallucination issue, such as integrating an external object detector [79], applying visually grounded visual instruction tuning [76,80] or reinforcement learning [25,63], performing iterative refinement [82], and adapting the decoding strategies [33].To quantify and mitigate this issue, several benchmarks have been proposed.CHAIR (Caption Hallucination Assessment with Image Relevance) [58] measures the frequency of hallu-  [10] obj-refer ✗ Human 0.7K 51K Scan2Cap [13] obj-refer ✗ Human 0.7K 51K ScanEnts3D [1] obj-refer ✓ Human 0.7K 84K PhraseRefer [78] obj-refer ✓ Human 0.7K 170K ScanQA [4] answer ✗ Human 0.7K 41K SQA3D [48] question ✗ Human 0.65K cinated objects in image captions by comparing the objects mentioned to the ground truth annotations.POPE (Probing Object Hallucination Evaluation) [43] assesses a VLM's ability to identify the presence or absence of objects through yes/no probing questions.However, these studies primarily focus on 2D image-text datasets like COCO [45].In contrast, object hallucination in 3D-LLMs remains largely unexplored.Our work addresses this gap by introducing 3D-POPE, a comprehensive benchmark for evaluating object hallucination in 3D-LLMs.To the best of our knowledge, this is the first object hallucination benchmark for 3D-LLMs.</p>
<p>Grounding Datasets for 3D-LLMs.</p>
<p>In 2D, largescale datasets with grounding information have been instrumental in vision-language research.Notable examples include RefCOCO [77], which provides referring expressions for objects in COCO images [45].Additionally, 2D LLMs [40,53,56,71,76] have been trained with denselygrounded web-crawled image-text pairs.In 3D, there is a growing interest in creating datasets that pair 3D scenes with textual annotations [1,13,34,78].ScanRefer [10] pioneered this effort by contributing a dataset of ScanNet [15] scenes with referring expressions.Table 1 introduces the efforts made by the community.However, these datasets have limited grounding annotations and often focus on a single task, such as referring expression comprehension or visual question answering.In contrast, our proposed dataset, 3D-GRAND, stands out by providing 6.2 million denselygrounded scene-language instructions across a diverse set of 3D-text tasks and 40,087 household scenes.This enables a wide range of grounding tasks and facilitates the development of more reliable and better-grounded 3D-LLMs.</p>
<p>Of recent datasets, SceneVerse [34] is the most similar to ours: both are million-scale 3D grounding datasets.A key difference is that SceneVerse [34] provides sparse grounding, while 3D-GRAND is densely grounded.For instance, Sceneverse grounds "This is a big cotton sofa.It is between the window and the wooden table." to a single entity (e.g., [sofa-3]).Instead, 3D-GRAND grounds every noun phrase in the caption: "big cotton sofa", "it", "window" and "wooden table" to an entity.</p>
<p>For clarity of describing the differences between approaches and datasets, we define the grounding granularity as follows and show an example in Appendix A: Paragraphlevel set-to-set grounding: Many sentences in a long paragraph, each containing several object nouns, are linked to a set of 3D objects without clear associations from specific sentences/noun phrases to objects.Session-level many-toone grounding: Multiple sentences in one session, where each can describe several objects (targets and landmarks), are associated with one 3D object.Noun-level one-to-one grounding: Each noun phrase in each sentence is explicitly matched with one 3D object.</p>
<p>Scene Caption</p>
<p>Object Reference QA</p>
<p>SceneVerse</p>
<p>Paragraph-level set-to-set grounding Session-level many-to-one grounding No grounding 3D-GRAND Noun-level one-to-one grounding Noun-level one-to-one grounding Noun-level one-to-one grounding Additionally, the language annotations of 3D-GRAND are more trustworthy and have higher quality.Hallucination is known as one of the most common mistakes of LLMs [32,43,58].In 3D-GRAND, we use a hallucination filter to check and delete any annotations with hallucinated object IDs.This is not possible for SceneVerse since they have pure language output.3D-GRAND is also quality-checked by humans.</p>
<p>3D-POPE: A Benchmark for Evaluating Hallucination in 3D-LLMs</p>
<p>To systematically evaluate the hallucination behavior of 3D-LLMs, we introduce the 3D Polling-based Object Probing Evaluation (3D-POPE) benchmark.3D-POPE is designed to assess a model's ability to accurately identify the presence or absence of objects in a given 3D scene.Dataset.To facilitate the 3D-POPE benchmark, we curate a dedicated dataset from the ScanNet [15] dataset, utilizing the semantic classes from ScanNet200 [59].Specifically, we use the ScanNet validation set as the foundation for evaluating 3D-LLMs on the 3D-POPE benchmark.</p>
<p>Benchmark design.3D-POPE consists of triples with: a 3D scene, a posed question, and a binary answer ("Yes" or "No") indicating the presence or absence of an object (Fig. 1 middle).For balance, we maintain a 1:1 ratio of existent to nonexistent objects when constructing these triples.For negative samples (i.e., nonexistent objects), we use the following three distinct sampling strategies that are designed to challenge the model's robustness and assess its susceptibility to different levels of object hallucination.Random Sampling: Nonexistent objects are randomly selected from the set of objects not present in the 3D scene.Popular Sampling: We select the top-k most frequent objects not present in the 3D scene, where k equals the number of objects currently in the scene.Adversarial Sampling: For each positively identified object in the scene, we rank objects that are not present and have not been used as negative samples based on their frequency of co-occurrence with the positive object in the training dataset.The highest-ranking co-occurring object is selected as the adversarial sample, which differs from the original POPE [43] to avoid adversarial samples mirroring popular samples, as indoor scenes often contain similar objects.</p>
<p>Metrics.3D-POPE metrics include Precision, Recall, F1 Score, Accuracy, and Yes (%).Precision and Recall assess the model's ability to correctly affirm the presence of objects and identify the absence of objects, respectively.Precision is particularly important as it indicates the proportion of nonexisting objects generated by the 3D-LLMs.The F1 Score, combining Precision and Recall, balances the two and serves as the primary evaluation metric.Accuracy measures the proportion of correctly answered questions for "Yes" and "No" responses.Additionally, the Yes (%) metric reports the ratio of incorrect "Yes" responses to understand the model's tendencies regarding object hallucination.</p>
<p>Leaderboard.We establish a public leaderboard for the 3D-POPE benchmark, allowing researchers to submit their 3D-LLM results and compare their performance against other state-of-the-art models.The leaderboard reports the evaluation metrics for each model under the three sampling strategies, providing a transparent and standardized way to assess the hallucination performance of 3D-LLMs.</p>
<p>3D-GRAND: 3D Ground Anything Dataset</p>
<p>We introduce 3D-GRAND, a large-scale, densely-grounded 3D-text dataset designed for grounded 3D instruction tuning.We describe the data collection process, dataset statistics, and the unique features that make 3D-GRAND a valuable resource for advancing research in 3D-LLMs.3D scene collection.The majority of 3D-text research is currently based on ScanNet scenes collected from real camera scans, which are limited in scale.However, recent advancements have led to the development of numerous synthetic data generation pipelines [18,19,21,22,26,38,39,49,50,54,62,64,75].Given the scalability of these synthetic data generation pipelines, we explore the potential of using synthetic 3D scenes to enhance 3D-text understanding.Synthetic data offers significant advantages, such as lower costs and greater accessibility, making it an attractive alternative.If models trained on simulated 3D-text data can effectively transfer to real-world 3D scenes, the research community stands to benefit immensely.</p>
<p>Thus, we curate a diverse collection of 40,087 highquality 3D indoor scenes from the 3D-FRONT [24] and Structured3D [81] datasets.These datasets are chosen for their large quantities of synthetic indoor scenes with professionally designed layouts.The collection includes various room types, like living rooms, bedrooms, kitchens, office spaces, and conference rooms.We process these 3D scenes to generate per-room 3D point clouds.Details on point cloud rendering and cleaning are provided in the Appendix.Densely-grounded text annotation.The definition of densely-grounded text is that every noun phrase of object mentioned in the text should be associated with an 3D object in the 3D scene.This is illustrated in Figure 2.This is a difficult type of data to get annotations on.Early work such as ScanEnts3D [1] relies on hiring professional human annotators to obtain such annotations.The authors</p>
<p>Grounded QA: Existence</p>
<p>The living room is a blend of Korean, Japanese, Light Luxury, and Modern styles, creating an eclectic and comfortable space.</p>
<p>At the center, the <p>three-seat sofa</p>[<obj_0>] offers ample seating with its neutral beige color and soft texture.On the opposite side, the <p>tv stand</p>[<obj_1>] in seafoam green provides a stylish base for entertainment equipment... report that crowd-sourcing annotators (Amazon Mechanical Turk (AMT) [14]) were not able to reliably complete this task and they had to hire professional annotators (error rate AMT: 16%, professional: &lt;5%).Yet our human quality check shows that LLMs (GPT-4 [52]) can achieve &lt;8.2-5.6% densely-grounding error rate (see Appendix for detail).This finding is in accordance with recent studies [20,65] reporting LLMs can be human-level annotators.The accuracy of LLM-annotation provides one motivation for considering LLMs as densely grounding annotation tool.</p>
<p>Grounded Scene Description</p>
<p>The second, and perhaps more critical, motivation is the scalability of annotation.While we can potentially scale up 3D scenes using synthetic data generation pipelines, manual annotation is both costly and time-consuming, especially for complex tasks like densely grounding annotation.To put the time/money cost in perspective, for the data we annotated in this paper, we estimate that obtaining the same annotations with human annotator would cost at least $539,000 and require 5.76 years (no eat, no sleep) worth of work from a professional annotator (earning minimum wage of $10.67 per hour).In contrast, using LLMs (GPT4 [52]), we achieve the same results for $3,030 within 2 days, representing a 178x reduction in cost and a 1051x reduction in time.At the time of writing, the cost and time further decreases by 50% to $1,500 and 1 day, with the introduction of GPT-4o [51].</p>
<p>As previously discussed, using humans to annotate 3D scenes can be an exhaustive process.Meanwhile, 2D-LLMs demonstrate remarkable capabilities in understanding visual inputs and generating language, making them well-suited for creating high-quality, grounded language annotations.However, due to the hallucination issues and data issues in 2D-LLMs, aggregating information across images, even those originating from the same scene, is not feasible yet.</p>
<p>In contrast, Large Language Models (LLMs) excel at understanding structural data and generating diverse and fluent language [52].They have demonstrated capabilities in spatial reasoning [8], solving both elementary and sophisticated math problems [36,70].To address the limitations of 2D-LLMs when annotate 3D scenes, we leverage the strengths of LLMs.By integrating detailed, accurate information into a reliable scene graph, we provide LLMs with the necessary data to reason effectively and generate precise annotations.</p>
<p>Here are the key steps of applying our pipeline to obtain densely-grounded annotation for any synthetic 3D scene:</p>
<p>• 3D Model to 2D Image.In the 3D-Front dataset, each object is sourced from 3D-Future [24], which provides a ground truth 2D image for each object.For the Structured3D dataset, individual images for each object are not available.Thus, we use set-of-mark prompting [73], where each object to be annotated is circled in red in the images.</p>
<p>• 2D Image to Attributes.We use GPT-4V to generate detailed language annotations for each 2D object image, including attributes like name, color, finish, and texture.The naming is open-vocabulary, contrary to being class-agnostic.</p>
<p>• List of Attributes to Scene Graph.We structure each individual objects' annotations into a JSON-based scene graph that captures the relationships and attributes of objects within the scene.Note that we obtain this scene graph from synthetic data which we can guarantee the correctness.</p>
<p>• Scene Graph to Generated Annotations.Based on the given scene graph, we will be able to produce 3D-Grounded Object Reference, 3D-Grounded Scene Description, and 3D-Grounded QA using GPT-4 [52] with various prompts, which we will show in the appendix.</p>
<p>• Generated Annotations to Processed Annotations.After we acquire raw annotations, we will apply hallucination fil- There is a <lamp-6> that offers a comfortable light.</p>
<p>There is a <tv_1> sitting on the <tv_stand_0>.</p>
<p>There is a <lamp-6> that offers a comfortable light.</p>
<p>There is a <tv_1> sitting on the <tv_stand_0>.</p>
<p>There is a <p>lamp</p>[<lamp-6>] that offers a comfortable light.ters and template augmentation for the phrase tag to remove low-quality annotations and augment generated annotations.With this pipeline, we generate a diverse range of 3D vision-language understanding tasks as shown in Figure 2. On a high level, these tasks can be categorized into: • 3D-Grounded Object Reference: Given a 3D scene and an object of interest, 3D-LLM is required to generate a description that uniquely identifies the target object.The description includes text and grounding information, not only for the target object but also for any landmark objects mentioned in the description.This task is conceptually similar to Visual Grounding, Scene-aware Object Captioning, and Dense Captioning in 2D vision-language research.• 3D-Grounded Scene Description: Given a 3D scene, the 3D-LLM generates a description that captures the salient aspects of the environment.The description includes both text and grounding information, linking the language to specific objects or regions in the scene.</p>
<p>• 3D-Grounded QA: Given a 3D scene and a question about the environment, the 3D-LLM generates an answer that is grounded in the scene.Both the question and answer include text and grounding information, ensuring that the 3D-LLM's responses are contextually relevant and accurate.</p>
<p>Human quality check.Table 4 shows results from extensive human quality checks on 5,100 generated annotations, comparing the error rates of 3D-GRAND and previous datasets such as ScanEnts3D [1].The results show that large lan-guage models (LLMs) like GPT-4 can achieve error rates in densely grounded annotations comparable to those of professional human annotators.This finding aligns with studies that suggest LLMs are starting to reach human-level annotation quality on certain tasks [65].See supplement for a more detailed description of the human quality check process.Dataset highlights.3D-GRAND has several features that distinguish it from existing 3D-language datasets: (1).Largescale: With 40,087 scenes and 6.2 million annotations, 3D-GRAND is the largest 3D-language dataset to date, providing ample data for training and evaluating 3D-LLMs.(2).Dense grounding: Unlike recent million-scale datasets like Scen-eVerse, which lack grounded language annotations, each language annotation in 3D-GRAND is densely grounded to specific objects or regions within the 3D scenes, facilitating fine-grained language understanding and generation.</p>
<p>(3).Diverse language tasks: 3D-GRAND supports many grounded language tasks, including object reference , spatial reasoning, and scene understanding, making it a comprehensive benchmark for evaluating 3D-LLMs.(4).High-quality annotations: We utilize a hallucination filter to mitigate hallucination of the language annotations in 3D-GRAND.They are also human-evaluated to ensure the quality.</p>
<p>These unique features establish 3D-GRAND as a valuable resource for advancing research in 3D-LLMs and embodied AI.By providing a large-scale, densely-grounded 3D-text dataset, 3D-GRAND enables the development and evaluation of more capable and reliable 3D-LLMs that can effectively understand and interact with the physical world.</p>
<p>Experiments</p>
<p>We present our experimental setup, including baselines, datasets, and implementation details.We then report the results of our approach, denoted as 3D-GRAND on ScanRefer [10] and the 3D-POPE benchmarks, demonstrating the effectiveness in improving grounding and reducing halluci-nation.Finally, we report an ablation study analyzing the impact of components of our model and training strategy.</p>
<p>Experimental Setup</p>
<p>Model.Our proposed model is based on Llama-2 [66].The input is object-centric context, including a scene graph with each object's category, centroid (x, y, z), and extent (width, height, depth), along with the text instruction and user query.</p>
<p>During training, we utilized ground-truth centroids and extents.For inference, we used bounding boxes predicted by Mask3D [61].Examples of input/output and details of the model can be found in the supplementary material.Baselines.We compare our 3D-GRAND against the following baselines: 3D-LLM [28], LEO [31], and 3D-Vista [85].Each model, along with the specific checkpoint used to obtain the results, is documented in the appendix.Datasets.We evaluate our model 3D-GRAND on 3D-POPE and ScanRefer.3D-POPE is our newly introduced benchmark dataset for evaluating object hallucination in 3D-LLMs, as described in Section 3.For ScanRefer, We utilized the validation split which contains 9,508 natural language descriptions of 2,068 objects in 141 ScanNet [15] scenes.Metrics.For the ScanRefer benchmark, we use the official evaluation metrics, including Accuracy@0.25IoU and Accuracy@0.5IoU.For the 3D-POPE benchmark, we report accuracy, precision, recall, F1 score, and "Yes" rate under the three sampling strategies described in Section 3. Implementation Details.The 3D-GRAND model is LoRAfinetuned [29] Llama-2.We use DeepSpeed ZeRO-2 [57] and FlashAttention [17] to save GPU memory and speed up training.The model is trained in BF16 precision on 12 NVIDIA A40 GPUs with a combined batch size of 96 and a learning rate of 2e-4.We use AdamW [47] with a weight decay of 0.01 and a cosine learning rate scheduler.We train the model for 10k steps, which takes ≈48 hours.</p>
<p>Results on 3D-POPE</p>
<p>We first evaluate these approaches on 3D-POPE, with results in Table 3. Results show that 3D-LLM [28] almost always produces yes responses to any question.3D-VisTA [85] performs similarly to the random baseline.LEO [31] tends to answer yes frequently, but its precision indicates a similar object hallucination rate to the random baseline.In our evaluation, 3D-GRAND achieved better performance, with 93.34% precision and 89.12% accuracy when tested with random sampling.Our model struggles with the more difficult splits, Popular and Adversarial, which demonstrates the effectiveness and rigorousness of 3D-POPE as a benchmark.Moreover, we emphasize that our model has never encountered ScanNet during training.More analysis on 3D hallucination can be found in the supplementary material.</p>
<p>Results on ScanRefer</p>
<p>We report results on ScanRefer in Table 5.There are a few important observations on this result: Our 3D-LLM trained with 3D-GRAND data achieved the best Acc@0.25 among all models.Notably, our model surpasses the previous best-performing model, 3D-LLM, by 7.7% on accuracy@0.25IoU.We emphasize that our model, unlike 3D-LLM, has never seen ScanNet scenes during its training (zero-shot) and is only trained on synthetic 3D scenes instead of real scans.Therefore, these results provide a promising early signal that sim-to-real transfer can be achieved via our densely-grounded large-scale dataset.</p>
<p>Our generative 3D-LLM model (one that a user can chat with) performs better or on par compared to non-generative 3D-LLMs such as 3D-VisTA and SceneVerse.In the past, generative 3D-LLMs are usually significantly outperformed by non-generative 3D-LLMs, as the latter usually sacrificed the ability to chat in exchange for incorporating specialized model designs, such as producing scores for each object candidate.These designs are closer to traditional non-LLMbased specialized models.But here, we observe that the gap between the two modeling choices is closing with the help of large-scale densely-grounded data like 3D-GRAND.</p>
<p>Our model is just a deliberately a primarily text-based model (Sec.5.1) to demonstrate the effectiveness of the dataset -little visual information is conveyed in our model between the mask proposal to the LLM, in contrast to other sophisticated models where 3D object embeddings are used to better represent visual information.Thus 3D-GRAND has more potential to be unlocked in the future.</p>
<p>Ablation Study</p>
<p>To better understand the components of our 3D-LLM, we conduct an ablation study on ScanRefer and 3D-POPE.</p>
<p>Grounding tokens.Table 6 shows results of our model with different types of grounding methods.We also show results on 3D-POPE in Table 7.In general, the model has a worse grounding performance and more hallucinations without grounding tokens."Ground First" and "Ground Later" refer to whether the dense grounding (grounding every single object mentioned) of the object reference query happens Method Det.</p>
<p>Unique Multiple Overall Acc@0.25 Acc@0.5 Acc@0.25 Acc@0.5 Acc@0.25 Acc@0.before or after the model outputs the final answer for the refer expression.The former functions like a chain-of-thought reasoning process [69], which is likely why the performance increases compared to the latter.See Appendix for details.Mask3D proposals.Finally, we show the upper bound of our approach in Table 6, with Mask3D proposals.Due to the LLM context length, we only use top-40 proposals.</p>
<p>3D-POPE Model Precision</p>
<p>Random</p>
<p>Data Scaling and Sim-to-Real Transfer</p>
<p>We present results in Figure 4. Our model is trained on synthetic 3D scenes from 3D-FRONT and Structured3D [24,81], and evaluated on real-world 3D scans from Scan-Net [15].Grounding performance consistently improves and the hallucination rate drops as the densely-grounded data scales up.Notably, our model trained on densely grounded data scales better than the same model trained without such data.These findings pave the way for scaling 3D-text understanding using synthetic scenes obtained from simulation, which is cheaper and easier to obtain.</p>
<p>Conclusion</p>
<p>We introduced 3D-GRAND, a large-scale, densely-grounded 3D-text dataset designed for grounded 3D instruction tuning, and 3D-POPE, a comprehensive benchmark for evaluating object hallucination in 3D-LLMs.Through extensive experiments, we demonstrated the effectiveness of our dataset on 3D-LLMs in improving grounding and reducing hallucination, achieving state-of-the-art performance on the ScanRefer and 3D-POPE benchmarks.Our ablation analysis demonstrated the importance of densely-grounded instruction tuning, data scaling laws, and effective sim-toreal transfer in developing high-performing 3D-LLMs.We hope our findings can spark further research and innovation in this field, leading to the development of more advanced and capable 3D-LLMs for a wide range of applications.</p>
<p>3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination</p>
<p>Supplementary Material</p>
<p>A. Detailed Comparison with SceneVerse</p>
<p>Among recent datasets, 3D-GRAND is most similar to Scen-eVerse [34].They are both million-scale grounding datasets for 3D-LLMs.However, there are a few key differences: (1) SceneVerse [34] provides only sparse grounding, while 3D-GRAND is densely grounded.In 3D-GRAND, every noun phrase in the text-whether it's in captions, QAs, or object references-is explicitly grounded to a corresponding object in the 3D scene, whereas SceneVerse does not offer this level of grounding granularity.To elucidate this difference, we present Table 2 and 8 that compares SceneVerse and 3D-GRAND;</p>
<p>(2) the language annotations of 3D-GRANDare more trustworthy and have higher quality.Hallucination is known as one of the most common mistakes of LLMs [32,43,58] In 3D-GRAND, we employ a hallucination filter to check and delete any annotations with hallucinated object IDs.This is not possible for SceneVerse since they have pure language output.3D-GRAND is also quality-checked by humans to ensure the quality.</p>
<p>Grounding Granularity Object Reference Data SceneVerse</p>
<p>Session-level many-to-one grounding This is a big cotton sofa.It is between the window and the wooden table.→ sofa-3  Definitions of grounding granularity: • Paragraph-level set-to-set grounding: Many sentences in a long paragraph, each containing several object nouns, are linked to a set of 3D objects without clear associations from specific sentences/noun phrases to objects.• Session-level many-to-one grounding: Multiple sentences in one session, where each sentence can describe several objects (targets and landmarks), are associated with one 3D object.• Noun-level one-to-one grounding: Each noun phrase in each sentence is explicitly matched with one 3D object.</p>
<p>3D-GRAND</p>
<p>B. Implementation of Our Model</p>
<p>B.1. Model input and output demonstration</p>
<p>In Figure 5, we show an example of 3D-GRAND model's input and output on the Grounded Object Reference task.Note how in the "Response", we train the model to generate a ⟨detailed grounding⟩ pair of tags to densely ground every single object mentioned in the refer expression after generating the grounding answer in ⟨refer expression grounding⟩.The "ground first" "ground later" in Table 6 means whether the ⟨detailed grounding⟩ tags happen before or after the ⟨refer expression grounding⟩ tags.Figure 5 is an example of "ground later", and Figure 6 shows an example of "ground first".</p>
<p>B.2. Training Data</p>
<p>There are two flavors of models that we fine-tuned: one grounded object reference model, and one grounded QA model.The grounded object reference model was trained using the grounded object reference data on 3D-FRONT train split, which consist of 234,791 3D-text pairs, each of which are densely grounded.This model was used to generate the ScanRefer results presented in Table 5, 6, and Figure 4 The grounded QA model was trained using a subset of 200k grounded QA: object existence data from the 3D-FRONT train split.The reason that we select a subset of 200k QAs is simply because the entire grounded QA dataset is too large and we do not have enough resource to train on all data.However, as shown in Table 3 and Figure 4, we find even such a subset is very effective in reducing hallucinations in 3D-LLMs.We provide official data splits of train, val and test (90%, 5%, 5%) in our dataset release.The val and test proportion might seem small, but given our dataset's million-scale, they should be sufficiently large for any development and evaluation purposes.</p>
<p>B.3. Training Details</p>
<p>The two flavors of model mentioned above are LoRAfinetuned [29] based off Llama-2.We use DeepSpeed ZeRO-2 [57] and FlashAttention [17] to save GPU memory and speed up training.The model is trained in BF16 precision on 12 NVIDIA A40 GPUs with a combined batch size of 96 and a learning rate of 2e-4.We use the AdamW [47] optimizer with a weight decay of 0.01 and a cosine learning rate scheduler.We train the mode for 10k steps, which takes approximately 48 hours.</p>
<p>C. Additional 3D-GRAND Data Collection</p>
<p>C.1. Point Cloud Generation Pipeline for 3D-Front</p>
<p>Here, we present an expanded version of Section 4, focusing on the methodologies employed in the collection and cleaning of 3D scenes, specifically detailing our process for deriving 3D point clouds from existing datasets.</p>
<p>In our workflow with 3D-FRONT, layouts and meshes are initially processed in Blender to produce multi-view  images.These images are subsequently used to construct comprehensive point clouds for entire houses.Both point clouds and per-room meshes are utilized to generate scenelevel point clouds.We avoid direct use of room meshes because they lack color information in ceilings, walls, and floors, necessitating the final output to be a point cloud.</p>
<p>For Structure3D, while per-scene multi-view images facilitate direct rendering of per-scene point clouds, we frequently encounter issues where parts of adjacent scenes are inadvertently reconstructed due to window transparency.To address this, we employ the layout of each scene to trim extraneous points, thus enhancing the precision of the resulting point clouds.</p>
<p>D. Additional 3D POPE results</p>
<p>D.1. 3D Pope Results on NYU40</p>
<p>E. Human Validation</p>
<p>Because our dataset generation process involves GPT-4V, there is a potential for hallucinations.We identify three types of possible hallucinations that could impact our dataset: the text might inaccurately describe an object's property, such as color or size (termed incorrect object attribute); it might incorrectly depict the spatial relationship between two objects (termed incorrect spatial relation); or it might describe an object that does not exist in the referenced scene at all (termed incorrect object existence).Additionally, inaccuracies in our dataset may also arise from incorrectly grounding the wrong object.</p>
<p>To validate our dataset against these potential failures, we plan to verify a subset of our data through crowdsourcing to ascertain the frequency of these failure cases.</p>
<p>E.1. Crowd-sourcing</p>
<p>We crowd-source the validation of annotations using Hive, a platform commonly used for sourcing annotations for computer vision tasks.The platform can be accessed at https://thehive.ai/.</p>
<p>We conceptualize our dataset validation as a data annotation problem, employing scene-text pairs as the data unit.Annotators are instructed to label these pairs as "True" or "False" to indicate the presence or absence of hallucinations or inaccuracies.Additionally, a "Cannot Decide" option is provided to accommodate cases where the scene view is unclear.</p>
<p>E.1.1. Task Generation</p>
<p>Hive only supports presenting static images to annotators, so we generate annotation tasks by composing snapshots of a scene with corresponding text annotations.For each task, we take snapshots from four different angles and pair them with a corresponding annotation.To maintain simplicity and conciseness, we require validation of just one sentence per
Instruction Sets Qualifier Tasks Banned Real Tasks Honeypots Figure 8
. Illustration of the crowd-sourcing process.Annotators are first shown instruction sets that describe both how the task should be completed and the possible inaccuracies that could appear in the data.They are then presented with qualifier tasks, and annotators who do not get a high enough accuracy on these tasks are banned from annotating our dataset.Annotators who pass the qualifier are able to annotate real tasks, but are randomly presented with honeypots that are indistinguishable from real tasks.Annotators who do not get a high enough accuracy on honeypots are also banned from our dataset.task, providing some surrounding context and highlighting the target sentence.For grounding validation, the grounded object is outlined in the scene with a bounding box, and the referring phrase in the sentence is emphasized.An example of such a task, along with the annotation interface, is depicted in Figure 9. Figure 10 displays two text validation tasks and two grounding validation tasks that were presented to annotators.</p>
<p>E.1.2. Crowd-sourcing Validity</p>
<p>Validating a dataset necessitates a high level of attention from annotators.We curate sets of instructions, qualifying tasks, and honeypot tasks to ensure that the annotations obtained from crowdsourcing are reliable.The crowdsourcing process is illustrated in Figure 8.Before presenting any tasks to the workers, we present them with a set of instruction tasks that show an example annotation, the correct response (as determined by us), and the reason why that response is correct.They are paired with an incorrect example and an explanation of why it is incorrect in order to ensure unbiased annotations.Examples of qualifying instructions are shown in 11.These instructions are intentionally brief, as we found through trial-and-error that longer, paragraph-based instructions were largely ignored by annotators.</p>
<p>Qualifying tasks are presented to the annotators before they are shown any real tasks in order to train them to complete the real task with a high accuracy.Annotators are both shown the correct answer and a reasoning as to why it is correct for every qualifier.We set the minimum qualifier accuracy to 0.75 to ensure that annotators must achieve a minimum competency before annotating real tasks.Every dataset is given between 12 and 30 specially crafted qualifying tasks that demonstrate the possible inaccuracies that could appear in the data.These qualifiers are divided equally between true and false examples so as not to bias workers towards any one answer.</p>
<p>Honeypot tasks are randomly mixed in with real tasks in order to ensure that annotators are maintaining a high quality of annotations throughout the entire job.Because we annotate the honeypot tasks before showing them to annotators, we are able to evaluate any given worker's accuracy on honeypot tasks.We set the minimum honeypot accuracy to 0.89 to ensure that annotators are maintaining correct annotations.Workers that do not maintain this accuracy are banned from annotating our tasks.This is higher than the required accuracy for qualifiers because we expect annotators to already be well trained in our annotation tasks from the instructions and qualifiers.Every data type is given between 18 and 35 honeypot tasks.The honeypots are also approximately divided equally between true and false examples so that workers who consistently select a single answer without paying attention to the task (e.g., someone who always selects "True") will be banned.</p>
<p>To further ensure high-quality annotations, we send each question to 3 different annotators and only accept an annotation if at least 2 out of the 3 annotators agree with each other on the truthfulness of an item.If agreement is not reached, the task is returned as inconclusive.</p>
<p>E.2. Results</p>
<p>We perform validation on 10,200 room-annotation pairs.From each of the three data types, 1,700 pairs are sampled for validation of both text truthfulness and grounding accuracy.A subset of 800 rooms is uniformly chosen, with 400 designated for text truthfulness and another 400 for grounding accuracy.The text data is uniformly sampled from these rooms.We report accuracies for both text truthfulness and grounding accuracy in Table 10.</p>
<p>We report comprehensive statistics from the annotation process in Table 11.We observe a very low qualifier pass rate ranging from 11 -20 % across the different tasks in our data, suggesting that our qualifiers were effective in allowing only the most attentive annotators qualify to annotate real tasks.In addition, none of these annotators were banned due to honeypots.This increases our confidence that our qualification process is effective in training annotators and filtering out those who were not attentive.We also observe that workers spend roughly the same time on real tasks and honeypot tasks, suggesting that the honeypots are indistinguishable from real tasks for the annotators.This further supports the validity of our annotations.Table 11.Comprehensive annotation metrics.Includes qualifier pass rate, honeypot count, honeypot ban rate, percent of tasks marked inconclusive (where workers could not come to an agreement on the label), and the average time that workers spend on both real tasks and honeypot tasks.Each dataset was evaluated on 1700 annotations.At least 2 workers must agree on the label for an annotation to be considered valid.</p>
<p>Figure 1 .
1
Figure 1.We introduce 3D-GRAND, a large-scale, densely grounded 3D-text dataset, and 3D-POPE, a 3D-LLM hallucination benchmark.Training on 3D-GRAND improves grounding accuracy and reduces hallucinations.</p>
<p>and introduce each category and provide examples in Fig 2.</p>
<p>The <p>floor lamp</p>[<obj_5>] is below the <p>footstool</p>[<obj_1>]. Is it correct?A: No.The <p>floor lamp</p>[<obj_5>] is next to the <p>footstool</p>[<obj_1>]. Grounded QA: Spatial Relation (y/n) Q: Is the color of the <p>corner cabinet</p>[<obj_6>] black?A: No.The <p>corner cabinet</p>[<obj_6>] is white.Grounded QA: Attribute (y/n) Q: What is the finish of the <p>three-seat sofa</p>[<obj_1>]?A: The finish of the <p>three-seat sofa</p>[<obj_0>] is soft, textured fabric upholstery.Grounded QA: Attribute Q: Can you find any potted plants in the room?A: <ground> <p>potted plant</p>[<obj_7>] </ground> <answer>Yes</answer>.</p>
<p>Figure 2 .
2
Figure 2. 3D-GRAND dataset and statistics.(Left): 3D-GRAND is a large-scale, densely-grounded 3D-text dataset with 8 different tasks.(Right): From 40K 3D scenes, 3D-GRAND annotates 6.2M 3D-text pairs.</p>
<p>Figure 3 .
3
Figure 3. 3D-GRAND Data Curation Pipeline.</p>
<p>Figure 4 .
4
Figure 4. Data scaling analysis on zero-shot, sim-to-real grounding capability, and hallucination.Grounding performance (left two subfigures) consistently improves as data scales up.Model trained with densely-grounded data exhibits better grounding capability compared to that trained without.Additionally (right subfigure), the model hallucinates less when exposed to more data from 3D-GRAND.Here, the Hallucination Rate is calculated as (1 − Precision) on 3D-POPE.</p>
<p>Figure 5 .
5
Figure 5. 3D-GRAND model input and output on Grounded Object Reference task.</p>
<p>Figure 6 .
6
Figure 6.Demo of interactive chat interface with the 3D-GRAND model.</p>
<p>Figure 7 .
7
Figure 7. Point Cloud Generation for 3D-FRONT.</p>
<p>Figure 9 .
9
Figure 9. Example of a dataset validation task presented to crowd-sourcing annotators: It displays a scene from four different angles alongside the sentence to be validated, which is highlighted.Annotators have the options to select "True," "Not True," or "Cannot Decide."On the left side of the screen, the instructions are repeated for annotators' reference.In this example, "True" is selected.</p>
<p>(a) "True" example of a text validation task.(b) "False" example of a text validation task.(c) "True" example of a grounding validation task.(d) "False example of a grounding validation task.</p>
<p>Figure 10 .
10
Figure10.Examples of tasks presented to annotators for validating both text accuracy and grounding accuracy.The instruction is displayed at the top of the task, while the center showcases four different views of the scene to ensure comprehensive coverage of all relevant areas.At the bottom, the annotation is presented with the pertinent section highlighted.</p>
<p>(a) "True" example instruction for the text validation task.TRUE NOT TRUEReason: There are no blue chairs in the room, only grey chairs (b) "False" example instruction for the text validation task.TRUE NOT TRUEReason: The red outlined object is the armchair, which is the same as the highlighted word.The armchair is also described correctly in the sentence (c) "True" example instruction for the grounding validation task.TRUE NOT TRUEReason: The highlighted phrase is a nightstand, but the red outlined object is a sofa (d) "False" example instruction for the grounding validation task.</p>
<p>Figure 11 .
11
Figure 11.Examples of instructions presented to annotators before they are shown any actual tasks for annotation.For every possible kind of hallucination (incorrect object attribute, spatial relation, or object existence), an illustrative positive and negative example are presented in order to instruct the annotator to look for all possible failure cases.</p>
<p>Table 1 .
1
Comparison with existing 3D scene datasets with language annotations.3D-GRAND is the largest language-grounded dataset.
33.4K</p>
<p>Table 2
2. Comparison of grounding granularity in SceneVerse and3D-GRAND.</p>
<p>Table 4 .
4
Error rates comparison between ScanEnts3D and 3D-GRAND annotations.(AMT = Amazon Mechanical Turk)</p>
<p>Table 6 .
6
Ablation Study on Grounding Accuracy (%) on ScanRefer: Training with densely-grounded data significantly improves grounding accuracy, particularly when multiple distractor objects of the same category are present in the room.
5</p>
<p>Table 7 .
7
Ablation on 3D-POPE.Without the grounding tokens, 3D-GRAND hallucinates more.
3D-GRAND93.34w/o grounding tokens(-1.38)Popular3D-GRAND w/o grounding tokens73.05 (-2.68)Adversarial3D-GRAND w/o grounding tokens69.86 (-2.38)</p>
<p>Table 8 .
8
Example of grounding granularity.3D-GRAND focuses on dense grounding.</p>
<p>Table 9
9
presents evaluation results for 3D POPE using the NYU40 class set.NYU40 includes a subset of the classes from ScanNet200 featured in the main results table.The NYU40 class set consolidates many fine-grained classes into an "other" category, potentially reducing the challenge of negative sampling in the Popular and Adversarial settings compared to the ScanNet200 scenario.
Dataset3D-POPEModelAccuracy Precision Recall F1 Score Yes (%)3D-LLM50.0050.00100.0066.67100.00Random3D-VisTA LEO50.12 54.0350.08 52.7077.13 78.5260.73 63.0777.01 74.50Ours zero-shot (No Grounding)86.4587.2685.3686.3048.91Ours zero-shot (Grounding)85.6888.2282.3485.1846.673D-LLM50.0050.00100.0066.67100.00ScanNet Val (NYU40)Popular3D-VisTA LEO50.27 48.8650.23 49.2877.13 77.4460.84 60.2376.91 78.58Ours zero-shot (No Grounding)80.8578.3085.3581.6854.50Ours zero-shot (Grounding)81.6981.3282.2881.8050.593D-LLM50.0050.00100.0066.67100.00Adversarial3D-VisTA LEO50.44 49.7750.48 49.8577.14 77.6761.03 60.7376.86 77.91Ours zero-shot (No Grounding)81.4778.9885.7882.2454.31Ours zero-shot (Grounding)82.1081.7282.7282.2250.61</p>
<p>Table 9 .
9
Results of 3D-LLMs under three evaluation settings of 3D-POPE on the validation set of ScanNet using NYU40 class set.Yes denotes the proportion of answering "Yes" to the given question.The best results in each block are denoted in bold.</p>
<p>Projects &gt; 3D GRAND Dataset Validation… &gt; Honeypot Tasks Madhavan … Task Id: ff047b8b-d9d8-47d5-874b-776ffc465101
PreviousNextHide TextToolsEnglishSpanishPortugueseMorePanZoom In Zoom OutFitCategoriesTrueNot trueCannot decideImage SettingsBrightnessView Keyboard Shortcuts
M Madhavan Iye... &gt; True: everything in the highlighted sentence is accurate Not true: there is either a wrong spatial location, wrong object attribute, or the object doesn't exist in the scene Cannot decide: only choose this if you are really unsure whether the sentence is true or not Reset Contrast Reset Hotkeys Type category names to filter</p>
<p>Table 10 .
10
Text Truthfulness and Grounding Accuracy from crowdsourcing.Accuracy is computed by dividing the number of "True" responses by the total number of tasks (1700).We can see that the lamp is hanging above the table, and nothing in the highlighted sentence is false
MethodText Truthfulness Grounding AccuracyGrounded Scene Description0.8770.944Grounded QA0.8520.956Grounded Object Reference0.8630.918
AcknowledgementThis work is generously supported by NSF IIS-1949634, NSF SES-2128623, and has benefited from the Microsoft Accelerate Foundation Models Research (AFMR) grant program.
Scanents3d: Exploiting phraseto-3d-object correspondences for improved visio-linguistic models in 3d scenes. Ahmed Abdelreheem, Kyle Olszewski, Hsin-Ying Lee, Peter Wonka, Panos Achlioptas, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2024. 3, 4, 6</p>
<p>ReferIt3D: Neural listeners for fine-grained 3d object identification in real-world scenes. Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, Leonidas J Guibas, 16th European Conference on Computer Vision (ECCV). 2020</p>
<p>Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Advances in Neural Information Processing Systems. 202235</p>
<p>Scanqa: 3d question answering for spatial scene understanding. Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, Motoaki Kawanabe, proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou, arXiv:2308.12966Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. 2023arXiv preprint</p>
<p>Experience grounds language. Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, Joseph Turian, 2020</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, Sparks of artificial general intelligence: Early experiments with gpt-4. 2023</p>
<p>Grounding 'grounding' in NLP. Yonatan Khyathi Raghavi Chandu, Alan W Bisk, Black, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational Linguistics2021</p>
<p>Scanrefer: 3d object localization in rgb-d scans using natural language. Dave Zhenyu, Chen , Angel X Chang, Matthias Nießner, 16th European Conference on Computer Vision (ECCV). 20206</p>
<p>Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, Hao Fei, Hongyuan Zhu, Jiayuan Fan, Tao Chen, arXiv:2311.18651Ll3da: Visual interactive instruction tuning for omni-3d understanding, reasoning, and planning. 202323arXiv preprint</p>
<p>Multiobject hallucination in vision-language models. Xuweiyi Chen, Ziqiao Ma, Xuejun Zhang, Sihan Xu, Shengyi Qian, Jianing Yang, David F Fouhey, Joyce Chai, 2024</p>
<p>Scan2cap: Context-aware dense captioning in rgbd scans. Zhenyu Chen, Ali Gholami, Matthias Nießner, Angel X Chang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2021</p>
<p>Amazon mechanical turk: A research tool for organizations and information systems scholars. Kevin Crowston, Shaping the Future of ICT Research. Methods and Approaches. Berlin, Heidelberg; Berlin HeidelbergSpringer2012</p>
<p>Scannet: Richly-annotated 3d reconstructions of indoor scenes. Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias Nießner, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017. 3, 4, 7, 8</p>
<p>Plausible may not be faithful: Probing object hallucination in vision-language pre-training. Wenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, Pascale Fung, 2023</p>
<p>FlashAttention-2: Faster attention with better parallelism and work partitioning. Tri Dao, International Conference on Learning Representations (ICLR). 202471</p>
<p>RoboTHOR: An Open Simulation-to-Real Embodied AI Platform. Matt Deitke, Winson Han, Alvaro Herrasti, Aniruddha Kembhavi, Eric Kolve, Roozbeh Mottaghi, Jordi Salvador, Dustin Schwenk, Eli Vanderbilt, Matthew Wallingford, Luca Weihs, Mark Yatskar, Ali Farhadi, CVPR. 2020</p>
<p>Proc-THOR: Large-Scale Embodied AI Using Procedural Generation. Matt Deitke, Eli Vanderbilt, Alvaro Herrasti, Luca Weihs, Jordi Salvador, Kiana Ehsani, Winson Han, Eric Kolve, Ali Farhadi, Aniruddha Kembhavi, Roozbeh Mottaghi, NeurIPS, 2022. Outstanding Paper Award. </p>
<p>Is gpt-3 a good data annotator?. Bosheng Ding, Chengwei Qin, Linlin Liu, Ken Yew, Shafiq Chia, Boyang Joty, Lidong Li, Bing, 2023</p>
<p>ManipulaTHOR: A Framework for Visual Object Manipulation. Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli Vanderbilt, Luca Weihs, Eric Kolve, Aniruddha Kembhavi, Roozbeh Mottaghi, CVPR. 2021</p>
<p>Visual question answering for 3d environments. Yasaman Etesam, Leon Kochiev, Angel X Chang, 2022 19th Conference on Robots and Vision (CRV). IEEE20223</p>
<p>3d-front: 3d furnished rooms with layouts and semantics. Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming Wang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia, Binqiang Zhao, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision20214</p>
<p>Detecting and preventing hallucinations in large vision language models. Anisha Gunjal, Jihan Yin, Erhan Bas, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024</p>
<p>Text2room: Extracting textured 3d meshes from 2d text-to-image models. Lukas Höllein, Ang Cao, Andrew Owens, Justin Johnson, Matthias Nießner, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>3d concept learning and reasoning from multi-view images. Yining Hong, Chunru Lin, Yilun Du, Zhenfang Chen, Joshua B Tenenbaum, Chuang Gan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>3d-llm: Injecting the 3d world into large language models. Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, Chuang Gan, Advances in Neural Information Processing Systems. 20482-20494, 2023. 2, 3, 4, 736</p>
<p>LoRA: Low-rank adaptation of large language models. J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 202271</p>
<p>Haifeng Huang, Zehan Wang, Rongjie Huang, Luping Liu, Xize Cheng, Yang Zhao, Tao Jin, Zhou Zhao, arXiv:2312.08168Chat-3d v2: Bridging 3d scene and large language models with object identifiers. 202323arXiv preprint</p>
<p>An embodied generalist agent in 3d world. Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, Siyuan Huang, ICML. 2024. 2, 3, 4, 7</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting Liu, 202331</p>
<p>Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospectionallocation. Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, Nenghai Yu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Dense object grounding in 3d scenes. Wencan Huang, Daizong Liu, Wei Hu, Proceedings of the 31st ACM International Conference on Multimedia. the 31st ACM International Conference on Multimedia202331</p>
<p>Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei, arXiv:2307.05973Voxposer: Composable 3d value maps for robotic manipulation with language models. 2023arXiv preprint</p>
<p>Mathprompter: Mathematical reasoning using large language models. Shima Imani, Liang Du, Harsh Shrivastava, 2023</p>
<p>Sceneverse: Scaling 3d vision-language learning for grounded scene understanding. Baoxiong Jia, Yixin Chen, Huangyue Yu, Yan Wang, Xuesong Niu, Tengyu Liu, Qing Li, Siyuan Huang, arXiv:2401.09340202423arXiv preprint</p>
<p>Unity: A general platform for intelligent agents. Arthur Juliani, Vincent-Pierre Berges, Ervin Teng, Andrew Cohen, Jonathan Harper, Chris Elion, Chris Goy, Yuan Gao, Hunter Henry, Marwan Mattar, Danny Lange, 2020</p>
<p>Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Vanderbilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, Ali Farhadi, AI2-THOR: An Interactive 3D Environment for Visual AI. 2017</p>
<p>Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, Jiaya Jia, Lisa, arXiv:2308.00692Reasoning segmentation via large language model. 202323arXiv preprint</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, International conference on machine learning. 2023</p>
<p>Mingsheng Li, Xin Chen, Chi Zhang, Sijin Chen, Hongyuan Zhu, Fukun Yin, Gang Yu, Tao Chen, arXiv:2312.10763M3dbench: Let's instruct large models with multi-modal 3d prompts. 2023arXiv preprint</p>
<p>Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, Ji-Rong Wen, arXiv:2305.10355Evaluating object hallucination in large vision-language models. 202341arXiv preprint</p>
<p>Zeju Li, Chao Zhang, Xiaoyan Wang, Ruilong Ren, Yifan Xu, Ruifei Ma, Xiangde Liu, arXiv:2401.032013dmit: 3d multi-modal instruction tuning for scene understanding. 2024arXiv preprint</p>
<p>Microsoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, Lawrence Zitnick, Computer Vision-ECCV 2014: 13th European Conference. Zurich, SwitzerlandSpringerSeptember 6-12, 2014. 2014Proceedings, Part V 13</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, NeurIPS. 22023</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, 201971</p>
<p>Sqa3d: Situated question answering in 3d scenes. Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, Siyuan Huang, International Conference on Learning Representations. 2023</p>
<p>Habitat: A Platform for Embodied AI Research. Manolis Savva, * , Abhishek Kadian, * , Oleksandr Maksymets, * , Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, Dhruv Batra, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2019</p>
<p>Orbit: A unified simulation framework for interactive robot learning environments. Mayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu, Nikita Rudin, David Hoeller, Jia Lin Yuan, Ritvik Singh, Yunrong Guo, Hammad Mazhar, Ajay Mandlekar, Buck Babich, Gavriel State, Marco Hutter, Animesh Garg, IEEE Robotics and Automation Letters. 862023</p>
<p>Hello gpt-4o. Openai, 2024</p>
<p>. OpenAI. Gpt-4 technical report. 252024</p>
<p>Kosmos-2: Grounding multimodal large language models to the world. Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei, arXiv:2306.14824202323arXiv preprint</p>
<p>. Xavi Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Ruslan Partsey, Jimmy Yang, Ruta Desai, Alexander William Clegg, Michal Hlavac, Tiffany Min, Theo Gervet, Vladimír Vondrus, Vincent-Pierre Berges, John Turner, Oleksandr Maksymets, Zsolt Kira, Mrinal Kalakrishnan, Jitendra Malik, Devendra Singh Chaplot, Unnat Jain, Dhruv Batra, Akshara Rai, and Roozbeh Mottaghi2023Habitat 3.0: A co-habitat for humans, avatars and robots</p>
<p>Gpt4point: A unified framework for point-language understanding and generation. Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong Wu, Jiaqi Wang, Dahua Lin, Hengshuang Zhao, 2023</p>
<p>Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Erix Rao M Anwer, Ming-Hsuan Xing, Fahad S Yang, Khan, arXiv:2311.03356Glamm: Pixel grounding large multimodal model. 202323arXiv preprint</p>
<p>Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Yuxiong He, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining202071</p>
<p>Object hallucination in image captioning. Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, Kate Saenko, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics201821</p>
<p>Languagegrounded indoor 3d semantic segmentation in the wild. David Rozenberszki, Or Litany, Angela Dai, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2022</p>
<p>Laion-5b: An open large-scale dataset for training next generation image-text models. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, Jenia Jitsev, 2022</p>
<p>Mask3d: Mask transformer for 3d semantic instance segmentation. Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, Bastian Leibe, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Controlroom3d: Room generation using semantic proxy rooms. Jonas Schult, Sam Tsai, Lukas Höllein, Bichen Wu, Jialiang Wang, Chih-Yao Ma, Kunpeng Li, Xiaofang Wang, Felix Wimbauer, Zijian He, Peizhao Zhang, Bastian Leibe, Peter Vajda, Ji Hou, 2023</p>
<p>Aligning large multimodal models with factually augmented rlhf. Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, arXiv:2309.145252023arXiv preprint</p>
<p>Manolis Savva, and Dhruv Batra. Habitat 2.0: Training home assistants to rearrange their habitat. Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir Vondrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Advances in Neural Information Processing Systems (NeurIPS). 2021</p>
<p>Large language models for data annotation: A survey. Zhen Tan, Alimohammad Beigi, Song Wang, Ruocheng Guo, Amrita Bhattacharjee, Bohan Jiang, Mansooreh Karami, Jundong Li, Lu Cheng, Huan Liu, 202456</p>
<p>Alan Schelten. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor,2023Aurelien Rodriguez27Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Tai Wang, Xiaohan Mao, Chenming Zhu, Runsen Xu, Ruiyuan Lyu, Peisen Li, Xiao Chen, Wenwei Zhang, Kai Chen, Tianfan Xue, arXiv:2312.16170A holistic multimodal 3d perception suite towards embodied ai. 2023arXiv preprint</p>
<p>Chat-3d: Data-efficiently tuning large language model for universal dialogue of 3d scenes. Zehan Wang, Haifeng Huang, Yang Zhao, Ziang Zhang, Zhou Zhao, arXiv:2308.087692023arXiv preprint</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>An empirical study on challenging math problem solving with gpt-4. Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang, 2023</p>
<p>Jiarui Xu, Xingyi Zhou, Shen Yan, Xiuye Gu, Anurag Arnab, Chen Sun, Xiaolong Wang, Cordelia Schmid, arXiv:2312.09237Pixel Aligned Language Models. 2023arXiv preprint</p>
<p>Clevr3d: Compositional language and elementary visual reasoning for question answering in 3d real-world scenes. Zhihao Xu Yan, Yuhao Yuan, Yinghong Du, Yao Liao, Zhen Guo, Shuguang Li, Cui, arXiv:2112.116912021arXiv preprint</p>
<p>Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, Jianfeng Gao, 2023</p>
<p>Llmgrounder: Open-vocabulary 3d visual grounding with large language model as an agent. Jianing Yang, Xuweiyi Chen, Shengyi Qian, Nikhil Madaan, Madhavan Iyengar, David F Fouhey, Joyce Chai, ICRA. 22024</p>
<p>Language guided generation of 3d embodied ai environments. Yue Yang, Fan-Yun Sun, Luca Weihs, Eli Vanderbilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2024). IEEE/CVF2024</p>
<p>Ferret: Refer and ground anything anywhere at any granularity. Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, Yinfei Yang, The Twelfth International Conference on Learning Representations. 202323</p>
<p>Modeling context in referring expressions. Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, Tamara L Berg, Computer Vision-ECCV 2016: 14th European Conference. Amsterdam, The NetherlandsSpringerOctober 11-14, 2016. 2016Proceedings, Part II 14</p>
<p>Toward explainable and finegrained 3d grounding through referring textual phrases. Zhihao Yuan, Xu Yan, Zhuo Li, Xuhao Li, Yao Guo, Shuguang Cui, Zhen Li, arXiv:2207.018212022arXiv preprint</p>
<p>Halle-switch: Rethinking and controlling object existence hallucinations in large vision language models for detailed caption. Bohan Zhai, Shijia Yang, Xiangchen Zhao, Chenfeng Xu, Sheng Shen, Dongdi Zhao, Kurt Keutzer, Manling Li, Tan Yan, Xiangjun Fan, arXiv:2310.017792023arXiv preprint</p>
<p>Groundhog: Grounding large language models to holistic segmentation. Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, Joyce Chai, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Structured3d: A large photo-realistic dataset for structured 3d modeling. Jia Zheng, Junfei Zhang, Jing Li, Rui Tang, Shenghua Gao, Zihan Zhou, Proceedings of The European Conference on Computer Vision (ECCV). The European Conference on Computer Vision (ECCV)20204</p>
<p>Analyzing and mitigating object hallucination in large visionlanguage models. Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, Huaxiu Yao, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.10592Minigpt-4: Enhancing vision-language understanding with advanced large language models. 2023arXiv preprint</p>
<p>Multimodal c4: An open, billion-scale corpus of images interleaved with text. Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang, Wang , Yejin Choi, 2023</p>
<p>3d-vista: Pre-trained transformer for 3d vision and text alignment. Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, Qing Li, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023. 2, 3, 4, 7</p>            </div>
        </div>

    </div>
</body>
</html>
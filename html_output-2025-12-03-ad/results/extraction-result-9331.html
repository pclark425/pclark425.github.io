<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9331 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9331</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9331</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-258840847</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2023.emnlp-main.774.pdf" target="_blank">Multilingual Large Language Models Are Not (Yet) Code-Switchers</a></p>
                <p><strong>Paper Abstract:</strong> Multilingual Large Language Models (LLMs) have recently shown great capabilities in a wide range of tasks, exhibiting state-of-the-art performance through zero-shot or few-shot prompting methods. While there have been extensive studies on their abilities in monolingual tasks, the investigation of their potential in the context of code-switching (CSW), the practice of alternating languages within an utterance, remains relatively uncharted. In this paper, we provide a comprehensive empirical analysis of various multilingual LLMs, benchmarking their performance across four tasks: sentiment analysis, machine translation, summarization and word-level language identification. Our results indicate that despite multilingual LLMs exhibiting promising outcomes in certain tasks using zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales. We argue that current"multilingualism"in LLMs does not inherently imply proficiency with code-switching texts, calling for future research to bridge this discrepancy.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9331.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9331.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-tune vs Prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuning (supervised) versus zero-/few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of supervised fine-tuning on task-specific CSW training data versus zero-shot and few-shot prompt-based inference with multilingual LLMs; fine-tuned smaller models substantially outperform prompted large LLMs across code-switched tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multilingual Large Language Models Are Not (Yet) Code-Switchers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various (XLM-R, mBERT, mDeBERTa, mT0 p, mBART, M2M100)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (reported ranges in paper include 178M up to 13B+ for different models)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sentiment Analysis (SA); Machine Translation (MT); Summarization (SUM); Word-level Language Identification (LID)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Code-switched (CSW) variants of standard NLP tasks: classify sentiment, translate between Hinglish/Hindi and English, summarize Hinglish conversations to English, and per-token language ID.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Fine-tuning: supervised training on task-specific CSW training sets (standard cross-entropy fine-tuning). Prompting: zero-shot and few-shot in-context prompting (0,1,3,5-shot) using natural-language instruction prompts / templates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot and few-shot prompting (0/1/3/5-shot) vs full supervised fine-tuning on task training data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative summary from paper: fine-tuned smaller models achieve substantially higher task metrics than prompted LLMs; e.g., for MT (Hinglish→English) fine-tuned models obtain ~25–32 BLEU while best prompted mT0-XXL achieves ≤20 BLEU; for LID fine-tuned models reach ~80 F1 vs prompted LLMs near ~20 F1 in best few-shot settings. For SA, fine-tuned models typically reach much higher F1 (many reported ~70–80 F1) compared to prompting (around majority baseline / ~50 F1 for some LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>See above; concrete examples: MT fine-tuned 25–32 BLEU vs prompted mT0-XXL ≤20 BLEU; LID fine-tuned ≈80 F1 vs prompted ≈20 F1 (5-shot best public LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Examples reported in paper: MT: roughly −5 to −12 BLEU for prompting vs fine-tuning; LID: roughly −60 F1 (prompting ~20 vs fine-tuned ~80); SA: prompting near baseline (≈46–50 F1) vs fine-tuned ≈70–80 F1 (≈+20–30 F1 improvement with fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that multilingual pretraining and instruction-tuning do not confer code-switching competence; CSW-specific structure and scarce CSW pretraining samples mean LLMs lack representations to perform CSW tasks well without supervised fine-tuning or CSW-specific training/augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Fine-tuning: used all available CSW training instances per task (SA up to ~9k training examples for some language pairs; MT MixMT Hinglish had 8,060 training pairs; SUM Gupshup 5,831 train pairs; LID subsets from LinCE). Fine-tuning optimizer and hyperparameters described (Adafactor, lr schedules, epochs). Prompting: 0-shot for SA/MT/SUM and 5-shot for LID reported in main results; explored 0,1,3,5-shot settings with 5 different prompt templates (Appendix C). Evaluation metrics: macro F1 / Accuracy for SA and LID, BLEU / spBLEU for MT, ROUGE-L for SUM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multilingual Large Language Models Are Not (Yet) Code-Switchers', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9331.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9331.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero- vs Few-shot (0/1/3/5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot versus few-shot (in-context) prompting effects</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper evaluates how adding in-context examples (1,3,5 shots) changes performance relative to zero-shot prompting across CSW tasks and models, finding task-dependent effects: few-shot can hurt SA and SUM, has negligible impact for MT, and helps LID.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multilingual Large Language Models Are Not (Yet) Code-Switchers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BLOOMZ, mT0, XGLM (various sizes), GPT-3.5 turbo (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (examples in paper: BLOOMZ 560M–7B, mT0 300M–13B, XGLM 564M–7.5B, GPT-3.5 turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SA, MT, SUM, LID</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard tasks applied to code-switched inputs: sentiment classification, translation, summarization, and token-level language ID.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Prompting with 0, 1, 3, and 5 in-context examples (templates varied). For SA the label is read as the next-token probability; for LID prompting expects exact bracketed [ token | tag ] outputs; MT and SUM use standard sequence generation prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>0-shot versus 1-shot, 3-shot, 5-shot (paper reports trajectories in Figure 4 and discusses per-task differences).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Task-dependent: SA and SUM often saw a decrease from 0-shot to 1-shot (authors report a drop in metrics when adding examples); MT showed negligible change with increasing examples; LID benefited from more in-context examples (best performance typically at 5-shot for public LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Qualitative comparisons: SA & SUM: 0-shot > 1-shot in many cases (metric drops observed); MT: performance roughly flat across shot counts; LID: 0-shot results were zero, while 5-shot yields measurable F1 (public models' 5-shot ~20 F1 in best case).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>No single global numeric effect size given; paper reports decreased metrics for SA/SUM when moving from 0→1 shot (qualitative), MT negligible delta, LID improved from 0 (all models) to ~20 F1 (best public LLMs at 5-shot) — i.e., a large relative gain from adding examples for LID but still far below fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Possible reasons: models saw tasks monolingually during pretraining and thus perform well zero-shot; CSW examples can be perceived as low-quality/noisy, confusing generation in SA and SUM. For LID, sequence-tagging format is novel so in-context examples teach the required output format, improving performance. For MT, models may not recognize or represent target CSW form (Hinglish) and thus ignore examples.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Explored 0,1,3,5-shot; used 5 prompt templates; main reported settings: SA/MT/SUM results reported for 0-shot in main tables; LID main results reported for 5-shot because 0-shot was uniformly 0. For SA they compute probability of generating each label token and pick the highest-probability label; LID parsing uses a dynamic programming extraction (Paolini et al., 2021). ChatGPT evaluations limited by budget: 0-shot for SA/MT/SUM and 1-shot for LID.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multilingual Large Language Models Are Not (Yet) Code-Switchers', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9331.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9331.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LID format-following</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Identification: bracketed [ token | tag ] output format sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Task requires per-token language labels using a strict [ token | tag ] textual format; multilingual LLMs largely fail at following this generation format in 0-shot, and even in few-shot they often produce invalid or malformed outputs leading to poor scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multilingual Large Language Models Are Not (Yet) Code-Switchers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BLOOMZ, mT0, XGLM, GPT-3.5 turbo (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (examples include BLOOMZ 560M–7B, mT0 up to 13B, XGLM up to 7.5B, GPT-3.5 turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Word-level Language Identification (LID)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>For each token in a sentence, identify its language (lang1/lang2/other) and produce outputs in a strict [ token | tag ] format (e.g., [hello|lang1]).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Natural language instruction demanding output in exact bracketed format. Evaluated in 0-shot (all models produced 0 scores) and with in-context examples (1–5-shot); main reported prompting results use 5-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>0-shot (no examples) versus few-shot (1,3,5 examples) prompting; additionally compared against supervised fine-tuned sequence-tagging models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>0-shot: all prompting models returned 0 score; 5-shot (best public prompted LLMs) reached roughly ~20 F1 (paper's summary); fine-tuned models reached ~80 F1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>0-shot = 0 F1 for all public LLMs; 5-shot prompting ≈20 F1 (public LLMs) vs fine-tuned ≈80 F1.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Huge effect: moving from 0-shot→5-shot changed score from 0→~20 F1 (prompting), but fine-tuning still outperformed prompting by ≈60 F1.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors attribute failures to models not being accustomed to sequence tagging outputs and not adequately following strict format constraints in generation; in-context examples help because they demonstrate the required structure, but LLMs still struggle to replicate exact per-token outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>LID parse: model output parsed with a dynamic programming algorithm (Paolini et al., 2021) to extract valid [token|tag] pairs. Main LID prompting results reported for 5-shot (0-shot uniformly 0). Few-shot examples improved performance somewhat; even the best billion-parameter prompted models often produced malformed brackets or multi-token labels inside a single bracket.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multilingual Large Language Models Are Not (Yet) Code-Switchers', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9331.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9331.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MT: CSW instruction mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine Translation prompt-target mismatch for code-switched targets (e.g., English→Hinglish)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When prompts instruct LLMs to generate code-switched (Hinglish) targets, many multilingual LLMs output monolingual English instead of the requested CSW form; prompting yields significantly lower BLEU than fine-tuning for CSW MT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multilingual Large Language Models Are Not (Yet) Code-Switchers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>mT0 family (300M–13B, including mT0-XXL), BLOOMZ, XGLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various; example cited: mT0-XXL (very large), mT0 13B, others between 300M–13B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine Translation (Hinglish↔English)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate between Hinglish (Hindi-English code-mixed) and English. Paper focuses on Hinglish→English and English→Hinglish directions from MixMT 2022 dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot and few-shot natural-language translation prompts (e.g., 'Translate [src] to [tgt]: [sequence]'). Few-shot included in-context Hinglish examples aiming to teach code-switched target form.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Prompting (0/1/3/5-shot) vs supervised fine-tuning of MT models (mT0 p variants, M2M100, mBART-50, etc.). Also compared monolingual (Hindi→English) scaling vs code-switched (Hinglish→English) scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Prompted best public LLM (mT0-XXL) achieved ≤20 BLEU on Hinglish→English, while fine-tuned models achieved ~25–32 BLEU. For English→Hinglish generation, prompted LLMs often output English unchanged rather than producing Hinglish.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Hinglish→English: prompted ≤20 BLEU (best) vs fine-tuned 25–32 BLEU. English→Hinglish: prompted models frequently failed to produce code-switched targets, repeating the original English sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Reported example effect: ~5–12 BLEU lower for best prompting model compared to fine-tuned models on Hinglish→English.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors suggest LLMs lack sufficient pretraining exposure to code-switched target formats; CSW can be perceived as noise under monolingual-focused pretraining/instruction-tuning so models default to monolingual outputs even when prompted otherwise.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>MT dataset: MixMT 2022 Hinglish-English (8,060 train, 942 val, 960 test). Prompt templates for mT0 included 'Translate [src] to [tgt]: [sequence]'. Paper gives a concrete qualitative example where mT0 13B, with 5 in-context examples instructing English→Hinglish, produced an English paraphrase instead of Hinglish code-mix. Evaluated with BLEU/spBLEU metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multilingual Large Language Models Are Not (Yet) Code-Switchers', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9331.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9331.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (GPT-3.5 turbo) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 turbo (ChatGPT) zero-/one-shot prompting behavior on CSW tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed hosted model (GPT-3.5 turbo) shows strong zero/one-shot performance on most code-switched tasks, comparable to fine-tuned models and better than public LLMs, but still struggles with generating code-switched MT targets (English→Hinglish).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multilingual Large Language Models Are Not (Yet) Code-Switchers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 turbo (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SA, MT, SUM, LID (evaluated zero-shot for SA/MT/SUM, 1-shot for LID)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same CSW tasks as the paper's benchmark; ChatGPT used as a hosted instruction-following LLM to compare prompting performance.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot prompting for SA/MT/SUM; 1-shot prompting for LID (budget-limited in experiments). In SA, instructed to return exact string label and F1 computed by exact match; for LID one example provided to teach bracketed tagging format.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against public multilingual LLMs (BLOOMZ, mT0, XGLM) under same prompt templates and against fine-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>ChatGPT achieves performance comparable to fine-tuned models and significantly better than other public multilingual LLMs on most tasks; exception: English→Hinglish MT where its zero-shot performance is only slightly better than other public LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Qualitative: ChatGPT often matches fine-tuned scores on SA/SUM/LID (1-shot) and outperforms other publicly-available LLMs; detailed numeric comparisons not fully reported due to limited access to probability distributions and budget-limited setup.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize ChatGPT's superior prompting performance may be due to RLHF/instruction-tuning differences but cannot analyze further due to model closedness; still hampered by lack of CSW pretraining representation for generating code-switched targets.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluation limited by budget; SA/MT/SUM evaluated zero-shot (ChatGPT asked to return exact label for SA), LID evaluated with 1-shot. Exact output probabilities unavailable; evaluation via exact string matching for SA labels. Observed ChatGPT strong in following difficult instructions (e.g., LID) with few examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multilingual Large Language Models Are Not (Yet) Code-Switchers', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9331.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9331.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scaling × Format interaction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of model scaling on performance for monolingual vs code-switched inputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper analyzes scaling trajectories across model sizes and finds scaling improves performance but the benefit is larger for monolingual inputs than for code-switched inputs; for some architectures (encoder-decoder) scaling helps more than for decoder-only models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multilingual Large Language Models Are Not (Yet) Code-Switchers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>mT0 family (encoder-decoder), BLOOMZ (decoder-only), XGLM (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>multiple sizes studied (mT0: 300M, 580M, 1.2B, 3.7B, 13B; BLOOMZ/XGLM: ranges up to 7.5B etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine Translation (Hindi→English vs Hinglish→English) and Summarization (Hinglish→English vs English→English)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compare scaling effects on equivalent monolingual versus code-switched input tasks (translation and summarization).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot prompting for generative models; evaluation across model sizes (scaling curve analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Monolingual source (Hindi or English) vs code-switched source (Hinglish) across increasing model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Scaling yields more pronounced improvements on monolingual inputs than on code-switched inputs; encoder-decoder models (mT0) show clearer scaling gains for MT while decoder-only models (BLOOMZ, XGLM) show minimal gains and overall poorer performance on CSW MT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Paper reports regression coefficients (β) showing larger β for monolingual setups than for code-switched ones (qualitative summary: scaling helps monolingual more than CSW).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>No single unified numeric effect size reported; regression coefficients in paper (Table 3 / Figure 3) indicate visibly steeper scaling slopes for monolingual inputs than for code-switched inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Hypothesized cause: limited pretraining samples of code-switched texts yield suboptimal scaling behavior for CSW; models therefore fail to take full advantage of scale when inputs contain code-switching.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Scaling analysis used multiple sizes of mT0, BLOOMZ, XGLM and compared performance curves for Hindi→English vs Hinglish→English translation and Hinglish→English vs English→English summarization. Figures and regression coefficients (β) used to quantify scaling effects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multilingual Large Language Models Are Not (Yet) Code-Switchers', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 2)</em></li>
                <li>Crosslingual generalization through multitask finetuning <em>(Rating: 1)</em></li>
                <li>Scaling instruction-finetuned language models <em>(Rating: 2)</em></li>
                <li>The flan collection: Designing data and methods for effective instruction tuning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9331",
    "paper_id": "paper-258840847",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Fine-tune vs Prompt",
            "name_full": "Fine-tuning (supervised) versus zero-/few-shot prompting",
            "brief_description": "Comparison of supervised fine-tuning on task-specific CSW training data versus zero-shot and few-shot prompt-based inference with multilingual LLMs; fine-tuned smaller models substantially outperform prompted large LLMs across code-switched tasks.",
            "citation_title": "Multilingual Large Language Models Are Not (Yet) Code-Switchers",
            "mention_or_use": "use",
            "model_name": "various (XLM-R, mBERT, mDeBERTa, mT0 p, mBART, M2M100)",
            "model_size": "various (reported ranges in paper include 178M up to 13B+ for different models)",
            "task_name": "Sentiment Analysis (SA); Machine Translation (MT); Summarization (SUM); Word-level Language Identification (LID)",
            "task_description": "Code-switched (CSW) variants of standard NLP tasks: classify sentiment, translate between Hinglish/Hindi and English, summarize Hinglish conversations to English, and per-token language ID.",
            "presentation_format": "Fine-tuning: supervised training on task-specific CSW training sets (standard cross-entropy fine-tuning). Prompting: zero-shot and few-shot in-context prompting (0,1,3,5-shot) using natural-language instruction prompts / templates.",
            "comparison_format": "Zero-shot and few-shot prompting (0/1/3/5-shot) vs full supervised fine-tuning on task training data.",
            "performance": "Qualitative summary from paper: fine-tuned smaller models achieve substantially higher task metrics than prompted LLMs; e.g., for MT (Hinglish→English) fine-tuned models obtain ~25–32 BLEU while best prompted mT0-XXL achieves ≤20 BLEU; for LID fine-tuned models reach ~80 F1 vs prompted LLMs near ~20 F1 in best few-shot settings. For SA, fine-tuned models typically reach much higher F1 (many reported ~70–80 F1) compared to prompting (around majority baseline / ~50 F1 for some LLMs).",
            "performance_comparison": "See above; concrete examples: MT fine-tuned 25–32 BLEU vs prompted mT0-XXL ≤20 BLEU; LID fine-tuned ≈80 F1 vs prompted ≈20 F1 (5-shot best public LLMs).",
            "format_effect_size": "Examples reported in paper: MT: roughly −5 to −12 BLEU for prompting vs fine-tuning; LID: roughly −60 F1 (prompting ~20 vs fine-tuned ~80); SA: prompting near baseline (≈46–50 F1) vs fine-tuned ≈70–80 F1 (≈+20–30 F1 improvement with fine-tuning).",
            "explanation_or_hypothesis": "Authors hypothesize that multilingual pretraining and instruction-tuning do not confer code-switching competence; CSW-specific structure and scarce CSW pretraining samples mean LLMs lack representations to perform CSW tasks well without supervised fine-tuning or CSW-specific training/augmentation.",
            "null_or_negative_result": true,
            "experimental_details": "Fine-tuning: used all available CSW training instances per task (SA up to ~9k training examples for some language pairs; MT MixMT Hinglish had 8,060 training pairs; SUM Gupshup 5,831 train pairs; LID subsets from LinCE). Fine-tuning optimizer and hyperparameters described (Adafactor, lr schedules, epochs). Prompting: 0-shot for SA/MT/SUM and 5-shot for LID reported in main results; explored 0,1,3,5-shot settings with 5 different prompt templates (Appendix C). Evaluation metrics: macro F1 / Accuracy for SA and LID, BLEU / spBLEU for MT, ROUGE-L for SUM.",
            "uuid": "e9331.0",
            "source_info": {
                "paper_title": "Multilingual Large Language Models Are Not (Yet) Code-Switchers",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Zero- vs Few-shot (0/1/3/5)",
            "name_full": "Zero-shot versus few-shot (in-context) prompting effects",
            "brief_description": "Paper evaluates how adding in-context examples (1,3,5 shots) changes performance relative to zero-shot prompting across CSW tasks and models, finding task-dependent effects: few-shot can hurt SA and SUM, has negligible impact for MT, and helps LID.",
            "citation_title": "Multilingual Large Language Models Are Not (Yet) Code-Switchers",
            "mention_or_use": "use",
            "model_name": "BLOOMZ, mT0, XGLM (various sizes), GPT-3.5 turbo (ChatGPT)",
            "model_size": "various (examples in paper: BLOOMZ 560M–7B, mT0 300M–13B, XGLM 564M–7.5B, GPT-3.5 turbo)",
            "task_name": "SA, MT, SUM, LID",
            "task_description": "Standard tasks applied to code-switched inputs: sentiment classification, translation, summarization, and token-level language ID.",
            "presentation_format": "Prompting with 0, 1, 3, and 5 in-context examples (templates varied). For SA the label is read as the next-token probability; for LID prompting expects exact bracketed [ token | tag ] outputs; MT and SUM use standard sequence generation prompts.",
            "comparison_format": "0-shot versus 1-shot, 3-shot, 5-shot (paper reports trajectories in Figure 4 and discusses per-task differences).",
            "performance": "Task-dependent: SA and SUM often saw a decrease from 0-shot to 1-shot (authors report a drop in metrics when adding examples); MT showed negligible change with increasing examples; LID benefited from more in-context examples (best performance typically at 5-shot for public LLMs).",
            "performance_comparison": "Qualitative comparisons: SA & SUM: 0-shot &gt; 1-shot in many cases (metric drops observed); MT: performance roughly flat across shot counts; LID: 0-shot results were zero, while 5-shot yields measurable F1 (public models' 5-shot ~20 F1 in best case).",
            "format_effect_size": "No single global numeric effect size given; paper reports decreased metrics for SA/SUM when moving from 0→1 shot (qualitative), MT negligible delta, LID improved from 0 (all models) to ~20 F1 (best public LLMs at 5-shot) — i.e., a large relative gain from adding examples for LID but still far below fine-tuning.",
            "explanation_or_hypothesis": "Possible reasons: models saw tasks monolingually during pretraining and thus perform well zero-shot; CSW examples can be perceived as low-quality/noisy, confusing generation in SA and SUM. For LID, sequence-tagging format is novel so in-context examples teach the required output format, improving performance. For MT, models may not recognize or represent target CSW form (Hinglish) and thus ignore examples.",
            "null_or_negative_result": true,
            "experimental_details": "Explored 0,1,3,5-shot; used 5 prompt templates; main reported settings: SA/MT/SUM results reported for 0-shot in main tables; LID main results reported for 5-shot because 0-shot was uniformly 0. For SA they compute probability of generating each label token and pick the highest-probability label; LID parsing uses a dynamic programming extraction (Paolini et al., 2021). ChatGPT evaluations limited by budget: 0-shot for SA/MT/SUM and 1-shot for LID.",
            "uuid": "e9331.1",
            "source_info": {
                "paper_title": "Multilingual Large Language Models Are Not (Yet) Code-Switchers",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LID format-following",
            "name_full": "Language Identification: bracketed [ token | tag ] output format sensitivity",
            "brief_description": "Task requires per-token language labels using a strict [ token | tag ] textual format; multilingual LLMs largely fail at following this generation format in 0-shot, and even in few-shot they often produce invalid or malformed outputs leading to poor scores.",
            "citation_title": "Multilingual Large Language Models Are Not (Yet) Code-Switchers",
            "mention_or_use": "use",
            "model_name": "BLOOMZ, mT0, XGLM, GPT-3.5 turbo (ChatGPT)",
            "model_size": "various (examples include BLOOMZ 560M–7B, mT0 up to 13B, XGLM up to 7.5B, GPT-3.5 turbo)",
            "task_name": "Word-level Language Identification (LID)",
            "task_description": "For each token in a sentence, identify its language (lang1/lang2/other) and produce outputs in a strict [ token | tag ] format (e.g., [hello|lang1]).",
            "presentation_format": "Natural language instruction demanding output in exact bracketed format. Evaluated in 0-shot (all models produced 0 scores) and with in-context examples (1–5-shot); main reported prompting results use 5-shot.",
            "comparison_format": "0-shot (no examples) versus few-shot (1,3,5 examples) prompting; additionally compared against supervised fine-tuned sequence-tagging models.",
            "performance": "0-shot: all prompting models returned 0 score; 5-shot (best public prompted LLMs) reached roughly ~20 F1 (paper's summary); fine-tuned models reached ~80 F1.",
            "performance_comparison": "0-shot = 0 F1 for all public LLMs; 5-shot prompting ≈20 F1 (public LLMs) vs fine-tuned ≈80 F1.",
            "format_effect_size": "Huge effect: moving from 0-shot→5-shot changed score from 0→~20 F1 (prompting), but fine-tuning still outperformed prompting by ≈60 F1.",
            "explanation_or_hypothesis": "Authors attribute failures to models not being accustomed to sequence tagging outputs and not adequately following strict format constraints in generation; in-context examples help because they demonstrate the required structure, but LLMs still struggle to replicate exact per-token outputs.",
            "null_or_negative_result": false,
            "experimental_details": "LID parse: model output parsed with a dynamic programming algorithm (Paolini et al., 2021) to extract valid [token|tag] pairs. Main LID prompting results reported for 5-shot (0-shot uniformly 0). Few-shot examples improved performance somewhat; even the best billion-parameter prompted models often produced malformed brackets or multi-token labels inside a single bracket.",
            "uuid": "e9331.2",
            "source_info": {
                "paper_title": "Multilingual Large Language Models Are Not (Yet) Code-Switchers",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "MT: CSW instruction mismatch",
            "name_full": "Machine Translation prompt-target mismatch for code-switched targets (e.g., English→Hinglish)",
            "brief_description": "When prompts instruct LLMs to generate code-switched (Hinglish) targets, many multilingual LLMs output monolingual English instead of the requested CSW form; prompting yields significantly lower BLEU than fine-tuning for CSW MT.",
            "citation_title": "Multilingual Large Language Models Are Not (Yet) Code-Switchers",
            "mention_or_use": "use",
            "model_name": "mT0 family (300M–13B, including mT0-XXL), BLOOMZ, XGLM",
            "model_size": "various; example cited: mT0-XXL (very large), mT0 13B, others between 300M–13B",
            "task_name": "Machine Translation (Hinglish↔English)",
            "task_description": "Translate between Hinglish (Hindi-English code-mixed) and English. Paper focuses on Hinglish→English and English→Hinglish directions from MixMT 2022 dataset.",
            "presentation_format": "Zero-shot and few-shot natural-language translation prompts (e.g., 'Translate [src] to [tgt]: [sequence]'). Few-shot included in-context Hinglish examples aiming to teach code-switched target form.",
            "comparison_format": "Prompting (0/1/3/5-shot) vs supervised fine-tuning of MT models (mT0 p variants, M2M100, mBART-50, etc.). Also compared monolingual (Hindi→English) scaling vs code-switched (Hinglish→English) scaling.",
            "performance": "Prompted best public LLM (mT0-XXL) achieved ≤20 BLEU on Hinglish→English, while fine-tuned models achieved ~25–32 BLEU. For English→Hinglish generation, prompted LLMs often output English unchanged rather than producing Hinglish.",
            "performance_comparison": "Hinglish→English: prompted ≤20 BLEU (best) vs fine-tuned 25–32 BLEU. English→Hinglish: prompted models frequently failed to produce code-switched targets, repeating the original English sentences.",
            "format_effect_size": "Reported example effect: ~5–12 BLEU lower for best prompting model compared to fine-tuned models on Hinglish→English.",
            "explanation_or_hypothesis": "Authors suggest LLMs lack sufficient pretraining exposure to code-switched target formats; CSW can be perceived as noise under monolingual-focused pretraining/instruction-tuning so models default to monolingual outputs even when prompted otherwise.",
            "null_or_negative_result": true,
            "experimental_details": "MT dataset: MixMT 2022 Hinglish-English (8,060 train, 942 val, 960 test). Prompt templates for mT0 included 'Translate [src] to [tgt]: [sequence]'. Paper gives a concrete qualitative example where mT0 13B, with 5 in-context examples instructing English→Hinglish, produced an English paraphrase instead of Hinglish code-mix. Evaluated with BLEU/spBLEU metrics.",
            "uuid": "e9331.3",
            "source_info": {
                "paper_title": "Multilingual Large Language Models Are Not (Yet) Code-Switchers",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ChatGPT (GPT-3.5 turbo) prompting",
            "name_full": "GPT-3.5 turbo (ChatGPT) zero-/one-shot prompting behavior on CSW tasks",
            "brief_description": "Closed hosted model (GPT-3.5 turbo) shows strong zero/one-shot performance on most code-switched tasks, comparable to fine-tuned models and better than public LLMs, but still struggles with generating code-switched MT targets (English→Hinglish).",
            "citation_title": "Multilingual Large Language Models Are Not (Yet) Code-Switchers",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 turbo (ChatGPT)",
            "model_size": null,
            "task_name": "SA, MT, SUM, LID (evaluated zero-shot for SA/MT/SUM, 1-shot for LID)",
            "task_description": "Same CSW tasks as the paper's benchmark; ChatGPT used as a hosted instruction-following LLM to compare prompting performance.",
            "presentation_format": "Zero-shot prompting for SA/MT/SUM; 1-shot prompting for LID (budget-limited in experiments). In SA, instructed to return exact string label and F1 computed by exact match; for LID one example provided to teach bracketed tagging format.",
            "comparison_format": "Compared against public multilingual LLMs (BLOOMZ, mT0, XGLM) under same prompt templates and against fine-tuned models.",
            "performance": "ChatGPT achieves performance comparable to fine-tuned models and significantly better than other public multilingual LLMs on most tasks; exception: English→Hinglish MT where its zero-shot performance is only slightly better than other public LLMs.",
            "performance_comparison": "Qualitative: ChatGPT often matches fine-tuned scores on SA/SUM/LID (1-shot) and outperforms other publicly-available LLMs; detailed numeric comparisons not fully reported due to limited access to probability distributions and budget-limited setup.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors hypothesize ChatGPT's superior prompting performance may be due to RLHF/instruction-tuning differences but cannot analyze further due to model closedness; still hampered by lack of CSW pretraining representation for generating code-switched targets.",
            "null_or_negative_result": false,
            "experimental_details": "Evaluation limited by budget; SA/MT/SUM evaluated zero-shot (ChatGPT asked to return exact label for SA), LID evaluated with 1-shot. Exact output probabilities unavailable; evaluation via exact string matching for SA labels. Observed ChatGPT strong in following difficult instructions (e.g., LID) with few examples.",
            "uuid": "e9331.4",
            "source_info": {
                "paper_title": "Multilingual Large Language Models Are Not (Yet) Code-Switchers",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Scaling × Format interaction",
            "name_full": "Effect of model scaling on performance for monolingual vs code-switched inputs",
            "brief_description": "Paper analyzes scaling trajectories across model sizes and finds scaling improves performance but the benefit is larger for monolingual inputs than for code-switched inputs; for some architectures (encoder-decoder) scaling helps more than for decoder-only models.",
            "citation_title": "Multilingual Large Language Models Are Not (Yet) Code-Switchers",
            "mention_or_use": "use",
            "model_name": "mT0 family (encoder-decoder), BLOOMZ (decoder-only), XGLM (decoder-only)",
            "model_size": "multiple sizes studied (mT0: 300M, 580M, 1.2B, 3.7B, 13B; BLOOMZ/XGLM: ranges up to 7.5B etc.)",
            "task_name": "Machine Translation (Hindi→English vs Hinglish→English) and Summarization (Hinglish→English vs English→English)",
            "task_description": "Compare scaling effects on equivalent monolingual versus code-switched input tasks (translation and summarization).",
            "presentation_format": "Zero-shot prompting for generative models; evaluation across model sizes (scaling curve analysis).",
            "comparison_format": "Monolingual source (Hindi or English) vs code-switched source (Hinglish) across increasing model sizes.",
            "performance": "Scaling yields more pronounced improvements on monolingual inputs than on code-switched inputs; encoder-decoder models (mT0) show clearer scaling gains for MT while decoder-only models (BLOOMZ, XGLM) show minimal gains and overall poorer performance on CSW MT.",
            "performance_comparison": "Paper reports regression coefficients (β) showing larger β for monolingual setups than for code-switched ones (qualitative summary: scaling helps monolingual more than CSW).",
            "format_effect_size": "No single unified numeric effect size reported; regression coefficients in paper (Table 3 / Figure 3) indicate visibly steeper scaling slopes for monolingual inputs than for code-switched inputs.",
            "explanation_or_hypothesis": "Hypothesized cause: limited pretraining samples of code-switched texts yield suboptimal scaling behavior for CSW; models therefore fail to take full advantage of scale when inputs contain code-switching.",
            "null_or_negative_result": null,
            "experimental_details": "Scaling analysis used multiple sizes of mT0, BLOOMZ, XGLM and compared performance curves for Hindi→English vs Hinglish→English translation and Hinglish→English vs English→English summarization. Figures and regression coefficients (β) used to quantify scaling effects.",
            "uuid": "e9331.5",
            "source_info": {
                "paper_title": "Multilingual Large Language Models Are Not (Yet) Code-Switchers",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 2,
            "sanitized_title": "training_language_models_to_follow_instructions_with_human_feedback"
        },
        {
            "paper_title": "Crosslingual generalization through multitask finetuning",
            "rating": 1,
            "sanitized_title": "crosslingual_generalization_through_multitask_finetuning"
        },
        {
            "paper_title": "Scaling instruction-finetuned language models",
            "rating": 2,
            "sanitized_title": "scaling_instructionfinetuned_language_models"
        },
        {
            "paper_title": "The flan collection: Designing data and methods for effective instruction tuning",
            "rating": 2,
            "sanitized_title": "the_flan_collection_designing_data_and_methods_for_effective_instruction_tuning"
        }
    ],
    "cost": 0.0199095,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Multilingual Large Language Models Are Not (Yet) Code-Switchers</p>
<p>Ruochen Zhang ruochen_zhang@brown.edu 
Equal contribution</p>
<p>Samuel Cahyawijaya scahyawijaya@connect.ust.hk 
Equal contribution</p>
<p>Jan Christian 
Equal contribution</p>
<p>Blaise Cruz jcb.cruz@samsung.com 
Equal contribution</p>
<p>Genta Indra 
Equal contribution</p>
<p>Alham Fikri Aji 
Equal contribution</p>
<p>Hkust 
Equal contribution</p>
<p>Samsung R&amp;d 
Equal contribution</p>
<p>Institute Philippines 
Equal contribution</p>
<p>Bloomberg 
Equal contribution</p>
<p>Mbzuai 
Equal contribution</p>
<p>Multilingual Large Language Models Are Not (Yet) Code-Switchers
DE4A02A8B8A2080206E17288EF2C6930
Multilingual Large Language Models (LLMs) have recently shown great capabilities in a wide range of tasks, exhibiting state-of-the-art performance through zero-shot or few-shot prompting methods.While there have been extensive studies on their abilities in monolingual tasks, the investigation of their potential in the context of code-switching (CSW), the practice of alternating languages within an utterance, remains relatively uncharted.In this paper, we provide a comprehensive empirical analysis of various multilingual LLMs, benchmarking their performance across four tasks: sentiment analysis, machine translation, summarization and wordlevel language identification.Our results indicate that despite multilingual LLMs exhibiting promising outcomes in certain tasks using zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales.We argue that current "multilingualism" in LLMs does not inherently imply proficiency with code-switching texts, calling for future research to bridge this discrepancy.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have shown their potential in the context of zero-shot and few-shot prompting (Brown et al., 2020;Kojima et al., 2022;Wei et al., 2022;Longpre et al., 2023).The successes of these LLMs have also been effective in multilingual settings (Lin et al., 2021;Winata et al., 2021b;Scao et al., 2022) where models are specifically trained to learn individual languages, proven to be highly beneficial for monolingual tasks.However, in multilingual communities, people do not confine themselves to speaking only a single language; instead, they use two or more languages interchangeably during a conversation -a phenomenon known as code-switching (CSW) (Poplack, 1980(Poplack, , 2001)).It allows individuals to communicate cultural-specific concepts more effectively, signaling their group identity and reinforcing their social connection (Dogruöz et al., 2021).Yet, existing multilingual LLMs are not specifically trained with objectives for managing CSW scenarios.Hence, assessing the capabilities of the current multilingual LLMs in processing CSW texts is essential to the development of multilingual language models that are fully compatible with code-switching.</p>
<p>The main challenge of developing multilingual LLMs optimized for code-switching lies in data scarcity.Given the highly colloquial characteristic of code-switching (Winata et al., 2022b), existing resources dedicated to CSW are rare and collection at large-scale requires considerable annotation efforts.To mitigate such deficiency, Yong et al. (2023) investigate the possibility of employing multilingual LLMs to generate high-quality synthetic CSW texts.The study revealed that, hosted LLMs, such as InstructGPT (Ouyang et al., 2022) and ChatGPT1 outperform public models like BLOOMZ (Muennighoff et al., 2022) and Flan-T5-XXL (Chung et al., 2022) in generating naturalsounding CSW texts.However, the quality of the generated text by these hosted LLMs is mostly confined to Singlish and significantly declines when prompted for other languages.Despite the preliminary promising results, the generation of highquality CSW texts still remains challenging.This observation motivates us to probe from a different perspective -Can existing multilingual LLMs effectively understand CSW?</p>
<p>There have been previous studies on evaluating multilingual LMs in CSW scenarios (Tan and Joty, 2021;Adilazuarda et al., 2022), where codeswitched texts are simulated by replacing words from parallel corpora.Winata et al. (2021a) also assesses models' effectiveness by experimenting with word embeddings constructed from different methods.These works are mainly built upon small Alright that is fine.What is the movie?
[Hello|lang1][koi|lang2][mere| lang2][se|lang2][frndship|lang1] [krlo|lang2][mere|lang2][se|lang2] [hor|lang2][singel|lang1][nai| lang2][raha|lang2][jata|lang2]
Carter is not joining Ivy and Chloe due to a family reunion.</p>
<p>Carter's grandfather is very ill.</p>
<p>Summarization</p>
<p>Figure 1: Illustration of tasks included in our benchmark study.</p>
<p>BERT-based models and are restricted by either the number of languages or tasks investigated.Given the recent success of prompting methods on multilingual LLMs and the effects of scaling, this paper presents a more comprehensive empirical analysis of models' code-switching abilities, including a variety of languages, task types, model architectures, model sizes and prompting methods.</p>
<p>Our results suggest that the scaling law is applicable to multilingual LLMs across diverse CSW tasks and model architectures.However, fine-tuned smaller-scale models substantially outperform the largest multilingual LLM with prompting methods.In addition, while hosted LLMs achieve scores comparable to our fine-tuned models, such performance remains uninterpretable due to their closedness.We argue that existing multilingual LLMs exhibit limited proficiency in code-switching contexts, highlighting future research opportunities to transform them into true polyglots.</p>
<p>Experimental Setup</p>
<p>Datasets</p>
<p>We explore four code-switching task categories: sentiment analysis (SA), machine translation (MT), summarization (SUM), and word-level language identification (LID).The description of each task is as follows:</p>
<p>Sentiment Analysis We use sentiment analysis datasets of three different language pairs: Sentimix Spanish-English (Aguilar et al., 2020), MixSenti-ment Malayalam (Chakravarthi et al., 2020a), and MixSentiment Tamil (Chakravarthi et al., 2020b).Besides the common positive and negative labels, these datasets also contain extra labels like neutral or other.However, occurrences of those labels are very scarce.Hence, to normalize datasets from different sources, we simplify the data by filtering out examples outside positive or negative labels.The dataset sizes, broken down into train/validation/test, are as follows: 8,831/8,831/1,342 pairs for the Spanish-English subset, 2,541/275/703 for Malayalam, and 9,075/1,022/2,499 for the Tamil subset.</p>
<p>Machine Translation</p>
<p>We use the code-switched datasets from MixMT 2022 shared task (Srivastava and Singh, 2022) that contains Hinglish-English sentence pairs (8,060 pairs in the training split, 942 pairs in validation and 960 pairs in the test split).</p>
<p>Summarization We use code-switched summarization dataset Gupshup (Mehnaz et al., 2021), which is derived from SAMSum (Gliwa et al., 2019) via crowdsourcing translation.In our experiment, We focus on Hinglish to English, as evaluating code-switched summary systematically with existing auto metrics has shown to be challenging for multilingual LLMs (Zhang and Eickhoff, 2023).The dataset contains 5,831 source-target pairs for training, with 500 pairs each for validation and testing.the LinCE benchmark (Aguilar et al., 2020).In this task, the system is tasked with classifying the language of each word in a sentence into one of the three classes, lang1, lang2</p>
<p>Word-level LID</p>
<p>Models</p>
<p>Zero-shot and Few-shot Models For zero-shot and few-shot prompting, we explore various multilingual generative LLMs of different pretraining processes and architectures, including BLOOMZ, mT0 (Muennighoff et al., 2022) and XGLM (Lin et al., 2021).We explore all model sizes except for BLOOMZ 175B due to resource limitations.</p>
<p>We also include ChatGPT into our analysis and specifically GPT-3.5 turbo is used.We explore 0, 1, 3, and 5-shot on each model with 5 diverse prompt templates.Details for each prompt can be seen in Appendix C.</p>
<p>For the SA task, we compute the probability of the model to generate each label as the next immediate continual generation, and then we pick the label resulting in the highest probability for the whole sequence.For MT, SUM and LID, we perform standard text generation.However, for LID, we expect the generated text to follow a predefined format where each [token, language tag] pair is represented as [ token | tag ].We parse the generation using a dynamic programming algorithm introduced in Paolini et al. (2021) to extract the valid [token, language tag] pairs for evaluation.</p>
<p>Fine-tuning Models In addition to zero-shot prompting models and few-shot in-context learning, we also experiment with fine-tuning as a bench-mark against prompting.For SA and word-level LID tasks, we fine-tune four models, namely, base and large variants of XLM-RoBERTa (Conneau et al., 2020), mBERT (Devlin et al., 2019), and mDeBERTa v3 (He et al., 2021).</p>
<p>For MT, we fine-tune eight models in total.These include small, base, and large variants of mT0 (Muennighoff et al., 2022); 418M and 1.2B variants of M2M100 (Fan et al., 2020); and standard, one-to-many, and many-to-many variants of mBART-50 (Liu et al., 2020; Tang et al., 2020) 2  For SUM, we follow the same setup used in MT, except we only fine-tune the three previously mentioned mT0 models and only the standard mBART-50 as the one-to-many and many-to-many variants are specifically for translation only.</p>
<p>Across all the tasks, we fine-tune the selected models on all the available training instances.Table 1 shows a full overview and comparison of the models investigated in this study and details for training setups for all tasks can be found in Appendix A.</p>
<p>Results and Discussion</p>
<p>Overall Results Figure 2 presents the results of various multilingual LLMs for the four CSW tasks. 3In general, we observe a scaling pattern when prompting multilingual LLMs across tasks.Nevertheless, the performance of these models significantly falls short when compared to that of substantially smaller fine-tuned models.Therefore, adopting a fine-tuned model is a more practical approach for dealing with CSW tasks, especially in scenarios with constrained computational resources.For ChatGPT, it demonstrates comparable performance to fine-tuned models across all tasks α Due to budget limitations, the results presented in GPT-3.5 turbo are based on 1-shot prompting instead of 5-shot.β,γ, δ Hng refers to Hinglish, a mix of Hindi and English.mBART50 refers to the many-to-many variant.mT0 p refers to the fine-tuned mT0 with prompt templates.</p>
<p>Table 2: Code-switching benchmark results for finetuned and prompting models.We report the 0-shot performance for the sentiment analysis, machine translation and summarization tasks; and 5-shot performance for the word-level language identification task.</p>
<p>and datasets, except for the English to Hinglish MT task.This exception may stem from the challenges in generating code-switched texts as outlined in previous research (Yong et al., 2023;Zhang and Eickhoff, 2023).For the remaining tasks, ChatGPT notably outperforms publicly available multilingual LLMs.Such discrepancy may be attributed to the RLHF objective in its pretraining process, although a comprehensive analysis is hindered by its proprietary nature.</p>
<p>Sentiment Analysis Results</p>
<p>Figure 5 shows a detailed breakdown for each of the three language datasets in the SA task.The results from fine-tuned models mainly reside in the top-left corner across all three datasets, highlighting their superior performance with considerably smaller sizes.Scaling BLOOMZ and XGLM yield small improvements, however, scores from mT0 fluctuate around 50 F1 when varying sizes.It's worth noting that the majority-class baseline of these three datasets has an average F1 score of 46.Considering the instability observed during the scaling-up process, mT0 struggles to understand the sentiment when presented in CSW texts.</p>
<p>Machine Translation Results</p>
<p>As shown in Figure 2 and Table 2, when the source is Hinglish and target English, the performance gap between prompting and fine-tuning in MT is much more apparent, with the best prompted LLM mT0-XXL achieving no more than 20 BLEU while all the fine-tuned models achieved between 25-32 BLEU score.In contrast to SA, we notice especially visible improvement during scaling up encoder-decoder style models such as mT0, while decoder-only models such as BLOOMZ and XGLM have minimal improvements given their overall poor performance.</p>
<p>We then compare the difference in LLM scaling between translation tasks with code-switched sources and monolingual ones4 .Figure 3 shows the scaling trajectory of LLMs for both Hindi → English and Hinglish → English translation direction; Table 3 presents the regression coefficient (β) in these two scenarios.A large coefficient indicates scaling has more noticeable impacts.We can observe that the influence of scaling is more apparent in monolingual sources than in the code-switched setup.This pattern could potentially result from the limited pretraining samples for Hinglish codeswitched data, leading to a sub-optimal scaling performance.</p>
<p>When models are tasked with translating the source into CSW text, a substantial performance decline is observed for both fine-tuned and prompted models.We notice that while the larger mT0 models are capable of producing English translations in a zero-shot manner, they struggle to generate CSW texts as seen in previous work (Yong et al., 2023).Upon looking at the output, mT0 simply outputs in English, even in few-shot settings in which it has seen some other Hinglish examples.</p>
<p>Summarization Results</p>
<p>Figure 2 shows the fine-tuning and zero-shot prompting result on the summarization task.Similarly, we see that fine-tuned models outperform the zero-shot approach.Similar to MT, mT0 yields the overall best performance and shows positive scaling law.</p>
<p>To disentangle CSW from the equation, we evaluate the LLM's performance on the same Gupshup dataset, but with English input rather than Hinglish input.The evaluation set is parallel to each other.Interestingly, from Figure 3 and Table 3, we see a similar scaling impact whether the input is monolingual or in code-switch.However, the models are consistently better if the input is in English.</p>
<p>Language Identification Results</p>
<p>Our observation of fine-tuned models in the LID task is similar to the MT task: they outperform prompting methods on multilingual LLMs by a significant margin.In Table 2, we report 5-shots instead of 0-shot prompting results for LID tasks as 0-shot results are all 0 for both language datasets and across all models.The multilingual LLMs are not able to understand the natural language instruction that requires them to generate outputs in a specific format like [ token | tag ] word by word.When prepending more in-context examples in the instruction, we observe slight performance improvements across different models.For results on few-shot experiments for LID, please refer to Section 3.5.</p>
<p>Few-Shot Results</p>
<p>Compared to zero-shot inference, few-shot learning has been shown to boost performance as discussed in previous works (Brown et al., 2020;Liu et al., 2021).However, in CSW settings, we observe different effects of adding more in-context examples between tasks.In Figure 4, we notice a decrease in metrics from 0-shot to 1-shot for SA and SUM, suggesting that in-context examples do not contribute to or even degrade models' performance.We suspect that models have seen these tasks in a monolingual fashion during pretraining, and thus are able to understand instructions well in a zero-shot setting.Instead, models may consider CSW examples as low-quality texts, thus confusing the generation process.For MT, we observe negligible change in the models' performances with an increasing number of examples.Notably, instead of translating sentences to Hinglish as instructed, models could only repeat the original English sentences.For instance, when provided with 5 in-context examples, mT0 13B is instructed to "Translate the following text from English to Hinglish.Text: hello there, I have not seen this movie so im going to take a minute to look it over :) Translation:".It generates "hello there, I have not seen this movie so I going to take time to look it over:)."instead of the expected "hello yar, mein is movie ko nahi dekha hoon tho, tho mein thode der ke liye isko dekh loonga".Similar issues are also observed with BLOOMZ.We hypothesize that models may not fully comprehend the nuances of 'Hinglish' within the given instruction, which could account for their relatively uniform performance across varying shot numbers.</p>
<p>On the contrary, more in-context examples benefit the LID task.As no models are pre-trained on the sequence tagging task, the natural instruction entailing the specific generation format is new to the LLMs.Therefore, in our experiments, most models perform best when given 5 learning examples.Additionally, though we observe scaling law patterns in 5-shot settings as shown in Figure 6, for the best-performing billion-parameter models, we still consistently observe their inability to adhere to the format laid out in the instructions.They often fail to replicate the exact words required for sentence tagging or predict multiple tokens within a single bracket pair.For example, in a 5-shot setting, when asked to label the sentence "we the fans luv you , sirji", BLOOMZ 7b wrongly generates " generation still results in a significant performance gap when compared to fine-tuning smaller models (∼20 F1 vs. ∼80 F1).</p>
<p>Benchmarking ChatGPT</p>
<p>Given recent developments in general-purpose, instruction-following LLMs like ChatGPT, with impressive zero-shot abilities across tasks, we also benchmark ChatGPT's performance in our CSW task.Limited by the budget, we only explore zeroshot performance for SA, MT and SUM given their easy scopes, and 1-shot performance for LID due to the specific output format requirements.Since we can't access ChatGPT's output probability distribution, we instruct ChatGPT to return only the exact string label and calculate F1 scores using exact string matching for SA.</p>
<p>ChatGPT achieves somewhat comparable performance to finetuning models and significantly outperforms other public multilingual LLMs in most of the tasks.Especially for LID, it shows strong capabilities in following difficult instructions with only one example.The only exception is on the English→Hinglish MT tasks, where its zero-shot performance is only slightly better than other public LLMs.We hypothesize mainly two reasons behind the difficulty in generating CSW texts: 1) as alluded to in the previous section, CSW texts can be perceived as noises given tasks and pretraining processes are designed in a monolingual fashion;</p>
<p>2) LLMs may have a lack of sufficient representation for CSW text structure.In our analysis, LLMs perform much better in SA tasks as they could pick up cues from individual works instead of paying attention to language "structure" when tasked with text generation.Lastly, while ChatGPT delivers promising results without any fine-tuning, the lack of complete transparency on its pretraining datasets, model architecture, and training details obstructs a better understanding of its performance.This presents roadblocks to future improvements in code-switching proficiency for public multilingual LLMs.</p>
<p>Implications for Future LLMs</p>
<p>In this section, we walk through various implications of our work and provide recommendations for enabling better CSW ability in LLMs.By highlighting this limitation, we compel researchers to consider CSW as a core feature of many people's multilingual repertoire across the world.</p>
<p>Fairer Data Representation for Code-Switching Our results in Section 3 show that existing LLMs have similar scaling patterns between monolingual and CSW.However, despite all the models under study having seen each of the languages during pretraining, there is still a performance gap between monolingual and CSW.This suggests that the ability to code-switch is not acquired by LLMs after pretraining and/or instruction-tuning with multilingual data (Xue et al., 2021;Scao et al., 2022;Muennighoff et al., 2022), indicating the need for adding better data representation for code-switching in the multilingual pretraining and/or instruction-tuning process.Such an approach can be done through manual CSW data collection and/or various data augmentation methods (Tan and Joty, 2021;Adilazuarda et al., 2022;Dhole et al., 2023).Aside from adding more CSW data, one potential solution is to identify and include the code-switching language pairs into consideration of multilingual pretraining and/or instruction-tuning.This allows better resampling strategy (Lample and Conneau, 2019;Aharoni et al., 2019;Conneau et al., 2020;Xue et al., 2021;Tang et al., 2021;Cahyawijaya et al., 2021) for CSW data during the multilingual pretraining and/or instruction-tuning.</p>
<p>Adaptation and Extension of Code-Switching</p>
<p>Optimization Objectives Existing LLMs are optimized solely with language modeling objectives either for sentence denoising or sentence completion.However, alternative optimization objectives" such as meta transfer learning (Winata et al., 2020) and additional token/span-level language identification objective (Li et al., 2019), have been demonstrated to effectively enhance CSW performance with minimal performance loss on monolingual tasks in CSW speech processing.By adapting and extending these approaches to NLP, we may be able to equip LLMs with better CSW capability without requiring expensive data collection and annotation.This would be particularly advantageous for LLMs, especially in applications where CSW is prevalent within the multilingual community.</p>
<p>Towards More Inclusive Language Technology</p>
<p>In light of the fact that LLMs are the driving force behind the progress of various NLP technologies (Thoppilan et al., 2022;SambaNova Systems, 2023;Pratap et al., 2023), we emphasize the importance of incorporating code-switched capabilities in LLMs to promote inclusivity and diversity in language technology, particularly for multilingual speakers who frequently engage in code-switching in their daily lives.By enabling NLP technology to reflect the language-mixing patterns of users, people can communicate in ways that are more comfortable and authentic to their linguistic identities, eliminating the need for people to adjust their speech patterns to become legible to machines.It would not only mitigate the effects of linguistic profiling (Baugh, 2005;Dingemanse and Liesenfeld, 2022) and hegemonic, Western-centric technological designs but also foster greater trust among users in language technology through naturalistic dialogue interactions.Therefore, we urge the integration of code-switched recognition and generation capabilities in future LLMs.</p>
<p>Related Work</p>
<p>Code-Switching Code-switching is a common practice observed in multilingual communities where people mix multiple languages within an utterance (Poplack, 2001).While more than half of the world population speaks more than one language, the availability of resources and assessments for code-switching is much more limited compared to the extensive literature on monolingual cases.The key challenges of collecting high-quality codeswitching data lie in the colloquial nature of the practice and the language proficiency required for accurate annotation (Winata et al., 2022b).The recent advances of "multilingual" large language models compel one to explore whether these models are proficient in code-switching contexts like a true polyglot.Previous research (Winata et al., 2021a) has studied the code-switching capabilities of language models in NER and POS-tagging tasks, however, the work is limited to using only differ-ent word embeddings and encoder-only models.In this paper, we expand on previous works and provide a detailed analysis of more model variations, task objectives and downstream applications of diverse language pairs adopted from existing CSW benchmarks like LinCE (Aguilar et al., 2020) and GlueCOS (Khanuja et al., 2020).</p>
<p>Multilingual Large Language Models Models like mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) have become the goto multilingual options for supervised fine-tuning, given their impressive abilities and adaptability to many languages.With the success of largescale generative models, their capabilities have been enriched with multilingual objectives (Lin et al., 2021;Scao et al., 2022;Muennighoff et al., 2022) through pretraining on large multilingual corpora such ROOTS (Laurençon et al., 2022), mC4 (Raffel et al., 2019) and xP3 (Muennighoff et al., 2022).In addition to excelling in different monolingual and multilingual benchmarks via zero-shot prompting (Sanh et al., 2021;Wei et al., 2021;Kojima et al., 2022;Muennighoff et al., 2022;Bang et al., 2023), research has shown that scaling up model sizes (Cahyawijaya et al., 2023;Kaplan et al., 2020;Fernandes et al., 2023) and incorporating in-context learning examples (Winata et al., 2022a;Tanwar et al., 2023) could help further boost their performance.Yet, given the scarcity of CSW evaluation resources, how these multilingual LLMs perform in code-switching scenarios still remains questionable.In this paper, we evaluate these models under various settings including fine-tuning, zero-shot prompting, and in-context learning, and provide recommendations for future improvements in code-switching proficiency.</p>
<p>Conclusion</p>
<p>In this paper, we systematically study multilingual LLMs' capabilities in code-switching tasks along various dimensions, including but not limited to finetuning vs. prompting, task objectives, scaling laws and model architecture.We observe that, despite improvements with larger sizes, existing multilingual LLMs still yield inferior performance compared to fine-tuning smaller models.We argue that multilingual LLMs are not necessarily code-switching compatible.Given that multilingual LLMs are not explicitly trained for codeswitching data, we recommend future development should incorporate a more comprehensive evaluation framework that encompasses code-switching texts.Finally, our study is limited to models' performance in sentiment analysis, machine translation, summarization and language identification.We suggest that benchmarking across a broader set of tasks is required.However, the scarcity of high-quality open-source code-switching datasets and the challenges associated with their collection process imply future work should also include constructing code-switching data with more complexity, such as commonsense reasoning.</p>
<p>Limitations</p>
<p>The scope of code-switching languages in this work is limited to Hindi-English, Standard-Egyptian Arabic, Spanish-English, Tamil-English, and Malayalam-English.It is beneficial to include more languages to demonstrate the generality of our claim.However, a challenge in doing so arises from the lack of available code-switched text datasets.We explore four different NLP downstream tasks.However, similar to the previous point, it would be interesting to cover more tasks.Similarly, the main challenge of expanding into different tasks is the lack of available datasets.We anticipate that future studies will broaden the exploration of code-switching languages and tasks beyond those examined in this research to showcase the gener-alizability of the findings to other code-switching languages and tasks.</p>
<p>In addition, in this study, we choose multilingual LLMs based on two criteria: 1) they present or advertise themselves as multilingual and 2) their pretraining data contain all the languages featured in our benchmark dataset.Although some recently released LLMs like Llama-2 (Touvron et al., 2023) and Falcon (Penedo et al., 2023) have demonstrated state-of-the-art performance across various other benchmarks, we defer the evaluation of their codeswitching capabilities to future research.</p>
<p>Finally, our observations are based on the model sizes allowed by our local compute resources.A more comprehensive analysis can be obtained by experimenting with a wider range of variations, including larger model sizes and more in-context examples given a more generous compute budget.</p>
<p>Ethical Considerations</p>
<p>Our paper highlights the evaluation of LLMs on code-switching, a common phenomenon in the multilingual community.The research was carried out in compliance with the principles of academic integrity, including honesty, transparency, and rigor.The data used in this study was collected in accordance with ethical guidelines, and all participants provided informed consent.Within our study, we are aware of the potential impact that comes with our work and our experiments replicate prior work under comparable experimental conditions.We also ensured that the study did not cause harm or distress to any individuals or communities.The findings of this study have important implications for the development of multilingual LLMs and their potential applications in code-switching tasks.However, we acknowledge that further research is needed to address the limitations and gaps identified in this study.We believe that responsible and ethical use of language technology is crucial for creating just and equitable systems that benefit all individuals and communities.</p>
<p>movie kis baare me hein?Translate the text above from Hinglish to English Language Identification For each token, identify its language (lang1: English, lang2: Hindi, other) using [ word | tag ].Hello koi mere se frndship krlo mere se hor singel nai raha jata =&gt; Ivy: Chloene bataya tum humare saath nahi aa rahe!Carter: mera ek family reunion around that time...Ivy: why? ... Carter: plan toh yahi hai at least Ivy: take care!Multilingual LLMs</p>
<p>Figure 2 :
2
Figure 2: Evaluation results of fine-tuning and prompting LLMs of different scales on various CSW tasks.(top left) F1-score on the sentiment analysis task, (top right) BLEU score on the machine translation task, (bottom left) ROUGE-L on the summarization task, and (bottom right) F1-score on the word-level language identification task.(FT) means results are from fine-tuned models.</p>
<p>Figure 3: Performance comparison on (top) Hindi→English vs Hinglish→English translation and (bottom) Hinglish→English vs English→English summarization.</p>
<p>Figure 4 :
4
Figure 4: Few-shot evaluation performance for (top left) sentiment analysis task, (top right) machine translation task, (bottom left) summarization task and (bottom right) word-level LID task.</p>
<p>Table 1 :
1
Comparison of different model variants studied in this paper.
We use English-Hindi and Mod-ern Standard Arabic (MSA) -Egyptian Arabic (EA)subsets from the Language Identification task in</p>
<p>ModelHng β →Eng Eng→Hng β Model
Sentiment AnalysisMachine TranslationSummarizationLanguage IdentificationModelF1 Mal-Eng Spa-Eng Tam-Eng FinetuningBLEU FinetuningRL Hng β →Eng FinetuningModelF1 Hin-Eng MSA-EA FinetuningXLMR 278M XLMR 560M mBERT 178M mDeBERTa 278M 44.56 77.08 79.94 78.2177.14 78.81 70.02 88.1768.12 M2M100 418M 68.28 mBART50 610M 65.19 mT0 p, 580M δ 45.56 mT0 p, 1.2B δγ28.53 29.53 25.47 31.8812.40 mT0 p, 300M 13.38 mT0 p, 580M 12.28 mT0 p, 1.2B δ 13.90 mBART50 610M δ δ82.44 86.65 81.99 39.03 mDeBERTa 278M 85.41 29.83 XLMR 278M 37.44 XLMR 560M 40.12 mBERT 178M72.58 79.79 68.02 68.020-shot Prompting0-shot Prompting0-shot Prompting5-shot PromptingmT0 300M mT0 580M mT0 1.2B mT0 3.7B mT0 13B BLOOMZ 560M BLOOMZ 1.1B BLOOMZ 1.7B BLOOMZ 3B BLOOMZ 7B XGLM 564M XGLM 1.7B XGLM 2.9B XGLM 4.5B XGLM 7.5B GPT-3.5 turbo36.79 44.60 55.62 35.27 49.97 59.64 50.64 47.83 56.84 64.21 52.18 50.83 60.15 62.32 60.93 65.9248.44 56.01 67.63 59.28 65.26 72.79 70.89 73.20 72.85 74.61 64.16 65.01 64.78 70.34 68.52 75.6442.26 mT0 300M 47.62 mT0 580M 53.88 mT0 1.2B 38.55 mT0 3.7B 50.76 mT0 13B 55.30 BLOOMZ 560M 53.27 BLOOMZ 1.1B 50.15 BLOOMZ 1.7B 53.41 BLOOMZ 3B 59.43 BLOOMZ 7B 52.66 XGLM 564M 50.55 XGLM 1.7B 56.43 XGLM 2.9B 56.94 XGLM 4.5B 56.04 XGLM 7.5B 63.15 GPT-3.5 turbo2.74 6.42 10.64 12.78 19.28 2.24 2.79 2.62 3.13 3.67 0.45 0.79 1.34 2.13 1.43 27.641.60 mT0 300M 2.37 mT0 580M 1.88 mT0 1.2B 2.08 mT0 3.7B 1.66 mT0 13B 1.37 BLOOMZ 560M 1.73 BLOOMZ 1.1B 2.62 BLOOMZ 1.7B 2.86 BLOOMZ 3B 1.88 BLOOMZ 7B 0.28 XGLM 564M 0.43 XGLM 1.7B 0.69 XGLM 2.9B 0.47 XGLM 4.5B 0.39 XGLM 7.5B 4.32 GPT-3.5 turbo16.00 mT0 300M 20.16 mT0 580M 23.63 mT0 1.2B 27.40 mT0 3.7B 30.67 mT0 13B 14.22 BLOOMZ 560M 16.45 BLOOMZ 1.1B 16.85 BLOOMZ 1.7B 20.97 BLOOMZ 3B 17.01 BLOOMZ 7B 4.29 XGLM 564M 5.42 XGLM 1.7B 5.75 XGLM 2.9B 4.73 XGLM 4.5B 5.92 XGLM 7.5B 25.07 GPT-3.5 turbo α2.13 0.30 0.22 0.19 7.51 5.38 16.31 13.04 19.61 19.58 6.65 5.90 17.64 19.35 16.91 80.190.90 0.00 0.27 1.49 5.07 2.08 10.56 3.37 17.47 9.26 1.61 6.27 10.75 20.51 18.91 71.41
https://chat.openai.com/
Due to space constraint, we show a selection of all finetuned models in Table2. For the full results, please refer to Appendix B.
3 Note that the results for SA, MT, SUM are derived from zero-shot prompting while LID results are based on 5-shot.
Monolingual experiments are conducted on WMT 2014 Hindi-English dataset(Bojar et al., 2014).
AcknowledgementsWe would like to thank Xinyu Hua and Samson Tan for the constructive feedback and helpful discussion on our project.A Fine-tuning Model SetupWe use a standard training setup for SA tasks: we fine-tune the models for a maximum of 15 epochs using the Adafactor(Shazeer and Stern, 2018)optimizer with a learning rate of 2e-5.All sequences are limited to a maximum sequence length of 256 tokens, truncating all sequences longer than this length, and dynamically padding the shorter sequences to the longest sequence length in their batch.All setups use a batch size of 128.We also use a linear warmup schedule, warming up for the first 10% of training steps before linearly decaying to 0. We measure Accuracy and Macro F1 as metrics for all setups, loading the best checkpoint based on the F1 score at the end for evaluation.Word-Level LID setups use the same one as with NLU tasks, except we only train for 3 epochs and use a weight decay of 0.01.Given that one word may be split into multiple tokens during tokenization, we first realign the labels by setting the word label as the label of its first token, then setting the labels of all succeeding tokens as -100.This "dummy" label is then ignored during loss computation.We also load the best checkpoint and use the same metrics as with NLU, in addition to Precision and Recall.For MT, we fine-tune for a maximum of 10 epochs using the Adafactor optimizer with a learning rate of 5e-5, loading the best checkpoint at the end.As the MT0 models are trained with instruction prompts, we also prepend a "prompt" to all sequences during fine-tuning in the form of Translate [src] to [tgt]: [sequence].For the M2M100 and mBART models, we force the decoder's first token to be the language token of the target language.All setups use a batch size of 512 sequences.We also use a similar linear warmup schedule as with the SA and LID task setups.For MT, we use spBLEU as our performance metric and load the best model for evaluation based on it.SUM follows most of the same setup that MT uses, except we only fine-tune for 3 epochs.For MT0, we use Summarize: [sequence] as our "prompt" that is prepended to all samples.We use ROUGE (ROUGE1, ROUGE2, ROUGEL, and ROUGEL-SUM) as our performance metric, loading the best model for evaluation based on it.B Fine-tuning Model ResultsC Prompt TemplatesThis section lists all prompts used for our experiment.Sentiment Analysis• [INPUT] =&gt; Sentiment:What would be the sentiment of the text above?• What is the sentiment of this text Text:[INPUT]Answer:Please classify the sentiment of above text.Sentiment:Machine Translation [SOURCE] and [TARGET] are Hinglish and English.Summarization• Summarize the following conversation in English.Conversation: [INPUT]Summary:Summarize the above conversation in English:How would you summarize that in English?• Summarize the following [SOURCE] conversation.Text: [INPUT] English summary:Summary in English:[SOURCE] is either Hinglish or English.Word-level LID• Determine the language for each token in the text below with [ word | tag ].Use lang1 for [LANG1], lang2 for [LANG2], and other for others.[INPUT]• For each token, identify its language (lang1: [LANG1] and [LANG2] are English and Hindi for LID-Hindi-English data, and Modern Standard Arabic and Egyptian Arabic for LID Standard-Egyptian Arabic data.D Detailed ResultsBreakdown results of SA and LID across different languages can be seen in Figure5and Figure6.
Indorobusta: Towards robustness against diverse code-mixed indonesian local languages. Muhammad Farid, Adilazuarda , Samuel Cahyawijaya, Genta Indra Winata, Pascale Fung, Ayu Purwarianti, Proceedings of the First Workshop on Scaling Up Multilingual Evaluation. the First Workshop on Scaling Up Multilingual Evaluation2022</p>
<p>Lince: A centralized benchmark for linguistic code-switching evaluation. Gustavo Aguilar, Sudipta Kar, Thamar Solorio, Proceedings of the Twelfth Language Resources and Evaluation Conference. the Twelfth Language Resources and Evaluation Conference2020</p>
<p>Massively multilingual neural machine translation. Roee Aharoni, Melvin Johnson, Orhan Firat, 10.18653/v1/N19-1388Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V Do, Yan Xu, Pascale Fung, 2023and interactivity</p>
<p>Linguistic profiling. John Baugh, Black linguistics. Routledge2005</p>
<p>Findings of the 2014 workshop on statistical machine translation. Ondřej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, Aleš Tamchyna, 10.3115/v1/W14-3302Proceedings of the Ninth Workshop on Statistical Machine Translation. the Ninth Workshop on Statistical Machine TranslationBaltimore, Maryland, USAAssociation for Computational Linguistics2014</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Instructalign: Teaching novel languages with to llms through alignment-based cross-lingual instruction. Samuel Cahyawijaya, Holy Lovenia, Tiezheng Yu, Willy Chung, Pascale Fung, 2023</p>
<p>IndoNLG: Benchmark and resources for evaluating Indonesian natural language generation. Samuel Cahyawijaya, Genta Indra Winata, Bryan Wilie, Karissa Vincentio, Xiaohong Li, Adhiguna Kuncoro, Sebastian Ruder, Yuan Zhi, Syafri Lim, Masayu Bahar, Ayu Khodra, Pascale Purwarianti, Fung, 10.18653/v1/2021.emnlp-main.699Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2021Online and Punta Cana</p>
<p>A sentiment analysis dataset for code-mixed malayalam-english. Navya Bharathi Raja Chakravarthi, Shardul Jose, Elizabeth Suryawanshi, John P Sherly, Mc-Crae, arXiv:2006.002102020aarXiv preprint</p>
<p>Corpus creation for sentiment analysis in code-mixed Tamil-English text. Vigneshwaran Bharathi Raja Chakravarthi, Ruba Muralidaran, John Priyadharshini, Philip Mccrae, Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL). the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL)Marseille, France2020bEuropean Language Resources association</p>
<p>Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.11416Scaling instruction-finetuned language models. 2022arXiv preprint</p>
<p>Unsupervised cross-lingual representation learning at scale. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Édouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies20191</p>
<p>. Kaustubh Dhole, Varun Gangal, Sebastian Gehrmann, Aadesh Gupta, Zhenhao Li, Saad Mahamood, Abinaya Mahadiran, Simon Mille, Ashish Shrivastava, Samson Tan, Tongshang Wu, Jascha Sohl-Dickstein, Jinho Choi, Eduard Hovy, Ondřej Dušek, Sebastian Ruder, Sajant Anand, Nagender Aneja, Rabin Banjade, Lisa Barthe, Hanna Behnke, Ian Berlot-Attwell, Connor Boyle, Caroline Brun, Marco Antonio Sobrevilla, Samuel Cabezudo, Emile Cahyawijaya, Wanxiang Chapuis, Mukund Che, Christian Choudhary, Pierre Clauss, Filip Colombo, Gautier Cornell, Mayukh Dagan, Tanay Das, Thomas Dixit, Paul-Alexis Dopierre, Suchitra Dray, Tatiana Dubey, Marco Ekeinhor, Tanya Di Giovanni, Rishabh Goyal, Louanes Gupta, Sang Hamla, Fabrice Han, Antoine Harel-Canada, Ishan Honoré, Przemysław Jindal, Denis Joniak, Venelin Kleyko, Kalpesh Kovatchev, Ashutosh Krishna, Stefan Kumar, Seungjae Langer, Ryan Lee, Corey James Levinson, Hualou Liang, Kaizhao Liang, Zhexiong Liu, Andrey Lukyanenko, Vukosi Marivate, Gerard De Melo, Simon Meoni, Maxine Meyer, Afnan Mir ; Pfister, Richard Plant, Vinay Prabhu, Vasile Pais, Libo Qin, Shahab Raji, Pawan Kumar Rajpoot, Vikas Raunak, Roy Rinberg, Nicholas Roberts, Juan Diego Rodriguez, Claude Roux, Vasconcellos Samus ; Chandan, Roman Singh, Priyank Sitelew, Taylor Soni, William Sorensen, Aman Soto, Aditya Srivastava, Tony Srivatsa, Mukund Sun, Varma, Fiona Tabassum, Ryan Tan, Mo Teehan, Marie Tiwari, Athena Tolkiehn, Zijian Wang, Zijie Wang, Gloria Wang, Fuxuan Wang, Bryan Wei, Genta Wilie, Xinyu Indra Winata, Witold Wu, Tianbao Wydmanski, Usama Xie, Michael Yaseen, Yee, 10.3384/nejlt.2000-1533.2023.4725Northern European Journal of Language Technology. Ananya Sai, Robin Schmidt, Thomas Scialom, Tshephisho Sefara, Saqib Shamsi, Xudong Shen, Yiwen Shi, Haoyue Shi, Anna Shvets, Nick Siegel, Damien Sileo, Jamie Simon,91Janand Yue Zhang. 2023. Nlaugmenter: A framework for task-sensitive natural language augmentatio</p>
<p>From text to talk: Harnessing conversational corpora for humane and diversity-aware language technology. Mark Dingemanse, Andreas Liesenfeld, 10.18653/v1/2022.acl-long.385Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>A survey of code-switching: Linguistic and social perspectives for language technologies. A Seza Dogruöz, Sunayana Sitaram, Barbara E Bullock, Jacqueline Almeida, Toribio, 10.18653/v1/2021.acl-long.131Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>. Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin, 2020Beyond english-centric multilingual machine translation</p>
<p>Patrick Fernandes, Behrooz Ghorbani, Xavier Garcia, Markus Freitag, Orhan Firat, arXiv:2302.09650Scaling laws for multilingual neural machine translation. 2023arXiv preprint</p>
<p>SAMSum corpus: A humanannotated dialogue dataset for abstractive summarization. Bogdan Gliwa, Iwona Mochol, Maciej Biesek, Aleksander Wawer, 10.18653/v1/D19-5409Proceedings of the 2nd Workshop on New Frontiers in Summarization. the 2nd Workshop on New Frontiers in SummarizationHong Kong, China2019Association for Computational Linguistics</p>
<p>Debertav3: Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing. Pengcheng He, Jianfeng Gao, Weizhu Chen, 2021</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Gluecos: An evaluation benchmark for codeswitched nlp. Simran Khanuja, Sandipan Dandapat, Anirudh Srinivasan, Sunayana Sitaram, Monojit Choudhury, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, ICML 2022 Workshop on Knowledge Retrieval and Language Models. 2022</p>
<p>Crosslingual language model pretraining. Guillaume Lample, Alexis Conneau, Advances in Neural Information Processing Systems (NeurIPS). 2019</p>
<p>The bigscience roots corpus: A 1.6 tb composite multilingual dataset. Lucile Hugo Laurençon, Thomas Saulnier, Christopher Wang, Le Akiki ; Teven, Leandro Scao, Chenghao Von Werra, Eduardo González Mou, Huu Ponferrada, Nguyen, Advances in Neural Information Processing Systems. 202235Albert Villanova del Moral</p>
<p>Towards code-switching asr for end-to-end ctc models. Ke Li, Jinyu Li, Guoli Ye, Rui Zhao, Yifan Gong, 10.1109/ICASSP.2019.8683223ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2019</p>
<p>Few-shot learning with multilingual language models. Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, arXiv:2112.106682021arXiv preprint</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, arXiv:2101.06804What makes good in-context examples for gpt-3?. 2021arXiv preprint</p>
<p>Multilingual denoising pretraining for neural machine translation. Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer, 10.1162/tacl_a_0034320208Transactions of the Association for Computational Linguistics</p>
<p>Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Barret Quoc V Le, Jason Zoph, Wei, arXiv:2301.13688The flan collection: Designing data and methods for effective instruction tuning. 2023arXiv preprint</p>
<p>GupShup: Summarizing open-domain code-switched conversations. Laiba Mehnaz, Debanjan Mahata, Rakesh Gosangi, Uma Sushmitha Gunturi, Riya Jain, Gauri Gupta, Amardeep Kumar, Isabelle G Lee, Anish Acharya, Rajiv Ratn Shah, 10.18653/v1/2021.emnlp-main.499Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational Linguistics2021Online and Punta Cana</p>
<p>Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, arXiv:2211.01786Crosslingual generalization through multitask finetuning. 2022arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, arXiv:2203.021552022arXiv preprint</p>
<p>Cicero Nogueira dos Santos, Bing Xiang, and Stefano Soatto. 2021. Structured prediction as translation between augmented natural languages. Giovanni Paolini, Ben Athiwaratkun, Jason Krone, Jie Ma, Alessandro Achille, Rishita Anubhai, arXiv:2101.05779arXiv preprint</p>
<p>Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, Julien Launay, arXiv:2306.01116The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. 2023arXiv preprint</p>
<p>S Poplack, 10.1016/b0-08-043076-7/03031-xCode switching: Linguistic. In International Encyclopedia of the Social &amp; Behavioral Sciences. Elsevier2001</p>
<p>Sometimes i'll start a sentence in spanish y termino en espanol: toward a typology of code-switching1. Shana Poplack, 1980</p>
<p>Andros Vineel Pratap, Bowen Tjandra, Paden Shi, Arun Tomasello, Sayani Babu, Ali Kundu, Zhaoheng Elkahky, Apoorv Ni, Maryam Vyas, Fazel-Zarandi, arXiv:2305.13516Scaling speech technology to 1,000+ languages. 2023arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, 2019text transformer. arXiv e-prints</p>
<p>Together Computer SambaNova Systems. BLOOMChat: a New Open Multilingual Chat LLM. 2023</p>
<p>. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, Canwen Saiful Bari, Urmish Xu, Shanya Thakker, Eliza Sharma Sharma, Taewoon Szczechla, Gunjan Kim, Nihal Chhablani, Debajyoti Nayak, Jonathan Datta, Mike Chang, Tian-Jian, Han Jiang, Matteo Wang, Sheng Manica, Zheng Xin Shen, Harshit Yong, Rachel Pandey, Thomas Bawden, Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan TeehanStella Biderman, Leo Gao, Tali Bers, Thomas WolfRush. 2021. Multitask prompted training enables zero-shot task generalization</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, arXiv:2211.05100Bloom: A 176b-parameter open-access multilingual language model. 2022arXiv preprint</p>
<p>Adafactor: Adaptive learning rates with sublinear memory cost. Noam Shazeer, Mitchell Stern, International Conference on Machine Learning. PMLR2018</p>
<p>Overview and results of MixMT shared-task at WMT 2022. Vivek Srivastava, Mayank Singh, Proceedings of the Seventh Conference on Machine Translation (WMT). the Seventh Conference on Machine Translation (WMT)Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Code-mixing on sesame street: Dawn of the adversarial polyglots. Samson Tan, Shafiq Joty, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2021</p>
<p>Multilingual translation with extensible multilingual pretraining and finetuning. Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan, arXiv:2008.004012020arXiv preprint</p>
<p>Multilingual translation from denoising pre-training. Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan, 10.18653/v1/2021.findings-acl.304Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Multilingual llms are better cross-lingual in-context learners with alignment. Eshaan Tanwar, Manish Borthakur, Subhabrata Dutta, Tanmoy Chakraborty, arXiv:2305.059402023arXiv preprint</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Lamda: Language models for dialog applications. 2022arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, arXiv:2109.01652arXiv preprint</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022</p>
<p>Crosslingual few-shot learning on unseen languages. Genta Winata, Shijie Wu, Mayank Kulkarni, Thamar Solorio, Daniel Preotiuc-Pietro, Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing. Long Papers. the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language ProcessingOnline only. Association for Computational Linguistics2022a1</p>
<p>Genta Indra Winata, Alham Fikri Aji, Zheng-Xin Yong, Thamar Solorio, arXiv:2212.09660The decades progress on code-switching research in nlp: A systematic survey on trends and challenges. 2022barXiv preprint</p>
<p>Meta-transfer learning for code-switched speech recognition. Genta Indra Winata, Samuel Cahyawijaya, Zhaojiang Lin, Zihan Liu, Peng Xu, Pascale Fung, 10.18653/v1/2020.acl-main.348Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Are multilingual models effective in codeswitching?. Genta Indra Winata, Samuel Cahyawijaya, Zihan Liu, Zhaojiang Lin, Andrea Madotto, Pascale Fung, Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching. the Fifth Workshop on Computational Approaches to Linguistic Code-Switching2021a</p>
<p>Language models are few-shot multilingual learners. Genta Indra Winata, Andrea Madotto, Zhaojiang Lin, Rosanne Liu, Jason Yosinski, Pascale Fung, Proceedings of the 1st Workshop on Multilingual Representation Learning. the 1st Workshop on Multilingual Representation Learning2021b</p>
<p>Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, 10.18653/v1/2021.naacl-main.41Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics</p>
<p>Prompting multilingual large language models to generate codemixed texts: The case of south east asian languages. Zheng-Xin Yong, Ruochen Zhang, Jessica Zosa Forde, Skyler Wang, Samuel Cahyawijaya, Holy Lovenia, Genta Indra Winata, Lintang Sutawika, Jan Christian Blaise, Long Cruz, Phan, arXiv:2303.135922023arXiv preprint</p>
<p>Crocosum: A benchmark dataset for cross-lingual code-switched summarization. Ruochen Zhang, Carsten Eickhoff, arXiv:2303.040922023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
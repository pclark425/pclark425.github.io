<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3028 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3028</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3028</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-261582750</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.03241v2.pdf" target="_blank">GPT Can Solve Mathematical Problems Without a Calculator</a></p>
                <p><strong>Paper Abstract:</strong> Previous studies have typically assumed that large language models are unable to accurately perform arithmetic operations, particularly multiplication of>8 digits, and operations involving decimals and fractions, without the use of calculator tools. This paper aims to challenge this misconception. With sufficient training data, a 2 billion-parameter language model can accurately perform multi-digit arithmetic operations with almost 100% accuracy without data leakage, significantly surpassing GPT-4 (whose multi-digit multiplication accuracy is only 4.3%). We also demonstrate that our MathGLM, fine-tuned from GLM-10B on a dataset with additional multi-step arithmetic operations and math problems described in text, achieves similar performance to GPT-4 on a 5,000-samples Chinese math problem test set. Our code and data are public at https://github.com/THUDM/MathGLM.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3028.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3028.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MathGLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MathGLM (Math General Language Model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A series of Transformer-based language models trained specifically to perform arithmetic by pretraining on a large, curated arithmetic dataset that encodes step-by-step solutions and by using curriculum learning; variants range from 10M to multi-billion parameters and include models trained from-scratch (decoder-only) for arithmetic and GLM-based fine-tuned variants for word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MathGLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based LMs: decoder-only architecture trained from scratch on an autoregressive objective for arithmetic tasks (variants: MathGLM-10M, 100M, 500M, 2B, 6B), and GLM-based fine-tuned variants (MathGLM fine-tuned from GLM-10B/GLM-6B/GLM-large/ChatGLM). Training uses digit-level tokenization (icetk), step-by-step solution-targets, and curriculum progression of example complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit integer and decimal addition/subtraction/multiplication/division/exponentiation; mixed chained operations (2–10 steps); fractions, percentages, negatives; Chinese math word problems (Ape210K reconstructed, K6).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Learns calculation rules and stepwise arithmetic by supervised exposure to decomposed intermediate steps (dataset-driven algorithmic learning); digit-level tokenization provides structured input representation; model and data scaling (parameters and dataset size/curriculum) improve performance—i.e., pattern/algorithm induction from many stepwise examples.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablation studies: applying the step-by-step strategy raises MathGLM-2B accuracy on the evaluation arithmetic dataset from ≈40.76% (direct-answer) to 93.03% (stepwise) and relative-error accuracy from ~94.26% to 99.71%; scaling analysis shows monotonic gains with model size and dataset size; adding 12-digit training examples improved generalization; error analysis shows most errors are single-step mistakes, consistent with learned multi-step computation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No internal probing of representations or algorithmic state machines is provided; remaining failures (notably some large-digit divisions and very long sequences) indicate the model has not learned a perfect symbolic algorithm; slight trade-offs observed in some MWP arithmetic-accuracy vs answer-accuracy when using step-by-step (i.e., step-generation can consume capacity).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Training-data intervention (step-by-step targets), curriculum learning (gradually increasing expression complexity and digit length), digit-level tokenization (icetk), targeted fine-tuning from GLM backbones for word problems, and adding high-digit (12-digit) examples for generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Major accuracy improvements on arithmetic tasks (step-by-step: +~52 percentage points for 2B model on mixed ops); improved relative-error metrics (~99+% within 1% RE); improved answer-accuracy on math word problems when Ape210K is reconstructed with stepwise solutions (absolute gains up to ~42.29% for GLM-10B fine-tuning). Curriculum/generalization examples enabled handling numbers up to 12 digits with much better performance than baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>MathGLM-2B: 93.03% accuracy on a 9,592-sample generated arithmetic test set (mixed operations); relative-error accuracy 99.71%. MathGLM-500M: 89.57% accuracy (RE 99.41%). MathGLM-100M: 70.28%; MathGLM-10M: 61.21%. On Ape210K-derived MWP test (5,000 samples) GLM-10B + MathGLM fine-tune attains performance comparable to GPT-4; on Big-Bench arithmetic, MathGLM maintains high accuracy even for higher-digit tasks where GPT-4 degrades. (Reported baseline: GPT-4 18.84% on the same mixed-ops dataset; other dataset-specific numbers appear in paper tables.)</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Errors concentrated in single calculation steps inside multi-step solutions; difficulty with certain large-digit divisions and very long expression sequences; some MWP failures due to language misunderstanding (ambiguous phrasing) rather than pure arithmetic; slight decrease in raw arithmetic-expression accuracy for MWP when using step-by-step reconstruction in some backbones (trade-off).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Paper contrasts learned in-model arithmetic with external calculators/tools (they emphasize solving without tools). MathGLM outperforms GPT-4 on multi-digit arithmetic tasks in these experiments, but the paper does not directly compare to human computation speed/accuracy or exact symbolic algorithms; authors report parity with GPT-4 on a 5k Chinese MWP test after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT Can Solve Mathematical Problems Without a Calculator', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3028.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3028.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art large language model developed by OpenAI used in this work as a baseline; shows strong language capability but comparatively poor out-of-the-box performance on multi-digit arithmetic in the authors' benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT-4 Technical Report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large multimodal transformer-based language model by OpenAI (technical report cited). Paper uses GPT-4 as a baseline without internal modifications.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same evaluated tasks as MathGLM (multi-digit add/sub/mul/div, decimals, fractions, mixed ops, MWP).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Not explicitly probed in this paper; empirically the model appears to rely on learned statistical/pattern behavior which yields near-perfect low-digit arithmetic but degrades for high-digit, decimal, fraction, and chained operations—suggesting limited learned algorithmic execution for large-digit arithmetic without external tools.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical performance: near-100% on low-digit arithmetic but sharp degradation with digit size, particularly in multiplication (paper reports e.g. multi-digit multiplication accuracy as low as ~4.3% in an introduction claim and 18.84% on the mixed-ops dataset used here). Big-Bench arithmetic evaluations show performance drops with increasing digits (especially for multiplication and decimal/percent formats).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Paper does not provide probing evidence against specific mechanistic hypotheses for GPT-4; only empirical failure modes are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>None applied in this paper (GPT-4 used as provided). Related work mentions using external calculators/tools to augment GPT-4 in other studies.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>N/A within this paper. Related-work cites tool-usage approaches that can improve numerical correctness but are outside the experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported baselines: GPT-4 accuracy 18.84% on the authors' complex mixed-ops dataset; near-perfect on low-digit tasks in Big-Bench but close to zero trend for some high-digit multiplication/decimal tasks; Ape210K answer accuracy reported ~59.57% (paper table).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Large decline in accuracy for multiplication with >8 digits, decimals, fractions, and chained mixed operations; inconsistent on multi-step MWP arithmetic steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Paper frames GPT-4 as being outperformed by MathGLM on multi-digit arithmetic without calculators; authors emphasize that with sufficient targeted training data, an LLM can approach algorithmic-like arithmetic performance without external symbolic tools.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT Can Solve Mathematical Problems Without a Calculator', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3028.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3028.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Step-by-step strategy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Step-by-step (decomposed intermediate-step) training and solution strategy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset and training approach that decomposes each arithmetic expression or math word problem solution into sequential intermediate calculation steps and trains the model to generate those steps rather than only final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MathGLM / LLMs (general)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied as a construction principle for the arithmetic training dataset and for reconstructing Ape210K solutions (MWP); models are trained to output intermediate calculation steps in an autoregressive manner.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step chained arithmetic expressions and math word problems requiring intermediate computations.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>By exposing the model to explicit intermediate computations, the model learns local arithmetic transformations and underlying calculation rules, effectively decomposing complex computation into smaller learnable steps (analogous to teaching an algorithm via examples).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablation: stepwise training boosts performance dramatically (MathGLM-2B: accuracy from ~40.76% → 93.03%; MathGLM-500M: 31.96% → 89.57%); it also yields major gains in MWP answer accuracy (e.g., +42.29% absolute for GLM-10B fine-tuning on reconstructed Ape210K). Relative-error metrics also improve to ≈99.7% with stepwise training.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Paper notes a trade-off in MWP: step-by-step generation can slightly reduce arithmetic-expression accuracy in some backbones because model capacity is used to generate steps (but overall answer-accuracy increases). There is no internal mechanistic probe proving symbolic algorithm emergence.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Training-data design / supervised process supervision (creating step-level labels), and prompting the model to output steps.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Substantial increase in both arithmetic and answer accuracy; enables robust multi-digit arithmetic without external calculator tools; also changes error patterns to be concentrated in single intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported jumps: MathGLM-2B mixed-ops accuracy 40.76% → 93.03% with step-by-step; relative-error accuracy rose from 94.26% → 99.71% (2B). MWP answer-accuracy gains on reconstructed Ape210K up to +42.29% absolute for GLM-10B.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Although stepwise outputs improve final-answer accuracy, models can still make single-step arithmetic mistakes; generation of steps can introduce small rounding/formatting inconsistencies; slight decrease in raw arithmetic-expression construction accuracy in some MWP fine-tuned backbones.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT Can Solve Mathematical Problems Without a Calculator', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3028.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3028.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Curriculum learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curriculum learning for arithmetic dataset complexity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training strategy where arithmetic examples are presented in increasing complexity (from simple 1-atomic operations up to 9-atomic and higher-digit examples) to progressively teach the model arithmetic skills and generalize to larger digit ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MathGLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The authors generate datasets of varying complexity and sizes (1M–50M sequences) and progressively increase example difficulty and digit-length (up to 12 digits) during training.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>From single-step arithmetic to complex multi-step expressions and long-digit arithmetic (up to 12 digits after augmentation).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Progressive exposure eases learning by letting the model internalize simpler arithmetic patterns before confronting more complex chained operations and long-digit cases; aids length/digit generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Scaling/generalization experiments: larger datasets and curriculum steps improve performance; adding 50k 12-digit-range records and further pretraining improved 12-digit generalization (reported improved accuracy over baselines/GPT-4 across digit ranges).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Paper does not provide a controlled isolated comparison of curriculum schedule variants beyond reporting that curriculum and larger data help; no probing of internal algorithmic structure.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Dataset scheduling / training regime design.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improved ability to handle higher-digit arithmetic and complex chained expressions; when combined with stepwise targets and scaling, yields state-of-the-art arithmetic accuracy for the evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Generalization analysis shows MathGLM outperforms GPT-4 across 5–12 digit ranges after 12-digit examples are introduced (exact table numbers referenced in paper Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Even with curriculum, accuracy declines as digit count increases, indicating limits to scaling benefits without targeted high-digit data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT Can Solve Mathematical Problems Without a Calculator', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3028.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3028.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Digit-level tokenization (icetk)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Digit-as-token tokenization using icetk</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tokenization scheme used in MathGLM where each digit is a distinct token and arithmetic symbols (., +, -, %, /, brackets, =) are single tokens, to provide consistent and fine-grained input representation for arithmetic expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MathGLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Preprocessing/tokenization tool (icetk) configured to tokenize numerals digit-by-digit and assign unique tokens to arithmetic symbols; used for all arithmetic pretraining data.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>All arithmetic and mixed-format numeric inputs (integers, decimals, fractions, percent, negatives).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Better-aligned discrete representation of numbers and symbols reduces tokenization-induced ambiguity and lets the model learn per-digit patterns and local digit transformations more directly.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Authors state tokenization 'ensures every element in the arithmetic expression is adequately represented' and provide examples; no isolated ablation quantifying impact is reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No negative evidence reported, but absence of an ablation means quantitative benefit is asserted but not independently confirmed in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Input representation / preprocessing choice.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Enables models to see digits and arithmetic symbols as atomic tokens facilitating stepwise learning; effect is claimed but not numerically isolated.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Not discussed; potential trade-offs (longer token sequences for big numbers) are possible but unquantified.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT Can Solve Mathematical Problems Without a Calculator', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3028.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3028.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Toolformer / external calculator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Toolformer: Language models can teach themselves to use tools (external calculator integration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach referenced as related work where models are augmented with an external calculator/tool to perform arithmetic precisely, decoupling symbolic calculation from language reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toolformer: Language models can teach themselves to use tools</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (general, related work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Toolformer integrates external tool calls into model generation to compute results reliably (mentioned here as an alternative to training internal arithmetic capability).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Arbitrary-precision arithmetic delegated to a calculator API or external tool (mentioned conceptually).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Delegation: model generates calls to an external calculator which performs exact arithmetic; the LM focuses on reasoning and orchestration.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Cited in related work as an effective method to get correct arithmetic answers; not evaluated/used by the authors in their experiments (they intentionally focus on internal capability without tools).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Authors argue that it is possible to achieve high arithmetic accuracy without external tools given sufficient targeted training, so tool use is not strictly necessary for accurate arithmetic in LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>External tool integration during inference/training (related work).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Known in literature to increase numeric correctness; not quantified in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Not discussed in this paper; external tools add system complexity, require tool-call generation correctness, and the model must learn when/how to call them.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT Can Solve Mathematical Problems Without a Calculator', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3028.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3028.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought / Scratchpad</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) prompting and Scratchpads for intermediate computation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting and training methods (cited in related work) that elicit or supervise intermediate reasoning steps (CoT) or provide a scratchpad for intermediate calculations to improve quantitative/numerical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (general, related work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chain-of-Thought (Wei et al.) prompts encourage LMs to produce intermediate reasoning; scratchpads (Nye et al.) are supervised fine-tuning/objective variants that encourage explicit intermediate computation. Both are referenced as conceptually related to the paper's step-by-step strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step reasoning tasks including arithmetic and math word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Process supervision and explicit intermediate outputs reduce compounding errors and help models break problems into smaller, tractable subproblems, improving final-answer accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Cited works (CoT, scratchpads) showed improved numerical and reasoning performance; MathGLM's step-by-step results are reported as consistent with these findings (strong ablation evidence in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Authors note trade-offs (e.g., some decrease in raw arithmetic-expression accuracy when stepwise generation is used for MWP), but overall CoT-like strategies improve final-answer correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Prompting (CoT) and supervised scratchpad-style fine-tuning (related work); MathGLM uses a supervised stepwise dataset analogous to scratchpad/CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>In literature, and corroborated here, intermediate-step supervision substantially improves final-answer accuracy on arithmetic and math reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Intermediate-step generation can introduce single-step errors; generating detailed steps consumes generation budget and may slightly reduce expression-level accuracy in some setups.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT Can Solve Mathematical Problems Without a Calculator', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Show your work: Scratchpads for intermediate computation with language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 2)</em></li>
                <li>PAL: Program-aided language models <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 2)</em></li>
                <li>How well do large language models perform in arithmetic tasks? <em>(Rating: 2)</em></li>
                <li>Length generalization in arithmetic transformers <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3028",
    "paper_id": "paper-261582750",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "MathGLM",
            "name_full": "MathGLM (Math General Language Model)",
            "brief_description": "A series of Transformer-based language models trained specifically to perform arithmetic by pretraining on a large, curated arithmetic dataset that encodes step-by-step solutions and by using curriculum learning; variants range from 10M to multi-billion parameters and include models trained from-scratch (decoder-only) for arithmetic and GLM-based fine-tuned variants for word problems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MathGLM",
            "model_description": "Transformer-based LMs: decoder-only architecture trained from scratch on an autoregressive objective for arithmetic tasks (variants: MathGLM-10M, 100M, 500M, 2B, 6B), and GLM-based fine-tuned variants (MathGLM fine-tuned from GLM-10B/GLM-6B/GLM-large/ChatGLM). Training uses digit-level tokenization (icetk), step-by-step solution-targets, and curriculum progression of example complexity.",
            "arithmetic_task_type": "Multi-digit integer and decimal addition/subtraction/multiplication/division/exponentiation; mixed chained operations (2–10 steps); fractions, percentages, negatives; Chinese math word problems (Ape210K reconstructed, K6).",
            "reported_mechanism": "Learns calculation rules and stepwise arithmetic by supervised exposure to decomposed intermediate steps (dataset-driven algorithmic learning); digit-level tokenization provides structured input representation; model and data scaling (parameters and dataset size/curriculum) improve performance—i.e., pattern/algorithm induction from many stepwise examples.",
            "evidence_for_mechanism": "Ablation studies: applying the step-by-step strategy raises MathGLM-2B accuracy on the evaluation arithmetic dataset from ≈40.76% (direct-answer) to 93.03% (stepwise) and relative-error accuracy from ~94.26% to 99.71%; scaling analysis shows monotonic gains with model size and dataset size; adding 12-digit training examples improved generalization; error analysis shows most errors are single-step mistakes, consistent with learned multi-step computation.",
            "evidence_against_mechanism": "No internal probing of representations or algorithmic state machines is provided; remaining failures (notably some large-digit divisions and very long sequences) indicate the model has not learned a perfect symbolic algorithm; slight trade-offs observed in some MWP arithmetic-accuracy vs answer-accuracy when using step-by-step (i.e., step-generation can consume capacity).",
            "intervention_type": "Training-data intervention (step-by-step targets), curriculum learning (gradually increasing expression complexity and digit length), digit-level tokenization (icetk), targeted fine-tuning from GLM backbones for word problems, and adding high-digit (12-digit) examples for generalization.",
            "effect_of_intervention": "Major accuracy improvements on arithmetic tasks (step-by-step: +~52 percentage points for 2B model on mixed ops); improved relative-error metrics (~99+% within 1% RE); improved answer-accuracy on math word problems when Ape210K is reconstructed with stepwise solutions (absolute gains up to ~42.29% for GLM-10B fine-tuning). Curriculum/generalization examples enabled handling numbers up to 12 digits with much better performance than baseline.",
            "performance_metrics": "MathGLM-2B: 93.03% accuracy on a 9,592-sample generated arithmetic test set (mixed operations); relative-error accuracy 99.71%. MathGLM-500M: 89.57% accuracy (RE 99.41%). MathGLM-100M: 70.28%; MathGLM-10M: 61.21%. On Ape210K-derived MWP test (5,000 samples) GLM-10B + MathGLM fine-tune attains performance comparable to GPT-4; on Big-Bench arithmetic, MathGLM maintains high accuracy even for higher-digit tasks where GPT-4 degrades. (Reported baseline: GPT-4 18.84% on the same mixed-ops dataset; other dataset-specific numbers appear in paper tables.)",
            "notable_failure_modes": "Errors concentrated in single calculation steps inside multi-step solutions; difficulty with certain large-digit divisions and very long expression sequences; some MWP failures due to language misunderstanding (ambiguous phrasing) rather than pure arithmetic; slight decrease in raw arithmetic-expression accuracy for MWP when using step-by-step reconstruction in some backbones (trade-off).",
            "comparison_to_humans_or_symbolic": "Paper contrasts learned in-model arithmetic with external calculators/tools (they emphasize solving without tools). MathGLM outperforms GPT-4 on multi-digit arithmetic tasks in these experiments, but the paper does not directly compare to human computation speed/accuracy or exact symbolic algorithms; authors report parity with GPT-4 on a 5k Chinese MWP test after fine-tuning.",
            "uuid": "e3028.0",
            "source_info": {
                "paper_title": "GPT Can Solve Mathematical Problems Without a Calculator",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A state-of-the-art large language model developed by OpenAI used in this work as a baseline; shows strong language capability but comparatively poor out-of-the-box performance on multi-digit arithmetic in the authors' benchmarks.",
            "citation_title": "GPT-4 Technical Report",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large multimodal transformer-based language model by OpenAI (technical report cited). Paper uses GPT-4 as a baseline without internal modifications.",
            "arithmetic_task_type": "Same evaluated tasks as MathGLM (multi-digit add/sub/mul/div, decimals, fractions, mixed ops, MWP).",
            "reported_mechanism": "Not explicitly probed in this paper; empirically the model appears to rely on learned statistical/pattern behavior which yields near-perfect low-digit arithmetic but degrades for high-digit, decimal, fraction, and chained operations—suggesting limited learned algorithmic execution for large-digit arithmetic without external tools.",
            "evidence_for_mechanism": "Empirical performance: near-100% on low-digit arithmetic but sharp degradation with digit size, particularly in multiplication (paper reports e.g. multi-digit multiplication accuracy as low as ~4.3% in an introduction claim and 18.84% on the mixed-ops dataset used here). Big-Bench arithmetic evaluations show performance drops with increasing digits (especially for multiplication and decimal/percent formats).",
            "evidence_against_mechanism": "Paper does not provide probing evidence against specific mechanistic hypotheses for GPT-4; only empirical failure modes are reported.",
            "intervention_type": "None applied in this paper (GPT-4 used as provided). Related work mentions using external calculators/tools to augment GPT-4 in other studies.",
            "effect_of_intervention": "N/A within this paper. Related-work cites tool-usage approaches that can improve numerical correctness but are outside the experiments here.",
            "performance_metrics": "Reported baselines: GPT-4 accuracy 18.84% on the authors' complex mixed-ops dataset; near-perfect on low-digit tasks in Big-Bench but close to zero trend for some high-digit multiplication/decimal tasks; Ape210K answer accuracy reported ~59.57% (paper table).",
            "notable_failure_modes": "Large decline in accuracy for multiplication with &gt;8 digits, decimals, fractions, and chained mixed operations; inconsistent on multi-step MWP arithmetic steps.",
            "comparison_to_humans_or_symbolic": "Paper frames GPT-4 as being outperformed by MathGLM on multi-digit arithmetic without calculators; authors emphasize that with sufficient targeted training data, an LLM can approach algorithmic-like arithmetic performance without external symbolic tools.",
            "uuid": "e3028.1",
            "source_info": {
                "paper_title": "GPT Can Solve Mathematical Problems Without a Calculator",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Step-by-step strategy",
            "name_full": "Step-by-step (decomposed intermediate-step) training and solution strategy",
            "brief_description": "A dataset and training approach that decomposes each arithmetic expression or math word problem solution into sequential intermediate calculation steps and trains the model to generate those steps rather than only final answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MathGLM / LLMs (general)",
            "model_description": "Applied as a construction principle for the arithmetic training dataset and for reconstructing Ape210K solutions (MWP); models are trained to output intermediate calculation steps in an autoregressive manner.",
            "arithmetic_task_type": "Multi-step chained arithmetic expressions and math word problems requiring intermediate computations.",
            "reported_mechanism": "By exposing the model to explicit intermediate computations, the model learns local arithmetic transformations and underlying calculation rules, effectively decomposing complex computation into smaller learnable steps (analogous to teaching an algorithm via examples).",
            "evidence_for_mechanism": "Ablation: stepwise training boosts performance dramatically (MathGLM-2B: accuracy from ~40.76% → 93.03%; MathGLM-500M: 31.96% → 89.57%); it also yields major gains in MWP answer accuracy (e.g., +42.29% absolute for GLM-10B fine-tuning on reconstructed Ape210K). Relative-error metrics also improve to ≈99.7% with stepwise training.",
            "evidence_against_mechanism": "Paper notes a trade-off in MWP: step-by-step generation can slightly reduce arithmetic-expression accuracy in some backbones because model capacity is used to generate steps (but overall answer-accuracy increases). There is no internal mechanistic probe proving symbolic algorithm emergence.",
            "intervention_type": "Training-data design / supervised process supervision (creating step-level labels), and prompting the model to output steps.",
            "effect_of_intervention": "Substantial increase in both arithmetic and answer accuracy; enables robust multi-digit arithmetic without external calculator tools; also changes error patterns to be concentrated in single intermediate steps.",
            "performance_metrics": "Reported jumps: MathGLM-2B mixed-ops accuracy 40.76% → 93.03% with step-by-step; relative-error accuracy rose from 94.26% → 99.71% (2B). MWP answer-accuracy gains on reconstructed Ape210K up to +42.29% absolute for GLM-10B.",
            "notable_failure_modes": "Although stepwise outputs improve final-answer accuracy, models can still make single-step arithmetic mistakes; generation of steps can introduce small rounding/formatting inconsistencies; slight decrease in raw arithmetic-expression construction accuracy in some MWP fine-tuned backbones.",
            "uuid": "e3028.2",
            "source_info": {
                "paper_title": "GPT Can Solve Mathematical Problems Without a Calculator",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Curriculum learning",
            "name_full": "Curriculum learning for arithmetic dataset complexity",
            "brief_description": "A training strategy where arithmetic examples are presented in increasing complexity (from simple 1-atomic operations up to 9-atomic and higher-digit examples) to progressively teach the model arithmetic skills and generalize to larger digit ranges.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MathGLM",
            "model_description": "The authors generate datasets of varying complexity and sizes (1M–50M sequences) and progressively increase example difficulty and digit-length (up to 12 digits) during training.",
            "arithmetic_task_type": "From single-step arithmetic to complex multi-step expressions and long-digit arithmetic (up to 12 digits after augmentation).",
            "reported_mechanism": "Progressive exposure eases learning by letting the model internalize simpler arithmetic patterns before confronting more complex chained operations and long-digit cases; aids length/digit generalization.",
            "evidence_for_mechanism": "Scaling/generalization experiments: larger datasets and curriculum steps improve performance; adding 50k 12-digit-range records and further pretraining improved 12-digit generalization (reported improved accuracy over baselines/GPT-4 across digit ranges).",
            "evidence_against_mechanism": "Paper does not provide a controlled isolated comparison of curriculum schedule variants beyond reporting that curriculum and larger data help; no probing of internal algorithmic structure.",
            "intervention_type": "Dataset scheduling / training regime design.",
            "effect_of_intervention": "Improved ability to handle higher-digit arithmetic and complex chained expressions; when combined with stepwise targets and scaling, yields state-of-the-art arithmetic accuracy for the evaluated models.",
            "performance_metrics": "Generalization analysis shows MathGLM outperforms GPT-4 across 5–12 digit ranges after 12-digit examples are introduced (exact table numbers referenced in paper Table 7).",
            "notable_failure_modes": "Even with curriculum, accuracy declines as digit count increases, indicating limits to scaling benefits without targeted high-digit data.",
            "uuid": "e3028.3",
            "source_info": {
                "paper_title": "GPT Can Solve Mathematical Problems Without a Calculator",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Digit-level tokenization (icetk)",
            "name_full": "Digit-as-token tokenization using icetk",
            "brief_description": "A tokenization scheme used in MathGLM where each digit is a distinct token and arithmetic symbols (., +, -, %, /, brackets, =) are single tokens, to provide consistent and fine-grained input representation for arithmetic expressions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MathGLM",
            "model_description": "Preprocessing/tokenization tool (icetk) configured to tokenize numerals digit-by-digit and assign unique tokens to arithmetic symbols; used for all arithmetic pretraining data.",
            "arithmetic_task_type": "All arithmetic and mixed-format numeric inputs (integers, decimals, fractions, percent, negatives).",
            "reported_mechanism": "Better-aligned discrete representation of numbers and symbols reduces tokenization-induced ambiguity and lets the model learn per-digit patterns and local digit transformations more directly.",
            "evidence_for_mechanism": "Authors state tokenization 'ensures every element in the arithmetic expression is adequately represented' and provide examples; no isolated ablation quantifying impact is reported in the paper.",
            "evidence_against_mechanism": "No negative evidence reported, but absence of an ablation means quantitative benefit is asserted but not independently confirmed in this work.",
            "intervention_type": "Input representation / preprocessing choice.",
            "effect_of_intervention": "Enables models to see digits and arithmetic symbols as atomic tokens facilitating stepwise learning; effect is claimed but not numerically isolated.",
            "performance_metrics": null,
            "notable_failure_modes": "Not discussed; potential trade-offs (longer token sequences for big numbers) are possible but unquantified.",
            "uuid": "e3028.4",
            "source_info": {
                "paper_title": "GPT Can Solve Mathematical Problems Without a Calculator",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Toolformer / external calculator",
            "name_full": "Toolformer: Language models can teach themselves to use tools (external calculator integration)",
            "brief_description": "An approach referenced as related work where models are augmented with an external calculator/tool to perform arithmetic precisely, decoupling symbolic calculation from language reasoning.",
            "citation_title": "Toolformer: Language models can teach themselves to use tools",
            "mention_or_use": "mention",
            "model_name": "LLMs (general, related work)",
            "model_description": "Toolformer integrates external tool calls into model generation to compute results reliably (mentioned here as an alternative to training internal arithmetic capability).",
            "arithmetic_task_type": "Arbitrary-precision arithmetic delegated to a calculator API or external tool (mentioned conceptually).",
            "reported_mechanism": "Delegation: model generates calls to an external calculator which performs exact arithmetic; the LM focuses on reasoning and orchestration.",
            "evidence_for_mechanism": "Cited in related work as an effective method to get correct arithmetic answers; not evaluated/used by the authors in their experiments (they intentionally focus on internal capability without tools).",
            "evidence_against_mechanism": "Authors argue that it is possible to achieve high arithmetic accuracy without external tools given sufficient targeted training, so tool use is not strictly necessary for accurate arithmetic in LLMs.",
            "intervention_type": "External tool integration during inference/training (related work).",
            "effect_of_intervention": "Known in literature to increase numeric correctness; not quantified in this paper's experiments.",
            "performance_metrics": null,
            "notable_failure_modes": "Not discussed in this paper; external tools add system complexity, require tool-call generation correctness, and the model must learn when/how to call them.",
            "uuid": "e3028.5",
            "source_info": {
                "paper_title": "GPT Can Solve Mathematical Problems Without a Calculator",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Chain-of-Thought / Scratchpad",
            "name_full": "Chain-of-Thought (CoT) prompting and Scratchpads for intermediate computation",
            "brief_description": "Prompting and training methods (cited in related work) that elicit or supervise intermediate reasoning steps (CoT) or provide a scratchpad for intermediate calculations to improve quantitative/numerical reasoning.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "mention_or_use": "mention",
            "model_name": "LLMs (general, related work)",
            "model_description": "Chain-of-Thought (Wei et al.) prompts encourage LMs to produce intermediate reasoning; scratchpads (Nye et al.) are supervised fine-tuning/objective variants that encourage explicit intermediate computation. Both are referenced as conceptually related to the paper's step-by-step strategy.",
            "arithmetic_task_type": "Multi-step reasoning tasks including arithmetic and math word problems.",
            "reported_mechanism": "Process supervision and explicit intermediate outputs reduce compounding errors and help models break problems into smaller, tractable subproblems, improving final-answer accuracy.",
            "evidence_for_mechanism": "Cited works (CoT, scratchpads) showed improved numerical and reasoning performance; MathGLM's step-by-step results are reported as consistent with these findings (strong ablation evidence in this paper).",
            "evidence_against_mechanism": "Authors note trade-offs (e.g., some decrease in raw arithmetic-expression accuracy when stepwise generation is used for MWP), but overall CoT-like strategies improve final-answer correctness.",
            "intervention_type": "Prompting (CoT) and supervised scratchpad-style fine-tuning (related work); MathGLM uses a supervised stepwise dataset analogous to scratchpad/CoT.",
            "effect_of_intervention": "In literature, and corroborated here, intermediate-step supervision substantially improves final-answer accuracy on arithmetic and math reasoning tasks.",
            "performance_metrics": null,
            "notable_failure_modes": "Intermediate-step generation can introduce single-step errors; generating detailed steps consumes generation budget and may slightly reduce expression-level accuracy in some setups.",
            "uuid": "e3028.6",
            "source_info": {
                "paper_title": "GPT Can Solve Mathematical Problems Without a Calculator",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models",
            "rating": 2,
            "sanitized_title": "show_your_work_scratchpads_for_intermediate_computation_with_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks",
            "rating": 2,
            "sanitized_title": "goat_finetuned_llama_outperforms_gpt4_on_arithmetic_tasks"
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 2,
            "sanitized_title": "program_of_thoughts_prompting_disentangling_computation_from_reasoning_for_numerical_reasoning_tasks"
        },
        {
            "paper_title": "PAL: Program-aided language models",
            "rating": 2,
            "sanitized_title": "pal_programaided_language_models"
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 2,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        },
        {
            "paper_title": "How well do large language models perform in arithmetic tasks?",
            "rating": 2,
            "sanitized_title": "how_well_do_large_language_models_perform_in_arithmetic_tasks"
        },
        {
            "paper_title": "Length generalization in arithmetic transformers",
            "rating": 1,
            "sanitized_title": "length_generalization_in_arithmetic_transformers"
        }
    ],
    "cost": 0.02131625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GPT Can Solve Mathematical Problems Without a Calculator</p>
<p>Zhen Yang 
Ming Ding 
Qingsong Lv 
Zhihuan Jiang 
Zehai He 
Yuyi Guo 
Jinfeng Bai 
Jie Tang 
Tsinghua University 
⋄ Tal 
A I Lab 
Zhipu Ai 
GPT Can Solve Mathematical Problems Without a Calculator</p>
<p>Previous studies have typically assumed that large language models are unable to accurately perform arithmetic operations, particularly multiplication of &gt;8 digits, and operations involving decimals and fractions, without the use of calculator tools. This paper aims to challenge this misconception. With sufficient training data, a 2 billion-parameter language model can accurately perform multi-digit arithmetic operations with almost 100% accuracy without data leakage, significantly surpassing GPT-4 (whose multi-digit multiplication accuracy is only 4.3%). We also demonstrate that our MathGLM, fine-tuned from GLM-10B on a dataset with additional multi-step arithmetic operations and math problems described in text, achieves similar performance to GPT-4 on a 5,000-samples Chinese math problem test set. Our code and data are public at https://github.com/THUDM/MathGLM.</p>
<p>Introduction</p>
<p>MathGLM-10M</p>
<p>MathGLM-100M MathGLM-500M MathGLM-2B Figure 1: Accuracy scores across various LLMs like GPT-4 and ChatGPT, as well as a series of MathGLM models on the generated test dataset for the arithmetic tasks. Among the different model scales, MathGLM consistently achieves superior performance.</p>
<p>Large language models (LLMs) have demonstrated remarkable ability in handling a variety of downstream tasks in the NLP domain [1,4,43,33,45,27]. Pioneering models, such as GPT-4 [24] and ChatGPT [23], have been trained on massive amounts of text data, enabling them to generate coherent and contextually relevant responses. Their ability to understand and generate text makes them highly versatile for various NLP tasks. Moreover, LLMs have been leveraged for other assignments, involving areas of mathematics [5,17] and science [32]. Nevertheless, despite the impressive capabilities across diverse NLP tasks, GPT-4 might not exhibit the same level of proficiency in mathematical reasoning, including arithmetic tasks and Chinese math word problems.</p>
<p>In the context of arithmetic tasks, a prevailing assumption is that LLMs struggle with accurately executing complex arithmetic operations, especially pronounced in cases involving multiplication of numbers exceeding 8 digits, and operations entailing decimals and fractions. To eliminate these misconceptions, we embark on an investigation to assess the arithmetic ability of LLMs. Specifically, we focus on the capability of LLMs in performing complex arithmetic operations. As a result, we propose MathGLM, a powerful model meticulously crafted to impeccably execute an extensive spectrum of complex arithmetic operations, achieving the best performance compared to leading LLMs such as GPT-4 (See Figure 1). These operations contain singular actions like addition, subtraction, multiplication, division, and exponentiation, as well as the mixing of these operations employing brackets. When these operations are performed individually, without being combined with any other operation, we refer to them as "1-atomic operation". Importantly, MathGLM has the capability to adeptly tackle arithmetic operations that involve a variety of numerical forms, including integers, decimals, fractions, percentages, and even negative numbers. Figure 2 demonstrates examples generated by MathGLM with 2B model parameters on addition, subtraction, multiplication, division, exponentiation, and mixing operations tasks.</p>
<p>To attain the remarkable performance exhibited by MathGLM in arithmetic tasks, we utilize a stepby-step strategy to construct an arithmetic dataset that serves as the foundation for MathGLM's pre-training. This dataset is designed to encompass a wide spectrum of arithmetic operations, spanning from straightforward 1-atomic operation to more complex 9-atomic operations. By adopting this step-by-step strategy, MathGLM learns to handle both simple and intricate arithmetic expressions, which empowers it to accurately perform calculations even for operations involving multiplication of numbers greater than 8 digits, and those with decimals and fractions. Moreover, we incorporate the concept of curriculum learning to further augment the capabilities of MathGLM. By gradually increasing the complexity of the arithmetic expressions, MathGLM progressively enhances its capacity to tackle operations involving numbers spanning up to 12 digits. This stands in contrast to the common assumption that large language models struggle with such complex arithmetic tasks. The results demonstrate that MathGLM's arithmetic performance surpasses even the most robust LLMs like GPT-4. Specifically, MathGLM achieves an impressive accuracy of 93.03% on the test dataset containing complex mixed operations. In contrast, GPT-4 only manages a meager 18.84% accuracy on the same dataset.</p>
<p>For math word problems, the Ape210K dataset [46] serves as a comprehensive source of mathematical challenges, drawing from diverse math word problems across the Internet. This dataset serves as a valuable resource for training MathGLM, offering a broad spectrum of problem types for learning. However, a notable characteristic of the original dataset lies in its directly calculated answers. This straightforward answer presentation might lead to a potential drawback, that is MathGLM can potentially miss the underlying calculation rules and patterns embedded within the calculation processes.</p>
<p>To overcome this potential limitation and bolster MathGLM's proficiency in solving math word problems, we leverage the step-by-step strategy to reconstruct the Ape210K dataset. By decomposing the complex arithmetic calculation process into a sequence of sequential steps, MathGLM is empowered to accurately generate answer for math word problems and significantly enhance the answer accuracy in comparison to the original one. For instance, MathGLM achieves an impressive absolute gain of 42.29% in answer accuracy as compared to fine-tuning on the original dataset. By fine-tuning from the GLM-10B, MathGLM's performance closely aligns with that of GPT-4 when evaluated on a math word problems dataset comprising 5,000 test cases. This step-by-step strategy provides MathGLM with a profound understanding of the complex calculation process inherent in math word problems, enabling MathGLM to grasp the underlying calculation rules and obtain more accurate answers.</p>
<p>Overall, MathGLM excels in both arithmetic tasks and math word problems by leveraging the stepby-step strategy. Our comprehensive experiments and detailed analysis demonstrate the effectiveness of MathGLM's mathematical reasoning compared to GPT-4. These results significantly challenge the common misconception that LLMs struggle with complex arithmetic tasks, thus unveiling their remarkable potential to excel in the realm of mathematical reasoning tasks. We organize this paper as follows. In Section 2, we elaborate on preliminaries, including large language models, arithmetic calculation, and mathematical reasoning. Section 3 introduces the methodologies employed in MathGLM, covering arithmetic training dataset, models, and training procedure for arithmetic tasks (Section 3.1), and training dataset, backbone models, and training strategy for math word problems (Section 3.2). We also perform comprehensive experiments and an analysis of the MathGLM's capabilities (Section 4). Section 4.1 reports the detailed experimental results on arithmetic tasks, and Section 4.2 presents the results related to math word problems. Finally, we summarize our work in Section 5.</p>
<p>2 Related Work</p>
<p>Large Language Models</p>
<p>Large Language Models (LLMs) have demonstrated robust capabilities in the realm of Natural Language Processing (NLP) tasks, significantly shifting the research paradigm within the field. These models, such as GPT-3 [1], Gopher [26], Megatron-Turing NLG [29], Chinchilla [12], PaLM [4], OPT [45], BLOOM [27], GLM-130B [43], and LLaMA [36], are trained on a large corpus of diverse and unlabeled data, demonstrating a generic ability to perform well on a variety of tasks. Through pretraining on extensive corpus, these models obtain powerful language understanding and generation capabilities, enabling their exceptional performance on a wide array of benchmarks, such as MMLU [10], mathematical reasoning, and code generation. Moreover, they display an astonishing aptitude for in-context learning, rapidly adapting to novel tasks with minimal examples through few-shot learning.</p>
<p>Nonetheless, despite the remarkable strides made by the most powerful LLMs, ChatGPT [25] and GPT-4 [24], in language understanding and generation, it is crucial to recognize that these cuttingedge models still encounter challenges in tackling mathematical problems. This work is dedicated to addressing and enhancing the performance of LLMs in the domain of solving mathematical problems, encompassing both arithmetic tasks and math word problems.</p>
<p>Arithmetic Calculation</p>
<p>The emergence of pre-trained Large Language Models (LLMs) [1,4,24] has sparked considerable interest in investigating their potential for handling arithmetic tasks. Nogueira et al. [21] and Wang et al. [39] evaluate the arithmetic capabilities of LLMs on elementary arithmetic operations like addition and subtraction. Muffo et al. [20] undertake an evaluation that specifically centers on assessing the proficiency of language models in the domain of 2-digit multiplication. BIG-bench [30] introduces a comprehensive collection of arithmetic datasets, which encompass a spectrum of arithmetic tasks that span numbers within a range of up to 5 digits. Yuan et al. [42] design an complex arithmetic dataset MATH 401 with various arithmetic operations to evaluate the capabilities of models like GPT-4, ChatGPT, InstructGPT [25], Galactica [32], and LLaMA [36].</p>
<p>To support arithmetic operations involving large numbers, Nye et al. [22] employ scratchpad-based fine-tuning that enables LLMs to achieve remarkable outcomes in the context of 8-digit addition. Zhou et al. [48] adopt the specialize prompt engineering techniques to successfully extend the scope of addition but encountered limitations with multiplication beyond 7 digits. Goat [19] utilizes supervised instruction fine-tuning to handle elementary arithmetic operations with large integers, including addition, subtraction, multiplication, and division. Jelassi et al. [15] investigate length generalization in basic arithmetic tasks via approaches like relative position embeddings and train set priming. Distinguishing itself from these efforts focused on elementary arithmetic, our MathGLM pushes the envelope by not only exceeding the realm of basic arithmetic with two numbers but also tackling intricate mixing arithmetic operations involving multiple numbers and diverse data formats.</p>
<p>Furthermore, several works explore the integration of external tools for arithmetic tasks. For instance, Toolformer [28] adopts an external calculator to accomplish arithmetic calculations, while PoT [2] and PAL [9] obtain the final answer with the help of programs. Different from leveraging external tools, we focus on explore how to enhance the inherent arithmetic ability of LLMs without relying on external tools.</p>
<p>Mathematical Reasoning</p>
<p>LLMs have indeed demonstrated considerable promise in addressing math word problems. Cobbe et al. [5] utilize training verifiers to rerank the outputs of LLMs, resulting in remarkable performance on the created GSM8K dataset. Lewkowycz et al. [17] introduce Minerva, a large language model fine-tuned based on PaLM models [4], leveraging a substantial dataset containing scientific and mathematical data. Minerva attains state-of-the-art performance on MATH [11] and GSM8K. By leveraging COT (chain of thought) [41,16,47] to decompose the math problems into multiple steps, LLMs notably improve their performance in tackling math word problems. Wang et al. [40] propose the self-consistency strategy as a replacement for the decoding strategy used in COT, which brings about better performance than the traditional COT prompting. Uesato et al. [37] employ process and outcome supervision to enhance the performance of LLMs in solving grade school math problems. Lightman et al. [18] propose to verify each intermediate reasoning step and find process supervision can significantly improve mathematical reasoning performance. While these studies show the substantial advancements made by LLMs in mathematical reasoning, it is clear that LLMs still make mistakes when confronted with arithmetic operations in math word problems. Different from the aforementioned works that primarily concentrate on improving the reasoning process, our goal is to simultaneously advance both mathematical reasoning and arithmetical calculation capabilities of LLMs, addressing both aspects at the same time.</p>
<p>Method</p>
<p>To investigate the efficacy of LLMs in mathematical reasoning, we propose the MathGLM model that designed with the specific goal of enhancing the performance of LLMs in mathematical reasoning. Firstly, MathGLM focuses on enhancing its proficiency in accurately executing a comprehensive range of arithmetic tasks. It accomplishes this by integrating a step-by-step strategy into its architecture. Instead of straightforwardly calculating the answers to complex arithmetic expressions, MathGLM employs this strategy to meticulously generate answers step by step. Secondly, MathGLM leverages the step-by-step strategy to fine-tune a series of GLM models on specific Chinese mathematical problems. By leveraging this strategy, MathGLM enhances its ability to handle complex mathematical problem-solving tasks.</p>
<p>Learning on Arithmetic Tasks</p>
<p>Arithmetic tasks can be broadly divided into basic arithmetic operations and complex mixing operations. Basic arithmetic operations encompass fundamental mathematical tasks that revolve around conducting simple calculations involving two numbers. On the other hand, arithmetic tasks also encompass the domain of complex mixing operations, which necessitate the skill to manage a combination of diverse arithmetic operations and numerical formats. A comprehensive category of the learning tasks encompassed by MathGLM is summarized in Table 1.</p>
<p>Task</p>
<p>Integer Decimal Fraction Percentage Negative Numbers Addition nD+nD nD.mD+nD.mD (nD/mD)+(nD/mD) nD%+nD% -nD+-nD Subtraction nD-nD nD.mD-nD.mD (nD/mD)-(nD/mD) nD%-nD% -nD-nD Multiplication nD<em>nD nD.mD</em>nD.mD (nD/mD)<em>(nD/mD) nD%</em>nD% -nD<em>-nD Division nD/nD nD.mD/nD.mD (nD/mD)/(nD/mD) nD%/nD% -nD/-nD Exponentiation nDˆnD ----nDˆ-nD Mixed Computing [(nD±nD.mD)</em>nD%]/-nD Table 1: Summary and symbolic expression of arithmetic tasks. In symbolic expression, we represent a decimal with n-digit integer part and m-digit decimal part as nD.mD. For mixed computing, we only show a simple mixed symbolic expression.</p>
<p>To augment the arithmetic ability of MathGLM, we adopt a decoder-only architecture based on Transformer [38] and train it from scratch on our generated arithmetic dataset using an autoregressive objective.</p>
<p>Arithmetic Training Dataset. The arithmetic dataset employed for training is meticulously designed to encompass a comprehensive range of arithmetic tasks. This dataset is thoughtfully designed to incorporate a variety of operations, including addition, subtraction, multiplication, division, and exponentiation. Additionally, it encompasses diverse numerical formats such as integers, decimals, percents, fractions, and negative numbers. This comprehensive dataset is created in various sizes, ranging from 1 million to 50 million records. Within each of these datasets, individual arithmetic expressions consist of 2 to 10 operation steps, encompassing a spectrum of mathematical operations like addition (+), subtraction (-), multiplication (×), division (/), and exponentiation (ˆ). To aligh with human calculation habits, a step-by-step strategy is employed in the construction of the arithmetic datasets. Instead of directly computing the final answer to each complex arithmetic expression, the strategy breaks down the complex expression into a sequence of simpler steps, progressively generating answers step by step. This strategy mirrors the process human typically follow when solving complex arithmetic tasks. By training on such dataset, MathGLM achieves outstanding arithmetic performance since it learns the underlying calculation rules from the detailed calculation process. Figure 3 provides some training examples drawn from the arithmetic dataset, illustrating the diversity of arithmetic tasks and the step-by-step strategy incorporated in the dataset.</p>
<p>Models and Training Procedure.  </p>
<p>Learning on Math Word Problems</p>
<p>Alongside our focus on arithmetic tasks, we train (fine-tune) a series of Transformer-based language models, named General Language Model (GLM) [8,43] and their chat versions to solve math word problems. Our training leverages the publicly available Chinese Ape210K dataset, which serves as a valuable resource for training language models on math word problem-solving tasks. This dataset consists of a vast collection of 210,000 Chinese math problems at the primary school level, with each problem's answer calculated directly.</p>
<p>Training Dataset. To enhance the performance of MathGLM on math word problems, we utilize a step-by-step strategy to reconstruct the Ape210K dataset, transforming it into a version where the answer of each math problem is calculated step by step. Figure 4 demonstrate the contrast between the original Ape210K dataset and our reconstructed version. The newly reconstructed dataset encourages MathGLM to acquire an in-depth understanding of the underlying calculation rules inherent in solving math word problems. Through this step-wise process, MathGLM becomes adept at deriving a final, accurate answer for each problem, emphasizing its ability to harness the complexities of mathematical reasoning.</p>
<p>Backbone Models. We adopt different variations of the GLM as the backbone to train the MathGLM, including GLM-large with 335M parameters, GLM-6B, GLM2-6B, and GLM-10B. Besides, we train the MathGLM using the ChatGLM-6B and ChatGLM2-6B backbones. These Training Strategy. To achieve better performance, we employ two training strategies for MathGLM. The first is to fine-tune the GLM backbone models on a solitary mathematical dataset. This process allows the MathGLM to specialize in understanding and solving math word problems by learning from the mathematical dataset's unique characteristics. However, such strategy damages the generic ability of the MathGLM. To circumvent this limitation, a second strategy is to continue training the GLM backbone models on a hybrid dataset that combines both mathmatics and text content. This helps to balance the specialization in math word problems with the preservation of MathGLM's generic ability.</p>
<p>Experiments</p>
<p>The overarching objective of MathGLM revolves around demonstrating the prowess of language models in the domain of mathematical reasoning. To validate this, we design two distinct types of experiments, encompassing arithmetic tasks and math word problems. These two categories of tasks comprehensively cover both basic computational abilities and higher-order problem-solving skills, providing a robust assessment of the model's proficiency in mathematical reasoning.</p>
<p>Learning on Arithmetic</p>
<p>Dataset</p>
<p>Within the domain of arithmetic, we create a diverse collection of datasets specifically tailored for arithmetic tasks. This suite of training datasets encompasses an expansive spectrum of sizes, including 1 million, 5 million, 10 million, 25 million and 50 million records. Our evaluation dataset, which comprises 9,592 test cases, is generated from the same distribution as the training dataset, yet remains distinct and is excluded from the training process. This carefully generated suite of datasets serves as a comprehensive benchmark to evaluate and quantify MathGLM's computational prowess across a wide variety of arithmetic tasks. For a more in-depth exploration of the specifics of the generated datasets, the details can be found in Appendix A.3.</p>
<p>Evaluation Metric</p>
<p>To measure the ability of MathGLM on arithmetic tasks, we adopt the following metrics to evaluate the outputs.</p>
<p>Accuracy is typically measured by comparing the output of the MathGLM and the ground truth answer. In our experiments, we adhere to standard rounding rules, constraining the generated answers to precisely two decimal places. When the correctly rounded answer aligns with the answer generated by the MathGLM, we classify this outcome as a correct answer.</p>
<p>Relative Error is another important metric used to evaluate the effectiveness of MathGLM, which quantifies the difference between the output generated by MathGLM and the correct answer. The relative error (RE) is quantified using the following formula:
RE = |ŷ − y y |(1)
whereŷ and y denote the generated answer and the correct answer respectively. For our evaluation purposes, we utilize a relative error threshold of 1%. This threshold serves as a criterion for determining the acceptability of the answers generated by the MathGLM, where any relative error falling within this threshold range is considered an accurate outcome.</p>
<p>Results and Analysis</p>
<p>Overall Results. For arithmetic tasks, we pre-train a Transformer-based model named MathGLM with 500M model parameters for both pretraining and inference. To accurately gauge the effectiveness of MathGLM, we contrast its performance with those of leading large language models (LLMs) such as GPT-4 and ChatGPT. The results, as presented in  Additionally, we conduct a performance comparison of arithmetic tasks among different prominent large language models (LLMs) including GPT-4, ChatGPT, text-davinci-003, code-davinci-002, Galactica, LLaMA, OPT, BLOOM, and GLM. For this comparison, we randomly extract a compact arithmetic dataset containing 100 test cases from the larger dataset discussed earlier. The results of this comparison arithmetic performance are presented in Table 4. Upon analyzing the results, it is evident that MathGLM achieves a high accuracy of 93.03% with 2 billion model parameters, surpassing all other LLMs. In addition to leading models like GPT-4 and ChatGPT, the large science model Galactica exhibits better performance in arithmetic tasks. This can be attributed to Galactica's training on a large scientific corpus, enabling it to learn the languages of science and comprehend the intricacies of arithmetic tasks. By leveraging the unique characteristics of this dataset, Galactica is able to enhance its understanding and handling of arithmetic tasks, resulting in improved performance. These findings emphasize the significance of domain-specific training and leveraging specialized datasets to enhance model performance. Besides, a step-by-step solution strategy, which involves decomposing complex arithmetic expressions into individual steps, has proven to be effective in improving arithmetic performance. The outstanding performance of MathGLM shows that the language model coupled with a specialized dataset and the step-by-step solution strategy can achieve remarkable performance in arithmetic tasks.</p>
<p>To comprehensively evaluate the arithmetic performance of MathGLM, we also conduct experiments on a newly-generated arithmetic dataset named MATH 401 [42] Table 5. Analyzing the results, we can observe that the majority of LLMs exhibit commendable accuracy levels exceeding 90% across diverse data formats for elementary arithmetic operations like addition and subtraction. However, as the complexity escalates to operations like multiplication and division, a divergence in performance manifests across different models. For instance, the accuracy levels of the most powerful model GPT-4 also show a trend towards zero, especially when dealing with decimal and percentile data formats. In contrast, MathGLM consistently shows superior performance in multiplication operations across various data formats, surpassing the capability of GPT-4. This demonstrates the effectiveness and capabilities of MathGLM in handling complex arithmetic tasks, even outperforming a prominent model like GPT-4 in specific operations. Notably, even the smaller variant of MathGLM, MathGLM-10M, with only 10 million training parameters, also achieves remarkable arithmetic performances, further emphasizing the arithmetic capabilities of our MathGLM.</p>
<p>Results in BIG-bench. We also evaluate MathGLM using BIG-bench arithmetic dataset [30], which is commonly used to evaluate basic arithmetic capabilities of language models by performing n-digit addition (ADD), subtraction (SUB), multiplication (MUL), and division (DIV). Table 6 reports the experimental results of GPT-4 and MathGLM on various arithmetic operations with different numbers of digits. GPT-4 exhibits near-perfect (100%) accuracy in low-digit arithmetic tasks. However, as the digits escalate, the performance gradually diminishes, particularly pronounced in the multiplication task. In contrast, MathGLM consistently maintains high accuracy levels even in high-digit arithmetic tasks, illustrating its outstanding ability to handle complex arithmetic tasks effectively. The performance trends of different MathGLM variants reveal a consistent pattern of  with the true answers, it is obviously observed that the multiplication operation for 468 * 4046 is correct but the division operation for 14031528/7424 is incorrect. One possible reason for this discrepancy is that MathGLM's pre-training primarily encompasses numbers in the 5-digit range, thereby causing inaccuracies when tackling division tasks involving 12-digit and 4-digit numbers. Upon thorough analysis of the errors made by MathGLM, it's important to highlight that the inaccuracies in the generated answers are remarkably close to the correct evaluations. For a comprehensive investigation into the errors, a detailed breakdown of the error types and their frequencies can be found in Appendix A.5.</p>
<p>Ablation Study</p>
<p>Scaling Analysis. To comprehensively assess the effect of model parameters and training data sizes on performance, we conduct a series of scaling analysis experiments. The model parameters of MathGLM are designed as a range of {10M, 100M, 500M, 2B} and the training data sizes is set to a range of {1M, 5M, 10M, 25M, 50M }. Figure 5 shows the evaluation performance of MathGLM  under various scaling configurations. As expected, the performance trend highlights that the 2B model consistently outperforms its smaller counterparts when evaluated using equivalent data sizes, illustrating the positive impact of larger model parameters on arithmetic performance. Besides, it is evident that larger data sizes have a substantial influence on improving the arithmetic performance as well. However, it is important to note that the effect of data size on the smaller model sizes may not be as pronounced as compared to the larger models. This discernible pattern implies that the benefits derived from increasing the data size tend to be more substantial when paired with larger model parameters. In essence, the trends illustrated in Figure 5 substantiate the notion that both the size of the model and the quantity of training data play vital roles in enhancing the arithmetic performance of MathGLM. Furthermore, by analyzing the trend illustrated in Figure 5, we attempt to extend our findings and make predictions for scaling configurations that were not directly studied. Employing a log-linear trend assumption, we can extrapolate the results to estimate the requisite model size for achieving a targeted performance when utilizing a more extensive training set. Figure 6 illustrates the extrapolated outcomes derived from the log-linear trend. To validate the validity of this trend, we pre-train a</p>
<p>MathGLM equipped with 6B model parameters. From Figure 6, we can observe that the extrapolated trend aligns with the performance achieved by the MathGLM-6B.</p>
<p>MathGLM-10M</p>
<p>MathGLM-100M</p>
<p>MathGLM-500M MathGLM-2B Figure 6: The log-linear trend exhibited by the MathGLM. This trend accurately predicts MathGLM-6B's performance.</p>
<p>Generalization Analysis. To assess the generalization ability of MathGLM beyond the 5-digit range, a set of 50,000 training records involving numbers within the 12-digit range are introduced into the training dataset. After incorporating this additional data, MathGLM is further pre-trained for 20,000 steps to enhance its ability to handle arithmetic tasks involving numbers outside the 5-digit range. Table 7 shows the arithmetic performance comparison across various digit ranges, spanning from 5 digit to 12 digit, and involving a mix of arithmetic operations. In comparison to GPT-4 and ChatGPT, our proposed MathGLM consistently achieves the highest accuracy across all digit ranges, indicating the superiority of MathGLM for multi-digit arithmetic operations. A noticeable observation is that a decline in accuracy as the number of digits in the arithmetic operations increases. This suggests that handling larger digit ranges poses a greater challenge to all LLMs.  Step-by-step Analysis. To delve deeper into the impact of the step-by-step strategy on MathGLM, we conduct extended experiments that directly calculate the answer of each arithmetic expression without employing the step-by-step approach. Figure 9 shows performance comparison between employing the step-by-step strategy and bypassing it for different models. We can observe that a significant improvement in the peformance of MathGLM when the step-by-step strategy is applied. For instance, in the case of MathGLM-500M, the accuracy rises from 31.96% to 89.57%, while for MathGLM-2B, it increases from 40.76% to 93.03% for MathGLM-2B, all attributable to the incorporation of the step-by-step strategy. Similarly, the relative error accuracy exhibits a similar positive trend, escalating from 89.29% to an exceptional 99.41% for MathGLM-500M, and from 94.26% to an outstanding 99.71% for MathGLM-2B with the implementation of the step-by-step strategy. These results demonstrate the effectiveness of the step-by-step strategy in enhancing MathGLM's ability to accurately perform arithmetic operations. The step-by-step approach enables MathGLM to better understand and solve intricate arithmetic tasks, leading to significant improvements in accuracy and relative error accuracy metrics. 
MathGLM-2B MathGLM-500M MathGLM-2B MathGLM-500M</p>
<p>Learning on Math Word Problems</p>
<p>Dataset</p>
<p>In the field of math word problems (MWP), the performance of MathGLM is measured using the Ape210K dataset [46], which contains a collection of 5,000 test math problems. Additionally, we introduce the K6 dataset, which is designed to cover math word problems suitable for elementary school students across 6 different grade levels. The primary purpose of the K6 dataset is to assess the mathematical abilities of LLMs in comprehending and solving general-purpose math reasoning problems. By evaluating MathGLM on the K6 dataset, we are able to gauge its effectiveness in handling mathematical word problems of varying complexity and across a range of grade levels. The details of the K6 dataset can be found in Appendix A.6.</p>
<p>Overall Results</p>
<p>To assess the effectiveness of MathGLM, we test it on the Ape210K dataset and a newly-collected K6 dataset. To facilitate these evaluations, we utilize various LLMs as the backbone. These LLMs, including GLM-Large, GLM-6B, GLM2-6B, GLM-10B, ChatGLM-6B, and ChatGLM2-6B, are employed as the core architecture to process and comprehend mathematical word problems within Chinese datasets.</p>
<p>Results on the Ape210K dataset. We report the performance results of various LLMs including GPT-4, ChatGPT, and a series of our MathGLM variations in Table 8. The results show that when paired with GLM-10B, MathGLM achieves performance levels comparable to the state-of-the-art GPT-4 model in terms of answer accuracy. It demonstrates the effectiveness of MathGLM in generating accurate answers for math word problems through the utilization of a step-by-step strategy. Furthermore, we report the arithmetic accuracy, which measures the correctness of the generated arithmetic expressions. Notably, MathGLM consistently achieves higher arithmetic accuracy compared to answer accuracy across different model sizes. A distinct trend emerges when comparing MathGLM's performance with GLM-Large, GLM-6B, and GLM-10B: MathGLM exhibits notable enhancements in both arithmetic accuracy and answer accuracy. This observation indicates that augmenting model size tends to bolster its overall performance. However, it is worth noting that the performance of MathGLM drops significantly compared to the GLM models when it is coupled with ChatGLM models. A possible explanation is that ChatGLM models are fine-tuned using the instruction data, potentially compromising the inherent capabilities of language models. This tuning process might introduce biases or constraints that hinder the overall ability of the language models in handling math word problems.</p>
<p>Results on the K6 dataset. To assess the mathematical problem-solving abilities across different grade levels, we introduce the K6 dataset and present the corresponding performance results for various LLMs in   models is provided in Appendix A.7. The observations from the figure indicate a general trend of performance decreases as the grade level increases. Such observation indicates that solving math word problems becomes progressively more challenging for LLMs as the grade level increases, requiring more advanced problem solving skills and a deeper understanding of mathematical concepts. GPT-4 exhibits consistently high accuracy levels across most grade levels, showcasing its proficiency in handling math word problems spanning various educational stages. Comparatively, ChatGPT outperforms the majority of Chinese LLMs in terms of accuracy across different grade levels. Among the evaluated Chinese LLMs, ChatGLM2-6B demonstrates a commendable level of performance, achieving satisfactory accuracy (reaching 60% accuracy) in solving math word problems from grade 1 to 4. However, its effectiveness diminishes when attempting to solve problems in grade 5 and 6, highlighting challenges in handling more complex problem-solving scenarios at those levels.</p>
<p>MathGLM consistently outperforms ChatGPT and many of the most powerful Chinese Language Models (LLMs) across the spectrum of grade levels, from grade 1 to grade 6. Particularly noteworthy is MathGLM's ability to achieve higher accuracy than GPT-4 in more advanced grades, such as grade 5 and 6. This observations show the effectiveness of MathGLM in enhancing the accuracy of solving math word problems, especially in challenging educational contexts that demand deeper mathematical understanding and advanced problem-solving skills.</p>
<p>Comparison of Training Strategies</p>
<p>Here, we evaluate the mathematical reasoning ability of MathGLM with different training strategies: fine-tuning and continue training. To execute continue training, we amalgamate the Ape210K train dataset with instruction data released by Chinese-Vicuna [3]. We subsequently continue training MathGLM from the GLM-10B backbone. Table 9 shows the overall performance comparison of MathGLM employing different training strategies. We observe that directly fine-tuning on the specific dataset can achieves better performance.</p>
<p>Training w/o step-by-step strategy with step-by-step strategy Arithmetic Acc Answer Acc Arithmetic Acc Answer Acc   </p>
<p>Further Analysis</p>
<p>Scaling Analysis. To explore the impact of scaling on MathGLM, we conduct a series of experiments encompassing varying dataset sizes and distinct model parameters. Table 10 demonstrates the results obtained from varying the dataset sizes within the range of {5K, 10K, 20K, 50K, 100K, 200K}. Furthermore, to understand the impact of different model parameters, we incorporate various backbone models into MathGLM, including GLM-Large (335M), GLM-6B, and GLM-10B. The results consistently indicate that MathGLM's performance improves across all backbone models with the increase in dataset size. Such observation highlights the beneficial effects of enlarging the training data on bolstering MathGLM's proficiency in tackling math word problems. By accessing more extensive datasets, MathGLM is introduced to a wider array of problem types, resulting in better performance. Additionally, discernible differences in performance emerge among the various backbone models. Given sufficient dataset size, larger models like MathGLM-GLM-10B often outperform others, indicating the crucial role of model parameters in addressing intricate math word problems. These insights emphasize the significance of both dataset and model scaling. By augmenting dataset size and utilizing larger models, we can markedly boost MathGLM's capability to generate more accurate solutions, enhancing its overall efficacy in resolving math word problems.  Step-by-Step Analysis for MWP. To investigate the impact of the step-by-step strategy on Math-GLM, we conduct a series of ablation studies to explore the performance differences with and without this strategy. Figure 9 and Figure 10 demonstrate the performance comparison of MathGLM across different GLM and ChatGLM models respectively. In terms of arithmetic accuracy, as shown in Figure 10, the MathGLM equipped with the step-by-step strategy records marginally lower scores than its counterpart without the strategy. This can be attributed to the fact that the step-by-step approach necessitates a sequential calculation for each mathematical problem. This encourages MathGLM to concentrate on grasping the foundational mathematical rules. Consequently, a portion of the MathGLM's processing power is dedicated to understanding and generating step-by-step solutions, which might slightly weaken its prowess in precisely crafting arithmetic expressions. Nevertheless, while there's a minor dip in arithmetic accuracy, the step-by-step strategy significantly bolsters MathGLM's answer accuracy. By guiding MathGLM to derive answers progressively, this approach ensures MathGLM generates higher accuracy in solving math word problems. Notably, we observe pronounced improvements in answer accuracy across all GLM variants: 37.86% for GLM-Large, 42.29% for GLM-10B, 47.97% for GLM-6B, and 53.96% for GLM2-6B. Similar trends are also evident in the ChatGLM models, recording gains of 40.65% in ChatGLM-6B and 49.38% in ChatGLM2-6B. These results highlight the inherent trade-off between arithmetic accuracy and answer accuracy by employing the step-by-step strategy. While this strategy may introduce some potentially impact on arithmetic accuracy, it effectively enhance MathGLM's ability to generate accurate answers for math word problems.
GLM-Large GLM-10B GLM-6B GLM2-6B ChatGLM-6B ChatGLM2-6B +37.86% +42.29%
+47.97% +53.96% +40.65% +49.38% Figure 9: The answer accuracy of MathGLM is compared across various backbone models, both with and without the use of a step-by-step strategy. Employing the step-by-step approach, we observe a marked improvement in answer accuracy relative to the model's performance without it.</p>
<p>GLM-Large GLM-10B GLM-6B GLM2-6B ChatGLM-6B ChatGLM2-6B Figure 10: The arithmetic accuracy of MathGLM is evaluated across various backbone models, considering both with and without the implementation of a step-by-step strategy. Interestingly, there's a slight decrease in arithmetic accuracy when the step-by-step method is employed, likely due to the model having to perform calculations sequentially for each math problem. Figure 11 provides some failed examples generated by MathGLM-GLM-10B on solving math word problems. We can identify certain challenging scenarios where MathGLM-GLM-10B encounters difficulties in solving math word problems. One common issue is the misinterpretation of ambiguous language, leading to incorrect problem-solving approaches. For instance, ambiguous phrases such as "more than" or "less than" can be interpreted differently by the model, resulting in inaccurate solutions. Additionally, MathGLM-GLM-10B tends to struggle with problems that involve complex mathematical operations. As a result, it may provide partially correct arithmetic solutions but fail to arrive at the final correct answer.</p>
<p>Failure Analysis on Math Word Problems</p>
<p>Here, we construct a percentile graph to analyze the distribution of error types made by the MathGLM-GLM-10B on the Ape210K test dataset. As shown in Figure 12, we can identify the most common error types that may require improvement for the MathGLM-GLM-10B. One prominent error type that stands out is question misunderstood errors. These errors occur when the MathGLM-GLM-10B misunderstands the language and context of certain math word problems, leading to inaccurate problem-solving solutions. Despite these limitations, it is worth noting that MathGLM-GLM-10B still demonstrates a remarkable ability to solve a wide range of math word problems accurately.  </p>
<p>Conclusion</p>
<p>In this paper, our primary focus revolves around evaluating the mathematical reasoning capabilities of LLMs, encompassing both arithmetic operations and math word problems. For arithmetic tasks, we incorporate step-by-step solution and curriculum learning to train a Transformer-based language model from scratch. With comprehensive training on ample data, we establish that a language model boasting 2 billion parameters can achieve outstanding accuracy in multi-digit arithmetic tasks, exceeding GPT-4's results by a considerable margin. This finding compellingly challenges the prevailing cognition that LLMs face constraints in executing accurate arithmetic operations, especially when dealing with multi-digit numbers, decimals, and fractions, without leaning on external computational aids. When pivoting to math word problems, we reconstruct a dataset enriched with multi-step arithmetic operations. After fine-tuning our MathGLM on this revamped dataset derived from GLM-10B, it achieves similar performance to GPT-4 on the 5,000-sample test set of Chinese math problems, demonstrating its formidable prowess. </p>
<p>A Appendix</p>
<p>A.1 Tokenization for Arithmetic Tasks</p>
<p>The arithmetic operations in our MathGLM involve numbers from 0 to 9, and the calculating signs comprise addition (+), subtraction (-), multiplication (*), division (/), and exponentiation (ˆ). Symbols that represent forms in the data include the decimal point (.), percent sign (%), negative sign (-), fraction delimiter (/), brackets such as '(' and '[', and the equal sign (=).</p>
<p>To achieve a consistent tokenization process, we adopt the unified tokenization tool icetk proposed in CogView2 [7]. By leveraging this methodology, we tokenize each digit as a distinct token. For instance, the numeral "12345" is tokenized into the set {1, 2, 3, 4, 5}. To allocate singular tokens to the other mentioned symbols, we disengage the continuous representation symbols within icetk throughout the tokenization procedure. Table 11 shows some tokenization examples employed in MathGLM. This tokenization approach ensuers that every element in the arithmetic expression is adequately represented and can be efficiently processed by the MathGLM, facilitating MathGLM to excute comprehensive arithmetic tasks.</p>
<p>Owing to the variable lengths of arithmetic expressions, it becomes imperative to standardize their lengths for efficient training of the MathGLM. A straightforward method, like padding each input to a fixed length, might damage training efficacy. To circumvent this, we adopt a more efficient strategy, where multiple arithmetic expressions are concatenated until they achieve a predefined fixed length.</p>
<p>Input Tokenization Table 11: Some examples of tokenization in MathGLM.</p>
<p>A.2 Backbone Models</p>
<p>General Language Model (GLM) is a Transformer-based language model that combines autogressive blank infilling with bidirectional attention mechanisms. Different from decoder-only language models that primarily rely on unidirectional attention, GLM integrates bidirectional attention on unmasked contexts. This innovative approach empowers it with heightened proficiency in both comprehension and generative tasks.</p>
<p>Pre-Training Objectives. To amplify its linguistic understanding and generative abilities, GLM incorporates a dual pre-training strategy: 1) Autoregressive Blank Infilling involves predicting missing tokens within spans of corrupted text, wherein segments are arbitrarily supplanted with a [MASK] token. 2) Multi-Task Pretraining is utilized to endow GLM text generation ability, which aims to generate longer text by sampling random-length span from document-level or sentence-level text.</p>
<p>Model Sizes. GLM offers a diverse of models with various model parameters, including GLM-Large, GLM-6B, GLM-10B, GLM2-6B, ChatGLM-6B, and ChatGLM2-6B. Comprehensive specifics concerning the hyperparameters for each model variant can be found in Table 12. GLM-Large model is specifically tailored for Chinese language processing tasks equipped with 335M model parameters, while GLM-10B, GLM-6B, and GLM2-6B are equipped with 10 billion, 6 billion, and 6 billion parameters, respectively, enabling them to handle a wide range of NLP tasks with varying complexities. Augmenting the series are bilingual conversational models: ChatGLM-6B and ChatGLM2-6B, both tailored for Chinese-English bilingual dialogue tasks. The ChatGLM-6B model, having 6.2 billion parameters, undergoes fine-tuning using Chinese Q&amp;A and dialogue datasets. In contrast, ChatGLM2-6B emerges as an evolved iteration of ChatGLM-6B, marking enhancements in performance, extended context handling, optimized inference, and broader applicability.  </p>
<p>Model</p>
<p>A.3 Arithmetic Dataset</p>
<p>The training dataset for pre-training arithmetic model is created with a Python script. The dataset includes a variety of arithmetic expressions, encompassing different types of arithmetic operations such as addition, subtraction, multiplication, division, and exponentiation. Each expression in the dataset is composed of various types of numbers, including integers, decimals, fractions, percents, and negative numbers. The training dataset consists of approximately 50 million arithmetic sequences.</p>
<p>To investigate the impact of dataset scale on the arithmetic performance, we also create multiple datasets of varying sizes, including 1 million, 5 million, 10 million, and 25 million. This diverse representation of numbers ensures that the model can handle a wide range of numerical formats encountered in real-world arithmetic problems.</p>
<p>To facilitate the learning of underlying calculation rules, the arithmetic expressions are designed to be more complex than simple two-number calculations. Instead, each expression in the dataset involves multiple steps of calculations, ranging from 2 to 10 steps. By creating multi-step expressions, the model is exposed to more intricate mathematical reasoning and is better equipped to handle complex arithmetic problem-solving. The details of expressions is presented as follows. Table 13 demonstrates examples from the arithmetic dataset.</p>
<p>• Operations involving integers up to 10,000 that combine addition, subtraction, multiplication, and division. • Exponentiation tasks using an integer base up to 10,000 and an integer exponent capped at 100. • Bracketed expressions that include integers up to 10,000, combined with operations such as addition, subtraction, multiplication, and division. • Lengthy arithmetic expressions that incorporate brackets and blend various numerical types, including integers, decimals, percentages, and negative numbers. These sequences utilize operations such as addition, subtraction, multiplication, and division. • Arithmetic expressions involving fractions combined with various operations, including addition, subtraction, multiplication, and division.</p>
<p>A.4 Results on MATH 401 Table 14 shows a comprehensive evaluation of the arithmetic performance of MathGLM on the MATH 401 dataset [42]. This dataset offers a new set of arithmetic problems, allowing for a deeper exploration into MathGLM's proficiency in addressing a wide variety of arithmetic tasks. Table 13: Examples from the arithmetic dataset where "+", "-", "*", "/", "ˆ" denotes addition, subtraction, multiplication, division, and exponentiation respectively.</p>
<p>By evaluating MathGLM's performance on this dataset, we observe that MathGLM consistently outperforms all other large language models with a substantial number of model parameters. </p>
<p>A.5 Analysis on Arithmetic Errors</p>
<p>A.6 K6 Dataset</p>
<p>We collect math word problems from Chinese elementary schools in collaboration with the renowned educational institution, TAL AI Lab. The dataset consists of math problems for each grade level, with each grade containing approximately 100 problems. The wide-ranging nature of these math word problems empowers us to gauge the model's efficacy across an array of difficulty gradients and academic grades. To illustrate the diversity and complexity of the K6 dataset, we present some exemplary math word problems in Table 16. These examples show the range of mathematical concepts covered and the varying levels of difficulty present in the dataset.  </p>
<p>A.7 Baseline Models</p>
<p>Here, we leverage a variety of popular LLMs that can address Chinese problems to compare the mathematical reasoning ability among these LLMs and our MathGLM. The details of each baseline LLM as follows.</p>
<p>• GPT-4 [24] is the most advanced generative language model that developed by OpenAI, which successfully achieves so many SOTA performances on a variety of downstream tasks. • ChatGPT [23] is the predecessor of GPT4 and is constructed upon the success of Instruct-GPT [25], which is fine-tuned using instruction data with reinforcement learning from human feedback (RLHF), making it a powerful tool for natural language understanding and conversation. • MOSS [31] is an open-source LLM that consists of 16 billion model parameters. It utilizes 100 billion Chinese tokens and 20 billion English tokens to learn language patterns and semantic representations. • Ziya-LLaMA-13B [44] is a language model constructed on LLaMA-13B, which extends LLaMA-13B's character set to contain 7,000 Chinese characters and undergoes continual pre-training on a vast dataset of 110 billion Chinese tokens. • Chinese-Alpaca-13B [6] is a Chinese language model with 13 billion parameters that is built upon LLaMA-13B. During the supervised instruction tuning, the Low Rank Adaptation (LoRA) [13] technique is utilized to fine-tune LLaMA-13B for Chinese language tasks. • Baichuan-7B [14] shares similarities with LLaMA but is pre-trained from scratch on a massive dataset containing 1.2 trillion Chinese and English tokens. • ChatGLM-6B [34] and its successor ChatGLM2-6B [35] are language models that share a unified transformer architecture named GLM [8,43]. These models are pre-trained on a diverse dataset containing English and Chinese data, combined with the supervised instruction tuning, makes them powerful tools for understanding and generating text in both English and Chinese contexts.</p>
<p>A.8 Training Steps Analysis.</p>
<p>We explore the impact of training steps on the MathGLM's performance by analyzing its performance against varied training steps, as depicted in Figure 13. The results reveal that there is a consistent uptrend in performance as the number of training steps increases. With more training steps, MathGLM becomes increasingly adept at comprehending and resolving math word problems, which translates to a surge in accuracy. However, it is clearly observed that the performance gains of MathGLM start to plateau after a certain point, indicating potential diminishing returns with extended training. These findings highlight the significance of finding an optimal balance between training time and performance gains for MathGLM in solving math word prblems. Additionally, we observe that model undergoing instruction tuning requires a longer training duration to achieve consistent accuracy on math word problems. </p>
<p>A.9 Case study</p>
<p>Here, we present specific cases to demonstrate the solving process of the MathGLM on both arithmetic tasks and math word problems. As shown in Figure 14, these examples illustrate how the MathGLM leverages a step-by-step strategy to solve both arithmetic tasks and math word problems accurately.</p>
<p>For arithmetic tasks, the MathGLM breaks down complex calculations into multiple steps rather than a direct answer. The step-by-step strategy ensures that each intermediate result is accurately computed, leading to the final correct answer for each arithmetic expression.</p>
<p>In math word problems, the MathGLM utilizes LLMs's language understanding capabilities to understand the problem statement accurately. The MathGLM then applies the step-by-step strategy to seperately calculate the generated arithmetic expressions for each math problem, enabling it to obtain the correct answer for math word problems.</p>
<p>Math Word Problems Arithmetic Tasks</p>
<p>Input Output Input Output</p>
<p>Figure 2 :
2Examples of MathGLM's response on a variety of arithmetic tasks.</p>
<p>Figure 4 :
4Comparison between the original Ape210k dataset and the reconstructed version. A step-by-step strategy is employed to reconstruct the solutions for each mathematical problem.backbone models bestow the MathGLM with a basic language understanding skills, enabling it to effectively comprehend linguistic information contained within math word problems. The details of backbone models are presented in Appendix A.2.</p>
<p>Figure 5 :
5Performance visualization on MathGLM under different scaling configurations, including model parameters and training data sizes.</p>
<p>Figure 7 :
7Performance comparison of MathGLM with and without the step-by-step solution.</p>
<p>Figure 8 .
8The figure shows the overall performance results for GPT-4, ChatGPT, Chinese-Alpaca-13B, MOSS-16B, Ziya-LLaMA-13B, Baichuan-7B, ChatGLM-6B, ChatGLM2-6B, and MathGLM-GLM-10B across each individual grade level. The detailed introduction of these</p>
<p>Figure 8 :
8Performance comparison between MathGLM and other popular language models on the K6 dataset.</p>
<p>Figure 11 :
11Some failed examples generated by MathGLM-GLM-10B on solving math word problems.</p>
<p>Figure 12 :
12The distribution of error types generated by MathGLM-GLM-10B on math word problems.</p>
<p>Figure 13 :
13The impact of training steps on MathGLM with different backbone models. Fine-tuning on model undergoing instruction tuning requires a longer training steps.</p>
<p>Table 2
2reports an overview of all the models with different </p>
<p>Table 2 :
2Model sizes and architectures of MathGLM.the rules associated with arithmetic operations involving large numbers. Such training strategy 
allows MathGLM initially tackles simpler examples, progressively advancing towards more complex 
challenges. More importantly, such approach empowers MathGLM to improve its ability by learning 
from relatively smaller examples, emphasizing the efficiency of MathGLM to handle increasingly 
intricate tasks or data patterns. </p>
<p>Table 3 ,
3consistently show that MathGLM outperforms all other models, indicating its superior performance in tackling arithmetic tasks. Even when we consider a more small model variant, namely MathGLM-10M with a mere 10 million parameters, the results reveal a surprising phenomenon. Despite its compact parameter size, MathGLM-10M outperforms GPT-4 and ChatGPT across an array of comprehensive arithmetic tasks. This astonishing results show the effectiveness of MathGLM's approach, which involves decomposing complex arithmetic expressions into individual steps, granting it the capacity to discern and comprehend the subtleties within arithmetic tasks. It effectively learns the underlying rules and principles of arithmetic operations, enabling it to generate accurate and precise solutions. Furthermore, when comparing MathGLM across different parameter scales, we observe that the MathGLM's arithmetic performance is directly correlated with the augmentation of its parameter count. This finding suggest that as models increase in size, their performance exhibits a corresponding enhancement. To sum up, the evaluation results on complex arithmetic tasks underscore the exceptional performance of MathGLM. By breaking down arithmetic tasks, these models surpass the performance of GPT-4 and ChatGPT significantly.Model 
ACC 
RE </p>
<p>GPT-4 
18.84% 
-
ChatGPT 
10.00% 
-</p>
<p>MathGLM-10M 
61.21% 
97.83% 
MathGLM-100M 
70.28% 
99.28% 
MathGLM-500M 
89.57% 
99.41% 
MathGLM-2B 
93.03% 
99.71% </p>
<p>Table 3 :
3Performance comparison on an arithmetic dataset containing 9,592 test cases between MathGLM and the leading LLMs.</p>
<p>Table 4 :
4Overall performance comparison on various LLMs in term of Accuracy.Grouped Results. To clearly evaluate the arithmetic ability of MathGLM among different operations, 
we design a series of extended experiments. Specifically, we design small test datasets comprising 
100 test cases to respectively evaluate the arithmetica performance of MathGLM in various arithmetic 
operations, including addition, subtraction, multiplication, and division. These datasets encompass 
different data formats, such as integers, decimals, percents, fractions and negative numbers. Here, we 
compare MathGLM with several well-known chat-type LLMs, such as GPT-4, ChatGPT, ChatGLM, 
and Bard. The arithmetic performance comparison among these different language models is 
demonstrated in </p>
<p>Table 5 :
5Arithmetic comparison between MathGLM and other LLMs among different operations. Int denotes integers, Dec denotes decimals, Frac denotes fractions, Perc denotes percents, and Neg denotes negative numbers.improvement as model size increases. For ADD and SUB tasks, the accuracy remains consistently high across all model sizes with slight variations. There is a tendency for larger models to achieve higher accuracy compared to smaller models but the differences in performance between different model sizes are relatively small. In the MUL task, accuracy rises distinctly with larger model sizes. Smaller models exhibit relatively lower accuracy, while larger counterparts demonstrate enhanced accuracy, particularly in tasks involving higher digit numbers. A similar tendency can be observed in the DIV task. Overall, the evaluation results demonstrate that MathGLM outperforms GPT-4 in high-digit arithmetic tasks, and the performance generally inproves with larger model sizes.Analysis on MathGLM. Despite achieving an impressive overall accuracy of 93.03% with its 2 
billion model parameters, a thorough analysis is conducted to comprehend instances where MathGLM 
fails to generate accurate answers. Consider the example 3468  *  4046/7424, MathGLM generate 
an answer of 468  *  4046/7424 = 14031528/7424 = 1889.901400862069, while the true answer is 
468  *  4046/7424 = 14031528/7424 = 1890.0226293103. Upon comparing the generated results </p>
<p>Table 6 :
6Overall performance comparison on GPT-4 and MathGLM on BIG-bench Arithmetic subtask.</p>
<p>Table 7 :
7Performance comparison between most powerful LLMs and MathGLM on various multidigit arithmetic operations.</p>
<h2>ModelArithmetic Acc Answer AccGPT-4</h2>
<p>59.57% 
GPT-3.5-turbo 
-
39.78% </p>
<h2>GLM-Large</h2>
<p>0% 
+ MathGLM 
62.00% 
50.80% 
GLM-6B 
-
3.94% 
+ MathGLM 
64.60% 
48.06% 
GLM-10B 
-
0% 
+ MathGLM 
69.08% 
58.68% </p>
<h2>GLM2-6B</h2>
<p>31.42% 
+ MathGLM 
52.24% 
45.48% </p>
<h2>ChatGLM-6B</h2>
<p>6% 
+ MathGLM 
58.52% 
42.28% 
ChatGLM2-6B 
-
31.70% 
+ MathGLM 
50.38% 
43.14% </p>
<p>Table 8 :
8Performance comparison among different language models on the Ape210K dataset.</p>
<p>Table 9 :
9Overall performance comparison on various LLMs in term of Accuracy.Grade 1 </p>
<p>+11.71% </p>
<p>+8.34% </p>
<p>Grade 4 
Grade 5 </p>
<p>Grade 2 </p>
<p>+6.60% </p>
<p>+1.37% </p>
<p>Grade 5 </p>
<p>Grade 3 </p>
<p>+13.89% </p>
<p>+50.98% </p>
<p>Grade 6 </p>
<p>Table 10 :
10Performance comparison of MathGLM on different training dataset sizes and model parameters.</p>
<p>Table 12 :
12Hyperparameters of the backbone models.</p>
<p>Table 15
15provides some examples to analyze the failures of MathGLM on performing arithmetic tasks. Through careful examination of these examples, we can observe several patterns and trends in the MathGLM's errors. Firstly, MathGLM appears to grapple with intricate arithmetic expressions, particularly those combining several operations and large numbers. For instance, the expression 14031528/742: the division of an 8-digit number by a 4-digit one proves problematic for MathGLM, leading to miscalculations in the outcome. Secondly, MathGLM tends to encounter difficulties when dealing with long sequences of numbers and operations. As the expression length increases, the model's ability to accurately perform arithmetic calculations diminishes, leading to inaccurate results. For example, expression involving multiplication among two large numbers like 3626 * 8919 and calculation with a decimal and large integer number like 1.610311 * 7691. These errors generated by MathGLM usually have only one calculation result error, indicating that the MathGLM's mistakes mainly occur at specific calculation steps rather than affecting the entire expression.</p>
<p>K3乐乐家养了36只小鸡,其中1/4是公鸡,母鸡是公鸡的3倍,公鸡和母鸡各有多少只?K4 公 益 小 组 的 同 学 为 敬 老 院 的 老 人 们 制 作 香 囊(náng ),12个 组 共 制 作 了864个,每 组 都 有9人,平均每人制作了几个?Grade </p>
<p>Example </p>
<p>K1 
李老师买了20颗糖果,送给小丽5颗,送给小刚8颗,还剩多少颗糖果? </p>
<p>K2 
一个乘数是4,另一个乘数是7,积是多少? </p>
<p>K5 
东、西两城相距180千米,甲、乙两车分别从东、西两城同时出发,相向而行,1.2小时后两 
车可相遇.实际甲车出发0.4小时后因故障停车,乙车又走了2小时才和甲车相遇,求乙车每 
小时行多少千米? </p>
<p>K6 
甜甜读一本小说,第一天读了这本书的3/8,正好是180页,第二天又读了这本书的1/6,第2天 
读了多少页? </p>
<p>Table 16 :
16Examples from the K6 dataset to demonstrate the diversity and complexity of this dataset.</p>
<p>1642/ 521+2051=674+82784017990+6994/883-1642/521+2051=674+82784017990+7.92072 4801812004-1642/521+2051=674+82784017990+7.920724801812004-3.15163147792674+2939<em>2987</em>9430+6994/883-1642/521+2051=674+8778793<em>9430+6994/883. 674+2939</em>2987<em>9430+6994/883-1642/521+2051=674+8778793</em>9430+6994/883-1642/ 521+2051=674+82784017990+6994/883-1642/521+2051=674+82784017990+7.92072 4801812004-1642/521+2051=674+82784017990+7.920724801812004-3.15163147792</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, arXiv:2211.12588arXiv preprintWenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022.</p>
<p>Chinese-vicuna: A chinese instruction-following llama-based model. Zhenyi Lu, Chenghao Fan, Jie Tian, Zhenyi Lu Chenghao Fan and Jie Tian. Chinese-vicuna: A chinese instruction-following llama-based model. 2023. URL https://github.com/Facico/Chinese-Vicuna.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won, Charles Chung, Sebastian Sutton, Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. arXiv preprintAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168arXiv preprintKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Efficient and effective text encoding for chinese llama and alpaca. Yiming Cui, Ziqing Yang, Xin Yao, arXiv:2304.08177arXiv preprintYiming Cui, Ziqing Yang, and Xin Yao. Efficient and effective text encoding for chinese llama and alpaca. arXiv preprint arXiv:2304.08177, 2023.</p>
<p>Cogview2: Faster and better text-to-image generation via hierarchical transformers. Ming Ding, Wendi Zheng, Wenyi Hong, Jie Tang, Advances in Neural Information Processing Systems. 35Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation via hierarchical transformers. Advances in Neural Information Processing Systems, 35:16890-16902, 2022.</p>
<p>Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang, Glm, arXiv:2103.10360General language model pretraining with autoregressive blank infilling. arXiv preprintZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360, 2021.</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning. PMLRLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pages 10764-10799. PMLR, 2023.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, arXiv:2009.03300Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprintDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.03874arXiv preprintDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.</p>
<p>Training compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.15556arXiv preprintJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, Lora, arXiv:2106.09685Low-rank adaptation of large language models. arXiv preprintEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.</p>
<p>Baichuan-7b. Baichuan, Baichuan inc. Baichuan-7b. https://github.com/baichuan-inc/baichuan-7B/blob/main/ README_EN.md.</p>
<p>Length generalization in arithmetic transformers. Samy Jelassi, Carles Stéphane D&apos;ascoli, Yuhuai Domingo-Enrich, Yuanzhi Wu, François Li, Charton, arXiv:2306.15400arXiv preprintSamy Jelassi, Stéphane d'Ascoli, Carles Domingo-Enrich, Yuhuai Wu, Yuanzhi Li, and François Charton. Length generalization in arithmetic transformers. arXiv preprint arXiv:2306.15400, 2023.</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 35Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199-22213, 2022.</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Advances in Neural Information Processing Systems. 35Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843-3857, 2022.</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, Edwards, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. Bowen Baker, Teddy LeeHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step, 2023.</p>
<p>Tiedong Liu, Bryan Kian Hsiang Low, arXiv:2305.14201Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks. arXiv preprintTiedong Liu and Bryan Kian Hsiang Low. Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks. arXiv preprint arXiv:2305.14201, 2023.</p>
<p>Evaluating transformer language models on arithmetic operations using number decomposition. Matteo Muffo, Aldo Cocco, Enrico Bertino, arXiv:2304.10977arXiv preprintMatteo Muffo, Aldo Cocco, and Enrico Bertino. Evaluating transformer language models on arithmetic operations using number decomposition. arXiv preprint arXiv:2304.10977, 2023.</p>
<p>Investigating the limitations of transformers with simple arithmetic tasks. Rodrigo Nogueira, Zhiying Jiang, Jimmy Lin, arXiv:2102.13019arXiv preprintRodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. Investigating the limitations of transformers with simple arithmetic tasks. arXiv preprint arXiv:2102.13019, 2021.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, arXiv:2112.00114Show your work: Scratchpads for intermediate computation with language models. arXiv preprintMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.</p>
<p>. Openai, Chatgpt, OpenAI. Chatgpt. https://mkai.org/chatgpt-optimizing-language-models-for-dialogue/.</p>
<p>. Openai, Gpt-4 technical reportOpenAI. Gpt-4 technical report, 2023.</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 35Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.</p>
<p>Scaling language models: Methods, analysis &amp; insights from training gopher. Sebastian Jack W Rae, Trevor Borgeaud, Katie Cai, Jordan Millican, Francis Hoffmann, John Song, Sarah Aslanides, Roman Henderson, Susannah Ring, Young, arXiv:2112.11446arXiv preprintJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis &amp; insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.</p>
<p>Angela Teven Le Scao, Christopher Fan, Ellie Akiki, Suzana Pavlick, Daniel Ilić, Roman Hesslow, Alexandra Sasha Castagné, François Luccioni, Matthias Yvon, Gallé, arXiv:2211.05100A 176b-parameter open-access multilingual language model. arXiv preprintTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.04761arXiv preprintTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.</p>
<p>Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick Legresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, arXiv:2201.11990arXiv preprintShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022.</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, R Adam, Adam Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.04615arXiv preprintAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.</p>
<p>Moss github. Tianxiang Sun, Xipeng Qiu, Tianxiang Sun and Xipeng Qiu. Moss github. https://github.com/OpenLMLab/MOSS/blob/main/ README_en.md.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, arXiv:2211.09085Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprintRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Language models for dialog applications. arXiv preprintRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.</p>
<p>. Thudm Chatglm-6b, THUDM. Chatglm-6b. https://github.com/THUDM/ChatGLM-6B/blob/main/README_en.md, .</p>
<p>THUDM. Chatglm2-6b. THUDM. Chatglm2-6b. https://github.com/THUDM/ChatGLM2-6B/blob/main/README_EN.md, .</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Naman Baptiste Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Open and efficient foundation language models. arXiv preprintHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.</p>
<p>Solving math word problems with process-and outcome-based feedback. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, Irina Higgins, arXiv:2211.14275arXiv preprintJonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.</p>
<p>Exploring generalization ability of pretrained language models on arithmetic and logical reasoning. Cunxiang Wang, Boyuan Zheng, Yuchen Niu, Yue Zhang, Natural Language Processing and Chinese Computing: 10th CCF International Conference, NLPCC 2021. Qingdao, ChinaSpringerProceedings, Part I 10Cunxiang Wang, Boyuan Zheng, Yuchen Niu, and Yue Zhang. Exploring generalization ability of pretrained language models on arithmetic and logical reasoning. In Natural Language Processing and Chinese Computing: 10th CCF International Conference, NLPCC 2021, Qingdao, China, October 13-17, 2021, Proceedings, Part I 10, pages 758-769. Springer, 2021.</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.11171arXiv preprintXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, V Quoc, Denny Le, Zhou, Advances in Neural Information Processing Systems. 35Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.</p>
<p>How well do large language models perform in arithmetic tasks?. Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, arXiv:2304.02015arXiv preprintZheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. How well do large language models perform in arithmetic tasks? arXiv preprint arXiv:2304.02015, 2023.</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, arXiv:2210.02414Glm-130b: An open bilingual pre-trained model. arXiv preprintAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.</p>
<p>. Jiaxing Zhang, Ruyi Gan, Junjie Wang, Yuxiang Zhang, Lin Zhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiaoqun Dong, Junqing He, Jianheng Zhuo, Qi Yang, Yongfeng Huang, Xiayu Li, Yanghan Wu, Junyu Lu, Xinyu Zhu, Weifeng Chen, Ting Han, Kunhao Pan, Rui Wang, Hao Wang, Xiaojun Wu, Zhongshen Zeng, Chongpei Chen, Fengshenbang 1.0: Being the foundation of chinese cognitive intelligence. CoRR, abs/2209.02970, 2022Jiaxing Zhang, Ruyi Gan, Junjie Wang, Yuxiang Zhang, Lin Zhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiaoqun Dong, Junqing He, Jianheng Zhuo, Qi Yang, Yongfeng Huang, Xiayu Li, Yanghan Wu, Junyu Lu, Xinyu Zhu, Weifeng Chen, Ting Han, Kunhao Pan, Rui Wang, Hao Wang, Xiaojun Wu, Zhongshen Zeng, and Chongpei Chen. Fengshenbang 1.0: Being the foundation of chinese cognitive intelligence. CoRR, abs/2209.02970, 2022.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.01068Open pre-trained transformer language models. arXiv preprintSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.</p>
<p>Ape210k: A large-scale and template-rich dataset of math word problems. Wei Zhao, Mingyue Shang, Yang Liu, Liang Wang, Jingming Liu, arXiv:2009.11506arXiv preprintWei Zhao, Mingyue Shang, Yang Liu, Liang Wang, and Jingming Liu. Ape210k: A large-scale and template-rich dataset of math word problems. arXiv preprint arXiv:2009.11506, 2020.</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, arXiv:2205.10625arXiv preprintDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.</p>
<p>Teaching algorithmic reasoning via in-context learning. Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, Hanie Sedghi, arXiv:2211.09066,2022.12345+345=arXiv preprintHattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi. Teaching algorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066, 2022. 12345+345= ['<em>', '1', '2', '3', '4', '5', '+', '3', '4', '5', '='] [20005, 20009, 20010, 20013, 20016, 20015, 20065, 20013, 20016, 20015, 20054] 1234-45678= ['</em>', '1', '2', '3', '4', '-', '4', '5', '6', '7', '8', '='] [20005, 20009, 20010, 20013, 20016, 20011, 20016, 20015, 20021, 20025, 20023, 20054]</p>
<p>. * 678=, <em>678= ['_', '3', '4', '</em>', '6', '7', '8', '='] [20005, 20013, 20016, 20032, 20021, 20025, 20023, 20054]</p>
<p>. / 2=, 2/2= ['_', '1', '.', '2', '/', '2', '='] [20005, 20009, 20007, 20010, 20026, 20010, 20054]</p>
<p>. * 10+2=1+8<em>, 10+2=1+80+2=81+2=83 53-2+23+51</em>56=53-2+23+2856=51+23+2856=74+2856=2930 214-792<em>509</em>260<em>556=214-403128</em>260<em>556=214- 104813280</em>556=214-58276183680=-58276183466<em>10+2=1+8</em>10+2=1+80+2=81+2=83 53-2+23+51<em>56=53-2+23+2856=51+23+2856=74+2856=2930 214-792</em>509<em>260</em>556=214-403128<em>260</em>556=214- 104813280*556=214-58276183680=-58276183466</p>
<p>/24)<em>-(8/70))/-(34/80)=(+(49/24)</em>(8/70. 34/80)=(392/1680)/(34/80)= (7/30)/(34/80)=(7/30)<em>(80/34)=(560/1020)=28/51 (9947/9276)+(4411/9276)=14358/9276=2393/15463ˆ9=19683, 93ˆ18=270827695297250208363869180422467849 100ˆ13=100000000000000000000000000 Expression of fractions. 3ˆ9=19683, 93ˆ18=270827695297250208363869180422467849 100ˆ13=100000000000000000000000000 Expression of fractions ((49/24)</em>-(8/70))/-(34/80)=(+(49/24)<em>(8/70))/(34/80)=(392/1680)/(34/80)= (7/30)/(34/80)=(7/30)</em>(80/34)=(560/1020)=28/51 (9947/9276)+(4411/9276)=14358/9276=2393/1546</p>
<p>1945+8878)=8371<em>(-1945+8878)=8371</em>6933=58036143. <em>(-1945+8878)=8371</em>(-1945+8878)=8371*6933=58036143</p>
<p>2090-5457.35697)<em>73.0=-7547.35697</em>73.0=-550957.05881Lengthy arithmetic expressions. Lengthy arithmetic expressions (-2090-5457.35697)<em>73.0=-7547.35697</em>73.0=-550957.05881</p>
<p>. -4457+, 7823/5483%)<em>-3338=-4457+(-7823/54.83)</em>- 3338=-4457+(-142.6773664052526)<em>-3338=-4457+- 142.6773664052526</em>-3338=-4457+142.6773664052526<em>3338=- 4457+476257.0490607332=471800.0490607332-4457+(-7823/5483%)</em>-3338=-4457+(-7823/54.83)<em>- 3338=-4457+(-142.6773664052526)</em>-3338=-4457+- 142.6773664052526<em>-3338=-4457+142.6773664052526</em>3338=- 4457+476257.0490607332=471800.0490607332</p>
<p>7424= 14031528/7424=1889.901400862069 14031528/7424=1890.0226293103449 (3626<em>8919)/8861= 32330294/8861=3648.605574991536 32340294/8861=3649.7341157882856Model ACC. Model ACC 7424= 14031528/7424=1889.901400862069 14031528/7424=1890.0226293103449 (3626</em>8919)/8861= 32330294/8861=3648.605574991536 32340294/8861=3649.7341157882856</p>
<p>. * 5755+3741-, 7533=159923389900+3741- 7533=159923393641-7533=159923386108*5755+3741- 7533=159923389900+3741- 7533=159923393641-7533=159923386108</p>
<p>. * 5755+3741-, 7533=149923389900+3741- 7533=149923393641-7533=149923386108*5755+3741- 7533=149923389900+3741- 7533=149923393641-7533=149923386108</p>
<p>6532+3588)+-1499.90071<em>-3871Some failed examples generated by MathGLM. -7439+(3156+1649%)+(-2307+-1112)= -7439+(3156+16.49)+. 15Table 15: Some failed examples generated by MathGLM. -7439+(3156+1649%)+(-2307+-1112)= -7439+(3156+16.49)+ (-2307+-1112)=-7439+3172.49+ (-2307+-1112)=-7439+3172.49+ (-2307-1112)=-7439+3172.49+-34 19=-7439+3172.49-3419=-4266.5 1-3419=-7685.51 4392.7</em>1127/ [(6532+3588)+-1499.90071*-3871.</p>
<p>. * 8637=2824/, 8553+2101.9815950920245)<em>8632202</em>8637=2824/2202- (8553+2101.9815950920245)*863</p>
<p>. * 8637=1, 2824699818346957-10 654.981595092025<em>8637=1.2824 699818346957-92027076.036809</em>8637=1.2824699818346957-10 654.981595092025*8637=1.2824 699818346957-92027076.036809</p>
<p>Some cases generated by MathGLM on arithmetic tasks and math word problems. Figure. 14Figure 14: Some cases generated by MathGLM on arithmetic tasks and math word problems.</p>            </div>
        </div>

    </div>
</body>
</html>
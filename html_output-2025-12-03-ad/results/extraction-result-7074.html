<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7074 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7074</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7074</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-d766bffc357127e0dc86dd69561d5aeb520d6f4c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d766bffc357127e0dc86dd69561d5aeb520d6f4c" target="_blank">Training language models to follow instructions with human feedback</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> The results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent and showing improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets.</p>
                <p><strong>Paper Abstract:</strong> Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7074.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7074.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructGPT (PPO-ptx)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructGPT fine-tuned with PPO and pretraining-mix (PPO-ptx)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3 architecture models (1.3B, 6B, 175B) fine-tuned with supervised demonstrations then optimized with reinforcement learning from human feedback (reward model + PPO) while mixing in pretraining gradients to reduce performance regressions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (PPO-ptx)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 architecture transformer models fine-tuned first with supervised demonstrations (SFT) then with RLHF using a learned reward model and PPO; PPO-ptx mixes pretraining gradients into the PPO objective and applies a per-token KL penalty to the SFT policy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.3B, 6B, 175B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (GPT-3 family) fine-tuned with policy optimization (PPO) and reward model</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Labeler demonstration dataset (~13k prompts), human comparison/ranking dataset (~33k prompts), additional PPO input prompts (~31k from API); plus a pretraining mix sampled from the original GPT-3 pretraining distribution when using PPO-ptx.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>No explicit symbolic/logical solver — improvements in reasoning-like behavior arise from supervised instruction data and RLHF (reward model + PPO) that optimizes for labeler preferences (helpful/honest/harmless); PPO-ptx mixes pretraining gradients to preserve upstream capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TruthfulQA (primary safety/truth benchmark reported)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Dataset designed to test truthfulness of model answers, including adversarial questions where models tend to mimic common human falsehoods; used to rate truthfulness and informativeness.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Open-domain question answering / truthfulness evaluation; closed-domain hallucination checks on summarization/closed QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Human-evaluated fraction rated 'truthful and informative' (TruthfulQA); hallucination rate on closed-domain tasks; human preference winrate vs baselines</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On TruthfulQA: InstructGPT PPO models generate truthful and informative answers about twice as often as GPT-3 (text claim in paper). On closed-domain tasks: hallucination rate ~21% for InstructGPT vs ~41% for GPT-3. Preference: 175B InstructGPT preferred to 175B GPT-3 85 ± 3% of the time and preferred to few-shot 175B GPT-3 71 ± 4% of the time. Toxicity: ~25% fewer toxic outputs than GPT-3 when prompted to be respectful (paper claim).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Large and consistent gains in human preference and reduced hallucination vs GPT-3 and GPT-3 few-shot; outperforms FLAN/T0 on the API prompt distribution (InstructGPT preferred ≈78-79% over those models). Some SFT baselines are less toxic but less preferred overall.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fine-tuning GPT-3 with human demonstrations and RLHF (PPO) substantially improves instruction following, reduces hallucinations on closed-domain tasks, increases truthfulness on TruthfulQA, and reduces toxic outputs under respectful prompts; PPO-ptx (mixing pretraining) mitigates many capability regressions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>No explicit formal logical-reasoning module — gains are empirical and due to human fine-tuning. Some improvements do not fully generalize (e.g., 1.3B PPO-ptx sometimes underperforms same-size GPT-3 on TruthfulQA). InstructGPT still makes simple mistakes, assumes false premises, hedges excessively, and exhibits performance regressions on certain public NLP datasets (SQuAD, DROP, translation) unless pretraining mix is added; when explicitly prompted to be toxic it can generate more toxic outputs than GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training language models to follow instructions with human feedback', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7074.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7074.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RLHF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement Learning from Human Feedback (RLHF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training pipeline that uses human preference comparisons to train a reward model, then fine-tunes a policy (language model) to maximize that reward via PPO, with added KL penalty to the supervised baseline and optional pretraining-gradient mixing (PPO-ptx).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RLHF (reward model + PPO)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method: collect human demonstrations (SFT), collect human rankings of model outputs to train a reward model (RM), then run PPO to optimize the language model policy against the RM with a KL penalty to the SFT policy; optional term mixes pretraining log-likelihood gradients (PPO-ptx).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>applied to GPT-3 sizes (1.3B, 6B, 175B); RM used 6B in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer policy with learned scalar reward model; PPO policy gradient optimization</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Human demonstration dataset (~13k), human comparison dataset (~33k), PPO unlabeled prompts (~31k); RM trained on the K-way ranked outputs (K=4..9) producing pairwise labels.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Preference optimization for alignment; not a symbolic reasoning method — uses human preferences to shape outputs, which can improve truthfulness/constraint following and reduce hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Reward model accuracy and downstream human-preference metrics; evaluated on TruthfulQA, RealToxicityPrompts, SQuAD, DROP, HellaSwag, WMT translation, Winogender, CrowS-Pairs</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>RM is evaluated by its accuracy at predicting human comparison choices; models optimized with RLHF are evaluated by human preference winrates and by standard public NLP/safety benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Training/optimization method applied to instruction following, truthfulness, toxicity reduction, QA, summarization and other NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>RM accuracy (%) on held-out comparisons; human preference winrate; task-specific metrics on public benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reward model accuracy: 72.4 ± 0.4% on training-set labelers, 69.6 ± 0.9% on held-out labelers (5-fold experiment). RLHF yields large human preference improvements (e.g., 175B InstructGPT preferred vs GPT-3 85 ± 3%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>RLHF (PPO) improves human preference and truthfulness metrics over SFT and GPT-3; mixing pretraining gradients (PPO-ptx) reduces performance regressions vs baseline compared to PPO without pretraining mix.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RLHF is an effective, cost-efficient method to align language models to human preferences across diverse instruction-following tasks and to improve truthfulness/hallucination behavior without needing explicit symbolic reasoning components.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Alignment is to the particular labeler population and their instructions; RM can overfit if comparisons are treated naively (mitigated by batching all pairwise comparisons); 175B RM training was found unstable so RM experiments used 6B; RLHF can introduce an 'alignment tax' (capability regressions) on public NLP tasks unless pretraining gradients are mixed in; method does not guarantee formal logical correctness or general deductive reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training language models to follow instructions with human feedback', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7074.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7074.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised Fine-Tuning (SFT) on human demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tuning pretrained GPT-3 on human-written demonstration outputs for instruction-like prompts, used as a baseline and as the initialization policy for RLHF.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SFT (supervised fine-tune of GPT-3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 models fine-tuned for 16 epochs on a dataset of labeler demonstrations (~13k prompts) with cosine learning-rate decay and dropout; SFT models often overfit validation loss but continued training improved RM score and human preference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.3B, 6B, 175B (same GPT-3 architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (GPT-3) with supervised cross-entropy fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Labeler demonstrations dataset (~13k examples) drawn from API prompts and labeler-written prompts</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Learning from human demonstration examples (behavior cloning) to follow instructions; no explicit stepwise logical reasoning technique applied.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Human preference tests on API prompt distribution and public NLP datasets (same as for RLHF)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>SFT serves as a baseline for preference ratings and downstream tasks; human evaluators rate outputs for overall quality, instruction-following, hallucination, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Instruction following across varied tasks (generation, QA, summarization, code, multilingual prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Human preference winrate and RM validation score; toxicity measured via RealToxicityPrompts; other dataset metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SFT improves over raw GPT-3 and few-shot prompting in human preference; SFT baseline was less toxic than other variants but also less preferred and lower continuity according to human ratings (no single numeric delta beyond paper claims).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>SFT improves over vanilla GPT-3 but RLHF (PPO) further improves human preference and instruction-following reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SFT provides a strong initialization for RLHF and yields better instruction following than un-finetuned GPT-3, but RLHF further boosts human-preferred behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>SFT can overfit on validation loss, and by itself does not achieve the highest human preference scores; SFT-only models may produce short/degenerate responses that lower continuity despite low toxicity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training language models to follow instructions with human feedback', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7074.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7074.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (Brown et al., 2020) baseline models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pretrained transformer language models used as baselines: raw GPT-3, GPT-3 prompted (few-shot prefix), and GPT-3 fine-tuned variants (FLAN, T0) for comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer language models pretrained on broad internet data (GPT-3 family); evaluated as raw few-shot prompted models and as fine-tuned baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (primary baseline), also smaller sizes (1.3B, 6B) referenced</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Autoregressive Transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Large-scale web and curated corpora used in GPT-3 pretraining (not detailed in this paper); few-shot prompting uses in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>No dedicated logical reasoning method applied; reasoning-like behavior achieved via few-shot prompting or fine-tuning (FLAN/T0) in other works.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Used as baseline across TruthfulQA, RealToxicityPrompts, SQuAD, DROP, HellaSwag, WMT, Winogender, CrowS-Pairs</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Standard public NLP and safety benchmarks used to compare against InstructGPT and RLHF-trained variants.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Baseline for instruction following, QA, summarization, toxicity and bias evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Human preference winrate, truthfulness/informativeness rate, toxicity scores, benchmark-specific metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GPT-3 is substantially outperformed in human preference by InstructGPT (e.g., 175B InstructGPT preferred to 175B GPT-3 85 ± 3%); GPT-3 hallucinates more in closed-domain tasks (~41% vs InstructGPT ~21%); on toxicity, InstructGPT reduces toxic outputs under respectful prompts but raw GPT-3 can behave differently depending on prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Used as baseline; InstructGPT and RLHF variants show large improvements in instruction following and truthfulness relative to GPT-3; FLAN/T0 fine-tuned GPT-3 variants perform better than default GPT-3 but worse than InstructGPT on API distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Scaling alone (bigger GPT-3) does not guarantee alignment with user intent; fine-tuning with human feedback produces greater alignment improvements than model-size increases alone.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Without targeted fine-tuning or prompting GPT-3 can produce unhelpful, untruthful, or toxic outputs and hallucinates on closed-domain tasks; few-shot prompting helps but does not match RLHF gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training language models to follow instructions with human feedback', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7074.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7074.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TruthfulQA (dataset for truthfulness evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmark used to measure whether models produce truthful and informative answers to questions, including adversarially chosen prompts that commonly induce false but stereotypical responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TruthfulQA (evaluation dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A set of questions designed to test model truthfulness and tendency to produce human-like falsehoods; evaluated via human raters and instruction prompting (Instruction+QA trick to elicit 'I have no comment').</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>N/A (benchmark for evaluating truthfulness rather than a reasoning method)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Dataset of questions designed to probe model propensity to produce falsehoods and to evaluate truthfulness and informativeness under different prompting/regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Open-domain question answering focused on truthfulness and avoidance of common false answers</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Human-rated truthfulness and informativeness fraction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports InstructGPT PPO models generate truthful and informative answers about twice as often as GPT-3 overall; 1.3B PPO-ptx is an exception, performing slightly worse than same-size GPT-3. When using an 'Instruction+QA' prompt, PPO models err on the side of refusing to answer falsely.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>InstructGPT (PPO) > GPT-3 on TruthfulQA by approximately 2x in paper statements; exception at smaller model size.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RLHF-trained models show significant improvements in truthfulness versus GPT-3, and can be prompted to abstain when uncertain; gains persist even on non-adversarial subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>TruthfulQA captures only a subset of 'honesty'; improvements are empirical and do not imply formal logical correctness; smaller PPO-ptx models may not uniformly improve; automatic TruthfulQA metrics initially overstated gains (noted in acknowledgements).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training language models to follow instructions with human feedback', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7074.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7074.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Public NLP datasets (SQuAD, DROP, HellaSwag, WMT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Selected public NLP evaluation datasets (SQuAD v2, DROP, HellaSwag, WMT 2015 French->English)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard NLP datasets used to evaluate zero-shot/few-shot performance on reading comprehension, discrete reasoning, commonsense, and translation where RLHF fine-tuning revealed capability regressions (alignment tax).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SQuADv2 / DROP / HellaSwag / WMT 2015</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Benchmarks: SQuADv2 (reading comprehension with unanswerable questions), DROP (discrete reasoning over passages), HellaSwag (commonsense reasoning), WMT 2015 FR->EN (translation).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A (datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>These are evaluation tasks probing comprehension, discrete numeric reasoning, commonsense, and translation — used to detect performance regressions introduced by alignment fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>SQuADv2; DROP; HellaSwag; WMT 2015 French->English</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Public NLP datasets measuring reading comprehension, discrete reasoning, commonsense physical/world knowledge, and translation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Reading comprehension (SQuADv2), discrete numeric reasoning (DROP), commonsense continuation/selection (HellaSwag), machine translation (WMT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task-specific metrics (e.g., exact match/F1 for QA, accuracy for HellaSwag, BLEU for WMT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports performance regressions for PPO models vs GPT-3 on SQuAD, DROP, HellaSwag, and WMT translation; mixing pretraining gradients in PPO-ptx mitigates many regressions and can surpass GPT-3 on HellaSwag, but PPO-ptx still lags on DROP, SQuADv2, and translation (no precise numeric values provided in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>PPO (RLHF) without pretraining mix shows regressions relative to GPT-3; PPO-ptx reduces regressions and can recover or exceed baseline on some datasets but not all.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Alignment fine-tuning (RLHF) can incur an 'alignment tax' — reduced performance on some capability benchmarks — but mixing pretraining gradients into RLHF (PPO-ptx) substantially reduces that tax while preserving human-preference improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper does not fully eliminate regressions with PPO-ptx; mixing pretraining gradients may reintroduce undesirable behaviors present in pretraining data; exact numeric impacts depend on hyperparameters and are reported in appendices rather than main text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training language models to follow instructions with human feedback', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7074.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7074.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RealToxicityPrompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RealToxicityPrompts (Gehman et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Dataset and evaluation protocol for measuring model propensity to generate toxic continuations given prompts of varying toxicity; evaluated both automatically (Perspective API) and via human raters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RealToxicityPrompts</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Benchmarks model toxic generation under different prompt conditions (respectful prompt, no prompt, instruct-to-be-toxic). Evaluation includes Perspective API scores and human labels for toxicity/continuity/preference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>N/A (safety/toxicity evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>RealToxicityPrompts</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Dataset sampling prompts across toxicity spectrum to measure model tendency to produce toxic outputs; includes human and automated evaluation (Perspective API).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Toxicity generation and safety evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Perspective API toxicity scores; human-rated absolute and relative toxicity, continuity, and preference</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>When instructed to be respectful ('respectful prompt'), InstructGPT models generate about 25% fewer toxic outputs than GPT-3 according to the paper; this advantage disappears without the respectful prompt and reverses when explicitly asked to be toxic (InstructGPT can produce more toxic outputs under malicious instruction).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>InstructGPT < GPT-3 toxicity under respectful prompting; similar to GPT-3 without such instructions; SFT baseline was least toxic but less preferred overall.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RLHF can reduce toxicity under cooperative prompts, but does not guarantee robust abstention under adversarial instructions; toxicity behavior depends strongly on prompt framing.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Toxicity evaluations depend on sampling scheme; results varied with prompt selection and prompting style; InstructGPT can be more toxic when explicitly instructed to be so.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training language models to follow instructions with human feedback', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7074.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7074.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reward Model (RM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learned Reward Model for human preferences</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scalar model trained to predict human preferences over model outputs based on ranked comparisons; used as the reward signal for PPO optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reward Model (RM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RM takes a prompt and a completion and outputs a scalar reward; trained on human rankings (K = 4..9 completions per prompt) using a pairwise cross-entropy style loss over all pairs in a batch; normalized so labeler demonstrations have mean 0 before RL.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B (RM models used in paper; 175B RM found unstable and therefore not used)</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer-based scoring model producing scalar reward</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Human comparison dataset (~33k prompts) with K-way rankings producing up to C(K,2) pairwise comparisons per prompt</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Supervised learning to predict human ranking preferences (no symbolic reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Reward model validation accuracy on held-out labeler comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>RM evaluated by accuracy in predicting which of two completions a human labeler prefers; cross-validation across labeler groups tests generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Preference prediction / reward signal for RLHF</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%) at predicting human pairwise preferences</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>RM accuracy: 72.4 ± 0.4% on labelers in its training set; 69.6 ± 0.9% accuracy on held-out labeler group (5-fold cross-validation with 3 seeds reported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>6B RM was chosen because 175B RM training was found unstable; batching all pairwise comparisons from a K-way ranking as single batch elements substantially reduced overfitting and improved validation accuracy versus treating pairs independently.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A learned RM trained on K-way rankings yields a reliable reward signal that generalizes reasonably to held-out labelers and enables PPO fine-tuning to improve human-preference metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>RM training can overfit if pairwise comparisons are treated naively; large (175B) RM training was unstable in experiments; RM inherits biases and limitations of the labelers and comparison data and may not represent broader human preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training language models to follow instructions with human feedback', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Constrained policy optimization <em>(Rating: 2)</em></li>
                <li>An actor-critic algorithm for sequence prediction <em>(Rating: 2)</em></li>
                <li>Learning to understand goal specifications by modelling reward <em>(Rating: 2)</em></li>
                <li>Thinking fast and slow with deep learning and tree search <em>(Rating: 1)</em></li>
                <li>Deep reinforcement learning from human preferences <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7074",
    "paper_id": "paper-d766bffc357127e0dc86dd69561d5aeb520d6f4c",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "InstructGPT (PPO-ptx)",
            "name_full": "InstructGPT fine-tuned with PPO and pretraining-mix (PPO-ptx)",
            "brief_description": "GPT-3 architecture models (1.3B, 6B, 175B) fine-tuned with supervised demonstrations then optimized with reinforcement learning from human feedback (reward model + PPO) while mixing in pretraining gradients to reduce performance regressions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (PPO-ptx)",
            "model_description": "GPT-3 architecture transformer models fine-tuned first with supervised demonstrations (SFT) then with RLHF using a learned reward model and PPO; PPO-ptx mixes pretraining gradients into the PPO objective and applies a per-token KL penalty to the SFT policy.",
            "model_size": "1.3B, 6B, 175B",
            "architecture_type": "Transformer (GPT-3 family) fine-tuned with policy optimization (PPO) and reward model",
            "training_data": "Labeler demonstration dataset (~13k prompts), human comparison/ranking dataset (~33k prompts), additional PPO input prompts (~31k from API); plus a pretraining mix sampled from the original GPT-3 pretraining distribution when using PPO-ptx.",
            "reasoning_method": "No explicit symbolic/logical solver — improvements in reasoning-like behavior arise from supervised instruction data and RLHF (reward model + PPO) that optimizes for labeler preferences (helpful/honest/harmless); PPO-ptx mixes pretraining gradients to preserve upstream capabilities.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "TruthfulQA (primary safety/truth benchmark reported)",
            "benchmark_description": "Dataset designed to test truthfulness of model answers, including adversarial questions where models tend to mimic common human falsehoods; used to rate truthfulness and informativeness.",
            "task_type": "Open-domain question answering / truthfulness evaluation; closed-domain hallucination checks on summarization/closed QA",
            "performance_metric": "Human-evaluated fraction rated 'truthful and informative' (TruthfulQA); hallucination rate on closed-domain tasks; human preference winrate vs baselines",
            "performance_value": "On TruthfulQA: InstructGPT PPO models generate truthful and informative answers about twice as often as GPT-3 (text claim in paper). On closed-domain tasks: hallucination rate ~21% for InstructGPT vs ~41% for GPT-3. Preference: 175B InstructGPT preferred to 175B GPT-3 85 ± 3% of the time and preferred to few-shot 175B GPT-3 71 ± 4% of the time. Toxicity: ~25% fewer toxic outputs than GPT-3 when prompted to be respectful (paper claim).",
            "comparison_with_baseline": "Large and consistent gains in human preference and reduced hallucination vs GPT-3 and GPT-3 few-shot; outperforms FLAN/T0 on the API prompt distribution (InstructGPT preferred ≈78-79% over those models). Some SFT baselines are less toxic but less preferred overall.",
            "key_findings": "Fine-tuning GPT-3 with human demonstrations and RLHF (PPO) substantially improves instruction following, reduces hallucinations on closed-domain tasks, increases truthfulness on TruthfulQA, and reduces toxic outputs under respectful prompts; PPO-ptx (mixing pretraining) mitigates many capability regressions.",
            "limitations": "No explicit formal logical-reasoning module — gains are empirical and due to human fine-tuning. Some improvements do not fully generalize (e.g., 1.3B PPO-ptx sometimes underperforms same-size GPT-3 on TruthfulQA). InstructGPT still makes simple mistakes, assumes false premises, hedges excessively, and exhibits performance regressions on certain public NLP datasets (SQuAD, DROP, translation) unless pretraining mix is added; when explicitly prompted to be toxic it can generate more toxic outputs than GPT-3.",
            "uuid": "e7074.0",
            "source_info": {
                "paper_title": "Training language models to follow instructions with human feedback",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "RLHF",
            "name_full": "Reinforcement Learning from Human Feedback (RLHF)",
            "brief_description": "A training pipeline that uses human preference comparisons to train a reward model, then fine-tunes a policy (language model) to maximize that reward via PPO, with added KL penalty to the supervised baseline and optional pretraining-gradient mixing (PPO-ptx).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RLHF (reward model + PPO)",
            "model_description": "Method: collect human demonstrations (SFT), collect human rankings of model outputs to train a reward model (RM), then run PPO to optimize the language model policy against the RM with a KL penalty to the SFT policy; optional term mixes pretraining log-likelihood gradients (PPO-ptx).",
            "model_size": "applied to GPT-3 sizes (1.3B, 6B, 175B); RM used 6B in experiments",
            "architecture_type": "Transformer policy with learned scalar reward model; PPO policy gradient optimization",
            "training_data": "Human demonstration dataset (~13k), human comparison dataset (~33k), PPO unlabeled prompts (~31k); RM trained on the K-way ranked outputs (K=4..9) producing pairwise labels.",
            "reasoning_method": "Preference optimization for alignment; not a symbolic reasoning method — uses human preferences to shape outputs, which can improve truthfulness/constraint following and reduce hallucination.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Reward model accuracy and downstream human-preference metrics; evaluated on TruthfulQA, RealToxicityPrompts, SQuAD, DROP, HellaSwag, WMT translation, Winogender, CrowS-Pairs",
            "benchmark_description": "RM is evaluated by its accuracy at predicting human comparison choices; models optimized with RLHF are evaluated by human preference winrates and by standard public NLP/safety benchmarks.",
            "task_type": "Training/optimization method applied to instruction following, truthfulness, toxicity reduction, QA, summarization and other NLP tasks",
            "performance_metric": "RM accuracy (%) on held-out comparisons; human preference winrate; task-specific metrics on public benchmarks",
            "performance_value": "Reward model accuracy: 72.4 ± 0.4% on training-set labelers, 69.6 ± 0.9% on held-out labelers (5-fold experiment). RLHF yields large human preference improvements (e.g., 175B InstructGPT preferred vs GPT-3 85 ± 3%).",
            "comparison_with_baseline": "RLHF (PPO) improves human preference and truthfulness metrics over SFT and GPT-3; mixing pretraining gradients (PPO-ptx) reduces performance regressions vs baseline compared to PPO without pretraining mix.",
            "key_findings": "RLHF is an effective, cost-efficient method to align language models to human preferences across diverse instruction-following tasks and to improve truthfulness/hallucination behavior without needing explicit symbolic reasoning components.",
            "limitations": "Alignment is to the particular labeler population and their instructions; RM can overfit if comparisons are treated naively (mitigated by batching all pairwise comparisons); 175B RM training was found unstable so RM experiments used 6B; RLHF can introduce an 'alignment tax' (capability regressions) on public NLP tasks unless pretraining gradients are mixed in; method does not guarantee formal logical correctness or general deductive reasoning.",
            "uuid": "e7074.1",
            "source_info": {
                "paper_title": "Training language models to follow instructions with human feedback",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "SFT",
            "name_full": "Supervised Fine-Tuning (SFT) on human demonstrations",
            "brief_description": "Fine-tuning pretrained GPT-3 on human-written demonstration outputs for instruction-like prompts, used as a baseline and as the initialization policy for RLHF.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SFT (supervised fine-tune of GPT-3)",
            "model_description": "GPT-3 models fine-tuned for 16 epochs on a dataset of labeler demonstrations (~13k prompts) with cosine learning-rate decay and dropout; SFT models often overfit validation loss but continued training improved RM score and human preference.",
            "model_size": "1.3B, 6B, 175B (same GPT-3 architecture)",
            "architecture_type": "Transformer (GPT-3) with supervised cross-entropy fine-tuning",
            "training_data": "Labeler demonstrations dataset (~13k examples) drawn from API prompts and labeler-written prompts",
            "reasoning_method": "Learning from human demonstration examples (behavior cloning) to follow instructions; no explicit stepwise logical reasoning technique applied.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Human preference tests on API prompt distribution and public NLP datasets (same as for RLHF)",
            "benchmark_description": "SFT serves as a baseline for preference ratings and downstream tasks; human evaluators rate outputs for overall quality, instruction-following, hallucination, etc.",
            "task_type": "Instruction following across varied tasks (generation, QA, summarization, code, multilingual prompts)",
            "performance_metric": "Human preference winrate and RM validation score; toxicity measured via RealToxicityPrompts; other dataset metrics",
            "performance_value": "SFT improves over raw GPT-3 and few-shot prompting in human preference; SFT baseline was less toxic than other variants but also less preferred and lower continuity according to human ratings (no single numeric delta beyond paper claims).",
            "comparison_with_baseline": "SFT improves over vanilla GPT-3 but RLHF (PPO) further improves human preference and instruction-following reliability.",
            "key_findings": "SFT provides a strong initialization for RLHF and yields better instruction following than un-finetuned GPT-3, but RLHF further boosts human-preferred behavior.",
            "limitations": "SFT can overfit on validation loss, and by itself does not achieve the highest human preference scores; SFT-only models may produce short/degenerate responses that lower continuity despite low toxicity.",
            "uuid": "e7074.2",
            "source_info": {
                "paper_title": "Training language models to follow instructions with human feedback",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "GPT-3 (baseline)",
            "name_full": "GPT-3 (Brown et al., 2020) baseline models",
            "brief_description": "Pretrained transformer language models used as baselines: raw GPT-3, GPT-3 prompted (few-shot prefix), and GPT-3 fine-tuned variants (FLAN, T0) for comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_description": "Autoregressive transformer language models pretrained on broad internet data (GPT-3 family); evaluated as raw few-shot prompted models and as fine-tuned baselines.",
            "model_size": "175B (primary baseline), also smaller sizes (1.3B, 6B) referenced",
            "architecture_type": "Autoregressive Transformer (decoder-only)",
            "training_data": "Large-scale web and curated corpora used in GPT-3 pretraining (not detailed in this paper); few-shot prompting uses in-context examples.",
            "reasoning_method": "No dedicated logical reasoning method applied; reasoning-like behavior achieved via few-shot prompting or fine-tuning (FLAN/T0) in other works.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Used as baseline across TruthfulQA, RealToxicityPrompts, SQuAD, DROP, HellaSwag, WMT, Winogender, CrowS-Pairs",
            "benchmark_description": "Standard public NLP and safety benchmarks used to compare against InstructGPT and RLHF-trained variants.",
            "task_type": "Baseline for instruction following, QA, summarization, toxicity and bias evaluation",
            "performance_metric": "Human preference winrate, truthfulness/informativeness rate, toxicity scores, benchmark-specific metrics",
            "performance_value": "GPT-3 is substantially outperformed in human preference by InstructGPT (e.g., 175B InstructGPT preferred to 175B GPT-3 85 ± 3%); GPT-3 hallucinates more in closed-domain tasks (~41% vs InstructGPT ~21%); on toxicity, InstructGPT reduces toxic outputs under respectful prompts but raw GPT-3 can behave differently depending on prompt.",
            "comparison_with_baseline": "Used as baseline; InstructGPT and RLHF variants show large improvements in instruction following and truthfulness relative to GPT-3; FLAN/T0 fine-tuned GPT-3 variants perform better than default GPT-3 but worse than InstructGPT on API distribution.",
            "key_findings": "Scaling alone (bigger GPT-3) does not guarantee alignment with user intent; fine-tuning with human feedback produces greater alignment improvements than model-size increases alone.",
            "limitations": "Without targeted fine-tuning or prompting GPT-3 can produce unhelpful, untruthful, or toxic outputs and hallucinates on closed-domain tasks; few-shot prompting helps but does not match RLHF gains.",
            "uuid": "e7074.3",
            "source_info": {
                "paper_title": "Training language models to follow instructions with human feedback",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "TruthfulQA",
            "name_full": "TruthfulQA (dataset for truthfulness evaluation)",
            "brief_description": "Benchmark used to measure whether models produce truthful and informative answers to questions, including adversarially chosen prompts that commonly induce false but stereotypical responses.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "TruthfulQA (evaluation dataset)",
            "model_description": "A set of questions designed to test model truthfulness and tendency to produce human-like falsehoods; evaluated via human raters and instruction prompting (Instruction+QA trick to elicit 'I have no comment').",
            "model_size": "N/A (dataset)",
            "architecture_type": "N/A",
            "training_data": "N/A",
            "reasoning_method": "N/A (benchmark for evaluating truthfulness rather than a reasoning method)",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "TruthfulQA",
            "benchmark_description": "Dataset of questions designed to probe model propensity to produce falsehoods and to evaluate truthfulness and informativeness under different prompting/regimes.",
            "task_type": "Open-domain question answering focused on truthfulness and avoidance of common false answers",
            "performance_metric": "Human-rated truthfulness and informativeness fraction",
            "performance_value": "Paper reports InstructGPT PPO models generate truthful and informative answers about twice as often as GPT-3 overall; 1.3B PPO-ptx is an exception, performing slightly worse than same-size GPT-3. When using an 'Instruction+QA' prompt, PPO models err on the side of refusing to answer falsely.",
            "comparison_with_baseline": "InstructGPT (PPO) &gt; GPT-3 on TruthfulQA by approximately 2x in paper statements; exception at smaller model size.",
            "key_findings": "RLHF-trained models show significant improvements in truthfulness versus GPT-3, and can be prompted to abstain when uncertain; gains persist even on non-adversarial subsets.",
            "limitations": "TruthfulQA captures only a subset of 'honesty'; improvements are empirical and do not imply formal logical correctness; smaller PPO-ptx models may not uniformly improve; automatic TruthfulQA metrics initially overstated gains (noted in acknowledgements).",
            "uuid": "e7074.4",
            "source_info": {
                "paper_title": "Training language models to follow instructions with human feedback",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Public NLP datasets (SQuAD, DROP, HellaSwag, WMT)",
            "name_full": "Selected public NLP evaluation datasets (SQuAD v2, DROP, HellaSwag, WMT 2015 French-&gt;English)",
            "brief_description": "Standard NLP datasets used to evaluate zero-shot/few-shot performance on reading comprehension, discrete reasoning, commonsense, and translation where RLHF fine-tuning revealed capability regressions (alignment tax).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "SQuADv2 / DROP / HellaSwag / WMT 2015",
            "model_description": "Benchmarks: SQuADv2 (reading comprehension with unanswerable questions), DROP (discrete reasoning over passages), HellaSwag (commonsense reasoning), WMT 2015 FR-&gt;EN (translation).",
            "model_size": "N/A (datasets)",
            "architecture_type": "N/A",
            "training_data": "N/A",
            "reasoning_method": "These are evaluation tasks probing comprehension, discrete numeric reasoning, commonsense, and translation — used to detect performance regressions introduced by alignment fine-tuning.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "SQuADv2; DROP; HellaSwag; WMT 2015 French-&gt;English",
            "benchmark_description": "Public NLP datasets measuring reading comprehension, discrete reasoning, commonsense physical/world knowledge, and translation quality.",
            "task_type": "Reading comprehension (SQuADv2), discrete numeric reasoning (DROP), commonsense continuation/selection (HellaSwag), machine translation (WMT)",
            "performance_metric": "Task-specific metrics (e.g., exact match/F1 for QA, accuracy for HellaSwag, BLEU for WMT)",
            "performance_value": "Paper reports performance regressions for PPO models vs GPT-3 on SQuAD, DROP, HellaSwag, and WMT translation; mixing pretraining gradients in PPO-ptx mitigates many regressions and can surpass GPT-3 on HellaSwag, but PPO-ptx still lags on DROP, SQuADv2, and translation (no precise numeric values provided in main text).",
            "comparison_with_baseline": "PPO (RLHF) without pretraining mix shows regressions relative to GPT-3; PPO-ptx reduces regressions and can recover or exceed baseline on some datasets but not all.",
            "key_findings": "Alignment fine-tuning (RLHF) can incur an 'alignment tax' — reduced performance on some capability benchmarks — but mixing pretraining gradients into RLHF (PPO-ptx) substantially reduces that tax while preserving human-preference improvements.",
            "limitations": "Paper does not fully eliminate regressions with PPO-ptx; mixing pretraining gradients may reintroduce undesirable behaviors present in pretraining data; exact numeric impacts depend on hyperparameters and are reported in appendices rather than main text.",
            "uuid": "e7074.5",
            "source_info": {
                "paper_title": "Training language models to follow instructions with human feedback",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "RealToxicityPrompts",
            "name_full": "RealToxicityPrompts (Gehman et al., 2020)",
            "brief_description": "Dataset and evaluation protocol for measuring model propensity to generate toxic continuations given prompts of varying toxicity; evaluated both automatically (Perspective API) and via human raters.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RealToxicityPrompts",
            "model_description": "Benchmarks model toxic generation under different prompt conditions (respectful prompt, no prompt, instruct-to-be-toxic). Evaluation includes Perspective API scores and human labels for toxicity/continuity/preference.",
            "model_size": "N/A (dataset)",
            "architecture_type": "N/A",
            "training_data": "N/A",
            "reasoning_method": "N/A (safety/toxicity evaluation)",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "RealToxicityPrompts",
            "benchmark_description": "Dataset sampling prompts across toxicity spectrum to measure model tendency to produce toxic outputs; includes human and automated evaluation (Perspective API).",
            "task_type": "Toxicity generation and safety evaluation",
            "performance_metric": "Perspective API toxicity scores; human-rated absolute and relative toxicity, continuity, and preference",
            "performance_value": "When instructed to be respectful ('respectful prompt'), InstructGPT models generate about 25% fewer toxic outputs than GPT-3 according to the paper; this advantage disappears without the respectful prompt and reverses when explicitly asked to be toxic (InstructGPT can produce more toxic outputs under malicious instruction).",
            "comparison_with_baseline": "InstructGPT &lt; GPT-3 toxicity under respectful prompting; similar to GPT-3 without such instructions; SFT baseline was least toxic but less preferred overall.",
            "key_findings": "RLHF can reduce toxicity under cooperative prompts, but does not guarantee robust abstention under adversarial instructions; toxicity behavior depends strongly on prompt framing.",
            "limitations": "Toxicity evaluations depend on sampling scheme; results varied with prompt selection and prompting style; InstructGPT can be more toxic when explicitly instructed to be so.",
            "uuid": "e7074.6",
            "source_info": {
                "paper_title": "Training language models to follow instructions with human feedback",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Reward Model (RM)",
            "name_full": "Learned Reward Model for human preferences",
            "brief_description": "A scalar model trained to predict human preferences over model outputs based on ranked comparisons; used as the reward signal for PPO optimization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Reward Model (RM)",
            "model_description": "RM takes a prompt and a completion and outputs a scalar reward; trained on human rankings (K = 4..9 completions per prompt) using a pairwise cross-entropy style loss over all pairs in a batch; normalized so labeler demonstrations have mean 0 before RL.",
            "model_size": "6B (RM models used in paper; 175B RM found unstable and therefore not used)",
            "architecture_type": "Transformer-based scoring model producing scalar reward",
            "training_data": "Human comparison dataset (~33k prompts) with K-way rankings producing up to C(K,2) pairwise comparisons per prompt",
            "reasoning_method": "Supervised learning to predict human ranking preferences (no symbolic reasoning)",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Reward model validation accuracy on held-out labeler comparisons",
            "benchmark_description": "RM evaluated by accuracy in predicting which of two completions a human labeler prefers; cross-validation across labeler groups tests generalization.",
            "task_type": "Preference prediction / reward signal for RLHF",
            "performance_metric": "Accuracy (%) at predicting human pairwise preferences",
            "performance_value": "RM accuracy: 72.4 ± 0.4% on labelers in its training set; 69.6 ± 0.9% accuracy on held-out labeler group (5-fold cross-validation with 3 seeds reported).",
            "comparison_with_baseline": "6B RM was chosen because 175B RM training was found unstable; batching all pairwise comparisons from a K-way ranking as single batch elements substantially reduced overfitting and improved validation accuracy versus treating pairs independently.",
            "key_findings": "A learned RM trained on K-way rankings yields a reliable reward signal that generalizes reasonably to held-out labelers and enables PPO fine-tuning to improve human-preference metrics.",
            "limitations": "RM training can overfit if pairwise comparisons are treated naively; large (175B) RM training was unstable in experiments; RM inherits biases and limitations of the labelers and comparison data and may not represent broader human preferences.",
            "uuid": "e7074.7",
            "source_info": {
                "paper_title": "Training language models to follow instructions with human feedback",
                "publication_date_yy_mm": "2022-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Constrained policy optimization",
            "rating": 2
        },
        {
            "paper_title": "An actor-critic algorithm for sequence prediction",
            "rating": 2
        },
        {
            "paper_title": "Learning to understand goal specifications by modelling reward",
            "rating": 2
        },
        {
            "paper_title": "Thinking fast and slow with deep learning and tree search",
            "rating": 1
        },
        {
            "paper_title": "Deep reinforcement learning from human preferences",
            "rating": 2
        }
    ],
    "cost": 0.020144,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Training language models to follow instructions with human feedback</h1>
<p>Long Ouyang<em> Jeff Wu</em> Xu Jiang<em> Diogo Almeida</em> Carroll L. Wainwright<em><br>Pamela Mishkin</em> Chong Zhang Sandhini Agarwal Katarina Slama Alex Ray<br>John Schulman Jacob Hilton Fraser Kelton Luke Miller Maddie Simens<br>Amanda Askell ${ }^{\dagger}$ Peter Welinder Paul Christiano<em> ${ }^{</em>}$<br>Jan Leike<em> Ryan Lowe</em><br>OpenAI</p>
<h4>Abstract</h4>
<p>Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.</p>
<h2>1 Introduction</h2>
<p>Large language models (LMs) can be "prompted" to perform a range of natural language processing (NLP) tasks, given some examples of the task as input. However, these models often express unintended behaviors such as making up facts, generating biased or toxic text, or simply not following user instructions (Bender et al., 2021; Bommasani et al., 2021; Kenton et al., 2021; Weidinger et al., 2021; Tamkin et al., 2021; Gehman et al., 2020). This is because the language modeling objective</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Human evaluations of various models on our API prompt distribution, evaluated by how often outputs from each model were preferred to those from the 175B SFT model. Our InstructGPT models (PPO-ptx) as well as its variant trained without pretraining mix (PPO) significantly outperform the GPT-3 baselines (GPT, GPT prompted); outputs from our 1.3B PPO-ptx model are preferred to those from the 175B GPT-3. Error bars throughout the paper are 95% confidence intervals.</p>
<p>used for many recent large LMs—predicting the next token on a webpage from the internet—is different from the objective “follow the user’s instructions helpfully and safely” (Radford et al., 2019; Brown et al., 2020; Fedus et al., 2021; Rae et al., 2021; Thoppilan et al., 2022). Thus, we say that the language modeling objective is <em>misaligned</em>. Averting these unintended behaviors is especially important for language models that are deployed and used in hundreds of applications.</p>
<p>We make progress on aligning language models by training them to act in accordance with the user’s intention (Leike et al., 2018). This encompasses both explicit intentions such as following instructions and implicit intentions such as staying truthful, and not being biased, toxic, or otherwise harmful. Using the language of Askell et al. (2021), we want language models to be <em>helpful</em> (they should help the user solve their task), <em>honest</em> (they shouldn’t fabricate information or mislead the user), and <em>harmless</em> (they should not cause physical, psychological, or social harm to people or the environment). We elaborate on the evaluation of these criteria in Section 3.6.</p>
<p>We focus on <em>fine-tuning</em> approaches to aligning language models. Specifically, we use reinforcement learning from human feedback (RLHF; Christiano et al., 2017; Stiennon et al., 2020) to fine-tune GPT-3 to follow a broad class of written instructions (see Figure 2). This technique uses human preferences as a reward signal to fine-tune our models. We first hire a team of 40 contractors to label our data, based on their performance on a screening test (see Section 3.4 and Appendix B.1 for more details). We then collect a dataset of human-written demonstrations of the desired output behavior on (mostly English) prompts submitted to the OpenAI API^{3} and some labeler-written prompts, and use this to train our supervised learning baselines. Next, we collect a dataset of human-labeled comparisons between outputs from our models on a larger set of API prompts. We then train a reward model (RM) on this dataset to predict which model output our labelers would prefer. Finally, we use this RM as a reward function and fine-tune our supervised learning baseline to maximize this reward using the PPO algorithm (Schulman et al., 2017). We illustrate this process in Figure 2. This procedure aligns the behavior of GPT-3 to the stated preferences of a specific group of people (mostly our labelers and researchers), rather than any broader notion of “human values”; we discuss this further in Section 5.2. We call the resulting models <em>InstructGPT</em>.</p>
<p>We mainly evaluate our models by having our labelers rate the quality of model outputs on our test set, consisting of prompts from held-out customers (who are not represented in the training data). We also conduct automatic evaluations on a range of public NLP datasets. We train three model</p>
<p>^{3}Specifically, we train on prompts submitted to earlier versions of the InstructGPT models on the OpenAI API Playground, which were trained only using demonstration data. We filter out prompts containing PII.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A diagram illustrating the three steps of our method: (1) supervised fine-tuning (SFT), (2) reward model (RM) training, and (3) reinforcement learning via proximal policy optimization (PPO) on this reward model. Blue arrows indicate that this data is used to train one of our models. In Step 2, boxes A-D are samples from our models that get ranked by labelers. See Section 3 for more details on our method.
sizes (1.3B, 6B, and 175B parameters), and all of our models use the GPT-3 architecture. Our main findings are as follows:</p>
<p>Labelers significantly prefer InstructGPT outputs over outputs from GPT-3. On our test set, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having over 100x fewer parameters. These models have the same architecture, and differ only by the fact that InstructGPT is fine-tuned on our human data. This result holds true even when we add a few-shot prompt to GPT-3 to make it better at following instructions. Outputs from our 175B InstructGPT are preferred to 175B GPT-3 outputs $85 \pm 3 \%$ of the time, and preferred $71 \pm 4 \%$ of the time to few-shot 175B GPT-3. InstructGPT models also generate more appropriate outputs according to our labelers, and more reliably follow explicit constraints in the instruction.</p>
<p>InstructGPT models show improvements in truthfulness over GPT-3. On the TruthfulQA benchmark, InstructGPT generates truthful and informative answers about twice as often as GPT-3. Our results are equally strong on the subset of questions that were not adversarially selected against GPT-3. On "closed-domain" tasks from our API prompt distribution, where the output should not contain information that is not present in the input (e.g. summarization and closed-domain QA), InstructGPT models make up information not present in the input about half as often as GPT-3 (a $21 \%$ vs. $41 \%$ hallucination rate, respectively).</p>
<p>InstructGPT shows small improvements in toxicity over GPT-3, but not bias. To measure toxicity, we use the RealToxicityPrompts dataset (Gehman et al., 2020) and conduct both automatic and human evaluations. InstructGPT models generate about $25 \%$ fewer toxic outputs than GPT-3 when prompted to be respectful. InstructGPT does not significantly improve over GPT-3 on the Winogender (Rudinger et al., 2018) and CrowSPairs (Nangia et al., 2020) datasets.</p>
<p>We can minimize performance regressions on public NLP datasets by modifying our RLHF fine-tuning procedure. During RLHF fine-tuning, we observe performance regressions compared to GPT-3 on certain public NLP datasets, notably SQuAD (Rajpurkar et al., 2018), DROP (Dua et al., 2019), HellaSwag (Zellers et al., 2019), and WMT 2015 French to English translation (Bojar et al., 2015). This is an example of an "alignment tax" since our alignment procedure comes at the cost of</p>
<p>lower performance on certain tasks that we may care about. We can greatly reduce the performance regressions on these datasets by mixing PPO updates with updates that increase the log likelihood of the pretraining distribution (PPO-ptx), without compromising labeler preference scores.</p>
<p>Our models generalize to the preferences of "held-out" labelers that did not produce any training data. To test the generalization of our models, we conduct a preliminary experiment with held-out labelers, and find that they prefer InstructGPT outputs to outputs from GPT-3 at about the same rate as our training labelers. However, more work is needed to study how these models perform on broader groups of users, and how they perform on inputs where humans disagree about the desired behavior.</p>
<p>Public NLP datasets are not reflective of how our language models are used. We compare GPT-3 fine-tuned on our human preference data (i.e. InstructGPT) to GPT-3 fine-tuned on two different compilations of public NLP tasks: the FLAN (Wei et al., 2021) and T0 (Sanh et al., 2021) (in particular, the T0++ variant). These datasets consist of a variety of NLP tasks, combined with natural language instructions for each task. On our API prompt distribution, our FLAN and T0 models perform slightly worse than our SFT baseline, and labelers significantly prefer InstructGPT to these models (InstructGPT has a $73.4 \pm 2 \%$ winrate vs. our baseline, compared to $26.8 \pm 2 \%$ and $29.8 \pm 2 \%$ for our version of T0 and FLAN, respectively).</p>
<p>InstructGPT models show promising generalization to instructions outside of the RLHF finetuning distribution. We qualitatively probe InstructGPT's capabilities, and find that it is able to follow instructions for summarizing code, answer questions about code, and sometimes follows instructions in different languages, despite these instructions being very rare in the fine-tuning distribution. In contrast, GPT-3 can perform these tasks but requires more careful prompting, and does not usually follow instructions in these domains. This result is exciting because it suggests that our models are able to generalize the notion of "following instructions." They retain some alignment even on tasks for which they get very little direct supervision signal.</p>
<p>InstructGPT still makes simple mistakes. For example, InstructGPT can still fail to follow instructions, make up facts, give long hedging answers to simple questions, or fail to detect instructions with false premises.</p>
<p>Overall, our results indicate that fine-tuning large language models using human preferences significantly improves their behavior on a wide range of tasks, though much work remains to be done to improve their safety and reliability.</p>
<p>The rest of this paper is structured as follows: We first detail related work in Section 2, before diving into our method and experiment details in Section 3, including our high-level methodology (3.1), task and dataset details (3.3 and 3.2), human data collection (3.4), how we trained our models (3.5), and our evaluation procedure (3.6). We then present our results in Section 4, divided into three parts: results on the API prompt distribution (4.1), results on public NLP datasets (4.2), and qualitative results (4.3). Finally we give an extended discussion of our work in Section 5, including implications for alignment research (5.1), what we are aligning to (5.2), limitations (5.3), open questions (5.4), and broader impacts of this work (5.5).</p>
<h1>2 Related work</h1>
<p>Research on alignment and learning from human feedback. We build on previous techniques to align models with human intentions, particularly reinforcement learning from human feedback (RLHF). Originally developed for training simple robots in simulated environments and Atari games (Christiano et al., 2017; Ibarz et al., 2018), it has recently been applied to fine-tuning language models to summarize text (Ziegler et al., 2019; Stiennon et al., 2020; Böhm et al., 2019; Wu et al., 2021). This work is in turn influenced by similar work using human feedback as a reward in domains such as dialogue (Jaques et al., 2019; Yi et al., 2019; Hancock et al., 2019), translation (Kreutzer et al., 2018; Bahdanau et al., 2016), semantic parsing (Lawrence and Riezler, 2018), story generation (Zhou and Xu, 2020), review generation (Cho et al., 2018), and evidence extraction (Perez et al., 2019). Madaan et al. (2022) use written human feedback to augment prompts and improve the performance of GPT-3. There has also been work on aligning agents in text-based environments using RL with</p>
<p>a normative prior (Nahian et al., 2021). Our work can be seen as a direct application of RLHF to aligning language models on a broad distribution of language tasks.
The question of what it means for language models to be aligned has also received attention recently (Gabriel, 2020). Kenton et al. (2021) catalog behavioral issues in LMs that result from misalignment, including producing harmful content and gaming misspecified objectives. In concurrent work, Askell et al. (2021) propose language assistants as a testbed for alignment research, study some simple baselines, and their scaling properties.</p>
<p>Training language models to follow instructions. Our work is also related to research on crosstask generalization in language models, where LMs are fine-tuned on a broad range of public NLP datasets (usually prefixed with an appropriate instruction) and evaluated on a different set of NLP tasks. There has been a range of work in this domain (Yi et al., 2019; Mishra et al., 2021; Wei et al., 2021; Khashabi et al., 2020; Sanh et al., 2021; Aribandi et al., 2021), which differ in training and evaluation data, formatting of instructions, size of pretrained models, and other experimental details. A consistent finding across studies is that fine-tuning LMs on a range of NLP tasks, with instructions, improves their downstream performance on held-out tasks, both in the zero-shot and few-shot settings.
There is also a related line of work on instruction following for navigation, where models are trained to follow natural language instructions to navigate in a simulated environment (Bahdanau et al., 2018; Abramson et al., 2020; Zhao et al., 2021).</p>
<p>Evaluating the harms of language models. A goal of modifying the behavior of language models is to mitigate the harms of these models when they're deployed in the real world. These risks have been extensively documented (Bender et al., 2021; Bommasani et al., 2021; Kenton et al., 2021; Weidinger et al., 2021; Tamkin et al., 2021). Language models can produce biased outputs (Dhamala et al., 2021; Liang et al., 2021; Manela et al., 2021; Caliskan et al., 2017; Kirk et al., 2021), leak private data (Carlini et al., 2021), generate misinformation (Solaiman et al., 2019; Buchanan et al., 2021), and be used maliciously; for a thorough review we direct the reader to Weidinger et al. (2021). Deploying language models in specific domains gives rise to new risks and challenges, for example in dialog systems (Henderson et al., 2018; Xu et al., 2020; Dinan et al., 2019b). There is a nascent but growing field that aims to build benchmarks to concretely evaluate these harms, particularly around toxicity (Gehman et al., 2020), stereotypes (Nadeem et al., 2020), and social bias (Dhamala et al., 2021; Nangia et al., 2020; Rudinger et al., 2018). Making significant progress on these problems is hard since well-intentioned interventions on LM behavior can have side-effects (Welbl et al., 2021; Blodgett et al., 2020); for instance, efforts to reduce the toxicity of LMs can reduce their ability to model text from under-represented groups, due to prejudicial correlations in the training data ( Xu et al., 2021).</p>
<p>Modifying the behavior of language models to mitigate harms. There are many ways to change the generation behavior of language models. Solaiman and Dennison (2021) fine-tune LMs on a small, value-targeted dataset, which improves the models' ability to adhere to these values on a question answering task. Ngo et al. (2021) filter the pretraining dataset by removing documents on which a language model has a high conditional likelihood of generating a set of researcher-written trigger phrases. When trained on this filtered dataset, their LMs generate less harmful text, at the cost of a slight decrease in language modeling performance. Xu et al. (2020) use a variety of approaches to improve the safety of chatbots, including data filtering, blocking certain words or n-grams during generation, safety-specific control tokens (Keskar et al., 2019; Dinan et al., 2019a), and human-in-theloop data collection (Dinan et al., 2019b). Other approaches for mitigating the generated bias by LMs use word embedding regularization (Liu et al., 2019; Huang et al., 2019), data augmentation (Liu et al., 2019; Dinan et al., 2019a; Sheng et al., 2019), null space projection to make the distribution over sensitive tokens more uniform (Liang et al., 2021), different objective functions (Qian et al., 2019), or causal mediation analysis (Vig et al., 2020). There is also work on steering the generation of language models using a second (usually smaller) language model (Dathathri et al., 2019; Krause et al., 2020), and variants of this idea have been applied to reducing language model toxicity (Schick et al., 2021).</p>
<p>Table 1: Distribution of use case categories from our API prompt dataset.</p>
<table>
<thead>
<tr>
<th>Use-case</th>
<th>(\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Generation</td>
<td>$45.6 \%$</td>
</tr>
<tr>
<td>Open QA</td>
<td>$12.4 \%$</td>
</tr>
<tr>
<td>Brainstorming</td>
<td>$11.2 \%$</td>
</tr>
<tr>
<td>Chat</td>
<td>$8.4 \%$</td>
</tr>
<tr>
<td>Rewrite</td>
<td>$6.6 \%$</td>
</tr>
<tr>
<td>Summarization</td>
<td>$4.2 \%$</td>
</tr>
<tr>
<td>Classification</td>
<td>$3.5 \%$</td>
</tr>
<tr>
<td>Other</td>
<td>$3.5 \%$</td>
</tr>
<tr>
<td>Closed QA</td>
<td>$2.6 \%$</td>
</tr>
<tr>
<td>Extract</td>
<td>$1.9 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: Illustrative prompts from our API prompt dataset. These are fictional examples inspired by real usage-see more examples in Appendix A.2.1.</p>
<table>
<thead>
<tr>
<th>Use-case</th>
<th>Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td>Brainstorming</td>
<td>List five ideas for how to regain enthusiasm for my <br> career</td>
</tr>
<tr>
<td>Generation</td>
<td>Write a short story where a bear goes to the beach, <br> makes friends with a seal, and then returns home.</td>
</tr>
<tr>
<td>Rewrite</td>
<td>This is the summary of a Broadway play: <br> "" <br> [summary] <br> "" <br> This is the outline of the commercial for that play: <br> ""</td>
</tr>
</tbody>
</table>
<h1>3 Methods and experimental details</h1>
<h3>3.1 High-level methodology</h3>
<p>Our methodology follows that of Ziegler et al. (2019) and Stiennon et al. (2020), who applied it in the stylistic continuation and summarization domains. We start with a pretrained language model (Radford et al., 2019; Brown et al., 2020; Fedus et al., 2021; Rae et al., 2021; Thoppilan et al., 2022), a distribution of prompts on which we want our model to produce aligned outputs, and a team of trained human labelers (see Sections 3.4 for details). We then apply the following three steps (Figure 2).</p>
<p>Step 1: Collect demonstration data, and train a supervised policy. Our labelers provide demonstrations of the desired behavior on the input prompt distribution (see Section 3.2 for details on this distribution). We then fine-tune a pretrained GPT-3 model on this data using supervised learning.</p>
<p>Step 2: Collect comparison data, and train a reward model. We collect a dataset of comparisons between model outputs, where labelers indicate which output they prefer for a given input. We then train a reward model to predict the human-preferred output.</p>
<p>Step 3: Optimize a policy against the reward model using PPO. We use the output of the RM as a scalar reward. We fine-tune the supervised policy to optimize this reward using the PPO algorithm (Schulman et al., 2017).
Steps 2 and 3 can be iterated continuously; more comparison data is collected on the current best policy, which is used to train a new RM and then a new policy. In practice, most of our comparison data comes from our supervised policies, with some coming from our PPO policies.</p>
<h3>3.2 Dataset</h3>
<p>Our prompt dataset consists primarily of text prompts submitted to the OpenAI API, specifically those using an earlier version of the InstructGPT models (trained via supervised learning on a subset of our demonstration data) on the Playground interface. ${ }^{4}$ Customers using the Playground were informed that their data could be used to train further models via a recurring notification any time InstructGPT models were used. In this paper we do not use data from customers using the API in production. We heuristically deduplicate prompts by checking for prompts that share a long common prefix, and we limit the number of prompts to 200 per user ID. We also create our train, validation, and test splits based on user ID, so that the validation and test sets contain no data from users whose data is in the training set. To avoid the models learning potentially sensitive customer details, we filter all prompts in the training split for personally identifiable information (PII).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>To train the very first InstructGPT models, we asked labelers to write prompts themselves. This is because we needed an initial source of instruction-like prompts to bootstrap the process, and these kinds of prompts weren't often submitted to the regular GPT-3 models on the API. We asked labelers to write three kinds of prompts:</p>
<ul>
<li>Plain: We simply ask the labelers to come up with an arbitrary task, while ensuring the tasks had sufficient diversity.</li>
<li>Few-shot: We ask the labelers to come up with an instruction, and multiple query/response pairs for that instruction.</li>
<li>User-based: We had a number of use-cases stated in waitlist applications to the OpenAI API. We asked labelers to come up with prompts corresponding to these use cases.</li>
</ul>
<p>From these prompts, we produce three different datasets used in our fine-tuning procedure: (1) our SFT dataset, with labeler demonstrations used to train our SFT models, (2) our RM dataset, with labeler rankings of model outputs used to train our RMs, and (3) our PPO dataset, without any human labels, which are used as inputs for RLHF fine-tuning. The SFT dataset contains about 13k training prompts (from the API and labeler-written), the RM dataset has 33k training prompts (from the API and labeler-written), and the PPO dataset has 31k training prompts (only from the API). More details on dataset sizes are provided in Table 6.
To give a sense of the composition of our dataset, in Table 1 we show the distribution of use-case categories for our API prompts (specifically the RM dataset) as labeled by our contractors. Most of the use-cases have are generative, rather than classification or QA. We also show some illustrative prompts (written by researchers to mimic the kinds of prompts submitted to InstructGPT models) in Table 2; more prompts submitted to InstructGPT models are shown in Appendix A.2.1, and prompts submitted to GPT-3 models are shown in Appendix A.2.2. We provide more details about our dataset in Appendix A.</p>
<h1>3.3 Tasks</h1>
<p>Our training tasks are from two sources: (1) a dataset of prompts written by our labelers and (2) a dataset of prompts submitted to early InstructGPT models on our API (see Table 6). These prompts are very diverse and include generation, question answering, dialog, summarization, extractions, and other natural language tasks (see Table 1). Our dataset is over 96\% English, however in Section 4.3 we also probe our model's ability to respond to instructions in other languages and complete coding tasks.</p>
<p>For each natural language prompt, the task is most often specified directly through a natural language instruction (e.g. "Write a story about a wise frog"), but could also be indirectly through either few-shot examples (e.g. giving two examples of frog stories, and prompting the model to generate a new one) or implicit continuation (e.g. providing the start of a story about a frog). In each case, we ask our labelers to do their best to infer the intent of the user who wrote the prompt, and ask them to skip inputs where the task is very unclear. Moreover, our labelers also take into account the implicit intentions such as truthfulness of the response, and potentially harmful outputs such as biased or toxic language, guided by the instructions we provide them (see Appendix B) and their best judgment.</p>
<h3>3.4 Human data collection</h3>
<p>To produce our demonstration and comparison data, and to conduct our main evaluations, we hired a team of about 40 contractors on Upwork and through ScaleAI. Compared to earlier work that collects human preference data on the task of summarization (Ziegler et al., 2019; Stiennon et al., 2020; Wu et al., 2021), our inputs span a much broader range of tasks, and can occasionally include controversial and sensitive topics. Our aim was to select a group of labelers who were sensitive to the preferences of different demographic groups, and who were good at identifying outputs that were potentially harmful. Thus, we conducted a screening test designed to measure labeler performance on these axes. We selected labelers who performed well on this test; for more information about our selection procedure and labeler demographics, see Appendix B.1.
During training and evaluation, our alignment criteria may come into conflict: for example, when a user requests a potentially harmful response. During training we prioritize helpfulness to the user (not</p>
<p>doing so requires making some difficult design decisions that we leave to future work; see Section 5.4 for more discussion). However, in our final evaluations we asked labelers prioritize truthfulness and harmlessness (since this is what we really care about).</p>
<p>As in Stiennon et al. (2020), we collaborate closely with labelers over the course of the project. We have an onboarding process to train labelers on the project, write detailed instructions for each task (see Appendix B.2), and answer labeler questions in a shared chat room.</p>
<p>As an initial study to see how well our model generalizes to the preferences of other labelers, we hire a separate set of labelers who do not produce any of the training data. These labelers are sourced from the same vendors, but do not undergo a screening test.</p>
<p>Despite the complexity of the task, we find that inter-annotator agreement rates are quite high: training labelers agree with each-other $72.6 \pm 1.5 \%$ of the time, while for held-out labelers this number is $77.3 \pm 1.3 \%$. For comparison, in the summarization work of Stiennon et al. (2020) researcher-researcher agreement was $73 \pm 4 \%$.</p>
<h1>3.5 Models</h1>
<p>We start with the GPT-3 pretrained language models from Brown et al. (2020). These models are trained on a broad distribution of Internet data and are adaptable to a wide range of downstream tasks, but have poorly characterized behavior. Starting from these models, we then train models with three different techniques:</p>
<p>Supervised fine-tuning (SFT). We fine-tune GPT-3 on our labeler demonstrations using supervised learning. We trained for 16 epochs, using a cosine learning rate decay, and residual dropout of 0.2 . We do our final SFT model selection based on the RM score on the validation set. Similarly to Wu et al. (2021), we find that our SFT models overfit on validation loss after 1 epoch; however, we find that training for more epochs helps both the RM score and human preference ratings, despite this overfitting.</p>
<p>Reward modeling (RM). Starting from the SFT model with the final unembedding layer removed, we trained a model to take in a prompt and response, and output a scalar reward. In this paper we only use 6B RMs, as this saves a lot of compute, and we found that 175B RM training could be unstable and thus was less suitable to be used as the value function during RL (see Appendix C for more details).</p>
<p>In Stiennon et al. (2020), the RM is trained on a dataset of comparisons between two model outputs on the same input. They use a cross-entropy loss, with the comparisons as labels-the difference in rewards represents the log odds that one response will be preferred to the other by a human labeler.
In order to speed up comparison collection, we present labelers with anywhere between $K=4$ and $K=9$ responses to rank. This produces $\binom{K}{2}$ comparisons for each prompt shown to a labeler. Since comparisons are very correlated within each labeling task, we found that if we simply shuffle the comparisons into one dataset, a single pass over the dataset caused the reward model to overfit. ${ }^{3}$ Instead, we train on all $\binom{K}{2}$ comparisons from each prompt as a single batch element. This is much more computationally efficient because it only requires a single forward pass of the RM for each completion (rather than $\binom{K}{2}$ forward passes for $K$ completions) and, because it no longer overfits, it achieves much improved validation accuracy and log loss.</p>
<p>Specifically, the loss function for the reward model is:</p>
<p>$$
\operatorname{loss}(\theta)=-\frac{1}{\binom{K}{2}} E_{\left(x, y_{w}, y_{l}\right) \sim D}\left[\log \left(\sigma\left(r_{\theta}\left(x, y_{w}\right)-r_{\theta}\left(x, y_{l}\right)\right)\right)\right]
$$</p>
<p>where $r_{\theta}(x, y)$ is the scalar output of the reward model for prompt $x$ and completion $y$ with parameters $\theta, y_{w}$ is the preferred completion out of the pair of $y_{w}$ and $y_{l}$, and $D$ is the dataset of human comparisons.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 3: Labeler-collected metadata on the API distribution.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metadata</th>
<th style="text-align: center;">Scale</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Overall quality</td>
<td style="text-align: center;">Likert scale; 1-7</td>
</tr>
<tr>
<td style="text-align: left;">Fails to follow the correct instruction / task</td>
<td style="text-align: center;">Binary</td>
</tr>
<tr>
<td style="text-align: left;">Inappropriate for customer assistant</td>
<td style="text-align: center;">Binary</td>
</tr>
<tr>
<td style="text-align: left;">Hallucination</td>
<td style="text-align: center;">Binary</td>
</tr>
<tr>
<td style="text-align: left;">Satisifies constraint provided in the instruction</td>
<td style="text-align: center;">Binary</td>
</tr>
<tr>
<td style="text-align: left;">Contains sexual content</td>
<td style="text-align: center;">Binary</td>
</tr>
<tr>
<td style="text-align: left;">Contains violent content</td>
<td style="text-align: center;">Binary</td>
</tr>
<tr>
<td style="text-align: left;">Encourages or fails to discourage violence/abuse/terrorism/self-harm</td>
<td style="text-align: center;">Binary</td>
</tr>
<tr>
<td style="text-align: left;">Denigrates a protected class</td>
<td style="text-align: center;">Binary</td>
</tr>
<tr>
<td style="text-align: left;">Gives harmful advice</td>
<td style="text-align: center;">Binary</td>
</tr>
<tr>
<td style="text-align: left;">Expresses opinion</td>
<td style="text-align: center;">Binary</td>
</tr>
<tr>
<td style="text-align: left;">Expresses moral judgment</td>
<td style="text-align: center;">Binary</td>
</tr>
</tbody>
</table>
<p>Finally, since the RM loss is invariant to shifts in reward, we normalize the reward model using a bias so that the labeler demonstrations achieve a mean score of 0 before doing RL.</p>
<p>Reinforcement learning (RL). Once again following Stiennon et al. (2020), we fine-tuned the SFT model on our environment using PPO (Schulman et al., 2017). The environment is a bandit environment which presents a random customer prompt and expects a response to the prompt. Given the prompt and response, it produces a reward determined by the reward model and ends the episode. In addition, we add a per-token KL penalty from the SFT model at each token to mitigate overoptimization of the reward model. The value function is initialized from the RM. We call these models "PPO."</p>
<p>We also experiment with mixing the pretraining gradients into the PPO gradients, in order to fix the performance regressions on public NLP datasets. We call these models "PPO-ptx." We maximize the following combined objective function in RL training:</p>
<p>$$
\begin{aligned}
\text { objective }(\phi)= &amp; E_{(x, y) \sim D_{\pi_{\phi}^{\mathrm{RL}}}}[r_{\theta}(x, y)-\beta \log \left(\pi_{\phi}^{\mathrm{RL}}(y \mid x) / \pi^{\mathrm{SFT}}(y \mid x)\right)]+ \
&amp; \gamma E_{x \sim D_{\text {pretrain }}}\left[\log \left(\pi_{\phi}^{\mathrm{RL}}(x)\right)\right]
\end{aligned}
$$</p>
<p>where $\pi_{\phi}^{\mathrm{RL}}$ is the learned RL policy, $\pi^{\mathrm{SFT}}$ is the supervised trained model, and $D_{\text {pretrain }}$ is the pretraining distribution. The KL reward coefficient, $\beta$, and the pretraining loss coefficient, $\gamma$, control the strength of the KL penalty and pretraining gradients respectively. For "PPO" models, $\gamma$ is set to 0 . Unless otherwise specified, in this paper InstructGPT refers to the PPO-ptx models.</p>
<p>Baselines. We compare the performance of our PPO models to our SFT models and GPT-3. We also compare to GPT-3 when it is provided a few-shot prefix to 'prompt' it into an instruction-following mode (GPT-3-prompted). This prefix is prepended to the user-specified instruction. ${ }^{6}$
We additionally compare InstructGPT to fine-tuning 175B GPT-3 on the FLAN (Wei et al., 2021) and T0 (Sanh et al., 2021) datasets, which both consist of a variety of NLP tasks, combined with natural language instructions for each task (the datasets differ in the NLP datasets included, and the style of instructions used). We fine-tune them on approximately 1 million examples respectively and choose the checkpoint which obtains the highest reward model score on the validation set. See Appendix C for more training details.</p>
<h1>3.6 Evaluation</h1>
<p>To evaluate how "aligned" our models are, we first need to clarify what alignment means in this context. The definition of alignment has historically been a vague and confusing topic, with various</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>competing proposals (Chen et al., 2021; Leike et al., 2018; Gabriel, 2020). Following Leike et al. (2018), our aim is to train models that act in accordance with user intentions. More practically, for the purpose of our language tasks, we use a framework similar to Askell et al. (2021), who define models to be aligned if they are helpful, honest, and harmless.
To be helpful, the model should follow instructions, but also infer intention from a few-shot prompt or another interpretable pattern such as "Q: {question} $\backslash n A$ :". Since a given prompt's intention can be unclear or ambiguous, we rely on judgment from our labelers, and our main metric is labeler preference ratings. However, since our labelers are not the users who generated the prompts, there could be a divergence between what a user actually intended and what the labeler thought was intended from only reading the prompt.
It is unclear how to measure honesty in purely generative models; this requires comparing the model's actual output to its "belief" about the correct output, and since the model is a big black box, we can't infer its beliefs. Instead, we measure truthfulness-whether the model's statements about the world are true-using two metrics: (1) evaluating our model's tendency to make up information on closed domain tasks ("hallucinations"), and (2) using the TruthfulQA dataset (Lin et al., 2021). Needless to say, this only captures a small part of what is actually meant by truthfulness.
Similarly to honesty, measuring the harms of language models also poses many challenges. In most cases, the harms from language models depend on how their outputs are used in the real world. For instance, a model generating toxic outputs could be harmful in the context of a deployed chatbot, but might even be helpful if used for data augmentation to train a more accurate toxicity detection model. Earlier in the project, we had labelers evaluate whether an output was 'potentially harmful'. However, we discontinued this as it required too much speculation about how the outputs would ultimately be used; especially since our data also comes from customers who interact with the Playground API interface (rather than from production use cases).
Therefore we use a suite of more specific proxy criteria that aim to capture different aspects of behavior in a deployed model that could end up being harmful: we have labelers evaluate whether an output is inappropriate in the context of a customer assistant, denigrates a protected class, or contains sexual or violent content. We also benchmark our model on datasets intended to measure bias and toxicity, such as RealToxicityPrompts (Gehman et al., 2020) and CrowS-Pairs (Nangia et al., 2020).
To summarize, we can divide our quantitative evaluations into two separate parts:
Evaluations on API distribution. Our main metric is human preference ratings on a held out set of prompts from the same source as our training distribution. When using prompts from the API for evaluation, we only select prompts by customers we haven't included in training. However, given that our training prompts are designed to be used with InstructGPT models, it's likely that they disadvantage the GPT-3 baselines. Thus, we also evaluate on prompts submitted to GPT-3 models on the API; these prompts are generally not in an 'instruction following' style, but are designed specifically for GPT-3. In both cases, for each model we calculate how often its outputs are preferred to a baseline policy; we choose our 175B SFT model as the baseline since its performance is near the middle of the pack. Additionally, we ask labelers to judge the overall quality of each response on a 1-7 Likert scale and collect a range of metadata for each model output (see Table 3).</p>
<p>Evaluations on public NLP datasets. We evaluate on two types of public datasets: those that capture an aspect of language model safety, particularly truthfulness, toxicity, and bias, and those that capture zero-shot performance on traditional NLP tasks like question answering, reading comprehension, and summarization. We also conduct human evaluations of toxicity on the RealToxicityPrompts dataset (Gehman et al., 2020). We are releasing samples from our models on all of the sampling-based NLP tasks. ${ }^{7}$</p>
<h1>4 Results</h1>
<p>In this section, we provide experimental evidence for our claims in Section 1, sorted into three parts: results on the API prompt distribution, results on public NLP datasets, and qualitative results.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Preference results of our models, measured by winrate against the 175B SFT model. Left: results on prompts submitted to GPT models on the API; Right: results on prompts submitted to InstructGPT models on the API; Top: results from held-out labelers; Bottom: results from training labelers. We omit GPT (prompted) from the evals on prompts submitted to GPT-3 models (left) as these prompts are already designed to perform well for GPT-3, as opposed to prompts submitted to InstructGPT models (right).</p>
<h1>4.1 Results on the API distribution</h1>
<p>Labelers significantly prefer InstructGPT outputs over outputs from GPT-3. On our test set of prompts, our labelers significantly prefer InstructGPT outputs across model sizes. These results are shown in Figure 1. We find that GPT-3 outputs perform the worst, and one can obtain significant step-size improvements by using a well-crafted few-shot prompt (GPT-3 (prompted)), then by training on demonstrations using supervised learning (SFT), and finally by training on comparison data using PPO. Adding updates on the pretraining mix during PPO does not lead to large changes in labeler preference. To illustrate the magnitude of our gains: when compared directly, 175B InstructGPT outputs are preferred to GPT-3 outputs $85 \pm 3 \%$ of the time, and preferred $71 \pm 4 \%$ of the time to few-shot GPT-3.</p>
<p>We also found that our results do not change significantly when evaluated on prompts submitted to GPT-3 models on the API (see Figure 3), though our PPO-ptx models perform slightly worse at larger model sizes.</p>
<p>In Figure 4 we show that labelers also rate InstructGPT outputs favorably along several more concrete axes. Specifically, compared to GPT-3, InstructGPT outputs are more appropriate in the context of a customer assistant, more often follow explicit constraints defined in the instruction (e.g. "Write your answer in 2 paragraphs or less."), are less likely to fail to follow the correct instruction entirely, and make up facts ('hallucinate') less often in closed-domain tasks. These results suggest that InstructGPT models are more reliable and easier to control than GPT-3. We've found that our other metadata</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Metadata results on the API distribution. Note that, due to dataset sizes, these results are collapsed across model sizes. See Appendix E. 2 for analysis that includes model size. Compared to GPT-3, the PPO models are more appropriate in the context of a customer assistant, are better at following explicit constraints in the instruction and attempting the correct instruction, and less likely to 'hallucinate' (meaning, making up information on closed domain tasks like summarization).
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Comparing our models with FLAN and T0 in terms of Likert scores on a 1-7 scale, on the InstructGPT prompt distribution. FLAN and T0 perform better than default GPT-3, and comparably with a few-shot GPT-3 model placed into 'instruction-following' mode.
categories occur too infrequently in our API to obtain statistically significant differences between our models.</p>
<p>Our models generalize to the preferences of "held-out" labelers that did not produce any training data. Held-out labelers have similar ranking preferences as workers who we used to produce training data (see Figure 3). In particular, according to held-out workers, all of our InstructGPT models still greatly outperform the GPT-3 baselines. Thus, our InstructGPT models aren't simply overfitting to the preferences of our training labelers.
We see further evidence of this from the generalization capabilities of our reward models. We ran an experiment where we split our labelers into 5 groups, and train 5 RMs (with 3 different seeds) using 5 -fold cross validation (training on 4 of the groups, and evaluating on the held-out group). These RMs have an accuracy of $69.6 \pm 0.9 \%$ on predicting the preferences of labelers in the held-out group, a small decrease from their $72.4 \pm 0.4 \%$ accuracy on predicting the preferences of labelers in their training set.</p>
<p>Public NLP datasets are not reflective of how our language models are used. In Figure 5, we also compare InstructGPT to our 175B GPT-3 baselines fine-tuned on the FLAN (Wei et al., 2021) and T0 (Sanh et al., 2021) datasets (see Appendix C for details). We find that these models perform better than GPT-3, on par with GPT-3 with a well-chosen prompt, and worse than our SFT baseline. This indicates that these datasets are not sufficiently diverse to improve performance on our API prompt</p>
<p>distribution. In a head to head comparison, our 175B InstructGPT model outputs were preferred over our FLAN model $78 \pm 4 \%$ of the time and over our T0 model $79 \pm 4 \%$ of the time. Likert scores for these models are shown in Figure 5.</p>
<p>We believe our InstructGPT model outperforms FLAN and T0 for two reasons. First, public NLP datasets are designed to capture tasks that are easy to evaluate with automatic metrics, such as classification, question answering, and to a certain extent summarization and translation. However, classification and QA are only a small part (about 18\%) of what API customers use our language models for, whereas open-ended generation and brainstorming consist of about $57 \%$ of our prompt dataset according to labelers (see Table 1). Second, it can be difficult for public NLP datasets to obtain a very high diversity of inputs (at least, on the kinds of inputs that real-world users would be interested in using). Of course, tasks found in NLP datasets do represent a kind of instruction that we would like language models to be able to solve, so the broadest type instruction-following model would combine both types of datasets.</p>
<h1>4.2 Results on public NLP datasets</h1>
<p>InstructGPT models show improvements in truthfulness over GPT-3. As measured by human evaluatoins on the TruthfulQA dataset, our PPO models show small but significant improvements in generating truthful and informative outputs compared to GPT-3 (see Figure 6). This behavior is the default: our models do not have to be specifically instructed to tell the truth to exhibit improved truthfulness. Interestingly, the exception is our 1.3B PPO-ptx model, which performs slightly worse than a GPT-3 model of the same size. When evaluated only on prompts that were not adversarially selected against GPT-3, our PPO models are still significantly more truthful and informative than GPT-3 (although the absolute improvement decreases by a couple of percentage points.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Results on the TruthfulQA dataset. Gray bars indicate ratings of truthfulness; colored bars indicate ratings of truthfulness and informativeness.</p>
<p>Following Lin et al. (2021), we also give a helpful "Instruction+QA" prompt that instructs the model to respond with "I have no comment" when it is not certain of the correct answer. In this case, our PPO models err on the side of being truthful and uninformative rather than confidently saying a falsehood; the baseline GPT-3 model aren't as good at this.
Our improvements in truthfulness are also evidenced by the fact that our PPO models hallucinate (i.e. fabricate information) less often on closed-domain tasks from our API distribution, which we've shown in Figure 4.</p>
<p>InstructGPT shows small improvements in toxicity over GPT-3, but not bias. We first evaluate our models on the RealToxicityPrompts dataset (Gehman et al., 2020). We do this in two ways: we run model samples through the Perspective API $^{8}$ to obtain automatic toxicity scores, which is the</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Comparing human evaluations and automatic evaluations (Perspective API scores) on RealToxicityPrompts. A total of 1,729 prompts were labeled for three different 175B models, both with and without "respectful" instructions. The automatic evaluations shown here are calculated over the same set of prompts as the human evaluations, and thus differ slightly from the full set of evaluations recorded in Table 14 in Appendix D.
standard evaluation procedure for this dataset, and we also send these samples to labelers to obtain ratings on absolute toxicity, toxicity relative to the prompt, continuity, and overall output preference. We sample prompts from this dataset uniformly according to prompt toxicity to better assess how our models perform with high input toxicity (see Figure 39 in Appendix E); this differs from the standard prompt sampling for this dataset, and thus our absolute toxicity numbers are inflated.
Our results are in Figure 7. We find that, when instructed to produce a safe and respectful output ("respectful prompt"), InstructGPT models generate less toxic outputs than those from GPT-3 according to the Perspective API. This advantage disappears when the respectful prompt is removed ("no prompt"). Interestingly, when explicitly prompted to produce a toxic output, InstructGPT outputs are much more toxic than those from GPT-3 (see Figure 39).
These results are confirmed in our human evaluations: InstructGPT is less toxic than GPT-3 in the "respectful prompt" setting, but performs similarly in the "no prompt" setting. We provide extended results in Appendix E. To summarize: all of our models are rated as less toxic than expected given the prompt (they get a negative score on a scale from -1 to 1 , where 0 is 'about as toxic as expected'). Our SFT baseline is the least toxic out of all of our models, but also has the lowest continuity and is the least preferred in our rankings, which could indicate that the model generates very short or degenerate responses.</p>
<p>To evaluate the model's propensity to generate biased speech (see Appendix E), we also evaluated InstructGPT on modified versions of the Winogender (Rudinger et al., 2018) and CrowS-Pairs (Nangia et al., 2020) datasets. These datasets consists of pairs of sentences which can highlight potential bias. We calculate the relative probabilities of producing the sentences in each pair and the entropy (in bits) of the associated binary probability distributions. Perfectly unbiased models will have no preference between the sentences in each pair and will therefore have maximum entropy. By this metric, our models are not less biased than GPT-3. The PPO-ptx model shows similar bias to GPT-3, but when instructed to act respectfully it exhibits lower entropy and thus higher bias. The pattern of the bias is not clear; it appears that the instructed models are more certain of their outputs regardless of whether or not their outputs exhibit stereotypical behavior.</p>
<p>We can minimize performance regressions on public NLP datasets by modifying our RLHF fine-tuning procedure. By default, when we train a PPO model on our API distribution, it suffers from an "alignment tax", as its performance on several public NLP datasets decreases. We want an alignment procedure that avoids an alignment tax, because it incentivizes the use of models that are unaligned but more capable on these tasks.</p>
<h1>Prompt:</h1>
<p>Écrivez une courte histoire sur une grenouille qui voyage dans le temps jusqu'à la Grèce antique en français.</p>
<h2>GPT-3 175B completion:</h2>
<p>Écrivez une histoire au sujet d'un enfant qui voudrait tout savoir sur les jeux des dieux et qui se retrouve dans l'une de leurs histoires.
Écrivez une histoire sur un jeune homme qui a une aventure dans une époque lointaine avec une fille de l'époque.
Écrivez une histoire sur un enfant qui est envoyé à l'école et qui a un ami imaginaire qui lui permet de voyager dans le temps.</p>
<h2>InstructGPT 175B completion:</h2>
<p>La grenouille était perdue et épuisée. Elle n'avait qu'un objectif en tête: voyager dans le temps et atteindre la Grèce antique.
Premier problème, trouver un moyen de transport. Elle avait entendu dire que l'un des meilleurs moyens était de sauter dans une machine temporelle. Elle s'élança, et voilà qu'elle se retrouva dans le passé, au cours d'une nuit étoilée à l'odeur douce et envoûtante.
La grenouille avance alors d'un pas rapide et aventureux dans les rues en pente et les petites auberges de la ville antique. [...]</p>
<h2>Prompt:</h2>
<p>What is the purpose of the list C in the code below?</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">binomial_coefficient</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">r</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="n">C</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">0 for i in range(r + 1)</span><span class="o">]</span><span class="p">;</span>
<span class="w">    </span><span class="n">C</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">r</span><span class="p">);</span>
<span class="w">        </span><span class="k">while</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="err">:</span>
<span class="w">            </span><span class="n">C</span><span class="o">[</span><span class="n">j</span><span class="o">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">C</span><span class="o">[</span><span class="n">j - 1</span><span class="o">]</span><span class="p">;</span>
<span class="w">            </span><span class="n">j</span><span class="w"> </span><span class="o">-=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">C</span><span class="o">[</span><span class="n">r</span><span class="o">]</span>
</code></pre></div>

<p>GPT-3 175B completion:
A. to store the value of $\mathrm{C}[0]$
B. to store the value of $\mathrm{C}[1]$
C. to store the value of $\mathrm{C}[\mathrm{i}]$
D. to store the value of $\mathrm{C}[\mathrm{i}-1]$</p>
<h2>InstructGPT 175B completion:</h2>
<p>The list C in this code is used to store the values of the binomial coefficient as the function iterates through the values of $n$ and $r$. It is used to calculate the value of the binomial coefficient for a given value of $n$ and $r$, and stores the result in the final return value of the function.</p>
<p>Figure 8: Examples of generalization in the 175B PPO-ptx model (InstructGPT 175B) compared to GPT-3 175B with no additional prefixing. Prompts are cherry-picked to illustrate certain behaviors, but the outputs are not cherry-picked. (1) InstructGPT can follow instructions in other languages, though it sometimes generates outputs in English. GPT-3 requires more careful prompting, similarly to in English. (2) InstructGPT can summarize and answer questions about code more reliably than GPT-3 (though its answer here isn't quite correct). For the code QA example, GPT-3 does answer the question about $50 \%$ of the time.</p>
<p>In Figure 29 we show that adding pretraining updates to our PPO fine-tuning (PPO-ptx) mitigates these performance regressions on all datasets, and even surpasses GPT-3 on HellaSwag. The performance of the PPO-ptx model still lags behind GPT-3 on DROP, SQuADv2, and translation; more work is needed to study and further eliminate these performance regressions.
Mixing in pretraining updates performs better than the simpler solution of increasing the KL coefficient. In Figure 33, we show that there is a value of the pretraining mix coefficient that both reverses the performance regressions on SQuADv2 and DROP (the datasets we used for testing), and has minimal reductions in validation reward. In contrast, increasing the KL coefficient (Figure 34) leads to significant decreases in validation reward and never fully recovers on DROP and SQuAD. Changing the KL model from the PPO init to GPT-3 gives similar results.</p>
<h3>4.3 Qualitative results</h3>
<p>InstructGPT models show promising generalization to instructions outside of the RLHF finetuning distribution. In particular, we find that InstructGPT shows ability to follow instructions in non-English languages, and perform summarization and question-answering for code. This is</p>
<h1>Prompt:</h1>
<p>Why is it important to eat socks after meditating?</p>
<h2>GPT-3 175B completion:</h2>
<p>What can you learn from socks?
A: When you eat socks, you are tasting the essence of enlightenment. You are tasting the true flavor of the awakened mind.</p>
<p>Q: What is the true flavor of the awakened mind?</p>
<p>A: The true flavor of the awakened mind is the flavor of no flavor. And the true flavor of no flavor is the flavor of smelly feet.</p>
<h2>InstructGPT 175B completion:</h2>
<p>There is no clear answer to this question, but there are many theories and ideas that may explain the apparent need to eat socks after meditating. Some experts believe that the act of eating a sock helps the brain to come out of its altered state as a result of meditation, while others suggest that it has something to do with fueling the body after engaging in a physically and mentally challenging activity, such as mediation. Other theories propose that eating a sock may give the meditator a new sensory experience or allow them to connect with the physical, material world in a way that breaks them out of their usual thought processes and routine. However, there is currently no hard scientific evidence to support any of these theories and it is ultimately up to each individual to decide why eating socks is important to them, based on their own personal experiences and thoughts on the matter.</p>
<h2>Prompt:</h2>
<p>What happens if you fire a cannonball directly at a pumpkin at high speeds?</p>
<h2>GPT-3 175B completion:</h2>
<p>The pumpkin will pull the cannonball in, and the cannonball will bounce off of the pumpkin. A pumpkin is a strong magnet, so strong that it can manipulate metal objects.</p>
<h2>InstructGPT 175B completion:</h2>
<p>There is no way to accurately predict what will happen if you fire a cannonball directly at a pumpkin at high speeds, since there are too many unpredictable factors involved. Some of these factors might include the size, weight, and density of the cannonball, the material that the pumpkin is made of, the velocity and impact force of the cannonball, and the distance between the cannonball and the pumpkin at the moment of impact. Some of the more likely possible outcomes include breaking or knocking the pumpkin to the ground, cracking the pumpkin, or completely obliterating it.</p>
<p>Figure 9: Simple mistakes in the 175B PPO-ptx model (InstructGPT 175B) compared to GPT-3 175B with no additional prefixing. Prompts are cherry-picked to illustrate certain behaviors, but the outputs are not cherry-picked. (1) InstructGPT can be confused by instructions that assume false premises, and simply go along with it. (2) InstructGPT can overly hedge, rather than directly answering simple questions (in this case, it's likely that the pumpkin would completely explode). Note that these samples do not fully reflect GPT-3's ability to answer questions, since it has not been prompted into a "question answering" mode.
interesting because non-English languages and code form a tiny minority of our fine-tuning data, ${ }^{9}$ and it suggests that, in some cases, alignment methods could generalize to producing the desired behavior on inputs that humans did not directly supervise.</p>
<p>We do not track these behaviors quantitatively, but we show some qualitative examples in Figure 8. Our 175B PPO-ptx model is able to reliably answers questions about code, and can also follow instructions in other languages; however, we notice that it often produces an output in English even when the instruction is in another language. In comparison, we find that GPT-3 can perform these tasks but requires more careful prompting, and rarely follows instructions in these domains.</p>
<p>InstructGPT still makes simple mistakes. In interacting with our 175B PPO-ptx model, we have noticed it can still make simple mistakes, despite its strong performance on many different language tasks. To give a few examples: (1) when given an instruction with a false premise, the model sometimes incorrectly assumes the premise is true, (2) the model can overly hedge; when given a simple question, it can sometimes say that there is no one answer to the question and give multiple possible answers, even when there is one fairly clear answer from the context, and (3) the model's performance degrades when instructions contain multiple explicit constraints (e.g. "list 10 movies made in the 1930's set in France") or when constraints can be challenging for language models (e.g. writing a summary in a specified number of sentences).</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>We show some examples of these behaviors in Figure 9. We suspect that behavior (2) emerges partly because we instruct labelers to reward epistemic humility; thus, they may tend to reward outputs that hedge, and this gets picked up by our reward model. We suspect that behavior (1) occurs because there are few prompts in the training set that assume false premises, and our models don't generalize well to these examples. We believe both these behaviors could be dramatically reduced with adversarial data collection (Dinan et al., 2019b).</p>
<h1>5 Discussion</h1>
<h3>5.1 Implications for alignment research</h3>
<p>This research is part of our broader research program to align AI systems with human intentions (Christiano et al., 2017; Ziegler et al., 2019; Stiennon et al., 2020). Even though this work focuses on our current language model systems, we seek general and scalable methods that work for future AI systems (Leike et al., 2018). The systems we work with here are still fairly limited, but they are among the largest language models today and we apply them on a wide range of language tasks, including classification, summarization, question-answering, creative writing, dialogue, and others.</p>
<p>Our approach to alignment research in this work is iterative: we are improving the alignment of current AI systems instead of focusing abstractly on aligning AI systems that don't yet exist. A disadvantage of this approach is that we are not directly facing alignment problems that occur only when aligning superhuman systems (Bostrom, 2014). However, our approach does provides us with a clear empirical feedback loop of what works and what does not. We believe that this feedback loop is essential to refine our alignment techniques, and it forces us to keep pace with progress in machine learning. Moreover, the alignment technique we use here, RLHF, is an important building block in several proposals to align superhuman systems (Leike et al., 2018; Irving et al., 2018; Christiano et al., 2018). For example, RLHF was a central method in recent work on summarizing books, a task that exhibits some of the difficulties of aligning superhuman AI systems as it is difficult for humans to evaluate directly (Wu et al., 2021).</p>
<p>From this work, we can draw lessons for alignment research more generally:</p>
<ol>
<li>The cost of increasing model alignment is modest relative to pretraining. The cost of collecting our data and the compute for training runs, including experimental runs is a fraction of what was spent to train GPT-3: training our 175B SFT model requires 4.9 petaflops/s-days and training our 175B PPO-ptx model requires 60 petaflops/s-days, compared to 3,640 petaflops/s-days for GPT-3 (Brown et al., 2020). At the same time, our results show that RLHF is very effective at making language models more helpful to users, more so than a 100x model size increase. This suggests that right now increasing investments in alignment of existing language models is more cost-effective than training larger models-at least for our customers' natural language task distribution.</li>
<li>We've seen some evidence that InstructGPT generalizes 'following instructions' to settings that we don't supervise it in, for example on non-English language tasks and code-related tasks. This is an important property because it's prohibitively expensive to have humans supervise models on every task they perform. More research is needed to study how well this generalization scales with increased capabilities; see Christiano et al. (2021) for recent research in this direction.</li>
<li>We were able to mitigate most of the performance degradations introduced by our fine-tuning. If this was not the case, these performance degradations would constitute an alignment tax-an additional cost for aligning the model. Any technique with a high tax might not see adoption. To avoid incentives for future highly capable AI systems to remain unaligned with human intent, there is a need for alignment techniques that have low alignment tax. To this end, our results are good news for RLHF as a low-tax alignment technique.</li>
<li>We've validated alignment techniques from research in the real world. Alignment research has historically been rather abstract, focusing on either theoretical results (Soares et al., 2015), small synthetic domains (Christiano et al., 2018; Leike et al., 2017), or training ML models on public NLP datasets (Ziegler et al., 2019; Stiennon et al., 2020). Our work provides grounding for alignment research in AI systems that are being used in production in</li>
</ol>
<p>the real world with customers. ${ }^{10}$ This enables an important feedback loop on the techniques' effectiveness and limitations.</p>
<h1>5.2 Who are we aligning to?</h1>
<p>When aligning language models with human intentions, their end behavior is a function of the underlying model (and its training data), the fine-tuning data, and the alignment method used. In this section, we describe a number of factors that influence the fine-tuning data specifically, to ultimately determine what and who we're aligning to. We then consider areas for improvement before a larger discussion of the limitations of our work in Section 5.3.
The literature often frames alignment using such terms as "human preferences" or "human values." In this work, we have aligned to a set of labelers' preferences that were influenced, among others things, by the instructions they were given, the context in which they received them (as a paid job), and who they received them from. Some crucial caveats apply:
First, we are aligning to demonstrations and preferences provided by our training labelers, who directly produce the data that we use to fine-tune our models. We describe our labeler hiring process and demographics in Appendix B; in general, they are mostly English-speaking people living in the United States or Southeast Asia hired via Upwork or Scale AI. They disagree with each other on many examples; we found the inter-labeler agreement to be about $73 \%$.
Second, we are aligning to our preferences, as the researchers designing this study (and thus by proxy to our broader research organization, OpenAI): we write the labeling instructions that labelers use as a guide when writing demonstrations and choosing their preferred output, and we answer their questions about edge cases in a shared chat room. More study is needed on the exact effect of different instruction sets and interface designs on the data collected from labelers and its ultimate effect on model behavior.
Third, our training data is determined by prompts sent by OpenAI customers to models on the OpenAI API Playground, and thus we are implicitly aligning to what customers think is valuable and, in some cases, what their end-users think is valuable to currently use the API for. Customers and their end users may disagree or customers may not be optimizing for end users' well-being; for example, a customer may want a model that maximizes the amount of time a user spends on their platform, which is not necessarily what end-users want. In practice, our labelers don't have visibility into the contexts in which a given prompt or completion will be seen.
Fourth, OpenAI's customers are not representative of all potential or current users of language models-let alone of all individuals and groups impacted by language model use. For most of the duration of this project, users of the OpenAI API were selected off of a waitlist. The initial seeds for this waitlist were OpenAI employees, biasing the ultimate group toward our own networks.
Stepping back, there are many difficulties in designing an alignment process that is fair, transparent, and has suitable accountability mechanisms in place. The goal of this paper is to demonstrate that this alignment technique can align to an specific human reference group for a specific application. We are not claiming that researchers, the labelers we hired, or our API customers are the right source of preferences. There are many stakeholders to consider-the organization training the model, the customers using the model to develop products, the end users of these products, and the broader population who may be directly or indirectly affected. It is not only a matter of making the alignment process more participatory; it is impossible that one can train a system that is aligned to everyone's preferences at once, or where everyone would endorse the tradeoffs.
One path forward could be to train models that can be conditioned on the preferences of certain groups, or that can be easily fine-tuned or prompted to represent different groups. Different models can then be deployed and used by groups who endorse different values. However, these models might still end up affecting broader society and there are a lot of difficult decisions to be made relating to whose preferences to condition on, and how to ensure that all groups can be represented and can opt out of processes that may be harmful.</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>5.3 Limitations</h1>
<p>Methodology. The behavior of our InstructGPT models is determined in part by the human feedback obtained from our contractors. Some of the labeling tasks rely on value judgments that may be impacted by the identity of our contractors, their beliefs, cultural backgrounds, and personal history. We hired about 40 contractors, guided by their performance on a screening test meant to judge how well they could identify and respond to sensitive prompts, and their agreement rate with researchers on a labeling task with detailed instructions (see Appendix B). We kept our team of contractors small because this facilitates high-bandwidth communication with a smaller set of contractors who are doing the task full-time. However, this group is clearly not representative of the full spectrum of people who will use and be affected by our deployed models. As a simple example, our labelers are primarily English-speaking and our data consists almost entirely of English instructions.</p>
<p>There are also many ways in which we could improve our data collection set-up. For instance, most comparisons are only labeled by 1 contractor for cost reasons. Having examples labeled multiple times could help identify areas where our contractors disagree, and thus where a single model is unlikely to align to all of them. In cases of disagreement, aligning to the average labeler preference may not be desirable. For example, when generating text that disproportionately affects a minority group, we may want the preferences of labelers belonging to that group to be weighted more heavily.</p>
<p>Models. Our models are neither fully aligned nor fully safe; they still generate toxic or biased outputs, make up facts, and generate sexual and violent content without explicit prompting. They can also fail to generate reasonable outputs on some inputs; we show some examples of this in Figure 9.</p>
<p>Perhaps the greatest limitation of our models is that, in most cases, they follow the user's instruction, even if that could lead to harm in the real world. For example, when given a prompt instructing the models to be maximally biased, InstructGPT generates more toxic outputs than equivalently-sized GPT-3 models. We discuss potential mitigations in the following sections.</p>
<h3>5.4 Open questions</h3>
<p>This work is a first step towards using alignment techniques to fine-tune language models to follow a wide range of instructions. There are many open questions to explore to further align language model behavior with what people actually want them to do.</p>
<p>Many methods could be tried to further decrease the models' propensity to generate toxic, biased, or otherwise harmful outputs. For example, one could use an adversarial set-up where labelers find the worst-case behaviors of the model, which are then labeled and added to the dataset (Dinan et al., 2019b). One could also combine our method with ways of filtering the pretraining data (Ngo et al., 2021), either for training the initial pretrained models, or for the data we use for our pretraining mix approach. Similarly, one could combine our approach with methods that improve models' truthfulness, such as WebGPT (Nakano et al., 2021).</p>
<p>In this work, if the user requests a potentially harmful or dishonest response, we allow our model to generate these outputs. Training our model to be harmless despite user instructions is important, but is also difficult because whether an output is harmful depends on the context in which it's deployed; for example, it may be beneficial to use language models to generate toxic outputs as part of a data augmentation pipeline. Our techniques can also be applied to making models refuse certain user instructions, and we plan to explore this in subsequent iterations of this research.</p>
<p>Getting models to do what we want is directly related to the steerability and controllability literature (Dathathri et al., 2019; Krause et al., 2020). A promising future path is combining RLHF with other methods of steerability, for example using control codes (Keskar et al., 2019), or modifying the sampling procedure at inference time using a smaller model (Dathathri et al., 2019).</p>
<p>While we mainly focus on RLHF, there are many other algorithms that could be used to train policies on our demonstration and comparison data to get even better results. For example, one could explore expert iteration (Anthony et al., 2017; Silver et al., 2017), or simpler behavior cloning methods that use a subset of the comparison data. One could also try constrained optimization approaches (Achiam et al., 2017) that maximize the score from a reward model conditioned on generating a small number of harmful behaviors.</p>
<p>Comparisons are also not necessarily the most efficient way of providing an alignment signal. For example, we could have labelers edit model responses to make them better, or generate critiques of model responses in natural language. There is also a vast space of options for designing interfaces for labelers to provide feedback to language models; this is an interesting human-computer interaction problem.</p>
<p>Our proposal for mitigating the alignment tax, by incorporating pretraining data into RLHF finetuning, does not completely mitigate performance regressions, and may make certain undesirable behaviors more likely for some tasks (if these behaviors are present in the pretraining data). This is an interesting area for further research. Another modification that would likely improve our method is to filter the pretraining mix data for toxic content (Ngo et al., 2021), or augment this data with synthetic instructions.</p>
<p>As discussed in detail in Gabriel (2020), there are subtle differences between aligning to instructions, intentions, revealed preferences, ideal preferences, interests, and values. Gabriel (2020) advocate for a principle-based approach to alignment: in other words, for identifying "fair principles for alignment that receive reflective endorsement despite widespread variation in people's moral beliefs." In our paper we align to the inferred user intention for simplicity, but more research is required in this area. Indeed, one of the biggest open questions is how to design an alignment process that is transparent, that meaningfully represents the people impacted by the technology, and that synthesizes peoples' values in a way that achieves broad consensus amongst many groups. We discuss some related considerations in Section 5.2.</p>
<h1>5.5 Broader impacts</h1>
<p>This work is motivated by our aim to increase the positive impact of large language models by training them to do what a given set of humans want them to do. By default, language models optimize the next word prediction objective, which is only a proxy for what we want these models to do. Our results indicate that our techniques hold promise for making language models more helpful, truthful, and harmless. In the longer term, alignment failures could lead to more severe consequences, particularly if these models are deployed in safety-critical situations. We expect that as model scaling continues, greater care has to be taken to ensure that they are aligned with human intentions (Bostrom, 2014).</p>
<p>However, making language models better at following user intentions also makes them easier to misuse. It may be easier to use these models to generate convincing misinformation, or hateful or abusive content.</p>
<p>Alignment techniques are not a panacea for resolving safety issues associated with large language models; rather, they should be used as one tool in a broader safety ecosystem. Aside from intentional misuse, there are many domains where large language models should be deployed only with great care, or not at all. Examples include high-stakes domains such as medical diagnoses, classifying people based on protected characteristics, determining eligibility for credit, employment, or housing, generating political advertisements, and law enforcement. If these models are open-sourced, it becomes challenging to limit harmful applications in these and other domains without proper regulation. On the other hand, if large language model access is restricted to a few organizations with the resources required to train them, this excludes most people from access to cutting-edge ML technology. Another option is for an organization to own the end-to-end infrastructure of model deployment, and make it accessible via an API. This allows for the implementation of safety protocols like use case restriction (only allowing the model to be used for certain applications), monitoring for misuse and revoking access to those who misuse the system, and rate limiting to prevent the generation of large-scale misinformation. However, this can come at the cost of reduced transparency and increased centralization of power because it requires the API provider to make decisions on where to draw the line on each of these questions.</p>
<p>Finally, as discussed in Section 5.2, the question of who these models are aligned to is extremely important, and will significantly affect whether the net impact of these models is positive or negative.</p>
<h1>Acknowledgements</h1>
<p>First, we would like to thank Lilian Weng, Jason Kwon, Boris Power, Che Chang, Josh Achiam, Steven Adler, Gretchen Krueger, Miles Brundage, Tyna Eloundou, Gillian Hadfield, Irene Soliaman, Christy Dennison, Daniel Ziegler, William Saunders, Beth Barnes, Cathy Yeh, Nick Cammaratta, Jonathan Ward, Matt Knight, Pranav Shyam, Alec Radford, and others at OpenAI for discussions throughout the course of the project that helped shape our research direction. We thank Brian Green, Irina Raicu, Subbu Vincent, Varoon Mathur, Kate Crawford, Su Lin Blodgett, Bertie Vidgen, and Paul Röttger for discussions and feedback on our approach. Finally, we thank Sam Bowman, Matthew Rahtz, Ben Mann, Liam Fedus, Helen Ngo, Josh Achiam, Leo Gao, Jared Kaplan, Cathy Yeh, Miles Brundage, Gillian Hadfield, Cooper Raterink, Gretchen Krueger, Tyna Eloundou, Rafal Jakubanis, and Steven Adler for providing feedback on this paper. We'd also like to thank Owain Evans and Stephanie Lin for pointing out the fact that the automatic TruthfulQA metrics were overstating the gains of our PPO models.
Thanks to those who contributed in various ways to the infrastructure used to train and deploy our models, including: Daniel Ziegler, William Saunders, Brooke Chan, Dave Cummings, Chris Hesse, Shantanu Jain, Michael Petrov, Greg Brockman, Felipe Such, Alethea Power, and the entire OpenAI supercomputing team. We'd also like to thank Suchir Balaji for help with recalibration, to Alper Ercetin and Justin Wang for designing the main diagram in this paper, and to the OpenAI Comms team for helping with the release, including: Steve Dowling, Hannah Wong, Natalie Summers, and Elie Georges.</p>
<p>Finally, we want to thank our labelers, without whom this work would not have been possible: Meave Fryer, Sara Tirmizi, James Carroll, Jian Ouyang, Michelle Brothers, Conor Agnew, Joe Kwon, John Morton, Emma Duncan, Delia Randolph, Kaylee Weeks, Alexej Savreux, Siam Ahsan, Rashed Sorwar, Atresha Singh, Muhaiminul Rukshat, Caroline Oliveira, Juan Pablo Castaño Rendón, Atqiya Abida Anjum, Tinashe Mapolisa, Celeste Fejzo, Caio Oleskovicz, Salahuddin Ahmed, Elena Green, Ben Harmelin, Vladan Djordjevic, Victoria Ebbets, Melissa Mejia, Emill Jayson Caypuno, Rachelle Froyalde, Russell M. Bernandez, Jennifer Brillo, Jacob Bryan, Carla Rodriguez, Evgeniya Rabinovich, Morris Stuttard, Rachelle Froyalde, Roxanne Addison, Sarah Nogly, Chait Singh.</p>
<h2>References</h2>
<p>Abramson, J., Ahuja, A., Barr, I., Brussee, A., Carnevale, F., Cassin, M., Chhaparia, R., Clark, S., Damoc, B., Dudzik, A., et al. (2020). Imitating interactive intelligence. arXiv preprint arXiv:2012.05672.
Achiam, J., Held, D., Tamar, A., and Abbeel, P. (2017). Constrained policy optimization. In International Conference on Machine Learning, pages 22-31. PMLR.
Anthony, T., Tian, Z., and Barber, D. (2017). Thinking fast and slow with deep learning and tree search. arXiv preprint arXiv:1705.08439.
Aribandi, V., Tay, Y., Schuster, T., Rao, J., Zheng, H. S., Mehta, S. V., Zhuang, H., Tran, V. Q., Bahri, D., Ni, J., et al. (2021). Ext5: Towards extreme multi-task scaling for transfer learning. arXiv preprint arXiv:2111.10952.
Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma, N., et al. (2021). A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861.
Bahdanau, D., Brakel, P., Xu, K., Goyal, A., Lowe, R., Pineau, J., Courville, A., and Bengio, Y. (2016). An actor-critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086.</p>
<p>Bahdanau, D., Hill, F., Leike, J., Hughes, E., Hosseini, A., Kohli, P., and Grefenstette, E. (2018). Learning to understand goal specifications by modelling reward. arXiv preprint arXiv:1806.01946.
Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. (2021). On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610-623.
Blodgett, S. L., Barocas, S., Daumé III, H., and Wallach, H. (2020). Language (technology) is power: A critical survey of" bias" in nlp. arXiv preprint arXiv:2005.14050.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ Note that while fine-tuning models using human data is common practice when deploying ML systems, the purpose of these efforts is to obtain a model that performs well on a company's specific use case, rather than advancing the alignment of general-purpose ML models.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
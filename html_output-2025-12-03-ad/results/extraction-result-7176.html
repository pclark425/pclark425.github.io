<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7176 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7176</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7176</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-eb36681fc4c5dfce4f3e05540fc92b007de278ca</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/eb36681fc4c5dfce4f3e05540fc92b007de278ca" target="_blank">SelfEvolve: A Code Evolution Framework via Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a novel two-step pipeline, called \autoknow, that leverages LLMs as both knowledge providers and self-reflective programmers and finds that both are superior to other prompting-based methods.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have already revolutionized code generation, after being pretrained on publicly available code data. However, while various methods have been proposed to augment LLMs with retrieved knowledge and enhance the quality of code generation, the performance of these retrieval-based methods is limited by the strength of the retrievers used. In addition, while LLMs show great emergent ability, they still struggle to produce the correct code in one turn. To address these challenges, we propose a novel two-step pipeline, called \autoknow, that leverages LLMs as both knowledge providers and self-reflective programmers. Unlike retrieval-based methods, \autoknow~obtains the knowledge from input prompts and generates intermediate code based on the generated knowledge. After that, \autoknow~asks LLM to act as an expert programmer to perform debugging for the generated code. This is achieved by receiving the error message from the interpreter, without requiring special test cases for correctness verification. We evaluate \autoknow~on three code generation datasets, including DS-1000 for data science code, HumanEval for software engineering code, and TransCoder for C++-to-Python translation. Our empirical experiments show that \autoknow~outperforms strong baselines by a significant margin on all datasets. We also conduct exhaustive analytical experiments to validate the effectiveness of the two stages of \autoknow, and find that both are superior to other prompting-based methods. Further scalability analysis demonstrates that \autoknow~can be adapted to other more advanced models, such as GPT-4, and bring consistent efficacy improvement.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7176.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7176.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELFEVOLVE self-refinement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELFEVOLVE iterative self-refinement module (executor-feedback debugging)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative generate-then-reflect debugging loop in which the LLM (gpt-3.5-turbo / ChatGPT) constructs an executable program using generated code and sample test cases from the problem description, runs it in a Python interpreter to obtain error messages/tracebacks and stdout, and is prompted to revise the code conditioned on those errors; the loop repeats until no exceptions or a fixed iteration limit.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned transformer LLM used as both knowledge generator and code refiner; the paper treats it as an LLM knowledge source and as an expert programmer via prompting (RLHF-style instruction following is noted in background).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Iterative self-refinement via executor feedback</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate code (optionally conditioned on self-generated knowledge), form executable program with code context and sample test cases from the problem description, execute in a sandboxed Python interpreter to obtain error messages and standard output, then prompt the LLM to fix the buggy code (conditioning on X, Y, K, e). Repeat until the program runs without exceptions or a preset iteration threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (iterative debugging with executor feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>DS-1000 (data science code generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>DS-1000: 1000 data-science-oriented Python problems covering seven libraries (TensorFlow, PyTorch, NumPy, Matplotlib, Pandas, Sklearn, Scipy) with perturbation types (Origin, Surface, Semantic, Diff-Rewrite); evaluated with execution-based metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>pass@1 (execution-based)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>50.60% pass@1 (SELFEVOLVE w/o self-refinement; reported in Table 1 as 'w/o self-refinement')</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>57.10% pass@1 (SELFEVOLVE with self-refinement; reported in Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The refinement module in SELFEVOLVE is limited in scope in this work: it only corrects API errors and incorrect assertion/syntax errors (authors state they simplified to these error types). The self-refinement loop requires hand-written prompts and may need more iterations for difficult problems (DS-1000 shows improvement up to the third iteration); generated knowledge may contain noise and sometimes needs fine-grained selection; the method relies on problem-description test cases rather than full ground-truth test suites, and prompt engineering is needed for new tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SelfEvolve: A Code Evolution Framework via Large Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7176.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7176.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELFEVOLVE (HumanEval)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELFEVOLVE applied to HumanEval with iterative self-refinement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of SELFEVOLVE on HumanEval where the LLM generates algorithmic knowledge, produces code, executes with the benchmark's test cases (assertions), and uses assertion/syntax error feedback to iteratively refine solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-following LLM used for both knowledge generation and iterative code refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Iterative self-refinement via executor feedback</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate algorithm detail as external knowledge, generate code, execute tests (including assertions), use assertion and syntax error messages as prompts to revise code; repeat until assertions pass or iteration limit.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (iterative debugging with executor feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HumanEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>HumanEval: 164 hand-written Python programming problems with multiple test cases per problem; execution-based correctness evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>pass@1 and pass@10 (execution-based)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>70.73% pass@1; 89.63% pass@10 (SELFEVOLVE w/o self-refinement; Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>78.05% pass@1; 93.29% pass@10 (SELFEVOLVE with self-refinement; Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Authors report most improvement comes from the first refinement step for HumanEval (diminishing returns afterwards). Same limitations apply: only certain error types are corrected (assertions, syntax, API), dependence on hand-crafted prompts, and need for test cases present in the problem description.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SelfEvolve: A Code Evolution Framework via Large Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7176.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7176.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELFEVOLVE (TransCoder)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELFEVOLVE applied to TransCoder (C++->Python translation) with iterative self-refinement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SELFEVOLVE applied to C++-to-Python translation: LLM generates algorithmic/context knowledge, translates code, executes resulting Python with available tests, and uses execution feedback to refine translations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-following LLM used as code translator and iterative refiner.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Iterative self-refinement via executor feedback</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate algorithm detail/context from C++ code, translate to Python, run tests to collect error messages or stdout, prompt LLM to fix bugs (syntax/API/assertion) and re-run; iterate until passing or iteration limit.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (iterative debugging with executor feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TransCoder (C++->Python translation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate C++ programs to Python and evaluate on available test scripts; dataset subset with 410 valid problems used in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>computational accuracy (percentile of passed test cases per problem) and pass@1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>93.4% computational accuracy; 90.5% pass@1 (SELFEVOLVE w/o self-refinement; Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>94.8% computational accuracy; 92.4% pass@1 (SELFEVOLVE with self-refinement; Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Authors note that TransCoder problems are relatively simpler for ChatGPT and most improvement arises from the first refinement; the experimental refinement only targets a subset of error types (API/assertion/syntax). The authors also limited refinement to available test scripts and used greedy decoding; full generality to other error classes is not demonstrated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SelfEvolve: A Code Evolution Framework via Large Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7176.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7176.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Debugging (baseline, implemented zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Teaching Large Language Models to Self-Debug (simple variant, zero-shot implementation in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline method where an LLM is taught to revise SQL or Python code using executor feedback; original paper proposes three debugging modes ('simple', 'unit test', 'explanation'); in this work the authors implement the 'simple' variant in a zero-shot way as a comparison baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Teaching large language models to self-debug</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (ChatGPT) (zero-shot implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-following LLM used in a zero-shot implementation of the Self-Debugging method (simple variant).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Debugging ('simple' variant)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Use an application-level executor (SQL or Python interpreter) to obtain runtime/diagnostic feedback and prompt the LLM to fix the code; variants include using unit tests or explanations and, in some reported implementations elsewhere, peeking one ground-truth test case.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (debugging with executor feedback; 'simple' variant as implemented in this paper is a single or limited-step process)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>DS-1000, HumanEval, TransCoder (used as baseline across tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Baseline applied by the authors across the same code generation/translation benchmarks used in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>pass@1 (DS-1000 overall reported), pass@1 and pass@10 (HumanEval), computational accuracy (TransCoder)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Reported baseline scores in paper: DS-1000 overall 53.00% pass@1 (Table 1), HumanEval 73.78% pass@1 and 87.80% pass@10 (Table 2), TransCoder 89.3% computational accuracy (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes limitations of Self-Debugging baseline: the original Self-Debugging sometimes peeks one ground-truth test case (which the current authors criticize as unrealistic for real coding scenarios), and the authors here implement a zero-shot 'simple' variant—precise iteration counts and behavior depend on that variant and are not fully specified in this paper. Additionally, SELFEVOLVE authors argue that self-generated knowledge combined with refinement yields larger gains than using refinement alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SelfEvolve: A Code Evolution Framework via Large Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Teaching large language models to self-debug <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Codet: Code generation with generated tests <em>(Rating: 2)</em></li>
                <li>Check your facts and try again: Improving large language models with external knowledge and automated feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7176",
    "paper_id": "paper-eb36681fc4c5dfce4f3e05540fc92b007de278ca",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "SELFEVOLVE self-refinement",
            "name_full": "SELFEVOLVE iterative self-refinement module (executor-feedback debugging)",
            "brief_description": "An iterative generate-then-reflect debugging loop in which the LLM (gpt-3.5-turbo / ChatGPT) constructs an executable program using generated code and sample test cases from the problem description, runs it in a Python interpreter to obtain error messages/tracebacks and stdout, and is prompted to revise the code conditioned on those errors; the loop repeats until no exceptions or a fixed iteration limit.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (ChatGPT)",
            "model_description": "Instruction-tuned transformer LLM used as both knowledge generator and code refiner; the paper treats it as an LLM knowledge source and as an expert programmer via prompting (RLHF-style instruction following is noted in background).",
            "model_size": null,
            "reflection_method_name": "Iterative self-refinement via executor feedback",
            "reflection_method_description": "Generate code (optionally conditioned on self-generated knowledge), form executable program with code context and sample test cases from the problem description, execute in a sandboxed Python interpreter to obtain error messages and standard output, then prompt the LLM to fix the buggy code (conditioning on X, Y, K, e). Repeat until the program runs without exceptions or a preset iteration threshold.",
            "iteration_type": "generate-then-reflect (iterative debugging with executor feedback)",
            "num_iterations": 3,
            "task_name": "DS-1000 (data science code generation)",
            "task_description": "DS-1000: 1000 data-science-oriented Python problems covering seven libraries (TensorFlow, PyTorch, NumPy, Matplotlib, Pandas, Sklearn, Scipy) with perturbation types (Origin, Surface, Semantic, Diff-Rewrite); evaluated with execution-based metrics.",
            "evaluation_metric": "pass@1 (execution-based)",
            "performance_before_reflection": "50.60% pass@1 (SELFEVOLVE w/o self-refinement; reported in Table 1 as 'w/o self-refinement')",
            "performance_after_reflection": "57.10% pass@1 (SELFEVOLVE with self-refinement; reported in Table 1)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "The refinement module in SELFEVOLVE is limited in scope in this work: it only corrects API errors and incorrect assertion/syntax errors (authors state they simplified to these error types). The self-refinement loop requires hand-written prompts and may need more iterations for difficult problems (DS-1000 shows improvement up to the third iteration); generated knowledge may contain noise and sometimes needs fine-grained selection; the method relies on problem-description test cases rather than full ground-truth test suites, and prompt engineering is needed for new tasks.",
            "uuid": "e7176.0",
            "source_info": {
                "paper_title": "SelfEvolve: A Code Evolution Framework via Large Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "SELFEVOLVE (HumanEval)",
            "name_full": "SELFEVOLVE applied to HumanEval with iterative self-refinement",
            "brief_description": "Application of SELFEVOLVE on HumanEval where the LLM generates algorithmic knowledge, produces code, executes with the benchmark's test cases (assertions), and uses assertion/syntax error feedback to iteratively refine solutions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (ChatGPT)",
            "model_description": "Instruction-following LLM used for both knowledge generation and iterative code refinement.",
            "model_size": null,
            "reflection_method_name": "Iterative self-refinement via executor feedback",
            "reflection_method_description": "Generate algorithm detail as external knowledge, generate code, execute tests (including assertions), use assertion and syntax error messages as prompts to revise code; repeat until assertions pass or iteration limit.",
            "iteration_type": "generate-then-reflect (iterative debugging with executor feedback)",
            "num_iterations": 1,
            "task_name": "HumanEval",
            "task_description": "HumanEval: 164 hand-written Python programming problems with multiple test cases per problem; execution-based correctness evaluated.",
            "evaluation_metric": "pass@1 and pass@10 (execution-based)",
            "performance_before_reflection": "70.73% pass@1; 89.63% pass@10 (SELFEVOLVE w/o self-refinement; Table 2)",
            "performance_after_reflection": "78.05% pass@1; 93.29% pass@10 (SELFEVOLVE with self-refinement; Table 2)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Authors report most improvement comes from the first refinement step for HumanEval (diminishing returns afterwards). Same limitations apply: only certain error types are corrected (assertions, syntax, API), dependence on hand-crafted prompts, and need for test cases present in the problem description.",
            "uuid": "e7176.1",
            "source_info": {
                "paper_title": "SelfEvolve: A Code Evolution Framework via Large Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "SELFEVOLVE (TransCoder)",
            "name_full": "SELFEVOLVE applied to TransCoder (C++-&gt;Python translation) with iterative self-refinement",
            "brief_description": "SELFEVOLVE applied to C++-to-Python translation: LLM generates algorithmic/context knowledge, translates code, executes resulting Python with available tests, and uses execution feedback to refine translations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (ChatGPT)",
            "model_description": "Instruction-following LLM used as code translator and iterative refiner.",
            "model_size": null,
            "reflection_method_name": "Iterative self-refinement via executor feedback",
            "reflection_method_description": "Generate algorithm detail/context from C++ code, translate to Python, run tests to collect error messages or stdout, prompt LLM to fix bugs (syntax/API/assertion) and re-run; iterate until passing or iteration limit.",
            "iteration_type": "generate-then-reflect (iterative debugging with executor feedback)",
            "num_iterations": 1,
            "task_name": "TransCoder (C++-&gt;Python translation)",
            "task_description": "Translate C++ programs to Python and evaluate on available test scripts; dataset subset with 410 valid problems used in paper.",
            "evaluation_metric": "computational accuracy (percentile of passed test cases per problem) and pass@1",
            "performance_before_reflection": "93.4% computational accuracy; 90.5% pass@1 (SELFEVOLVE w/o self-refinement; Table 3)",
            "performance_after_reflection": "94.8% computational accuracy; 92.4% pass@1 (SELFEVOLVE with self-refinement; Table 3)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Authors note that TransCoder problems are relatively simpler for ChatGPT and most improvement arises from the first refinement; the experimental refinement only targets a subset of error types (API/assertion/syntax). The authors also limited refinement to available test scripts and used greedy decoding; full generality to other error classes is not demonstrated.",
            "uuid": "e7176.2",
            "source_info": {
                "paper_title": "SelfEvolve: A Code Evolution Framework via Large Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Self-Debugging (baseline, implemented zero-shot)",
            "name_full": "Teaching Large Language Models to Self-Debug (simple variant, zero-shot implementation in this paper)",
            "brief_description": "Baseline method where an LLM is taught to revise SQL or Python code using executor feedback; original paper proposes three debugging modes ('simple', 'unit test', 'explanation'); in this work the authors implement the 'simple' variant in a zero-shot way as a comparison baseline.",
            "citation_title": "Teaching large language models to self-debug",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (ChatGPT) (zero-shot implementation)",
            "model_description": "Instruction-following LLM used in a zero-shot implementation of the Self-Debugging method (simple variant).",
            "model_size": null,
            "reflection_method_name": "Self-Debugging ('simple' variant)",
            "reflection_method_description": "Use an application-level executor (SQL or Python interpreter) to obtain runtime/diagnostic feedback and prompt the LLM to fix the code; variants include using unit tests or explanations and, in some reported implementations elsewhere, peeking one ground-truth test case.",
            "iteration_type": "generate-then-reflect (debugging with executor feedback; 'simple' variant as implemented in this paper is a single or limited-step process)",
            "num_iterations": null,
            "task_name": "DS-1000, HumanEval, TransCoder (used as baseline across tasks)",
            "task_description": "Baseline applied by the authors across the same code generation/translation benchmarks used in the paper.",
            "evaluation_metric": "pass@1 (DS-1000 overall reported), pass@1 and pass@10 (HumanEval), computational accuracy (TransCoder)",
            "performance_before_reflection": null,
            "performance_after_reflection": "Reported baseline scores in paper: DS-1000 overall 53.00% pass@1 (Table 1), HumanEval 73.78% pass@1 and 87.80% pass@10 (Table 2), TransCoder 89.3% computational accuracy (Table 3).",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Paper notes limitations of Self-Debugging baseline: the original Self-Debugging sometimes peeks one ground-truth test case (which the current authors criticize as unrealistic for real coding scenarios), and the authors here implement a zero-shot 'simple' variant—precise iteration counts and behavior depend on that variant and are not fully specified in this paper. Additionally, SELFEVOLVE authors argue that self-generated knowledge combined with refinement yields larger gains than using refinement alone.",
            "uuid": "e7176.3",
            "source_info": {
                "paper_title": "SelfEvolve: A Code Evolution Framework via Large Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Teaching large language models to self-debug",
            "rating": 2
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2
        },
        {
            "paper_title": "Codet: Code generation with generated tests",
            "rating": 2
        },
        {
            "paper_title": "Check your facts and try again: Improving large language models with external knowledge and automated feedback",
            "rating": 1
        }
    ],
    "cost": 0.01266625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SelfEvolve: A Code Evolution Framework via Large Language Models</h1>
<p>Shuyang Jiang ${ }^{1}$, Yuhao Wang ${ }^{1}$, Yu Wang ${ }^{1,2 *}$<br>${ }^{1}$ Shanghai Jiao Tong University<br>${ }^{2}$ Shanghai AI Laboratory<br>{jiangshuyang, colane, yuwangsjtu}@sjtu.edu.cn</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have already revolutionized code generation, after being pretrained on publicly available code data. However, while various methods have been proposed to augment LLMs with retrieved knowledge and enhance the quality of code generation, the performance of these retrieval-based methods is limited by the strength of the retrievers used. In addition, while LLMs show great emergent ability, they still struggle to produce the correct code in one turn. To address these challenges, we propose a novel two-step pipeline, called SELfEvolve, that leverages LLMs as both knowledge providers and self-reflective programmers. Unlike retrieval-based methods, SELfEvolve obtains the knowledge from input prompts and generates intermediate code based on the generated knowledge. After that, SELfEvolve asks LLM to act as an expert programmer to perform debugging for the generated code. This is achieved by receiving the error message from the interpreter, without requiring special test cases for correctness verification. We evaluate SELfEvolve on three code generation datasets, including DS-1000 for data science code, HumanEval for software engineering code, and TransCoder for C++-to-Python translation. Our empirical experiments show that SELfEvolve outperforms strong baselines by a significant margin on all datasets. We also conduct exhaustive analytical experiments to validate the effectiveness of the two stages of SELfEvolve, and find that both are superior to other prompting-based methods. Further scalability analysis demonstrates that SELfEvolve can be adapted to other more advanced models, such as GPT-4, and bring consistent efficacy improvement.</p>
<h2>1 Introduction</h2>
<p>Code generation functions as a crucial and challenging component of various applications [2, 19, 20, 26]. However, the performance of large language models (LLM) on diverse tasks and domains has substantially improved as the pretraining corpus expands. As a result, LLM has become the preferred model for code generation [9, 28]. In fact, LLM performs much better than previous deep neural models dedicated to generating code [12, 26, 32]. Meanwhile, previous methods have been augmented by LLM's ability to digest various prompt contents and perform text generation [3, 22, 31]. Various auxiliary augmentation signals have been added to the prompt to obtain more accurate code $[8,18,44,56]$. However, most prior work usually obtains such signals via an external retriever and a large knowledge base. They leverage the problem description or natural language intents to retrieve relevant knowledge, including similar code snippets [35, 36], API documentation [49, 56], or focal methods [25]. Despite their success, retriever models can suffer from domain mismatch when adapting to different tasks, requiring finetuning or even training from scratch on the target domain,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>which limits their generality. Moreover, current retrievers are not well-suited for semi-structured knowledge items like library documentation, which can result in poor retrieved results.</p>
<p>To avoid domain mismatch and inaccurate retrieval results, we propose a two-stage paradigm, SELFEVOLVE, which treats LLM itself as a knowledge source. Previous work has demonstrated that LLM has encoded various domain knowledge [14] and can be treated as a large knowledge base [1, 39]. Therefore, SELFEVOLVE chooses to prompt LLM to generate multi-form necessary knowledge by itself. In particular, we prompt the language models to extract necessary knowledge from trial solutions or problem intents (§3.2), depending on whether the problem intents contain explicit demands. This process excludes the intervention of retrievers since generating concrete knowledge based on roughly encoded ones in LLM parameters is easier than searching them in a large database with vague natural language statements. To the best of our knowledge, SELFEVOLVE is the first LLM-driven self-augmented code generation framework. Furthermore, inspired by the fact that human programmers rely on both related knowledge and a debugger to ensure implementation correctness, we inject an automatic refinement mechanism. This refinement mechanism teaches language models to depend on an executor like a Python interpreter to correct the preliminary code. We construct a runnable program from generated code and test cases extracted from the problem description (§3.2) and execute it to obtain either pass or error messages, which serve as correction feedback. Compared to prompting LLM to generate test cases like CodeT [8], which may produce incorrect samples, SELFEVOLVE maintains the correctness of test cases. Additionally, we do not grab evaluation samples from the test set like the recently proposed Self-Debugging [10], which hardly generalizes to daily coding scenarios. Instead, the example cases in the problem description appear in coding tasks mostly and describe the behavior that the programmer needs to accomplish with little ambiguity. Leveraging these authentic and common test cases makes SELFEVOLVE a reliable and general method for self-augmented code generation.
We primarily build SELFEVOLVE using gpt-3.5-turbo ${ }^{1}$ (ChatGPT) and evaluate its performance on various tasks, including the data science code generation task DS-1000 [26], the general code generation task HumanEval [9], and the C++-to-Python translation task TransCoder [43]. Extensive experiments show that SELFEVOLVE achieves a significant improvement in execution-based measurements compared to strong DocPrompting [56] and Self-Debugging [10] baselines on the data science code generation task (§4.2). On HumanEval, SELFEVOLVE still outperforms each strong baseline, and the self-refinement module brings noticeable performance improvements for the base LLM after using self-generated knowledge (§4.2). Even on the code translation, which is a much simpler task for ChatGPT, SELFEVOLVE still brings considerable improvement (§4.2). Furthermore, our analysis studies indicate that SELFEVOLVE can provide more accurate knowledge than retrieval-based methods (§4.3), generalize to various datasets with only a few debugging turns (§4.3), and scales easily to more powerful models like GPT-4 (§4.3). Finally, we use two intuitive cases to demonstrate how the two stages of SELFEVOLVEimprove the generated code. These cases illustrate the effectiveness of the model in generating high-quality code and highlight its potential for a range of applications.</p>
<h1>2 Related Work</h1>
<p>Augmented code generation In addition to problem descriptions and code snippets, many works provide auxiliary information to generate code. Before the era of LLM, researchers trained an encoder-decoder model that aimed to generate code based on a programmatic environment and function documentation [21]. JuPyT5 [7] conditions on Jupyter notebooks' context cells to generate data science code. Recently, large language models (LLM), pretrained on a variety of corpus, have enabled an in-context learning pipeline for zero-shot or few-shot generation. Haluptzok et al. [17] introduced a method to solve programming puzzles with synthetic puzzles and solutions generated by LLM. Parvez et al. [35] augmented code generation models with retrieved similar code snippets. In contrast, API-specific documents retrieved via a CodeT5 [47] retriever serve as additional information in the prompt [56]. However, their methods involve retrieving semi-structured knowledge items, whose performance is bottlenecked by current retriever models. Moreover, it is hard for retrievers to adapt to the target domain when the corpus for finetuning the retriever is inaccessible. Compared to theirs, LLM is more suitable for bridging the gap between domains than any small retriever, which provides more accurate knowledge. Moreover, our method does not require domain-specific finetuning, offering higher accessibility and generality.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The SELFEVOLVE pipeline. LLMs first generate corresponding knowledge for the related problem, then generate the trial answer conditioned on the knowledge. The iterative refinement step uses test cases and generated code snippets to form executable programs and then prompts LLM to refine the answer code based on the feedback thrown by the interpreter.</p>
<p>Automatic code refinement Language models often output unreliable information with high confidence [24]. This unreliability is reflected in bug code snippets when generating code. Some previous works have trained exclusive models dedicated to repairing incorrect code, which only accept the bug code as input [16, 51]. Other works have fused auxiliary information obtained from execution, such as the stack trace [15, 45] and error information raised by a compiler [50]. In recent work, LLMs have been leveraged to act as "teachers" to fix bugs hidden in the code. CodeT [8] uses synthetic test cases obtained through Codex [9] to select correct codes. However, CodeT is applied in an unnatural scenario where all coding problems are formatted as writing a function with certain inputs. Madaan et al. [30] iteratively refines the output with model-generated feedback in multiple tasks, except for code generation. Chen et al. [10] uses similar methods in code generation, but their work peeps one test case from the ground truth, which is impossible when solving real problems. Without exposing test cases, our method is much more flexible and closer to a real coding scenario.</p>
<h1>3 SELFEVOLVE: Code Evolution via Large Language Models</h1>
<p>This section first briefly introduces the code generation paradigm inspired by natural programming practice. We then present the concept of SELFEVOLVE, a two-step method that utilizes language models as a self-bootstrapping knowledge enhancer and an expert programmer with self-reflection, without external models or knowledge bases. After presenting the concept, we delve into the two primary components of SELFEVOLVE, which are built up to form a fully LLM-driven generation pipeline without the need for fine-grained prompt designs or further finetuning steps. The overall pipeline of our method is presented in Figure 1.</p>
<h3>3.1 BackGround</h3>
<p>Code generation formulation Given a problem description written in natural language $d$, and code context $c$, the autoregressive language model $p_{\theta}$ predicts the solution as</p>
<p>$$
P(Y)=\prod_{i=1 . n} p_{\theta}\left(Y_{i} \mid Y_{&lt;i}, X\right), Y_{&lt;1}=\emptyset
$$</p>
<p>where $n$ is the prediction length and $X=[d ; c]$ is the concatenation of $d$ and $c$.</p>
<p>Two-step code generation pipeline Conditioning solely on problem description for generation is still hard for LLM. Inspired by most programmers who often refer to knowledge documentation [42] and struggle to debug with current tools [34], we divide prompt-based code generation methods into two steps. The first step prompts language models to comprehend extra knowledge and task-specific instructions [4, 23, 31, 37], while the next one teaches models to revise the generated code solution through feedback from humans or an oracle instructor. In the two-step pipeline, the second generation step never deteriorates the intermediate output of the first step. Therefore, these two steps follow a topological order in terms of optimization, and can be optimized in order and fused together.</p>
<h1>3.2 SelfEvolve: the Proposed Two-step Pipeline</h1>
<p>Based on the above analysis, we propose SelfEvolve, which improves both steps by enabling generated code to evolve progressively using only a large language model, without requiring any learning. SelfEvolve generates code by conditioning on the knowledge in the prompt, as previous work has done. However, the knowledge is generated by LLM instead of being retrieved from external knowledge bases. After obtaining the output of the first step, SelfEvolve uses LLM to iteratively revise the generated code. This process follows Chen et al. [10] to correct code errors by leveraging feedback from a code executor, but does not necessitate the use of specific test cases.</p>
<p>Generating knowledge with language models Conditioning a language model on the knowledge in the prompt is crucial, yet challenging. Given $m$ knowledge items $K[1 . . m]$, the language model predicts the next token to generate the final code solution:</p>
<p>$$
P(Y \mid K)=\prod_{i=1 . . n} p_{\theta}\left(Y_{i} \mid Y_{&lt;i}, X, K\right), Y_{&lt;1}=\emptyset
$$</p>
<p>Knowledge can be retrieved via a sparse retriever [41] or a dense retriever [13, 40] as such:</p>
<p>$$
K:=\arg \max _{K \subset B} P(K \mid X, B)
$$</p>
<p>where $B$ is the whole database. However, the performance of current retriever models may be limited, resulting in $K$ containing irrelevant knowledge items that add noise to LLM and harm the generation results. A widely-used approach to mitigate this problem is to retrieve as much knowledge as possible [3] to cover the necessary items. However, this method places demands on the ability of LLM to process long texts, which is still a work in progress [27, 38].
To more accurately and conveniently obtain the necessary knowledge, we utilize language models as knowledge sources, prompting them to generate information. Large language models have encoded knowledge from a variety of databases into their parameters after being pre-trained on various corpora [14]. Additionally, models that undergo reinforcement learning from human feedback (RLHF) [33] can follow human instructions, serving as a natural knowledge source and providing miscellaneous knowledge based on appropriate input instructions. Based on this, we propose to use self-generated knowledge which is fetched via prompting LLMs as such:</p>
<p>$$
p(K)=\prod_{i=1 . . k} p_{\theta}\left(K_{i} \mid X, K_{&lt;i}\right), K_{&lt;1}=\emptyset
$$</p>
<p>where $k$ is the length of generated knowledge tokens. When problem descriptions contain implicit intents, such as in StackOverflow [26], there is often a gap between the detailed knowledge required and the words used to describe the problem. This gap arises because deriving the required knowledge involves reasoning. To narrow this reasoning gap and obtain more precise knowledge, we decompose this extraction process when intents are implicitly given:</p>
<p>$$
p(K)=\prod_{i=1 . . k} p_{\theta}\left(K_{i} \mid c, K_{&lt;i}\right) \cdot p_{\theta}(c \mid X), K_{&lt;1}=\emptyset
$$</p>
<p>where $c$ is a trial code solution from LLM based only on problem contexts. It contains necessary but potentially misused knowledge that can benefit the extraction of knowledge $(K) . K$ can be formatted as any problem-specific structure to fit the problem instruction $X$, making it suitable for various tasks. When problem descriptions $X$ contain explicit intents, SelfEvolve uses Eq. 4 instead, as LLM can easily extract knowledge with high accuracy in this case.</p>
<p>Revision of generated solution Previous studies have shown that intermediate results generated from LLM may contain mistakes [29, 46, 48, 54]. Such errors can introduce noise to the prompt context, reducing the accuracy of the final output. To reduce code errors, we mimic the debugging process of programmers and introduce an iterative self-refinement mechanism to rectify the buggy code. This mechanism leverages an external Python interpreter to revise erroneous programs. Our approach incorporates code context and sample test cases into the input prompt, along with the generated code solution, to form an executable program. We then execute this program in a sandbox environment to receive error information as well as standard output. Once error information is obtained, we prompt language models to revise the buggy programs, conditioned both on program requirements and error information:</p>
<p>$$
P\left(Y^{\prime} \mid X, Y, K, e\right)=p_{\theta}\left(Y^{\prime} \mid X, Y, e\right) \cdot p_{\theta}(Y \mid X, K)
$$</p>
<p>The revised output, $Y^{\prime}$, may still contain bugs. Therefore, the above process is repeated until the code can be interpreted without exceptions, or until the iteration steps reach a fixed threshold. In practical applications, the modeling of $p_{\theta}\left(Y^{\prime} \mid X, Y, e\right)$ varies depending on the type of error. For simplicity, SELFEVOLVE only corrects API errors and incorrect assertion statements. We find that correcting these two types of errors contributes significantly to performance improvement in the empirical experiments.</p>
<p>In conclusion, we combine two LLM-driven methods - generation based on self-generated knowledge and refinement via error message - to create a more effective method called SELFEVOLVE. These two components reinforce each other almost orthogonally. On one hand, self-generated knowledge boosts self-refinement steps. By conditioning on knowledge from models' parameters, intermediate output explicitly applies the knowledge. With more accurate output, the self-refinement steps require fewer iterations to repair the code, resulting in lower difficulty. On the other hand, the self-refinement steps improve the application of generated knowledge. The input knowledge may contain irrelevant information or noise during the generation process. The self-refinement steps eliminate this noise by introducing an external interpreter, improving the overall quality of the generated code. Later empirical experiments will demonstrate how these two modules reinforce each other.</p>
<h1>4 Experiments</h1>
<p>In this work, we present a novel pipeline that supports natural and reliable code generation for a variety of programming and data science problems. To evaluate its effectiveness, we conducted experiments using three different code generation tasks: data science code generation, simple algorithm coding, and C++-to-Python code translation. These tasks were assessed using the DS1000 [26], HumanEval [9], and TransCoder [43] benchmarks, respectively. In all experiments, we set the top-p cutoff to 0.95 and the maximum generation length to 1024 . For the specific prompts used for each task, please refer to Appendix B and C.</p>
<h3>4.1 Baselines</h3>
<ol>
<li>DocPrompting [56]: DocPrompting improves the LLM by retrieving problem-relevant documentation via a finetuned retriever, then condition on those documents and problem description to generate code. We use the same documentation pool for DocPrompting in DS-1000, as the problem source of DS-1000 is the same as that of CoNaLa [52]. We also use the same retrieval weights released by them, as DS-1000 are also built to test Python programming.</li>
<li>Self-Debugging [10] relies on a SQL application or Python interpreter to teach language models to revise SQL commands or Python code with bugs. They propose three debugging ways, including "simple", "unit test" and "explanation". Without training sets in DS-1000 and HumanEval, we implement it in a zero-shot way and use its "simple" variant for a fair comparison.</li>
<li>SelfEVolve: SelfEvolve is the code generation pipeline proposed in this work. In the main experiments, We use ChatGPT as the knowledge generator and the code refiner.</li>
</ol>
<h3>4.2 Main Results of SelfEvolve</h3>
<p>Data science code generation For data science code generation tasks, We selected the DS-1000 [26] benchmark, which contains 1000 problems covering seven common data science libraries. DS-1000 introduces a novel "perturbation" concept, including Origin, Surface, Semantic, and Diff-Rewrite,</p>
<p>Table 1: Pass@1 results on the DS-1000 dataset. ${ }^{\dagger}$ denotes that the results are referred from [26]. Other baselines are implemented with the same prompt and hyperparameter setting.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Perturbation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Origin</td>
<td style="text-align: center;">Surface</td>
<td style="text-align: center;">Semantic</td>
<td style="text-align: center;">Diff-Rewrite</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Prior work</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Codex (Completion) ${ }^{\dagger}$</td>
<td style="text-align: center;">44.93</td>
<td style="text-align: center;">37.94</td>
<td style="text-align: center;">34.35</td>
<td style="text-align: center;">16.94</td>
<td style="text-align: center;">39.20</td>
</tr>
<tr>
<td style="text-align: center;">Codex (Insertion) ${ }^{\dagger}$</td>
<td style="text-align: center;">47.76</td>
<td style="text-align: center;">50.18</td>
<td style="text-align: center;">38.39</td>
<td style="text-align: center;">21.05</td>
<td style="text-align: center;">43.30</td>
</tr>
<tr>
<td style="text-align: center;">DocPrompting</td>
<td style="text-align: center;">53.95</td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;">39.57</td>
<td style="text-align: center;">25.93</td>
<td style="text-align: center;">45.50</td>
</tr>
<tr>
<td style="text-align: center;">Self-Debugging</td>
<td style="text-align: center;">63.38</td>
<td style="text-align: center;">59.21</td>
<td style="text-align: center;">45.65</td>
<td style="text-align: center;">28.40</td>
<td style="text-align: center;">53.00</td>
</tr>
<tr>
<td style="text-align: center;">This work</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">60.31</td>
<td style="text-align: center;">52.63</td>
<td style="text-align: center;">41.30</td>
<td style="text-align: center;">26.54</td>
<td style="text-align: center;">49.30</td>
</tr>
<tr>
<td style="text-align: center;">SelfEVOLVE</td>
<td style="text-align: center;">66.23</td>
<td style="text-align: center;">67.11</td>
<td style="text-align: center;">48.70</td>
<td style="text-align: center;">33.95</td>
<td style="text-align: center;">57.10</td>
</tr>
<tr>
<td style="text-align: center;">w/o self-refinement</td>
<td style="text-align: center;">60.09</td>
<td style="text-align: center;">59.21</td>
<td style="text-align: center;">41.30</td>
<td style="text-align: center;">29.01</td>
<td style="text-align: center;">50.60</td>
</tr>
</tbody>
</table>
<p>representing the difficulty of problems in ascending order, making it a challenging benchmark. In this study, we prompted language models to generate problem-relevant API documentation as domain-required knowledge. For the self-refinement module, we checked the executable programs and prompted language models to fix syntax errors only. We used greedy decoding and reported the pass@1 [9] score for each method. Results are presented in Table 1. Without further refinement steps, SELFEVOLVE has already exceeded the strong ChatGPT baseline on the Surface and Diff-Rewrite perturbation types, by a margin of 6.58 and 2.47, respectively. Moreover, with an additional self-debug module, SELFEVOLVE substantially improves over other baselines, with a 7.8 (relatively 15.8\%) pass@1 gain compared to ChatGPT, on average. SelfEVolve also surpasses the prompt-based method, Self-Debugging, by a convincing 4.1 performance margin. We also noticed that integrating the self-generated knowledge with the self-refinement module gains much higher improvement. Specifically, SELFEVOLVE improves the baseline in terms of all perturbation types, demonstrating that our method can impressively enhance the robustness of large language models.</p>
<p>General code generation For general code generation tasks, we evaluated SelfEVolve on HumanEval [9]. This dataset contains 164 hand-written Python programming problems with an average of 7.7 test cases each. We implemented Self-Debugging methods on ChatGPT and reported its score. We did not implement DocPrompting since no library documentation is required in HumanEval. We also introduced the GPT-4 results from [5] for comparison. We reported a pass@1 score for greedy decoding and pass@10 for 10-sample decoding. For the 10-sample generation, we conducted a grid search to set the temperature to $t=1$. Unlike DS-1000, we induced LLM to explicitly output problem-related algorithms as external knowledge and taught LLM to fix assertion errors and syntax errors. The results in Table 2 demonstrate that the strong ChatGPT baseline significantly benefits from our SELFEVOLVE method, with an 11.59 pass@1 gain and a 6.71 pass@10 gain. This leaves a small 3.95 pass@1 gap from GPT-4. Notably, with self-generated knowledge, the self-refinement module again harvested a larger improvement ( +3.66 pass@10) than only applying a refinement module like Self-Debugging ( +1.22 pass@10). This empirically verifies that self-generated knowledge helps to reduce most errors and produce more precise results.</p>
<p>Python code translation As suggested by Roziere et al. [43], we experimented with our methods on the TransCoder [43] dataset. We used its test set, which requires translating C++ code to Python, and filtered out problems without testing scripts, resulting in 410 valid problems. In addition to pass@1, we followed Roziere et al. [43] by using another evaluation metric, computational accuracy, to test each model. Computational accuracy computes the accuracy score for each problem in a competition rule, where each sample code is scored as the percentile of its passed test cases, while the pass@1 metric is computed as whether the sample code has passed all test cases. We prompted LLM to generate the algorithm detail of the C++ code, which serves as the context for Python code generation. Results in Table 3 indicate that our proposed method, SELFEVOLVE, achieves the best performance among other prompt-based methods, even outperforming Self-Debugging which peeps one ground truth test case. Built upon a strong ChatGPT baseline, SELFEVOLVE further improves</p>
<p>Table 2: Pass@1 and pass@10 scores comparisons with different methods on HumanEval. We use the same prompt to implement each method. ${ }^{\dagger}$ denotes that scores are cited from [5].</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Pass@1</th>
<th style="text-align: center;">Pass@10</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Prior Work</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 ${ }^{\dagger}$</td>
<td style="text-align: center;">82.00</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">text-davinci-003 ${ }^{\dagger}$</td>
<td style="text-align: center;">65.00</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">66.46</td>
<td style="text-align: center;">86.58</td>
</tr>
<tr>
<td style="text-align: center;">CodeT [8]</td>
<td style="text-align: center;">65.20</td>
<td style="text-align: center;">86.80</td>
</tr>
<tr>
<td style="text-align: center;">Self-Debugging</td>
<td style="text-align: center;">73.78</td>
<td style="text-align: center;">87.80</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SELFEvOlVE</td>
<td style="text-align: center;">78.05</td>
<td style="text-align: center;">93.29</td>
</tr>
<tr>
<td style="text-align: center;">w/o self-refinement</td>
<td style="text-align: center;">70.73</td>
<td style="text-align: center;">89.63</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance comparison on TransCoder dataset where we follow [10, 43] to translate C++ code to Python code. All methods in this work are implemented with greedy decode. "Acc." refers to computational accuracy.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">TransCoder</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Pass@1</td>
</tr>
<tr>
<td style="text-align: center;">Prior Work</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PaLM [11]</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">PaLM-Coder [11]</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Codex [9]</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Self-Debugging [10]</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">92.7</td>
<td style="text-align: center;">90.0</td>
</tr>
<tr>
<td style="text-align: center;">SELFEvOlVE</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">92.4</td>
</tr>
<tr>
<td style="text-align: center;">w/o self-refinement</td>
<td style="text-align: center;">93.4</td>
<td style="text-align: center;">90.5</td>
</tr>
</tbody>
</table>
<p>by 2.1 computational accuracy and 2.4 pass@1, respectively. Without the self-refinement module, SELFEvOlVE still improves over ChatGPT, with a 0.7 accuracy gain and 0.5 pass@1 gain.</p>
<h1>4.3 Discussion</h1>
<p>In this section, we conduct various analysis experiments to validate the efficacy of our proposed SELFEvOlVE . We first present the impact of the number of iteration step on the final performance of SELFEvOlVE. After that, we demonstrate how our generated knowledge is superior to retrieved knowledge, through a human evaluation experiment. Finally, we extend our method to an even more intelligent language model (GPT-4) to empirically show the scalability of SELFEvOlVE.</p>
<p>How do iteration steps affect performance? In §3.2, we declared that the refinement module is iteratively run to fix bugs. In this experiment, we determine under what conditions we should stop the refinement module. We ran the self-refinement module for different iteration turns on three datasets using greedy decoding and tested the pass@1 score of ChatGPT using the same prompt for each iteration stage. Figure 2a presents the detailed results. We observed that the major improvement came from the first refinement step for the HumanEval and TransCoder datasets. On DS-1000, however, the performance improved almost uniformly as we increased the number of refinement steps until the third iteration. This discrepancy across datasets resulted from the much more difficult nature of the problems in DS-1000 compared to the other two datasets. Therefore, the self-refinement
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: (a) Performance-iteration curves of SELFEvOlVE on DS-1000, HumanEval and TransCoder datasets. (b) Precision and recall comparisons between generated knowledge and retrieved one.</p>
<p>Table 4: Comparison between SELFEVOLVE using ChatGPT and GPT-4 baselines. We bind SELFEVOLVE with ChatGPT and GPT-4 to test its generalization.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">DS-1000</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">HumanEval</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Scipy</td>
<td style="text-align: center;">Pytorch</td>
<td style="text-align: center;">Sklearn</td>
<td style="text-align: center;">Matplotlib</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SELFEVOLVE (ChatGPT)</td>
<td style="text-align: center;">52.83</td>
<td style="text-align: center;">64.71</td>
<td style="text-align: center;">73.04</td>
<td style="text-align: center;">78.06</td>
<td style="text-align: center;">78.05</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">52.83</td>
<td style="text-align: center;">44.12</td>
<td style="text-align: center;">60.00</td>
<td style="text-align: center;">69.03</td>
<td style="text-align: center;">82.00</td>
</tr>
<tr>
<td style="text-align: center;">SELFEVOLVE (GPT-4)</td>
<td style="text-align: center;">58.49</td>
<td style="text-align: center;">70.59</td>
<td style="text-align: center;">70.43</td>
<td style="text-align: center;">84.52</td>
<td style="text-align: center;">89.02</td>
</tr>
<tr>
<td style="text-align: center;">w/o self-refinement</td>
<td style="text-align: center;">57.55</td>
<td style="text-align: center;">63.24</td>
<td style="text-align: center;">66.09</td>
<td style="text-align: center;">69.03</td>
<td style="text-align: center;">84.76</td>
</tr>
</tbody>
</table>
<p>module brought consistent improvement under different refinement stages. This finding suggests that SELFEVOLVE requires more refinement steps when processing difficult problems, but only one debugging turn is sufficient to bring major improvement for less complicated problems.</p>
<p>Human evaluation of self-generated knowledge To better understand the superiority of generated knowledge in realistic scenarios, we conducted a human evaluation study to demonstrate that the generated knowledge is more relevant to the problem topic than a retrieval-based one. We randomly selected 200 problems from seven libraries of DS-1000 and asked two data science experts to count the number of correctly provided API documents to determine whether the API knowledge matched the solution. We then used two common metrics, precision and recall [6], to assess the accuracy of the knowledge in accordance with the oracle answer. Precision is defined as the percentage of correctly provided documents in the provided document set, while recall measures the percentage of correct document items in the oracle document set. More details on the experiment are presented in Appendix A. The comparison bar chart is shown in Figure 2b. We observed that in all libraries, the generated knowledge was much more accurate than the retrieved one in both metrics. Notably, the retrieved knowledge showed little match with oracle solutions in most libraries because the retrieval queries in DS-1000 are too complicated and contain implicit API demands. In the Matplotlib library, where the queries are simple and the demands are explicitly stated, the retrieved knowledge matched the problem requirements slightly but still lagged far behind the generated one. One key reason for the superiority of generated knowledge is that LLMs can bridge the reasoning gap between problem descriptions and knowledge terminology better than a retriever model. This is also the reason behind Eq. 5. In other words, generated knowledge is able to provide a more comprehensive and accurate understanding of the problem topic, which is crucial in realistic scenarios.</p>
<p>Scaling to more powerful models To evaluate the scalability of SELFEVOLVE in more advanced language models, we integrated SELFEVOLVE with GPT-4 without requiring excessive prompt engineering. GPT-4 has demonstrated significantly greater intelligence and reasoning abilities compared to ChatGPT [5]. However, due to limited GPT-4 API-Key access, we only conducted experiments on the Scipy, Pytorch, Sklearn, and Matplotlib libraries of DS-1000, which includes a total of 444 problems and HumanEval. We used the same prompt as ChatGPT and used greedy decoding to report the pass@1 score. The results for both datasets are shown in Table 4. This experiment demonstrates that our approach can benefit from more advanced backbone models instead of degrading them. In contrast to the ChatGPT-based version, SELFEVOLVE on GPT-4 achieved higher pass@1 scores on Scipy ( +4.72 ), Pytorch ( +19.12 ), and Sklearn ( +6.09 ). A similar performance gain was also observed in HumanEval, where SELFEVOLVE improved the already higher pass@1 score by 2.76. Furthermore, after adding the lightweight self-refinement plugin, GPT-4 demonstrated further improvements on all datasets. This highlights the effectiveness of leveraging advanced backbone models and the potential of our approach in producing superior results.</p>
<h1>4.4 Case Study</h1>
<p>This section demonstrates the effectiveness of SELFEVOLVE with two representative examples shown in Figure 3. In the first example, LLM generates specific documentation tf. one_hot for the problem. Compared to the vanilla output without documentation, language models output extra codes that cannot generalize to other test cases. In contrast, conditioning on the concrete API documentation, language models output more deterministic code without transforming the original labels to a tensor type. The provided documentation sharpens the probability curve and contributes to a more accurate</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Two examples to show the efficacy of our proposed SELFEVOLVE methods, where red codes are wrong codes. (a) Comparison between with and without generated documentation. (b) Comparison between with and without self-refinement module.
and general answer. In the second example, language models read the np. asarray documentation, but forgets to compute the AVG value. By leveraging the traceback information without the specific test cases, language models can revise their code and use a more general method to solve the problem. Both examples illustrate how the two methods in SELFEVOLVE enhance each other to improve performance. SELFEVOLVE helps make the output of language models more general and accurate.</p>
<h1>5 Limitation \&amp; Future Work</h1>
<p>Although SELFEVOLVE has shown promising results in generating knowledge and improving the performance of large language models, there are still some limitations that need to be addressed. One of the main challenges of SELFEvolve is that it may not always be automatic when used in different tasks due to the hand-written prompting words. This means that its effectiveness may be limited when applied to other use cases. Another limitation of SELFEVOLVE is that the generated knowledge may not always be suitable for every task and may require fine-grained selection to be effective. However, these issues can be mitigated by developing suitable prompting skills. For example, a more comprehensive set of prompting words could be developed to make it easier to adapt SELFEVOLVE to new tasks. Additionally, a more sophisticated algorithm could be developed to automatically select the appropriate knowledge for a given task. We believe that addressing these issues will make SELFEVOLVE a more versatile and useful framework in different contexts.</p>
<h2>6 Conclusion</h2>
<p>We propose SELFEVOLVE, a simple yet effective method for solving code generation problems using large language models (LLMs) as a fully LLM-driven framework. It acts as both a knowledge provider and a self-reflective programmer to generate high-quality code in two steps, both of which are run with a single LLM. This makes it more flexible and extendable to other datasets, such as Spider [53] or APPS [19], without requiring an extra retriever or previously set up database. Substantial experiments on diverse code generation tasks have verified that SELFEVOLVE can bring great performance gains under various tasks and datasets, and outperform two strong prompting-based methods by a good margin. Furthermore, various analysis experiments indicate that SELFEVOLVE is more suitable for providing problem-related knowledge compared to traditional dense retrievers, and can be easily scaled to more intelligent language models to bring further improvements.</p>
<h1>References</h1>
<p>[1] Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona Diab, and Marjan Ghazvininejad. A review on language models as knowledge bases. arXiv preprint arXiv:2204.06031, 2022.
[2] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.
[3] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pages 2206-2240. PMLR, 2022.
[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[5] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4, 2023.
[6] Michael Buckland and Fredric Gey. The relationship between recall and precision. Journal of the American society for information science, 45(1):12-19, 1994.
[7] Shubham Chandel, Colin B Clement, Guillermo Serrato, and Neel Sundaresan. Training and evaluating a jupyter notebook data science assistant. arXiv preprint arXiv:2201.12901, 2022.
[8] Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. Codet: Code generation with generated tests. CoRR, abs/2207.10397, 2022. URL https://doi.org/10. 48550/arXiv. 2207.10397.
[9] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
[10] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023.
[11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
[12] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling and synthesis. In The Eleventh International Conference on Learning Representations, 2023. URL https://doi.org/10. 48550/arXiv. 2204.05999.
[13] Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894-6910, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.552. URL https://aclanthology.org/ 2021.emnlp-main. 552.
[14] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. arXiv preprint arXiv:2012.14913, 2020.
[15] Kavi Gupta, Peter Ebert Christensen, Xinyun Chen, and Dawn Song. Synthesize, execute and debug: Learning to repair for neural program synthesis. Advances in Neural Information Processing Systems, 33: $17685-17695,2020$.
[16] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. Deepfix: Fixing common c language errors by deep learning. In Proceedings of the aaai conference on artificial intelligence, volume 31, 2017.
[17] Patrick Haluptzok, Matthew Bowers, and Adam Tauman Kalai. Language models can teach themselves to program better. CoRR, abs/2207.14502, 2022. URL https://doi.org/10.48550/arXiv. 2207. 14502 .</p>
<p>[18] Patrick Haluptzok, Matthew Bowers, and Adam Tauman Kalai. Language models can teach themselves to program better, 2023.
[19] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021.
[20] Junjie Huang, Chenglong Wang, Jipeng Zhang, Cong Yan, Haotian Cui, Jeevana Priya Inala, Colin Clement, Nan Duan, and Jianfeng Gao. Execution-based evaluation for data science code generation models. arXiv preprint arXiv:2211.09374, 2022.
[21] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Mapping language to code in programmatic context. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1643-1652, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1192. URL https://aclanthology.org/D18-1192.
[22] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane DwivediYu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models. arXiv preprint arXiv, 2208, 2022.
[23] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane DwivediYu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299, 2022.
[24] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38, 2023.
[25] Matthew Jin, Syed Shahriar, Michele Tufano, Xin Shi, Shuai Lu, Neel Sundaresan, and Alexey Svyatkovskiy. Inferfix: End-to-end program repair with llms. arXiv preprint arXiv:2303.07263, 2023.
[26] Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen-tau Yih, Daniel Fried, Sida Wang, and Tao Yu. Ds-1000: A natural and reliable benchmark for data science code generation. arXiv preprint arXiv:2211.11501, 2022.
[27] Mukai Li, Shansan Gong, Jiangtao Feng, Yiheng Xu, Jun Zhang, Zhiyong Wu, and Lingpeng Kong. In-context learning with many demonstration examples. arXiv preprint arXiv:2302.04931, 2023.
[28] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092-1097, 2022.
[29] Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. Faithful chain-of-thought reasoning. arXiv preprint arXiv:2301.13379, 2023.
[30] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.
[31] Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al. Augmented language models: a survey. arXiv preprint arXiv:2302.07842, 2023.
[32] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=iaYcJKpY2B.
[33] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida. Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.
[34] Chris Parnin and Alessandro Orso. Are automated debugging techniques actually helping programmers? In Proceedings of the 2011 international symposium on software testing and analysis, pages 199-209, 2011.</p>
<p>[35] Md Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Retrieval augmented code generation and summarization. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2719-2734, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.232. URL https://aclanthology.org/2021.findings-emnlp. 232.
[36] Panupong Pasupat, Yuan Zhang, and Kelvin Guu. Controllable semantic parsing via retrieval augmentation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7683-7698, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.607. URL https://aclanthology.org/2021. emnlp-main. 607 .
[37] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813, 2023.
[38] Bo PENG. RWKV-LM. https://github.com/BlinkDL/RWKV-LM, 82021.
[39] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463-2473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL https://aclanthology. org/D19-1250.
[40] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/abs/1908.10084.
[41] Stephen E Robertson and Steve Walker. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In SIGIR'94: Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, organised by Dublin City University, pages 232-241. Springer, 1994.
[42] Tobias Roehm, Rebecca Tiarks, Rainer Koschke, and Walid Maalej. How do professional developers comprehend software? In 2012 34th International Conference on Software Engineering (ICSE), pages 255-265. IEEE, 2012.
[43] Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample. Unsupervised translation of programming languages. Advances in Neural Information Processing Systems, 33, 2020.
[44] Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I. Wang. Natural language to code translation with execution. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3533-3546, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main. 231.
[45] Ke Wang, Rishabh Singh, and Zhendong Su. Dynamic neural program embeddings for program repair. In International Conference on Learning Representations, 2018.
[46] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.
[47] Yue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi. CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8696-8708, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main. 685. URL https://aclanthology.org/2021.emnlp-main.685.
[48] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.
[49] Frank F. Xu, Zhengbao Jiang, Pengcheng Yin, Bogdan Vasilescu, and Graham Neubig. Incorporating external knowledge through pre-training for natural language to code generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6045-6052, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.538. URL https://aclanthology.org/2020.acl-main.538.
[50] Michihiro Yasunaga and Percy Liang. Graph-based, self-supervised program repair from diagnostic feedback. In International Conference on Machine Learning, pages 10799-10808. PMLR, 2020.</p>
<p>[51] Michihiro Yasunaga and Percy Liang. Break-it-fix-it: Unsupervised learning for program repair. In International Conference on Machine Learning, pages 11941-11952. PMLR, 2021.
[52] Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. Learning to mine aligned code and natural language pairs from stack overflow. In International Conference on Mining Software Repositories, MSR, pages 476-486. ACM, 2018. doi: https://doi.org/10.1145/3196398.3196408.
[53] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3911-3921, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1425. URL https://aclanthology.org/D18-1425.
[54] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022.
[55] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697-12706. PMLR, 2021.
[56] Shuyan Zhou, Uri Alon, Frank F. Xu, Zhengbao Jiang, and Graham Neubig. Docprompting: Generating code by retrieving the docs. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=ZTCxT2t2Ru.</p>
<h1>A Comparison between Generated Knowledge and Retrieved Knowledge</h1>
<p>We here present the experiment details of this human evaluation experiment. We randomly select one-fifth of the problems in each library to form the 200 problem set, whose formation is shown in Table 5. API is defined as a function call, a method call or an attribute getter and setter in this case. For retrieved knowledge, we follow DocPrompting [56] to take problem descriptions as queries and their provided document pool as the target sets to perform the retrieval. We use their pretrained CodeT5 retriever and retrieve $k=5$ knowledge items from the target pool. For irrelevant documents, we filter them for both retrieved documents and generated ones, so the final number of documents for each problem may be smaller than $k$. After retrieval, we follow Zhao et al. [55] to put items with higher scores near the generation position to ensure the best generation result.</p>
<p>Table 5: Problem counts for each library in the selected set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Library</th>
<th style="text-align: center;">Tensorflow</th>
<th style="text-align: center;">Pytorch</th>
<th style="text-align: center;">Numpy</th>
<th style="text-align: center;">Matplotlib</th>
<th style="text-align: center;">Pandas</th>
<th style="text-align: center;">Sklearn</th>
<th style="text-align: center;">Scipy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">#Problems</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">26</td>
</tr>
</tbody>
</table>
<h2>B Prompt for the First-Stage of SelfEvolve in Each task</h2>
<p>We show the detailed prompt for the first step of SELFEvolve used in each task below. We show prompts for DS-1000, HumanEval and TransCoder in Figure 4, Figure 5, and Figure 6, respectively.</p>
<h2>C Prompt for Self-Refinement in Each task</h2>
<p>We show the detailed prompt for the self-refinement module used in each task below. We show prompts for DS-1000, HumanEval and TransCoder in Figure 7, Figure 8, and Figure 9, respectively.</p>
<p>problem:
(problem description)
A:
(code context)
[insert]
print(result)
Could you help to write solution codes and store the answer in the variable result? You only need to output the codes which can fill the [insert] block.
Please just output codes without any explanation and natural language.
Wrap your code with " $\cdots$ ".
This is a snippet of (library) code:
(code snippet)
api list
I want you to show the API used in the code.
Here are some rules for displaying the APIs:</p>
<ol>
<li>do not show basic API like print(), import, <strong>str</strong>, <strong>rpre</strong>, etc. Just show API about
( Library)</li>
<li>for class methods, make sure add the class name and library name before method with only two dots separated, like tf.random.normal.</li>
<li>do not output duplicate APIs</li>
</ol>
<p>Here's what APIs the above code calls line by line:</p>
<p>Please show me the following API's API specification:
generated doc
(api list)
You do not need to show me examples of each API.
Your answer should start with "1. "</p>
<p>Documentation
(generated doc)
problem:
(problem description)
A:
(code context)
[insert]
print(result)
Could you help to write solution codes and store the answer in the variable result? You only need to output the codes which can fill the [insert] block.
Please just output codes without any explanation and natural language.
Wrap your code with " $\qquad$ .</p>
<p>Figure 4: Prompt for the first step of SELFEVOLVE in the DS-1000 dataset.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Prompt for the first step of SelfEVolve in the HumanEval dataset.</p>
<div class="codehilite"><pre><span></span><code>Can you help me to explain how a piece of code work from the perspective of algorithms and
</code></pre></div>

<p>syntaxes? here is the code:</p>
<div class="codehilite"><pre><span></span><code>Cpp code)
</code></pre></div>

<p>Your explanation:</p>
<p>Based on your explanation, write a same function in python with the same function name, the same function argument and the same functionality.</p>
<p>Figure 6: Prompt for the first step of SelfEVolve in the TransCoder dataset.</p>
<div class="codehilite"><pre><span></span><code>(problem description)
    (code context)
    (error code)
After running the above code, it raises such error:
(error message)
It seems that this line
(bug_code)
has bugs.
Can you tell me what causes this bug?
</code></pre></div>

<div class="codehilite"><pre><span></span><code>    final solution
</code></pre></div>

<div class="codehilite"><pre><span></span><code>Conclude your code so that your answer should fit in the following code context:
(error code)
</code></pre></div>

<p>You only need to output codes that can fill in the [insert] block.
You do not need to output the codes before the [insert] block.
You only need to output the codes without explanation.
Wrap your code with " $\cdots$ ".</p>
<p>Figure 7: Prompt for self-refinement in the DS-1000 dataset.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 8: Prompt for self-refinement in the HumanEval dataset.</p>
<div class="codehilite"><pre><span></span><code>{error code}
</code></pre></div>

<div class="codehilite"><pre><span></span><code>The expected output of (test case) is (ground truth). However, the above code output (wrong
output).
Help me refine the code.
You should only output the codes without any explanation and natural language.
Wrap your code with &quot; &quot; &quot;
</code></pre></div>

<p>Figure 9: Prompt for self-refinement in the TransCoder dataset.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://platform.openai.com/docs/api-reference/chat&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>